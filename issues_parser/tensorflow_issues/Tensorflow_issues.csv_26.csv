Issue Number,Issue Title,Issue Body
39729,Is there any good detailed  description of GraphDef and Saved Model ?,"
## URL(s) with the issue:

N/A

## Description of issue (what needs changing):

Need clear documentation with details  on what makes all these different formats different. 

### Clear description

Looking for  a clear description  (pun not intended)  of  GraphDef and Saved Model - their diffrences etc., needs for  them ( and others such as ckpt ) .

### Correct links

N/A

### Parameters defined

N/A

### Returns defined

N/A

### Raises listed and defined

N/A

### Usage example

N/A

### Request visuals, if applicable

N/A

### Submit a pull request?

N/A
"
39724,Optimize recall and accuracy for a sub-class in multi-class classification,"Hi everyone,

Sorry about asking this question but I am a beginner with Tensorflow and Keras.
Here is my question :
- I have annotated 2.000 images of living cells, 8.000 images of dead cells and 8.000 images with nothing. They are labeleled respectively as ""Living"", ""Death"" and ""Empty"". (Images are 299x299x1)
- I have run some models, which basically look like this:

```
model = Sequential([
                    Conv2D(16, (5,5), activation='relu', input_shape=(299,299,1)),
                    MaxPooling2D(pool_size=(2,2)),
                    Conv2D(32, (5,5), activation='relu'),
                    Flatten(),
                    Dense(200),
                    Activation('relu'),
                    Dropout(0.3),
                    Dense(3),
                    Activation('softmax')
])

opt = SGD(lr=0.001)
model.compile(loss = ""categorical_crossentropy"", 
              optimizer = opt,
              metrics=['accuracy'])
```
- At the end of training, validation accuracy is around 90%. Nevertheless, when I check with confusion matrix, it appears that classification is very good to detect ""Empty"" images (accuracy almost 99%), but it is less good for separation of ""Living"" from ""Death"" images (circa 80%).
- The main purpose for me is to optimize recall and accuracy for classification of ""Living"" cells. 

=> That's why I am wondering if there is a way to track recall and accuracy for a specific sub-class ('Living"" here) like in early-stop.

Thank you for your help !

Kindly !



"
39722,tf.experimental.CsvDataset breaking down if select_cols is provided,"tensorflow 2.2.0
ubuntu 20.04
```
mnist = tf.keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train_ = pd.DataFrame(x_train.reshape(60000,-1),columns = ['col_'+str(i) for i in range(28*28)])
x_train_['col_cat1'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]
x_train_['col_cat2'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]
pd.DataFrame(x_train_).to_csv('x_train_.csv',index=False)

cdtypes = pd.read_csv('x_train_.csv',nrows=1).dtypes

cdtypes = cdtypes.sort_index()

x_train_ = tf.data.experimental.CsvDataset(
    'data/x_train_.csv', [np.nan if i == (float or int) else '__missing__' for i in cdtypes],
    select_cols=cdtypes.index, field_delim=',', use_quote_delim=True, na_value='',header=True,
)
```
output:
```
ValueError                                Traceback (most recent call last)
<ipython-input-47-8d8f52fc868d> in <module>
     12 x_train_ = tf.data.experimental.CsvDataset(
     13     'data/x_train_.csv', [np.nan if i == (float or int) else '__missing__' for i in cdtypes],
---> 14     select_cols=cdtypes.index, field_delim=',', use_quote_delim=True, na_value=''
     15 )

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/experimental/ops/readers.py in __init__(self, filenames, record_defaults, compression_type, buffer_size, header, field_delim, use_quote_delim, na_value, select_cols)
    703         select_cols,
    704         argument_default=[],
--> 705         argument_dtype=dtypes.int64,
    706     )
    707     self._element_spec = tuple(

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/data/util/convert.py in optional_param_to_tensor(argument_name, argument_value, argument_default, argument_dtype)
     30   if argument_value is not None:
     31     return ops.convert_to_tensor(
---> 32         argument_value, dtype=argument_dtype, name=argument_name)
     33   else:
     34     return constant_op.constant(

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1339 
   1340     if ret is None:
-> 1341       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1342 
   1343     if ret is NotImplemented:

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    319                                          as_ref=False):
    320   _ = as_ref
--> 321   return constant(v, dtype=dtype, name=name)
    322 
    323 

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    260   """"""
    261   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 262                         allow_broadcast=True)
    263 
    264 

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    268   ctx = context.context()
    269   if ctx.executing_eagerly():
--> 270     t = convert_to_eager_tensor(value, ctx, dtype)
    271     if shape is None:
    272       return t

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     94       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     95   ctx.ensure_initialized()
---> 96   return ops.EagerTensor(value, ctx.device_name, dtype)
     97 
     98 

ValueError: invalid literal for int() with base 10: 'col_0'
```"
39721,tf.distribute.MirroredStrategy() ERROR:root:'NoneType' object has no attribute 'write',"**System information**
- using Amazon Sagemaker
- instance type = 'ml.p3.8xlarge',

**Describe the current behavior**
with running model.fit(), prior to progress bar, following message is displayed.
`ERROR:root:'NoneType' object has no attribute 'write'`

**Describe the expected behavior**
Message to not be present given it's not present in documentation examples.
https://www.tensorflow.org/tutorials/distribute/keras

**Standalone code to reproduce the issue**

```
# Resources:
# https://huggingface.co/transformers/model_doc/roberta.html#tfrobertamodel
# https://www.tensorflow.org/tutorials/distribute/keras
# https://www.kaggle.com/xhlulu/jigsaw-tpu-xlm-roberta

###################################################################
# IMPORTS Pt.1
###################################################################
print(""running imports, pt.1..."")

import os
print(os.getcwd())
print(os.listdir(os.getcwd()))

###################################################################
# INSTALLS
###################################################################
print(""running installs..."")

os.system('pip install transformers')

###################################################################
# IMPORTS Pt.2
###################################################################
print(""running imports..."")

import time

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# TF Imports
import tensorflow as tf
print(""tf version: "", tf.__version__)

# Keras Imports
from tensorflow import keras
from tensorflow.keras import backend as K
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Dropout, Input
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler
from tensorflow.keras.callbacks import CSVLogger

# Sklearn
from sklearn import metrics
from sklearn.model_selection import train_test_split

# Garbage Collector
import gc
import sys

import transformers
# from transformers import TFAutoModel, AutoTokenizer
# from transformers import RobertaTokenizer, TFRobertaModel
from transformers import DistilBertTokenizer, TFDistilBertModel
from tqdm.notebook import tqdm
from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors

# Boto is the Amazon Web Services (AWS) SDK for Python, which allows Python developers to write software that makes
# use of Amazon services like S3 and EC2. Boto provides an easy to use, object-oriented API as well as low-level direct
# service access.
import boto3
#import s3fs
# args
import argparse

###################################################################
# HELPER FUNCTIONS
###################################################################
print(""defining helper functions..."")


def parse_args():
    parser = argparse.ArgumentParser()
    # data directories
    parser.add_argument('--data', type=str, default=os.environ.get('SM_CHANNEL_DATA'))
    return parser.parse_known_args()


def regular_encode(texts, tokenizer, maxlen=512):
    enc_di = tokenizer.batch_encode_plus(
        texts, 
        return_attention_masks=False, 
        return_token_type_ids=False,
        pad_to_max_length=True,
        max_length=maxlen
    )
    return np.array(enc_di['input_ids'])


def build_model(transformer, max_len=512):
    """"""
    https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras
    """"""
    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=""input_word_ids"")
    sequence_output = transformer(input_word_ids)[0]
    cls_token = sequence_output[:, 0, :]
    out = Dense(1, activation='sigmoid')(cls_token)
    
    model = Model(inputs=input_word_ids, outputs=out)
    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])
    
    return model


def get_train_data(data_dir):
    train = pd.read_csv(os.path.join(data_dir, ""jigsaw_mjy_train_val_openaug_523200.csv""))
    test = pd.read_csv(os.path.join(data_dir, 'test.csv'))
    sub = pd.read_csv(os.path.join(data_dir, 'sample_submission.csv'))
    return train, test, sub

if __name__ == ""__main__"":
    
    ###################################################################
    # SETTINGS
    ###################################################################
    # Detect hardware, return appropriate distribution strategy
    try:
        # TPU detection. No parameters necessary if TPU_NAME environment variable is
        # set: this is always the case on Kaggle.
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
        print('Running on TPU ', tpu.master())
    except ValueError:
        tpu = None
    
    if tpu:
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        strategy = tf.distribute.experimental.TPUStrategy(tpu)
    else:
        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
        strategy = tf.distribute.MirroredStrategy()
        
    print(""REPLICAS: "", strategy.num_replicas_in_sync)
    
    ###################################################################
    # CONSTANTS
    ###################################################################
    print(""setting constants..."")
    
    # Configuration
    MAX_LEN = 192
    #MODEL = 'jplu/tf-xlm-roberta-large'
    #MODEL = 'roberta-base'
    MODEL = 'distilbert-base-multilingual-cased'
    myOutput = '/opt/ml/model/'
    expCounter = 1
    AUTO = tf.data.experimental.AUTOTUNE
    BATCH_SIZE = 16 * strategy.num_replicas_in_sync
    EPOCHS = 2
    
    ###################################################################
    # TOKENIZER
    ###################################################################
    print(""loading tozenizer..."")
    
    # First load the real tokenizer
    #tokenizer = AutoTokenizer.from_pretrained(MODEL)
    #tokenizer = RobertaTokenizer.from_pretrained(MODEL)
    tokenizer = DistilBertTokenizer.from_pretrained(MODEL)
    
    ###################################################################
    # DATA
    ###################################################################
    print(""loading training data..."")

    args, _ = parse_args()
    
    train, test, sub = get_train_data(args.data)
    x_train = regular_encode(train.comment_text.values, tokenizer, maxlen=MAX_LEN)
    x_test = regular_encode(test.content.values, tokenizer, maxlen=MAX_LEN)
    y_train = train.toxic.values
    
    x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.33, random_state=1331)
    
    train_dataset = (
        tf.data.Dataset
        .from_tensor_slices((x_train, y_train))
        .repeat()
        .shuffle(2048)
        .batch(BATCH_SIZE)
        .prefetch(AUTO)
    )
    
    valid_dataset = (
        tf.data.Dataset
        .from_tensor_slices((x_valid, y_valid))
        .batch(BATCH_SIZE)
        .cache()
        .prefetch(AUTO)
    )
    
    test_dataset = (
        tf.data.Dataset
        .from_tensor_slices(x_test)
        .batch(BATCH_SIZE)
    )

    ###################################################################
    # LOAD MODEL
    ###################################################################
    print(""loading model ..."")
    
    with strategy.scope():
        #transformer_layer = TFAutoModel.from_pretrained(MODEL)
        transformer_layer = TFDistilBertModel.from_pretrained(MODEL)
        model = build_model(transformer_layer, max_len=MAX_LEN)
    model.summary()
    
    ###################################################################
    # TRAINING
    ###################################################################
    print(""run training ..."")
    
    n_steps = x_train.shape[0] // BATCH_SIZE
    
    train_history = model.fit(
        train_dataset,
        steps_per_epoch=n_steps,
        validation_data=valid_dataset,
        epochs=EPOCHS
    )
    
    ###################################################################
    # SCORE
    ###################################################################
    print(""score test data ..."")
    sub['toxic'] = model.predict(test_dataset, verbose=1)
    sub.to_csv('submission.csv', index=False)
    
    ###################################################################
    # COMPLETE
    ###################################################################
    
    print(""Program complete!"")
    
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
ip-10-0-221-83:51:274 [1] NCCL INFO comm 0x7f5f70006a40 rank 1 nranks 4 cudaDev 1 nvmlDev 1 - Init COMPLETE
ip-10-0-221-83:51:275 [2] NCCL INFO comm 0x7f5f68006a40 rank 2 nranks 4 cudaDev 2 nvmlDev 2 - Init COMPLETE
ip-10-0-221-83:51:276 [3] NCCL INFO comm 0x7f5f6c007790 rank 3 nranks 4 cudaDev 3 nvmlDev 3 - Init COMPLETE
ip-10-0-221-83:51:268 [0] NCCL INFO Launch mode Group/CGMD
ERROR:root:'NoneType' object has no attribute 'write'
#015   1/5561 [..............................] - ETA: 61:32:20 - loss: 0.7133 - accuracy: 0.531
```

Full work can be found here:
https://github.com/yeamusic21/DistilBert-TF2-Keras-Multi-GPU-Sagemaker-Training
Data can be found here:
www.kaggle.com/yeayates21/jigsaw-multilingual-aug-mix
https://www.kaggle.com/c/jigsaw-multilingual-toxic-comment-classification/data
"
39720,"TFLite, Model with Conv2DTranspose fails to convert, fully quantization, int8","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): tf-nightly
- TensorFlow version (or github SHA if from source): tf-nightly


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

This issue is very similar to the [issue](https://github.com/tensorflow/tensorflow/issues/39718), but 
the problematic layer is Conv2DTranspose, so it is different model here.
I tested models with other layers and all are fine, except this one and the issue logged above, separately.

https://colab.research.google.com/drive/1g8wjs5D3N9blNpWYMIQ8R_AipZASUKH8?usp=sharing

```
import numpy as np
import tensorflow as tf

input_size = [5, 5, 2]
kernel_size = [3, 3, 6]
stride = [2, 2]

input_0 = tf.keras.layers.Input(shape=input_size)
layer_0 = tf.keras.layers.Conv2DTranspose(
            filters=kernel_size[-1],
            kernel_size=kernel_size[0:2],
            strides=stride,
            activation=None,
            use_bias=False,
            name = ""transpose_conv""
        )(input_0)
model = tf.keras.models.Model(inputs=[input_0], outputs=[layer_0])
model.summary()

keras_layer = [
  layer for layer in model.layers if layer.name == ""transpose_conv""
][0]
keras_layer.set_weights(
            [
                np.random.rand(
                    kernel_size[0],
                    kernel_size[1],
                    kernel_size[2],
                    input_size[2],
                ).astype(np.float32)
            ]
        )

num_calib = 1000
def _get_calib_data_func():
  def representative_data_gen():
    for _ in range(num_calib):
      yield [
        np.random.rand(
          1, input_size[0], input_size[1], input_size[2],
        ).astype(np.float32)
      ]

  return representative_data_gen

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = _get_calib_data_func()

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_model_INT8 = converter.convert()
```

**The output from the converter invocation**

```
RuntimeError                              Traceback (most recent call last)

<ipython-input-18-717edec90ae0> in <module>()
      3 
      4 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
----> 5 tflite_model_INT8 = converter.convert()

3 frames

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/optimize/calibrator.py in calibrate_and_quantize(self, dataset_gen, input_type, output_type, allow_float, resize_input)
     91     return self._calibrator.QuantizeModel(
     92         np.dtype(input_type.as_numpy_dtype()).num,
---> 93         np.dtype(output_type.as_numpy_dtype()).num, allow_float)
     94 
     95   def calibrate_and_quantize_single(self,

RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor functional_5/transpose_conv/Shape
Empty min/max for tensor functional_5/transpose_conv/Shape
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39719,TPU PyFunction results in UnavailableError: failed to connect to all addresses,"Similar to #38762, and unfixed in tf-nightly. See gist here https://colab.research.google.com/gist/jonashaag/128c2b9c51cb0df5d00ef072928631e3/38762.ipynb

```
ds = tf.data.Dataset.from_generator(f, output_types='int32').map(g)
(_ for _ in ds)
```

```
UnavailableError: failed to connect to all addresses
```"
39718,"TF Lite nightly: Model with Fully Connected layer can't be converted, fully quantization, int8","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): tf-nightly
- TensorFlow version (or github SHA if from source): tf-nightly


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing

```
import numpy as np
import tensorflow as tf

mnist = tf.keras.datasets.mnist
train_data, test_data = mnist.load_data()

pre_process = lambda x: x / 255.0
num_calib = 1000
calib_data = pre_process(
            train_data[0][0 : num_calib].astype(np.float32)
        )

model = tf.keras.Sequential(
            [
                tf.keras.layers.InputLayer(input_shape=(28, 28)),
                tf.keras.layers.Reshape(target_shape=(28, 28, 1)),
                tf.keras.layers.Conv2D(
                    filters=12, kernel_size=(3, 3), activation=tf.nn.relu
                ),
                tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
                tf.keras.layers.Flatten(),
                tf.keras.layers.Dense(10, activation=tf.nn.softmax),
            ]
        )
model.summary()

train_images = pre_process(train_data[0])
train_labels = train_data[1]
test_images = pre_process(test_data[0])
test_labels = test_data[1]
# Train the digit classification model
model.compile(
  optimizer=""adam"",
  loss=""sparse_categorical_crossentropy"",
  metrics=[""accuracy""],
)
model.fit(
  train_images,
  train_labels,
  epochs=1,
  validation_data=(test_images, test_labels),
)

def _get_calib_data_func():
  def representative_data_gen():
    for input_value in calib_data:
      input_value = np.expand_dims(input_value, axis=0).astype(np.float32)
      yield [input_value]

  return representative_data_gen

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.representative_dataset = _get_calib_data_func()

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
tflite_model_INT8 = converter.convert()
```



```
**RuntimeError: Max and min for dynamic tensors should be recorded during calibration: Failed for tensor sequential_2/reshape_2/Shape
Empty min/max for tensor sequential_2/reshape_2/Shape**
```

**Also, please include a link to the saved model or GraphDef**

```
https://colab.research.google.com/drive/1l3VnLtWBCP_IR8CV7bTps1UXPoDfT2ok?usp=sharing
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39716,TF 2.2.0 ptxas issue,"Running in linux some code I get the following warning:

`You are using ptxas 8.x, but TF requires ptxas 9.x (and strongly prefers >= 9.2.88).  Compilation of XLA kernels below will likely fail.`

However when I run with the flag `TF_CPP_VMODULE=asm_compiler=2` I get this output:

`2020-05-20 14:57:36.735438: I tensorflow/stream_executor/gpu/asm_compiler.cc:169] Looking for ptxas at /usr/local/cuda-10.1/bin/bin/ptxas`
`2020-05-20 14:57:36.735500: I tensorflow/stream_executor/gpu/asm_compiler.cc:169] Looking for ptxas at /usr/local/cuda/bin/ptxas`
`2020-05-20 14:57:36.735521: I tensorflow/stream_executor/gpu/asm_compiler.cc:178] Using ptxas at /usr/local/cuda/bin/ptxas`

So it seems that when it's looking in cuda-10.1 it's going in the wrong dir (there is no cuda-10.1/bin/bin, the ptxas binary is in cuda-10.1/bin). Does anyone know how to fix this?

The situation is somewhat similar to #33375 "
39715,tfa ReduceLROnPlateau callback from Tf keras is not recognizing cohen kappa metrics direction in 'Auto' mode,"Hi all,
the 'auto' mode in ReduceLROnPlateau  and ModelCheckpoint  are looking for specific string 'acc' in the name of the metrics to be monitor. this actually leads to unlickly scenarious of not working properly even while using metrics that are defined in tfa and hoping tf will be aware of the direction . this can be added in the doc to make the developers understand how to name their metrics or to set min max mode on their own. 
thanks

https://github.com/tensorflow/addons/issues/1865
"
39714,Two weird things when I use custom model `train_step` and loss,"**System information**
- colab reproduce url: https://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing
- windows 10
- TensorFlow version 2.2
- Python version:3.7
- CUDA/cuDNN version:10.1

```python
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.python.eager import backprop
from tensorflow.python.keras.engine import data_adapter

class CustomModel(tf.keras.Model):
    def __init__(self):
        super(CustomModel, self).__init__()
        self.flat = tf.keras.layers.Flatten(input_shape = (28, 28))
        
    def call(self, inputs, training=False, **kwargs):
        x = self.flat(inputs)
        out = (x, x, x)
        return out

    def train_step(self, data):
        data = data_adapter.expand_1d(data)
        x, y, sample_weight = data_adapter.unpack_x_y_sample_weight(data)

        with backprop.GradientTape() as tape:
            y_pred = self(x, training = True)
            loss0 = tf.reduce_sum(self.losses)
            loss1, loss2, loss3 = self.loss(y, y_pred)
            total_loss = tf.reduce_sum([loss0, loss1, loss2, loss3])
        grads = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        self.compiled_metrics.update_state(y, y_pred)
        return {m.name: m.result() for m in self.metrics}

class MultiTaskLoss(tf.keras.losses.Loss):
    def __init__(self):
        super(MultiTaskLoss, self).__init__(reduction = tf.keras.losses.Reduction.NONE)

    def call(self, y_true, y_pred):
        tf.print(y_pred[0].shape, y_pred[1].shape, y_pred[2].shape)
        loss1 = tf.reduce_sum(y_pred[0])
        loss2 = tf.reduce_sum(y_pred[1])
        loss3 = tf.reduce_sum(y_pred[2])
        return tf.cast(loss1, tf.float32), tf.cast(loss2, tf.float32), tf.cast(loss3, tf.float32)


tf.config.experimental_run_functions_eagerly(True)

tfds.list_builders()
dataset = tfds.load('mnist', split='train')
dataset = dataset.map(lambda exa: (exa['image'], exa['label']))
dataset = dataset.batch(8)
model = CustomModel()
loss = MultiTaskLoss()
model.compile(loss = loss, optimizer = 'Adam')
model.fit(dataset, epochs=1)
```

**current behavior**
1 can't recieve tuple y_pred in custom loss when using self.compiled_loss(y, y_pred)
2 `iterating over `tf.Tensor` is not allowed` exception raised when using autograph

**Describe the expected behavior**
hope everything is ok.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/15hJPKHgn8aPi1mVi3XFOFvOgj85QcL-Z?usp=sharing

"
39713,Does tensorflow 1.15.0 support int8 tflite convertion? Wrong accurary.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.15
- TensorFlow version (or github SHA if from source): 1.15.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```

converter=tf.lite.TFLiteConverter.from_frozen_graph(pb_path,input_arrays=input_tensor_name
                                                   ,output_arrays=class_tensor_name
                                                   ,input_shapes=input_tensor_shape)

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8
converter.representative_dataset=representative_data_gen

tflite_model=converter.convert()
 
 
with open('owntempuint.tflite','wb') as f:
    f.write(tflite_model)

```


**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:

The int8 model produced successfully, however, the accuracy is very low, while from the same .pb model whose accuracy is about 0.51, float tflite model achieve 0.47 accuracy, the int8 tflite model only has 0.04 with the same input.

"
39712,Non-OK-status: tensorflow::Env::Default()->DeleteFile(ptx_path) status: Not found: ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Centos7.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
pip install officail
- TensorFlow version (use command below):
1.15.2 with gpu
- Python version:
3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.0/ 7.6
- GPU model and memory:
v100 32G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Can't find ptxas binary in ${CUDA_DIR}/bin.  Will back to the GPU driver for PTX -> sass compilation.  This is OK so long as you don't see a warning below about an out-of-date driver version.
2020-04-07 16:28:20.691087: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Searched for CUDA in the following directories:
2020-04-07 16:28:20.691210: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   ./cuda_sdk_lib
2020-04-07 16:28:20.691315: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   /usr/local/cuda
2020-04-07 16:28:20.691454: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   .
2020-04-07 16:28:20.691571: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:75] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.
2020-04-07 16:28:20.725596: F tensorflow/stream_executor/cuda/ptxas_utils.cc:181] Non-OK-status: tensorflow::Env::Default()->DeleteFile(ptx_path) status: Not found: /tmp/tempfile-72d9c7c8-2841-4447-bd7d-3947098f8e24-6a7fc700-2624-5a2af2a888f80; No such file or directory
Fatal Python error: Aborted

**Describe the expected behavior**
1. the code in follows log the mislead warning? 
https://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc#L405
```
       bool log_warning = true;
          if (maybe_cubin.status().code() ==
              tensorflow::error::Code::NOT_FOUND) {
            // Missing ptxas is expected in some environments where CUDA SDK
            // binaries are not available. We don't want to spam logs with
            // identical warnings in this case.

            // TODO(jlebar): we should implement a LOG_FIRST_N and LOG_EVERY_N
            // for more general usage.
            static std::atomic<bool> warning_done(false);
            log_warning = !warning_done.exchange(true);
          }
          if (log_warning) {
            PrintCantFindCudaMessage(
```
2. if some exception that the ptx_path did not create, there will rasie a error that : `2020-04-07 16:28:20.725596: F tensorflow/stream_executor/cuda/ptxas_utils.cc:181] Non-OK-status: tensorflow::Env::Default()->DeleteFile(ptx_path) status: Not found:`

https://github.com/tensorflow/tensorflow/blob/4386a6640c9fb65503750c37714971031f3dc1fd/tensorflow/stream_executor/cuda/ptxas_utils.cc#L184
```

  // Write ptx into a temporary file.
  string ptx_path;
  if (!env->LocalTempFilename(&ptx_path)) {
    return port::InternalError(""couldn't get temp PTX file name"");
  }
  auto ptx_cleaner = tensorflow::gtl::MakeCleanup([&ptx_path] {
    TF_CHECK_OK(tensorflow::Env::Default()->DeleteFile(ptx_path));
  });

  TF_RETURN_IF_ERROR(
      tensorflow::WriteStringToFile(env, ptx_path, ptx_contents));
  VLOG(2) << ""ptx written to: "" << ptx_path;

```

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39710,SciSharp.TensorFlow.Redist-Windows-GPU missing for tf.net 0.20.0-alpha,"**Problem**

Is there a SciSharp.TensorFlow.Redist-Windows-GPU (tensorflow.dll) distribution available for Tensorflow.NET 0.20.0-alpha?

The most resent NuGet package available seems to be v. 1.15.1, which does not seem to be compatible with Tensorflow.NET 0.20.0-alpha.

When could a working redistributable be expected?
Is it possible to download it or build it manually? (please provide steps)

**Any other info / logs**
The following exception is thrown when trying to use SciSharp.TensorFlow.Redist-Windows-GPU version v. 1.15.1 with Tensorflow.NET 0.20.0-alpha:

System.TypeInitializationException: The type initializer for 'Tensorflow.Binding' threw an exception. ---> System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation. ---> System.EntryPointNotFoundException: Unable to find an entry point named 'VSpace_Handle' in DLL 'tensorflow'.
    at Tensorflow.c_api.VSpace_Handle(VSpace_callback_Ones ones, VSpace_callback_AggregateGrads aggregate_grads)
   at Tensorflow.tensorflow.InitGradientEnvironment()
--- End of inner exception stack trace ---
    at System.RuntimeTypeHandle.CreateInstance(RuntimeType type, Boolean publicOnly, Boolean noCheck, Boolean& canBeCached, RuntimeMethodHandleInternal& ctor, Boolean& bNeedSecurityCheck)
   at System.RuntimeType.CreateInstanceSlow(Boolean publicOnly, Boolean skipCheckThis, Boolean fillCache, StackCrawlMark& stackMark)
   at System.Activator.CreateInstance[T]()
   at Tensorflow.Binding.New[T]()
   at Tensorflow.Binding..cctor()
--- End of inner exception stack trace ---
    at Tensorflow.Binding.get_tf()
"
39709,"builtins.TypeError: Expected binary or unicode string, got 0","I am quite new to Tensorflow, so in summary I converted the pcap file into csv file and load it in the python, I have made a column named detectionnumber such that when the Info column contains the word ""SYN"" and ""ACK"",  or ""RST"", it will append the value into the column I have made. Then I managed to add the column named detection I have made into the csv file. But the problem lies when I try to get the csv file into the dataset, it keeps showing the error, what should I do to solve these issues?`
Here is the file: [data.zip](https://github.com/tensorflow/tensorflow/files/4656000/data.zip)

`import datetime
import tensorflow as tf
import numpy as np
from tensorflow import keras
from tensorflow import feature_column
from tensorflow.keras import layers
import pandas as pd
import tensorflow_datasets as tfds
import os
import csv

file_path = '/home/root2001/Documents/PythonForMachineLearning/L05-Network-Packets'
file_training_full = pd.read_csv(file_path, sep='delimiter', header=None)

f = open(file_path)
csvf = csv.reader(f)

#for row in csvf:
    #print(row)

#Adds a column to each row, a row contain SYN ACK append 1, RST append 2, or else append 0
protocol1 = ""SYN""
protocol2 = ""ACK""
protocol3 = ""RST""


detectionnumber = []
for row1 in csvf:
   if (protocol1 in row1[6] and protocol2 in row1[6] ):
      value = 1
      detectionnumber.append(value)
   elif (protocol3 in row1[6]):
      value = 2
      detectionnumber.append(value)
   else:
      value = 0
      detectionnumber.append(value)
   
print(detectionnumber)
test = len(detectionnumber)
print(test)

file_training_full[""detection""] = detectionnumber
print(file_training_full)

LABEL_COLUMN = 'detection'

def get_dataset(file_path, **kwargs):

   dataset = tf.data.experimental.make_csv_dataset(
      file_path,
      batch_size=100,
      label_name=LABEL_COLUMN,
      num_epochs=1, **kwargs) 
   return dataset

get_dataset(file_training_full)`"
39708,`Model.predict(...)` seems incapable of handling two inputs when using the tf.data-API,"**System information**
- Have I written custom code: *Yes, see sample below.*
- OS Platform and Distribution: *Ubuntu Server 20.04*
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): *2.2.0*
- Python version: *3.8.2*
- CUDA version: *10.1*
- GPU model and memory: *NVIDIA RTX 2080 Ti*

---

**Describe the current behavior**

While developing a model which expects two inputs (`X1` and `X2`) and generates one output (`y`), I noticed that `model.predict(...)` does not work as expected together with the `tf.data`-API.

In a simpler example, a model with only one input (`X`) can be used to make predictions by calling `y_pred = model.predict(X)`. Everything behaves as one would expected. You could also call `predict` on the full training set such as a `tf.data.Dataset` which consists of both inputs and expected outputs (for ex.: `Xy = tf.data.Dataset.zip((X, y_true))` followed by `y_pred = model.predict(Xy)`).

But let's get back to the previous example with two inputs. The following sample too works as expected:

```
X = dataset = tf.data.Dataset.zip((X1, X2))
Xy = dataset = tf.data.Dataset.zip((X, y_true))
y_pred = model1.predict(Xy)
```

But the more realistic application of `predict` were one only passes the inputs (without the true labels), does result into a crash:

```
X12 = dataset = tf.data.Dataset.zip((X1, X2))
y_pred = model2.predict(X12)  # <- crash :,(
```

... with the following error message:

```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/home/hohl/.pycharm_helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/home/hohl/.pycharm_helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/hohl/projects/hohl.thesis/test.py"", line 25, in <module>
    y_pred = model.predict(X)  # <- crashes
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 88, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1268, in predict
    tmp_batch_outputs = predict_function(iterator)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 505, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2657, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
AssertionError: in user code:
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *
        outputs = self.distribute_strategy.run(
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **
        return self(x, training=False)
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:717 call
        return self._run_internal_graph(
    /home/hohl/venvs/tf22/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py:899 _run_internal_graph
        assert str(id(x)) in tensor_dict, 'Could not compute output ' + str(x)
    AssertionError: Could not compute output Tensor(""dense/Identity:0"", shape=(None, 1), dtype=float32)
```

The following usage, on the hand, executes without any troubles:

```
X12 = dataset = tf.data.Dataset.zip((X1, X2))
X12y = dataset = tf.data.Dataset.zip((X12, y_true))
y_pred = model2.predict(X12y)  # <- fine :)
```

This way of usage works fine, so I guess in the previous example the model though I fed it inputs with labels instead of inputs consisting of two separate inputs. However, the latter example is a rather unrealistic usage for any real-word application. Once deployed you would not know the true labels, that's exactly why you would want to predict them.

---

**Describe the expected behavior**

I would always expect `predict` to work with `y_pred = model.predict(X)` independently of the number of inputs. 

I guess the support for also accepting a combined dataset (inputs and labels) as input can come handy when testing single-input models, but I would consider it as even far more important that `model.predict(X)` works for models with any number of inputs.

One more thing: if the current behaviour is supposed to stay this way, it would be very helpful if the error message gets a bit more self-explanatory and the above fact gets mentioned explicitly in the [`predict`-documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#predict). It took me quite a while to figure out, why my model crashes with the ambiguous `AssertionError: Could not compute output` error message.

---

**Standalone code to reproduce the issue**

Here is the above sample as a single Python script which you can run to quickly reproduce the issue:

```
import tensorflow as tf
from tensorflow.keras import Input, Model, layers


def build_model():
    input1 = Input(shape=(1,), dtype=tf.float32)
    input2 = Input(shape=(1,), dtype=tf.float32)
    y = layers.Concatenate(axis=0)([input1, input2])
    y = layers.Dense(1)(y)
    return Model(inputs=[input1, input2], outputs=y)


def make_dummy_data():
    X1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))
    X2 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))
    X = tf.data.Dataset.zip((X1, X2))
    y_true = tf.data.Dataset.from_tensor_slices(tf.random.uniform([100, 1], dtype=tf.float32))
    return X, y_true


X, y_true = make_dummy_data()
Xy = tf.data.Dataset.zip((X, y_true))
model = build_model()
model.compile(loss='mse')
model.fit(Xy, batch_size=32)
y_pred = model.predict(X)  # <- crashes with: ""AssertionError: Could not compute output ...""
#y_pred = model.predict(Xy)  # <- works
```
"
39707,matmul & slice make incorrect result for some specific dims,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1060, 16G

**Describe the current behavior**
If we have matrix **a**, **m**, if **b** is a sub-matrix of **a**,
then **matmul(b, m)** should also be a sub-matrix of **matmul(a, m)**,
but for some specific dims, there are some minor difference.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/17PyKnnxWc6UGZ_f-uxvlPBd6fL_9k9je?usp=sharing
``` 
import tensorflow as tf

def test(d1, d2):
    a = tf.random.uniform([d1 + 1, d2], dtype=tf.float32)
    b = tf.slice(a, [0, 0], [d1, d2])
    m = tf.random.uniform([d2, d2], dtype=tf.float32)
    return (tf.slice(a @ m, [0, 0], [d1, d2]) == b @ m).numpy().all()

for i in range(1, 20):
    print('{}\t{}'.format(i, test(i, 100)))
```

"
39706,Multiple step predict seems to be wrong,"![20200520_163852](https://user-images.githubusercontent.com/31833270/82420430-aa4d2b00-9aba-11ea-9ca2-160fba8afd6c.png)


The original data of x_test has a non-linear random walk property,

but the newly predicted 20 values have a linear shape.

Clearly, did I make the wrong prediction?

you can see full source here
https://colab.research.google.com/drive/1kk24KjpZQEZpdlBxr4D4DO-IGHJ0439v?usp=sharing

and my tf version is 2.1.0 and python 3.7.7
![image](https://user-images.githubusercontent.com/31833270/82441869-9238d400-9ad9-11ea-9ca0-1d17d649e00f.png)
"
39705,Failed to load the native TensorFlow runtime Error,"I am new to trying to use Spleeter after trying to look for a way to split/separate the instruments into individual instrumental stems (keyboard, drum, horns, guitar, etc into its own solo tracks)  played in an original track I am working on having recreated live. I received this error when trying to use the default audio_example.mp3 file as a test. I have the spleeter_master folder downloaded in my download folder, and miniconda3 saved to my Users/UserName folder directory. I installed spleeter using the pip install code displayed here and ran anaconda prompt from the search field.

(base) C:\Users\UserName>spleeter separate -i audio_example.mp3 -p spleeter:2stems -o output
Traceback (most recent call last):
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\users\username\miniconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\users\username\miniconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\username\miniconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\username\miniconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\username\miniconda3\Scripts\spleeter.exe\__main__.py"", line 7, in <module>
  File ""c:\users\username\miniconda3\lib\site-packages\spleeter\__main__.py"", line 54, in entrypoint
    main(sys.argv)
  File ""c:\users\username\miniconda3\lib\site-packages\spleeter\__main__.py"", line 36, in main
    enable_logging()
  File ""c:\users\username\miniconda3\lib\site-packages\spleeter\utils\logging.py"", line 60, in enable_logging
    tf_logger = get_tensorflow_logger()
  File ""c:\users\username\miniconda3\lib\site-packages\spleeter\utils\logging.py"", line 27, in get_tensorflow_logger
    from tensorflow.compat.v1 import logging
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow\__init__.py"", line 99, in <module>
    from tensorflow_core import *
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""c:\users\username\miniconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\username\miniconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\users\username\miniconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\users\username\miniconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39704,TFLiteConverter.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- TensorFlow installed from (source or binary): Pip
- TensorFlow version (or github SHA if from source): 2.1.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, CONCATENATION, CONV_2D, FULLY_CONNECTED, GATHER, RESHAPE, REVERSE_V2, SOFTMAX, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42803,Tensorflowのチュートリアル「Overfit and underfit」の不備,「初心者のためのTensorFlow 2.0 入門」のチュートリアルにある「overfit and underfit」で日本語版では、Google Colabで実行するとメモリ不足でクラッシュする。
39703,404 Not found,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):using pip 
- TensorFlow version (use command below):2.2.0-rc1
- Python version:3.5.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:10.1/
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Cannot able to import the resnet 50 model from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/eager/benchmarks/resnet50/resnet50.py
**Describe the expected behavior**
I want to use the above resnet50 model and not the tf.keras.applications.ResNet50 for the intermediate outputs
**Standalone code to reproduce the issue**
import tensorflow as tf
from tf.python.eager.benchmarks #not working
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39702,"Customized loss function requires eager tensor, but symbolic tensor is passed","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu==2.2.0
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: Quadro RTX 8000 / 48GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` v2.2.0-rc4-8-g2b96f3662b 2.2.0


**Describe the current behavior**
I need to apply a binary mask to the model output for computing loss. My current implementation uses a model that takes two inputs (the data and the mask), and use function closure to implement the customized loss. 

However, this raises the error ""tensorflow.python.eager.core._SymbolicException: Inputs to eager execution function cannot be Keras symbolic tensors"".

Apparently, the mask input is treated as symbolic tensor.

**Describe the expected behavior**

This only happens in the eager mode. Apply `disable_eager_execution()` will eliminate the problem. However, I want to know if there is any way to make this work in the eager mode.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

This is the [gist](https://colab.research.google.com/drive/1D_GitVtknoYk33hURTXzC0murA60UBH0?usp=sharing)

```
import tensorflow as tf
import numpy as np

import tensorflow.keras.backend as K

from tensorflow.keras.layers import Input, Flatten, Dense
from tensorflow.keras import Model

x_data = np.zeros((32, 28, 28))
x_mask = np.zeros((32, 10))
y = np.zeros((32, 10))

input_data = Input(shape=(28, 28))
input_mask = Input(shape=(10,))

output= Flatten()(input_data)
output = Dense(64, activation='relu')(output)
output = Dense(10)(output)
model = Model(inputs=[input_data, input_mask], outputs=output)


def custom_loss():
    def loss(y_true, y_pred):
        # This line causes the error
        return K.mean(K.square(y_true - y_pred * model.inputs[1]), axis=-1)   

        # This line doesn't cause the error
        # return K.mean(K.square(y_true - y_pred), axis=-1)   
    return loss


model.compile(
    optimizer=tf.keras.optimizers.SGD(),
    loss=custom_loss(),
    metrics=['accuracy'])

for i in range (2):
    print(i)
    model.train_on_batch([x_data, x_mask], y)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39701,GatherV2Grad prints a deprecation warning,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from: binary (from pip)
- TensorFlow version: 2.2.0
- Python version: 3.7

**Describe the current behavior**

GatherV2Grad prints a deprecation warning when used. The warning is from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_grad.py#L644

> WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:644: _EagerTensorBase.cpu (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
> Use tf.identity instead.

This issue is similar to https://github.com/tensorflow/tensorflow/issues/32049

**Describe the expected behavior**

No deprecation warning.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

class Model(tf.keras.Model):
  def __init__(self,):
    super(Model, self).__init__()
    self.layer = tf.keras.layers.Dense(100)
  def call(self, x):
    return self.layer(x)

model = Model()
model.build((1,2))
weights = model.trainable_variables

with tf.GradientTape(persistent=True) as tape:
    output = model(tf.zeros([1,2]))

gradients = tape.jacobian(output, weights, experimental_use_pfor=False)
```

This code is based on https://github.com/tensorflow/tensorflow/issues/36227, which was about something else, but does also show the deprecation warning."
39699,why the pb model(saved by keras API) structure is so complicated？,"Hi, I use keras API to save my model in pb format, but the model structure is so complicated!
![image](https://user-images.githubusercontent.com/11664658/82403250-f84d3900-9a90-11ea-92ad-5d2965c16328.png)



On the contrary, if I save the model in h5 format, the model structure is very simple, which is exactly what I want.
![image](https://user-images.githubusercontent.com/11664658/82403218-e4a1d280-9a90-11ea-8590-95befec014d1.png)

what is the reason, and how can I get a simple model in pb format?"
39697,Weird block of RNN in TF2.2,"**System information**
This bug exists in TF v2.2.0-rc4-8-g2b96f3662b 2.2.0 and on windows, linux, both CPU and GPU version. However, it does not exist in TF 2.1.

**Describe the current behavior**
Just check the code
```python
rnn = tf.keras.layers.GRU(3)
rnn(tf.keras.Input([None, 2]), tf.keras.Input([3, ]))


@tf.function(input_signature=[tf.TensorSpec(shape=(None, None, 2)),
                              tf.TensorSpec(shape=(None, 3))])
def test(x, i):
    with tf.GradientTape(persistent=True) as tape:
        rnn(x, initial_state=i)


test(np.random.randn(3, 2, 2).astype(np.float32),
     np.random.randn(3, 3).astype(np.float32))
```

The code will block in `test`, and the memory will grow unlimitedly.

The code will also block if `GRU` is replaced by `LSTM`. But it will run well if it is `SimpleRNN`.

This bug is a little complicated bacause when only all `rnn(tf.keras.Input...`, `@tf.function(input_signature=...)` and `with tf.GradientTape(persistent=True)...` exist, the block shows up.
"
39696,Different SHA256 for mkl_dnn,"in [tensorflow/workspace.bzl](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/workspace.bzl):
`tf_http_archive(
        name = ""mkl_dnn"",
        build_file = clean_dep(""//third_party/mkl_dnn:mkldnn.BUILD""),
        sha256 = ""31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f"",
        strip_prefix = ""mkl-dnn-0.21.3"",
        urls = [
            ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz"",
            ""https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz"",
        ],
    )`
When the mkl_dnn is downloaded from the first url( ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz""), the SHA256  is 31e78581e59d7e60d4becaba3834fc6a5bf2dccdae3e16b7f70d89ceab38423f , which is right.
But when the mkl_dnn is downloaded from the second url(https://github.com/intel/mkl-dnn/archive/v0.21.3.tar.gz), the SHA256 is a0211aeb5e7dad50b97fa5dffc1a2fe2fe732572d4164e1ee8750a2ede43fbec, which will fail to compile.
When I unzip the compressed package downloaded from the second url, its directory is oneDNN 0.21.3. But the files in them are exactly the same. Maybe this is the reason. 
hope you can check it and fix it. Thanks.
"
39694,TextLineDataset could be more expressive,"TextLineDataset could be more expressive. For instance, it could have more arguments like:

```python
train_dataset = tf.data.TextLineDataset(
                                     file_path, # dataset file path
                                     format, # file format: jsonl, csv, etc.
                                     fields # a set of colums
)
```

It would also be incredible if there was a way to indicate how to tokenize each of the fields."
39693,"Trying to run l08c09_forecasting_with_cnn.ipynb on spyder: ""WARNING:tensorflow:AutoGraph could not transform <function seq2seq_window_dataset.<locals>.<lambda> at 0x0000024D6C641D38> and will run it as-is.""","**System information**
- I am using a stock example script provided in Google Colab
https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l08c09_forecasting_with_cnn.ipynb#scrollTo=PgYwn9VM8OJi

- OS Platform and Distribution: Windows 10

- TensorFlow installed from (source or binary): installed using pip
- TensorFlow version (use command below): tf-nightly
- Python version: 3.7.6
- GPU model and memory: GTX-1060

**Describe the current behavior**

Trying to reproduce script from google colab in my PC using Spyder 4.

**Describe the expected behavior**

WARNING: AutoGraph could not transform <function seq2seq_window_dataset.<locals>.<lambda> at 0x0000024D6A385EE8> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 
Traceback (most recent call last):

  File ""C:\Users\pedro\OneDrive\Área de Trabalho\keras_BTC_cnn.py"", line 131, in <module>
    callbacks=[early_stopping, model_checkpoint])

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 819, in fit
    use_multiprocessing=use_multiprocessing)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 235, in fit
    use_multiprocessing=use_multiprocessing)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 614, in _process_training_inputs
    distribution_strategy=distribution_strategy)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 706, in _process_inputs
    use_multiprocessing=use_multiprocessing)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py"", line 702, in __init__
    x = standardize_function(x)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 684, in standardize_function
    return dataset.map(map_fn, num_parallel_calls=dataset_ops.AUTOTUNE)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 1591, in map
    self, map_func, num_parallel_calls, preserve_cardinality=True)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 3926, in __init__
    use_legacy_function=use_legacy_function)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 3147, in __init__
    self._function = wrapper_fn._get_concrete_function_internal()

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2395, in _get_concrete_function_internal
    *args, **kwargs)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\framework\func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 3140, in wrapper_fn
    ret = _wrapper_helper(*args)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 3082, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)

  File ""C:\Anaconda3\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 237, in wrapper
    raise e.ag_error_metadata.to_exception(e)

ValueError: in converted code:

    C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py:677 map_fn
        batch_size=None)
    C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training.py:2410 _standardize_tensors
        exception_prefix='input')
    C:\Anaconda3\lib\site-packages\tensorflow_core\python\keras\engine\training_utils.py:573 standardize_input_data
        'with shape ' + str(data_shape))

    ValueError: Error when checking input: expected input_1 to have 3 dimensions, but got array with shape (None, None)

**Standalone code to reproduce the issue**
Link above"
39692,tf.lite.experimental.load_delegate throws exception with try except block,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04, macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0 & v2.2.0-0-g2b96f3662b 2.2.0
- Python version:3.7.6
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Currently `tf.lite.experimental.load_delegate` throws `OSError` when the passed in library doesn't exist. However when use it with `try except` block, it will throw 

> Exception ignored in: <bound method Delegate.__del__ of <tensorflow.lite.python.interpreter.Delegate object at 0x7f6fc561db00>>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py"", line 125, in __del__
    if self._library is not None:
        AttributeError: 'Delegate' object has no attribute '_library'

**Describe the expected behavior**
Exception should be caught with `try except` block.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Without `try except` block, the behavior looks fine
```python
import tensorflow as tf

tf.lite.experimental.load_delegate('some invalid lib')  # expect an exception
```
With `try except` block, it throws an exception which can't be caught.
```python
import tensorflow as tf
try:
    tf.lite.experimental.load_delegate('some invalid lib')
except Exception as e:
    # do something to handle exception
    pass
```
>Exception ignored in: <bound method Delegate.__del__ of <tensorflow.lite.python.interpreter.Delegate object at 0x7f6fc561d518>>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py"", line 125, in __del__
    if self._library is not None:
AttributeError: 'Delegate' object has no attribute '_library'

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39689,Unclear Documentation for Keras SavedModel format,"In the [documentation](https://www.tensorflow.org/guide/keras/save_and_serialize#savedmodel_format) for keras for saving a model, it states that you can save a model in a SavedModel format by passing in a name for a directory. This creates a directory with a pb file and 2 other folders containing assets and weights. 

This doesn't seem to me to be a true SavedModel format as it's not a single pb file. Is there documentation explaining how to create the single pb that can be used for loading SavedModels (like Tensorflow Java or DL4J)? Loading doesn't seem to be possible from a directory, only a SavedModel pb file.

Maybe I'm missing something? Thank you!
"
39688,Wrong with converted tflite model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows10
- TensorFlow installed from (source or binary):pip install
- TensorFlow version (or github SHA if from source):1.13.1
 I downloaded [ssd_mobilenet_v2_quantized_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz) and tried to convert it to tflite format using following script:

```
import os
import tensorflow as tf
print(tf.__version__)
graph_def_file = ""E:/Tensorflow_detection_model_zoo/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/tflite_graph.pb""
input_arrays=[""normalized_input_image_tensor""]
output_arrays=['TFLite_Detection_PostProcess', 'TFLite_Detection_PostProcess:1', 'TFLite_Detection_PostProcess:2', 'TFLite_Detection_PostProcess:3']
input_shape={""normalized_input_image_tensor"": [1, 300, 300, 3]}

converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays, input_shape)
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
converter.allow_custom_ops = True
converter.post_training_quantize = True
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0]: (0., 1.)}  # mean, std_dev
tflite_uint8_model = converter.convert()
open(""uint8_model_converted_from_""+os.path.basename(os.path.dirname(graph_def_file))+"".tflite"", ""wb"").write(tflite_uint8_model)
```
So i got the output file : **uint8_model_converted_from_ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tflite**.
Then i used the following script to test this tflite model:
```
import numpy as np
import os
import tensorflow as tf
import cv2 as cv
from PIL import Image
print(tf.__version__)
print(os.getcwd())

interpreter = tf.lite.Interpreter(model_path=""uint8_model_converted_from_ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tflite"")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(""input_details : "", input_details)
print(""output_details : "", output_details)

image = cv.imread(""test_images/traffic.png"")
resize_img = cv.resize(image, (300, 300), interpolation=cv.INTER_CUBIC)
reshape_image = resize_img.reshape(300, 300, 3)
image_np_expanded = np.expand_dims(reshape_image, axis=0)
image_np_expanded = image_np_expanded.astype('uint8')
interpreter.set_tensor(input_details[0]['index'], image_np_expanded) 
interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
output_data_1 = interpreter.get_tensor(output_details[1]['index'])
output_data_2 = interpreter.get_tensor(output_details[2]['index'])
output_data_3 = interpreter.get_tensor(output_details[3]['index'])

original_image_height, original_image_width, _ = image.shape
detect_image = cv.rectangle(image,
                            (int(output_data[0][0][1]*original_image_width),  
                             int(output_data[0][0][0]*original_image_height)),
                            (int(output_data[0][0][3]*original_image_width),
                             int(output_data[0][0][2]*original_image_height)),
                            (0, 255, 0), 3)
Image.fromarray(detect_image).save('test_result.png')
print(original_image_width)
print(int(output_data[0][0][3]*original_image_width))
print(original_image_height)
print(int(output_data[0][0][2]*original_image_height))
cv.imshow('detect_result', detect_image)
cv.waitKey(0)
```
But after i run this script, pycharm report this error:
![YIOczF.png](https://s1.ax1x.com/2020/05/19/YIOczF.png)
Obviously the shape is array(**[1, 0, 4]**) which is unnormal, the correct one should be array([**1, 10, 4]**) where  10 means output **10 classes**. So where is error come from ? and how to fix it ?"
39687,Getting error while making a custom layer,"```
class FiltersChangeResidualBlock(Layer):

    def __init__(self, out_filters, **kwargs):
        """"""
        The class initialiser should call the base class initialiser, passing any keyword
        arguments along. It should also set the number of filters as a class attribute.
        """"""
        super(FiltersChangeResidualBlock, self).__init__(**kwargs)
        self.out_filters = out_filters
        
        
    def build(self, input_shape):
        """"""
        This method should build the layers according to the above specification. Make sure 
        to use the input_shape argument to get the correct number of filters, and to set the
        input_shape of the first layer in the block.
        """"""
        self.bn1_1 = BatchNormalization(input_shape=input_shape)
        self.conv1_1 = Conv2D(input_shape[1],(3,3), padding='SAME')
        self.bn2_2 = BatchNormalization()
        self.conv2_2 = Conv2D(self.out_filters,(3,3), padding='SAME')
        self.conv3_3 = Conv2D(self.out_filters, (1,1), padding='SAME')


        
        
    def call(self, inputs, training=False):
        """"""
        This method should contain the code for calling the layer according to the above
        specification, using the layer objects set up in the build method.
        """"""
        h = self.bn1_1(inputs, training=training)
        h = tf.nn.relu(h)
        h = self.conv1_1(h)
        h = self.bn2_2(training=training)(h)
        h = tf.nn.relu(h)
        h = self.conv2_2(h)
        x = self.conv3_3(inputs)
        k = Add()([x,h])
        return k


test_model = tf.keras.Sequential([FiltersChangeResidualBlock(16, input_shape=(32, 32, 3), name=""fc_resnet_block"")])
test_model.summary()
 ```
While executing the above code I'm getting the below error.
```
ValueError: in user code:

    <ipython-input-28-8301127c223d>:40 call  *
        h = self.bn2_2(training=training)(h)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:800 __call__  **
        'The first argument to `Layer.call` must always be passed.')

    ValueError: The first argument to `Layer.call` must always be passed.
```"
39686,Unresolved external symbol C ++ version,"## Operating environment
windows 10
vs 2019
tensorflow 2.1 C++

## error
Compile the C ++ version of tensorflow 2.1, and report unresolved external symbols when using it
![image](https://user-images.githubusercontent.com/57739867/82340969-6c042d00-9a22-11ea-9e77-a8778c52b04e.png)



## my code
#include <sstream>
#include <iostream>

#include ""tensorflow/cc/ops/standard_ops.h""
#include <tensorflow/core/framework/graph.pb.h>
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/threadpool.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/protobuf/meta_graph.pb.h""
#include ""tensorflow/core/framework/graph.pb.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/cc/saved_model/loader.h""

#include <fstream>
#include <iostream>
#include <string>
#include <vector>
#include <stdlib.h>

#include ""tensorflow/cc/ops/const_op.h""
#include ""tensorflow/cc/ops/standard_ops.h""
#include ""tensorflow/core/framework/tensor.h""
#include ""tensorflow/core/graph/default_device.h""
#include ""tensorflow/core/graph/graph_def_builder.h""
#include ""tensorflow/core/lib/core/errors.h""
#include ""tensorflow/core/lib/core/stringpiece.h""
//#include ""tensorflow/core/lib/core/threadool.h""
#include ""tensorflow/core/lib/io/path.h""
#include ""tensorflow/core/lib/strings/stringprintf.h""
#include ""tensorflow/core/platform/env.h""
#include ""tensorflow/core/platform/init_main.h""
#include ""tensorflow/core/platform/logging.h""
#include ""tensorflow/core/platform/types.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/core/util/command_line_flags.h""

using namespace tensorflow;
using tensorflow::Flag;
using tensorflow::Status;
using tensorflow::string;
using tensorflow::Tensor;

using std::string;

//using namespace tensorflow;

int main(int argc, char argv[]) {
	std::string model_path = R""(E:/pro_vs2019/test_vs2019/tf_model/)"";

	Session* session;
	SessionOptions options;
	options.config.mutable_gpu_options()->set_visible_device_list(""0""); //设置使用的gpu
	options.config.mutable_gpu_options()->set_allow_growth(true); //设置GPU内存自动增长
	TF_CHECK_OK(NewSession(options, &session));//创建新会话Session
	GraphDef graphdef; //Graph Definition for current model
	TF_CHECK_OK(ReadBinaryProto(Env::Default(), model_path, &graphdef)); //从pb文件中读取图模型;
	TF_CHECK_OK(session->Create(graphdef)); //将模型导入会话Session中;

	//tensorflow::Status status;
	//// SessionOptions sessionOptions;
	//// RunOptions runOptions;
	//// SavedModelBundle bundle;

	//// load everything
	//// status = LoadSavedModel(sessionOptions, runOptions, model_path, { ""serve"" }, &bundle);

	//GraphDef graphdef;

	//status = ReadBinaryProto(Env::Default(), model_path, &graphdef);

	//if (!status.ok()) {
	//	std::cout << status.ToString() << std::endl;
	//	return -1;
	//}
	//else std::cout << ""Succesfully have model loaded: "" << model_path << std::endl;

	return 0;
}"
39685,Unable to import Tensor Flow - Windows10,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version:Tensor flow 2.1
- Python version:Python 3.7.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version:Cuda 10.2.89
- GPU model and memory:NVidia GE Force GTX1050

**Describe the problem**
I have tried installing tensor flow from both conda as well pip and also tried creating venv.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Typed std command: 
python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))


**Any other info / logs**
(venv) C:\Users\Shree>python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))
Traceback (most recent call last):
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Shree\anaconda3\envs\venv\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

Please help me in resolving issue. 
"
39684,How to create an NLP pipeline in Tensorflow 2 in a simple and efficient approach?,"Using a `TorchText` to create a NLP pipeline is simple as one, two, three:

```python
# 1. indicate the fields and how to tokenizer them:
TEXT = data.Field(
    tokenize=tokenizer.encode,
    batch_first=True,
    fix_length=32
)
LABELS = data.Field()

# 2. bind it to the dataset path
train, val, test = data.TabularDataset.splits(
    path=dataset_path,
    train='train.json',
    validation='val.json',
    test='test.json', 
    format='json',
    fields=[('text', TEXT), ('labels', LABELS)])

# 3. get the train, val and test batches
train_iter, valid_iter, test_iter = data.BucketIterator.splits(
    (train, valid, test), batch_sizes=(16, 256, 256)
)
 
```

This is what you expect from a Deep Learning framework in which you don't need a lot of coding simply to prepare the data for each new project.

Is there anything similar in Tensorflow 2?
"
39682,Please Providing tensorflowLite dynamic library  for iOS platform.,Please Providing tensorflowLite dynamic library  for iOS platform.
39681,Colab Runtime Crashes training on musdb18,"

**System information**
- Have I written custom code :Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Using Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):2.0
- Python version:3.6

I have built an audio separation model and am trying to train it on the musdb18 dataset. It contains a total of 150 audio tracks, 100 for training and 50 for test. I load the dataset like: I load a song, preprocess it, then feed it to the network. Train my model for one epoch, then load another song and train it. I don't think it should exceed RAM limit, but somehow the runtime crashes after a few epochs, with a message that the runtime crashed using all RAM. Could someone please guide me why this is so.
When I trained my previous tensordlow model on a single song it worked smoothly."
39680,TF 2.2: Build failure on Win10 (Bad address issue),"Dear experts,

I am trying to compile Tensorflow 2.2.0 from source on a Win 10 system, including GPU support. My actual goal is compile the dll (but I tried to build the pip package as well and it does not seem to make a difference for this issue). I have been following the instructions on the official website (https://www.tensorflow.org/install/source_windows) as closely as possible, and have started from a completely fresh Windows install. However, I run into the issue that the build, at some point, always aborts with a strange ""Bad address"" failure of some tool (see below). I have no idea what else to try or how to get a more meaningful hint to the problem. Please help me out with advice here. Thanks in advance!


<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1809
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.5.6 (also tried 3.7.7, same issue)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): MS Visual Studio Buildtools 2019 (v 14.25.28610)
- CUDA/cuDNN version: CUDA 10.1 Update 2 / cuDNN 7.6.5.32
- GPU model and memory: NVidia Tesla K80

- MSYS2: packges as required in installation instructions of TF, all updated to the latest version


**Describe the problem**
The build aborts with a message like the following:

INFO: Analyzed target //tensorflow:tensorflow.dll (174 packages loaded, 15684 targets configured).
INFO: Found 1 target...
**ERROR: C:/users/admin.ml/_bazel_admin.ml/mamyapdv/external/llvm-project/llvm/BUILD:45:1: Executing genrule @llvm-project//llvm:config_gen failed (Exit 126)
/usr/bin/bash: bazel-out/x64_windows-opt/bin/third_party/llvm/expand_cmake_vars.exe: Bad address**
Target //tensorflow:tensorflow.dll failed to build

During multiple attempts, I saw different executables failing with the ""Bad address"" issue, so it seems to be related to some non-deterministic behaviour in the tool chain.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
>python configure.py
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is C:\ProgramData\Miniconda3\python.exe]:

Found possible Python library paths:
  C:\ProgramData\Miniconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\ProgramData\Miniconda3\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10]: 10.1

Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7

Please specify the comma-separated list of base paths to look for CUDA libraries and headers. [Leave empty to use the default]: C:/cuDNN/cuda,C:/cuDNN/cuda/bin,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/bin

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/cuDNN/cuda/lib/x64
    C:/cuDNN/cuda/include

Please specify a list of comma-separated CUDA compute capabilities you want to build with.
[Default is: 3.5,7.0]:

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.


>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings tensorflow:tensorflow.dll
```

"
39679,tf.keras.callbacks.ModelCheckpoint only saves weights although save_weights_only=False,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, although very minimal
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04.5 LTS 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Plain pip install
- TensorFlow version (use command below):
2.2.0
- Python version:
3.7.3
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A, reproducible with CPU
- GPU model and memory:
N/A


**Describe the current behavior**
tf.keras.callbacks.ModelCheckpoint only saves weights for a subclassed model, although save_weights_only=False.

**Describe the expected behavior**
tf.keras.callbacks.ModelCheckpoint saves the full model as a tf.SavedModel.

This behaviour is expected based on the API docs:
>  save_weights_only | if True, then only the model's weights will be saved (model.save_weights(filepath)), else **the full model is saved (model.save(filepath))**.`
https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint

And model.save:
> save_format | Either 'tf' or 'h5', indicating whether to save the model to **Tensorflow SavedModel** or HDF5. **Defaults to 'tf'** in TF 2.X, and 'h5' in TF 1.X.`
https://www.tensorflow.org/api_docs/python/tf/keras/Model#save


**Standalone code to reproduce the issue**
````
from pathlib import Path
from typing import Optional

import os
import tempfile
import tensorflow as tf

_TENSOR = tf.ones((10, 1, 1, 3))


class SomeModel(tf.keras.models.Model):
    def call(
        self,
        inputs: tf.Tensor,
        training: Optional[bool] = None,
        mask: Optional[tf.Tensor] = None,
    ) -> tf.Tensor:
        return inputs


def _get_model() -> SomeModel:
    model = SomeModel()
    model.compile()
    model.fit(x=_TENSOR, y=_TENSOR)
    return model


def _unexpected_failure() -> None:
    """"""
    The minimal working example to reproduce the bug.
    tf.keras.callbacks.ModelCheckpoint has a bug: it only saves weights, while the expected
    behaviour is that it saves using tf.SavedModel.
    """"""
    model = _get_model()
    with tempfile.TemporaryDirectory() as save_dir:
        model_save_dir = os.path.join(save_dir, ""model"")
        model.fit(
            x=_TENSOR,
            y=_TENSOR,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    model_save_dir, save_weights_only=False
                )
            ],
        )

        # We can see that only the weights are saved. This prints e.g.:
        # /tmp/tmpvuciirc6/model.data-00000-of-00001 contains: []
        # /tmp/tmpvuciirc6/model.index contains: []
        # /tmp/tmpvuciirc6/checkpoint contains: []
        print(_list_dir_content(Path(save_dir)))

        # Restoring throws an error:
        # Traceback (most recent call last):
        #  ............
        #   File ""[...]/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 189, in load_model
        #     loader_impl.parse_saved_model(filepath)
        #   File ""[...]/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 113, in parse_saved_model
        #     constants.SAVED_MODEL_FILENAME_PB))
        # OSError: SavedModel file does not exist at: /tmp/tmpvuciirc6/{saved_model.pbtxt|saved_model.pb}
        saved_model = tf.keras.models.load_model(save_dir, compile=False)


def _ok() -> None:
    """"""
    This shows that the same model can be saved as a tf.SavedModel just fine.
    model.save() works fine: it saves as a tf.SavedModel.
    """"""
    model = _get_model()
    with tempfile.TemporaryDirectory() as save_dir:
        model_save_dir = os.path.join(save_dir, ""model"")
        model.save(model_save_dir)

        # We can load the model again and run inference:
        saved_model = tf.keras.models.load_model(model_save_dir, compile=False)
        # We can run inference from the loaded model.
        saved_model(_TENSOR)

        # We can see that the model is saved as a tf.SavedModel. This print e.g.:
        # /tmp/tmp3f6w2q5i/model_regular_save contains: [PosixPath('/tmp/tmp3f6w2q5i/model/variables'), PosixPath('/tmp/tmp3f6w2q5i/model/assets'), PosixPath('/tmp/tmp3f6w2q5i/model/saved_model.pb')]
        print(_list_dir_content(Path(save_dir)))


class ModelCheckpointWorkAround(tf.keras.callbacks.ModelCheckpoint):
    def set_model(self, model: tf.keras.models.Model) -> None:
        # Work around, so that the if at
        # https://github.com/tensorflow/tensorflow/blob/1186e3f2098793952aa82bf356dfe51b967fb26c/tensorflow/python/keras/callbacks.py#L1189
        # is skipped, so that self.save_weights_only remains False.
        self.model = model


def _workaround() -> None:
    """"""
    A workaround that shows how the bug may be fixed.
    Saving and restoring using ModelCheckpointWorkAround works fine, the if at
    https://github.com/tensorflow/tensorflow/blob/1186e3f2098793952aa82bf356dfe51b967fb26c/tensorflow/python/keras/callbacks.py#L1189
    is circumvented.
    """"""
    model = _get_model()
    with tempfile.TemporaryDirectory() as save_dir:
        model_save_dir = os.path.join(save_dir, ""model"")
        model.fit(
            x=_TENSOR,
            y=_TENSOR,
            callbacks=[
                ModelCheckpointWorkAround(model_save_dir, save_weights_only=False)
            ],
        )

        # We can see the model is saved as a tf.SavedModel. This prints e.g.:
        # /tmp/tmpxmdhsfi_/model contains: [PosixPath('/tmp/tmpxmdhsfi_/model/variables'), PosixPath('/tmp/tmpxmdhsfi_/model/assets'), PosixPath('/tmp/tmpxmdhsfi_/model/saved_model.pb')]
        print(_list_dir_content(Path(save_dir)))

        # Running inference from a restored model works fine:
        saved_model = tf.keras.models.load_model(model_save_dir, compile=False)
        saved_model(_TENSOR)


def _list_dir_content(path: Path) -> str:
    return ""\n"".join(
        [f""{elem} contains: {list(elem.glob('*'))}"" for elem in path.glob(""*"")]
    )


if __name__ == ""__main__"":
    _ok()
    _workaround()
    _unexpected_failure()
````

**Other info / logs**
I **suspect** the if block at https://github.com/tensorflow/tensorflow/blob/1186e3f2098793952aa82bf356dfe51b967fb26c/tensorflow/python/keras/callbacks.py#L1189 can simply be removed, and it is a remnant from the past when tf.SavedModel did not exist yet, and H5 was the only option. Perhaps the author, @fchollet can confirm?

My suspicion is based on the fact that the provided work around works and also based on the error message here: https://github.com/tensorflow/tensorflow/blob/35b03590f6bcd5869355ce8a0c3a995fd23ecdd7/tensorflow/python/keras/saving/save.py#L123"
39677,[TF 2.2 API docs] tf.keras.applications.resnet_v2.preprocess_input docs Returns paragraph is misleading,"[https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet_v2/preprocess_input](url)
The documentation says in the 'Returns' paragraph that
> The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset, without scaling. 

However, according to [the function definition](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/applications/resnet_v2.py#L125-L139) it's not true anymore  since the **mode parameter** can't be set and it is always equal to **'tf'**.

Therefore, this docs part must be corrected to 
>will scale pixels between -1 and 1,  sample-wise


"
39676,form keras part recurrent_dropout causes a exploding loses ,"recurrent_dropout promotes exploding losses
https://github.com/fchollet/deep-learning-with-python-notebooks/issues/127
https://github.com/fchollet/deep-learning-with-python-notebooks/issues/52

same problem with  Tensorflow :  2.0.0 and  Keras :  **2.2.4-tf**

https://github.com/tensorflow/tensorflow/issues/35724#issuecomment-582712923

when recurrent_dropout is zero there is no problem
"
39675,Infinite Training,"Code is : 
```
import tensorflow as tf

datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True, validation_split=0.2)

train_generator = datagen.flow_from_directory(
    directory=""/content/dataset/data"",
    target_size=(224, 224),
    color_mode=""rgb"",
    batch_size=2,
    class_mode=""binary"",
    seed=42,
    subset='training'
)

valid_generator = datagen.flow_from_directory(
    directory=""/content/dataset/data"",
    target_size=(224, 224),
    color_mode=""rgb"",
    batch_size=2,
    class_mode=""binary"",
    seed=42, subset='validation'
)

model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(2, 5, activation='relu', input_shape=(224, 224, 3)),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.fit_generator(train_generator, epochs=1)
```

Output is 
```
799/Unknown - 231s 289ms/step - loss: 0.0250 - accuracy: 0.9981
```

Issue:
The model is training infinite it isn't stopping, I also stop running the cell and run the evaluation in test data, it is evaluating for infinite.  I have a total of 15 images in two classes. 

I am using tensorflow version `2.2.0` and keras version `2.3.0-tf` and running the code on google colab. Thanks."
39674,Cloud TPU not connecting,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am trying to run a custom script in Google cloud TPUs using TPUStratagy.
Whenever I try to start the TPUs to work it is hung up on connect to the cluster and not proceeding to the next step

2020-05-19 09:43:27.869431: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-19 09:43:27.875081: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz
2020-05-19 09:43:27.875376: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4630540 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-19 09:43:27.875476: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-19 09:43:27.882332: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> node-1:8470}
2020-05-19 09:43:27.882369: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:43986}

After this, it is not going any further or showing any error msgs.

*Code to connect to TPU*

`
cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://node-1')
tf.config.experimental_connect_to_cluster(cluster_resolver)
tf.tpu.experimental.initialize_tpu_system(cluster_resolver)
strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)
`

Instance name (VM) : instance - 1
Zone : us-central1-b
TPU name = node-1
**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39673,AllocateTensors fails in hello world example,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 32165792a
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Host

**Describe the problem**
I try to run the hello world example as described on page 92 of the TinyML book but it fails.

**Please provide the exact sequence of commands/steps when you ran into the problem**
Building the binary
`make -f tensorflow/lite/micro/tools/make/Makefile hello_world`
Running the binary
`tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world`
I get back
```
Failed to allocate memory. Requested: 400, available 264, missing: 136
Failed to allocate memory for node_and_registrations.
AllocateTensors() failed
[1]    9967 segmentation fault (core dumped)  tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/hello_world
```"
39672,tflite_convert with --experimental_new_converter=True produce wrong result,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): tf-nightly 2.2.0.dev20200408, master version also has this problem
- TensorFlow version (or github SHA if from source): 2.2.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
tflite_convert --saved_model_dir=./SavedModelLight_0012 --output_file=model_v12_24.tflite --experimental_new_converter=True
```

**The output from the converter invocation**

```
2020-05-19 15:33:51.065585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-19 15:33:52.879630: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-19 15:33:52.886456: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-05-19 15:33:52.886489: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: GPU003
2020-05-19 15:33:52.886495: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: GPU003
2020-05-19 15:33:52.886528: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 418.39.0
2020-05-19 15:33:52.886572: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 418.39.0
2020-05-19 15:33:52.886577: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 418.39.0
2020-05-19 15:33:52.886747: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-05-19 15:33:52.891806: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3191935000 Hz
2020-05-19 15:33:52.892139: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0xac00be0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-19 15:33:52.892151: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-19 15:33:59.546331: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-05-19 15:33:59.546416: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-05-19 15:33:59.605104: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:802] Optimization results for grappler item: graph_to_optimize
2020-05-19 15:33:59.605149: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   function_optimizer: Graph size after: 1554 nodes (1425), 2396 edges (2267), time = 35.24ms.
2020-05-19 15:33:59.605157: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:804]   function_optimizer: function_optimizer did nothing. time = 0.916ms.
I0519 15:34:00.563315 139800252675840 lite.py:672] Using experimental converter: If you encountered a problem please file a bug. You can opt-out by setting experimental_new_converter=False
```

**Also, please include a link to the saved model or GraphDef**

```
[generator.zip](https://github.com/tensorflow/tensorflow/files/4649090/generator.zip)
[tflite.zip](https://github.com/tensorflow/tensorflow/files/4649097/tflite.zip)

```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results
saved model output vs tflite output
![image](https://user-images.githubusercontent.com/38289304/82303562-aef7dd80-99ed-11ea-9d0f-f15d6bde1c57.png)

as you can see, tflite's output result has a weird texture, if I set --experimental_new_converter=False them I can get correct output, but I can't have dynamic shape."
39671,"Update ""Custom Training"" section for `Model.train_step`","## URL(s) with the issue:
https://www.tensorflow.org/tutorials/customization/custom_training
https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough

## Description of issue (what needs changing):
The tutorial for custom training makes no mention of the `train_step` and related methods that have been added to `keras.Model`.  

One of these tutorials should have a section and example on using a custom `train_step` method.
"
39670,ops.executing_eagerly_outside_functions AssertionError when using train_on_batch with MirroredStrategy and disable_eager_execution,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Quadro RTX

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` v2.2.0-rc4-8-g2b96f3662b 2.2.0


**Describe the current behavior**
Multi-GPU training (MirrorStrategy) with Keras train_on_batch API and disable_eager_execution() causes the following AssertionError:

```
Traceback (most recent call last):
  File ""mini_test.py"", line 27, in <module>
    model.train_on_batch(x, y)
  File ""/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_v1.py"", line 1050, in train_on_batch
    output_loss_metrics=self._output_loss_metrics)
  File ""/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py"", line 316, in train_on_batch
    output_loss_metrics=output_loss_metrics))
  File ""/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_eager.py"", line 250, in _process_single_batch
    with backend.eager_learning_phase_scope(1 if training else 0), \
  File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
    return next(self.gen)
  File ""/home/ubuntu/keras-examples/venv-keras-examples/lib/python3.6/site-packages/tensorflow/python/keras/backend.py"", line 456, in eager_learning_phase_scope
    assert ops.executing_eagerly_outside_functions()
AssertionError
```
**Describe the expected behavior**
Error only occurs when using train_on_batch, MirrorStrategy and disable_eager_execution all together at the same time. 
* Turn off MirrorStrategy or disable_eager_execution will make the error disappear. 
* Replace model.train_on_batch() by model.fit() will also make the error go away

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
import numpy as np

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

strategy = tf.distribute.MirroredStrategy(
    devices=[""/gpu:0"", ""/gpu:1""])

x = np.zeros((32, 28, 28))
y = np.zeros((32,))

with strategy.scope():
    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(10),
    ])

    model.compile(
        optimizer=tf.keras.optimizers.SGD(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

for i in range (2):
    model.train_on_batch(x, y)

print(tf.executing_eagerly())
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Replace model.train_on_batch by model.fit (the code below) will make the error go away. Comment out `disable_eager_execution()` or `with strategy.scope():` will also make the error disappear. 

```
import tensorflow as tf
import numpy as np

from tensorflow.python.framework.ops import disable_eager_execution
disable_eager_execution()

strategy = tf.distribute.MirroredStrategy(
    devices=[""/gpu:0"", ""/gpu:1""])

with strategy.scope():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(512)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(64, activation='relu'),       
        tf.keras.layers.Dense(10),
    ])

    model.compile(
        optimizer=tf.keras.optimizers.SGD(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

model.fit(
    train_dataset,
    epochs=2
)

print(tf.executing_eagerly())
```"
39669,How to hook cuda api in the latest tensorflow?,"I wanted to reimplement cuMemGetInfo and cuMemAlloc for gpu resource limitations, and it worked before, but recently it doesn't work any more:
```
CUresult cuMemGetInfo(size_t *free, size_t *total) {
  size_t used = 0;

//  if (g_vcuda_config.enable) {
//    atomic_action(pid_path, get_used_gpu_memory, (void *)&used);

      *total = UP_LIMIT;
      *free = UP_LIMIT - USED;

    return CUDA_SUCCESS;
//  }

//  return CUDA_ENTRY_CALL(cuda_library_entry, cuMemGetInfo_v2, free, total);
}
```

then I compile the file using gcc:
`gcc -shared -fPIC malloc_hook.cpp -o malloc_hook.so`

and load my library using LD_PRELOAD, it still gives the unlimited gpu memory.

tensorflow environment:
docker tensorflow/tensorflow:latest-gpu-py3

And run some inference benchmark:
```
root@cac2936adfe8:/# python3 /test/benchmarks/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py --model=resnet50
2020-05-19 06:41:22.733408: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer.so.6
2020-05-19 06:41:22.734860: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libnvinfer_plugin.so.6
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.
Instructions for updating:
non-resource variables are not supported in the long term
2020-05-19 06:41:23.324689: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-19 06:41:23.335171: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394360000 Hz
2020-05-19 06:41:23.335310: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5109470 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-19 06:41:23.335324: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-19 06:41:23.338012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-19 06:41:23.470965: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50f82a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-19 06:41:23.471014: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-05-19 06:41:23.472144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-05-19 06:41:23.472212: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-19 06:41:23.472267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-19 06:41:23.474753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-19 06:41:23.475185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-19 06:41:23.478005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-19 06:41:23.479674: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-19 06:41:23.479734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-19 06:41:23.481353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-05-19 06:41:23.481398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-19 06:41:23.851131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-19 06:41:23.851172: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-05-19 06:41:23.851197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-05-19 06:41:23.852244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10236 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)
TensorFlow:  2.1
Model:       resnet50
Dataset:     imagenet (synthetic)
Mode:        training
SingleSess:  False
Batch size:  64 global
             64 per device
Num batches: 100
Num epochs:  0.00
Devices:     ['/gpu:0']
NUMA bind:   False
Data format: NCHW
Optimizer:   sgd
Variables:   parameter_server
==========
Generating training model
WARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
W0519 06:41:23.879678 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:134: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.keras.layers.Conv2D` instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
W0519 06:41:23.881723 139849542941568 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
Please use `layer.__call__` method instead.
WARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
W0519 06:41:23.907096 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/convnet_builder.py:266: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.MaxPooling2D instead.
Initializing graph
WARNING:tensorflow:From /test/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2267: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
W0519 06:41:25.853999 139849542941568 deprecation.py:323] From /test/benchmarks/scripts/tf_cnn_benchmarks/benchmark_cnn.py:2267: Supervisor.__init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.MonitoredTrainingSession
2020-05-19 06:41:26.146340: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1555] Found device 0 with properties: 
pciBusID: 0000:af:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-05-19 06:41:26.146396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-19 06:41:26.146406: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-19 06:41:26.146423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-19 06:41:26.146433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-19 06:41:26.146442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-19 06:41:26.146453: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-19 06:41:26.146460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-19 06:41:26.147285: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1697] Adding visible gpu devices: 0
2020-05-19 06:41:26.147317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-19 06:41:26.147326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]      0 
2020-05-19 06:41:26.147332: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] 0:   N 
2020-05-19 06:41:26.148206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1241] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10236 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:af:00.0, compute capability: 7.5)
INFO:tensorflow:Running local_init_op.
I0519 06:41:27.420840 139849542941568 session_manager.py:504] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0519 06:41:27.491935 139849542941568 session_manager.py:507] Done running local_init_op.
Running warm up
2020-05-19 06:41:28.953542: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-19 06:41:29.262221: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Done warm up
```
It still gives the correct gpu memory, not the UP_LIMIT that i set.
Could anyone please help?Thanks."
39668,"Faulty error message: ""InternalError: Failed copying input tensor from"" on tf.concat","The following code:
```python
import tensorflow as tf
print(tf.version.GIT_VERSION, tf.version.VERSION)
 
t1 = tf.constant([])
print(t1)
t2 = tf.constant([""hello"", ""world""])
print(t2)
t3 = tf.concat([t1, t2], axis=0)
print(t3)
``` 
Gives the following on my installation:
```
v2.2.0-rc4-8-g2b96f3662b 2.2.0
tf.Tensor([], shape=(0,), dtype=float32)
tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
~/omp/m5/plots_server.py in <module>
      6 t2 = tf.constant([""hello"", ""world""])
      7 print(t2)
----> 8 t3 = tf.concat([t1, t2], axis=0)
      9 print(t3)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in concat(values, axis, name)
   1604           dtype=dtypes.int32).get_shape().assert_has_rank(0)
   1605       return identity(values[0], name=name)
-> 1606   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
   1607 
   1608 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py in concat_v2(values, axis, name)
   1179         pass  # Add nodes to the TensorFlow graph.
   1180     except _core._NotOkStatusException as e:
-> 1181       _ops.raise_from_not_ok_status(e, name)
   1182   # Add nodes to the TensorFlow graph.
   1183   if not isinstance(values, (list, tuple)):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6651   message = e.message + ("" name: "" + name if name is not None else """")
   6652   # pylint: disable=protected-access
-> 6653   six.raise_from(core._status_to_exception(e.code, message), None)
   6654   # pylint: enable=protected-access
   6655 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run ConcatV2: Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0. [Op:ConcatV2] name: concat
```
On collab I get the correct output:
```
v2.2.0-0-g2b96f3662b 2.2.0
tf.Tensor([], shape=(0,), dtype=float32)
tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-17-fcff1beaaddc> in <module>()
      6 t2 = tf.constant([""hello"", ""world""])
      7 print(t2)
----> 8 t3 = tf.concat([t1, t2], axis=0)
      9 print(t3)

4 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a string tensor [Op:ConcatV2] name: concat
```

**System information**
 ```
lsb_release -a 
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.4 LTS
Release:	18.04
Codename:	bionic
```
Tensorflow installed - a docker container:
```
FROM tensorflow/tensorflow:2.2.0-gpu-jupyter
...
```
CPU: Intel(R) Xeon(R) CPU D-1518 @ 2.20GHz
GPU:  NVIDIA Corporation GP104 [GeForce GTX 1070 Ti] (rev a1)

The code is obviously incorrect, but the seem-to-be faulty error message makes it very difficult to spot the problem.
"
39667,New converter in tf 2.2 does not propely convert PReLU operation,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): TF Google Colaboratory
- TensorFlow version (or github SHA if from source): TF 2.2


**Command used to run the converter or code if you’re using the Python API**

```
def bilinear_resize(x, rsize):
  return tf.compat.v1.image.resize_bilinear(x, [rsize,rsize], align_corners=True)

model=load_model('/content/slim-net-157-0.02.hdf5',compile=False)

converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
open(""model.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
1511172 
```

**Also, please include a link to the saved model or GraphDef**

```
https://github.com/anilsathyan7/Portrait-Segmentation/blob/master/models/slim_seg_512/slim-net-157-0.02.hdf5
```
The result is same for any model in general with PReLU operation

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
-  It splits PReLU into Neg, ReLU and Mul layers
-  I need to run it using tflite gpu delegate which supports PReLU operation

**Any other info / logs**

It produces correct results using old converter i.e 
with option **converter.experimental_new_converter=False**
"
39666,ImportError: DLL load failed: The specified module could not be found.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
39665,Why is TF 2.2 much slower than TF 2.0?,"Followup to [original](https://github.com/tensorflow/tensorflow/issues/33487); I've benched various configurations per code [here](https://stackoverflow.com/questions/58441514/why-is-tensorflow-2-much-slower-than-tensorflow-1#answer-58653636), and found ""Large model on Large data"" to perform much worse in TF 2.2 than in TF 2.0 or TF 1.14.0. Code for that particular case, and result plots, below.

Why is this the case? To me, this is a clear dealbreaker for upgrading.

<hr>

**Test plots**:

![image](https://user-images.githubusercontent.com/16495490/82284722-bba71200-99aa-11ea-9877-933e45d5d945.png)

Per above, Graph and Eager are **1.56x** and **1.97x** slower than their TF1 counterparts, respectively. 

<hr>

<details>
  <summary><b>Test code</b></summary>

```python
import numpy as np
import tensorflow as tf
import random
import matplotlib.pyplot as plt
from termcolor import cprint
from time import time

from tensorflow.keras.layers import Input, Dense, Conv1D
from tensorflow.keras.layers import Dropout, GlobalAveragePooling1D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import tensorflow.keras.backend as K

# tf.compat.v1.disable_eager_execution()

###############################################################################
def reset_seeds(reset_graph_with_backend=None, verbose=1):
    if reset_graph_with_backend is not None:
        K = reset_graph_with_backend
        K.clear_session()
        tf.compat.v1.reset_default_graph()
        if verbose:
            print(""KERAS AND TENSORFLOW GRAPHS RESET"")

    np.random.seed(1)
    random.seed(2)
    tf.compat.v1.set_random_seed(3)
    if verbose:
        print(""RANDOM SEEDS RESET"")

print(""TF version: {}"".format(tf.__version__))
reset_seeds()

def timeit(func, iterations, *args, _verbose=0, **kwargs):
    times = []
    t0 = time()
    for _ in range(iterations):
        t1 = time()
        func(*args, **kwargs)
        times.append(time() - t1)
        print(end='.'*int(_verbose))
    print(""Time/iter: %.4f sec"" % ((time() - t0) / iterations))
    plt.stem(times)
    plt.ylim(0, 15)

###############################################################################
def make_model_large(batch_shape):
    ipt   = Input(batch_shape=batch_shape)
    x     = Conv1D(64,  400, strides=4, padding='valid')(ipt)
    x     = Conv1D(128, 200, strides=1, padding='valid')(x)
    for _ in range(40):
        x = Conv1D(256,  12, strides=1, padding='same')(x)
    x     = Conv1D(512,  20, strides=2, padding='valid')(x)
    x     = Conv1D(1028, 10, strides=2, padding='valid')(x)
    x     = Conv1D(256,   1, strides=1, padding='valid')(x)
    x     = GlobalAveragePooling1D()(x)
    x     = Dense(256, activation='relu')(x)
    x     = Dropout(0.5)(x)
    x     = Dense(128, activation='relu')(x)
    x     = Dense(64,  activation='relu')(x)
    out   = Dense(1,   activation='sigmoid')(x)
    model = Model(ipt, out)
    model.compile(Adam(lr=1e-4), 'binary_crossentropy')
    return model

def make_data(batch_shape):
    return np.random.randn(*batch_shape), \
           np.random.randint(0, 2, (batch_shape[0], 1))

def test_all():
    for model_fn, model_name, iters in zip(make_model_fns, model_names,
                                           iterations):
        for batch_shape, shape_name in zip(batch_shapes, shape_names):
            reset_seeds(reset_graph_with_backend=K)
            data = make_data(batch_shape)
            model = model_fn(batch_shape)

            model.train_on_batch(*data)
            timeit(model.train_on_batch, iters, *data, _verbose=1)
            
            cprint("">> {}, {} done <<\n"".format(model_name, shape_name), 'blue')
            del model
###############################################################################

batch_shape_large  = (32, 14000, 30)
batch_shape = batch_shape_large
model_fn = make_model_large

batch_shapes = batch_shape_large,
make_model_fns = make_model_large,
iterations = [200]
shape_names = [""Large data""]
model_names = [""Large model""]

test_all()
```

</details>"
39664,tflite_interpreter.invoke() give error for 2 input tensors,"project description:
auto image caption model building using LSTM and ann. and convert  model into tflite model.
 
problem:
so, in this model we need to give 2 inputs one for image another one for text.
tflite model was created.
when i checking tflite model through input, `tflite_interpreter.invoke()` arise error.

i attached my ipynb link.kindly go through it.
advance thanks for every one.
[Notebook](https://github.com/mahesh2996/image_caption)

"
39663,"When using tf.keras.Model.predict(), the `batch_size` makes the results different","The code below is different:
images = np.array([cv2.imread('a.jpg'), cv2.imread('b.jpg')]) / 255.
model = tf.keras.applications.xception.Xception()
model.predict(images,  **batch_size**=1)

and:
model = tf.keras.applications.xception.Xception()
model.predict(images,  **batch_size**=2)
"
39662,Resize images before feeding for deep tensorflow model,"Hello everyone.
 I tested any size of input image for ssd detection models, it don’t effect in timing prediction and all the results are same and correct.
In your opinion, inside the frozen graph model has resizer node or operation for this task on GPU?"
39661,Unaligned 64-bit access causes crash on Hexagon platforms,"We've discovered the cause of a TF Lite Micro crash on some Qualcomm Hexagon platforms, and need to figure out a good solution. Here are the details:

Quote from tensorflow/lite/micro/micro_allocator.cc:327 :

```

      // Note that the zero_point field in the FlatBuffers schema is a 64-bit

      // integer, but the zero_point field in the TfLiteQuantizationParams struct

      // is a 32-bit integer.

      result->params.zero_point =

          static_cast<int32_t>(src_quantization->zero_point()->Get(0));

```

As stated in the comment, the zero_point is a 64bit value inside flatbuffer’s schema.

But when hexagon DSP tries to read a 64bit value, the address needs to be 64bit aligned. This is a hardware instruction level limitation of our DSP.

So, when the execution fell into IndirectHelper::Read(), hexagon-sim crashed.

Quote from tensorflow/lite/micro/tools/make/downloads/flatbuffers/include/flatbuffers/flatbuffers.h:104 :

```

>>  static return_type Read(const uint8_t *p, uoffset_t i) {

      return EndianScalar((reinterpret_cast<const T *>(p))[i]);

    }

```

This pointer “p” is not guaranteed to be 8 byte aligned. So when I am not lucky, the linker will give it a bad location. Hence, random crash.

 "
39660,test1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39658,TFLite quantized maximum/minimum ops still do basic comparison even with different input tensor scales,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 18.04.1-Ubuntu
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: from source, git hash 491d6e42ce118a0c0156ce93eca67541069c617f (note bug is not specific to this hash)
- **TensorFlow version (use command below)**: 2.1.0
- **Python version**: 3.6.9
- **Bazel version (if compiling from source)**: 3.0.0
- **GCC/Compiler version (if compiling from source)**:  7.5.0
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I found tensorflow lite quantized element-wise maximum/minimum ops has issue when its input tensors have different scales. It supposed to do rescaling on one of its input tensor before doing the max/min selection. but from the source code and my experiment it seems to do the direct comparison.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here's my synthetic tflite .mlir of my quantized uint8 deepspeech model:

module attributes {tfl.description = ""TOCO Converted."", tfl.schema_version = 3 : i32} {
  func @main(%arg0: tensor<16x2048x!quant.uniform<u8:f32, 0.24592778086662292>>) -> tensor<16x2048x!quant.uniform<u8:f32, 0.078431375324726104>> attributes {tf.entry_function = {inputs = ""input_tensor"", outputs = ""Minimum""}} {
    %0 = ""tfl.pseudo_qconst""() {qtype = tensor<!quant.uniform<u8<1:255>:f32, 0.078431375324726104>>, value = dense<-1> : tensor<i8>} : () -> tensor<!quant.uniform<u8<1:255>:f32, 0.078431375324726104>>
    %1 = ""tfl.minimum""(%arg0, %0) : (tensor<16x2048x!quant.uniform<u8:f32, 0.24592778086662292>>, tensor<!quant.uniform<u8<1:255>:f32, 0.078431375324726104>>) -> tensor<16x2048x!quant.uniform<u8:f32, 0.078431375324726104>>
    return %1 : tensor<16x2048x!quant.uniform<u8:f32, 0.078431375324726104>>
  }
}

input (rhs input to minimum op) and output of this graph are the same (since rhs input to minimum is constant 255):
[[15  1  0 ...  3  1  0]
 [14  4  0 ...  1  7  0]
 [ 8  1  0 ...  0  0  0]
 ...
 [ 0  0  0 ...  0  0  0]
 [ 0  0  0 ...  0  8  0]
 [ 0  3  9 ...  0  1  0]]

But since output tensor scale is 0.07843, and lhs input tensor scale is 0.24592. shouldn't output be rescaled in this case? My predicted result, which takes this into consideration, would be:
[[47  3  0 ...  9  3  0]
 [44 13  0 ...  3 22  0]
 [25  3  0 ...  0  0  0]
 ...
 [ 0  0  0 ...  0  0  0]
 [ 0  0  0 ...  0 25  0]
 [ 0  9 28 ...  0  3  0]]

And I take a look at tensorflow lite reference kernel implementation at tensorflow/lite/kernels/maximum_minimum.cc. Not surprisingly, it's doing the comparison directly without any rescale. Seems like tfl quantized maximum/minimum ops are assuming input tensors always have same scales?
"
39657,test,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
39656,Eager execution in TF 2.2.0rc2,"Tensorflow 2.2.0rc2
Windows 10
Python 3.6.8(Anaconda)
CuDNN 7.6.5 CUDA 10.1
Nvidia 1060 6Gb
I have a model based on Keras

> import tensorflow.keras as keras

when try 
```
model_details=model.fit(
    data_train,
    box_train,    
    batch_size=128,epochs=100,shuffle=True,validation_split=0.1,callbacks=[lr_scheduler,lr_reducer,checkpoint],verbose=1)
```

got error
`InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]`


but when disable Eager , everything is working?

`tf.compat.v1.disable_eager_execution()`"
39654,How to speed up text generation in TensorFlow reference example notebook?,"The tensorflow official example for text generation (https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb) runs in a loop as defined below. The text generation feels slow, and according to NVTOP only uses a fraction of the available GPU resources (15-20%).

```
def generate_text(model, start_string):
  # Evaluation step (generating text using the learned model)

  # Number of characters to generate
  num_generate = 1000

  # Converting our start string to numbers (vectorizing)
  input_eval = [char2idx[s] for s in start_string]
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty string to store our results
  text_generated = []

  # Low temperatures results in more predictable text.
  # Higher temperatures results in more surprising text.
  # Experiment to find the best setting.
  temperature = 1.0

  # Here batch size == 1
  model.reset_states()
  for i in range(num_generate):
      predictions = model(input_eval)
      # remove the batch dimension
      predictions = tf.squeeze(predictions, 0)

      # using a categorical distribution to predict the character returned by the model
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # We pass the predicted character as the next input to the model
      # along with the previous hidden state
      input_eval = tf.expand_dims([predicted_id], 0)

      text_generated.append(idx2char[predicted_id])

  return (start_string + ''.join(text_generated))
```

Do you have any suggestions on how I can speed this up? Or parallelize it by generating multiple examples at the same time? A quick look at cprofiler shows that 90% of the time is spent on the single line predictions = model(input_eval), so this is where we'd most likely find a speedup. Would appreciate any advice, and happy to submit a PR if I'm able to speed it up! 

**System information**
- I am running the TensorFlow reference text generation example:  https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb
- Tested on Debian and Google Colab (with GPU support)
- TensorFlow installed from (source or binary): Binary
-TensorFlow version: v2.1.0-rc2-17
- Python version: 3.7.5
- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.243
- GPU model and memory: NVidia Tesla T4

**Describe the current behavior**
Text generation works fine, but feels slow. Using NVTOP it shows only 15% GPU utilization on average.

**Describe the expected behavior**
Hoping to speed up text generation by better leveraging the GPU

**Standalone code to reproduce the issue**
This issue can be replicated by running the standard TensorFlow text generation tutorial on Google Colaboratory with GPU

**Other info / logs** Include any logs or source code that would be helpful to

![Screen Shot 2020-05-18 at 10 20 17 AM](https://user-images.githubusercontent.com/6510818/82244078-7f65aa00-98f5-11ea-95b0-87f1f5ab89fb.png)
"
39653,[Tf 2.x] Can't return variables as tf.keras.Model outputs,"**System information**
- Reproduced in Colab, currently with Tensorflow 2.2
- Reproduced in Debian Buster, Tensorflow 2.1, CPU, built from source.

**Describe the current behavior**
```python
import tensorflow as tf

inputs = tf.keras.Input([])
var = tf.Variable(3.0)

# OK
tf.keras.Model(inputs=inputs, outputs=[inputs*var])

# AttributeError exception
tf.keras.Model(inputs=inputs, outputs=[inputs*var,var])
# The same happens with variants like `tf.identity(var)`
```

**Describe the expected behavior**
No exception raised

**Standalone code to reproduce the issue**
Code [https://colab.research.google.com/drive/1EBXU51kVb6zrf1gA4iHcJInzkixrXk4l?usp=sharing](https://colab.research.google.com/drive/1EBXU51kVb6zrf1gA4iHcJInzkixrXk4l?usp=sharing)

**Other info / logs**
Same as: https://github.com/keras-team/keras/issues/12673, but that was on the wrong repo, I think.
"
39652,Tensorflow containers are missing from Docker Hub,"Your docker pages point at 

https://hub.docker.com/r/tensorflow/tensorflow 

Today that returns ""404"" Oops! Page not found."
39651,Checkpoint is not work properly,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1909 Home
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
tf.save_weights is not work properly

https://www.tensorflow.org/tutorials/keras/save_and_load#saved_model%EC%9D%84_%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0

When I follow the tutorial of tensorflow checkpoint section, It does not make *.ckpt file but makes .ckpt folder!!!!

**Describe the expected behavior**
It need make .ckpt files.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


```
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import cv2
import random
import os




np.random.seed(1)
random.seed(1)
tf.random.set_seed(1)

checkpoint_path = 'checkpoint\\cp-{epoch:04d}.ckpt'
checkpoint_dir = os.path.dirname(checkpoint_path)


(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

train_images = train_images / 255.0
test_images = test_images / 255.0


train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))


model = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.02, seed=1),
    tf.keras.layers.Dense(254, activation='relu'),
    tf.keras.layers.Dropout(0.02, seed=1),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.01, seed=1),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='sigmoid'),
    tf.keras.layers.Dense(10)
])



checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weight_only=True,
    period=2
)


model.save_weights(checkpoint_path.format(epoch=0))

epochs = 10

model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)


model.fit(
    train_dataset.shuffle(int(len(train_images)+500), seed=1).batch(256),
    epochs=epochs,
    callbacks=[checkpoint_callback],
    verbose=1
)


loss, acc = model.evaluate(test_images,  test_labels, verbose=2)
print(""Untrained model, accuracy: {:5.2f}%"".format(100*acc))

```"
39649,tf.math.reduce_mean takes too long and produces wrong result when input_tensor is uint32/64 and axis is array,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04, macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):v2.1.0-rc2-17-ge5bf8de410 2.1.0 & v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version:3.7.6
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
`tf.math.reduce_mean` hangs(?) or takes forever to compute for certain input (with dtype uint64 and uint32, and axis an array). Around when the slow down occurs, the function produces incorrect result.

This function affects other functions' performance: `tf.math.reduce_std` which calls `tf.math.reduce_variance` which calls `tf.math.reduce_mean`
**Describe the expected behavior**
It should not take forever to compute nor produce an incorrect result.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

An example that showed incorrect result:
```python
import tensorflow as tf
import numpy as np
import time

input_tensor = np.arange(4).astype('uint64')  # also occurs for 'uint32'

for i in range(20):
  axis = [0] * i
  print('axis = ', axis)
  start = time.time()
  res = tf.math.reduce_mean(input_tensor, axis=axis)
  end = time.time()
  time_diff = end - start
  print('result = ', res.numpy())
  print('-- took %d sec --' % time_diff)
```
An example that both became extremely slow and produced an incorrect result:
```python
import tensorflow as tf
import numpy as np
import time

# 15 is a magic number chosen for illustration
# a smaller number may not cause the slow-down but could still show the wrong result
input_tensor = np.arange(15).astype('uint64')  # also occurs for 'uint32'

for i in range(20):
  # produces incorrect result when i == 8.
  # slow down occurs around when i == 10
  axis = [0] * i
  print('axis = ', axis)
  start = time.time()
  res = tf.math.reduce_mean(input_tensor, axis=axis)
  end = time.time()
  time_diff = end - start
  print('result = ', res.numpy())
  print('-- took %d sec --' % time_diff)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39648,no such package '@androidsdk//com.android.support',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (MacOS 10.13.6):
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Apple Clang 10.0.0
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

Hello， dear tensorflow guys.

I want to build the official android demo from TF team.

My command is:

`bazel build -c opt --cxxopt='--std=c++11'   //tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo --experimental_repo_remote_exec
`

I set both Android SDK and NDK enviroment.

```
export ANDROID_SDK_API_LEVEL=26
export ANDROID_NDK_API_LEVEL=21
```


However, it has the error:

> ERROR: /Users/junyan/find_android_project/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:8:1: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'
> ERROR: /Users/junyan/find_android_project/tensorflow/tensorflow/lite/java/demo/app/src/main/BUILD:8:1: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo'
> ERROR: Analysis of target '//tensorflow/lite/java/demo/app/src/main:TfLiteCameraDemo' failed; build aborted: no such package '@androidsdk//com.android.support': BUILD file not found in directory 'com.android.support' of external repository @androidsdk. Add a BUILD file to a directory to mark it as a package


The build script is:

```
load(""@build_bazel_rules_android//android:rules.bzl"", ""android_binary"")

package(
    default_visibility = [""//visibility:private""],
    licenses = [""notice""],  # Apache 2.0
)

android_binary(
    name = ""TfLiteCameraDemo"",
    srcs = glob([""java/**/*.java""]),
    assets = [
        ""//tensorflow/lite/java/demo/app/src/main/assets:labels_mobilenet_quant_v1_224.txt"",
        ""@tflite_mobilenet_quant//:mobilenet_v1_1.0_224_quant.tflite"",
        ""@tflite_mobilenet_float//:mobilenet_v1_1.0_224.tflite"",
    ],
    assets_dir = """",
    custom_package = ""com.example.android.tflitecamerademo"",
    manifest = ""AndroidManifest.xml"",
    nocompress_extensions = [
        "".tflite"",
    ],
    resource_files = glob([""res/**""]),
    # In some platforms we don't have an Android SDK/NDK and this target
    # can't be built. We need to prevent the build system from trying to
    # use the target in that case.
    tags = [""manual""],
    deps = [
        ""//tensorflow/lite/java:tensorflowlite"",
        ""//tensorflow/lite/java:tensorflowlite_gpu"",
        ""//tensorflow/lite/java/src/testhelper/java/org/tensorflow/lite:testhelper"",
        ""@androidsdk//com.android.support:support-v13-25.2.0"",
        ""@androidsdk//com.android.support:support-v4-25.2.0"",
    ],
```


Should I download com.android.support:support-v13-25.2.0 aar and com.android.support:support-v4-25.2.0 under the directory of android-sdk to compile it successfully?

Thanks & Regards!"
39647,FailedPreconditionError:  Error while reading resource variable _AnonymousVar878 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar878/N10tensorflow3VarE does not exist. 	 [[node mul_584/ReadVariableOp (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_34197],"I'm getting this error and I'm not able to debug it. This is the complete code of CGAN.

    from __future__ import print_function, division
    from keras.datasets import mnist
    from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply
    from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D
    from keras.layers.advanced_activations import LeakyReLU
    from keras.layers.convolutional import UpSampling2D, Conv2D,Conv2DTranspose
    from keras.models import Sequential, Model
    from keras.optimizers import Adam
    from keras.layers import concatenate
    from tensorflow.keras.utils import to_categorical
    import matplotlib.pyplot as plt
    import numpy as np

    class CGAN():
        def __init__(self):
            # Input shape
            self.img_rows = 28
            self.img_cols = 28
            self.channels = 1
            self.img_shape = (self.img_rows, self.img_cols, self.channels)
            self.num_classes = 10
            self.latent_dim = 100

            optimizer = Adam(0.0002, 0.5)

        # Build and compile the discriminator
            self.discriminator = self.build_discriminator()
            self.discriminator.compile(loss=['binary_crossentropy'],
                optimizer=optimizer,
                metrics=['accuracy'])

        # Build the generator
            self.generator = self.build_generator()

        # The generator takes noise and the target label as input
        # and generates the corresponding digit of that label
            noise = Input(shape=(self.latent_dim,))
            label = Input(shape=(10,))
            img = self.generator([noise, label])

        # For the combined model we will only train the generator
            self.discriminator.trainable = False

        # The discriminator takes generated image as input and determines validity
        # and the label of that image
            valid = self.discriminator([img, label])

        # The combined model  (stacked generator and discriminator)
        # Trains generator to fool discriminator
            self.combined = Model([noise, label], valid)
            self.combined.compile(loss=['binary_crossentropy'],
            optimizer=optimizer)

        def build_generator(self):
          image_resize = self.img_rows // 4
      # network parameters
          kernel_size = 5
          layer_filters = [128, 64, 32, 1]

          inputs = Input(shape=(self.latent_dim,))
          x = inputs
          labels = Input(shape=(10,))

          x  = concatenate([inputs, labels], axis=1)
          x = Dense(image_resize * image_resize * layer_filters[0])(x)
          x = Reshape((image_resize, image_resize, layer_filters[0]))(x)

          for filters in layer_filters:
        # first two convolution layers use strides = 2
        # the last two use strides = 1
              if filters > layer_filters[-2]:
                  strides = 2
              else:
                  strides = 1
              x = BatchNormalization()(x)
              x = Activation('relu')(x)
              x = Conv2DTranspose(filters=filters,
                                  kernel_size=kernel_size,
                                 strides=strides,
                                 padding='same')(x)

          x = Activation('sigmoid')(x)
     
          return Model([inputs, labels], x, name='generator')


        def build_discriminator(self):
          kernel_size = 5
         layer_filters = [32, 64, 128, 256]

          inputs = Input(shape=(self.img_rows,self.img_rows,1))
          x = inputs
          labels = Input(shape=(10,))
          y = Dense(self.img_rows * self.img_rows)(labels)
          y = Reshape((self.img_rows, self.img_rows, 1))(y)
          x = concatenate([x, y])

          for filters in layer_filters:
        # first 3 convolution layers use strides = 2
        # last one uses strides = 1
            if filters == layer_filters[-1]:
              strides = 1
            else:
              strides = 2
            x = LeakyReLU(alpha=0.2)(x)
            x = Conv2D(filters=filters,
                    kernel_size=kernel_size,
                    strides=strides,
                    padding='same')(x)

          x = Flatten()(x)
          x = Dense(1)(x)
          x = Activation('sigmoid')(x)
     
          return Model([inputs, labels], x, name='discriminator')


        def train(self, epochs, batch_size=128, sample_interval=50):

        # Load the dataset
            (X_train, y_train), (_, _) = mnist.load_data()

        # Configure input
            X_train = (X_train.astype(np.float32) - 127.5) / 127.5
            X_train = np.expand_dims(X_train, axis=3)
            #y_train = to_categorical(y_train)
        

        # Adversarial ground truths
            valid = np.ones((batch_size, 1))
            fake = np.zeros((batch_size, 1))

            for epoch in range(epochs):

            # ---------------------
            #  Train Discriminator
            # ---------------------

            # Select a random half batch of images
                idx = np.random.randint(0, X_train.shape[0], batch_size)
                imgs, labels = X_train[idx], y_train[idx]
                labels = to_categorical(labels)
            # Sample noise as generator input
                noise = np.random.normal(0, 1, (batch_size, 100))

            # Generate a half batch of new images
                gen_imgs = self.generator.predict([noise, labels])

            # Train the discriminator
                d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)
                d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)
                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

            # ---------------------
            #  Train Generator
            # ---------------------

            # Condition on labels
                sampled_labels = to_categorical(np.random.randint(0, 10, batch_size).reshape(-1, 1))

            # Train the generator
                g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)

            # Plot the progress
                print (""%d [D loss: %f, acc.: %.2f%%] [G loss: %f]"" % (epoch, d_loss[0], 100*d_loss[1], g_loss))

            # If at save interval => save generated image samples
            #if epoch % sample_interval == 0:
             #   self.sample_images(epoch)

    ''' def sample_images(self, epoch):
        r, c = 2, 5
        noise = np.random.normal(0, 1, (r * c, 100))
        sampled_labels = to_categorical(np.arange(0, 10).reshape(-1, 1))

        gen_imgs = self.generator.predict([noise, sampled_labels])

        # Rescale images 0 - 1
        gen_imgs = 0.5 * gen_imgs + 0.5

        fig, axs = plt.subplots(r, c)
        cnt = 0
        for i in range(r):
            for j in range(c):
                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')
                axs[i,j].set_title(""Digit: %d"" % sampled_labels[cnt])
                axs[i,j].axis('off')
                cnt += 1
        fig.savefig(""images/%d.png"" % epoch)
        plt.close() '''


    if __name__ == '__main__':
        cgan = CGAN()
        cgan.train(epochs=20000, batch_size=32, sample_interval=200)

"
39645,How to implement data-dependent initialization with MultiGPU/TPU [Tensorflow >= 2.x],"# Environment
- tensorflow == 2.2.0 

# Reprodusable code
https://colab.research.google.com/gist/MokkeMeguru/fc7bc3df0e77a3a5ba2dbf16a806e6c7/data-dep-iinitialization.ipynb

# Problem
I want to weight initialization by first-batch data. 
So I need  method below condition:
- Aggregate data on Multi GPU / TPU
- Weight which will be initialized is trainable

I wrote these code

```python
class DataDepInit(tf.keras.layers.Layer):
  def __init__(self):
    super().__init__()

  def build(self, input_shape):
    # weight initialized by first batch
    self.w = self.add_weight(
        name=""mean"",
        shape=(1, 1, 1, 1),
        dtype=tf.float32,
        trainable=True,
        aggregation=tf.VariableAggregation.MEAN
    )
    # the controller about initialization
    self.initialized = self.add_weight(
        name=""init"",
        trainable=False,
        dtype=tf.bool,
    )

    self.initialized.assign(False)
    self.built = True
  
  def initialize(self, x):
    mean = tf.reduce_mean(x, axis=[0, 1, 2], keepdims=True)
    tf.print(""initialize"")
    self.w.assign(mean)

  def call(self, x):
    if not self.initialized:
      self.initialize(x)
      self.initialized.assign(True)
    return x - self.w

# ---------------------------------------------------------
with strategy.scope():
  x = tf.keras.Input(shape=(32, 32, 1))
  ddi = DataDepInit()
  model = tf.keras.Model(x, ddi(x))
  model.summary()
  
  def _step():
    model(tf.random.normal(shape=[128, 32, 32 , 1]))

  @tf.function
  def distributed_step():
    strategy.experimental_run_v2(_step, args=())

  for i in range(10):
    distributed_step()
```
But it causes error.

```
    <ipython-input-100-6d7fd2d86109>:15 _step  *
        model(tf.random.normal(shape=[128, 32, 32 , 1]))
    <ipython-input-79-fb0889bce767>:31 call  *
        self.initialize(x)
    <ipython-input-79-fb0889bce767>:27 initialize  *
        self.w.assign(mean)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py:798 assign  **
        return self._mirrored_update(assign_fn, *args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/values.py:786 _mirrored_update
        merge_fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2420 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/mirrored_strategy.py:1071 _merge_call
        ""`merge_call` called while defining a new graph or a tf.function. ""

    RuntimeError: `merge_call` called while defining a new graph or a tf.function. This can often happen if the function `fn` passed to `strategy.experimental_run()` is decorated with `@tf.function` (or contains a nested `@tf.function`), and `fn` contains a synchronization point, such as aggregating gradients. This behavior is not yet supported. Instead, please wrap the entire call `strategy.experimental_run(fn)` in a `@tf.function`, and avoid nested `tf.function`s that may potentially cross a synchronization boundary.
```

This solution may lead us to construct many data-dep-initialization layer is needed from weight normalization, Actnorm, etc. 

Thansk! "
39644,how to build tflite python for ios platform?,"how to build tflite python for ios platform. 
I want to run tflite on my app python interpreter."
39643,predict on batch and predict have different behavior Assert and control depedencies,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I have a custom keras layer with an assertion and a control depedencies: 

```
class CustomLayer(Layer):
  def call(self, inputs):
    assert_same_batch_size = tf.assert_equal(tf.shape(inputs[0])[0],
                                                 tf.shape(inputs[1])[0], message=""inputs do not have equal batch_size"")
    with tf.control_dependencies([assert_same_batch_size]):
      return tf.reshape(inputs[0], (tf.shape(inputs[0])[0], 3))   # i want it to break

a = Input(shape=1)
b = Input(shape=1)
out = CustomLayer()([a, b])
m = Model([a, b], out)
```

> `m.predict_on_batch([tf.constant(np.ones((10, 1))), tf.constant(np.ones((15, 1)))])`
```
ValueError: Cannot reshape a tensor with 10 elements to shape [10,3] (30 elements) for '{{node model_2/custom_layer_8/Reshape}} = Reshape[T=DT_FLOAT, Tshape=DT_INT32](model_2/Cast, model_2/custom_layer_8/Reshape/shape)' with input shapes: [10,1], [2] and with input tensors computed as partial shapes: input[1] = [10,3].
```

````
def generator(): # we use a generator to trick tf dataset into letting us use different nbr of rows in each vector
#(otherwise caught by tf.data.dataset.from_tensor_slices)
  for a in range(1):
    yield [tf.constant(np.ones((10, 1))), tf.constant(np.ones((15, 1)))]
m.predict(generator())
````

````
InvalidArgumentError:  assertion failed: [inputs do not have equal batch_size] [.....
````
**Describe the expected behavior**
The assertion error should be raised in both scenario with predict_on_batch and with predict. We dont know if its because control depencies is not used with predict_on_batch or if it comes from the assert. 


**Standalone code to reproduce the issue**
https://colab.research.google.com/gist/tanguycdls/d36d2a1175cec79143d0a3e9dddfbbad/untitled14.ipynb

"
39641,Inferring using a frozen graph requires more memory,"**System information**
- Have I written custom code: **yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.4 LTS**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **v1.14.0-rc1-22-gaf24dc91b5 1.14.0**
- Python version: **3.6.9**
- GPU model and memory: Not used

**Standalone code to reproduce the issue**
To freeze graph:
```
graph = tf.get_default_graph()
input_graph_def = graph.as_graph_def()
output_graph_def = tf.graph_util.convert_variables_to_constants(sess, input_graph_def, ['output/predictions'])
f.write(output_graph_def.SerializeToString())
```
To load graph:
```
with tf.gfile.GFile(graph_filepath, 'rb') as f:
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(f.read())
tf.import_graph_def(graph_def, self.placeholders)
scores = graph.get_tensor_by_name('import/output/scores:0')
self.normalized_scores = tf.nn.softmax(scores)
self.predictions = graph.get_tensor_by_name('import/output/predictions:0')
sess = tf.Session()
sess.graph.finalize()
```
To load checkpoint, I rebuild the graph from scratch, then use:
```
sess = tf.Session()
sess.run(tf.global_variables_initializer())
saver = tf.train.Saver()
ckpt = tf.train.get_checkpoint_state(ckpt_dir)
sess.graph.finalize()
```

**Describe the current behavior**
When using a checkpoint, a call to `sess.run(tf.global_variables_initializer())` takes up 260 MB (fair, considering raw weights are around that size), and then the process plateaus. 

When using a graph however:
* `graph_def.ParseFromString()` requires 255 MB, 
* `tf.import_graph_def()` uses an additional 264 MB, 
* and the first call to `sess.run` takes up 270 MB more. 

I can retrieve 255 MB by calling `del graph`, but `del graph_def` does nothing and the memory used by the graph_def is never released.

**Describe the expected behavior**
I should be able to free the 500+ MB used by the graph_def and imported graph, so that the model initialized with a frozen graph takes up as much memory as one initialized with a checkpoint.

Follow-up questions:
1. Shouldn't the model initialized with a frozen graph use even less memory than a checkpoint, considering that the training-only nodes were taken out? Does checkpoint contain values for training-only nodes? If not, why is it much larger? Is memory allocated for training-only nodes at all if I don't use an optimizer?
2. Why do calls to tf.Session() use different amounts of memory on different machines?

I use [memory_profiler](https://pypi.org/project/memory-profiler/) to measure the used memory.
The model is a CNN. "
39640,"Tflite ""Invoke"" function stuck problem","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux debian 9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: rk3399
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.1rc
- Python version: no
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: no
- GPU model and memory: Mali-T864 GPU



**Describe the current behavior**
We run the inference every frame, after calling the ""Invoke"" function some times, the function stuck. 


"
39639,Dataset iterating different behavior in TF 2.1 and 2.2,"System information
- OS Platform and Distribution: Windows 10 Home
- TensorFlow versions: 
  - 2.1: v2.1.0-rc2-17-ge5bf8de410 2.1.0
  - 2.2: v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.7.6

I was not sure how to report this issue as it might be bug or just expected behavior. There are difference in TF 2.1 and 2.2. This is a code snippet to reproduce my issue:

```python
import math
import numpy as np
import tensorflow as tf

# simple dataset with zeros
batch_size = 32
features = np.zeros((10000, 60, 2))
labels = np.zeros((10000, 1))
train_data = tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size)
train_steps = int(math.ceil(features.shape[0] / batch_size))

# simple model with Dense layers
inputs = tf.keras.Input(shape=(features[0].shape[0], features[0].shape[1]))
x = tf.keras.layers.Dense(32, activation=""relu"")(inputs)
outputs = tf.keras.layers.Dense(1, activation=""relu"")(x)
model = tf.keras.Model(inputs, outputs, name=""example_model"")

# model fitting
model.compile(loss=""mse"", optimizer=""adam"", metrics=[""mse""])
model.fit(train_data, epochs=100, steps_per_epoch=train_steps)
```

When I run this code in TF2.1 it will produce this error: https://pastebin.com/4M43SE44
After the first epoch, there are warnings about end of sequence, that my input ran out of data. And finally, as you can see in the pasted output, it raises `ValueError: Empty training data.`. 
When I change line with dataset creation to
<pre>
train_data = tf.data.Dataset.from_tensor_slices((features, labels)).batch(batch_size).<b>repeat()</b>
</pre>
than everything works as expected.

This is behavior I would expect. (Note `steps_per_epoch` attribute as I wan to control this by myself, of course when I do not use `repeat` and `steps_per_epoch` is set to None, it will work under TF2.1 as it will iterate whole dataset every epoch).

When I run the same code with TF2.2 (no repeat, train_steps are specified) it works without any issue. Is this behavior intentional? Why does it work in TF2.2 and not 2.1? Could anyone elaborate on this issue?"
39638,saved_model README.md uses deprecated code,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md

## Description of issue (what needs changing):

That documentation uses deprecated code like 'tf.Session()'

### Submit a pull request?

No, because I don't really know how it should be used now.
"
39637,"how to build tflite_runtime, I want to modify tflite_runtime.","how to build tflite_runtime, I want to modify tflite_runtime.
https://www.tensorflow.org/lite/guide/python"
39636,Debug keras code on Graph mode.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.15.1-2.2.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Sometimes I have to work on Graph mode(I can't use eager execution) and I can't debug Keras code with PDB since all code are converted by the autograph.
Could you disable the autograph on Graph mode?
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Developers
**Any Other info.**
"
39635,Bug in tf.keras bidirectional LSTM when time_major is true,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Macos Mojave 10.14.6
- TensorFlow installed from: Binary
- TensorFlow version: v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.6.3

**Bug description**

When using bidirectional layer with forward/backward lstms with time_major=True and merge mode concat(same issue exists in other modes too), it produces incorrect results due to the below line:
https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/layers/wrappers.py#L658
If time_major=true (the input shape of bidi lstm is - [seq_len, batch_size, hidden_size]), axis 1 represents batch dimesnion and we end up reversing y_rev in batch dimension before concatenation while it should have been reversed in the dimension representing seq_len(axis 0).

This works fine when LSTM is time_major=False as in that case axis 1 represents seq_len.  Ideally it should see which axis - axis 0 or axis 1 represent the time dimension and reverse on that axis, instead of generically reversing on axis 1.

Could you please fix this bug as the time_major version of the lstms is more efficient.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

seq_len = 2
batch_size = 1
feature_dim = 1

input = tf.keras.Input(shape=(seq_len, feature_dim))
# Transpose input to be time major
input_transposed = tf.transpose(input, perm=[1,0,2])
output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(1, return_sequences=True, time_major=True), name='bi')(input_transposed)
model = tf.keras.Model(inputs=input, outputs=output)

# Set all the weights to be one for simplicity
rnn_layer = model.get_layer('bi')
weights = rnn_layer.get_weights()
new_w = [np.ones(x, dtype=np.float32) for x in [(feature_dim, 4), (1, 4), (4)] * 2]
rnn_layer.set_weights(new_w)

model.save(""test.h5"")
x = np.ones((batch_size, seq_len, feature_dim), dtype=np.float32)
expected = model.predict(x)
print(expected)
```

Expected result is :
```
[[[0.6082834  0.87263733]    
  [0.87263733 0.6082834 ]]]
which is:
[[[ forward_layer_seq_1, backward_layer_seq_1 ]]
 [[ forward_layer_seq_2, backward_layer_seq_2]]]
But what we get is:
[[[0.6082834  0.6082834 ]]
 [[0.87263733 0.87263733]]]
[[[ forward_layer_seq_1, backward_layer_seq_2 ]]
 [[ forward_layer_seq_2, backward_layer_seq_1]]]
```
"
39634,"Error with my Customized layers: bound method could not be transformed, Bad argument number for Name: 3, expecting 4","**System information**

- OS Platform and Distribution: Linux Ubuntu 16.04:
- TensorFlow version : 1:14
- Python version: 3.7

**Standalone code to reproduce the issue**
I would like to customize a Dropout layer in which I can cache and reset the dropout mask manually. The code is as below:

```
class DropoutControl(Layer):

    def __init__(self, rate, seed=None, **kwargs):
        super(DropoutControl, self).__init__(**kwargs)
        self.rate = rate
        self.seed = seed
        self.cache_dropout_mask = None

    def reset_dropout(self):
        rate = ops.convert_to_tensor(
              self.rate, dtype=self.input_dtype, name=""rate"")
        random_tensor = random_ops.random_uniform(
            shape=self.shape, seed=self.seed, dtype=self.input_dtype)
        keep_prob = 1 - rate
        scale = 1 / keep_prob
        keep_mask = random_tensor >= rate
        self.cache_dropout_mask  = scale * math_ops.cast(keep_mask, self.input_dtype)

    def get_dropout_mask(self):
        return self.cache_dropout_mask 

    def call(self, inputs, training):
        if self.cache_dropout_mask is None:
            self.shape = array_ops.shape(inputs)
            self.input_dtype = inputs.dtype
            self.reset_dropout()

        def dropped_inputs():
          return inputs * self.cache_dropout_mask

        output = tf_utils.smart_cond(training,
                                     dropped_inputs,
                                     lambda: array_ops.identity(inputs))
        return output

    def compute_output_shape(self, input_shape):
        return input_shape

    def get_config(self):
        config = {
            'rate': self.rate,
            'seed': self.seed
        }
        base_config = super(DropoutControl, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))
```
**Describe the current behavior**
When I use this layer, the system gave me the following warning. What is this problem? Is it an issue that I have to fix? 

> WARNING:tensorflow:Entity <bound method DropoutControl.call of <model_utils.DropoutControl object at 0x7ffaa71b2650>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method DropoutControl.call of <model_utils.DropoutControl object at 0x7ffaa71b2650>>: AssertionError: Bad argument number for Name: 3, expecting 4

Thanks ahead!



"
39633,centos7 gcc: error: unrecognized command line option '-std=c++14',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):CentOS Linux release 7.6.1810 (Core)
- TensorFlow installed from (source or binary): source
- TensorFlow version: r2.2
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0
- GCC/Compiler version (if compiling from source):  7.3.1

**Describe the problem**

I follow the official build flow，but I met the the problem with 

gcc: error: unrecognized command line option '-std=c++14'

I have try install 
yum install devtoolset-7
scl enable devtoolset-7 bash

And when I type gcc --version，it show me 7.3.1

But when I run
./configure
bazel build --config=opt --config=monolithic //tensorflow/tools/lib_package:libtensorflow

It still show me 

gcc: error: unrecognized command line option '-std=c++14'

I have no idea how to solve this problem,please help me,thanks
"
39632,"tensorflow-io import error caused by ""com_google_absl""","**System information**
- Many Linux
- TensorFlow installed from source (master branch) and binary
- TensorFlow version: latest tf-nightly
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): GCC 8.4.0


**Describe the problem**

A NotImplementedError 
""undefined symbol: _ZNK10tensorflow10FileSystem8BasenameEN4absl11string_viewE""
occurs when importing tensorflow-io 0.13.0 or a series of tensorflow-io-nightly. The root cause is related with commit 6c7e338ae7f0b0f2e224319de7e2165141c148fb. Do you have any plan to fix this error or to replace the abseil-cpp? Or, do you have any build method to bypass this error?
"
39631,TimeDistributed(Dropout()) with the same dropout mask,"**System information**
- TensorFlow version (you are using): 1.14
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**

Here is an example block of my code. I am trying to apply a time distributed dropout to the output of a many to many GRU. I would like to keep the dropout to have the same dropout mask for all time steps. However, I did not find a solution to this purpose based on the current API. Did I miss anything or is it a new feature on the roadmap? Thanks a lot! 

```
from tensorflow.keras.layers import Dense, Input, GRU, Dropout, TimeDistributed
x= TimeDistributed(Dense(512, activation='relu', kernel_regularizer=l2(1e-5), \
                bias_regularizer=l2(1e-5), name='cam_fc'))(input_tensor)
out = GRU(
                512,
                dropout=0.1,
                recurrent_dropout=0.1,
                activation='relu', 
                kernel_regularizer=l2(1e-5),
                bias_regularizer=l2(1e-5),
                return_sequences=True, 
                name='intentNet_gru')(x, training=self.is_train)

out = TimeDistributed(Dropout(0.1))(out, training=self.is_train)
```
"
39629,Successive STFT transforms increases signal amplitude,"Hi,
I successively applied tensorflow's STFT and iSTFT transforms several time on a signal, and the amplitude of the signal grows with each transform. I tried librosa's and scipy's STFT and this doesn't happen. Here is the exact code:

```
import librosa
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

length=88
fft_size = (length-1)*2   # 174
hop_size = int(fft_size/4)# 43

def get_STFT_librosa(sig):
    spec_librosa = librosa.core.stft(np.array(sig), n_fft=fft_size, hop_length=hop_size, win_length=fft_size)
    return spec_librosa

def get_iSTFT_librosa(spec):
    inverse = librosa.core.istft(spec, hop_length=hop_size, win_length=fft_size)
    return inverse
    
def get_STFT_tf(sig):
    STFT = tf.signal.stft(np.array(sig), frame_length=fft_size, frame_step=hop_size, fft_length=fft_size)
    STFT = STFT.numpy()
    spec_tf = STFT.T
    return spec_tf

def get_iSTFT_tf(spec):
    STFT = tf.convert_to_tensor(spec.T)
    sig = tf.signal.inverse_stft(STFT, frame_length=fft_size, frame_step=hop_size, fft_length=fft_size)
    return sig.numpy()

def librosa_tf_comparison():
    time = np.arange(0, 1000, 0.1);
    sig = np.sin(time)

    #STFT level 1
    tf_STFT = get_STFT_tf(sig)
    lib_STFT = get_STFT_librosa(sig)

    tf_iSTFT = get_iSTFT_tf(tf_STFT)
    lib_iSTFT = get_iSTFT_librosa(lib_STFT)

    #STFT level 2
    tf_STFT2 = get_STFT_tf(tf_iSTFT)
    lib_STFT2 = get_STFT_librosa(lib_iSTFT)

    tf_iSTFT2 = get_iSTFT_tf(tf_STFT2)
    lib_iSTFT2 = get_iSTFT_librosa(lib_STFT2)


    #STFT level 3
    tf_STFT3 = get_STFT_tf(tf_iSTFT2)
    lib_STFT3 = get_STFT_librosa(lib_iSTFT2)

    tf_iSTFT3 = get_iSTFT_tf(tf_STFT3)
    lib_iSTFT3 = get_iSTFT_librosa(lib_STFT3)

    plt.plot(sig[:800])
    plt.plot(tf_iSTFT[:800])
    plt.plot(tf_iSTFT2[:800])
    plt.plot(tf_iSTFT3[:800])
    plt.show()

    plt.plot(sig[:800])
    plt.plot(lib_iSTFT[:800])
    plt.plot(lib_iSTFT2[:800])
    plt.plot(lib_iSTFT3[:800])
    plt.show()

librosa_tf_comparison()

```
Here is what the code gives:
![librosa](https://user-images.githubusercontent.com/10775105/82160064-e69e4280-9892-11ea-9638-eb4445f45539.png)
This is what a simple sine looks like after three consecutive STFT/iSTFT with librosa (there is only one curve because they are merged, as should be)
![tf](https://user-images.githubusercontent.com/10775105/82160062-e1d98e80-9892-11ea-8a3b-5fa285c687cb.png)
This is what the same curve looks like after three consecutive STFT/iSTFT with tensorflow. The original sine wave is in blue, I get the yellow curve after one STFT/iSTFT, then green, then red.

The factor between one iSTFT wave and the previous one seems to be always the same no matter the shape of the wave (for this set of parameters), about 0.659.

I checked and the NOLA constraint is met with these parameters. Is this normal behaviour?"
39628,Use of keras Sequence in Model.fit() broken,
39627,Repost of keras-team/keras #13118.,"#### System information

* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
* TensorFlow version: tf-nightly (2.2)

#### Describe the current behavior
Loading a model once and then repeatedly calling `model.predict(...)` results in continually increasing memory usage.

#### Describe the expected behavior
Calling `model.predict(...)` should not result in any permanent increase in memory usage.

#### Code to reproduce the issue

```python
import tensorflow as tf
import numpy as np

model = tf.keras.applications.mobilenet_v2.MobileNetV2()
X = np.random.rand(1, 224, 224, 3)

while True:
    # Leaks:
    y = model.predict(X)[0]
    
    # Does not leak:
    # y = [0]
```"
39626,Creating custom RNN layers with different input and state shapes,"Hi,
I'm trying to build a custom RNN cell, which is a wrapper of an LSTM cell (or any other RNN cell), and in particular, I would need to add multiple hidden states to this layer. Also, the dimension of the hidden state would be different from the input shape.
Following the RNN layer documentation, I've created the following layer:
```
class QLSTM(tf.keras.layers.Layer):
    def __init__(self, units, symbols, activation=""tanh"", **kwargs):
        super().__init__(**kwargs)
        self.lstm_cell = tf.keras.layers.LSTMCell(units, activation=activation)
        self.expectation = tfq.layers.Expectation()
        self.symbol_names = symbols
        # State size: LSTM hidden state, LSTM outputs, QPU expectancy
        self.state_size = [tf.TensorShape([2]),tf.TensorShape([2]),tf.TensorShape([1])]
        self.output_size = tf.TensorShape([1]) # QPU expectation

    def call(self,inputs,state):
        circuits = inputs[0]
        ops = inputs[1]

        joined_state = tf.keras.layers.concatenate(state[1],state[2])

        output_lstm, hidden_state = self.lstm_cell(joined_state,state[0])
        exp_out = self.expectation(circuits,
            symbol_names=self.symbol_names,
            symbol_values=output_lstm,
            operators=ops
        )
        return exp_out, [hidden_state,output_lstm,exp_out]
```
So, in a nutshell, I would expect to have a hidden state that carries 3 main objects: the previous hidden state of the inner LSTM cell, the output of the previous step of the LSTM cell, and another scalar output (here called `exp_out`). For the inputs, I would expect an array of two tensors, which I'll explain later.

Then, I create an instance of the layer and wrap it with the RNN layer so as to get all the outputs from the multiple timestamps:
```
rnn = tf.keras.layers.RNN(QLSTM(2,qaoa_symbols),return_sequences=True)
```
For the input tensors, I have two tensors which have a shape of [None,10,1] each. The second dimension (10) represents the timestamps, while the third dimension (1) represents the dimension of each timestamp. So I create the input layers with the following code:
```
op_inp = tf.keras.Input(shape=(10,1,), dtype=tf.dtypes.string)
circuit_inp = tf.keras.Input(shape=(10,1,), dtype=tf.dtypes.string)
```
Finally, when I try to pass these inputs to the RNN layer, i do the following:
```
rnn_2 = rnn([circuit_inp,op_inp])
```
But I get the following error:
```
ValueError: An `initial_state` was passed that is not compatible with `cell.state_size`. Received `state_spec`=ListWrapper([InputSpec(shape=(None, 10, 1), ndim=3)]); however `cell.state_size` is [2, 2, 1]
```
I've been trying to fins tutorials and other issues on how to create custom RNN but i haven't been able to solve this issue. Any idea on how can I specify the correct input shapes?
Thanks in advance!"
39625,tensorflow.python.distribute.cross_device_utils.build_collective_gather does not support TPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7

**Describe the current behavior**
throw an error

**Describe the expected behavior**
collect tensors like running on GPUs

**Standalone code to reproduce the issue**
```
import os

import tensorflow as tf
from tensorflow.python.distribute import cross_device_utils

if 'COLAB_TPU_ADDR' in os.environ:
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
  strategy = tf.distribute.experimental.TPUStrategy(resolver)
else:
  strategy = tf.distribute.MirroredStrategy()


def all_gather(context, x):
  num = context.num_replicas_in_sync
  ck = cross_device_utils.CollectiveKeys()
  x = cross_device_utils.build_collective_gather([x], num, ck)
  return x[0]


@tf.function
def step_fn():
  context = tf.distribute.get_replica_context()
  v = tf.zeros([1], tf.int32) + context.replica_id_in_sync_group
  d = all_gather(context, v)
  return d

ret = strategy.run(step_fn)
print(ret)
```

**Other info / logs** Include any logs or source code that would be helpful to
```
TypeError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:896 replicated_fn  *
        result[0] = fn(*replica_args, **replica_kwargs)
    <ipython-input-1-6f3a04d9c55b>:27 step_fn  *
        d = all_gather(context, v)
    <ipython-input-1-6f3a04d9c55b>:19 all_gather  *
        x = cross_device_utils.build_collective_gather([x], num, ck)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:421 build_collective_gather  **
        group_key = collective_keys.get_group_key_of_tensors(input_tensors)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:319 get_group_key_of_tensors
        return self.get_group_key(devices)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:299 get_group_key
        names = sorted(['%s:%d' % (d.device_type, d.device_index) for d in parsed])
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/cross_device_utils.py:299 <listcomp>
        names = sorted(['%s:%d' % (d.device_type, d.device_index) for d in parsed])

    TypeError: %d format: a number is required, not NoneType
```
On a two GPUs machine, the output would be
```
PerReplica:{
  0: tf.Tensor([0 1], shape=(2,), dtype=int32),
  1: tf.Tensor([0 1], shape=(2,), dtype=int32)
}
```"
39624,lecun_normal and he_normal crash when type casting: TypeError: he_normal() got an unexpected keyword argument 'dtype',"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device if the issue happens on mobile device:
- TensorFlow installed from (source or binary): idk
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: none
- GPU model and memory: none

**Describe the expected behavior**

Successful type casting in `tf.data.Dataset.map()` function

**Standalone code to reproduce the issue**

```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
from tensorflow.keras import Model
import tensorflow as tf
from tensorflow.keras.layers import Dense
import tensorflow_datasets as tfds
from tensorflow.keras.initializers import glorot_normal, he_normal, lecun_normal

dataset, info = tfds.load('binary_alpha_digits', with_info=True, split='train')
data = dataset.map(lambda x: (tf.cast(x['image'], tf.float32), x['label'])).batch(8)


class Model(Model):
    def __init__(self):
        super(Model, self).__init__()
        self.layer1 = Dense(16, kernel_initializer=he_normal)
        self.layer2 = Dense(units=info.features['label'].num_classes)

    def call(self, inputs, training=None, **kwargs):
        x = self.layer1(inputs)
        x = self.layer2(x)
        return x


model = Model()
_ = model(next(iter(data))[0])
```
`glorot_normal`: works
`he_normal`: doesn't work
`lecun_normal`: doesn't work
"
39623,TFRecordDataset mapped with crop is heavily impacted by image sizes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Ubuntu 18.04, Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2
- Python version: 3.7.7, 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Colab's GPU, V100 32GB, CPU


**Describe the current behavior**
When loading a dataset with large images, and mapping it to random crop, iterating over the dataset is significantly slower, even for a small dataset of 10 images in repeat. In my example I compare a dataset of 10 240x240x3 images, to one with ten 2400x2400x3 images, both of them randomly cropped to 120x120x3 - the latter was about 100 times slower to operate on (reduce_sum, in my toy example).

**Describe the expected behavior**
I would expect there to be no effect from the size of the images in the dataset if they are being cropped right after being loaded once the dataset resides in memory. 

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1uGeLJJp2gPvgC37WxbP_yANAXB8D83ld#scrollTo=DddLV9gxm5eL


"
39622,TensorFlow utilization for traditional HPC scientific application written in FORTRAN for distributed computing,"Hi All,

I want to discuss and know the feasibility of using tensor flow for HPC scientific application written in FORTRAN and C using MPI for data distribution in distributed computing environment.

Few  research and feasibility study I did by writing small example of vector dot product in tensor flow. it is fairly very convenient  to write this code using python for tensor flow compared to writing a code using MPI and FORTRAN in which all the data distribution has been done programmer and relatively complex code.

Advantages:
1. No data distribution or copying data to GPU is required in Tensor flow which is underlying managed by tensor flow.
2. writing python code for array declaration or random variable initialization is very convenient.
3. Optimized mathematical operations written in tensor flow that can be directly used.

I am sure if used proper computational and bandwidth load in code we can achieve good performance also.

My Question is How feasible it is to port or develop HPC scientific applications in Tensor flow  and what are the challenges which cannot be handled by tensor flow?
Can we suggest clients to port there code in tensor flow for such advantages ?

"
39621,Unable to install older version of tensorflow in Python3.8.2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: Installed 2.2 but want 1.x
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I am able to install newer version of TensorFlow, i.e, 2.2. However, I need an older version of TensorFlow (1.x) to run existing code (otherwise I have to do a lot of changes as it's big repository)


CMD: sudo pip3 install tensorflow-gpu==1.15

ERROR: Could not find a version that satisfies the requirement tensorflow-gpu==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)
ERROR: No matching distribution found for tensorflow-gpu==1.15
"
39620,"whl compiled, still tries to install hdfs and gcp packages while --config=nogcp --config=nohdfs parameters were passed for build","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora release 32
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version:  r2.2
- Python version: 3.8.1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 10.0.1
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
Even though tensorflow build command used had --config=nogcp --config=nohdfs parameters, the whl build while doing a pip install was trying to install dependencies like h5py and other gcp related packages. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Have used following comand:

```
bazel build -s --config=noaws --config=nogcp --config=nohdfs --config=nonccl --config=opt //tensorflow/tools/pip_package:build_pip_package
```
```
./bazel-bin/tensorflow/tools/pip_package/build_pip_package /home/riscv/tensorflow_pkg
```
```
pip3 install /home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
[riscv@fedora-riscv tensorflow]$ pip3 install /home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl                                           Defaulting to user installation because normal site-packages is not writeable
Processing /home/riscv/tensorflow_pkg/tensorflow-2.2.0-cp38-cp38-linux_riscv64.whl
Requirement already satisfied: astunparse==1.6.3 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.6.3)
Requirement already satisfied: absl-py>=0.7.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.9.0)
Collecting grpcio>=1.8.6
  Using cached grpcio-1.29.0.tar.gz (19.6 MB)
Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.0)
Collecting gast==0.3.3
  Using cached gast-0.3.3-py2.py3-none-any.whl (9.7 kB)
Collecting h5py<2.11.0,>=2.10.0
  Using cached h5py-2.10.0.tar.gz (301 kB)
Requirement already satisfied: scipy==1.4.1; python_version >= ""3"" in /usr/lib64/python3.8/site-packages (from tensorflow==2.2.0) (1.4.1)
Requirement already satisfied: wrapt>=1.11.1 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.12.1)
Requirement already satisfied: termcolor>=1.1.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.1.0)
Collecting tensorboard<2.3.0,>=2.2.0
  Using cached tensorboard-2.2.1-py3-none-any.whl (3.0 MB)
Collecting google-pasta>=0.1.8
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Requirement already satisfied: protobuf>=3.8.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.11.3)
Requirement already satisfied: opt-einsum>=2.3.2 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (3.2.1)
Requirement already satisfied: six>=1.12.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.14.0)
Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (1.18.3)
Requirement already satisfied: wheel>=0.26; python_version >= ""3"" in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (0.34.2)
Requirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorflow==2.2.0) (2.2.0)
Collecting werkzeug>=0.11.15
  Using cached Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)
Requirement already satisfied: markdown>=2.6.8 in /home/riscv/.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.2.2)
Collecting google-auth<2,>=1.6.3
  Using cached google_auth-1.14.3-py2.py3-none-any.whl (89 kB)
Collecting google-auth-oauthlib<0.5,>=0.4.1
  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.6.0.post3-py3-none-any.whl (777 kB)
Requirement already satisfied: setuptools>=41.0.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (46.1.3)
Requirement already satisfied: requests<3,>=2.21.0 in /home/riscv/.local/lib/python3.8/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)
Collecting pyasn1-modules>=0.2.1
  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)
Collecting rsa<4.1,>=3.1.4
  Using cached rsa-4.0-py2.py3-none-any.whl (38 kB)
Collecting cachetools<5.0,>=2.0.0
  Using cached cachetools-4.1.0-py3-none-any.whl (10 kB)
Collecting requests-oauthlib>=0.7.0
  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: chardet<4,>=3.0.2 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.25.9)
Requirement already satisfied: certifi>=2017.4.17 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.4.5.1)
Requirement already satisfied: idna<3,>=2.5 in /home/riscv/.local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.9)
Collecting pyasn1<0.5.0,>=0.4.6
  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)
Collecting oauthlib>=3.0.0
  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)
Building wheels for collected packages: grpcio, h5py
....
```

PS: I am trying to get Tensorflow built for RISCV architecture here."
39618,The model does not detect any objects after converting to tflite using tflite_convert,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.15
- TensorFlow version: 1.15


**Command used to run the converter or code if you’re using the Python API**

```
toco --graph_def_file=./tflite_graph.pb --output_file=detect.tflite \
 --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
 --allow_custom_ops --mean_values=128   \
 --std_dev_values=127

```

Note: I've also tried using TOCO to convert to Tflite but achieved same results.



**The output from the converter invocation**

```
2020-05-17 14:41:03.490293: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-17 14:41:04.310308: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:03:00.0
2020-05-17 14:41:04.311111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:81:00.0
2020-05-17 14:41:04.311381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-17 14:41:04.312893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-05-17 14:41:04.314180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-05-17 14:41:04.314535: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-05-17 14:41:04.316277: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-05-17 14:41:04.317545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-05-17 14:41:04.321652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-17 14:41:04.333378: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2020-05-17 14:41:04.333816: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-17 14:41:04.362575: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2151740000 Hz
2020-05-17 14:41:04.364831: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559a2dc7ece0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-17 14:41:04.364871: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-17 14:41:04.945461: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x559a2dce1120 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-17 14:41:04.945510: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-05-17 14:41:04.945521: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-05-17 14:41:04.947161: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:03:00.0
2020-05-17 14:41:04.948586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:81:00.0
2020-05-17 14:41:04.948648: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-17 14:41:04.948676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-05-17 14:41:04.948701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-05-17 14:41:04.948726: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-05-17 14:41:04.948753: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-05-17 14:41:04.948779: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-05-17 14:41:04.948806: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-17 14:41:04.954369: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0, 1
2020-05-17 14:41:04.954434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-05-17 14:41:04.962865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-17 14:41:04.962892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 1 
2020-05-17 14:41:04.962902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N N 
2020-05-17 14:41:04.962909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 1:   N N 
2020-05-17 14:41:04.965698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10481 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1)
2020-05-17 14:41:04.967095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10481 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1)

```

**Also, please include a link to the saved model or GraphDef**

I trained this using tensorflow object-detection api from [ssdlite_mobilenet_v2_coco](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) in model zoo. I also used graph_rewriter in the pipeline config file.

```
https://drive.google.com/file/d/1pG3UHIcD3aE8rw8l_8MCqo9f8i9mTl61/view?usp=sharing
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:

- When I load the detect.tflite file into my application it does not detect any objects. But, the frozen_inference_graph.pb file generated using export_inference_graph.py works fine.



"
39617,Compile Tensorflow with c++17 using gcc,"**System information**
- OS Platform and Distribution : Linux Ubuntu 18.04
- Platform: X86_64
- TensorFlow installed from (source or binary): from source
- TensorFlow version: 1.15 and 2.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Using Bazel in conda environment to generate the binary
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 7.5.0 and 8.4.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I am unable to compile Tensorflow 2.1 with c++17.

The configuration was the default values displayed. 

```
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: 
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.
```

Steps:

```
% git clone -b r2.1 https://github.com/tensorflow/tensorflow.git
[configure - as above]
```

Fix undefine `ABSL_MUST_USE_RESULT` error.
`tensorflow/compiler/xla/service/slow_operation_alarm.h`

```diff
-ABSL_MUST_USE_RESULT std::unique_ptr<SlowOperationAlarm> SlowCompilationAlarm();
+//ABSL_MUST_USE_RESULT 
+std::unique_ptr<SlowOperationAlarm> SlowCompilationAlarm();
```

Fix memcpy error:
`/tensorflow/core/lib/gif/gif_io.cc`

```diff
+#include <cstring>
```

```
% cd tensorflow
% export BAZEL_CXXOPTS=-std=c++17
% bazel build -s --verbose_failures -c dbg --cxxopt=-std=c++17 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 //tensorflow/tools/pip_package:build_pip_package
 ```

Error:
```
ERROR: /wrk/xsjhdnobkup1/vincentm/Projects/ML/tensorflow-2.1/tensorflow/tensorflow/lite/toco/BUILD:136:1: C++ compilation of rule '//tensorflow/lite/toco:model_cmdline_flags' failed (Exit 1)
tensorflow/lite/toco/args.cc: In member function 'bool toco::Arg<toco::IntList>::Parse(std::__cxx11::string)':
tensorflow/lite/toco/args.cc:121:12: error: 'SimpleAtoi' was not declared in this scope
       if (!SimpleAtoi(part, &element)) return false;
            ^~~~~~~~~~
tensorflow/lite/toco/args.cc:121:12: note: suggested alternative:
In file included from ./tensorflow/lite/toco/args.h:25:0,
                 from tensorflow/lite/toco/args.cc:16:
external/com_google_absl/absl/strings/numbers.h:183:27: note:   'absl::SimpleAtoi'
 ABSL_MUST_USE_RESULT bool SimpleAtoi(absl::string_view s, int_type* out) {
                           ^~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 986.681s, Critical Path: 199.60s
INFO: 13774 processes: 13774 local.
FAILED: Build did NOT complete successfully
```

I have a feeling it is the absl library that is suppose to augment the C++ standard library and extend C++11 with features in C++17.

Note that I experience the same issue with TF version 1.15.

Any help would be appreciated.

Thank you."
39616,No module named 'tensorflow.contrib',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: jupyter  notebooks
- TensorFlow installed from (source or binary): Github
- TensorFlow version: 2.1.0
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:Intel HD



**Describe the problem**

 11 sys.path.append(""C:/Users/xx/.conda/envs/tensorflow_env/Lib/site-packages/tensorflow/models"")
     12 import cv2
---> 13 from nets import vgg
     14 from preprocessing import vgg_preprocessing
     15 from mlxtend.preprocessing import shuffle_arrays_unison

~\.conda\envs\tensorflow_env\lib\site-packages\slim-0.1-py3.7.egg\nets\vgg.py in <module>
     43 
     44 import tensorflow as tf
---> 45 # from tensorflow.contrib import slim as contrib_slim
     46 
     47 slim = contrib_slim

ModuleNotFoundError: No module named 'tensorflow.contrib'



**Provide the exact sequence of commands / steps that you executed before running into the problem**

import os 
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from skimage.transform import resize
from tqdm import tqdm
from sklearn.model_selection import train_test_split
%matplotlib inline
sys.path.append(""C:/Users/Firoz/.conda/envs/tensorflow_env/Lib/site-packages/tensorflow/models"")
import cv2
from nets import vgg
from preprocessing import vgg_preprocessing
from mlxtend.preprocessing import shuffle_arrays_unison
checkpoints_dir = 'C:/Users/Firoz/checkpoints'
import warnings
warnings.filterwarnings(""ignore"", message=""numpy.dtype size changed"")



I have gone through these steps
1. https://github.com/tensorflow/tensorflow/issues/30794
2. https://github.com/tensorflow/tensorflow/issues/31350

 without downgrading my tensorflow,  so Assisstance ??"
39615,tf.keras.Model docs missing get_weights method and metrics property,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/Model

## Description of issue (what needs changing):

The docs is missing `get_weights`, `set_weights` method and `metrics` property.

`get_weights` method and `metrics` property are defined in the src. But not in the generated docs.

https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/training.py#L190-L197

https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/training.py#L361-L410

`set_weights` method is mentioned in [keras docs](https://keras.io/api/models/model_saving_apis/#setweights-method).
"
39614,Cannot convert model into tflite file format,"System Information :
- Tensorflow Object detection api : using pipeline.config file for ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03. 
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from source
- TensorFlow version : 1.14.0
- Python version: 3
- CUDA/cuDNN version: CUDA Version 10.0.130
- GPU model and memory: Tesla M60 

Problem :
After making use of 
python3 model_main.py --alsologtostderr --model_dir=training/ --pipeline_config_path=training/pipeline.config

I trained the model for 94k steps.
The model was converted to frozen graph format using export_tflite_ssd_graph.py. 
The model was trained using quantization aware training by the following lines in the pipeline.config:

graph_rewriter 
{ 
 quantization 
 { 
  delay: 48000 
  weight_bits: 8 
  activation_bits: 8
  }
}

Issue : 
Using tflite convert i made use of the frozen graph from the previous step to convert the model into QUANTIZED_UINT8 format. I use the following command :

tflite_convert   --output_file='../test3.tflite' --graph_def_file='.../tflite_graph.pb' --inference_type=QUANTIZED_UINT8
 --input_arrays='normalized_input_image_tensor' 
--output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' 
--mean_values=128 
--std_dev_values=128 
--input_shapes=1,300,300,3
 --change_concat_input_ranges=false 
--allow_nudging_weights_to_use_fast_gemm_kernel=true 
--allow_custom_ops

The model asks for default max & min ranges.

After giving the ranges the following error pops up : 
F ./tensorflow/lite/toco/toco_tooling.h:38] Check failed: s.ok() Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).
Fatal Python error: Aborted

Any help would be quite beneficial.

"
39613,Is it possible to have a keras API for PhasedLSTM similar to CuDNNLSTM?,Is it possible to have a keras API for PhasedLSTM similar to CuDNNLSTM?
39612,native crash when using GpuDelegate on Float16 nn from java android.,"when configuring:
```kotlin
Interpreter.Options().apply {
    val gpuDelegate = GpuDelegate()
    addDelegate(gpuDelegate)
}
```
the code crashes when the interpreter is built, even before running it, with a seg-fault from dereferencing a 0 pointer. as usual with seg-faults, no other information is given, like line-number or message.
  this error only happens when the nn is of type Float16, but not with Float32 (which, of coarse, look the exact same from the java side)
but adding eiter `.setAllowFp16PrecisionForFp32(true)` or `.setAllowFp16PrecisionForFp32(false)`
removes the crash.

i think the reason of the crash is that `Interperter.Options.allowFp16PrecisionForFp32` is declared `Boolean`, thus allowing `null` as a value, which crashes in native without a clear message.

as i see it, there are 3 solutions:
* declaring it `boolean` and initializing with `true` or `false`.
* checking on the java side if the value is null, in case the configuration needs it, and throwing an appropriate error with a clear message. ( i prefer this one)
* checkint / trying in native
"
39611,No gradients provided for any variable,"I'm currently coding a text generation program to compare GRU, LSTM, and Bi-LSTM.

Right now, I'm facing a problem with the gradients problem. It said no gradients provided for any variable.

My best guess is in the model or in the fitting, but no ideas. The other is the dataset.

[I'm using Google CoLab for this program](https://colab.research.google.com/drive/1G4eilixtLJdW0c5TkgiSzunsqK2VUXTT?usp=sharing). I share it in viewer mode (unless you want me to change it)."
39610,Custom Keras model and layers demonstrate different behavior (nonzero vs zero grads) on CPU vs GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): Tested against 2.1.0 and 2.2.0 GPU vs CPU, as well as TF-nightly 2.3.0.dev20200516 on GPU
- Python version: 3.6.9
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5
- GPU model and memory: GeForce GTX 1070 Ti


**Current, expected behavior, best repro I can get**
Hi TF core,

Writing some custom layers and models for an at-home research project, primarily using Keras, but also some lower-level TF constructs to programatically construct a few matrices. I am in an interesting position in this project currently, where I can flip between my sanity-check tests passing and failing by *changing the TF version backing my Python interpreter*, specifically by using a TF-GPU version versus a pure TF-CPU version. To be clear, all the gradients in my custom model are 0 (0, not `None`) when using TF-GPU and nonzero when using TF-CPU.

I can inline the tests whose pass and failure can be flipped in this way, but pulling an entire standalone example might be a nontrivial task due to the layering The failing test is as follows:

```python
  def test_grads_non_zero(self):
    model = classification_models.TwoLayerLinalgConv(1, 784)
    with tf.GradientTape() as g:
      result = model(MNIST_DATA)
      loss = tf.keras.losses.sparse_categorical_crossentropy(MNIST_CLASSES, result, from_logits=True)
    grads = g.gradient(loss, model.trainable_weights)
    for idx, grad_elem in enumerate(grads):
      self.assertFalse(grad_elem is None)
      self.assertNotEqual(self.evaluate(tf.reduce_sum(tf.math.abs(grad_elem))), 0.)
```

The model which is used here is a simple composition (via subclassing of `tf.keras.Model`), as follows:

```python
class TwoLayerLinalgConv(tf.keras.Model):
  """"""Simple 2-dimensional convolution-like model via custom layer.""""""

  def __init__(
      self,
      n_filters,
      data_size,
      # ...other things
      ):
    super(TwoLayerLinalgConv, self).__init__()
    # Some initializations
    # etc
    self.conv1 = layers.LaplacianLayer(n_filters=self._n_filters,
                                       n_input_channels=self._n_input_channels,
                                       n_params_to_fit=self._kernel_size,
                                       n_dimensions=self._n_dimensions)
        # Must have 1 filter here for the dense layer in the end.
    self.conv2 = layers.LaplacianLayer(n_filters=1,
                                       n_input_channels=n_filters,
                                       n_params_to_fit=self._kernel_size,
                                       n_dimensions=self._n_dimensions)
    self.dense = tf.keras.layers.Dense(output_dim, activation=None)

  def call(self, inputs):
    x = tf.nn.relu(self.conv1(tf.reshape(inputs, [-1, self._n_input_channels, self._data_size])))
    x = tf.nn.relu(self.conv2(x))
    return self.dense(tf.reshape(x, [-1, self._data_size]))
```

It is probably worth noting that the layers used in this model, when tested as follows, actually *pass* with CPU and GPU-backed environments:

```python
  def test_operator_trains(self):
    l = layers.LaplacianLayer(1, 1, 1)
    l.build([10, 1, MNIST_SIZE])
    with tf.GradientTape() as g:
      input_with_channel = tf.reshape(MNIST_DATA, [-1, 1, MNIST_SIZE])
      result_of_layer = l(input_with_channel)
      pred = tf.keras.layers.Dense(10)(result_of_layer)
      result = tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(
          MNIST_LABELS, tf.reshape(pred, shape=[10, 10]), from_logits=True))

    grad = g.gradient(result, l.trainable_weights)
    for grad_elem in grad:
      self.assertFalse(grad_elem is None)
      self.assertNotEqual(self.evaluate(tf.reduce_sum(tf.math.abs(grad_elem))), 0.)
```

I am wondering: is this kind of divergence a known issue? Is there a simple way to mitigate this problem? Is there any more information that would be helpful, or pointers on debugging? I am pretty much at a loss on where to go from here...

Thanks!
Keith"
39608,Equation not being rendered in docs page,"## URL(s) with the issue:

https://www.tensorflow.org/agents/tutorials/0_intro_rl

## Description of issue (what needs changing):

The equation after ""The optimal Q-function obeys the following Bellman optimality equation:"" is not rendering correctly. This is what I see on my screen:

```
$\begin{equation} Q^(s, a) = \mathbb{E}\left[ r + \gamma \max_{a'} Q^(s', a')\right] \end{equation}$
```
"
39607,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows 10 64 bit
- TensorFlow installed from (source or binary): I used conda 
- TensorFlow version: 1.13.1
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): Spyder (?) (is this meant?)
- CUDA/cuDNN version: 7.6.5  (cuda 10.0)
- GPU model and memory: Intel(R) UHD Graphics 620,  4164MB
Intel(R) Core(TM) I5-8650U CPU @ 1.70GHz 1.90GHz  ,  8.00 GB RAM



**Describe the problem**
When i try to import tensorflow in spyder:

import tensorflow as tf

I get the following error:
import tensorflow as tf
Traceback (most recent call last):

  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\rensj\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\rensj\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""<ipython-input-2-64156d691fe5>"", line 1, in <module>
    import tensorflow as tf

  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\rensj\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\rensj\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\rensj\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

.
---
I found some similar topics, and i updated my visual studios and drivers now I think. Still the same error.  Please help 
"
39606,tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol Zeros is already exposed as (),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.15
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I tried to install object_detection model and followed these instructions (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/installation.md). 
But the line ""import tensorflow as tf"" raised this:
```
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 83, in <module>
    from tensorflow.python import keras
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/__init__.py"", line 39, in <module>
    from tensorflow.python.keras import ops
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/keras/ops.py"", line 30, in <module>
    init_ops.Zeros)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/tf_export.py"", line 310, in __call__
    self.set_attr(undecorated_func, api_names_attr, self._names)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/util/tf_export.py"", line 322, in set_attr
    (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access
tensorflow.python.util.tf_export.SymbolAlreadyExposedError: Symbol Zeros is already exposed as ().
```
I am new to all of this and I have no idea how to deal with it."
39605,Memory Usage of a model,I'd like to have a method which returns the needed memory for a model since the allocated VRAM does not reflect the actual usage.
39604,Getting error saved model to tflite - Float 64 error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.2.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
converter = tf.lite.TFLiteConverter.from_saved_model(model_file_pb)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

```
**The output from the converter invocation**
ERROR
``` 
# Copy and paste the output here.

To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-05-16 20:04:12.645740: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f98f6906430 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-16 20:04:12.645752: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-16 20:04:12.885160: I tensorflow/cc/saved_model/loader.cc:234] Restoring SavedModel bundle.
2020-05-16 20:04:13.539394: I tensorflow/cc/saved_model/loader.cc:183] Running initialization op on SavedModel bundle at path: MODEL/PB/64/tf_i11_v210/
2020-05-16 20:04:13.765676: I tensorflow/cc/saved_model/loader.cc:364] SavedModel load for tags { serve }; Status: success: OK. Took 1183597 microseconds.
error: type of return operand 2 ('tensor<?x400xf32>') doesn't match function result type ('tensor<?x400xf64>')
Traceback (most recent call last):
  File ""/Users/visood/opt/anaconda3/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/Users/visood/opt/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/Users/visood/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/Users/visood/opt/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/Users/visood/opt/anaconda3/lib/python3.7/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/Users/visood/opt/anaconda3/lib/python3.7/site-packages/tensorflow/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: <unknown>:0: error: type of return operand 2 ('tensor<?x400xf32>') doesn't match function result type ('tensor<?x400xf64>')
<unknown>:0: note: see current operation: ""std.return""(%12, %11, %13, %14, %15) : (tensor<?x400xf64>, tensor<?x1x400xf64>, tensor<?x400xf32>, tensor<?x400xf32>, tensor<f32>) -> ()
```
**Also, please include a link to the saved model or GraphDef**
https://github.com/vissood/resources/tree/master/TFLITE_ISSUE
```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39603,Tensorflow profiler returning the wrong total_float_ops numbers for Keras Model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6
- Bazel version (if compiling from source):NA
- GCC/Compiler version (if compiling from source):NA
- CUDA/cuDNN version:NA
- GPU model and memory:NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I'm trying to calculate FLOPs, and MAC's for the Keras models. I'm using a Tensorflow profiler from tensorflow.compat.v1. I API. I've attached the code snippet to calculate the FLOPs by using the saved Keras model. The profiler returning an incremental total_float_ops values for the same model whenever I execute.
**Describe the expected behavior**
Since no changes in the saved model, the developer expects the same total_float_ops numbers. 
**Standalone code to reproduce the issue**
```python
import tensorflow.compat.v1 as tf
def get_flops(model_h5_path):
    session = tf.compat.v1.Session()
    graph = tf.compat.v1.get_default_graph()
        

    with graph.as_default():
        with session.as_default():
            model = tf.keras.models.load_model(model_h5_path)

            run_meta = tf.compat.v1.RunMetadata()
            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()
        
            # We use the Keras session graph in the call to the profiler.
            flops = tf.compat.v1.profiler.profile(graph=graph,
                                                  run_meta=run_meta, cmd='op', options=opts)
            print(flops)
            return flops.total_float_ops

FLOPs = get_flops(""model.h5"")
```
```bash
For Example:
First Time:
name: ""_TFProfRoot""
total_float_ops: 851781854

Second Time:
name: ""_TFProfRoot""
total_float_ops: 929216568

Third Time:
name: ""_TFProfRoot""
total_float_ops: 1006651282
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
39602,model.evaluate generates different accuracy and loss values on the same network and test dataset ,"System Info:
Windows 10 
Tensorflow 2.1.0
Im using GPU Nvidia v100-sxm2 on a cluster. 

I have trained a CNN model using Tensorflow 2 for gesture classification. The training is done and the results are good. my problem is for my task (Im trying to use Reinforcement Learning to prune the network) I need to evaluate the network many times. I have not changed the testing dataset. and didnt change anything about the network between evaluating. I have tried to freeze the network `model.trainable = False `  I thought it might have an influece but didn't help. 

I know it's not an issue but I already asked in [stackoverflow](https://stackoverflow.com/questions/61822122/running-model-evaluate-many-times-results-different-accuracy-and-loss-value-tens) but didn't get any useful answer and it effects my job alot. 

my code:

making dataset:

```
def split_dataset(dataset: tf.data.Dataset, validation_data_fraction: float):

    validation_data_percent = round(validation_data_fraction * 100)
    if not (0 <= validation_data_percent <= 100):
        raise ValueError(""validation data fraction must be ∈ [0,1]"")

    dataset = dataset.enumerate()
    train_dataset = dataset.filter(lambda f, data: f % 100 >= validation_data_percent)
    validation_dataset = dataset.filter(lambda f, data: f % 100 < validation_data_percent)

    # remove enumeration
    train_dataset = train_dataset.map(lambda f, data: data)
    validation_dataset = validation_dataset.map(lambda f, data: data)

    return train_dataset, validation_dataset

def load_data(path):
    data, label = data_prep(path)
    dataset = tf.data.Dataset.from_tensor_slices((data, label))
    dataset = dataset.shuffle(100000)
    train_dataset, rest = split_dataset(dataset, 0.3)
    test_dataset, valid_dataset = split_dataset(rest, 0.5)
    train_data = train_dataset.shuffle(1000).batch(10)
    valid_data = valid_dataset.batch(10)
    test_data = test_dataset.batch(10)
    return train_data, valid_data, test_data
```

my model:

```
def make_model():
    model = tf.keras.Sequential([
    Input((1,30,30)),
    Conv2D(filters = 8, kernel_size=(3,3), padding=""same"", activation=""relu"", name=""c1"", data_format=""channels_first""),
    Conv2D(filters = 16, kernel_size=(3,3), padding=""same"", activation=""relu"", name=""c2"", data_format=""channels_first""),
    MaxPool2D(pool_size=(2,2), strides=(1,1),padding=""same"", name=""m1"", data_format=""channels_first""),
    
    Conv2D(filters = 16, kernel_size=(3,3), padding=""same"", activation=""relu"", name=""c3"", data_format=""channels_first""),
    MaxPool2D(pool_size=(2,2), strides=(1,1),padding=""same"", name=""m2"",data_format=""channels_first""),
    
    Flatten(),
    Dense(256, activation=""relu"", use_bias=True),
    Dense(5,  use_bias=True)])
    return model


model = make_model()
print(model.summary())
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[""accuracy""])
model.fit(train_data, verbose=1, validation_data=valid_data, epochs=20)
```

and evaluating:

```
model.evaluate(test_data)

```
"
39601,REDUCE_MAX operator support for int8 tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.4
- TensorFlow installed from (source or binary): tf-nightly (2.2.0-dev20200405)

**Any other info / logs**

I am using tf-nightly that integrates the new converter and trying to export a fully quantized int8 model using post-training quantization with calibration. 

The output model holds int8 operators including int8 input and output nodes. However, there is a specific node **REDUCE_MAX** that seems not to have support in tflite, therefore, there are quantization and dequantization operations around it, ending-up having a ""hybrid"" model with reference fp32 implementations as is shown below.


<img width=""441"" alt=""Screenshot 2020-05-16 at 14 34 26"" src=""https://user-images.githubusercontent.com/3832904/82121078-5c66b900-9782-11ea-83a5-8f3d33dfc66a.png"">

Are there any plans for this OP to be supported in tflite and in TFLu in the future? Is it possible that this operator is already supported but the properties of the operator won't allow full quantization? 

Thanks!"
39600,overlapping bounding boxes,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Linux Ubuntu 18.04

- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.0
- Python version: 3.7.4 

- CUDA/cuDNN version: 10.2 
- GPU model and memory: GeForce GTX 1050



**Describe the current behavior**
i trained an object detection model using resnet101 faster rcnn, i keep getting these overlapping bounding boxes ( i know that i should apply IoU) but couldn't find a way to do it
![Screenshot from 2020-05-16 13-43-54](https://user-images.githubusercontent.com/62834628/82120053-54efe180-977b-11ea-903d-5e210a28d286.png)

here's the python code that i'm running : 
```
######## Image Object Detection Using Tensorflow-trained Classifier #########
#
# Author: Evan Juras
# Date: 1/15/18
# Description:
# This program uses a TensorFlow-trained neural network to perform object detection.
# It loads the classifier and uses it to perform object detection on an image.
# It draws boxes, scores, and labels around the objects of interest in the image.

## Some of the code is copied from Google's example at
## https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb

## and some is copied from Dat Tran's example at
## https://github.com/datitran/object_detector_app/blob/master/object_detection_app.py

## but I changed it to make it more understandable to me.

# Import packages
import os
import cv2
import numpy as np
import tensorflow as tf
import sys

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("".."")

# Import utilites
from utils import label_map_util
from utils import visualization_utils as vis_util

# Name of the directory containing the object detection module we're using
MODEL_NAME = 'trio'
IMAGE_NAME = '2.jpg'

# Grab path to current working directory
CWD_PATH = os.getcwd()

# Path to frozen detection graph .pb file, which contains the model that is used
# for object detection.
PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')

# Path to label map file
PATH_TO_LABELS = os.path.join(CWD_PATH,'training','tfobject.pbtxt')

# Path to image
PATH_TO_IMAGE = os.path.join(CWD_PATH,'test_images',IMAGE_NAME)

# Number of classes the object detector can identify
NUM_CLASSES = 3

# Load the label map.
# Label maps map indices to category names, so that when our convolution
# network predicts `5`, we know that this corresponds to `king`.
# Here we use internal utility functions, but anything that returns a
# dictionary mapping integers to appropriate string labels would be fine
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

# Load the Tensorflow model into memory.
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.compat.v1.GraphDef()
    with tf.compat.v2.io.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)

# Define input and output tensors (i.e. data) for the object detection classifier

# Input tensor is the image

image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

# Output tensors are the detection boxes, scores, and classes
# Each box represents a part of the image where a particular object was detected
detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

# Each score represents level of confidence for each of the objects.
# The score is shown on the result image, together with the class label.
detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

# Number of objects detected
num_detections = detection_graph.get_tensor_by_name('num_detections:0')

# Load image using OpenCV and
# expand image dimensions to have shape: [1, None, None, 3]
# i.e. a single-column array, where each item in the column has the pixel RGB value
image = cv2.imread(PATH_TO_IMAGE)
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
image_expanded = np.expand_dims(image_rgb, axis=0)

# Perform the actual detection by running the model with the image as input
(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})

# Draw the results of the detection (aka 'visulaize the results')

vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=8,
    min_score_thresh=0.8)

# All the results have been drawn on image. Now display the image.
cv2.imshow('Object detector', image)

# Press any key to close the image
cv2.waitKey(0)

# Clean up
cv2.destroyAllWindows()
```


"
39599,what-if-tool link broken in tensorflow.org Tools page,"#38076  URL(s) with the issue:
https://www.tensorflow.org/resources/tools#

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/resources/tools#

## Description of issue (what needs changing):
Link to what-if-tool is broken, as what-if moved to new repo.

### Clear description
Current broken link, when someone clicks on ""Get Started"" link at 
https://www.tensorflow.org/resources/tools

Broken link ->
https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/interactive_inference/What_If_Tool_Notebook_Usage.ipynb

New Correct link should be ->
https://github.com/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb

For example, why should someone use this method? How is it useful?

### Correct links
https://github.com/PAIR-code/what-if-tool/blob/master/What_If_Tool_Notebook_Usage.ipynb

I wanted to submit a pull-request for this
"
39598,Autograph AssertionError on custom layer call,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

The errors says, ""Please report this to the TensorFlow team. When filing the bug...""

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, custom layer
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Anaconda environment with pip install
- TensorFlow version (use command below):
v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version:
Python 3.6.10 |Anaconda, Inc.| (default, Mar 23 2020, 17:58:33) [MSC v.1916 64 bit (AMD64)]
- Bazel version (if compiling from source):
N/a
- GCC/Compiler version (if compiling from source):
N/a
- CUDA/cuDNN version:
10.1
- GPU model and memory:
GeForce RTX 2080 Ti 32Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)

Run from Anaconda

== check python ===================================================

== check os platform ===============================================

== are we in docker =============================================
No

== compiler =====================================================
./tf_env_collect.sh: line 102: c++: command not found

== check pips ===================================================

== check for virtualenv =========================================
False


== tensorflow import ============================================
Traceback (most recent call last):

  File ""<string>"", line 1, in <module>

ModuleNotFoundError: No module named 'tensorflow'


== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 147: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 8, 2, 'final', 0)


== bazel version  ===============================================

== check python ===================================================
python version: 3.6.10

python branch: 

python build version: ('default', 'Mar 23 2020 17:58:33')

python compiler version: MSC v.1916 64 bit (AMD64)

python implementation: CPython




== check os platform ===============================================
os: Windows

os kernel version: 10.0.18362

os release version: 10

os platform: Windows-10-10.0.18362-SP0

linux distribution: ('', '', '')

linux os distribution: ('', '', '')

mac version: ('', ('', '', ''), '')

uname: uname_result(system='Windows', node='DESKTOP-5O07H5P', release='10', version='10.0.18362', machine='AMD64', processor='Intel64 Family 6 Model 158 Stepping 13, GenuineIntel')

architecture: ('64bit', 'WindowsPE')

machine: AMD64




== are we in docker =============================================
No

== compiler =====================================================
bash: c++: command not found

== check pips ===================================================
numpy                         1.18.1                                        

numpydoc                      0.9.2                                         

protobuf                      3.11.4                                        

tensorflow-docs               0.0.084618e76cde6edf23c2e71fd1d126c3d18ba53d5-

tensorflow-gpu                2.1.0                                         

tensorflow-gpu-estimator      2.1.0                                         


== check for virtualenv =========================================
False


== tensorflow import ============================================
tf.version.VERSION = 2.1.0

tf.version.GIT_VERSION = v2.1.0-rc2-17-ge5bf8de410

tf.version.COMPILER_VERSION = MSVC 192428314

2020-05-16 10:55:03.809733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
bash: nvidia-smi: command not found

== cuda libs  ===================================================

== tensorflow installed from info ==================

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 6, 10, 'final', 0)


== bazel version  ===============================================


You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When building a model containing my custom layer I get this message: I don't get it on the first time the code is run.

Python 3.6.10 |Anaconda, Inc.| (default, Mar 23 2020, 17:58:33) [MSC v.1916 64 bit (AMD64)]
TensorFlow version: 2.1.0
v2.1.0-rc2-17-ge5bf8de410 2.1.0
Keras version: 2.2.4-tf
tf.Tensor([[150. 150. 150. 150. 150.]], shape=(1, 5), dtype=float32)
INFO:tensorflow:Converted call: <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>>
    args: (<tf.Tensor 'Placeholder:0' shape=(1, 5) dtype=float32>,)
    kwargs: {}

INFO:tensorflow:Not whitelisted: <method-wrapper '__call__' of method object at 0x00000299AA71E088>: default rule
INFO:tensorflow:Not whitelisted: <class 'layers.Linear'>: default rule
INFO:tensorflow:Not whitelisted: <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>>: default rule
INFO:tensorflow:Cache hit for entity <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>> key <code object call at 0x00000299CF180780, file ""C:\Users\ruper\Versioning\PCTSoftware\Libraries\python\tensorflow\pct\pctdl\layers.py"", line 24> subkey (<tensorflow.python.autograph.core.converter.ConversionOptions object at 0x00000299CF196F28>, frozenset()): _ConvertedEntityFactoryInfo(tf__call in tmph641gc44)
INFO:tensorflow:Error transforming entity <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>>
Traceback (most recent call last):
  File ""C:\Users\ruper\.conda\envs\pct-tf\lib\site-packages\tensorflow_core\python\autograph\impl\api.py"", line 526, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""C:\Users\ruper\.conda\envs\pct-tf\lib\site-packages\tensorflow_core\python\autograph\impl\conversion.py"", line 328, in convert
    return _instantiate(entity, converted_entity_info, free_nonglobal_var_names)
  File ""C:\Users\ruper\.conda\envs\pct-tf\lib\site-packages\tensorflow_core\python\autograph\impl\conversion.py"", line 266, in _instantiate
    factory = converted_entity_info.get_factory()
  File ""C:\Users\ruper\.conda\envs\pct-tf\lib\site-packages\tensorflow_core\python\autograph\impl\conversion.py"", line 92, in get_factory
    assert self.module_name in sys.modules
AssertionError
WARNING:tensorflow:AutoGraph could not transform <bound method Linear.call of <layers.Linear object at 0x00000299C8EE0BE0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 



**Describe the expected behavior**

I don't want the error/warning message (not sure what it means).

**Standalone code to reproduce the issue**

[test-layer.txt](https://github.com/tensorflow/tensorflow/files/4638168/test-layer.txt)
[layers.txt](https://github.com/tensorflow/tensorflow/files/4638169/layers.txt)

I have added the test code. Change to .py extension.

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39596,tf.keras.layers.Lambda can't infer output dimension of tf.slice,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

tf.keras.layers.Lambda can't infer output dimension of tf.slice even if the input dimension is partially known.

**Describe the expected behavior**

the output dimension should be partially known as input dimension.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3
import tensorflow as tf;
inputs=tf.keras.Input((None,32));
results=tf.keras.layers.Lambda(lambda x: tf.slice(x, (0,0,0),(-1,tf.shape(x)[1] - 1,-1)))(inputs);
print(results.shape); # the last dimension should be 32
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39595,Pixel range issue with `image_dataset_from_directory` after applying `convert_image_dtype`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0-dev20200514
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

After creating a dataset with `image_dataset_from_directory` I am mapping it to `tf.image.convert_image_dtype` for scaling the pixel values to the range of [0, 1] and also to convert them to `tf.float32` data-type. Looks like the value range is not getting changed. 

Here's the self-contained code:

```python
import matplotlib.pyplot as plt
import tensorflow as tf

# Get the flowers dataset
flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)

def scale(image, label):
    return tf.image.convert_image_dtype(image, tf.float32), label

batch = tf.keras.preprocessing.image_dataset_from_directory(flowers)
batch = batch.map(scale)

image_batch, label_batch = next(iter(batch))

print(tf.reduce_max(image_batch[1,:])) # outputs: tf.Tensor(248.96571, shape=(), dtype=float32)
print(image_batch.dtype) # outputs: <dtype: 'float32'>
```

**Describe the expected behavior**

If my understanding is correct, then `batch = batch.map(scale)` should already take care of the scaling step. 

**Standalone code to reproduce the issue**

[Colab Gist](https://colab.research.google.com/gist/sayakpaul/831f2a584f92cdcc97f81bf0eb344ad1/scratchpad.ipynb)
"
39594,Simple Quantization aware training (TF Tutorial) throws a warning,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): colab
- TensorFlow installed from (source or binary): 2.2
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/github/tensorflow/model-optimization/blob/master/tensorflow_model_optimization/g3doc/guide/quantization/training_example.ipynb



**The output from the converter invocation**

```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:465: set_learning_phase (from tensorflow.python.keras.backend) is deprecated and will be removed after 2020-10-11.
Instructions for updating:
Simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:105: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: /tmp/tmpjvd7obgu/assets
```

**Failure details**
There is no failure as such. But there is a warning which is not clear to any common user. It would be helpful if the warning guides the user to root-cause of the issue so that user will update `training` argument.



**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39593,How does delayed restoration of variables in tensorflow checkpoint restore work?,"I am trying to follow the checkpointing from tensorflow's checkpoint [guide][1]. I am failing to understand the exact semantics of how the variable restoration takes place. The issue that i am facing is best shown by the code bit below:
```
import tensorflow as tf
import numpy as np

#defining a simple subclass of keras model. Just one dense layer is present we will try to restore the kernel and bias of this layer using the `tf.train.Checkpoint`

class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)
  def call(self, x):
    return self.l1(x)

net = Net()

#initializing the net by running it on a demo input
_ = net(np.arange(4).reshape(-1,1)) #model has been initalized

#checking the weights
print(net.l1.bias,net.l1.kernel)

#saving the model parameters
ckpt = tf.train.Checkpoint(netin=net)
mgr = tf.train.CheckpointManager(ckpt,'./tf_ckpt',max_to_keep=1)
mgr.save()

#trying to load just the bias of the net's parameters
initializer = tf.keras.initializers.Constant(value=1.)
restore_bias_here = tf.Variable(initial_value=initializer(shape=(5,)))
ckpt_layer = tf.train.Checkpoint(bias = restore_bias_here)
ckpt_net = tf.train.Checkpoint(l1 = ckpt_layer)
ckpt1 = tf.train.Checkpoint(netin=ckpt_net)
#queing up the restores
ckpt1.restore(tf.train.latest_checkpoint('./tf_ckpt'))
```
Now, here is the problem. I can restore the kernel in a delayed fashion both from `ckpt_net` and from `ckpt_layer` but it seems i can do it only once. And the behavior shifts silently. Code to show the problem:

```
#and now trying to load just the kernel but from 1. ckpt_net created previously
delayed_restore_kernel_here = tf.Variable(initial_value=initializer(shape=(1,5)))
print(delayed_restore_kernel_here) #all 1's tensor
ckpt_layer.kernel = delayed_restore_kernel_here
print(delayed_restore_kernel_here) #the variable is loaded up perfectly

#2. from #ckpt_layer created previously. But this wont succeed
delayed_restore_kernel_here = tf.Variable(initial_value=initializer(shape=(1,5)))
print(delayed_restore_kernel_here) #all 1's tensor
ckpt_net.l1.kernel = delayed_restore_kernel_here
print(delayed_restore_kernel_here) #all 1's tensor
```
I can run the last two steps in any order and only the first one will run. Can somebody explain to me the exact semantics of how the delayed restoration of variables in tensorflow works under the hood? Many thanks in advance


  [1]: https://www.tensorflow.org/guide/checkpoint"
39592,tensorflow.keras deadlock when using model's predict() in parallel,"
**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 20.04
- Mobile device if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.10
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

In the toy code below, I am trying to call `predict()` from a `tf.keras` model running in parallel. However, it is getting stuck because of a deadlock.

**Describe the expected behavior**

All processes should have returned the result of `predict()` without getting stuck.

**Standalone code to reproduce the issue**

 You can find a minimal reproducible code in the gist below:
https://gist.github.com/leandrocouto/a11534688dc568ea23f20171a5dc643f


**Other info / logs**

There are two functions (`make_keras_picklable()` and `_unpack()`) that are a hotfix in order to make a tf.keras pickable. Apparently, when running code in parallel, the objects you pass as parameters needs to be pickled. I am using  `concurrent.futures` for the parallel calls. 
"
39591,installed but cannot run,"windows 10



**Describe the problem**

installed using pip3 20.1
python 3.7

import tensorflow


Traceback (most recent call last):
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Shiyi\OneDrive\python\tensorflow\tshirt.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Shiyi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\LocalCache\local-packages\Python37\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.7_3.7.2032.0_x64__qbz5n2kfra8p0\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
39590,undeclared identifier 'ABSL_FALLTHROUGH_INTENDED',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OSX 10.15.3
- source
- 2.2.0
- Python version:3.8.2
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): bazelisk --version -> bazel 2.0.0
- Clang10 and gcc10 + Apple gcc +Apple clang

> tf2.2/tensorflow/core/lib/io/BUILD:211:1: C++ compilation of rule '//tensorflow/core/lib/io:cache' failed (Exit 1)
> 
> tensorflow/core/lib/io/cache.cc:430:9: error: use of undeclared identifier 'ABSL_FALLTHROUGH_INTENDED'
>         ABSL_FALLTHROUGH_INTENDED;
>         ^
> tensorflow/core/lib/io/cache.cc:433:9: error: use of undeclared identifier 'ABSL_FALLTHROUGH_INTENDED'
>         ABSL_FALLTHROUGH_INTENDED;


bazelisk build --config=opt --config=macos --config=monolithic --config=v2 --config=mkl   //tensorflow/tools/pip_package:build_pip_package
"
39589,Why am I getting a no gradient provided for any variable error?,"I'm trying to do a style transfer program, and when I get to the optimization I get the no gradient provided for any variable.
[notebook](
https://github.com/RodrigoPina407/StyleTransfer/blob/master/StyleTransferV2.ipynb)

```
epochs=200

melhor_loss = 2000000000
melhor_imagem = None

min_value = MEAN
max_value = 255 + MEAN
loss = 0.0

for e in range(epochs):
  with tf.GradientTape() as tape:
    tape.watch(output_processado)
    output_feats = modelo(output_processado)

    loss = total_loss(content_feats, style_feats, output_feats)

    grad = tape.gradient(loss, output_processado)
    optimizer.apply_gradients(zip([grad],[output_processado]))

    clip = tf.clip_by_value(output, min_value, max_value)
    output_processado.assign(clip)
  if loss < melhor_loss:
    melhor_imagem = output_processado
    melhor_loss = loss
    print(""Epoch: "" + e + ""Loss diminui para "" + melhor_loss)
```


> ---------------------------------------------------------------------------
> ValueError                                Traceback (most recent call last)
> <ipython-input-236-4b717f5a772e> in <module>()
>      16 
>      17     grad = tape.gradient(loss, output_processado)
> ---> 18     optimizer.apply_gradients(zip([grad],[output_processado]))
>      19 
>      20     clip = tf.clip_by_value(output, min_value, max_value)
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
>     470       ValueError: If none of the variables have gradients.
>     471     """"""
> --> 472     grads_and_vars = _filter_grads(grads_and_vars)
>     473     var_list = [v for (_, v) in grads_and_vars]
>     474 
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)
>    1217   if not filtered:
>    1218     raise ValueError(""No gradients provided for any variable: %s."" %
> -> 1219                      ([v.name for _, v in grads_and_vars],))
>    1220   if vars_with_empty_grads:
>    1221     logging.warning(
> 
> ValueError: No gradients provided for any variable: ['Variable:0']
"
39588,tf.keras.model fit() significantly slower when using weighted validation data in comparison to tf2.1.0,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
ArchLinux & Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.2.0-rc4-8-g2b96f3662b 2.2.0
(compared to: v2.1.0-rc2-17-ge5bf8de 2.1.0)
- Python version:
3.7.5

The ArchLinux machine runs on CPU
The Ubuntu machine runs on GPU with:
- CUDA/cuDNN version:
10.1.243
- GPU model and memory:
GeForce GTX 1080 with 7126 MB memory

**Describe the current behavior**
When training a simple tf.keras.model multilayer perceptron with a call to .fit() containing a validation_data that contains weights results in a significant slower fit() then in comparison to TensorFlow 2.1.0 with the exact same code.

**Describe the expected behavior**
Similar performance between TensorFlow 2.1.0 and 2.2.0 when training a tf.keras.model with a weighted validation data set.

**Standalone code to reproduce the issue**
Package requirements for code snippet using python 3.7.5:
``` py
numpy= ""==1.18.2""
tensorflow = ""==2.2.0""
tensorflow-datasets = ""==3.1.0""
```
```py
import typing

import numpy as np
from tensorflow import keras
import tensorflow_datasets as tfds


def build_neural_network(input_dimension: int, number_of_classes: int, compile_options: dict):
    model = keras.Sequential()
    model.add(keras.layers.Dense(112, activation='relu', input_dim=input_dimension))
    model.add(keras.layers.Dense(112, activation='relu'))
    model.add(keras.layers.Dense(number_of_classes, activation='softmax'))

    model.compile(**compile_options)

    print(model.summary())

    return model

def load_in_images_and_labels_and_reshape(dataset) -> typing.Tuple[np.ndarray, np.ndarray]:
    images = []
    labels = []
    for image, label in tfds.as_numpy(dataset):
        new_image_shape = image.shape[0] * image.shape[1]
        images.append(image.reshape(new_image_shape))
        labels.append(label)

    return np.array(images), np.array(labels)


def train_neural_network(is_random_weighing: bool):
    dataset_train      = tfds.load('emnist', split='train', as_supervised=True)
    dataset_validation = tfds.load('emnist', split='test', as_supervised=True)

    train_images, train_labels           = load_in_images_and_labels_and_reshape(dataset_train)
    validation_images, validation_labels = load_in_images_and_labels_and_reshape(dataset_validation)
    train_labels      = keras.utils.to_categorical(train_labels)
    validation_labels = keras.utils.to_categorical(validation_labels)

    print(""load"")
    compile_options =  {
        ""loss"": ""categorical_crossentropy"",
        ""optimizer"": ""adam"",
        ""metrics"": [""categorical_accuracy""],
        ""weighted_metrics"": [""categorical_accuracy""]
    }
    network = build_neural_network(train_images.shape[-1], len(train_labels[0]), compile_options)

    fit_options = {    
        ""batch_size"": 2048,
        ""epochs"": 10,
        ""verbose"": 1,
        ""workers"": 1
    }
    if is_random_weighing:
        random_weights = np.random.rand(len(validation_images))
        validation_data_tuple = (validation_images, validation_labels, random_weights)
    else:
        validation_data_tuple = (validation_images, validation_labels)
    history = network.fit(train_images, train_labels, validation_data=validation_data_tuple, **fit_options)


if __name__ == ""__main__"":
    is_random_weighing = True
    train_neural_network(is_random_weighing)

```

**Other info / logs**
Running the above code snippet on the ArchLinux machine, run on CPU:
takes roughly 19 seconds per epoch. When the same code is run in TensorFlow 2.1.0 it takes roughly 5 seconds per epoch. When the weighing off the validation dataset is turned off with TensorFlow 2.2.0 (is_random_weighing = False) the performance becomes similar to TensorFlow 2.1.0; roughly 5 seconds per epoch.
The slowdown is also seen on the Ubuntu machine, run on GPU, but then due to likely different hardware, tf 2.2.0 is 7 times as slow as  tf 2.1.0.

The effect was not seen (but maybe it was not measurable) when using mnist in place of emnist.

The issue seems related to: #39039
In which the comment by @romanovzky brought to light that it might be due to the validation data or validation split. Although that is in the context of comparing a tensorflow estimator to keras.

This issue also seems related to: #39434
In which also from tf2.1 to tf.2.2 a  significant performance drop is seen.

It seems like another small puzzle piece in a larger puzzle (or I do something simple wrong on both machines).
"
39587,TS_SessionRun: 'Input to reshape' errors.  Requested shape is always tensor_size ^ 2,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 16.04, Ubuntu 18.04

- TensorFlow installed from (source or binary):
Binary, From pip. C-API downloaded from https://www.tensorflow.org/install/lang_c

- TensorFlow version (use command below):
('v2.1.0-rc2-17-ge5bf8de', '2.1.0')
C-API: Hello from TensorFlow C library version 1.15.0

- Python version:
Python 2.7.12

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
Loading a saved model with the C-API  fails to run the graph. All input placeholders fail in the 'reshape' operation. For example:
`TF_SessionRun status: 3:Input to reshape is a tensor with 3715 values, but the requested shape has 13801225`

`TF_SessionRun status: 3:Input to reshape is a tensor with 2 values, but the requested shape has 4`

`TF_SessionRun status: 3:Input to reshape is a tensor with 1422 values, but the requested shape has 2022084`
The number of values in the requested shape is always the 'expected number of values' ^ 2.
I suspect the following line of code plays a role in this: https://github.com/tensorflow/tensorflow/blob/77245d07d13522a5cb5d060390fffa1894df5bbf/tensorflow/core/kernels/reshape_op.h#L139

TF_LoadSessionFromSavedModel loads the model successfully. And it will correctly fail to run if I don't provide all inputs. I can see from stderr that it is calling the InitOp:
`2020-05-15 19:18:37.508528: I tensorflow/cc/saved_model/loader.cc:202] Restoring SavedModel bundle.`

`2020-05-15 19:18:37.529034: I tensorflow/cc/saved_model/loader.cc:151] Running initialization op on SavedModel bundle at path: /home/james/model/bq/1`

`2020-05-15 19:18:38.585963: I tensorflow/cc/saved_model/loader.cc:311] SavedModel load for tags { serve }; Status: success. Took 1097826 microseconds.`

More on the StackOverflow question: https://stackoverflow.com/questions/61787472/reshape-input-layer-requested-shape-size-always-input-shape-size-squared

`signature_def:`
` - name: 'serving_default'`
` - method_name: 'tensorflow/serving/predict'`

**Describe the expected behavior**
TF_LoadSessionFromSavedModel outputs a runnable graph, or if extra steps are required to run a saved_model with the C-API they should be documented.
Using the JNI code as an example, I cannot find extra steps there: https://github.com/tensorflow/tensorflow/blob/ff17316b19d5958605cebf941e4302d60f405784/tensorflow/java/src/main/native/session_jni.cc#L132
I cannot use the C++ shared lib because of the `No session factory registered for the given session options: {target: """" config: } Registered factories are {}.` issue that I cannot get around.

**Standalone code to reproduce the issue**
I don't think this is possible. The operations that fail are all populated by files in the 'assets' directory.
The values in the 'tensor with ??? values' are correct, the requested shape is not.
The code is pretty much just a standard C-API block, influenced by the JNI code above:

`session = TF_LoadSessionFromSavedModel(...);`
`for each input:`
`   inputs.push_back(TF_GraphOperationByName(...));`

`for each output:`
`   outputs.push_back(TF_GraphOperationByName(...));`

`TF_SessionRun(session, inputs, ..., outputs, ....);`


"
39586,tf.metrics.MeanIoU throws dimensions mismatch error when used with SparseCategoricalCrossEntropy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 20.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): TF.2.2.0
- Python version: 3.8
- Bazel version (if compiling from source): 2.0
- GCC/Compiler version (if compiling from source): gcc7
- CUDA/cuDNN version: 10.2
- GPU model and memory: Nvidia Titan Xp (12GB)


**Describe the current behavior**

`tf.metrics.meanIOU` throws a dimensions mismatch error when using a model trained using `SparseCategoricalCrossEntropy `for a semantic segmentation problem.
`My model outputs  dimensions = 224x224x2, y_true = 224x224x1 (class_indices).`

A workaround was suggested at https://github.com/tensorflow/tensorflow/issues/32875#issuecomment-629360707 and worked for TF2.1, but fails with a dimensions mismatch error for TF2.2.
 The suggested fix that doesn't work in TF.2.2:

```

# fix for miou calculation when using sparse_categorical _cross_entropy 
    # https://github.com/tensorflow/tensorflow/issues/32875

class UpdatedMeanIoU(tf.keras.metrics.MeanIoU):
    @tf.function
    def __call__(self, y_true, y_pred, sample_weight=None):
        self.y_pred = tf.math.argmax(self.y_pred, axis=-1)
        return super().__call__(y_true, self.y_pred, sample_weight=sample_weight)

```
Here is the error:

Traceback (most recent call last):

```

    ValueError: Dimension 0 in both shapes must be equal, but are 1048576 and 2097152. Shapes are [1048576] and [2097152].
    	From merging shape 0 with other shapes. for '{{node confusion_matrix/stack_1}} = Pack[N=2, T=DT_INT64, axis=1](confusion_matrix/control_dependency_2, confusion_matrix/control_dependency_3)' with input shapes: [1048576], [2097152].
```



Appreciate a fix."
39584,tf.keras.preprocessing.text_dataset_from_directory function,"AttributeError                            Traceback (most recent call last)
<ipython-input-15-9116635614ef> in <module>()
----> 1 train_ds = tf.keras.preprocessing.text_dataset_from_directory( x_train,
      2                                                               labels='inferred',
      3                                                               label_mode='int',
      4                                                               batch_size=batch_size,
      5                                                               validation_split=0.2,

AttributeError: module 'tensorflow.keras.preprocessing' has no attribute 'text_dataset_from_directory'"
39583,Training-Time updated custom Layer attributes are NOT saved,"
**System information**

- OS Platform and Distribution: Linux Ubuntu 19.10
- TensorFlow installed from: Docker Image latest-gpu-py3-jupyter
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: (major, minor, micro, release level, serial) (3, 6, 9, 'final', 0)
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 1050 Ti

**Describe the current behavior**
I am not able to save the updated attribute value `self.ratio` of the following custom `tf.keras.layer.Layer`
```
   class Kwta(Layer):

    def __init__(self, ratio, conv = False, data_format=""channels_last"", **kwargs):
        super(Kwta, self).__init__(**kwargs)
        self.conv = conv
        self.ratio = ratio
        self.data_format = data_format
        
    def build(self, input_shape):
        #Not important here, see Reproducible Example Section for the full code
        
    def call(self, inputs):
      #Not important here, see Reproducible Example Section for the full code
    
    def get_config(self):
        my_config = {
            'conv' : self.conv,
            'ratio' : self.ratio,
            'data_format' : self.data_format,
            'dim' : self.dim
        }

        base_config = super(Kwta, self).get_config()
        return dict(list(base_config.items()) + list(my_config.items()))
```
Which gets incrementally updated while training by the following custom `tf.keras.callbacks.Callback`

```
class incremental_learning_withDecreasing_ratio(tf.keras.callbacks.Callback):
    """""" Icrementally adjust the Kwta self.ratio attribute every 2 epochs.
         End the learning process when ratio == 0.3  """"""

    def __init__(self, delta = 0.05):
        super(incremental_learning_withDecreasing_ratio, self).__init__()
        self.delta = delta

    def on_epoch_begin(self, epoch, logs=None):
        # The update occurs at the beginning of every 2 epochs
        if epoch % 2 == 0:  
            for i in range(1, 5): # For each Kwta layer
                name = 'kwta_'+str(i)
                layer = self.model.get_layer(name = name)      
                layer.ratio -= self.delta
            
            print('\n Fine tuning: current ratio {} \n'.format(round(layer.ratio, 2)))
    
    def on_epoch_end(self, epoch, logs=None):
        layer = self.model.get_layer('kwta_1')
        if ( round(layer.ratio, 2) == 0.3 ) and epoch % 2 == 1: 
                print('\n Desired Ratio reached, stop training...')
                self.model.stop_training = True
```
This is the whole model on which I use the Kwta layer and the callback

```
kwta_cnn = tf.keras.models.Sequential([
  layers.Conv2D(32, 3, padding='same', activation=None, input_shape = (32, 32, 3)),
  Kwta(ratio=0.6, conv=True, name='kwta_1'),
  layers.Conv2D(32, 3, padding='same', activation=None),
  Kwta(ratio=0.6, conv=True, name='kwta_2'),
  layers.MaxPooling2D(pool_size=(2,2)),
  layers.Dropout(0.2, seed=42),

  layers.Conv2D(64, 3, padding='same', activation=None),
  Kwta(ratio=0.6, conv=True, name='kwta_3'),
  layers.Conv2D(64, 3, padding='same', activation=None),
  Kwta(ratio=0.6, conv=True, name='kwta_4'),
  layers.MaxPooling2D(pool_size=(2,2)),
  layers.Dropout(0.3, seed=42),

  layers.Flatten(),
  layers.Dense(10, activation='softmax', kernel_regularizer= tf.keras.regularizers.l2(0.0005))
])
```
When I fit the model `self.ratio` is correctly updated until its value is `0.3` as printed by the callback
```
history_kwta_ft = kwta_cnn.fit(x=x_train, y=y_train, epochs = 20, callbacks=[incremental_learning_withDecreasing_ratio()])
```
```
......
 
Fine tuning: current ratio 0.4 

Epoch 7/20
50000/50000 [==============================] - 40s 798us/sample - loss: 0.9049 - accuracy: 0.7080
Epoch 8/20
50000/50000 [==============================] - 39s 787us/sample - loss: 0.8921 - accuracy: 0.7122

 Fine tuning: current ratio 0.35 

Epoch 9/20
50000/50000 [==============================] - 39s 782us/sample - loss: 0.8862 - accuracy: 0.7141
Epoch 10/20
50000/50000 [==============================] - 39s 785us/sample - loss: 0.8819 - accuracy: 0.7147

 Fine tuning: current ratio 0.3 

Epoch 11/20
50000/50000 [==============================] - 40s 793us/sample - loss: 0.8812 - accuracy: 0.7174
Epoch 12/20
49920/50000 [============================>.] - ETA: 0s - loss: 0.8651 - accuracy: 0.7218
 Desired Ratio reached, stop training...
```
**Here is where the issue emerges**: I am getting two way different accuracies when I evaluate the trained model on the test set before and after saving it:
```
# Before Saving
kwta_cnn.evaluate(x = x_test, y = y_test, batch_size=128)
10000/10000 [==============================] - 5s 534us/sample - loss: 0.9503 - accuracy: 0.7017

kwta_cnn.save('saved_custom_model/kwta_cnn_minimal') 
del kwta_cnn
kwta_cnn = tf.keras.models.load_model('saved_custom_model/kwta_cnn_minimal')

# After Saving
kwta_cnn.evaluate(x = x_test, y = y_test, batch_size=128)
10000/10000 [==============================] - 3s 341us/sample - loss: 1.1248 - accuracy: 0.6349
```
After debugging for a while, I am strongly confident this drop in accuracy is due to `self.ratio` not being correctly saved by `model.save()`. In particular, I believe TF only store the original attribute (in this specific case `0.6`) and cannot deal with dynamic updated attributes that are neither `tf.Variable()` nor `self.add_weight()`. Unfortunatly the latter is not an option in my case since I seem to be able to handle only integers inside the `call(inputs)` method to perform my computation and not `tf.Tensor`s.

**Describe the expected behavior**
To be able to save and store a `tf.keras.layers.Layer` with training-time updated attributes.  

**Standalone code to reproduce the issue**
 [Colab Minimal Example](https://colab.research.google.com/drive/1LHvG8yNrX3C9SJHJ2OojZCryFSCw8nG0?usp=sharing)
"
39582,AttributeError: 'Tensor' object has no attribute 'numpy' in Tensorflow 2.1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary (pip install)
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: NA

**Describe the current behavior**
I am trying to generate a custom loss function but to do that I need to extract the value of the tensor. However, when I use the numpy() function, it says ""AttributeError: 'Tensor' object has no attribute 'numpy'"". I don't understand why this is happening. Also, tf.executing_eagerly() returns false even though I include the parameter ""run_eagerly=True"" in the compile statement. Any help would be appreciated.

**Describe the expected behavior**
Ideally, the code should let me access the value of the tensor for further manipulation.

**Standalone code to reproduce the issue**

```
def custom_error_finder(y_actual,y_pred):
	print(tf.executing_eagerly())
	count = 0
	qw = tf.py_function((y_actual).numpy())
	ya = ((y_actual[0].numpy()).decode())
	yp = ((y_pred[0].numpy()).decode())
	for i,j in ya,yp:
		if i!=j:
			count = count+1
	mse = pow(count,2)/len(ya)
	return mse
```
Above is the custom loss function that I am trying to use. The dataset I am training my model on contains only strings.
"
39581,Why is tensorflow-gpu still being released for new versions of tensorflow?,"The tensorflow documentation states that the `tensorflow` package supports both CPU and GPU (since version 2.x).

My question is why the `tensorflow-gpu` package is still being released for new versions of tensorflow.

This might be confusing to people (it was for me, definitely), especially because  https://pypi.org/project/tensorflow-gpu has no information on the matter.

I think it would be nice to add a note to the documentation and pypi, _why_ the `tensorflow-gpu` package still exists (if it must), so that other people don't have to go through the trouble of downloading and `diff`ing the wheels. :grimacing:

Cheers!"
39580,Model saved with Lambda layer with a ragged input breaks when reloading,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
With the following model ragged inputs -> Embeddings -> reduce_sum with lambda layer -> Dense a saved model cannot be loaded back. 

```
a = Input(ragged=True, name='a', shape=(None,), dtype=tf.int32)
b = Input(ragged=True, name='b', shape=(None,), dtype=tf.int32)
embed_a = Embedding(10,  3)(a)
embed_b = Embedding(10, 3)(b)
embed_a_reduced = Lambda(tf.reduce_sum, arguments=dict(axis=1))(embed_a)
embed_b_reduced = Lambda(tf.reduce_sum, arguments=dict(axis=1))(embed_b)
concat = Concatenate()([embed_a_reduced, embed_b_reduced])
out = Dense(1)(concat)
m = Model([a, b], out)
m([tf.ragged.constant([[0], [], [1]]), tf.ragged.constant([[0], [], [1]])])
m.save('/tmp/test')
reloaded_model = tf.keras.models.load_model('/tmp/test')
```
```
TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in wrapper(*args, **kwargs)
    200     try:
--> 201       return target(*args, **kwargs)
    202     except (TypeError, ValueError):

TypeError: 'str' object is not callable

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
14 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in wrapper(*args, **kwargs)
    203       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    204       # TypeError, when given unexpected types.  So we need to catch both.
--> 205       result = dispatch(wrapper, *args, **kwargs)
    206       if result is not OpDispatcher.NOT_SUPPORTED:
    207         return result

TypeError: 'module' object is not callable
```
if we debug dispatch is the module dispatch and not the function in that file. 

**Describe the expected behavior**
We should be able to save / load back the model.

**Standalone code to reproduce the issue**
https://colab.research.google.com/gist/tanguycdls/4685e5111c5e3bad43c7699977b49c9c/untitled.ipynb Tested on 2.2 but same error in nightlies. 

**Other info / logs** 
One fix is to use tf nightly and override a keras Layer it then works. 

"
39579,"Different training performance in eager, model.fit and estimator.train","**System information**
Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): binary
TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
Python version: 3.5
CUDA/cuDNN version: 10.1
GPU model and memory: GeForce RTX 2080Ti 11GB

**Describe the current behavior**
We find the training performance is different among eager, model.fit and estimator.train.
We try to transform a YOLOv3 model from tf.keras to tf.estimator and find the training performance is different while we fixed the initial weight, input image, optimizer, learning rate, trainable variables and regularization loss. The first loss is the same, however the following training performance is different. Keras model seems more roberst.

Thus we tried to use some simple model to reproduce the phynominon. The input is a (1, 224, 224 3) matrix with all 1 elements, weights is ImageNet weights. optimizer uses SGD and learning rate uses 1e-4.

We find the first loss of the three keep its value in repetitive experiments.
Eager: 7.28364
Estimator: 7.283639
Keras model: 7.2836

However, from the second step, the loss varies in different area.
Eager: 7.308 to 7.311
Estimator: 7.3092x
Keras model: 7.3085 to 7.3086

So, why the values are totally different? And how can we unite the performance.

**Describe the expected behavior**

We want the performance in these three APIs can be same.

**Standalone code to reproduce the issue**
***keras_model_2ckpt.py***
```
# Transform .h5 to .ckpt
import numpy as np
import tensorflow as tf
import logging
from tensorflow.python.ops import control_flow_ops
import os
from tensorflow.python.framework.ops import disable_eager_execution

disable_eager_execution()

def build_model(name, random_bn_params=None):
    '''
    bild model by name
    '''
    tf.keras.backend.set_learning_phase(True)
    if name == ""ResNet50"":
        model = tf.keras.applications.ResNet50(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    elif name == ""MobileNetV2"":
        model = tf.keras.applications.MobileNetV2(
            input_shape=(224, 224, 3),
            alpha = 0.35,
            pooling=""avg"")
    elif name == ""VGG16"":
        model = tf.keras.applications.VGG16(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    return model

model = build_model(""MobileNetV2"")
model_dict ={""mobile"":model}

#transform h5 file to ckpt
os.makedir(""ckpt_model"")
for name in model_dict:    
    image = tf.compat.v1.placeholder(tf.float32, [None, 224, 224, 3])    
    global_step = tf.compat.v1.train.get_or_create_global_step()    
    prediction = model_dict[name](image)
    sess = tf.compat.v1.keras.backend.get_session()
    sess.run(tf.compat.v1.variables_initializer([global_step]))
    saver = tf.compat.v1.train.Saver()
    save_path = saver.save(sess, ""ckpt_model/%s.ckpt"" % name)
```

***eager_train.py***
```
import numpy as np
import tensorflow as tf
import logging
from tensorflow.python.ops import control_flow_ops
import os

def build_model(name, random_bn_params=None):
    tf.keras.backend.set_learning_phase(True)
    if name == ""ResNet50"":
        model = tf.keras.applications.ResNet50(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    elif name == ""MobileNetV2"":
        model = tf.keras.applications.MobileNetV2(
            input_shape=(224, 224, 3),
            alpha = 0.35,
            pooling=""avg"")
    elif name == ""VGG16"":
        model = tf.keras.applications.VGG16(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    return model


model = build_model(""MobileNetV2"")
for lay in model.layers:
    lay.trainable=True

img = 1.0*np.ones(shape=(1,224, 224,3))
label = np.array([1])

b = (img, label)

def data_provider(repeat):
    return tf.data.Dataset.from_tensors(b).repeat(repeat)


feature = tf.constant(img)
optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4)
for index in range(10):
    with tf.GradientTape() as t:
        logits = model(feature)       

        loss_value  = tf.keras.losses.sparse_categorical_crossentropy(tf.constant(label), logits)
        grads = t.gradient(loss_value, model.trainable_variables)
        optimizer.apply_gradients(zip(grads, model.trainable_variables))
        print(loss_value)
```

***keras_fit_train.py***
```
import numpy as np
import tensorflow as tf
import logging
from tensorflow.python.ops import control_flow_ops
import os

def build_model(name, random_bn_params=None):
    tf.keras.backend.set_learning_phase(True)
    if name == ""ResNet50"":
        model = tf.keras.applications.ResNet50(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    elif name == ""MobileNetV2"":
        model = tf.keras.applications.MobileNetV2(
            input_shape=(224, 224, 3),
            alpha = 0.35,
            pooling=""avg"")
    elif name == ""VGG16"":
        model = tf.keras.applications.VGG16(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    return model


model = build_model(""MobileNetV2"")
for lay in model.layers:
    lay.trainable=True

img = 1.0*np.ones(shape=(1,224, 224,3))
label = np.array([1])
model.compile(optimizer=tf.keras.optimizers.SGD(1e-4),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

b = (img, label)

def data_provider(repeat):
    return tf.data.Dataset.from_tensors(b).repeat(repeat)
model.fit_generator(data_provider(1),
          epochs=10,
          validation_data=data_provider(1),
          shuffle=True)

```

***estimator_train.py***
```
import numpy as np
import tensorflow as tf
import logging
from tensorflow.python.ops import control_flow_ops
import os
LOGGER = logging.getLogger(""tensorflow"")
LOGGER.setLevel(logging.INFO)
def build_model(name, random_bn_params=None):
    tf.keras.backend.set_learning_phase(True)
    if name == ""ResNet50"":
        model = tf.keras.applications.ResNet50(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    elif name == ""MobileNetV2"":
        model = tf.keras.applications.MobileNetV2(
            input_shape=(224, 224, 3),
            alpha = 0.35,
            pooling=""avg"")
    elif name == ""VGG16"":
        model = tf.keras.applications.VGG16(
            input_shape=(224, 224, 3),
            pooling=""avg"")
    return model
def _init_variables_from_checkpoint(checkpoint_path, model_dir):
    flags_checkpoint_path = checkpoint_path
    # Warn the user if a checkpoint exists in the model_dir. Then ignore.
    if tf.compat.v1.train.latest_checkpoint(model_dir):
        LOGGER.info(
            ""Ignoring model_init_name because a checkpoint already exists in %s."" % model_dir)
        return None
    if flags_checkpoint_path is """":
        return None

    # Gather all trainable variables to initialize.
    variables_to_init = tf.compat.v1.trainable_variables()
    addition_variables_to_init = [v for v in tf.compat.v1.all_variables() if
                      ""moving_mean"" in v.name or ""moving_variance"" in v.name]
    variables_to_init.extend(addition_variables_to_init)

    variables_to_init_dict = {var.name.rsplit("":"", 1)[0]: var for var in variables_to_init}

    if tf.compat.v1.gfile.IsDirectory(flags_checkpoint_path):
        checkpoint_path = tf.compat.v1.train.latest_checkpoint(flags_checkpoint_path)
    else:
        checkpoint_path = flags_checkpoint_path

    LOGGER.info(""Fine-tuning from %s."" % checkpoint_path)

    # Gather all available variables to initialize.
    available_var_map = _get_variables_available_in_checkpoint(variables_to_init_dict,
                                                               checkpoint_path)

    init_op = tf.compat.v1.train.init_from_checkpoint(checkpoint_path, available_var_map)
    LOGGER.info(""%d/%d variables in checkpoint has been restored."" % (len(available_var_map),
                                                                      len(variables_to_init)))

    return tf.compat.v1.train.Scaffold(init_op=init_op)


def _get_variables_available_in_checkpoint(variables,
                                           checkpoint_path,
                                           include_global_step=False):
    """"""Returns the subset of variables in the checkpoint.

    Inspects given checkpoint and returns the subset of variables that are
    available in it.

    Args:
        variables: A dictionary of variables to find in checkpoint.
        checkpoint_path: Path to the checkpoint to restore variables from.
        include_global_step: Whether to include `global_step` variable, if it
            exists. Default True.

    Returns:
        A dictionary of variables.

    Raises:
        ValueError: If `variables` is not a dict.
    """"""
    if not isinstance(variables, dict):
        raise ValueError(""`variables` is expected to be a dict."")

    # Available variables
    ckpt_reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path)
    ckpt_vars_to_shape_map = ckpt_reader.get_variable_to_shape_map()
    if not include_global_step:
        ckpt_vars_to_shape_map.pop(tf.compat.v1.GraphKeys.GLOBAL_STEP, None)
    vars_in_ckpt = {}

    # for key in ckpt_vars_to_shape_map:
    #     LOGGER.info(""Available variable name: %s"", key)

    for variable_name, variable in sorted(variables.items()):
        if variable_name in ckpt_vars_to_shape_map:
            if ckpt_vars_to_shape_map[variable_name] == variable.shape.as_list():
                vars_in_ckpt[variable_name] = variable
            else:
                LOGGER.warning(""Variable [%s] is available in checkpoint, but has an incompatible ""
                               ""shape with model variable. Checkpoint shape: [%s], model variable ""
                               ""shape: [%s]. This variable will not be initialized from the ""
                               ""checkpoint."",
                               variable_name,
                               ckpt_vars_to_shape_map[variable_name],
                               variable.shape.as_list())
        else:
            LOGGER.warning(""Variable [%s] is not available in checkpoint"", variable_name)
    return vars_in_ckpt

def model_fn(features, labels, mode):
    # Set Keras learning phase for alter BatchNorm and Dropout performance.
    tf.keras.backend.set_learning_phase(mode == tf.estimator.ModeKeys.TRAIN)

    m = build_model(""MobileNetV2"")
    predictions = m(features)
    global_step = tf.compat.v1.train.get_or_create_global_step()
    
    scaffold = None
   # if MODEL_NAME in {""ResNet50"", ""VGG16"", ""MobileNetV2_1.0""}:
    scaffold = _init_variables_from_checkpoint(""./ckpt_model/moiblenet.ckpt"", '.')
    loss_value  = tf.keras.losses.sparse_categorical_crossentropy(labels, predictions)
    optimizer =tf.keras.optimizers.SGD(learning_rate=1e-4)
    update_ops = m.get_updates_for(features)
    optimizer.iterations = global_step
    train_op = optimizer.get_updates(loss_value, m.trainable_variables)
    train_op = control_flow_ops.group(train_op, *update_ops)


    # Create estimator_spec for Estimator.
    estimator_spec = tf.estimator.EstimatorSpec(
        mode=mode,
        predictions=predictions,
        loss = loss_value,
        train_op = train_op
#         scaffold = scaffold
    )
    return estimator_spec


estimator_config = tf.estimator.RunConfig(
  
    keep_checkpoint_max=1,
    save_summary_steps=1,
    log_step_count_steps =1

)

img = 1.0*np.ones(shape=(1,224, 224,3))
label = np.array([1])

b = (img, label)

def data_provider(repeat):
    return tf.data.Dataset.from_tensors(b).repeat(repeat)

estimator_handmake = tf.estimator.Estimator(model_fn=model_fn, config= estimator_config)

estimator_handmake.train(input_fn=lambda: data_provider(10), steps =10)
```
"
39578,Problem with layer generation under the functional API,"Hi there, I am having problems with the functional API.

 - OS: Mac OS Catalina 10.15.4
 - Python-Version: 3.7.4
 - Tensorflow-Version: 2.1.0

When I run the example for the functional API here (https://www.tensorflow.org/guide/keras/functional)

with this code snippet:
import numpy as np

import tensorflow as tf

from tensorflow import keras
from tensorflow.keras import layers

tf.keras.backend.clear_session()
inputs = keras.Input(shape=(784,))
dense = layers.Dense(64, activation='relu')
x = dense(inputs)

I get the following error:

AttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'append'

I do not know how to handle this error, can you help?

This is how the whole error output looks like:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-4-c3d73338788b> in <module>
      9 inputs = keras.Input(shape=(784,))
     10 dense = layers.Dense(64, activation='relu')
---> 11 x = dense(inputs)

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    746           # Build layer if applicable (if the `build` method has been
    747           # overridden).
--> 748           self._maybe_build(inputs)
    749           cast_inputs = self._maybe_cast_inputs(inputs)
    750 

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   2114         # operations.
   2115         with tf_utils.maybe_init_scope(self):
-> 2116           self.build(input_shapes)
   2117       # We must set self.built since user defined build functions are not
   2118       # constrained to set self.built.

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py in build(self, input_shape)
   1111         constraint=self.kernel_constraint,
   1112         dtype=self.dtype,
-> 1113         trainable=True)
   1114     if self.use_bias:
   1115       self.bias = self.add_weight(

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in add_weight(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)
    444         synchronization=synchronization,
    445         aggregation=aggregation,
--> 446         caching_device=caching_device)
    447     backend.track_variable(variable)
    448 

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py in _add_variable_with_custom_getter(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)
    742         dtype=dtype,
    743         initializer=initializer,
--> 744         **kwargs_for_getter)
    745 
    746     # If we set an initializer and the variable processed it, tracking will not

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer_utils.py in make_variable(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)
    140       synchronization=synchronization,
    141       aggregation=aggregation,
--> 142       shape=variable_shape if variable_shape else None)
    143 
    144 

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)
    256   def __call__(cls, *args, **kwargs):
    257     if cls is VariableV1:
--> 258       return cls._variable_v1_call(*args, **kwargs)
    259     elif cls is Variable:
    260       return cls._variable_v2_call(*args, **kwargs)

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in _variable_v1_call(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)
    217         synchronization=synchronization,
    218         aggregation=aggregation,
--> 219         shape=shape)
    220 
    221   def _variable_v2_call(cls,

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in <lambda>(**kwargs)
    195                         shape=None):
    196     """"""Call on Variable class. Useful to force the signature.""""""
--> 197     previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    198     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    199       previous_getter = _make_getter(getter, previous_getter)

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variable_scope.py in default_variable_creator(next_creator, **kwargs)
   2594         synchronization=synchronization,
   2595         aggregation=aggregation,
-> 2596         shape=shape)
   2597   else:
   2598     return variables.RefVariable(

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v2_call(*args, **kwargs)
    261     else:
--> 262       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    263 
    264 

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1409           aggregation=aggregation,
   1410           shape=shape,
-> 1411           distribute_strategy=distribute_strategy)
   1412 
   1413   def _init_from_args(self,

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1555               shared_name=shared_name,
   1556               name=name,
-> 1557               graph_mode=self._in_graph_mode)
   1558         # pylint: disable=protected-access
   1559         if (self._in_graph_mode and initial_value is not None and

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in eager_safe_variable_handle(initial_value, shape, shared_name, name, graph_mode)
    230   dtype = initial_value.dtype.base_dtype
    231   return _variable_handle_from_shape_and_dtype(
--> 232       shape, dtype, shared_name, name, graph_mode, initial_value)
    233 
    234 

~/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py in _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, initial_value)
    166     handle_data = cpp_shape_inference_pb2.CppShapeInferenceResult.HandleData()
    167     handle_data.is_set = True
--> 168     handle_data.shape_and_type.append(
    169         cpp_shape_inference_pb2.CppShapeInferenceResult.HandleShapeAndType(
    170             shape=shape.as_proto(), dtype=dtype.as_datatype_enum))

AttributeError: 'google.protobuf.pyext._message.RepeatedCompositeCo' object has no attribute 'append'

tf.__version__

'2.1.0'


"
39575,get_tensor CheckpointReader_GetTensor error,"
**System information**
- OS Platform and Distribution :win 10
- OS -V:1903
- TensorFlow installed from (source or binary):binary
- TensorFlow version:TensorFlow-gpu==2.0.0
- Python version:3.6.5
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:cuda:10.0.130 cudnn:7.6.5
-nvidia driver :445.87
- GPU model and memory:2070super 8G      64G Ram

The phenomenon that I encountered was this
```
from tensorflow.python import pywrap_tensorflow
model_reader = pywrap_tensorflow.NewCheckpointReader(r""E:\dl_model\chinese_L-12_H-768_A-12\bert_model.ckpt"")
var_dict = model_reader.get_variable_to_shape_map()
for key in var_dict:
    print(""variable name: "", key)
    print(model_reader.get_tensor(key))
```
I run this code.
ide print
```
C:\public\study\python\python.exe C:/public/study/work/test/test.py
2020-05-15 20:18:56.028434: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
variable name:  bert/encoder/layer_0/output/dense/kernel

Process finished with exit code -1073741819 (0xC0000005)
```
The code runs to
```
get_tensor

return CheckpointReader_GetTensor(self, compat.as_bytes(tensor_str))
```
When compat.as_bytes(tensor_str) get result.
I click step into.
The ide shows the code running for a while.
And then,print
`Process finished with exit code -1073741819 (0xC0000005).`

I try 3 models to read.They have the same result. I think it is not model cause question.
I try to delete cuda ,cudnn,nvidia driver.Install other version.
From 10.1.243/7.6.5  10.1.243/7.6.4 10.1.168/7.6.5 10.2.89/7.6.5 10.0.130/7.6.5 10.0.130/7.4.2
I install Visual Studio 2015/2017/2019 not include many plug-ins.
I install conda(python3.7) try to install tensorflow-gpu==2.0.0.

I don't know what causes this question.
Maybe it's my gpu environment,computer environment,maybe other.

My understanding of the problem is that python calls c.
So many people have no problem, so the probability is not a problem of the code.
Although I know about the installation environment, I shouldn't be looking for you on github.
But I really tried what I thought, what I found on the Internet, and it still hasn't been solved.
I hope you can help me, even if only in some directions.
If you need any other information, please tell me 



"
39574,Training produces Out Of Memory error with TF 2.* but works with TF 1.14,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
- TensorFlow installed from (source or binary): pip 20.1
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: Python 3.7.7
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory: Nvidia K80 12GB (AWS p2.xlarge Deep Learning AMI (Ubuntu 18.04) Version 28.1)


**Describe the current behavior**
Training a simple model on a data set:
    X shape: (35000, 200, 311)
    y shape: (35000, 200, 19)
Right before the start of the first epoch, when using TF 2.2.0 the training stops with an error:
`tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]`
(abbreviated for readability and because this error seems to point to Out Of Memory issues indirectly)
(also because the code example below can reproduce the error message)

Interestingly: The exact same setup, model and training set trains well on TF 1.14

NOTES:
* I've tried changing the batch size (down to 1) to no avail.
* Decreasing Training Set to ~10,000 test cases works fine.
* Error also occurs on freshly re-started instances.
* `allow_growth` option didn't make a difference.
* GPU memory is being used by training (confirmed by looking at GPU usage and by showing found GPU devices)

**Describe the expected behavior**
Training above model runs as expected and does not error out.

**Standalone code to reproduce the issue**
The [colab link](https://colab.research.google.com/drive/1IWzCIljUTBNfmtsP1Sefu6usrhMp4pGL?usp=sharing) can be executed, but if not run on the specified hardware, it wouldn't make sense to test this way.

Best to reproduce is running following code on the described setup with TF 1.14 and TF 2.2.0
```
import json
import numpy as np
from tensorflow.keras.layers import GRU, Bidirectional, Dense, Masking, Input, TimeDistributed
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.utils import to_categorical

y_raw = np.random.randint(19, size=(35000,200))
y = to_categorical(y_raw, num_classes=19)
X = np.random.rand(35000, 200, 311)

model = Sequential()
model.add(Masking(mask_value=0.0, input_shape=(200,311)))
model.add(
            Bidirectional(
                        GRU(
                            256,
                            return_sequences=True,
                            unroll=True,
                            recurrent_dropout=0.233,
                            recurrent_activation=""sigmoid""
                           )
                         )
         )

model.add(TimeDistributed(Dense(19, activation=""softmax"")))

model.compile(
            loss=""categorical_crossentropy"",
            optimizer=""rmsprop"",
            metrics=[""categorical_accuracy""],
            )

model.summary()

architecture_path = ""candidate_architecture.json""
model_json = model.to_json()
with open(architecture_path, 'w') as json_file:
    json_file.write(model_json)
print(f""Saved model architecture to {architecture_path}"")

filepath = ""candidate_model-{epoch:02d}-{val_categorical_accuracy:.4f}.h5""
checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)
callbacks_list = [checkpoint]

history = model.fit(X, y, epochs=10,
                    validation_split=0.2,
                    batch_size=16,
                    callbacks=callbacks_list)
```

**Other info / logs** 
Full traceback of error:

```
2020-05-14 07:16:34.619007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-14 07:16:34.652784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:16:34.653592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:00:1e.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-05-14 07:16:34.653944: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 07:16:34.656389: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 07:16:34.658467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 07:16:34.658882: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 07:16:34.661237: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 07:16:34.662515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 07:16:34.666590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 07:16:34.666743: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:16:34.667562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:16:34.668297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-05-14 07:17:53.690634: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-14 07:17:53.714191: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300050000 Hz
2020-05-14 07:17:53.714472: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f3970000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-14 07:17:53.714508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-14 07:17:53.816549: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:17:53.817400: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x555a5231f6d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-14 07:17:53.817432: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7
2020-05-14 07:17:53.817690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:17:53.818466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:00:1e.0 name: Tesla K80 computeCapability: 3.7
coreClock: 0.8235GHz coreCount: 13 deviceMemorySize: 11.17GiB deviceMemoryBandwidth: 223.96GiB/s
2020-05-14 07:17:53.818528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 07:17:53.818570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 07:17:53.818598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 07:17:53.818625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 07:17:53.818677: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 07:17:53.818722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 07:17:53.818752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 07:17:53.818859: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:17:53.819637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:17:53.820344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-05-14 07:17:53.820395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 07:17:53.822110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 07:17:53.822137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-05-14 07:17:53.822153: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-05-14 07:17:53.822277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:17:53.823052: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 07:17:53.823792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10691 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)
WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
WARNING:tensorflow:Layer gru will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU
Label count: 19
Model defined.
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
masking (Masking)            (None, 200, 311)          0
_________________________________________________________________
bidirectional (Bidirectional (None, 200, 512)          873984
_________________________________________________________________
time_distributed (TimeDistri (None, 200, 19)           9747
=================================================================
Total params: 883,731
Trainable params: 883,731
Non-trainable params: 0
_________________________________________________________________
None
Saved model architecture to candidate_architecture.json
2020-05-14 07:18:19.476160: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14639392000 exceeds 10% of free system memory.
2020-05-14 07:18:40.240359: W tensorflow/core/common_runtime/bfc_allocator.cc:434] Allocator (GPU_0_bfc) ran out of memory trying to allocate 13.63GiB (rounded to 14639392000)
Current allocation summary follows.
2020-05-14 07:18:40.243374: I tensorflow/core/common_runtime/bfc_allocator.cc:934] BFCAllocator dump for GPU_0_bfc
2020-05-14 07:18:40.243393: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (256):   Total Chunks: 11, Chunks in use: 11. 2.8KiB allocated for chunks. 2.8KiB in use in bin. 116B client-requested in use in bin.
2020-05-14 07:18:40.243409: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243434: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (1024):  Total Chunks: 1, Chunks in use: 1. 1.2KiB allocated for chunks. 1.2KiB in use in bin. 1.0KiB client-requested in use in bin.
2020-05-14 07:18:40.243451: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243474: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (4096):  Total Chunks: 2, Chunks in use: 2. 12.0KiB allocated for chunks. 12.0KiB in use in bin. 12.0KiB client-requested in use in bin.
2020-05-14 07:18:40.243496: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243568: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243595: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (32768):         Total Chunks: 3, Chunks in use: 1. 113.5KiB allocated for chunks. 38.0KiB in use in bin. 38.0KiB client-requested in use in bin.
2020-05-14 07:18:40.243630: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243655: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (131072):        Total Chunks: 1, Chunks in use: 0. 130.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243698: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243721: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (524288):        Total Chunks: 5, Chunks in use: 2. 4.07MiB allocated for chunks. 1.66MiB in use in bin. 1.50MiB client-requested in use in bin.
2020-05-14 07:18:40.243756: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (1048576):       Total Chunks: 2, Chunks in use: 2. 2.68MiB allocated for chunks. 2.68MiB in use in bin. 1.82MiB client-requested in use in bin.
2020-05-14 07:18:40.243773: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (2097152):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243782: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243792: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243825: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243841: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243861: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243893: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (134217728):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243916: I tensorflow/core/common_runtime/bfc_allocator.cc:941] Bin (268435456):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2020-05-14 07:18:40.243934: I tensorflow/core/common_runtime/bfc_allocator.cc:957] Bin for 13.63GiB was 256.00MiB, Chunk State:
2020-05-14 07:18:40.243949: I tensorflow/core/common_runtime/bfc_allocator.cc:970] Next region of size 1048576
2020-05-14 07:18:40.243962: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203be0000 of size 1280 next 1
2020-05-14 07:18:40.243976: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203be0500 of size 256 next 5
2020-05-14 07:18:40.243983: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 1203be0600 of size 1047040 next 18446744073709551615
2020-05-14 07:18:40.243994: I tensorflow/core/common_runtime/bfc_allocator.cc:970] Next region of size 2097152
2020-05-14 07:18:40.244014: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee0000 of size 256 next 3
2020-05-14 07:18:40.244027: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee0100 of size 256 next 4
2020-05-14 07:18:40.244039: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee0200 of size 256 next 10
2020-05-14 07:18:40.244058: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee0300 of size 6144 next 14
2020-05-14 07:18:40.244071: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee1b00 of size 6144 next 17
2020-05-14 07:18:40.244086: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee3300 of size 256 next 20
2020-05-14 07:18:40.244103: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee3400 of size 256 next 23
2020-05-14 07:18:40.244121: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ee3500 of size 256 next 24
2020-05-14 07:18:40.244134: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 1203ee3600 of size 38144 next 15
2020-05-14 07:18:40.244151: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203eecb00 of size 256 next 18
2020-05-14 07:18:40.244164: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203eecc00 of size 256 next 19
2020-05-14 07:18:40.244171: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 1203eecd00 of size 39168 next 21
2020-05-14 07:18:40.244179: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203ef6600 of size 38912 next 22
2020-05-14 07:18:40.244187: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 1203effe00 of size 133120 next 11
2020-05-14 07:18:40.244193: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203f20600 of size 256 next 13
2020-05-14 07:18:40.244204: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 1203f20700 of size 692224 next 6
2020-05-14 07:18:40.244223: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1203fc9700 of size 1140992 next 18446744073709551615
2020-05-14 07:18:40.244240: I tensorflow/core/common_runtime/bfc_allocator.cc:970] Next region of size 4194304
2020-05-14 07:18:40.244253: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 12040e0000 of size 256 next 8
2020-05-14 07:18:40.244269: I tensorflow/core/common_runtime/bfc_allocator.cc:990] Free  at 12040e0100 of size 786432 next 9
2020-05-14 07:18:40.244283: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 12041a0100 of size 786432 next 12
2020-05-14 07:18:40.244297: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1204260100 of size 955392 next 16
2020-05-14 07:18:40.244315: I tensorflow/core/common_runtime/bfc_allocator.cc:990] InUse at 1204349500 of size 1665792 next 18446744073709551615
2020-05-14 07:18:40.244332: I tensorflow/core/common_runtime/bfc_allocator.cc:995]      Summary of in-use Chunks by size:
2020-05-14 07:18:40.244348: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 11 Chunks of size 256 totalling 2.8KiB
2020-05-14 07:18:40.244366: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 1280 totalling 1.2KiB
2020-05-14 07:18:40.244380: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 2 Chunks of size 6144 totalling 12.0KiB
2020-05-14 07:18:40.244390: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 38912 totalling 38.0KiB
2020-05-14 07:18:40.244397: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 786432 totalling 768.0KiB
2020-05-14 07:18:40.244411: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 955392 totalling 933.0KiB
2020-05-14 07:18:40.244418: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 1140992 totalling 1.09MiB
2020-05-14 07:18:40.244433: I tensorflow/core/common_runtime/bfc_allocator.cc:998] 1 Chunks of size 1665792 totalling 1.59MiB
2020-05-14 07:18:40.244447: I tensorflow/core/common_runtime/bfc_allocator.cc:1002] Sum Total of in-use chunks: 4.39MiB
2020-05-14 07:18:40.244464: I tensorflow/core/common_runtime/bfc_allocator.cc:1004] total_region_allocated_bytes_: 7340032 memory_limit_: 11210358784 available bytes: 11203018752 curr_region_allocation_bytes_: 8388608
2020-05-14 07:18:40.244482: I tensorflow/core/common_runtime/bfc_allocator.cc:1010] Stats:
Limit:                 11210358784
InUse:                     4603904
MaxInUse:                  6655232
NumAllocs:                      66
MaxAllocSize:              1665792

2020-05-14 07:18:40.244501: W tensorflow/core/common_runtime/bfc_allocator.cc:439] *_____________****_________**************x*__________***********************x**************xxxxxxxxx
Traceback (most recent call last):
  File ""modules/training-suite-controller/training-suite-controller/train_open_model.py"", line 108, in <module>
    process(training_setup)
  File ""modules/training-suite-controller/training-suite-controller/train_open_model.py"", line 85, in process
    batch_size=training_setup['batch_size'], callbacks=callbacks_list)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 797, in fit
    shuffle=False))
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1338, in train_validation_split
    functools.partial(_split, indices=train_indices), arrays)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 617, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/util/nest.py"", line 617, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1335, in _split
    return array_ops.gather_v2(t, indices)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 4541, in gather_v2
    batch_dims=batch_dims)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py"", line 4524, in gather
    return gen_array_ops.gather_v2(params, indices, axis, name=name)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3755, in gather_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""/home/ubuntu/.cache/pypoetry/virtualenvs/snapaddy-deep-learning-training-suite-YlZOSDlZ-py3.7/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run GatherV2: Dst tensor is not initialized. [Op:GatherV2]
```
"
39572,The New Converter of Tensorflow 2.2.0 generate incorrect model output,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows/ubuntu
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.2.0


**Command used to run the converter or code if you’re using the Python API**


``` python
https://colab.research.google.com/drive/1uMiHGTFrj7R_rFcGJoje3Yfbap0sC9H-?usp=sharing
import tensorflow as tf
import numpy as np
np.set_printoptions(suppress=True)

length = 66

a = tf.constant(
    np.random.rand(length,length).astype(np.float32),
    shape=[length, length])

c = [ tf.constant(
    np.random.rand(length).astype(np.float32),
    shape=[ length]) for i in range(0,3)]


@tf.function
def func2(x ,c1, c2):
    return  tf.multiply(x, c2) + c2

@tf.function
def func(x):
    return  func2(tf.nn.bias_add(tf.matmul(x,a) , c[0] ), c[1],  c[2])
     


input =  np.random.rand(1,length).astype(np.float32)
original_output = func(input).numpy()

print(""Orginal:"")
print(original_output)


def save_and_test(experimental_new_converter , filename):

    lite = tf.lite.TFLiteConverter.from_concrete_functions([func.get_concrete_function(x = tf.TensorSpec(shape=[None,length], dtype=tf.float32) )])
    lite.experimental_new_converter  = experimental_new_converter
    open(filename,""wb"").write(lite.convert())

    interpreter = tf.lite.Interpreter(model_path=filename)
    interpreter.allocate_tensors()
    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], input)
    interpreter.invoke()

    print(filename, "":"")
    output = interpreter.get_tensor(interpreter.get_output_details()[0]['index'])
    print(output)
    print(filename, "" sum of absolute difference: "", np.sum(np.absolute(original_output - output)))


save_and_test(True, ""mlir"")
save_and_test(False, ""toco"")
```

**The output from the converter invocation**

``` 
Orginal:
[[ 6.9496317   7.9791102   0.23810546  1.6724037   9.687994   11.553456
   0.60509163  8.606714   17.731003    2.52809    16.075905    0.50299734
   6.119868   12.388639    8.383967   11.989329    7.3116794   8.7486725
   9.714089   12.450738    1.4212162   7.567941   14.836599    5.73806
   8.028838    2.5774102   7.9609547  19.35534    11.474097    1.2807347
   5.075203   17.249842    3.0866256  17.775652    7.390017   15.896519
  16.136719    6.5862875  12.216141    0.33137095 18.97356     1.9778614
   5.3641543   1.9284656   1.8500257  16.767817   12.7209     14.047361
   7.367909    3.1752422   5.949711   15.742522    2.2939441  16.137108
   1.6687201  13.069997    8.666047   16.782955    1.3117387   6.3798137
   6.557558   15.286772    7.5772853   9.168247    3.2267199   1.5568383 ]]

mlir :
[[ 8.160242   9.0523     7.3261642  8.867854   7.860682   8.722304
   7.2230916  8.622743   9.271859   7.7777925  9.489797   7.390472
   8.524878   9.906944   8.355099   8.330111   8.341421   8.271743
   8.937096   8.515015   7.225615   9.190289   8.9844675  8.162167
   8.694139   6.9362564  9.618863  10.1376915  8.462459   8.29322
   8.158456   8.796178   8.039083   9.774652   8.209907   9.378836
  10.289057   7.984726   8.897137   9.119129   9.802201   8.349303
   6.498592   8.638722   8.569397   9.730883   8.358421   9.018978
   8.4079275  6.496749   8.111706   8.541293   9.513434   9.01988
   7.2715945  8.307488   8.566431   9.748925   8.775068   7.9520493
   7.9917192  9.261906   7.9303417  9.471784   9.427535   7.840794 ]]
mlir  sum of absolute difference:  284.37515

toco :
[[ 6.9496307   7.9791093   0.23810542  1.6724037   9.687995   11.553459
   0.6050917   8.606714   17.731003    2.5280902  16.075907    0.50299734
   6.1198673  12.388639    8.383966   11.98933     7.311679    8.748673
   9.714088   12.450737    1.4212162   7.567941   14.836597    5.73806
   8.028838    2.5774097   7.960953   19.355337   11.474101    1.2807347
   5.075203   17.24984     3.086626   17.775652    7.390019   15.896517
  16.13672     6.586289   12.216139    0.33137095 18.973562    1.9778614
   5.364155    1.9284654   1.8500254  16.767818   12.7209015  14.047361
   7.367908    3.1752434   5.94971    15.74252     2.2939441  16.137106
   1.6687204  13.069999    8.666047   16.782953    1.3117385   6.379814
   6.557558   15.286772    7.577286    9.168246    3.2267199   1.5568382 ]]
toco  sum of absolute difference:  5.4582953e-05
```

**Failure details**
The conversion is successful, but the generated model is wrong,
The result running with the model is much different from the original result.




"
39571,"Convert ctc_decode in tf.keras to .pb model, Getting call error.","
**System information**
- Windows 10:
- TensorFlow installed from (binary):
- TensorFlow version (1.10):
- Python version: 3.6

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)


**Describe the current behavior**
I convert ctc_model `tensorflow.backend.keras.ctc_decode` to pb model, Then call with java ,There are exception

```
Exception in thread ""main"" java.lang.IllegalArgumentException: sequence_length is not a vector
	 [[Node: decode/CTCGreedyDecoder = CTCGreedyDecoder[merge_repeated=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](decode/Log, decode/ToInt32)]]
```
The code contains“sequence_length” is (parameter input_length is a vector):

```

@tf_export('keras.backend.ctc_decode')
def ctc_decode(y_pred, input_length, greedy=True, beam_width=100, top_paths=1):
  """"""Decodes the output of a softmax.

  Can use either greedy search (also known as best path)
  or a constrained dictionary search.

  Arguments:
      y_pred: tensor `(samples, time_steps, num_categories)`
          containing the prediction, or output of the softmax.
      input_length: tensor `(samples, )` containing the sequence length for
          each batch item in `y_pred`.
      greedy: perform much faster best-path search if `true`.
          This does not use a dictionary.
      beam_width: if `greedy` is `false`: a beam search decoder will be used
          with a beam of this width.
      top_paths: if `greedy` is `false`,
          how many of the most probable paths will be returned.

  Returns:
      Tuple:
          List: if `greedy` is `true`, returns a list of one element that
              contains the decoded sequence.
              If `false`, returns the `top_paths` most probable
              decoded sequences.
              Important: blank labels are returned as `-1`.
          Tensor `(top_paths, )` that contains
              the log probability of each decoded sequence.
  """"""
  y_pred = math_ops.log(array_ops.transpose(y_pred, perm=[1, 0, 2]) + epsilon())
  input_length = math_ops.to_int32(input_length)

  if greedy:
    (decoded, log_prob) = ctc.ctc_greedy_decoder(
        inputs=y_pred, sequence_length=input_length)
  else:
    (decoded, log_prob) = ctc.ctc_beam_search_decoder(
        inputs=y_pred,
        sequence_length=input_length,
        beam_width=beam_width,
        top_paths=top_paths)
  decoded_dense = [
      sparse_ops.sparse_to_dense(
          st.indices, st.dense_shape, st.values, default_value=-1)
      for st in decoded
  ]
  return (decoded_dense, log_prob)
```

**Describe the expected behavior**
Run smoothly

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39569,SyncBatchNormalization would cause gradient explosion occasionally,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: 8x V100 16G

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When using SyncBN in my network, sometimes i would see gradient explosion for no reason. However, there's no such problem when using regular BN.

i'm not 100% sure if this is a bug of SyncBN, but want to leave an issue here to see if anyone else has run into similar problem.

i printed gradient and loss for two mini batches, the first one is a normal batch, and gradient exploded in the second batch.
```
# normal batch
max gradient:  0.326069444 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.340717554 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.231284797 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.460334092 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  1.16038787 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.489672422 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.276092589 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  0.48520276 max grad var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  82.8172226 max weights var:  detector_scale_large_final_conv2d/bias:0
Trained batch: 1051 batch loss: 120.549194 batch l2 loss 5413.74561

# exploded batch
max gradient:  6.35045412e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  1.04548427e+19 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  8.70996347e+16 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  2.92331335e+18 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  1.48902926e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  7.38057557e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  3.23904746e+18 max grad var:  detector_scale_large_final_conv2d/bias:0
max gradient:  4.20917e+17 max grad var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
max weights after gd:  4.87674142e+14 max weights var:  detector_scale_large_final_conv2d/bias:0
Trained batch: 1052 batch loss: 120.594421 batch l2 loss 5413.69141
```

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39568,predict_signature_def to support multiple inputs and outputs,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 3.6
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

To covert a Keras model to Tensorflow, we need the input and output signatures. This is registered via the function `predict_signature_def`
This methods requires inputs  and outputs be a single Tensor.  If my Keras model has multiple inputs and outputs, how to I specify them in this API? 

**Will this change the current api? How?**

**Who will benefit with this feature?**
Developers looking to migrate from Keras to Tensor format

**Any Other info.**
"
39567,Unable to save ensembled tf.keras.Model instance ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
**NIL**
- TensorFlow installed from (source or binary):
**Binary**
- TensorFlow version (use command below):
**2.1.0**
- Python version:
**3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
**10.1 /  7.6.4**
- GPU model and memory:
**RTX 2070 Super/8GB**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am unable to save an ensembled model after training it to completion.

My ensembled network is as follows:
```python

inputs = tf.keras.layers.Input(shape=[224, 224, 3])

def Net(inputs):
    base_model = DenseNet121(include_top=False, weights='imagenet', input_tensor=inputs)
    output = base_model.get_layer(""pool3_conv"").output
    x = Conv2D(128, 3, activation='relu', padding='same')(output)
    x = BatchNormalization()(x)
    x = Conv2D(64, 3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(2, activation='softmax', name='clf_output')(x)

    model = tf.keras.models.Model(inputs=[base_model.input], outputs=[x])

    return model

def create_ensemble(models, inputs):
    for i in range(len(models)):
        # Each model is a Net object
        model = models[i]
        for layer in model.layers[1:]:
            layer.trainable = False
            layer._name = 'ensemble_' + str(i+1) + '_' + layer._name

    stack_outputs = [model(inputs) for model in models]
    merge = Concatenate()(stack_outputs) 
    x = Dense(16, activation='relu')(merge)
    x = Dense(2, activation='softmax')(x)

    print(inputs)
    model = tf.keras.models.Model(inputs=inputs, outputs=x, name='ensemble')

    return model
```
Portion of training code:
```python
 # Creating Ensemble
    print(""Creating Ensemble"")
    ensemble = create_ensemble(models, inputs)

    print(""Ensemble architecture: "")
    print(ensemble.summary())

    ensemble.compile(loss=""categorical_crossentropy"", 
                     optimizer=opt, 
                     metrics=[""accuracy""])

    ensemble_checkpoint = tf.keras.callbacks.ModelCheckpoint(""ensemble_backup_weights.h5"", 
                                                             monitor='val_loss', 
                                                             verbose=1, 
                                                             save_weights_only=True, 
                                                             save_best_only=True)
history = ensemble.fit(train_ds, 
                           epochs=EPOCHS, 
                           verbose=1,
                           steps_per_epoch=(len(imagePaths)-VAL_SPLIT) // BS, 
                           validation_data=(val_ds), 
                           validation_steps = VAL_SPLIT // BS, 
                           callbacks=[ensemble_checkpoint, reducelr])

    print(""Saving Ensemble"")
    ensemble.save_weights(""ensemble_weights.h5"")
    ensemble.save(""ensemble"", include_optimizer=False)
```

**Describe the expected behavior**
```ensemble.save``` should work normally. I am able to save the weights, but not the model as a whole.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Error encountered is as follows:
```
Epoch 00001: val_loss improved from inf to 0.45252, saving model to ensemble_backup_weights.h5
265/265 [==============================] - 52s 195ms/step - loss: 0.5693 - accuracy: 0.7101 - val_loss: 0.4525 - val_accuracy: 0.9116
Saving Ensemble
2020-05-15 11:42:31.230630: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
Traceback (most recent call last):
  File ""train.py"", line 146, in <module>
    main(ap.parse_args())
  File ""train.py"", line 140, in main
    ensemble.save(""ensemble"", include_optimizer=False)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1008, in save
    signatures, options)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py"", line 115, in save_model
    signatures, options)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/save.py"", line 78, in save
    save_lib.save(model, filepath, signatures, options)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 923, in save
    saveable_view, asset_info.asset_index)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 653, in _serialize_object_graph
    _write_object_proto(obj, obj_proto, asset_file_def_index)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/saved_model/save.py"", line 692, in _write_object_proto
    metadata=obj._tracking_metadata)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py"", line 2412, in _tracking_metadata
    return self._trackable_saved_model_saver.tracking_metadata
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/base_serialization.py"", line 57, in tracking_metadata
    self.python_properties,
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 40, in python_properties
    return self._python_properties_internal()
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/model_serialization.py"", line 35, in _python_properties_internal
    metadata = super(ModelSavedModelSaver, self)._python_properties_internal()
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/network_serialization.py"", line 32, in _python_properties_internal
    metadata = super(NetworkSavedModelSaver, self)._python_properties_internal()
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/layer_serialization.py"", line 58, in _python_properties_internal
    metadata['config'] = self.obj.get_config()
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 918, in get_config
    return copy.deepcopy(get_network_config(self))
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1993, in get_network_config
    layer_config = serialize_layer_fn(layer)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/utils/generic_utils.py"", line 198, in serialize_keras_object
    config = instance.get_config()
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 918, in get_config
    return copy.deepcopy(get_network_config(self))
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 2025, in get_network_config
    model_outputs = nest.pack_sequence_as(network._nested_outputs, model_outputs)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py"", line 504, in pack_sequence_as
    return _pack_sequence_as(structure, flat_sequence, expand_composites)
  File ""/home/tedmund/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow_core/python/util/nest.py"", line 453, in _pack_sequence_as
    len(flat_sequence), truncate(flat_sequence, 100)))
ValueError: The target structure is of type `<class 'tensorflow.python.framework.ops.Tensor'>`
  Tensor(""clf_output/Identity:0"", shape=(None, 2), dtype=float32)
However the input structure is a sequence (<class 'list'>) of length 0.
  []
nest cannot guarantee that it is safe to map one to the other.
```"
39566,Multithreaded Function Stops after Model is Instantiated,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.7
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: RTX 2070

This is kind of a follow up to [a previous issue](https://github.com/tensorflow/tensorflow/issues/39431) I had, about a Python hanging when `.predict` was called in a thread, however, that wasn't thoroughly reproducible.

I have further narrowed down the issue to a model being instantiated inside a thread.  Below is a useless function that demonstrates the issue:

```python
def my_function():
    print('barfoo')
    a = tf.keras.Sequential([tf.keras.layers.Dense(4, input_shape=(16,))])
    print('foobar')
```

When this isn't put in a thread with multiprocessing, it correctly prints:

```
barfoo
foobar
```

However, when put in a multiprocessing thread, it only prints:

```
barfoo
```

[Here](https://colab.research.google.com/gist/LryF/3898b2da42d706bb0be910b572d97a6c/other-test.ipynb) is a notebook that replicates the issue."
39565,Recursive support for tf.io.gfile.glob,"Currently, the `tf.io.gfile.glob` API do not support `recursive=True` kwargs, which is inconsistent with Python [glob.glob](https://docs.python.org/3/library/glob.html).

Example to demonstrate the issue:

```py
import glob
import tensorflow as tf

tf.io.gfile.makedirs('/tmp/a/b/c')

tf.io.gfile.glob('/tmp/a/**')  # ['/tmp/a/b']
glob.glob('/tmp/a/**', recursive=True)  # ['/tmp/a/', '/tmp/a/b', '/tmp/a/b/c']
```
It would be nice to support `recursive=True` for the Gfile API:

```py
tf.io.gfile.glob('/tmp/a/**', recursive=True)  # ['/tmp/a/', '/tmp/a/b', '/tmp/a/b/c']
```"
39564,[RNN] Error Optimizing Tensorflow keras model,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina v 10.15.3
- TensorFlow installed from (source or binary): binary (pip install tensorflow)
- TensorFlow version (or github SHA if from source):  2.1.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
```
import tensorflow as tf

x_inputs = tf.keras.layers.Input((3,229,1))
x = tf.keras.layers.Conv2D(32, (3,3), padding='same')(x_inputs)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)
x = tf.keras.layers.Dropout(0.25)(x)
x = tf.keras.layers.MaxPooling2D((1,2))(x)
x = tf.keras.layers.Conv2D(64, (3,3), padding='same')(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Activation('relu')(x)
x = tf.keras.layers.MaxPooling2D((1,2))(x)
x = tf.keras.layers.Reshape((-1,tf.keras.backend.int_shape(x)[-1] * tf.keras.backend.int_shape(x)[-2]))(x)
x = tf.keras.layers.LSTM(256)(x)
x = tf.keras.layers.Dense(1)(x)
x = tf.keras.layers.Activation('sigmoid')(x)
model = tf.keras.models.Model(x_inputs, x)
model.summary()

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.experimental_new_converter = True
converter.allow_custom_ops = True
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
```

**The output from the converter invocation**
```
2020-05-14 16:58:55.177804: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-05-14 16:58:55.177972: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-05-14 16:58:55.332306: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-05-14 16:58:55.332333: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 836 nodes (0), 1109 edges (0), time = 27.123ms.
2020-05-14 16:58:55.332339: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: Graph size after: 836 nodes (0), 1109 edges (0), time = 70.348ms.
2020-05-14 16:58:55.332343: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_forward_lstm_1_while_body_42042
2020-05-14 16:58:55.332348: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-05-14 16:58:55.332353: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-14 16:58:55.332358: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_backward_lstm_1_while_cond_42307
2020-05-14 16:58:55.332363: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-05-14 16:58:55.332369: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:55.332374: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_backward_lstm_1_while_body_42308
2020-05-14 16:58:55.332380: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-14 16:58:55.332385: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:55.332390: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_forward_lstm_2_while_body_42685
2020-05-14 16:58:55.332395: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.002ms.
2020-05-14 16:58:55.332399: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-14 16:58:55.332404: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_forward_lstm_1_while_cond_42041
2020-05-14 16:58:55.332409: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-14 16:58:55.332415: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:55.332421: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_backward_lstm_2_while_cond_42950
2020-05-14 16:58:55.332445: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:55.332486: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:55.332492: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_forward_lstm_2_while_cond_42684
2020-05-14 16:58:55.332497: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-14 16:58:55.332502: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:55.332507: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_backward_lstm_2_while_body_42951
2020-05-14 16:58:55.332512: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-14 16:58:55.332544: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-14 16:58:58.205307: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-05-14 16:58:58.205687: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-05-14 16:59:00.586252: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: graph_to_optimize
2020-05-14 16:59:00.586377: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 420 nodes (-60), 591 edges (-90), time = 1090.02698ms.
2020-05-14 16:59:00.586382: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 420 nodes (0), 591 edges (0), time = 381.193ms.
2020-05-14 16:59:00.586385: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_backward_lstm_1_while_body_42308_frozen
2020-05-14 16:59:00.586390: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (-2), 155 edges (0), time = 3.961ms.
2020-05-14 16:59:00.586393: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (0), 155 edges (0), time = 2.811ms.
2020-05-14 16:59:00.586474: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_forward_lstm_1_while_cond_42041_frozen
2020-05-14 16:59:00.586484: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.298ms.
2020-05-14 16:59:00.586489: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.254ms.
2020-05-14 16:59:00.586492: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_backward_lstm_1_while_cond_42307_frozen
2020-05-14 16:59:00.586496: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.761ms.
2020-05-14 16:59:00.586499: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.358ms.
2020-05-14 16:59:00.586657: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_1_forward_lstm_1_while_body_42042_frozen
2020-05-14 16:59:00.586668: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (-2), 155 edges (0), time = 4.465ms.
2020-05-14 16:59:00.586673: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (0), 155 edges (0), time = 2.419ms.
2020-05-14 16:59:00.586678: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_forward_lstm_2_while_body_42685_frozen
2020-05-14 16:59:00.586683: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (-2), 155 edges (0), time = 4.247ms.
2020-05-14 16:59:00.586687: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (0), 155 edges (0), time = 2.45ms.
2020-05-14 16:59:00.586790: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_backward_lstm_2_while_body_42951_frozen
2020-05-14 16:59:00.586802: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (-2), 155 edges (0), time = 4.046ms.
2020-05-14 16:59:00.586808: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 107 nodes (0), 155 edges (0), time = 2.454ms.
2020-05-14 16:59:00.586813: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_backward_lstm_2_while_cond_42950_frozen
2020-05-14 16:59:00.586817: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.334ms.
2020-05-14 16:59:00.586820: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.245ms.
2020-05-14 16:59:00.586958: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:814] Optimization results for grappler item: model_1_bidirectional_2_forward_lstm_2_while_cond_42684_frozen
2020-05-14 16:59:00.586967: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.286ms.
2020-05-14 16:59:00.586987: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816]   constant_folding: Graph size after: 14 nodes (0), 4 edges (0), time = 0.247ms.

``
**Any other info / logs**
The model comprises of multiple LSTM, and some other custom layers. Without quantization, the model conversion works only with the experimental_new_converter set to true. Removing the LSTM layer fixes the issue"
39560,Suggest to the user to define a cache volume for bazel,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/install/source#docker_linux_builds

## Description of issue (what needs changing):
We the current info about Docker build from source we will lost the cache when we destroy the container
### Clear description
The bazel cache will be stored in `/root/.cache/bazel/` that it is ephemeral and so it will lost if the container it is removed. So you are going to lost hours of builds just if you will restart the image with a new container. You need to suggest to mount an host volume for the bazel cache.

For example, why should someone use this method? How is it useful?"
39559,ImportError: cannot import name 'tf2' which type of error is that                                          tensorflow version=1.12 and python version=3.7,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
39558,Tf Lite conversion error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 2.1.0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FILL, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

```

**Standalone code to reproduce the issue** 
import tensorflow as tf
modelIMU = tf.keras.Sequential()
# inputs: A 3D tensor, with shape [batch, timesteps, feature].
modelIMU.add(GRU(3, input_shape=(2,6,),return_sequences=True))
modelIMU.add(GRU(3))
modelIMU.add(Dense(6))
modelIMU.compile(loss='mean_squared_error', optimizer='adam')
print(modelIMU.summary())

converter = tf.lite.TFLiteConverter.from_saved_model('modelIMU')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
quantized_model = converter.convert()
open(""modelIMU.tflite"", ""wb"").write(quantized_model)
Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
39557,Error in tflite conversion,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.0.0


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FILL, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""/home/momtaz/PycharmProjects/untitled2/venv/bin/toco_from_protos"", line 10, in <module>
    sys.exit(main())
  File ""/home/momtaz/PycharmProjects/untitled2/venv/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/momtaz/PycharmProjects/untitled2/venv/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/momtaz/PycharmProjects/untitled2/venv/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/momtaz/PycharmProjects/untitled2/venv/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/home/momtaz/PycharmProjects/untitled2/venv/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FILL, FULLY_CONNECTED, PACK, RESHAPE, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.

```

**Standalone code to reproduce the issue** 
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('models/modelIMU')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
quantized_model = converter.convert()
open(""modelIMU.tflite"", ""wb"").write(quantized_model)

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
39556,Much worse performance when using mixed precision training (using tensorflow.keras policy),"Hi! 

I'm training an image generative resnet-based model using MSE loss and Adam optimizer. I'm using the API tf.data to feed the model as fast as I can. Once I achieved a 100% GPU usage during training, I'm seeking for speeding up the process even more by using the experimental mixed_precision Keras API. I've previously tested this API by running this [tutorial](https://github.com/tlkh/pycon-sg19-tensorflow-tutorial) and it works as expected, getting an increase in performance of aproximatley 2x. However, when trying it with my model, the performance is much worse than using regular fp32 computation. I suspect there is something wrong with my model, however, I've tried to follow all the TensorCore performance considerations in order to correclty set the model's hyperparameters, such as channels and batch size being divisible by 8 and input images cast to float16 without any success

**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04.4 LTS
- TensorFlow installed from (container): tensorflow/tensorflow:nightly-gpu
- TensorFlow version (use command below): Tensorflow:  2.3.0-dev20200514
- Python version: 3.6.9
- CUDA/cuDNN version: V10.1.243 / CUDNN=7.6.4.38-1
- GPU model and memory: RTX 2080 8GB

**Describe the current behavior**
Worse performance using mixed precision Keras API than using regular fp32 computation.
**Describe the expected behavior**
Performance increases of aproximately 1.5x compared to fp32 computation.
**Standalone code to reproduce the issue**
[Colab](https://colab.research.google.com/drive/12sx2GQN5kwRke7fPkj5TkgtugN5y1X44?usp=sharing)

**Other info / logs** 
`Compute dtype: float16
Variable dtype: float32
Keras:  2.2.4-tf
Tensorflow:  2.3.0-dev20200514
Image format:  channels_last
2020-05-14 19:34:27.261423: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 19:34:27.261615: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1544] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce RTX 2080 computeCapability: 7.5
coreClock: 1.815GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s
2020-05-14 19:34:27.261632: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 19:34:27.261652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 19:34:27.261663: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 19:34:27.261672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 19:34:27.261682: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 19:34:27.261692: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 19:34:27.261701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 19:34:27.261732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 19:34:27.261931: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 19:34:27.262108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0
2020-05-14 19:34:27.262119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 19:34:27.262124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 
2020-05-14 19:34:27.262127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N 
2020-05-14 19:34:27.262168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 19:34:27.262368: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 19:34:27.262551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/device:GPU:0 with 6300 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)
Device : /device:CPU:0
         type : CPU
         desc :

Device : /device:XLA_CPU:0
         type : XLA_CPU
         desc :device: XLA_CPU device

Device : /device:XLA_GPU:0
         type : XLA_GPU
         desc :device: XLA_GPU device

Device : /device:GPU:0
         type : GPU
         desc :device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5

2020-05-14 19:34:31.803789: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.
2020-05-14 19:34:31.803820: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2020-05-14 19:34:31.804185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1
2020-05-14 19:34:31.873411: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed
Epoch 1/3
2020-05-14 19:34:39.768955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 19:34:40.485835: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 19:34:43.260095: I tensorflow/compiler/jit/xla_compilation_cache.cc:314] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.`

Thank you in advance."
39554,tf.convert_to_tensor throws ValueError for tf.float64 tensor and dtype=tf.float32 instead of silently casting,"I *think* this is a bug, at least it is inconsistent behavior.

**System information**
- custom code? no, just the minimal example to reproduce the error message
- OS Platform and Distribution: Linux Ubuntu 18.04
- PC
- TensorFlow installed from (source or binary): binary
- TensorFlow version: v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.7.6 (default, Jan  8 2020, 19:59:22)

**Describe the current behavior**
tf.convert_to_tensor accepts numpy np.float64 arrays when dtype is tf.float32 (and returns a tf.float32 Tensor).

If the argument however is a tensor of type tf.float64, an error is thrown instead of returning a tf.float32 tensor
**Describe the expected behavior**
I would expect **tf.convert_to_tensor** to treat the float64-Tensor and the float64-ndarray the same. In my understanding it is in general use for all tensorflow operators and should accept a broad range of inputs. I see no reason to reject Tensors, but accept ndarrays.
**Standalone code to reproduce the issue**
Script:
```
import numpy as np
import tensorflow as tf
v_np = np.ones(shape=(3), dtype=np.float64)
print(""A"",v_np,type(v_np[0]))
v_tf = tf.constant([1.0,1,1], dtype=tf.float64)
print(""B"",v_tf,type(v_tf[0].numpy()))
print(""C"",tf.convert_to_tensor(v_np,dtype=tf.float32))
print(""D"",tf.convert_to_tensor(v_tf,dtype=tf.float32))
```
Output:
```
A [1. 1. 1.] <class 'numpy.float64'>
B tf.Tensor([1. 1. 1.], shape=(3,), dtype=float64) <class 'numpy.float64'>
C tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)
Traceback (most recent call last):
  File ""/home/user/TENSORFLOW/myapp-tf/bug.py"", line 9, in <module>
    print(""D"",tf.convert_to_tensor(v_tf,dtype=tf.float32))
  File ""/home/user/miniconda2/envs/tf20b/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1256, in convert_to_tensor_v2
    as_ref=False)
  File ""/home/user/miniconda2/envs/tf20b/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py"", line 1290, in convert_to_tensor
    (dtype.name, value.dtype.name, value))
ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float64: <tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 1., 1.])>
```

"
39553,Error During Model Conversion from Keras to TFLite : AttributeError: 'Model' object has no attribute '_get_save_spec',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):18.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):2.2.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/1APB9ATKjQHfOq8kq2_SvvNYYz_AVnnZh
```
# Copy and paste here the exact command
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)

**The output from the converter invocation**
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-297-83ed6c530300> in <module>
----> 1 converter = tf.lite.TFLiteConverter.from_keras_model(model)

~/hdd/learning/dl/venv/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model(cls, model)
    430       # signature including the batch dimension specified by the user.
    431       input_signature = _saving_utils.model_input_signature(
--> 432           model, keep_original_batch_size=True)
    433 
    434     func = _saving_utils.trace_model_call(model, input_signature)
~/hdd/learning/dl/venv/lib/python3.6/site-packages/tensorflow/python/keras/saving/saving_utils.py in model_input_signature(model, keep_original_batch_size)
     75     TensorSpecs. This list does not contain the `training` argument.
     76   """"""
---> 77   input_specs = model._get_save_spec(dynamic_batch=not keep_original_batch_size)  # pylint: disable=protected-access
     78   if input_specs is None:
     79     return None

AttributeError: 'Model' object has no attribute '_get_save_spec'

![Screenshot from 2020-05-15 00-17-16](https://user-images.githubusercontent.com/17315769/81973844-5b2e7280-9642-11ea-9424-8ed29566c46e.png)
"
39552,TensorFlow Hub hosted Mv1/Mv2 fails execution in tflite runtime,"The mv1 and mv2 quantized models hosted here 
(mv1-224 0.25,0.5,0.75,1.0 and mv2-224)
(https://www.tensorflow.org/lite/guide/hosted_models) fails execution on the tflite runtime on my custom app modified from TensorFlow's demo app, running on a Google Pixel 3a mobile. 

The error was,
java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 150528 bytes and a Java Buffer with 602112 bytes.

It seems like there's an int8 vs float32 mismatch somewhere. However, if I manually take an mv2 and use the TFLite Converter (with quantization flag) to convert to a quantized model, it runs perfectly (both the models have the same size (hosted vs converted) hence ruling any issues of Converter not quantizing the model).

Can you please let me know why the hosted model could be erring out? 
"
39550,TPU issue using Colab - NotFoundError: 'AnonymousSeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-c0cd0e2d-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:TakeDataset],"**System information**
Pip installed TensorFlow 
TensorFlow version (use command below): tf-nightly/ 2.2.0-dev20200502
Python version: 3.6.9


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
When I try to run my model I get a NotFoundError when getting my dataset. The code works with a GPU but does not work with TPU.  I was previously getting a NotFoundError that had to do with PyFunc.

**Describe the expected behavior**
Epochs should run and give a result.

**Standalone code to reproduce the issue**
```
import requests
import os
url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/2.2.0-dev20200312'
resp = requests.post(url)
print(resp)
%pip install tf-nightly==2.2.0-dev20200505
```

```
import tensorflow as tf
from tensorflow.keras import backend as K
import os
import PIL
import csv
import shutil
import numpy as np
import sys
from PIL import Image
from tensorflow.keras import backend as K
import gc

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras.applications.inception_v3 import InceptionV3
# from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2

from tensorflow.keras.layers import Dense, Activation, Flatten
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

#*********************** tried this first ***************************************
# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
# tf.config.experimental_connect_to_cluster(resolver)
# # This is the TPU initialization code that has to be at the beginning.
# tf.tpu.experimental.initialize_tpu_system(resolver)
# strategy = tf.distribute.experimental.TPUStrategy(resolver)
#********************************************************************************

tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection
print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.experimental.TPUStrategy(tpu)

print(""REPLICAS: "", strategy.num_replicas_in_sync)


from PIL import Image
print(Image.__file__)

# import Image
print(Image.__file__)
```

#clone and mount drives as needed

```
def make_generator():

    train_datagen = ImageDataGenerator(rescale=1/255, horizontal_flip=True, rotation_range=90)
    
    train_generator = train_datagen.flow_from_directory(
        '/content/plant-path/tfdir/train/',  # This is the source directory for training images
        target_size=(400, 400),  # All images will be resized to 150x150
        batch_size=1, 
        
        class_mode='sparse')
    

    return train_generator

def create_model():
         pre_trained_model = InceptionV3(input_shape = (400, 400,3), include_top = False, weights = 'imagenet')

# pre_trained_model = InceptionResNetV2(include_top=False, weights='imagenet', input_tensor=None, input_shape=(1024, 1024,3))

         for layer in pre_trained_model.layers:
                    if layer.name == 'mixed1':
                                    break
# #     print(layer.name)
                    layer.trainable = False

                    last_layer = pre_trained_model.get_layer('mixed7')
                    last_output = last_layer.output

         from tensorflow.keras.optimizers import RMSprop
         from tensorflow.keras import regularizers
 

         x = Flatten()(last_output)
         x = layers.Dense(1024,  activation= 'relu')(x)
         x = layers.Dropout(.2)(x)
         x = layers.Dense(4, activation= 'softmax')(x)
         modelin = Model(pre_trained_model.input, x)
         return modelin



def get_callbacks(name_weights, patience_lr):
    mcp_save = ModelCheckpoint(name_weights, save_best_only=True, monitor='val_acc', mode='max')
#     reduce_lr_loss = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')
    return [mcp_save] #, reduce_lr_loss]


batch_size = 456
def scale(image, label):
    image = tf.squeeze(image)
    return image, label  

def get_dataset(i, batchsize = batch_size):
  dataset = tf.data.Dataset.from_generator(make_generator, (tf.float32, tf.float32),
     (tf.TensorShape([400, 400, 3]), tf.TensorShape([None])))
  dataset = dataset.shuffle(buffer_size = 2280)  
  dataset = dataset.cache()
  val = dataset.skip(i*456).take(456).batch(batch_size, drop_remainder=True).prefetch(4)
  
  train = dataset.skip(i*456+456).take(1824).concatenate(dataset.take(456*i)).batch(batch_size, drop_remainder=True).prefetch(15)
  return train, val
```

#K-fold algo


```
for i in range(5):
  
  #********* this was created to trouble shoot but did not resolve issue
  # dataset = tf.data.Dataset.from_generator(make_generator, (tf.float32, tf.float32),
  #    (tf.TensorShape([1, 400, 400, 3]), tf.TensorShape([None])))
  # dataset = dataset.shuffle(buffer_size = 2280)
    
    # val = dataset.skip(i*456).take(456).batch(batch_size, drop_remainder=True).prefetch(4)
  
    # train = dataset.skip(i*456+456).take(1824).concatenate(dataset.take(456*i)).batch(batch_size, drop_remainder=True).prefetch(15)  

  #******************************************************  
    
    train, val = get_dataset(i) 
    
    name_weights = ""/content/drive/My Drive/Plant/final_model_fold_D512_I400_mix_1_7_"" + str(i) + "".{epoch:02d}-{val_acc:.2f}.h5""
    callbacks = get_callbacks(name_weights = name_weights, patience_lr=10)

    
    with strategy.scope():
       modelinc = create_model()
       modelinc.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['acc'])

    modelinc.fit(
                dataset,
                # batch_size = batch_size,
                # steps_per_epoch=1824/batch_size,
                epochs=25,
                # shuffle=True,
                # verbose=1,
                # validation_data = val
                 ) #,
                #  callbacks = callbacks)
    
    print(modelinc.evaluate(val)) 
    K.clear_session()
    
    del name_weights
    del callbacks

    gc.collect()
```

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-7-4b2615ce6f75> in <module>()
     15   #******************************************************
     16 
---> 17     train, val = get_dataset(i)
     18 
     19     name_weights = ""/content/drive/My Drive/Plant/final_model_fold_D512_I400_mix_1_7_"" + str(i) + "".{epoch:02d}-{val_acc:.2f}.h5""

5 frames
<ipython-input-6-c4546546e618> in get_dataset(i, batchsize)
     56   dataset = dataset.shuffle(buffer_size = 2280)
     57   dataset = dataset.cache()
---> 58   val = dataset.skip(i*456).take(456).batch(batch_size, drop_remainder=True).prefetch(4)
     59 
     60   train = dataset.skip(i*456+456).take(1824).concatenate(dataset.take(456*i)).batch(batch_size, drop_remainder=True).prefetch(15)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in take(self, count)
   1271       Dataset: A `Dataset`.
   1272     """"""
-> 1273     return TakeDataset(self, count)
   1274 
   1275   def skip(self, count):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py in __init__(self, input_dataset, count)
   3727         input_dataset._variant_tensor,  # pylint: disable=protected-access
   3728         count=self._count,
-> 3729         **self._flat_structure)
   3730     super(TakeDataset, self).__init__(input_dataset, variant_tensor)
   3731 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in take_dataset(input_dataset, count, output_types, output_shapes, name)
   6474       return _result
   6475     except _core._NotOkStatusException as e:
-> 6476       _ops.raise_from_not_ok_status(e, name)
   6477     except _core._FallbackException:
   6478       pass

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6810   message = e.message + ("" name: "" + name if name is not None else """")
   6811   # pylint: disable=protected-access
-> 6812   six.raise_from(core._status_to_exception(e.code, message), None)
   6813   # pylint: enable=protected-access
   6814 

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: 'AnonymousSeedGenerator' is neither a type of a primitive operation nor a name of a function registered in binary running on n-c0cd0e2d-w-0. Make sure the operation or function is registered in the binary running in this process. [Op:TakeDataset]
"
39549,TypeError: unsupported operand type(s) for *: 'LSTM' and 'int',"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.


         self.kernel = self.add_weight(shape=(input_dim, self.units * 4),
                                      name='kernel',
                                      initializer=self.kernel_initializer,

TypeError: unsupported operand type(s) for *: 'LSTM' and 'int'"
39547,keras training parameter value incorrect,"tensorflow ver: 2.1.0

```python
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):

    def __init__(self):
        super(MyModel, self).__init__()
        self.dense1 = tf.keras.layers.Dense(4, activation=tf.nn.relu)
        self.dense2 = tf.keras.layers.Dense(5, activation=tf.nn.softmax)
        self.dropout = tf.keras.layers.Dropout(0.5)

    def call(self, inputs, training=None):
        x = self.dense1(inputs)
        if training is True:
            print(""in training"")
            x = self.dropout(x, training=training)
        elif training is None:
            print(""training None"")
        else:
            print(""not in training"")
            
        return self.dense2(x)

model = MyModel()

optimizer = tf.keras.optimizers.Adam(1e-4)
loss = tf.keras.losses.CategoricalCrossentropy()
model.compile(optimizer, loss)
x = tf.random.normal((5,))
y = tf.ones((5,))
model.fit(x, y, epochs=1)
```
The system report ""not in training first"", then ""in training"".
![image](https://user-images.githubusercontent.com/731496/81953648-84e39b80-963a-11ea-8c4e-ac5b49afcb9e.png)
"
39545,Efficient allreduce is not supported Issue,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2 (stable)
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1 / 7.6.4.38
- GPU model and memory: Nvidia Tesla V100 /32GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
v2.2.0-rc4-8-g2b96f3662b 2.2.0

**Describe the current behavior**
I have written a custom loop to train a model using tf.strategy.MirroredStrategy().
Currently, I am training on a machine, equipped with 2 Nvidia V100 Tesla cards. The training is started and continuing as expected. But t I am getting a Warning from both of the GPUS, I feel this can harm the performance of the GPUs

2020-05-14 15:04:14.880452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30261 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:40:00.0, compute capability: 7.0)
2020-05-14 15:04:14.883196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30261 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0)
WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices
WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices
2020-05-14 15:05:55.050229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
"
39544,NEED FAST HELP! Tensorflow Training Error,"Hi,

I am trying to code an AI with an linear regression algorithm with tensorflow.
But my model gives an error message when i try to train it:

This is my code
linear_est.train(train_input_fn)  # train
result = linear_est.evaluate(eval_input_fn)  # get model metrics/stats by testing on tetsing data

clear_output()  # clears consoke output
print(result)  # the result variable is simply a dict of stats about our model

And it says:
InvalidArgumentError: assertion failed: [Labels must be <= n_classes - 1] [Condition x <= y did not hold element-wise:] [x (head/losses/Cast:0) = ] [[8][10][2]...] [y (head/losses/check_label_range/Const:0) = ] [1]
	 [[{{node Assert}}]]

Take a look at the rest if you want to here:
https://drive.google.com/open?id=1kN3fnQQKJk2VdIZvj1XFrxg73C8BKaWz

Big Thanks and best Regards
Justin
"
39542,Cannot use TimeDistributed with hub.KerasLayer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
When trying to apply the `tf.keras.layers.TimeDistributed` layer on top of a `tensorflow_hub.KerasLayer`, an exception (and not a useful one) is being raised

**Describe the expected behavior**
Just like any other `tf.keras.layers.Layer`, I'd expect one generated by the `tensorflow_hub.KerasLayer` to work

**Standalone code to reproduce the issue**
```python3
import os

import numpy as np
import tensorflow as tf
import tensorflow_hub as hub

os.environ[""TFHUB_CACHE_DIR""] = '/tmp/tfhub'

# Create model
model = tf.keras.Sequential([tf.keras.layers.TimeDistributed(hub.KerasLayer(""https://tfhub.dev/google/bit/s-r101x1/1"",
                                                                            trainable=False)),
                             tf.keras.layers.LSTM(units=512),
                             tf.keras.layers.Dense(units=128,
                                                   activation=tf.nn.relu),
                             tf.keras.layers.Dense(units=1,
                                                   activation=tf.nn.sigmoid)])

model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
              metrics=['accuracy'])

# Fit
model.fit(x=np.random.randint(low=0, high=256, size=(1000, 10, 56, 56, 3)).astype(float),
          y=np.random.randint(low=0, high=2, size=(1000,)).astype(float), epochs=1)
```

**Other info / logs** Include any logs or source code that would be helpful to
Traceback: 

2020-05-14 15:40:50.293457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-14 15:40:50.314782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-14 15:40:50.315236: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.683GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-05-14 15:40:50.315361: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315403: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315438: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315474: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315508: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315542: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315577: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory
2020-05-14 15:40:50.315585: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-05-14 15:40:50.315827: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-14 15:40:50.337745: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3493215000 Hz
2020-05-14 15:40:50.338512: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f1b94000b60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-14 15:40:50.338545: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-14 15:40:50.341227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 15:40:50.341251: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      
Traceback (most recent call last):
  File "".../site-packages/IPython/core/interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-0f6a34a2cfff>"", line 27, in <module>
    y=np.random.randint(low=0, high=2, size=(1000,)).astype(float), epochs=1)
  File "".../site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
    return method(self, *args, **kwargs)
  File "".../site-packages/tensorflow/python/keras/engine/training.py"", line 848, in fit
    tmp_logs = train_function(iterator)
  File "".../site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File "".../site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File "".../site-packages/tensorflow/python/eager/def_function.py"", line 506, in _initialize
    *args, **kwds))
  File "".../site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File "".../site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File "".../site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File "".../site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File "".../site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File "".../site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
NotImplementedError: in user code:
    .../site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    .../site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    .../site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    .../site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    .../site-packages/tensorflow/python/keras/engine/training.py:531 train_step  **
        y_pred = self(x, training=True)
    .../site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    .../site-packages/tensorflow/python/keras/engine/sequential.py:291 call
        outputs = layer(inputs, **kwargs)
    .../site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    .../site-packages/tensorflow/python/keras/layers/wrappers.py:246 call
        output_shape = self.compute_output_shape(input_shape).as_list()
    .../site-packages/tensorflow/python/keras/layers/wrappers.py:190 compute_output_shape
        child_output_shape = self.layer.compute_output_shape(child_input_shape)
    .../site-packages/tensorflow/python/keras/engine/base_layer.py:699 compute_output_shape
        raise NotImplementedError
    NotImplementedError: 

"
39540,Weights for model net_10 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.,"Just added a build to model on checkpointing tutorial instead of creating the state in `__init__` as done in the [tutorial](https://www.tensorflow.org/guide/checkpoint) and im getting this error
```
class Net(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net, self).__init__()
    #self.l1 = tf.keras.layers.Dense(5)
  def build(self,input_shape):
    self.l1 = tf.keras.layers.Dense(5)
    self.dummy = tf.Variable(trainable=True,initial_value=tf.keras.initializers.glorot_normal()(shape=(1,),dtype=tf.float32))
    print('built layers')
  def call(self, x):
    return self.l1(x)

net = Net()
net.build([1,])
net.save_weights('easy_checkpoint')
```
output:
```
built layers
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-31-3b54dc506ffd> in <module>
      1 net = Net()
      2 net.build([1,])
----> 3 net.save_weights('easy_checkpoint')

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in save_weights(self, filepath, overwrite, save_format)
   1111         ValueError: For invalid/unknown format arguments.
   1112     """"""
-> 1113     self._assert_weights_created()
   1114     filepath_is_h5 = _is_hdf5_filepath(filepath)
   1115     if save_format is None:

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in _assert_weights_created(self)
   1560                        'Weights are created when the Model is first called on '
   1561                        'inputs or `build()` is called with an `input_shape`.' %
-> 1562                        self.name)
   1563 
   1564   def _graph_network_add_loss(self, symbolic_loss):

ValueError: Weights for model net_10 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.
```
Even though the build method is ran

Here is my hunch: The problem with my code is that the build does not execute the build of `self.l1` but just creates it. Things do work out fine if i add `self.l1` creation in `__init__` and call `super().__build__()` as the first line in Net's build. Things make sense so far but, the code fails again if i replace `super().build(input_shape)` with `self.l1.build(input_shape)`. Also, the code belows shows that all the variables are actually there. So, i am lost again. Any help is much appreciated
```
tf.random.set_seed(42)
class Net1(tf.keras.Model):
  """"""A simple linear model.""""""
  def __init__(self):
    super(Net1, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)
  def build(self,input_shape):
    super().build(input_shape)
    self.dummy = tf.Variable(trainable=True,initial_value=tf.keras.initializers.glorot_normal()(shape=(1,),dtype=tf.float32))
    print(self.variables)
  def call(self, x):
    return self.l1(x)

net = Net1()
net.build((10,1))
print('*'*50)
print(net.variables)

output:
[<tf.Variable 'dense_56/kernel:0' shape=(1, 5) dtype=float32, numpy=
array([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],
      dtype=float32)>, <tf.Variable 'dense_56/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.09575049], dtype=float32)>]
**************************************************
[<tf.Variable 'dense_56/kernel:0' shape=(1, 5) dtype=float32, numpy=
array([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],
      dtype=float32)>, <tf.Variable 'dense_56/bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>, <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.09575049], dtype=float32)>]
```

whereas,
```
tf.random.set_seed(42)
class Net1(tf.keras.Model):
  """"""A simple linear model.""""""

  def __init__(self):
    super(Net1, self).__init__()
    self.l1 = tf.keras.layers.Dense(5)
  def build(self,input_shape):
    self.l1.build(input_shape)
    self.dummy = tf.Variable(trainable=True,initial_value=tf.keras.initializers.glorot_normal()(shape=(1,),dtype=tf.float32))
    print('variables',self.l1.variables,self.dummy)
  def call(self, x):
    return self.l1(x)

net = Net1()
net.build((10,1))
print(net.variables)

output:
variables [<tf.Variable 'kernel:0' shape=(1, 5) dtype=float32, numpy=
array([[ 0.3291242 , -0.11798644, -0.294235  , -0.07103491, -0.9326792 ]],
      dtype=float32)>, <tf.Variable 'bias:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>] <tf.Variable 'Variable:0' shape=(1,) dtype=float32, numpy=array([0.09575049], dtype=float32)>
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-77-35561efcdc2f> in <module>
     15 net = Net1()
     16 net.build((10,1))
---> 17 print(net.variables)

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in variables(self)
   1965       A list of variables.
   1966     """"""
-> 1967     return self.weights
   1968 
   1969   @property

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in weights(self)
    498       A list of variables.
    499     """"""
--> 500     return self._dedup_weights(self._undeduplicated_weights)
    501 
    502   @property

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in _undeduplicated_weights(self)
    503   def _undeduplicated_weights(self):
    504     """"""Returns the undeduplicated list of all layer variables/weights.""""""
--> 505     self._assert_weights_created()
    506     weights = []
    507     for layer in self._layers:

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py in _assert_weights_created(self)
   1560                        'Weights are created when the Model is first called on '
   1561                        'inputs or `build()` is called with an `input_shape`.' %
-> 1562                        self.name)
   1563 
   1564   def _graph_network_add_loss(self, symbolic_loss):

ValueError: Weights for model net1_40 have not yet been created. Weights are created when the Model is first called on inputs or `build()` is called with an `input_shape`.
```
"
39539,[TFLite] Failed to create Hexagon delegate on Oneplus 5,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop Os 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2
- Python version: 3.7

**Describe the current behavior**

Device Information -

    adb shell getprop ro.product.device
    OnePlus5

    adb shell getprop ro.board.platform
    msm8998

($APP_ROOT)/app/src/main/jniLibs

```
├── arm64-v8a
│   ├── libhexagon_nn_skel.so
│   ├── libhexagon_nn_skel_v65.so
│   └── libhexagon_nn_skel_v66.so
└── armeabi-v7a
    ├── libhexagon_nn_skel.so
    ├── libhexagon_nn_skel_v65.so
    └── libhexagon_nn_skel_v66.so
```
($APP_ROOT)/app/build.gradle

    apply plugin: 'com.android.application'  
    apply plugin: 'kotlin-android'  
    apply plugin: 'kotlin-android-extensions'  
      
    android {  
      
      compileSdkVersion 29  
      buildToolsVersion ""29.0.3""  
      
      defaultConfig {  
      applicationId ""com.example.sr_tflite""  
      minSdkVersion 25  
      targetSdkVersion 29  
      versionCode 1  
      versionName ""1.0""  
      ndk {  
      abiFilters 'armeabi-v7a', 'arm64-v8a'  
      }  
      
      testInstrumentationRunner ""androidx.test.runner.AndroidJUnitRunner""  
      }  
      
      buildTypes {  
      release {  
      minifyEnabled false  
      proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'  
      }  
      aaptOptions {  
      noCompress ""tflite""  
      }  
     }  
    // To inline the bytecode built with JVM target 1.8 into  
    // bytecode that is being built with JVM target 1.6. (e.g. navArgs)  
      
      
      compileOptions {  
      sourceCompatibility JavaVersion.VERSION_1_8  
      targetCompatibility JavaVersion.VERSION_1_8  
      }  
      kotlinOptions {  
      jvmTarget = ""1.8""  
      }  
      
    }  
      
    dependencies {  
      def tfl_version = ""0.0.0-nightly""  
      implementation fileTree(dir: 'libs', include: ['*.jar'])  
        implementation ""org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version""  
      implementation 'androidx.appcompat:appcompat:1.1.0'  
      implementation 'androidx.core:core-ktx:1.2.0'  
      implementation 'com.google.android.material:material:1.1.0'  
      implementation 'androidx.constraintlayout:constraintlayout:1.1.3'  
      implementation 'androidx.navigation:navigation-fragment-ktx:2.0.0'  
      implementation 'androidx.navigation:navigation-ui-ktx:2.0.0'  
      implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly'  
      implementation 'org.tensorflow:tensorflow-lite-hexagon:0.0.0-nightly'  
      testImplementation 'junit:junit:4.12'  
      androidTestImplementation 'androidx.test.ext:junit:1.1.1'  
      androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'  
      implementation(""org.tensorflow:tensorflow-lite:${tfl_version}"") { changing = true }  
      implementation(""org.tensorflow:tensorflow-lite-gpu:${tfl_version}"") { changing = true }  
      implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'  
    }

I followed the [TensorFlow Lite Hexagon delegate](https://www.tensorflow.org/lite/performance/hexagon_delegate) guide on Oneplus 5. Tensorflow Lite was failed to create Hexagon delegate.

Log

    020-05-14 16:26:19.906 27848-27848/com.example.sr_tflite I/System.out: *******/data/app/com.example.sr_tflite-WR6XEFKEipjZ_vWQYv9MSQ==/lib/arm64
    2020-05-14 16:26:19.947 27848-27848/com.example.sr_tflite V/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1859: Successfully created user PD on domain 0 (attrs 0x0)
    2020-05-14 16:26:19.960 27848-28029/com.example.sr_tflite V/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:270: rpc latency thread start
    2020-05-14 16:26:19.961 27848-28027/com.example.sr_tflite E/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:729:Error 45: fopen failed for oemconfig.so. (No such file or directory)
    2020-05-14 16:26:19.961 27848-28027/com.example.sr_tflite E/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/apps_std_imp.c:729:Error 45: fopen failed for libhexagon_nn_skel.so. (No such file or directory)
    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:983: Error fffffffb: remote handle open domain failed. domain 0, name file:///libhexagon_nn_skel.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=adsp, dlerror cannot open oemconfig.so
    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:920: Error ffffffff: remote handle invoke failed. domain 0, handle 0, sc 1010200, pra 0x7fde45af88
    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1034: Error ffffffff: remote handle close failed. error 
    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite D/com.example.sr_tflite: vendor/qcom/proprietary/commonsys-intf/adsprpc/src/fastrpc_apps_user.c:1020: Error fffffffb: remote handle64 open failed. name file:///libhexagon_nn_skel.so?hexagon_nn_domains_skel_handle_invoke&_modver=1.0&_dom=adsp
    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite W/tflite: Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide.
    2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite I/tflite: Hexagon Delegate is not supported.
    2020-05-14 16:26:19.962 27848-27848/com.example.sr_tflite D/AndroidRuntime: Shutting down VM
    2020-05-14 16:26:19.966 27848-27848/com.example.sr_tflite E/AndroidRuntime: FATAL EXCEPTION: main
        Process: com.example.sr_tflite, PID: 27848
        java.lang.UnsupportedOperationException: This Device doesn't support Hexagon DSP execution.
            at org.tensorflow.lite.experimental.HexagonDelegate.<init>(HexagonDelegate.java:40)
            at com.example.sr_tflite_1.MainActivity$onCreate$1.onClick(MainActivity.kt:97)
            at android.view.View.performClick(View.java:6669)
            at android.view.View.performClickInternal(View.java:6638)
            at android.view.View.access$3100(View.java:789)
            at android.view.View$PerformClick.run(View.java:26145)
            at android.os.Handler.handleCallback(Handler.java:873)
            at android.os.Handler.dispatchMessage(Handler.java:99)
            at android.os.Looper.loop(Looper.java:193)
            at android.app.ActivityThread.main(ActivityThread.java:6898)
            at java.lang.reflect.Method.invoke(Native Method)
            at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:537)
            at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)
    2020-05-14 16:26:19.977 27848-28030/com.example.sr_tflite D/OSTracker: OS Event: crash
    2020-05-14 16:26:19.995 27848-27848/com.example.sr_tflite I/Process: Sending signal. PID: 27848 SIG: 9

I have downloaded the latest hexagon_nn_skel.run (v1.17) from the page but it still says `Failed to fetch Hexagon NN version. This might be because you're using incompatible versions of libhexagon_interface and libhexagon_nn_skel. You must use compatible versions. Refer to Tensorflow Lite Hexagon Delegate Guide. 2020-05-14 16:26:19.961 27848-27848/com.example.sr_tflite I/tflite: Hexagon Delegate is not supported.` Do i need to pull some other so files from somewhere else or my device is not supported.
Thanks

**Describe the expected behavior**

The DSP delegate should be initialized as my SoC is in the list of supported hardwares.
"
39538,JNI GPU Bindings Seem Broken,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.14, 1.15, nightly
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.2
- GPU model and memory: Tesla V100



**Describe the problem**

So, I might be crazy, but I think the more recent released builds of `libtensorflow_jni` which have GPU support do not, in fact, have GPU support. Our team was running some performance testing on an instance with a V100, and we noticed that our process wasn't appearing in `nvidia-smi`. On my local machine, I pulled down and dissected the JAR and found something odd: `ldd` appears to not show any dynamic links to any CUDA libraries:
```bash
root@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_jni.so
	linux-vdso.so.1 (0x00007ffc9a9b3000)
	libtensorflow_framework.so.2 => /tf-contents/nightly/libtensorflow_framework.so.2 (0x00007fc889320000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fc889318000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc8891c9000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc8891a6000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc88919b000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fc888fb8000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fc888f9d000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc888dab000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fc8a6c00000)
root@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_framework.so
	linux-vdso.so.1 (0x00007ffdda5f5000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f0dabd9d000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f0dabd7a000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f0dabd74000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f0dabc25000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f0daba44000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f0daba29000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f0dab835000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f0dad944000)
root@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_framework.so.2
	linux-vdso.so.1 (0x00007ffec27c5000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f7db8332000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f7db830f000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f7db8309000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f7db81ba000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f7db7fd9000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f7db7fbe000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f7db7dca000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f7db9ed9000)
root@ffa0a7ff3eed:/# ldd /tf-contents/nightly/libtensorflow_framework.so.2.0.0
	linux-vdso.so.1 (0x00007ffffa3cd000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6f4328e000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6f4326b000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f6f43265000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6f43116000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6f42f35000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6f42f1a000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6f42d26000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f6f44e35000)
```
I am seeing the same thing for the latest Maven releases of `libtensorflow_jni_gpu`:
**v1.14.0**
```bash
root@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so
	linux-vdso.so.1 (0x00007fffea2c3000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f9456307000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f94562e4000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f94562de000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f945618f000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f9455fae000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f9455f93000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f9455d9f000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f9457f9e000)
root@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1
	linux-vdso.so.1 (0x00007fff117d5000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f4e97aca000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f4e97aa7000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f4e97aa1000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f4e97952000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f4e97771000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f4e97756000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f4e97562000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f4e99761000)
root@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1.14.0
	linux-vdso.so.1 (0x00007fffa79f0000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fe6f6834000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fe6f6811000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fe6f680b000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fe6f66bc000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fe6f64db000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fe6f64c0000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fe6f62cc000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fe6f84cb000)
root@ffa0a7ff3eed:/# ldd /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_jni.so
	linux-vdso.so.1 (0x00007ffe7c2cc000)
	libtensorflow_framework.so.1 => /tf-contents/1.14.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 (0x00007f50738d8000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f50738d0000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f5073781000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f507375e000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f5073753000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f5073570000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f5073555000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f5073363000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f5087918000)
```

**v.1.15.0**
```bash
root@ffa0a7ff3eed:/# ldd /tf-contents/1.15.0/org/tensorflow/native/linux-x86_64/libtensorflow_jni.so
	linux-vdso.so.1 (0x00007ffe87dd4000)
	libtensorflow_framework.so.1 => /tf-contents/1.15.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1 (0x00007fbb4df46000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fbb4df3e000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fbb4ddef000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fbb4ddcc000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fbb4ddc1000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fbb4dbde000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fbb4dbc3000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fbb4d9d1000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fbb631f2000)
root@ffa0a7ff3eed:/# ldd /tf-contents/1.15.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so.1
	linux-vdso.so.1 (0x00007ffe5e258000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f010ef44000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f010ef21000)
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007f010ef1b000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f010edcc000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f010ebeb000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f010ebd0000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f010e9dc000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f0110c31000)
```

In comparison, take a look at the output for the contents of the 1.5.0 release (note, as mentioned earlier, this dissection was done on my local machine, so that is why there are so many `not found` entries corresponding to the CUDA libs):
```bash
root@ffa0a7ff3eed:/# ldd /tf-contents/1.5.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so
	linux-vdso.so.1 (0x00007ffec0f86000)
	libcublas.so.9.0 => not found
	libcuda.so.1 => not found
	libcudnn.so.7 => not found
	libcufft.so.9.0 => not found
	libcurand.so.9.0 => not found
	libcudart.so.9.0 => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007fed05cbb000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fed05b6c000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fed05b49000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fed05966000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fed0594b000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fed05759000)
	/lib64/ld-linux-x86-64.so.2 (0x00007fed06b9a000)
root@ffa0a7ff3eed:/# ldd /tf-contents/1.5.0/org/tensorflow/native/linux-x86_64/libtensorflow_jni.so
	linux-vdso.so.1 (0x00007ffd2dbd6000)
	libtensorflow_framework.so => /tf-contents/1.5.0/org/tensorflow/native/linux-x86_64/libtensorflow_framework.so (0x00007ff48ba0c000)
	libcublas.so.9.0 => not found
	libcusolver.so.9.0 => not found
	libcudart.so.9.0 => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff48ba02000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff48b9df000)
	libgomp.so.1 => not found
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff48b890000)
	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff48b885000)
	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff48b6a2000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff48b687000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff48b495000)
	/lib64/ld-linux-x86-64.so.2 (0x00007ff49463b000)
	libcublas.so.9.0 => not found
	libcuda.so.1 => not found
	libcudnn.so.7 => not found
	libcufft.so.9.0 => not found
	libcurand.so.9.0 => not found
	libcudart.so.9.0 => not found
```

These same differences were confirmed on our GPU instance. Am I crazy, or is this unexpected?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

See Above

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39537,[feature request] - please consider adding in gpumlib features to tensorflow,"**[feature request] - please consider adding in gpumlib features to tensorflow**

# Greetings

**I would like to point out to:**
- *incorporalting gpumlib features in tensorflow.*

**motivations for the assessment:**
- *gpumlib contains some interesting algorithms, that could perfectly enhance some features of tensorflow.*

**the source code for the framework can be found at:**
- *http://gpumlib.sourceforge.net/*"
39536,Failed to pass tf.data.Dataset object to multi-input tf.keras model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Linux Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
**NIL**
- TensorFlow installed from (source or binary):
**Binary**
- TensorFlow version (use command below):
**2.1.0**
- Python version:
**3.6**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
**10.1 / 7.6.4**
- GPU model and memory:
**RTX 2070 Super, 8GB RAM**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
```
ValueError: Failed to find data adapter that can handle input: (<class 'dict'> containing {""<class 'str'>""} keys and {""<class 'tensorflow.python.data.ops.dataset_ops.ZipDataset'>""} values), <class 'NoneType'>
```
An error is encountered when trying to pass multiple ```tf.data.Dataset``` objects to my multi-input Keras model. This is done by passing a dictionary with the keys corresponding to the name of each of my input layers and values corresponding to each ```tf.data.Dataset``` object.
I had no issue passing the same dataset object to a model taking in a single input, but when I tried to combine multiple Model instances into a single Model (for an ensemble CNN), this error is encountered.

My network and ensemble code:
```python
def Net(inputs):
    base_model = DenseNet121(include_top=False, weights='imagenet', input_tensor=inputs)
    output = base_model.get_layer(""pool3_conv"").output
    x = Conv2D(128, 3, activation='relu', padding='same')(output)
    x = BatchNormalization()(x)
    x = Conv2D(64, 3, activation='relu', padding='same')(x)
    x = BatchNormalization()(x)
    x = Flatten()(x)
    x = Dense(2, activation='softmax', name='clf_output')(x)

    model = tf.keras.models.Model(inputs=[base_model.input], outputs=[x])

    return model

def create_ensemble(models):
    for i in range(len(models)):
        # Each model is a Net object
        model = models[i]
        for layer in model.layers[1:]:
            layer.trainable = False
            layer._name = 'ensemble_' + str(i+1) + '_' + layer._name

    stack_inputs = [model.input for model in models]
    stack_outputs = [model.output for model in models]
    merge = Concatenate()(stack_outputs) 
    x = Dense(16, activation='relu')(merge)
    x = Dense(2, activation='softmax')(x)

    model = tf.keras.models.Model(inputs=stack_inputs, outputs=x, name='ensemble')

    return model
```
My training process is summarized as follows:
1. Train 5 instances of DenseNet121 models, **in sequence**
2. Create a new ```ensemble```, combining the outputs of the previously trained DenseNet121 models
3. Train the ensembled model **(error occurs on ```model.fit```)**

My training code:
```python
    IMAGE_DIR = os.path.join(DATA_DIR, ""images"")
    DEPTH_DIR = os.path.join(DATA_DIR, ""labels"")
    VAL_SPLIT = int(np.floor(0.1 * len(list(paths.list_images(IMAGE_DIR)))))
    print(f""Validation split is: {VAL_SPLIT} images"")

    imagePaths = sorted(list(paths.list_images(IMAGE_DIR)))
    depthPaths = sorted(list(paths.list_images(DEPTH_DIR)))
    labels = generate_labels(depthPaths)

    image_ds = tf.data.Dataset.from_tensor_slices(imagePaths)
    image_ds = create_image_dataset(
        image_ds, batch_size=BS, seed=42, training=True)

    labels_ds = tf.data.Dataset.from_tensor_slices(labels)
    labels_ds = create_labels_dataset(labels_ds, batch_size=BS, seed=42)

    # Splitting into training and validation dataset
    image_train, labels_train = image_ds.skip(VAL_SPLIT), labels_ds.skip(VAL_SPLIT)
    image_val, labels_val = image_ds.take(VAL_SPLIT), labels_ds.take(VAL_SPLIT)

    train_ds = tf.data.Dataset.zip((image_train, labels_train))
    val_ds = tf.data.Dataset.zip((image_val, labels_val))

    # Building the model
    models = []
    for i in range(1, N_MODELS+1):
        inputs = tf.keras.layers.Input(shape=[224, 224, 3], name=f'input_{i}')
        model = Net(inputs)
        models.append(model)

    print(""Checking model architecture: "")
    print(model.summary())

    opt = Lookahead(RAdam(lr=INIT_LR))
    opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)

    # Creating callbacks
    #earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto', restore_best_weights=True)

    reducelr = tf.keras.callbacks.LearningRateScheduler(reduce_lr)

    # Training the model
    print(""Training"")
    if args.pretrained == 0:
        for i in range(N_MODELS):
            model = models[i]
            model.compile(loss=""categorical_crossentropy"", 
                          optimizer=opt, 
                          metrics=[""accuracy""])

            checkpoint = tf.keras.callbacks.ModelCheckpoint(f""net_{i}_backup_weights.h5"", 
                                                            monitor='val_loss', 
                                                            verbose=1, 
                                                            save_weights_only=True, 
                                                            save_best_only=True)
            
            history = model.fit(train_ds, 
                                epochs=EPOCHS, 
                                verbose=1,
                                steps_per_epoch=(len(imagePaths)-VAL_SPLIT) // BS, 
                                validation_data=(val_ds), 
                                validation_steps=VAL_SPLIT // BS, 
                                callbacks=[checkpoint, reducelr]) 

            # Saving the model
            print(f""Saving model {i}"")
            model.save(f""net_{i}.h5"", include_optimizer=False)
            model.save_weights(f""net_{i}_weights.h5"")

    else:
        print(""Loading pretrained weights"")
        for i in range(args.pretrained):
            models[i].load_weights(f""net_{i}_weights.h5"")

    # Creating Ensemble
    print(""Creating Ensemble"")
    ensemble = create_ensemble(models)
    print(""Ensemble architecture: "")
    print(ensemble.summary())

    keras.utils.plot_model(model, ""ensemble.png"", show_shapes=True)
    ensemble.save(""ensemble.h5"")

    ensemble_train = {}
    ensemble_val = {}
    for i in range(1, args.pretrained+1):
        ensemble_train[f'input_{i}'] = train_ds
        ensemble_val[f'input_{i}'] = val_ds

    assert len(ensemble_train) == int(args.pretrained)
    assert len(ensemble_val) == int(args.pretrained)

    ensemble.compile(loss=""categorical_crossentropy"", 
                     optimizer=opt, 
                     metrics=[""accuracy""])

    ensemble_checkpoint = tf.keras.callbacks.ModelCheckpoint(""ensemble_backup_weights.h5"", 
                                                             monitor='val_loss', 
                                                             verbose=1, 
                                                             save_weights_only=True, 
                                                             save_best_only=True)

    history = ensemble.fit(ensemble_train, 
                           epochs=EPOCHS, 
                           verbose=1,
                           steps_per_epoch=(len(imagePaths)-VAL_SPLIT) // BS, 
                           validation_data=(ensemble_val), 
                           validation_steps = VAL_SPLIT // BS, 
                           callbacks=[ensemble_checkpoint, reducelr])
   print(""Saving Ensemble"")
      ensemble.save(""ensemble.h5"", include_optimizer=False)
      ensemble.save_weights(""ensemble_weights.h5"")

if __name__ == '__main__':
    ap = argparse.ArgumentParser()
    ap.add_argument('--pretrained', default=0, type=int, help='number of pretrained models to use')
    main(ap.parse_args())
    sess.close()
```
Output of ```print(train_ds)```: (batch_size=16, no. of classes=2)
```python
<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>
<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>
<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>
<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>
<ZipDataset shapes: ((16, 224, 224, 3), (16, 2)), types: (tf.float32, tf.float32)>
```

**Describe the expected behavior**
The ```tf.data.Dataset``` object should not require a 'y' value to be passed to ```model.fit``` explicitly, since I have combined the images and labels through ```tf.data.Dataset.zip```.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39535,Release GPU Memory(VRAM) after tf.keras.backend.clear_session(),"**System information**
- Windows 10 Microsoft Windows [Version 10.0.18362.418]
- TensorFlow 2.0 installed from Conda:
- Python version: 3.6.10
- CUDA/cuDNN version:   NVIDIA-SMI 445.75       Driver Version: 445.75       CUDA Version: 11.0
- GPU model and memory: NVIDIA 2060S


**Describe the current behavior**
There's no command which frees the previously used VRAM. Even deleting the model and the data had no effect on the VRAM.

**Describe the expected behavior**
Any of these commands should release the VRAM. 

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow import math, dtypes
from tensorflow import float32 as f32 
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Input
import random
import numpy as np # linear algebra
import gc

rseed=10
np.random.seed(rseed)
random.seed(rseed)
tf.compat.v1.set_random_seed(rseed)
    
def MMSE( preds,targets, mask_value=0.0):
    tf.print('\npred',preds)
    tf.print('target',targets)
    mask = dtypes.cast(tf.not_equal(targets,0),f32) 
    num_rating = math.reduce_sum(mask) #count ratings
    loss = math.reduce_sum(math.square(mask*(preds - targets))) / num_rating 
    return loss


input_dim = Input(shape = (3, ))
model = Sequential()
model.add(Dense(3,input_dim=3))
model.add(Dense(3))
model.compile(optimizer = Adam(lr=0.01),loss=[MMSE]) 
            
data  = tf.math.round(tf.random.normal(shape=[5,3]))
history = model.fit(data,data, epochs = 1, batch_size = 5,verbose=0, shuffle=False) 

del input_dim,model,data,history
tf.compat.v1.reset_default_graph()
tf.keras.backend.clear_session()
gc.collect()

```
I've used nvidia-smi to check the memory-usage.
"
39534,Build tensorflow-lite-with-select-tf-ops Failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution   Linux Ubuntu 18.04 
- TensorFlow installed from source
- TensorFlow version: r2.2
- Python version: Python 3.6.9
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source):gcc version 7.5.0
- CUDA/cuDNN version: no GPU
- GPU model and memory:no GPU

ndk-version ndkr18
sdk-version sdk 24.4.1

**Describe the problem**
when I complie tflite as https://www.tensorflow.org/lite/guide/ops_select#android_aar,
the error comes.
ERROR: no such target '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': target 'tensorflow-lite-with-select-tf-ops' not declared in package 'tensorflow/lite/java' defined by /tensorflow_src/tensorflow/lite/java/BUILD

**Provide the exact sequence of commands / steps that you executed before running into the problem**
firstly , I prepare the environment as the manual https://www.tensorflow.org/install/source#configure_the_build
then I run  command as https://www.tensorflow.org/lite/guide/ops_select#android_aar
bazel build --cxxopt='--std=c++11' -c opt             \
  --config=android_arm --config=monolithic          \
  //tensorflow/lite/java:tensorflow-lite-with-select-tf-ops
 
ERROR: Skipping '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': no such target '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': target 'tensorflow-lite-with-select-tf-ops' not declared in package 'tensorflow/lite/java' defined by /tensorflow_src/tensorflow/lite/java/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/lite/java:tensorflow-lite-with-select-tf-ops': target 'tensorflow-lite-with-select-tf-ops' not declared in package 'tensorflow/lite/java' defined by /tensorflow_src/tensorflow/lite/java/BUILD
"
39533,Tensorflow 2.1.0 on Windows: No OpKernel was registered to support Op 'SparseMatMul',"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.1.0
- Python version: 3.6.0
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: 



**Describe the problem**

When I run the following codes: 

```
    def _call(self, inputs):
        x = inputs

        # dropout
        if self.sparse_inputs:
            x = sparse_dropout(x, 1 - self.dropout, self.num_features_nonzero)
        else:
            x = tf.nn.dropout(x, 1 - self.dropout)

        # convolve
        supports = tf.matmul(tf.sparse_tensor_to_dense(self.support[0]), tf.diag(self.vars['kernel']), a_is_sparse=True,
                             b_is_sparse=True)
        supports = tf.matmul(supports, tf.sparse_tensor_to_dense(self.support[1]), a_is_sparse=True, b_is_sparse=True)
        pre_sup = dot(x, self.vars['weights_' + str(0)], sparse=self.sparse_inputs)
        output = dot(supports, pre_sup)

        if self.bias:
            output += self.vars['bias']

        return self.act(output)
```

The following error:
```
Traceback (most recent call last):
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1367, in _do_call
    return fn(*args)
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1350, in _run_fn
    self._extend_graph()
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1390, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' used by {{node wavelet_convolution_1/MatMul}}with these attrs: [transpose_b=false, Ta=DT_FLOAT, Tb=DT_FLOAT, b_is_sparse=true, a_is_sparse=true, transpose_a=false]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[wavelet_convolution_1/MatMul]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/zhuowei/Desktop/GAdv/GWNN-master/GraphWaveletNetwork/train.py"", line 128, in <module>
    sess.run(tf.global_variables_initializer())
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 960, in run
    run_metadata_ptr)
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1183, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1361, in _do_run
    run_metadata)
  File ""C:\Users\zhuowei\Anaconda3\lib\site-packages\tensorflow_core\python\client\session.py"", line 1386, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'SparseMatMul' used by node wavelet_convolution_1/MatMul (defined at \Users\zhuowei\Desktop\GAdv\GWNN-master\GraphWaveletNetwork\layers.py:320) with these attrs: [transpose_b=false, Ta=DT_FLOAT, Tb=DT_FLOAT, b_is_sparse=true, a_is_sparse=true, transpose_a=false]
Registered devices: [CPU, GPU]
Registered kernels:
  <no registered kernels>

	 [[wavelet_convolution_1/MatMul]]

Process finished with exit code 1
```

`SparseMatMul ` op is unavailable on GPU?
"
39532,"tflite with illegal instruction - Self compile of ""_interpreter_wrapper.so"" ?","**System information**
- Yocto Poky 3.0
- embedded device with ARM Cortex A9
- https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_armv7l.whl 
- Python version: 3.7
- GCC/Compiler 9.2:
- 512MB RAM:

**Describe the problem**
I can execute some tflite nets but not all. Not working: ""mobilenet_v1_1.0_224.tflite"" But working: ""mobilenet_v1_1.0_224_quant.tflite"". My suspect is that tensorflow is not compiled for my ARM CPU (Cortex A9) - although there is just one ARM32bit selection for a python whl.
I could recompile a ""libtensorflow-lite.a"" but howto create a python package or at least a substitute for the ""_interpreter_wrapper.so"" ? (this seems to be where the magic happens).

The only hint I found was for tensorflow (the big one) handling with docker. No experience with that, but I guess this doesn't use my cross-compiler and options."
39531,Import error,"import tensorflow as tf
tf.add(1, 2).numpy()

I ran the above code for non GPU version and got the following error.


ImportError                               Traceback (most recent call last)
~\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

~\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\New\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\Anaconda3\New\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-4-79ec058b6a86> in <module>
----> 1 import tensorflow as tf
      2 tf.add(1, 2).numpy()

~\Anaconda3\New\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\Anaconda3\New\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

~\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Vishal\Anaconda3\New\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Vishal\Anaconda3\New\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.



I upgraded it

(base) C:\Users\Vishal>pip install --upgrade tensorflow
Collecting tensorflow
  Using cached https://files.pythonhosted.org/packages/af/50/d7da24189d95e2084bb1cc350a8e4acdf1b0c9b3d57def7a348f0d9cb062/tensorflow-2.2.0-cp37-cp37m-win_amd64.whl
Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.28.1)
Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.1.0)
Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= ""3"" in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (0.33.4)
Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.6.3)
Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (0.2.0)
Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (2.10.0)
Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (3.11.3)
Requirement already satisfied, skipping upgrade: six>=1.12.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.12.0)
Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.11.2)
Requirement already satisfied, skipping upgrade: gast==0.3.3 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (0.3.3)
Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (0.9.0)
Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (2.2.0)
Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (3.2.1)
Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.16.4)
Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= ""3"" in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (1.4.1)
Requirement already satisfied, skipping upgrade: tensorboard<2.3.0,>=2.2.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorflow) (2.2.1)
Requirement already satisfied, skipping upgrade: setuptools in c:\users\vishal\anaconda3\new\lib\site-packages (from protobuf>=3.8.0->tensorflow) (41.0.1)
Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.14.3)
Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.1)
Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (0.15.4)
Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.6.0.post3)
Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (3.2.2)
Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)
Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in c:\users\vishal\anaconda3\new\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.2.8)
Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in c:\users\vishal\anaconda3\new\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.0)
Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (4.1.0)
Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.3.0)
Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < ""3.8"" in c:\users\vishal\anaconda3\new\lib\site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.17)
Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\users\vishal\anaconda3\new\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)
Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\users\vishal\anaconda3\new\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)
Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\users\vishal\anaconda3\new\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.24.2)
Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\users\vishal\anaconda3\new\lib\site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.6.16)
Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in c:\users\vishal\anaconda3\new\lib\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.4.8)
Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in c:\users\vishal\anaconda3\new\lib\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.1.0)
Requirement already satisfied, skipping upgrade: zipp>=0.5 in c:\users\vishal\anaconda3\new\lib\site-packages (from importlib-metadata; python_version < ""3.8""->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (0.5.1)
Installing collected packages: tensorflow
  Found existing installation: tensorflow 2.0.0
    Uninstalling tensorflow-2.0.0:
      Successfully uninstalled tensorflow-2.0.0
Successfully installed tensorflow-2.2.0

(base) C:\Users\Vishal>

"
39530,replace layer in resnet,"Is it possible to remove/replace the lower layers of a pretrained ResNet50 model in tf.keras.applications? trying to replace the first Conv2D layer with a new one. The only examples I've seen online do it for adding to the top of the network, not the first few layers.

For instance, I've tried doing this:

```
import tensorflow as tf
pretrained_resnet = tf.keras.applications.ResNet50(include_top=False, weights='imagenet')
inputs = tf.keras.Input(shape=(256,256,1))
x = tf.keras.layers.ZeroPadding2D()(inputs)
x = tf.keras.layers.Conv2D(filters=64,
                           kernel_size=(7,7),
                           strides=(2,2),
                           padding='same')(x)
outputs = pretrained_resnet.layers[3](x)
test = tf.keras.Model(inputs, pretrained_resnet.output)
```
But it gives an error."
39528,"ValueError: Error when checking target: expected Output to have 2 dimensions, but got array with shape (631, 80, 2641)","Alright, after I use GlobalAveragePooling1D, I encounter another problem. It's the dimension.

The shape of outputs of the model is 2 dimensions (batch_size, features) while the shape of the Y Train is 3 dimensions.

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-30-f6e17343a06b> in <module>()
     10           batch_size=batch_size,
     11           epochs=training_epoch,
---> 12           validation_split=0.1)

2 frames
/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
    133                         ': expected ' + names[i] + ' to have ' +
    134                         str(len(shape)) + ' dimensions, but got array '
--> 135                         'with shape ' + str(data_shape))
    136                 if not check_batch_axis:
    137                     data_shape = data_shape[1:]

ValueError: Error when checking target: expected Output to have 2 dimensions, but got array with shape (631, 80, 2641)

```
I have two choices. First is changing the shape of Y Train into two dimensions. Second is changing the model.

The model is following the model I (Bi-LSTM) in ""Neural Models for Sequence Chunking"". It said to ""average the input vectors"".

I will post the code here. Reminder, I'm using Google CoLab

```
# Padding
X = [[word2idx[w[0]] for w in s] for s in quran_sentences]
X = pad_sequences(maxlen=max_length, sequences=X, padding=""post"",value=word2idx[""PAD""])

y = [[tag2idx[w[1]] for w in s] for s in quran_sentences]
y = pad_sequences(maxlen=max_length, sequences=y, padding=""post"", value=tag2idx[""O""])
y = [to_categorical(i, num_classes=n_tags) for i in y]

X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42)

# Parameters
hidden_state_encoder_size = 100

# hidden_state_decoder_size = 200

batch_size = 64

training_epoch = 200

embedding_size = 80

dropout_rate = 0.5

# Model
# Input
inputs = Input(shape=(max_length,), name=""Input"")

# Embedding
# Output = (batch_size, input_length, output_dim)
embed = Embedding(input_dim=n_words+1,
                  output_dim=embedding_size,
                  input_length=max_length,
                  name=""Embedding"")(inputs)

# Bi-LSTM
# Output = (batch_size, steps, features)
encoder = Bidirectional(LSTM(units=hidden_state_encoder_size,
                             return_sequences=True,
                             dropout=dropout_rate,
                             name=""LSTM""),
                        name=""Bi-LSTM"")

hidden_states = encoder(embed)

# Average
# Output = (batch_size, features)
average = GlobalAveragePooling1D(name=""Average"")(hidden_states)

# Outputs
outputs = Dense(n_tags,
                activation=""softmax"",
                name=""Output"")(average)

model = Model(inputs, outputs, name=""Sequence Chunking"")

# Compile & Train
# Compile
model.compile(optimizer='rmsprop',
              loss='categorical_crossentropy',
              metrics=[""accuracy""])

# run training
model.fit(X_tr, np.array(y_tr),
          batch_size=batch_size,
          epochs=training_epoch,
          validation_split=0.1)
```"
39526,Documentation for ensuring CUPTI for Profiling is Misleading,"## URL(s) with the issue:
https://www.tensorflow.org/guide/profiler#install_the_profiler_and_gpu_prerequisites

## Description of issue (what needs changing):
The documentation says to do `ldconfig -p | grep libcupti` to check that CUPTI exists on the path, and to do `export LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:$LD_LIBRARY_PATH` to fix it if it is not on the path.  However, the documentation can be misleading in situations where an old install of CUDA 10.0 has been replaced with 10.1 (at least on my installation).


My output when checking the path is as below:
```console
tyler@lambda2:/usr/local/cuda/bin$ ldconfig -p | grep libcupti
	libcupti.so.10.0 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so.10.0
	libcupti.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so
```

Reading the documentation, this suggested to me that I did indeed have a version of libcupti on the path, and that everything should work. However, when I trained my model with the profiler on I saw the following error logs in the console.

```
2020-05-13 15:49:23.364143: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.
2020-05-13 15:49:23.364212: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2020-05-13 15:49:23.364588: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64
2020-05-13 15:49:23.364606: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
```

After double checking that I had CUDA 10.1 installed and not 10.2, I did the below
```console
tyler@lambda2:~/$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64
tyler@lambda2:~/$ echo $LD_LIBRARY_PATH
/usr/local/cuda-10.1/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/extras/CUPTI/lib64
```

This then allows the profiler to load CUPTI
```
2020-05-13 18:18:51.560268: I tensorflow/core/profiler/lib/profiler_session.cc:163] Profiler session started.
2020-05-13 18:18:51.560338: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs
2020-05-13 18:18:51.561266: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcupti.so.10.1
```

However, rerunning the command from the documentation for checking that CUPTI is on the path gives the same output as before

```console
tyler@lambda2:~/$ ldconfig -p | grep libcupti
	libcupti.so.10.0 (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so.10.0
	libcupti.so (libc6,x86-64) => /usr/lib/x86_64-linux-gnu/libcupti.so
```

### Desired fixes
After updating my path, I would expect that `ldconfig -p | grep libcupti` would update to show that `usr/local/cuda/extras/CUPTI/lib64` with version 10.1 is available. 

Additionally, I believe the documentation should explicitly state that running `ldconfig -p | grep libcupti` should show `libcupti.so.10.1` or greater


### Submit a pull request?

No, I'm not sure of what the best way to check for 10.1 or 10.2 would be
"
39524,"Can't access resource variable using LookupResource in a custom op, probably because of binary incompatibility","**System information**

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX Catalina
TensorFlow installed from (source or binary): binary
TensorFlow version: 1.14.0
Python version: 3.7
Installed using virtualenv? pip? conda?: conda
GCC/Compiler version (for custom op): 

Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
Apple clang version 11.0.3 (clang-1103.0.32.59)
Target: x86_64-apple-darwin19.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

**Describe the problem**
I have a runtime error when i run zmq_ops which is a modified version of tensorpacks's zmq_ops，and source code is in the attach files
[zmq_ops.tar.gz](https://github.com/tensorflow/tensorflow/files/4625555/zmq_ops.tar.gz)

when i run `benchmark.py` script，it raise the following errors:
2020-05-14 09:16:55.871310: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871310: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871357: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871373: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871321: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871403: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871421: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE
2020-05-14 09:16:55.871377: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at zmq_pull_op.cc:61 : Invalid argument: Trying to access resource using the wrong type. Expected N10tensorpack13ZMQConnectionE got N10tensorpack13ZMQConnectionE

issue #25113 have the same problem，but I can't find any solutions"
39523,Is there a way to convert a custom keras.utils.Sequence custom class to a tf.Data pipeline?,"When I was building up my data pipeline, the Tensorflow docs were very insistent that generators are unsafe for multiprocessing, and that the best way to build up a multiprocessing streaming pipeline is to extend tensorflow.keras.utils.Sequence into your own custom class. This is written here: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence

So I did that, but now Tensorflow is telling me that Sequence extensions are ALSO not ideal for multiprocessing through the warning message `multiprocessing can interact badly with TensorFlow, causing nondeterministic deadlocks. For high performance data pipelines tf.data is recommended.`. So now the recommendation is to use tf.Data. And, as it were, I keep running into deadlocks 4~ epochs into training now. 

Is there no converter between an existing sequence class and a tf.Data pipeline? It seems bizarre that the EXACT thing the Sequence extension class is recommended for seems to no longer work, and now only a brand new type of data pipeline will do the multiprocessing job. At the very least, this should be updated in the Sequence docs. "
39522,tf.math.l2_normalize support for complex dtypes,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**

[tf.math.l2_normalize](https://www.tensorflow.org/api_docs/python/tf/math/l2_normalize) currently does not appear to support complex datatypes:
```
tf.math.l2_normalize(1j)
```

```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-22-148da328f68d> in <module>
----> 1 tf.math.l2_normalize(1j)

~/anaconda3/envs/*/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py in l2_normalize_v2(x, axis, epsilon, name)
    641     x = ops.convert_to_tensor(x, name=""x"")
    642     square_sum = math_ops.reduce_sum(math_ops.square(x), axis, keepdims=True)
--> 643     x_inv_norm = math_ops.rsqrt(math_ops.maximum(square_sum, epsilon))
    644     return math_ops.multiply(x, x_inv_norm, name=name)
    645 

~/anaconda3/envs/*/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py in maximum(x, y, name)
   5770         raise
   5771     except _core._NotOkStatusException as e:
-> 5772       _ops.raise_from_not_ok_status(e, name)
   5773   # Add nodes to the TensorFlow graph.
   5774   try:

~/anaconda3/envs/*/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6604   message = e.message + ("" name: "" + name if name is not None else """")
   6605   # pylint: disable=protected-access
-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)
   6607   # pylint: enable=protected-access
   6608 

~/anaconda3/envs/*/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

NotFoundError: Could not find valid device for node.
Node:{{node Maximum}}
All kernels registered for op Maximum :
  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_INT64, DT_BFLOAT16, DT_HALF]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
 [Op:Maximum]
```

This is surprising / unexpected given that the basic computational blocks, like squaring, summing, square root, and dividing, all support complex datatypes. The issue appears to be that `math_ops.maximum` requires a real tensor as `epsilon` is assumed to be a real-valued lower-bound on `square_sum`. I'm not sure of the most natural solution at the moment.

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

Anyone trying to do something like normalize a complex tensor. As a use case, consider trying to use a `CosineSimilarity` loss on a network with a complex output.

**Any Other info.**

N/A"
39521,Undefined symbol in debug mode,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: v2.2.0-rc3
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): gcc 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Tensorflow was built inside the 'tensorflow/tensorflow:devel-py3' docker image with digest 0285d09f16ff

**Describe the problem**

After building tensorflow w/ debug info, tensorflow fails when imported with the following error: 

``` python
ImportError: /usr/local/lib/python3.6/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: 
undefined symbol: _ZN10tensorflow4data12experimental14SnapshotReader34kSnappyReaderOutputBufferSizeBytesE 
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. I cherry picked https://github.com/tensorflow/tensorflow/pull/39239/commits/cd8b64f7fbdc2bc8700b71dadb4c51744a752095 to fix https://github.com/tensorflow/tensorflow/issues/37498
2. bazel build --config=dbg --strip=never //tensorflow/tools/pip_package:build_pip_package
3. ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
4. cd /tmp/tensorflow_pkg
4. pip3 install tensorflow-2.2.0rc3-cp36-cp36m-linux_x86_64.whl
5. python3 -c 'import tensorflow'

If there is anything else worth providing (readelf output?) please let me know"
39520,tf.signal.rfft documentation refers to Tcomplex as an argument ,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/signal/rfft

## Description of issue (what needs changing):

### Clear description

The table of `Args` in the documentation includes `Tcomplex`:

```
Tcomplex | An optional tf.DType from: tf.complex64, tf.complex128. Defaults to tf.complex64.
```

But the function does not accept this argument. Calling `tf.signal.rfft(..., Tcomplex=...)` results in the error:

```
TypeError: _rfft() got an unexpected keyword argument 'Tcomplex'
```

This makes sense given the signature in the documentation:

```
tf.signal.rfft(
    input_tensor, fft_length=None, name=None
)
```

and https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/ops/signal/fft_ops.py#L114-L140

### Submit a pull request?

No. I could not find where this table was generated in the code."
39515,"Boolean support for tf.arg{min,max}","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS Catalina 10.15.2 (19C57)`
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `2.2.0`
- Python version: `3.7.5`

**Describe the feature and the current behavior/state.**
Feature: `tf.argmax` and `tf.argmin` do not fail when input is of dtype `tf.bool`.
Current_behavior: `tf.argmax` and `tf.argmin` fail when input is of dtype `tf.bool`.

**Will this change the current api? How?**
Would add new supported input type to `tf.argmax`.

**Who will benefit with this feature?**
Users who need to find the first for non-zero indices from their boolean tensor along specific axis.

**Sketch of a test case**
```python
import tensorflow as tf

tf.debugging.assert_equal(tf.argmax((False, True)), tf.cast(1, tf.int64))
tf.debugging.assert_equal(tf.argmin((False, True)), tf.cast(0, tf.int64))
```

**Logs for current behavior**
```
>>> import tensorflow as tf
>>> tf.debugging.assert_equal(tf.argmax((False, True)), tf.cast(1, tf.int64))
...
tensorflow.python.framework.errors_impl.NotFoundError: Could not find valid device for node.
Node:{{node ArgMax}}
All kernels registered for op ArgMax :
  device='XLA_CPU'; output_type in [DT_INT32, DT_INT64]; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]
  device='XLA_CPU_JIT'; output_type in [DT_INT32, DT_INT64]; Tidx in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]
  device='CPU'; T in [DT_INT64]; output_type in [DT_INT64]
  device='CPU'; T in [DT_INT64]; output_type in [DT_INT32]
  device='CPU'; T in [DT_INT32]; output_type in [DT_INT64]
  device='CPU'; T in [DT_INT32]; output_type in [DT_INT32]
  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT64]
  device='CPU'; T in [DT_UINT16]; output_type in [DT_INT32]
  device='CPU'; T in [DT_INT16]; output_type in [DT_INT64]
  device='CPU'; T in [DT_INT16]; output_type in [DT_INT32]
  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT64]
  device='CPU'; T in [DT_UINT8]; output_type in [DT_INT32]
  device='CPU'; T in [DT_INT8]; output_type in [DT_INT64]
  device='CPU'; T in [DT_INT8]; output_type in [DT_INT32]
  device='CPU'; T in [DT_HALF]; output_type in [DT_INT64]
  device='CPU'; T in [DT_HALF]; output_type in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT64]
  device='CPU'; T in [DT_BFLOAT16]; output_type in [DT_INT32]
  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT64]
  device='CPU'; T in [DT_FLOAT]; output_type in [DT_INT32]
  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT64]
  device='CPU'; T in [DT_DOUBLE]; output_type in [DT_INT32]
 [Op:ArgMax]
```"
39514,"Pip install tensorflow, got an error","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
none
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version:
tensorflow 2.2.0
- Python version:
3.6.10
- Installed using virtualenv? pip? conda?:
conda
- Bazel version (if compiling from source):
none
- GCC/Compiler version (if compiling from source):
none
- CUDA/cuDNN version:
19.273
- GPU model and memory:
1.7tac



**Describe the problem**

Launched CMD.exe and ran command ""conda install tensorflow"". Returned ERROR, no matching distribution found!


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39513,issue plz reply,"When i run this code 
python flow --model cfg/yolo.cfg --loadbin/yolo.weights --demo videofile.mp4 --gpu 1.0 --saveVideo
this error occurs 
Traceback (most recent call last):
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\Noddy\AppData\Local\Programs\Python\Python36\lib\imp.py"", line
243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Noddy\AppData\Local\Programs\Python\Python36\lib\imp.py"", line
343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""flow"", line 4, in <module>
    from darkflow.cli import cliHandler
  File ""C:\Users\Noddy\Downloads\darkflow-master\darkflow\cli.py"", line 3, in <m
odule>
    from .net.build import TFNet
  File ""C:\Users\Noddy\Downloads\darkflow-master\darkflow\net\build.py"", line 1,
 in <module>
    import tensorflow as tf
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Noddy\AppData\Roaming\Python\Python36\site-packages\tensorflow\
python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, descript
ion)
  File ""C:\Users\Noddy\AppData\Local\Programs\Python\Python36\lib\imp.py"", line
243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Noddy\AppData\Local\Programs\Python\Python36\lib\imp.py"", line
343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


plz see this error and reply as soon as possible
"
39512,Is tf.distribute.Strategy API use multi-processing？,"As python has GIL when use multi threading, so  tf.distribute.Strategy is using multi-processing or multi-threading?

"
39510,Which older versions(<2) are compatible with CUDA 10.1?,"I am facing compatibility problem with CUDA 10.1 and TF 1.3.
I would like to know,
Which TF versions (<2) are supported by CUDA 10.1? "
39509,Invalid results when running TFLite + ruy computation within a NodeJS v11+ addon on ARMv7,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes, `libdeepspeech.so`: https://github.com/mozilla/DeepSpeech/pull/2952
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian Buster, Armbian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): r2.2, master
- TensorFlow version (use command below): r2.2, master
- Python version: N/A
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): GCC 6.5.0 (RPi toolchain integrated in TensorFlow), GCC 7.2.1 (Linaro toolchain custom-added to TensorFlow
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Model computation differs when running the library inside a nodejs process (v11.0.0+), on ARMv7 hardware

**Describe the expected behavior**
Model computation should be the same

**Standalone code to reproduce the issue**
Reproduction environment is complicated for now (need to build libdeepspeech, the nodejs addon, install and run and compare to non nodejs), working on a much smaller one as of now.

How much simple would this needs to be? Our setup is a bit complicated.

Our model uses floats as input, so we need `EvalHybrid` to use the threaded-enabled fast-path enabled by `-DTFLITE_WITH_RUY_GEMV`.

Building for Android:
```
PYTHON_BIN_PATH=/usr/bin/python PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages TF_ENABLE_XLA=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_CUDA=0 TF_NEED_ROCM=0 TF_NEED_MPI=0 TF_DOWNLOAD_CLANG=0 CC_OPT_FLAGS=""-march=native -Wno-sign-compare"" TF_SET_ANDROID_WORKSPACE=1 ANDROID_NDK_HOME=$HOME/Documents/codaz/Mozilla/DeepSpeech/Android/android-ndk-r18b/ ANDROID_NDK_API_LEVEL=21 ANDROID_SDK_HOME=$HOME/Documents/codaz/Mozilla/DeepSpeech/Android/SDK/ ANDROID_API_LEVEL=27 ANDROID_BUILD_TOOLS_VERSION=28.0.3 ./configure && bazel clean && bazel build -s --verbose_failures --workspace_status_command=""bash native_client/bazel_workspace_status_cmd.sh"" --config=monolithic --config=android --config=android_arm --define=runtime=tflite --action_env ANDROID_NDK_API_LEVEL=21 --cxxopt=-std=c++11 --copt=-D_GLIBCXX_USE_C99 //native_client:libdeepspeech.so
```

Running on Android (Nokia 1.3, QM215 Cortex-A53 SoC):
```
DRX:/data/local/tmp $ LD_LIBRARY_PATH=$(pwd)/ ./deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav                                                                                                                                                                                                                                                                                    
TensorFlow: v2.2.0-rc3-31-ga6cee0345c
DeepSpeech: v0.7.0-30-gbb716efe
INFO: Initialized TensorFlow Lite runtime.
audio_format=1
num_channels=1
sample_rate=16000 (desired=16000)
bits_per_sample=16
res.buffer_size=93594
she had your dark suit in greasy wash water all year
```

Building for RPi3:
```
PYTHON_BIN_PATH=/usr/bin/python PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages TF_ENABLE_XLA=0 TF_NEED_OPENCL_SYCL=0 TF_NEED_CUDA=0 TF_NEED_ROCM=0 TF_NEED_MPI=0 TF_DOWNLOAD_CLANG=0 CC_OPT_FLAGS=""-march=native -Wno-sign-compare"" TF_SET_ANDROID_WORKSPACE=0 ./configure && bazel clean && bazel build -s --verbose_failures --workspace_status_command=""bash native_client/bazel_workspace_status_cmd.sh"" --config=monolithic --crosstool_top=@local_config_arm_compiler//:toolchain --cpu=armeabi --define=raspberry_pi_with_neon=true --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --copt=-march=armv7-a --copt=-mfloat-abi=hard --copt=-mfpu=neon-fp-armv8 --copt=-DRASPBERRY_PI --copt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mno-unaligned-access --define=tensorflow_mkldnn_contraction_kernel=0 --define=runtime=tflite --copt=-funsafe-math-optimizations --copt=-ftree-vectorize --copt=-pipe --copt=-DTFLITE_WITH_RUY_GEMV --define=tflite_with_ruy=true -c opt --copt=-pthread --linkopt=-lpthread //native_client:libdeepspeech.so
```

Running (C++ binary) on RPi3:
```
$ ./deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav
TensorFlow: v2.2.0-rc3-31-ga6cee0345c
DeepSpeech: v0.7.0-30-gbb716efe
she had your dark suit in greasy wash water all year
```

Running (NodeJS binding) on RPi3:
```
$ ./node ~/node_modules/.bin/deepspeech --model model_ldc93s1_16-2000.tflite --audio LDC93S1_pcms16le_1_16000.wav
Loading model from file model_ldc93s1_16-2000.tflite
TensorFlow: v2.2.0-rc3-31-ga6cee0345c
DeepSpeech: v0.7.0-30-gbb716efe
static napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate: 0x3287d98
static napi_value__* DeepSpeechNAPI::CreateModel(napi_env, napi_callback_info) ModelSate(int64_t): 52985240
Loaded model in 0.004686s.
Running inference.
static napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate(int64_t): 52985240
static napi_value__* DeepSpeechNAPI::SpeechToText(napi_env, napi_callback_info) ModelSate: 0x3287d98
she h yyour drk suit in greasy wash waer all year
Inference took 2.038s for 2.925s audio file.
```

**Other info / logs**
I have tested many hypothesis:
 - changing toolchain to gcc 6.5.0 bundled by tensorflow (we use a different one by default)
 - re-writing the nodejs swig-generated wrapper with n-api, in a very basic form
 - repro on master (commit 5be613ef4f3ec2608deed653ab4815bbbcfbe7f8)
 - repro on master with newer ruy (commit 808ff748e0c7dc746a413fe45fa022d63e6253e8)
 - bisected tensorflow: first repro is when tflite + ruy get the ability to run threads (commit be369f57e9e46d03ccd62f1031f9dc484c1016de)
 - bisected nodejs, issue first arises in https://github.com/nodejs/node/pull/21983/commits (obviously, hard to actionate)
 - repro with different model size (if input size is not a multiple of 4, **works**, we do not use threads somehow because of https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/lite/kernels/internal/optimized/neon_tensor_utils.cc#L1210)
 - same code, same nodejs version runs fine on ARM64 (Armbian on S905X), also excluded the SoC itself and the distro (repro under Armbian on S905X when running multilib armv7, repro on RPi3 and RPi4)
 - unable to reproduce and to get indication of any weird thing happening when running under valgrind on other platforms (valgrind on armv7/raspbian seems broken, valgrind on armv7/armbian dies because of unsupported instruction produced by `vfmaq_f32` in eigen)
 - **disabling** `kNeon` path in ruy but keeping threads, the computation works
 - **disabling** threads with `kNeon` enabled works
 - obviously verified that the input of the model is correct (dumped mfcc vectors, input states and output logits, and verified they were different only under nodejs runtime)
   - input here: https://github.com/lissyx/DeepSpeech/blob/bb716efe1ead50fc822d4f5faf0f2fa757adb2d5/native_client/tflitemodelstate.cc#L293-L299
   - output here: https://github.com/lissyx/DeepSpeech/blob/bb716efe1ead50fc822d4f5faf0f2fa757adb2d5/native_client/tflitemodelstate.cc#L308-L316
   - verified dumping the vector values (and verified as well the copy function)
   - we run several pass for the audio file, per small timesteps of 320ms, the very first output is already broken
 - no problem with the python bindings, java (android), even running concurrent threads (c++)
 - obviously tried debug build with no optimization at all
 - model trained on r1.15 and used on r2.2 (we produced a r2.2-trained one and there was the same issue)

**Current questions I am unable to reply**
 - is running under NodeJS exposing a bug that we have everywhere but that does not manifest?
 - `v8` used by NodeJS is using both threads **and** NEON instructions, when ruy's ARM code is also using threads and NEON in hand-written ASM?"
39507,with_values and map_flat_values for SparseTensor,"**System information**
- TensorFlow version (you are using): 2.2
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
`tf.RaggedTensor` offers two utility functions, `with_values` and `map_flat_values`,
which allow to modify the content of the `RaggedTensor` while keeping the structure.

This feature request is for adding the same to `tf.SparseTensor`.

**Will this change the current api? How?**
Add a new function `with_values` to `tf.SparseTensor` which updates the values
but leaves the indices unchanged.
Add a new function `tf.sparse.map_flat_values` (or maybe just `map_values`) that applies
an operation to the values Tensor of a `SparseTensor`, and updates it.

**Who will benefit with this feature?**
This would make the interfaces of sparse and ragged tensors more similar, and provides to very basic utility functions. 

**Any Other info.**
I do have a PR ready for the `with_values` function. 
Am working on `map_flat_values`. In the case of multiple `SparseTensor`s, should this function check that they have identical indices or not? `ragged.map_flat_values` checks row_splits, but I guess in a typical application the number of elements in row splits will be significantly lower than the number of values, whereas in a sparse tensor there are more indices (if the tensor is at least 2 dimensional) than values, making for a larger relative performance impact."
39506,CUDA installation for Ubuntu 18.4,"When I want to install CUDA for Ubuntu 18.4 I follow this guide: https://www.tensorflow.org/install/gpu#ubuntu_1804_cuda_101

When I want to install `cuda-10-1` it sais: `E: Unable to locate package cuda-10-1`.

Did NVIDIA change anything on its side? Does the documentation need an update? Some weeks ago this installation was all ok..."
39505,Support ragged output in TextVectorization layer,"**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently TextVectorization supports ragged input, but always outputs dense tensor padded with 0.
It would be cool if in INT mode it will output ragged tensor, so caller may call downstream layers that supports ragged tensors and convert to dense when he needed.

**Will this change the current api? How?**
No api changes

**Who will benefit with this feature?**
Users of ragged tensors
"
39504,TextVectorization layer error with 3D input data,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): Colab 2.x
- TensorFlow version (use command below): v2.2.0-0-g2b96f3662b 2.2.0
- Python version: Colab default
- Bazel version (if compiling from source): no
- GCC/Compiler version (if compiling from source): no
- CUDA/cuDNN version: no
- GPU model and memory: no

**Describe the current behavior**
TextVectorization layer limits input shape to 2D tensors.

**Describe the expected behavior**
TextVectorization layer should work well with tensors of any shape, e.g.:
1D - skipgram word2vec input ([batch of words])
2D - default [batch; words] input for many nlp tasks (currently works)
3D - fasttext skipgram input ([batch; words; character ngrams]

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/122NTBK6Gr2V2cHSVyXG5xqANh_N-oeNo?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
```python
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-9-fdd620ffcaa5> in <module>()
      1 # Fails with 1D data
----> 2 vectorization(tf.ragged.constant(data1))

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in set_shape(self, shape)
   1105       raise ValueError(
   1106           ""Tensor's shape %s is not compatible with supplied shape %s"" %
-> 1107           (self.shape, shape))
   1108 
   1109   # Methods not supported / implemented for Eager Tensors.

ValueError: Tensor's shape (11,) is not compatible with supplied shape (None, None)
```
"
39503,google colab GPU is too slow (x1 time CPU speed) when using tensorflow 1.15.0,"hello every one;
I tried to run the code below in google colab with both of version of tensorflow 2.0 and 1.15.0
with the 2.0 version i had no problems (mostly GPU's speed was over 29 time faster than CPU's speed) but when using the 1.15.0 version the GPU was as fast as CPU no difference at all. any solutions or suggestions. i need 1.15.0 version because of the object detection api. thank you very much for your help
                                                                  Code
`%tensorflow_version 1.x
import tensorflow.compat.v2 as tf
#import tensorflow as tf
print(tf.__version__)
#tf.enable_v2_behavior
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
import timeit

device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  print(
      '\n\nThis error most likely means that this notebook is not '
      'configured to use a GPU.  Change this in Notebook Settings via the '
      'command palette (cmd/ctrl-shift-P) or the Edit menu.\n\n')
  raise SystemError('GPU device not found')

def cpu():
  with tf.device('/cpu:0'):
    random_image_cpu = tf.random.normal((100, 100, 100, 3))
    net_cpu = tf.keras.layers.Conv2D(32, 7)(random_image_cpu)
    return tf.math.reduce_sum(net_cpu)

def gpu():
  with tf.device('/device:GPU:0'):
    random_image_gpu = tf.random.normal((100, 100, 100, 3))
    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)
    return tf.math.reduce_sum(net_gpu)
  

cpu()
gpu()


print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '
      '(batch x height x width x channel). Sum of ten runs.')
print('CPU (s):')
cpu_time = timeit.timeit('cpu()', number=10, setup=""from __main__ import cpu"")
print(cpu_time)
print('GPU (s):')
gpu_time = timeit.timeit('gpu()', number=10, setup=""from __main__ import gpu"")
print(gpu_time)
print('GPU speedup over CPU: {}x'.format(int(cpu_time/gpu_time)))`
                                                             results
`  TensorFlow 1.x selected.
version tensorflow:1.15.0
Found GPU at: /device:GPU:0
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.
CPU (s):
0.17279088399936882
GPU (s):
0.18540516800021578
GPU speedup over CPU: 0x`

`tf.version= 2.2.0
Found GPU at: /device:GPU:0
Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.
CPU (s):
2.4908546100000137
GPU (s):
0.10798676599998203
GPU speedup over CPU: 23x
2.2.0`"
39502,DLL load failed pywrap_tensorflow_internal,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Installed using pip
- TensorFlow version: version 2.2
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA - 10.1 and cuDNN - 7.6.5
- GPU model and memory: Nvidia Geforce 1050 Ti



**Describe the problem**
I'm trying to install TensorFlow GPU. All the requirements are downloaded according to [this](https://www.tensorflow.org/install/gpu). I have downloaded and used TensorFlow GPU before but everytime when i download i get same type error. Previously downgrading TensorFlow solved the issue however this time it didn't work. I get error ImportError: DLL load failed: The specified module could not be found. and cryptic error saying pywrap installation failed.  
The issue is similar to [this](https://github.com/tensorflow/tensorflow/issues/22512).  
I followed the instructions in the issue to solve it.  
The msvcp140.dll file is present in my system32 folder.
![Capture](https://user-images.githubusercontent.com/38111546/81801381-f71c8900-9531-11ea-9813-c41a3894a537.PNG)



**Provide the exact sequence of commands / steps that you executed before running into the problem**

I get the problem when  I import Tensorflow.
`import tensorflow`


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Full traceback:
```
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\anike\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>
```


"
39501,Errors encountered in model training,"
I encountered an errors when I was training mask_rcnn model with my own dataset .  So I ask for you help . I will appreciate it if you can help me out.

Caused by op 'GatherV2_4', defined at:
  File ""model_main.py"", line 110, in <module>
    tf.app.run()
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""model_main.py"", line 106, in main
    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\training.py"", line 471, in train_and_evaluate
    return executor.run()
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\training.py"", line 610, in run
    return self.run_local()
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\training.py"", line 711, in run_local
    saving_listeners=saving_listeners)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\estimator\estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\model_lib.py"", line 308, in model_fn
    features[fields.InputDataFields.true_image_shape])
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 846, in predict
    true_image_shapes))
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 1016, in _predict_second_stage
    image_shape, true_image_shapes)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 746, in _proposal_postprocess
    anchors, image_shape_2d, true_image_shapes)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 1700, in _postprocess_rpn
    groundtruth_weights_list)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 1786, in _sample_box_classifier_batch
    single_image_groundtruth_weights)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\meta_architectures\faster_rcnn_meta_arch.py"", line 1914, in _sample_box_classifier_minibatch_single_image
    groundtruth_weights=groundtruth_weights)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\core\target_assigner.py"", line 196, in assign
    reg_weights = self._create_regression_weights(match, groundtruth_weights)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\core\target_assigner.py"", line 334, in _create_regression_weights
    groundtruth_weights, ignored_value=0., unmatched_value=0.)
  File ""C:\Users\瑞\Documents\GitHub\models\research\object_detection\core\matcher.py"", line 214, in gather_based_on_match
    gathered_tensor = self._gather_op(input_tensor, gather_indices)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 2675, in gather
    return gen_array_ops.gather_v2(params, indices, axis, name=name)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3331, in gather_v2
    ""GatherV2"", params=params, indices=indices, axis=axis, name=name)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""C:\ProgramData\Anaconda3\envs\first_keras\lib\site-packages\tensorflow\python\framework\ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): indices[0] = 0 is not in [0, 0)
         [[node GatherV2_4 (defined at C:\Users\瑞\Documents\GitHub\models\research\object_detection\core\matcher.py:214)  = GatherV2[Taxis=DT_INT32, Tindices=DT_INT64, Tparams=DT_FLOAT, _device=""/device:CPU:0""](cond/Merge, Reshape_8, GatherV2_3/axis)]]
         [[node IteratorGetNext (defined at model_main.py:106)  = IteratorGetNext[output_shapes=[[1], [1,?,?,3], [1,2], [1,3], [1,100], [1,100,4], [1,100,4], [1,100,4], [1,100], [1,100,?,?], [1,100], [1,100], [1]], output_types=[DT_INT32, DT_FLOAT, DT_INT32, DT_INT32, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT32, DT_FLOAT, DT_BOOL, DT_FLOAT, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](IteratorV2)]]"
39499,Error after running the program,"I ran this program to check if  Tensorflow is installed or not in jupyter notebook. And Got the following error.

import tensorflow as tf
hello = tf.constant('Hello, TensorFlow!')
sess = tf.Session()
print(sess.run(hello))



ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-8-25b92e4d5dec>"", line 2, in <module>
    hello = tf.constant('Hello, TensorFlow!')
AttributeError: module 'tensorflow' has no attribute 'constant'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'AttributeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
ImportError: cannot import name 'tf_logging' from 'tensorflow.python.platform' (C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\python\platform\__init__.py)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\ultratb.py"", line 1101, in get_records
    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\ultratb.py"", line 319, in wrapped
    return f(*args, **kwargs)
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\ultratb.py"", line 353, in _fixed_getinnerframes
    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))
  File ""C:\Users\Vishal\Anaconda3\New\lib\inspect.py"", line 1502, in getinnerframes
    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)
  File ""C:\Users\Vishal\Anaconda3\New\lib\inspect.py"", line 1460, in getframeinfo
    filename = getsourcefile(frame) or getfile(frame)
  File ""C:\Users\Vishal\Anaconda3\New\lib\inspect.py"", line 696, in getsourcefile
    if getattr(getmodule(object, filename), '__loader__', None) is not None:
  File ""C:\Users\Vishal\Anaconda3\New\lib\inspect.py"", line 733, in getmodule
    if ismodule(module) and hasattr(module, '__file__'):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Vishal\Anaconda3\New\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked
  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Vishal\Anaconda3\New\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\interactiveshell.py"", line 3325, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-8-25b92e4d5dec>"", line 2, in <module>
    hello = tf.constant('Hello, TensorFlow!')
AttributeError: module 'tensorflow' has no attribute 'constant'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\IPython\core\interactiveshell.py"", line 2039, in showtraceback
    stb = value._render_traceback_()
AttributeError: 'AttributeError' object has no attribute '_render_traceback_'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 2453, in <module>
    from tensorflow.python.util import deprecation
  File ""C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 25, in <module>
    from tensorflow.python.platform import tf_logging as logging
ImportError: cannot import name 'tf_logging' from 'tensorflow.python.platform' (C:\Users\Vishal\Anaconda3\New\lib\site-packages\tensorflow\python\platform\__init__.py)


Failed to load the native TensorFlow runtime.
"
39498,Construct tf.SparseTensor with tf.int32 dense_shape,"**System information**
- TensorFlow version (you are using): 2.2
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
Currently, `tf.SparseTensor` raises an error when constructed with a `dense_shape` of
`int32` dtype. This is annoying, as it e.g. prevents constructs of the form
```
sparse = tf.SparseTensor(indices, values, tf.shape(other_tensor))
```
because `tf.shape` by default returns an int32 tensor. 

**Will this change the current api? How?**
Instead of throwing an error, `tf.SparseTensor.__init__` should convert int32 shapes to
int64 shapes. Since the shapes are expected to have only a few entries, this should not
lead to any unexpected performance impacts.

**Who will benefit with this feature?**
This is mostly a very small convenience improvement, but has the benefit of making `tf.shape`
and `tf.SparseTensor` interact more seamlessly.

**Any Other info.**
One step further in that direction would be to have tf.shape return int64 tensors when called on a SparseTensor, but this would be a non backwards-compatible change, so I'm not sure if thats possible/worth it."
39497,tf.keras.layers.RNN object within tf.keras.Model not automatically built after calling when `go_backward` not set to `True`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker (tensorflow/tensorflow:2.2.0-gpu)
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: as shipped with official docker image (tensorflow/tensorflow:2.2.0-gpu)
- GPU model and memory: Nvidia GTX 1050, 4GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
`tf.keras.layers.RNN` objects defined within tf.keras.Model are not automatically built after calling the model with input tensors **if argument `go_backward` is not set to `True`**. 
**Describe the expected behavior**
the `built` flag should be true for tf.keras.layers.RNN objects within `tf.keras.Model` object should be flaged as built after calling the model object with tensor.
**Standalone code to reproduce the issue**
```
import tensorflow as tf
# from crf import CRF

class CrfModel(tf.keras.Model):
    def __init__(self, *args, **kwargs):
        super(CrfModel, self).__init__(*args, **kwargs)


class BiRNNCrf(CrfModel):
    def __init__(self, vocab_size, embedding_dim, cell_creator, num_tags, *args, **kwargs):
        super().__init__(self, *args, **kwargs)
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)
        self.forward_cell = cell_creator()
        self.forward_rnn_layer = tf.keras.layers.RNN(self.forward_cell, return_sequences=True, return_state=False)
        self.backward_cell = cell_creator()
        self.backward_rnn_layer = tf.keras.layers.RNN(self.backward_cell, return_sequences=True, return_state=False, go_backwards=True)
        self.bi_rnn_layer = tf.keras.layers.Bidirectional(self.forward_rnn_layer, backward_layer=self.backward_rnn_layer)
        self.fc = tf.keras.layers.Dense(num_tags)
        # self.crf = CRF(num_tags)

    def lookup(self, inputs):
        out = self.embedding(inputs)
        return out
    
    def birnn(self, inputs, sequence_mask):
        out = self.bi_rnn_layer(inputs, mask=sequence_mask)
        return out

    # @tf.function(input_signature=[tf.TensorSpec([None, None, None], dtype=tf.float32, name=""input_ids""), tf.TensorSpec([None], dtype=tf.int32, name=""sequence_length"")])
    def call(self, inputs, sequence_length):
        out = self.lookup(inputs)
        mask = tf.sequence_mask(sequence_length, maxlen=tf.shape(inputs)[1])
        out = self.birnn(out, mask)
        out = self.fc(out)
        # out = self.crf(out, sequence_length)
        return out

def cell_creator():
    return tf.keras.layers.LSTMCell(300)

""""""
def build(self, input_shape, *args, **kwargs):
    super(self, tf.keras.Model).build(input_shape, *args, **kwargs)
    self.crf.build(input_shape)
""""""

if __name__ == ""__main__"":
    nn = BiRNNCrf(1000, 300, cell_creator, 3)
    assert isinstance(nn, tf.keras.Model)
    # nn.build(input_shape=[tf.TensorShape([None,None,3]), tf.TensorShape([None])]) # Not allowed by tensorflow
    nn(inputs=tf.constant([[1, 0, 0], [1, 1, 1]]), sequence_length=tf.constant([1,3]))
    print(nn.forward_rnn_layer.built) # False
    print(nn.backward_rnn_layer.built) # True

    nn.summary() # Fails

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Log from Tensorflow indicating that the `forward_rnn_layer` is not automatically built
```
ValueError: You tried to call `count_params` on lstm_cell, but the layer isn't built. You can build it manually via: `lstm_cell.build(batch_input_shape)`.
```"
39496,Bug caused by creating custom layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS but it should work with Ubuntu as well.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15
- Python version: 3.6
- GPU model and memory: CPU

I found a bug related to creating custom layer in TF. Here is a short code to replicate::

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

class CustomModel(tf.keras.layers.Layer):
    #this class is for source sequence
    def __init__(self,):
        super(CustomModel, self).__init__()

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(32, 512, 512),
                                      initializer=tf.keras.initializers.glorot_uniform(seed=1),
                                      trainable=True) #dumb
    def call(self, inputs):
        return inputs[0]

def main():
    def create_model(source_vocab, target_vocab, relationship_vocab):
        source = tf.keras.layers.Input(dtype='int32', shape=(64,), name='source')
        target = tf.keras.layers.Input(dtype='int32', shape=(64,), name='target')
        relationship = tf.keras.layers.Input(dtype='int32', shape=(1,), name='relationship')
        embedding_source = tf.keras.layers.Embedding(512, 512, input_length=64)(source)
        embedding_target = tf.keras.layers.Embedding(512, 512, input_length=64)(target)
        final_layer = CustomModel()([relationship, embedding_source, embedding_target])
        model = tf.keras.models.Model(inputs=[source, target, relationship], outputs=final_layer)
        return model
    model = create_model(1000, 1000, 500)
    print(model.summary())

if __name__ == '__main__':
    main()

```
I believe this is a bug so I posted it here so that you can find and fix it.
Meanwhile one workaround I found is to change the order of parameters. Below you can find the solution:

```
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf

class CustomModel(tf.keras.layers.Layer):
    #this class is for source sequence
    def __init__(self,):
        super(CustomModel, self).__init__()

    def build(self, input_shape):
        self.kernel = self.add_weight(shape=(32, 512, 512),
                                      initializer=tf.keras.initializers.glorot_uniform(seed=1),
                                      trainable=True) #dumb
    def call(self, inputs):
        return inputs[0]

def main():
    def create_model(source_vocab, target_vocab, relationship_vocab):
        source = tf.keras.layers.Input(dtype='int32', shape=(64,), name='source')
        target = tf.keras.layers.Input(dtype='int32', shape=(64,), name='target')
        relationship = tf.keras.layers.Input(dtype='int32', shape=(1,), name='relationship')
        embedding_source = tf.keras.layers.Embedding(512, 512, input_length=64)(source)
        embedding_target = tf.keras.layers.Embedding(512, 512, input_length=64)(target)
        final_layer = CustomModel()([embedding_source, embedding_target, relationship])
        model = tf.keras.models.Model(inputs=[source, target, relationship], outputs=final_layer)
        return model
    model = create_model(1000, 1000, 500)
    print(model.summary())

if __name__ == '__main__':
    main()

```




"
39495,Bug when using TensorFlow Lite Object Detection Android Demo for a custom model,"Hi, I'm using the [TensorFlow Lite Object Detection Android Demo](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android) for my object detection model on Pixel 4.
The demo is all right and operating normally on Pixel 4, but when I used other custom model such as [ssd_mobilenet_v2_mnasfpn_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_mnasfpn_shared_box_predictor_320x320_coco_sync_2020_05_06.tar.gz) and [ssd_mobilenet_v3_large_coco](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v3_large_coco_2020_01_14.tar.gz), the app will flash and quit, sometimes report error:
![Screenshot_20200513-162420](https://user-images.githubusercontent.com/47172283/81790612-351cab80-9538-11ea-91d1-e1b904f5faa5.png)

Can anyone help about this error?
"
39494,Distributed training and efficient data input pipeline,"Hi, I'm using tensorflow 2.0 in 4 gpus machine, my tensorflow code using keras api, like follows:
```
model = CNN_model()
model.compile(optimizer='adam', loss='mse')
model.fit()
```
and my data pipeline is user-defined generator, like follows:
```
def train_data():
    while True:
        for i in shuffle(train_id):
            df = pd.read_pickle(df_train_val['file_name'].iloc[i],
                                compression='gzip')
            feature = df.to_numpy().reshape(df.shape[0], -1)
            yield feature.reshape(-1, 56),feature.reshape(-1, 56)
gn = tf.data.Dataset.from_generator(train_data, output_types=(tf.float32, tf.float32),
                                          output_shapes=((None, 56),(None, 56))
                                         ).prefetch(tf.data.experimental.AUTOTUNE)
```
train_id is a file_index，I have more than 200K files in local machine, so in this case one batch is a file, my problem is the gpu util is very low, about 15%，I tried tf.distribute.MirroredStrategy, just by 
```
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    model = CNN_model()
    model.compile(optimizer='adam', loss='mse')
model.fit()
```
but, it didn't work, and slower than one gpu, I see the gpu util is lowed.
so, I have 3 questions: how can I make the input from_generator pipeline more efficient and let the gpu util increase or speed up the training process? why prefetch and tf.distribute.MirroredStrategy not work? is there somewhere for me to study these?"
39493,I try to install tensoflow version 1.5 and it does not happen!,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
**- TensorFlow version: 1.5**
- Python version:
**- Installed using virtualenv? pip? conda?: pip**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**I try to install TensorFlow version 1.5. I need it for my Mask R-CNN model. I work in Kaggle notebook. Until two days ago this problem did not exist.**

**This is what I do. It is the first thing I do before continuing with my project:
```
!pip install tensorflow==1.5 #need that because of Mask-RCNN version
!pip install keras==2.1.5

import tensorflow
print(tensorflow.__version__)
import keras
print(keras.__version__)
```
**


**This is the error it shows:
_**WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d978>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d198>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d3c8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d25d5c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f082d28c390>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/tensorflow/
ERROR: Could not find a version that satisfies the requirement tensorflow==1.5 (from versions: none)
ERROR: No matching distribution found for tensorflow==1.5
WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061aae80>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/
WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd5c0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/
WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd630>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/
WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/
WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fa3061dd400>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/keras/
ERROR: Could not find a version that satisfies the requirement keras==2.1.5 (from versions: none)
ERROR: No matching distribution found for keras==2.1.5**_
**

"
39492,How to invoke Flex delegate for FlexIdentityN node on Raspberry Pi,"**System information**
- Google Colab and Raspberry Pi 4


FlexIdentityN Tensorflow Lite Custom Ops Error

Hello,

For our project we tried to build an Image Classification Model using a CNN to detect 4 categories of waste. The link to the full model code is shared below.

https://colab.research.google.com/drive/1aAaQ7kq2BDYRlspmM-N2Uuh6CZkQGRNb?usp=sharing

We used Tensorflow 1.15 instead of the latest Tensorflow 2.2 because I couldn’t get the validation split command in model.fit to work on Tensorflow 2 (Any explanation for this problem will be appreciated too). Looking at newer tutorials now, I believe we have to split the data into training and testing data, instead of using validation split, correct? 

The model needs to be deployed on a Raspberry Pi 4 module. To do that whilst converting the model file to a .tflite file to run on the Raspberry Pi, we encountered this problem.

Errors

(1.
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
File ""/tensorflow-1.15.2/python3.6/bin/toco_from_protos"", line 8, in
sys.exit(main())
File ""/tensorflow-1.15.2/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
app.run(main=execute, argv=[sys.argv[0]] + unparsed)
File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py"", line 40, in run
_run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
_run_main(main, args)
File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
sys.exit(main(argv))
File ""/tensorflow-1.15.2/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
enable_mlir_converter)

2.
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.)

![FlexIdentityN seen in Netron](https://user-images.githubusercontent.com/62724378/81785708-3b049380-951c-11ea-9570-c31dece7aa6d.png)


As of now, I have no explanation as to how the FlexIdentityN Layer is formed and why. Any explanation on that will be appreciated as well.

These Errors can be solved(?) Using the following code.
```
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                         tf.lite.OpsSet.SELECT_TF_OPS]
```


However the same IdentityN Error pops up on the Raspberry Pi 4 when running the code also shown in the Google Colab file. The Error on the Raspberry Pi is shown below.

![FlexIdentityN Error](https://user-images.githubusercontent.com/62724378/81785756-4fe12700-951c-11ea-8f55-9c2194daf41c.png)


The link to the full Raspberry Pi Code is below.

https://colab.research.google.com/drive/1ptbKJ5CmQDjnTChzEIJ-5uyjYaiNYWqg?usp=sharing

I’ve followed this tutorial on YouTube by Edje Electronics to install Tensorflow on the Raspberry Pi.

https://www.youtube.com/watch?v=aimSGOAUI8Y

The Tutorial automatically installs the latest version in a virtual environment. (Maybe the difference in tensorflow version could be a problem)

Thank You.
"
39491,Can tfdbg support dumping tensor inside the function？,"I am using tf 1.15. 
I write a script containing if operator and function.
I use tfdbg to dump the tensor.
when I user ""lt"" command to list all tensors, I can't see the tensors inside the function.
Is it possible to dump tensors inside the function?

"
39490,tensorflow_cc.dll build fails when enabling MKL (Windows 10),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): MSVC 2019

**Describe the problem**
tensorflow_cc.dll build fails when enabling MKL

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Add indicated line (with +) to tensorflow/lite/build_def.bzl (line 162)
```
tf_cc_shared_object(
    name = name,
    copts = copts,
+   features = [""windows_export_all_symbols""],
    linkstatic = linkstatic,
    linkopts = linkopts + tflite_jni_linkopts(),
```

```
python configure.py
    empty
    empty
    n
    n
    empty
    n
```

`bazel build -c opt --config=mkl //tensorflow:tensorflow_cc`

**Any other info / logs**
```
ERROR: D:/tensorflow-2.2.0/tensorflow/core/kernels/BUILD:7965:1: C++ compilation of rule '//tensorflow/core/kernels:mkl_input_conversion_op' failed (Exit 2)
.\tensorflow/core/util/mkl_util.h(1263): error C2131: expression did not evaluate to a constant
.\tensorflow/core/util/mkl_util.h(1262): note: failure was caused by a read of a variable outside its lifetime
.\tensorflow/core/util/mkl_util.h(1262): note: see usage of 'dim'
.\tensorflow/core/util/mkl_util.h(1264): error C2131: expression did not evaluate to a constant
.\tensorflow/core/util/mkl_util.h(1262): note: failure was caused by a read of a variable outside its lifetime
.\tensorflow/core/util/mkl_util.h(1262): note: see usage of 'dim'
.\tensorflow/core/util/mkl_util.h(1266): error C3863: array type 'dnnl_dim_t [kNumDims]' is not assignable
.\tensorflow/core/util/mkl_util.h(1267): error C3863: array type 'dnnl_dim_t [kNumDims]' is not assignable
Target //tensorflow:tensorflow_cc failed to build
INFO: Elapsed time: 7603.936s, Critical Path: 3739.78s
INFO: 5297 processes: 5297 local.
FAILED: Build did NOT complete successfully
```
I also get this warning, which is probably unrelated:
`WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/387c3f74fd8efdc0be464b0e1a8033cc1eeb739c.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
`"
39489,Add support of uint8 and int8 for MEAN op,"@tensorflow/micro

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/kernels/reduce.cc#L113

// TODO(b/144955155): Support uint8(b/144955155) and int8(b/144955018)

Could you implement this so we can use the MEAN op in int8 model?"
39488,Lite benchmark reports inconsistent runtime on raspberry pi 4,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian 10
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.1.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Raspberry Pi 4

**Describe the problem**

We compiled lite/tools/benchmark/benchmark_model using bazel build on Pi4. And use it to profile a tflite network (attached: 0.tflite). The reported runtime is **~2.3sec** (0.log). However, when we run the same tflite network using an explicit call (tflite_test.py), it only reports **~270ms** runtime. Why?

For the same model, we have tried it on an Ubuntu 18.04 workstation. Both reports consistent runtime.

Note that you need to rename the file suffix in order to run them.

**Please provide the exact sequence of commands/steps when you ran into the problem**
% bazel --version
bazel 2.1.0- (@non-git)
% bazel build --linkopt=-latomic -c opt tensorflow/lite/tools/benchmark:benchmark_model
% ~/NAS/tensorflow/bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model --graph=./0.tflite --enable_op_profiling=true --num_runs=10 > 0.log
% python tflite_test.py -m 0.tflite -n 10
Iteration 10 Time: 0:00:02.724248                          # 2.7s/10 = 270ms



[0.log](https://github.com/tensorflow/tensorflow/files/4620249/0.log)
[tflite_test.py.txt](https://github.com/tensorflow/tensorflow/files/4620251/tflite_test.py.txt)
[0.tflite.txt](https://github.com/tensorflow/tensorflow/files/4620253/0.tflite.txt)


"
39487,Add support of float in lite/kernels/kernel_util.cc#CalculateActivationRangeQuantized,"Source:  https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/kernel_util.cc#L197

the CalculateActivationRangeQuantized method which only supports int8 and int16 output and raise error for float type:

```
  int32_t qmin = 0;
  int32_t qmax = 0;
  if (output->type == kTfLiteUInt8) {
    qmin = std::numeric_limits<uint8_t>::min();
    qmax = std::numeric_limits<uint8_t>::max();
  } else if (output->type == kTfLiteInt8) {
    qmin = std::numeric_limits<int8_t>::min();
    qmax = std::numeric_limits<int8_t>::max();
  } else if (output->type == kTfLiteInt16) {
    qmin = std::numeric_limits<int16_t>::min();
    qmax = std::numeric_limits<int16_t>::max();
  } else {
    TF_LITE_ENSURE(context, false);
  }
```

Could you add support so we can use float model after quantization"
39486,Slow memory copy on Jetson Nano,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Yes
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): https://forums.developer.nvidia.com/t/official-tensorflow-for-jetson-nano/71770/13
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10.0.326 and cuDNN 7.6.3.28
- GPU model and memory: Maxwell 128-core GPU, Maximum Operating Frequency: 921MHz, 4ch x 16-bit LPDDR4
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I'm running some experiments with semantic segmentation models on a Nano. One of the models I am using is [DeepLabv3 with a MNv2 backbone, trained on the ADE20K dataset](http://download.tensorflow.org/models/deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz). 

I noticed that the performance I was getting is very poor (~720ms per inference), on par with what I get on my five-year-old laptop running TensorFlow on CPU.

I debugged the issue and traced it to the memory copy, that is, `EagerTensor.numpy()`. Inference is very fast, as expected, but then converting the results to a format I can use in the rest of my workflow takes MUCH longer (up to 20x inference time if I use heavier networks like PSPNet).

I tried using TF1.x semantics with `tf.Session()` but no dice, `tf.Session.run()` becomes the bottleneck with similar timings. I tried converting the model to TensorRT... which makes the model even slower! (~920ms per inference)

Now, normally I would just say ""oh well, memory copy is slow, duh"" and try to find another way around it. But isn't the Nano supposed to share memory between its CPU and GPU? Is there any way to exploit this peculiarity? Or is it something I am doing wrong?


**Describe the expected behavior**
Total inference time including memory copy should be much lower.

**Standalone code to reproduce the issue**
Download DeepLabv3 as linked above, convert to SavedModel using [this script](https://github.com/kmfrick/TFLite_DeepLabv3_Inference/blob/master/convert_to_saved.py), then run
```python
import tensorflow as tf
import numpy as np
import time
imported = tf.saved_model.load('deeplabv3_mnv2_ade20k_train_2018_12_03_saved')
inp = np.random.uniform(0, 255, [1, 513, 513, 3])
t0 = time.time(); imported.signatures['serving_default'](tf.cast(inp, tf.uint8)); t1 = time.time(); print((t1 - t0) * 1000)
```



"
39485,"Using the code from the official api documentation, I cannot pass the feature column to the keras model","Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://tensorflow.google.cn/versions/r2.1/api_docs/python/tf/io/parse_example
https://tensorflow.google.cn/versions/r2.1/api_docs/python/tf/feature_column/bucketized_column?hl=en
## Description of issue (what needs changing):
but the code ：features = tf.io.parse_example (
serialized = record, features = tf.feature_column.make_parse_example_spec (l_list))
In the case of serialized = record, what type of object should be filled here to complete parse_example?


### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example
dnn_activation='relu'
volume = tf.feature_column.numeric_column('volume', shape=number_shape)
l_list=[volume]
features = tf.io.parse_example(
serialized=record, features=tf.feature_column.make_parse_example_spec(l_list))
input_l=keras.layers.DenseFeatures(l_list)
dnn1=keras.layers.Dense(300,activation=dnn_activation)(input_l)
output=keras.layers.Dense(1,activation='tanh')(dnn1)
model = Model(inputs = [input_l],outputs = [output])
model.compile(loss = ""mae"",optimizer = ""Adam"")

I tried to input the feature column to keras model, but the code ：features = tf.io.parse_example (
serialized = record, features = tf.feature_column.make_parse_example_spec (l_list))
In the case of serialized = record, what type of object should be filled here to complete parse_example? I studied the official API carefully, but the official API filled in serialized = ... instead. I can't understand how to complete the code. I tried to fill in various methods such as tfrecord, fill in tensor, and I couldn't complete the code. My data is a csv file. Can you provide a complete example, including data reading, feature column processing, and training?? I really cannot complete the code. Thank you very much, this is very important to me. Thank you very much, my email: 402868327@qq.com
Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
39484,Error while converting LSTM Models to TFLite Format,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```
**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.


Error Message:
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CAST, CONCATENATION, FILL, FULLY_CONNECTED, GATHER, PACK, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, TRANSPOSE. Here is a list of operators for which you will need custom implementations: TensorListFromTensor, TensorListReserve, TensorListStack, While.
Traceback (most recent call last):
  File ""/opt/conda/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/opt/conda/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/opt/conda/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/opt/conda/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56, in execute
    enable_mlir_converter)

I am sharing a short reference of my model.
Colab Link -> https://colab.research.google.com/drive/1ufY6PmQsXoI_J_FVS6r8L6bTnRc62Zmj?usp=sharing

"
39483,Tensorflow _  Symbol Initializer is already exposed as (),"Hello Everyone,
I'm trying to run the code posted on this page:

https://medium.com/dataflair/class-data-science-project-for-2020-traffic-signs-recognition-12b09c131742

OS: windows 10
Python: 3.6.1 (I've tried many versions, same issue)
I'm using anaconda 3
I've faced a Tensorflow Failure
I've tried the recommendations in posted in this discussion:
https://gist.github.com/mrry/ee5dbcfdd045fa48a27d56664411d41c

Now, I'm getting new error as follows:

---------------------------------------------------------------------------
SymbolAlreadyExposedError                 Traceback (most recent call last)
<ipython-input-2-644c6f02db98> in <module>
      4 import matplotlib.pyplot as plt
      5 from PIL import Image
----> 6 import tensorflow as tf
      7 from sklearn.model_selection import train_test_split
      8 from keras.utils import to_categorical

~\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     25 import sys as _sys
     26 
---> 27 from tensorflow._api.v2 import audio
     28 from tensorflow._api.v2 import autograph
     29 from tensorflow._api.v2 import bitwise

~\anaconda3\lib\site-packages\tensorflow\_api\v2\audio\__init__.py in <module>
      6 from __future__ import print_function as _print_function
      7 
----> 8 from tensorflow.python.ops.gen_audio_ops import decode_wav
      9 from tensorflow.python.ops.gen_audio_ops import encode_wav
     10 

~\anaconda3\lib\site-packages\tensorflow\python\ops\gen_audio_ops.py in <module>
      7 import six as _six
      8 
----> 9 from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
     10 from tensorflow.python.eager import context as _context
     11 from tensorflow.python.eager import core as _core

~\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     81 from tensorflow.python import data
     82 from tensorflow.python import distribute
---> 83 from tensorflow.python import keras
     84 from tensorflow.python.feature_column import feature_column_lib as feature_column
     85 from tensorflow.python.layers import layers

~\anaconda3\lib\site-packages\tensorflow\python\keras\__init__.py in <module>
     35 from tensorflow.python.keras import metrics
     36 from tensorflow.python.keras import models
---> 37 from tensorflow.python.keras import ops
     38 from tensorflow.python.keras import optimizers
     39 from tensorflow.python.keras import preprocessing

~\anaconda3\lib\site-packages\tensorflow\python\keras\ops.py in <module>
     26 # pylint: disable=bad-continuation
     27 keras_export(v1=[""keras.initializers.Initializer""])(
---> 28     init_ops.Initializer)
     29 keras_export(v1=[""keras.initializers.Zeros"", ""keras.initializers.zeros""])(
     30     init_ops.Zeros)

~\anaconda3\lib\site-packages\tensorflow\python\util\tf_export.py in __call__(self, func)
    332 
    333     _, undecorated_func = tf_decorator.unwrap(func)
--> 334     self.set_attr(undecorated_func, api_names_attr, self._names)
    335     self.set_attr(undecorated_func, api_names_attr_v1, self._names_v1)
    336     return func

~\anaconda3\lib\site-packages\tensorflow\python\util\tf_export.py in set_attr(self, func, api_names_attr, names)
    344         raise SymbolAlreadyExposedError(
    345             'Symbol %s is already exposed as %s.' %
--> 346             (func.__name__, getattr(func, api_names_attr)))  # pylint: disable=protected-access
    347     setattr(func, api_names_attr, names)
    348 

SymbolAlreadyExposedError: Symbol Initializer is already exposed as ().

**Could you please help me**
"
39479,Causal padding for tfp.layers.Convolution1DFlipout,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): 
tensorflow: 2.2.0 
tensorflow-probability: 0.9.0   
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.** 
Can ""causal"" padding be added as an option to tfp.layers.Convolution1DFlipout just like it is available in tf.keras.layers.Conv1D ?


"
39478,macOS build issue with tflite using make,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.4 (Catalina)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
When attempting to build from source on macOS using `make`, the `benchmark_model` binary fails to build due to the `--whole-archive` flag

**Provide the exact sequence of commands / steps that you executed before running into the problem**

    ./tensorflow/lite/tools/make/download_dependencies.sh
    ./tensorflow/lite/tools/make/build_lib.sh

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
$ ./tensorflow/lite/tools/make/build_lib.sh
+ set -e
+++ dirname ./tensorflow/lite/tools/make/build_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/Users/username/tensorflow-master/tensorflow/lite/tools/make
+ TENSORFLOW_DIR=/Users/username/tensorflow-master/tensorflow/lite/tools/make/../../../..
+ make -j 4 -C /Users/username/tensorflow-master/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile
g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITHOUT_XNNPACK -I. -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/../../../../../ -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/../../../../../../ -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/ -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/eigen -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/absl -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/gemmlowp -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/ruy -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/neon_2_sse -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/farmhash/src -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include \
  -o /Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/bin/benchmark_model /Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_main.o \
   -Wl,--whole-archive /Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/lib/benchmark-lib.a -Wl,--no-whole-archive  -lstdc++ -lpthread -lm -lz -ldl
g++ -O3 -DNDEBUG -fPIC --std=c++11  -DTFLITE_WITHOUT_XNNPACK -I. -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/../../../../../ -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/../../../../../../ -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/ -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/eigen -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/absl -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/gemmlowp -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/ruy -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/neon_2_sse -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/farmhash/src -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/Users/username/tensorflow-master/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include \
  -o /Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/bin/benchmark_model_performance_options /Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/obj/tensorflow/lite/tools/benchmark/benchmark_tflite_performance_options_main.o \
   /Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/lib/benchmark-lib.a  -lstdc++ -lpthread -lm -lz -ldl
ld: unknown option: --whole-archive
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/username/tensorflow-master/tensorflow/lite/tools/make/gen/osx_x86_64/bin/benchmark_model] Error 1
make: *** Waiting for unfinished jobs....
```"
39476,tensorflow-gardener fighting with itself,"I'm not sure if anyone cares, but parsing the commit history, it appears that tensorflow-gardener is repeatedly adding whitespace and then removing whitespace from the same file. It does this 5-6 times a day and has been doing so for at least the last month. You could probably cut the # of commits in half by fixing this bug.

See https://github.com/tensorflow/tensorflow/commits/ec2837b2a112ae3ada2c10173c12ed9b2f129b02/tensorflow/go/op/wrappers.go"
39475,tf.divide does not return a tensor,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
tensorflow 2.2.0
- Python version:
Python 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
tf.divide returns number instead of tensor

**Describe the expected behavior**

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import math as m

print(tf.add(5, 2))
print(tf.multiply(5, 2))
print(tf.divide(5, 2))
print(tf.multiply(tf.add(3, 2), tf.add(14, 32)))
print(tf.multiply(2.54, tf.divide(8, 2.6)))
print(tf.subtract(6.3, 2.1045))
print(tf.pow(3.6, 2))
print(tf.add(1, tf.pow(2, 2)))
print(tf.sqrt(5.0))
print(tf.cos(m.pi))

```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

output
tf.Tensor(7, shape=(), dtype=int32)
tf.Tensor(10, shape=(), dtype=int32)
2.5
tf.Tensor(230, shape=(), dtype=int32)
tf.Tensor(7.815385, shape=(), dtype=float32)
tf.Tensor(4.1955004, shape=(), dtype=float32)
tf.Tensor(12.959999, shape=(), dtype=float32)
tf.Tensor(5, shape=(), dtype=int32)
tf.Tensor(2.236068, shape=(), dtype=float32)
tf.Tensor(-1.0, shape=(), dtype=float32)

"
39474,load_model uses compat.v1 BatchNorm,"**System information**
- Have I written custom code : Yes
- OS Platform and Distribution: Ubuntu
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2.0.dev20200304
- Python version: 3.6.9

**Describe the current behavior**
``` tf.keras.models.load_model``` loads models in with ```tf.compat.v1.keras.layers.BatchNormalization```

**Describe the expected behavior**
It should load models with tf.keras.layers.BatchNormalization

**Standalone code to reproduce the issue**
See https://github.com/tensorflow/model-optimization/issues/386"
39469,"Keras Conv3DTranspose inconsistency between padding, output_shape and output_padding","## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv3DTranspose

## Description of issue (what needs changing):

Output shape computation is shown as below on above documentation
```
new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +
output_padding[2])
```
but padding is either 'valid' or 'same'
so, is padding computed based on traditional convolution computation (ref: https://www.tensorflow.org/api_docs/python/tf/nn/convolution) and then used here?
This is unclear from current documentation how `same`/`valid` mode is being used.


### Clear description

Clarification about how these modes are reflected in computing actual padding and then used in specified formula.

### Computation of output shape
`deconv_output_length` from keras/utils/conv_utils.py is used for computing the output shape considering output_padding which should be reflected into documentation concisely
ref: https://github.com/tensorflow/tensorflow/blob/dd2ea875d92eeb83e81b1cb92e29e61d488e98b2/tensorflow/python/keras/utils/conv_utils.py#L168

"
39468,"fatal error: too many errors emitted, stopping now [-ferror-limit=] 20 errors generated.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina version 10.15.4 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): from source
- TensorFlow version: 2.2
- Python version: 3.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): Apple clang version 11.0.3 (clang-1103.0.32.29)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I'm trying to build tensorflow from source on macOS , within less than a few minutes, I get a sequence of c++ errors and the build fails

```
ERROR: /private/var/tmp/_bazel_emadboctor/e4710f39cfc38993e5fe8f0eb15a51f2/external/com_google_p
rotobuf/BUILD:148:1: C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1)
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:68:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_BoolValue_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:82:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_BytesValue_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:96:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_DoubleValue_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:110:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_FloatValue_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:124:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_Int32Value_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:138:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_Int64Value_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:152:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_StringValue_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:166:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_UInt32Value_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:180:91: error: cannot initialize a member subobject of type 'int' with an lvalue of type 'void ()'
    {{ATOMIC_VAR_INIT(::PROTOBUF_NAMESPACE_ID::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsscc_info_UInt64Value_google_2fprotobuf_2fwrappers_2eproto}, {}};
                                                                                          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:310:20: error: no class named 'HasBitSetters' in 'google::protobuf::DoubleValue'
class DoubleValue::HasBitSetters {
      ~~~~~~~~~~~~~^
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:315:24: error: redefinition of 'kValueFieldNumber' as different kind of symbol
const int DoubleValue::kValueFieldNumber;
                       ^
/usr/local/include/google/protobuf/wrappers.pb.h:227:5: note: previous definition is here
    kValueFieldNumber = 1,
    ^
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:412:19: error: out-of-line definition of 'MergePartialFromCodedStream' does not match any declaration in 'google::protobuf::DoubleValue'
bool DoubleValue::MergePartialFromCodedStream(
                  ^~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:456:19: error: out-of-line definition of 'SerializeWithCachedSizes' does not match any declaration in 'google::protobuf::DoubleValue'
void DoubleValue::SerializeWithCachedSizes(
                  ^~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:474:46: error: out-of-line definition of 'InternalSerializeWithCachedSizesToArray' does not match any declaration in 'google::protobuf::DoubleValue'
::PROTOBUF_NAMESPACE_ID::uint8* DoubleValue::InternalSerializeWithCachedSizesToArray(
                                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:561:19: error: redefinition of 'Swap'
void DoubleValue::Swap(DoubleValue* other) {
                  ^
/usr/local/include/google/protobuf/wrappers.pb.h:154:15: note: previous definition is here
  inline void Swap(DoubleValue* other) {
              ^
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:575:19: error: redefinition of 'UnsafeArenaSwap'
void DoubleValue::UnsafeArenaSwap(DoubleValue* other) {
                  ^
/usr/local/include/google/protobuf/wrappers.pb.h:162:8: note: previous definition is here
  void UnsafeArenaSwap(DoubleValue* other) {
       ^
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:595:19: error: no class named 'HasBitSetters' in 'google::protobuf::FloatValue'
class FloatValue::HasBitSetters {
      ~~~~~~~~~~~~^
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:600:23: error: redefinition of 'kValueFieldNumber' as different kind of symbol
const int FloatValue::kValueFieldNumber;
                      ^
/usr/local/include/google/protobuf/wrappers.pb.h:378:5: note: previous definition is here
    kValueFieldNumber = 1,
    ^
external/com_google_protobuf/src/google/protobuf/wrappers.pb.cc:697:18: error: out-of-line definition of 'MergePartialFromCodedStream' does not match any declaration in 'google::protobuf::FloatValue'
bool FloatValue::MergePartialFromCodedStream(
                 ^~~~~~~~~~~~~~~~~~~~~~~~~~~
fatal error: too many errors emitted, stopping now [-ferror-limit=]
20 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /Users/emadboctor/Desktop/tensorflow/tensorflow/core/framework/BUILD:1084:1 C++ compilation of rule '@com_google_protobuf//:protobuf' failed (Exit 1)
INFO: Elapsed time: 58.678s, Critical Path: 7.64s
INFO: 213 processes: 213 local.
FAILED: Build did NOT complete successfully
(tfb) emadboctor@MacBook-Pro tensorflow % 
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
conda create -n tfb
conda activate tfb
conda install pip six numpy wheel setuptools mock 'future>=0.17.1'
pip install -U keras_applications --no-deps
pip install -U keras_preprocessing --no-deps
git clone https://github.com/tensorflow/tensorflow.git
wget https://github.com/bazelbuild/bazel/releases/download/2.0.0/bazel-2.0.0-installer-darwin-x86_64.sh
chmod +x bazel-2.0.0-installer-darwin-x86_64.sh
./bazel-2.0.0-installer-darwin-x86_64.sh --user
export PATH=/home/emadboctor/bin:$PATH
cd tensorflow
./configure
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /Users/emadboctor/anaconda3/envs/tfb/bin/python]: 


Found possible Python library paths:
  /Users/emadboctor/anaconda3/envs/tfb/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/Users/emadboctor/anaconda3/envs/tfb/lib/python3.8/site-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: 
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: y
Clang will be downloaded and used to compile tensorflow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Do you wish to build TensorFlow with iOS support? [y/N]: y
iOS support will be enabled for TensorFlow.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

bazel build -c opt --config=mkl --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --jobs 8 --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39467,Build tensorflow 2.2.0 failed in '@upb//:upb' with the length argument in strncpy,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Linux Ubuntu 18.04.4 LTS
- TensorFlow installed from (source or binary):
  source
- TensorFlow version:
  2.2.0
- Python version:
  3.7.7
- Bazel version (if compiling from source):
  2.0.0
- GCC/Compiler version (if compiling from source):
  10.1.0

**Describe the problem**

```console
In file included from /usr/include/string.h:494,                                                                
                 from external/upb/upb/upb.h:16,                                                                                                                                                                                              
                 from external/upb/upb/upb.c:2:                                                                 
In function 'strncpy',                                                                                         
    inlined from 'upb_status_seterrmsg' at external/upb/upb/upb.c:40:3:                                         
/usr/include/x86_64-linux-gnu/bits/string_fortified.h:106:10: error: '__builtin_strncpy' specified bound 127 equals destination size [-Werror=stringop-truncation]                                                             
  106 |   return __builtin___strncpy_chk (__dest, __src, __len, __bos (__dest));                                                                                                                                                              
      |          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                 
cc1: all warnings being treated as errors                                                                      
Target //tensorflow/tools/pip_package:build_pip_package failed to build                                         
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Build with bazel following the documentation.

**Any other info / logs**

It's a trivial misuse of strncpy.  Here is a simple fix.

```patch
--- upb/upb/upb.c~      2020-05-12 14:33:15.046048687 -0500                                                                                                                                           
+++ upb/upb/upb.c       2020-05-12 14:33:30.281987830 -0500                                                                                                                                           
@@ -37,7 +37,7 @@                                                                                                                                                                                                                                             
 void upb_status_seterrmsg(upb_status *status, const char *msg) {                                                                                                                                                                                             
   if (!status) return;                                                                                                                                                                                                                                       
   status->ok = false;                                         
-  strncpy(status->msg, msg, sizeof(status->msg));             
+  strncpy(status->msg, msg, sizeof(status->msg)-1);           
   nullz(status);                                              
 }                                                             
                                                               
```"
39465,model.fit hangs if validation_steps is zero,"
**System information**
- I written custom code (as opposed to using a stock example script provided in TensorFlow)
- **OS Platform and Distribution:** Kaggle Kernel, 'Linux-4.19.112+-x86_64-with-debian-buster-sid'
- **TensorFlow installed from (source or binary):** installed from pip
- **TensorFlow version (use command below):** 2.1
- **Python version:** 3.7.6
- **CUDA/cuDNN version:** V10.1.243
- **GPU model and memory:** Tesla P100-PCIE-16GB

**Current Behaviour**
`model.fit` suspends  execution or hangs after training first epoch runs if value passed for `validation_steps=len(valid_inputs)//BATCH_SIZE` is equal to zero. 
Moreover, **validate for _x_ steps** is not printed before training starts.

I believe a ValueError could be raised if validation_steps is 0 and not None. (or execution could proceed further with next epoch over training set) 

**Standalone code to reproduce the issue**
```
VALIDATION_STEPS=len(valid_noisy)//BATCH_SIZE # Results in 0
model.fit(
    train_dataset,
    steps_per_epoch=len(train_noisy)//BATCH_SIZE,
    epochs=200,
    validation_data=valid_dataset,
    validation_steps=VALIDATION_STEPS
)

```
[Complete Notebook](https://nbviewer.jupyter.org/github/vasudev13/No-Pain-No-GANz/blob/master/conv%20autoencoders/src/notebooks/denoising-dirty-documents.ipynb) 


If feasible, can I contribute via a PR if the above issue is valid and can be addressed by not a core contributor?

Thanks a bunch!"
39464,IdentityN,"**System information**
- Google Colab
- TensorFlow 1.15.2

1
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
  File ""/tensorflow-1.15.2/python3.6/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/tensorflow-1.15.2/python3.6/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)

2
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONV_2D, FULLY_CONNECTED, MAX_POOL_2D, SOFTMAX. Here is a list of operators for which you will need custom implementations: IdentityN.


**Standalone code to reproduce the issue** 
https://colab.research.google.com/drive/1aAaQ7kq2BDYRlspmM-N2Uuh6CZkQGRNb?usp=sharing

![FlexIdentityN seen in Netron](https://user-images.githubusercontent.com/62724378/81730803-b1b57880-94ab-11ea-9166-50e71d1c275e.png)


**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
39463,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"<em>Hello, I am getting a libcublas import error. </em>

**System information**
NAME=""Ubuntu""
VERSION=""18.04.1 LTS (Bionic Beaver)""
ID_LIKE=debian
VERSION_CODENAME=bionic
- TensorFlow installed from anaconda.
- TensorFlow version: v2
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda 
- CUDA/cuDNN version: 9.0
- GPU model and memory: 12 cores, 64GB RAM, 4x Titan Xp GPUs



**Problem**
ImportError

**Replication **
I have used my environment before perfectly fine, but stopped working today without me knowingly modifying anything that would cause this. I found similar errors like [this one](https://github.com/tensorflow/tensorflow/issues/15604). It seems that most of them are CUDA mismatch. Mine is not, running `which nvcc` gives /usr/local/cuda-9.0/bin/nvcc. Any clues?


**logs**
It seems libcublas isn't installed which is strange as I never recalled uninstalling it. 
Running `ldconfig -v` gives me the following: 

/sbin/ldconfig.real: Can't stat /usr/local/lib/x86_64-linux-gnu: No such file or directory
/sbin/ldconfig.real: Path `/lib/x86_64-linux-gnu' given more than once
/sbin/ldconfig.real: Path `/usr/lib/x86_64-linux-gnu' given more than once
/usr/lib/x86_64-linux-gnu/libfakeroot:
        libfakeroot-0.so -> libfakeroot-tcp.so
/usr/local/lib:
/lib/x86_64-linux-gnu:
        liblvm2cmd.so.2.02 -> liblvm2cmd.so.2.02
        libpthread.so.0 -> libpthread-2.27.so
        libmemusage.so -> libmemusage.so
        libcidn.so.1 -> libcidn-2.27.so
        libpcre.so.3 -> libpcre.so.3.13.3
        libncurses.so.5 -> libncurses.so.5.9
        libnss_files.so.2 -> libnss_files-2.27.so
        libply-splash-core.so.4 -> libply-splash-core.so.4.0.0
        liblvm2app.so.2.2 -> liblvm2app.so.2.2
        libulockmgr.so.1 -> libulockmgr.so.1.0.1
        libmvec.so.1 -> libmvec-2.27.so
        libidn.so.11 -> libidn.so.11.6.16
        libpci.so.3 -> libpci.so.3.5.2
        libnss_hesiod.so.2 -> libnss_hesiod-2.27.so
        libexpat.so.1 -> libexpat.so.1.6.7
        libfdisk.so.1 -> libfdisk.so.1.1.0
        libm.so.6 -> libm-2.27.so
        libnss_systemd.so.2 -> libnss_systemd.so.2
        libiw.so.30 -> libiw.so.30
        libmount.so.1 -> libmount.so.1.1.0
        libply-splash-graphics.so.4 -> libply-splash-graphics.so.4.0.0
        libnss_mdns6.so.2 -> libnss_mdns6.so.2
        libreadline.so.7 -> libreadline.so.7.0
        libnl-3.so.200 -> libnl-3.so.200.24.0
        libdevmapper-event.so.1.02.1 -> libdevmapper-event.so.1.02.1
        libthread_db.so.1 -> libthread_db-1.0.so
        libaudit.so.1 -> libaudit.so.1.0.0
        libsepol.so.1 -> libsepol.so.1
        libc.so.6 -> libc-2.27.so
        libext2fs.so.2 -> libext2fs.so.2.4
        libpamc.so.0 -> libpamc.so.0.82.1
        libwrap.so.0 -> libwrap.so.0.7.6
        libcryptsetup.so.12 -> libcryptsetup.so.12.2.0
        libpam_misc.so.0 -> libpam_misc.so.0.82.0
        libusb-1.0.so.0 -> libusb-1.0.so.0.1.0
        libbz2.so.1.0 -> libbz2.so.1.0.4
        libhistory.so.7 -> libhistory.so.7.0
        libnss_nisplus.so.2 -> libnss_nisplus-2.27.so
        libslang.so.2 -> libslang.so.2.3.1
        libattr.so.1 -> libattr.so.1.1.0
        liblzo2.so.2 -> liblzo2.so.2.0.0
        libSegFault.so -> libSegFault.so
        libgpg-error.so.0 -> libgpg-error.so.0.22.0
        libply-boot-client.so.4 -> libply-boot-client.so.4.0.0
        libkmod.so.2 -> libkmod.so.2.3.2
        libdevmapper-event-lvm2.so.2.02 -> libdevmapper-event-lvm2.so.2.02
        libcap.so.2 -> libcap.so.2.25
        libjson-c.so.3 -> libjson-c.so.3.0.1
        libe2p.so.2 -> libe2p.so.2.3
        libcrypt.so.1 -> libcrypt-2.27.so
        libnewt.so.0.52 -> libnewt.so.0.52.20
        libnss_nis.so.2 -> libnss_nis-2.27.so
        libply.so.4 -> libply.so.4.0.0
        libapparmor.so.1 -> libapparmor.so.1.4.2
        libatm.so.1 -> libatm.so.1.0.0
        libseccomp.so.2 -> libseccomp.so.2.4.1
        libparted.so.2 -> libparted.so.2.0.1
        libnih.so.1 -> libnih.so.1.0.0
        libparted-fs-resize.so.0 -> libparted-fs-resize.so.0.0.1
        libdns-export.so.1100 -> libdns-export.so.1100.1.1
        libpcprofile.so -> libpcprofile.so
        libtirpc.so.1 -> libtirpc.so.1.0.10
        libfuse.so.2 -> libfuse.so.2.9.7
        libnfsidmap.so.0 -> libnfsidmap.so.0.3.0
        libnss_mdns_minimal.so.2 -> libnss_mdns_minimal.so.2
        libutil.so.1 -> libutil-2.27.so
        libsmartcols.so.1 -> libsmartcols.so.1.1.0
        libblkid.so.1 -> libblkid.so.1.1.0
        libdl.so.2 -> libdl-2.27.so
        libnss_mdns4_minimal.so.2 -> libnss_mdns4_minimal.so.2
        libisns-nocrypto.so.0 -> libisns-nocrypto.so.0
        libdevmapper.so.1.02.1 -> libdevmapper.so.1.02.1
/sbin/ldconfig.real: /lib/x86_64-linux-gnu/ld-2.27.so is the dynamic linker, ignoring

        ld-linux-x86-64.so.2 -> ld-2.27.so
        libmnl.so.0 -> libmnl.so.0.2.0
        libnss_myhostname.so.2 -> libnss_myhostname.so.2
        libbsd.so.0 -> libbsd.so.0.8.7
        libprocps.so.6 -> libprocps.so.6.0.0
        libnss_mdns6_minimal.so.2 -> libnss_mdns6_minimal.so.2
        libnss_mdns.so.2 -> libnss_mdns.so.2
        libreadline.so.5 -> libreadline.so.5.2
        libnsl.so.1 -> libnsl-2.27.so
        libbrlapi.so.0.6 -> libbrlapi.so.0.6.6
        libcom_err.so.2 -> libcom_err.so.2.1
        liblzma.so.5 -> liblzma.so.5.2.2
        libss.so.2 -> libss.so.2.0
        libBrokenLocale.so.1 -> libBrokenLocale-2.27.so
        libz.so.1 -> libz.so.1.2.11
        libtinfo.so.5 -> libtinfo.so.5.9
        libgcc_s.so.1 -> libgcc_s.so.1
        librt.so.1 -> librt-2.27.so
        libgcrypt.so.20 -> libgcrypt.so.20.2.1
        libselinux.so.1 -> libselinux.so.1
        libnss_compat.so.2 -> libnss_compat-2.27.so
        libisc-export.so.169 -> libisc-export.so.169.0.1
        libuuid.so.1 -> libuuid.so.1.3.0
        libhistory.so.5 -> libhistory.so.5.2
        libudev.so.1 -> libudev.so.1.6.9
        libncursesw.so.5 -> libncursesw.so.5.9
        libacl.so.1 -> libacl.so.1.1.0
        libanl.so.1 -> libanl-2.27.so
        libresolv.so.2 -> libresolv-2.27.so
        libdbus-1.so.3 -> libdbus-1.so.3.19.4
        libnl-genl-3.so.200 -> libnl-genl-3.so.200.24.0
        libatasmart.so.4 -> libatasmart.so.4.0.5
        libnss_dns.so.2 -> libnss_dns-2.27.so
        libntfs-3g.so.88 -> libntfs-3g.so.88.0.0
        libpam.so.0 -> libpam.so.0.83.1
        libcap-ng.so.0 -> libcap-ng.so.0.0.0
        libkeyutils.so.1 -> libkeyutils.so.1.5
        libsystemd.so.0 -> libsystemd.so.0.21.0
        libnss_mdns4.so.2 -> libnss_mdns4.so.2
/usr/lib/x86_64-linux-gnu:
        librtmp.so.1 -> librtmp.so.1
        libnvidia-gtk2.so.430.26 -> libnvidia-gtk2.so.430.26
        libEGL_mesa.so.0 -> libEGL_mesa.so.0.0.0
        libwoff2dec.so.1.0.2 -> libwoff2dec.so.1.0.2
        libsndio.so.6.1 -> libsndio.so.6.1
        libcupsppdc.so.1 -> libcupsppdc.so.1
        libboost_program_options.so.1.65.1 -> libboost_program_options.so.1.65.1
        libpolkit-backend-1.so.0 -> libpolkit-backend-1.so.0.0.0
        libfwup.so.1 -> libfwup.so.1.10
        libgtksourceview-3.0.so.1 -> libgtksourceview-3.0.so.1.8.0
        libgee-0.8.so.2 -> libgee-0.8.so.2.6.1
        libhttp_parser.so.2.7.1 -> libhttp_parser.so.2.7.1
        libisccc.so.160 -> libisccc.so.160.0.2
        librasqal.so.3 -> librasqal.so.3.0.0
        libplds4.so -> libplds4.so
        libnvidia-glsi.so.430.26 -> libnvidia-glsi.so.430.26
        libbd_swap.so.2 -> libbd_swap.so.2.0.0
        libp11-kit.so.0 -> libp11-kit.so.0.3.0
        libclutter-gtk-1.0.so.0 -> libclutter-gtk-1.0.so.0.800.4
        libgstphotography-1.0.so.0 -> libgstphotography-1.0.so.0.1401.0
        libnvidia-gtk3.so.430.26 -> libnvidia-gtk3.so.430.26
        libxcb-dri2.so.0 -> libxcb-dri2.so.0.0.0
        libfreerdp-client2.so.2 -> libfreerdp-client2.so.2.0.0
        liblirc_client.so.0 -> liblirc_client.so.0.6.0
        libnm.so.0 -> libnm.so.0.1.0
        libnvidia-fbc.so.1 -> libnvidia-fbc.so.430.26
        libevent-2.1.so.6 -> libevent-2.1.so.6.0.2
        libcc1.so.0 -> libcc1.so.0.0.0
        libnvidia-rtcore.so.430.26 -> libnvidia-rtcore.so.430.26
        libdazzle-1.0.so.0 -> libdazzle-1.0.so.0
        libfftw3l_omp.so.3 -> libfftw3l_omp.so.3.5.7
        libflite_cmu_grapheme_lang.so.1 -> libflite_cmu_grapheme_lang.so.2.1
        libbabeltrace-ctf.so.1 -> libbabeltrace-ctf.so.1.0.0
        libclutter-gst-3.0.so.0 -> libclutter-gst-3.0.so.0.26.0
        libisl.so.19 -> libisl.so.19.0.0
        libzvbi-chains.so.0 -> libzvbi-chains.so.0.0.0
        libsamba-passdb.so.0 -> libsamba-passdb.so.0.27.0
        libgdata.so.22 -> libgdata.so.22.3.0
        libopen-pal.so.20 -> libopen-pal.so.20.10.1
        liboauth.so.0 -> liboauth.so.0.8.7
        libgsm.so.1 -> libgsm.so.1.0.12
        libqqwing.so.2 -> libqqwing.so.2.1.0
        libnspr4.so -> libnspr4.so
        libprotoc.so.10 -> libprotoc.so.10.0.0
        libGL.so.1 -> libGL.so.1.0.0
        libgxps.so.2 -> libgxps.so.2.2.2
        libavfilter.so.6 -> libavfilter.so.6.107.100
        libOpenCL.so.1 -> libOpenCL.so.1.0.0
        libsmbconf.so.0 -> libsmbconf.so.0
        libdcerpc-binding.so.0 -> libdcerpc-binding.so.0.0.1
        libnvidia-tls.so.430.26 -> libnvidia-tls.so.430.26
        libXfixes.so.3 -> libXfixes.so.3.1.0
        libbtrfs.so.0 -> libbtrfs.so.0.1
        libclucene-contribs-lib.so.1 -> libclucene-contribs-lib.so.2.3.3.4
        libfftw3.so.3 -> libfftw3.so.3.5.7
        libvisio-0.1.so.1 -> libvisio-0.1.so.1.0.6
        libtcmalloc_minimal_debug.so.4 -> libtcmalloc_minimal_debug.so.4.3.0
        libxcb-shm.so.0 -> libxcb-shm.so.0.0.0
        libtevent-util.so.0 -> libtevent-util.so.0.0.1
        libssl3.so -> libssl3.so
        libstartup-notification-1.so.0 -> libstartup-notification-1.so.0.0.0
        libapr-1.so.0 -> libapr-1.so.0.6.3
        libbrotlienc.so.1 -> libbrotlienc.so.1.0.4
        libaudio.so.2 -> libaudio.so.2.4
        libmpfr.so.6 -> libmpfr.so.6.0.1
        libmessaging-menu.so.0 -> libmessaging-menu.so.0.0.0
        libxcb-dri3.so.0 -> libxcb-dri3.so.0.0.0
        libboost_locale.so.1.65.1 -> libboost_locale.so.1.65.1
        libIexMath-2_2.so.12 -> libIexMath-2_2.so.12.0.0
        libXt.so.6 -> libXt.so.6.0.0
        libabw-0.1.so.1 -> libabw-0.1.so.1.0.2
        libidn2.so.0 -> libidn2.so.0.3.3
        libswscale.so.4 -> libswscale.so.4.8.100
        libGLESv1_CM_nvidia.so.1 -> libGLESv1_CM_nvidia.so.430.26
        libapt-pkg.so.5.0 -> libapt-pkg.so.5.0.2
        libutempter.so.0 -> libutempter.so.1.1.6
        libflite_cmu_us_slt.so.1 -> libflite_cmu_us_slt.so.2.1
        libmediaart-2.0.so.0 -> libmediaart-2.0.so.0.904.0
        libwx_gtk2u_xrc-3.0.so.0 -> libwx_gtk2u_xrc-3.0.so.0.4.0
        libnvidia-fatbinaryloader.so.430.26 -> libnvidia-fatbinaryloader.so.430.26
        libbrotlicommon.so.1 -> libbrotlicommon.so.1.0.4
        libtcmalloc_and_profiler.so.4 -> libtcmalloc_and_profiler.so.4.3.0
        libargon2.so.0 -> libargon2.so.0
        libgstsdp-1.0.so.0 -> libgstsdp-1.0.so.0.1401.0
        libtwolame.so.0 -> libtwolame.so.0.0.0
        libnvidia-eglcore.so.430.26 -> libnvidia-eglcore.so.430.26
        libcupsimage.so.2 -> libcupsimage.so.2
        libhunspell-1.6.so.0 -> libhunspell-1.6.so.0.0.1
        libpgm-5.2.so.0 -> libpgm-5.2.so.0.0.122
        libbabeltrace-dummy.so.1 -> libbabeltrace-dummy.so.1.0.0
        libecal-1.2.so.19 -> libecal-1.2.so.19.0.0
        libssh-gcrypt.so.4 -> libssh-gcrypt.so.4.5.0
        liblcms2.so.2 -> liblcms2.so.2.0.8
        libedataserverui-1.2.so.2 -> libedataserverui-1.2.so.2.0.0
        libXdamage.so.1 -> libXdamage.so.1.1.0
        libnvidia-glvkspirv.so.430.26 -> libnvidia-glvkspirv.so.430.26
        libflite_cmu_us_awb.so.1 -> libflite_cmu_us_awb.so.2.1
        libsvn_diff-1.so.1 -> libsvn_diff-1.so.1.0.0
        libgdk-x11-2.0.so.0 -> libgdk-x11-2.0.so.0.2400.32
        libcurl.so.4 -> libcurl.so.4.5.0
        libmythes-1.2.so.0 -> libmythes-1.2.so.0.0.0
        libexpatw.so.1 -> libexpatw.so.1.6.7
        libnorm.so.1 -> libnorm.so.1.0.0
        libGLESv2.so.2 -> libGLESv2.so.2.0.0
        libavahi-client.so.3 -> libavahi-client.so.3.2.9
        libxkbcommon-x11.so.0 -> libxkbcommon-x11.so.0.0.0
        libexempi.so.3 -> libexempi.so.3.4.5
        libgdk_pixbuf-2.0.so.0 -> libgdk_pixbuf-2.0.so.0.3611.0
        libwoff2enc.so.1.0.2 -> libwoff2enc.so.1.0.2
        libgphoto2.so.6 -> libgphoto2.so.6.0.0
        libgdk-3.so.0 -> libgdk-3.so.0.2200.30
        libva-x11.so.2 -> libva-x11.so.2.100.0
        libavahi-common.so.3 -> libavahi-common.so.3.5.3
        libgspell-1.so.1 -> libgspell-1.so.1.3.0
        librest-0.7.so.0 -> librest-0.7.so.0.0.0
        libsonic.so.0 -> libsonic.so.0.2.0
        libmpi_mpifh.so.20 -> libmpi_mpifh.so.20.11.0
        libndr.so.0 -> libndr.so.0.1.0
        libflite_cmu_indic_lang.so.1 -> libflite_cmu_indic_lang.so.2.1
        libmpi_usempi_ignore_tkr.so.20 -> libmpi_usempi_ignore_tkr.so.20.10.0
        libXtst.so.6 -> libXtst.so.6.1.0
        libhpip.so.0 -> libhpip.so.0.0.1
        libsvn_auth_kwallet-1.so.1 -> libsvn_auth_kwallet-1.so.1.0.0
        libchromaprint.so.1 -> libchromaprint.so.1.4.3
        libcupsmime.so.1 -> libcupsmime.so.1
        libespeak-ng.so.1 -> libespeak-ng.so.1.1.49
        libsnapd-glib.so.1 -> libsnapd-glib.so.1.0.0
        libxcb-keysyms.so.1 -> libxcb-keysyms.so.1.0.0
        libsvn_client-1.so.1 -> libsvn_client-1.so.1.0.0
        libxcb-present.so.0 -> libxcb-present.so.0.0.0
        libQt5Widgets.so.5 -> libQt5Widgets.so.5.9.5
        libijs-0.35.so -> libijs-0.35.so
        libsmbios_c.so.2 -> libsmbios_c.so.2.2.1
        libk5crypto.so.3 -> libk5crypto.so.3.1
        libasyncns.so.0 -> libasyncns.so.0.3.1
        libyajl.so.2 -> libyajl.so.2.1.0
        libinproctrace.so -> libinproctrace.so
        libplc4.so -> libplc4.so
        libraw.so.16 -> libraw.so.16.0.0
        libGLX_nvidia.so.0 -> libGLX_nvidia.so.430.26
        libbs2b.so.0 -> libbs2b.so.0.0.0
        liblz4.so.1 -> liblz4.so.1.7.1
        libXvMCW.so.1 -> libXvMCW.so.1.0.0
        libnetapi.so.0 -> libnetapi.so.0
        libsbc.so.1 -> libsbc.so.1.2.1
        libcdda_interface.so.0 -> libcdda_interface.so.0.10.2
        liborcus-parser-0.13.so.0 -> liborcus-parser-0.13.so.0.0.0
        liblouisutdml.so.8 -> liblouisutdml.so.8.0.0
        libfftw3q.so.3 -> libfftw3q.so.3.5.7
        libmtdev.so.1 -> libmtdev.so.1.0.0
        liblqr-1.so.0 -> liblqr-1.so.0.3.2
        libmwaw-0.3.so.3 -> libmwaw-0.3.so.3.0.13
        libgthread-2.0.so.0 -> libgthread-2.0.so.0.5600.4
        libnuma.so.1 -> libnuma.so.1.0.0
        libwayland-client.so.0 -> libwayland-client.so.0.3.0
        libopenmpt.so.0 -> libopenmpt.so.0.1.1
        librdmacm.so.1 -> librdmacm.so.1.1.17.1
        libgstriff-1.0.so.0 -> libgstriff-1.0.so.0.1401.0
        liblsan.so.0 -> liblsan.so.0.0.0
        libpipeline.so.1 -> libpipeline.so.1.5.0
        libopcodes-2.30-system.so -> libopcodes-2.30-system.so
        libudisks2.so.0 -> libudisks2.so.0.0.0
        libgstaudio-1.0.so.0 -> libgstaudio-1.0.so.0.1401.0
        libgnomekbd.so.8 -> libgnomekbd.so.8.0.0
        libdcerpc-samr.so.0 -> libdcerpc-samr.so.0.0.1
        libibus-1.0.so.5 -> libibus-1.0.so.5.0.517
        librevenge-0.0.so.0 -> librevenge-0.0.so.0.0.4
        libdmapsharing-3.0.so.2 -> libdmapsharing-3.0.so.2.9.39
        libssh-gcrypt_threads.so.4 -> libssh-gcrypt_threads.so.4.5.0
        libxcb-sync.so.1 -> libxcb-sync.so.1.0.0
        libvdpau.so.1 -> libvdpau.so.1.0.0
        libm17n-core.so.0 -> libm17n-core.so.0.4.1
        libuv.so.1 -> libuv.so.1.0.0
        libubsan.so.0 -> libubsan.so.0.0.0
        libXrandr.so.2 -> libXrandr.so.2.2.0
        libbluray.so.2 -> libbluray.so.2.0.2
        libisccfg.so.160 -> libisccfg.so.160.1.2
        librdf.so.0 -> librdf.so.0.0.0
        libndr-krb5pac.so.0 -> libndr-krb5pac.so.0.0.1
        libisc-pkcs11.so.169 -> libisc-pkcs11.so.169.0.1
        libaccountsservice.so.0 -> libaccountsservice.so.0.0.0
        libXcomposite.so.1 -> libXcomposite.so.1.0.0
        libheimntlm.so.0 -> libheimntlm.so.0.1.0
        libcdda_paranoia.so.0 -> libcdda_paranoia.so.0.10.2
        libgs.so.9 -> libgs.so.9.26
        libsoup-2.4.so.1 -> libsoup-2.4.so.1.8.0
        libtotem-plparser-mini.so.18 -> libtotem-plparser-mini.so.18.1.1
        libssl.so.1.1 -> libssl.so.1.1
        libjavascriptcoregtk-4.0.so.18 -> libjavascriptcoregtk-4.0.so.18.16.5
        libatomic.so.1 -> libatomic.so.1.2.0
        libform.so.5 -> libform.so.5.9
        libnghttp2.so.14 -> libnghttp2.so.14.15.2
        libunwind-ptrace.so.0 -> libunwind-ptrace.so.0.0.0
        libwhoopsie-preferences.so.0 -> libwhoopsie-preferences.so.0.0.0
        libxcb-image.so.0 -> libxcb-image.so.0.0.0
        libflite_cmu_time_awb.so.1 -> libflite_cmu_time_awb.so.2.1
        libgmime-3.0.so.0 -> libgmime-3.0.so.0.200.0
        libdconf.so.1 -> libdconf.so.1.0.0
        libiptc.so.0 -> libiptc.so.0.0.0
        libnvidia-cfg.so.1 -> libnvidia-cfg.so.430.26
        libfftw3_threads.so.3 -> libfftw3_threads.so.3.5.7
        libmlx4.so.1 -> libmlx4.so.1.0.17.1
        libxcb-res.so.0 -> libxcb-res.so.0.0.0
        libxcb-render.so.0 -> libxcb-render.so.0.0.0
        libapt-inst.so.2.0 -> libapt-inst.so.2.0.0
        liblockfile.so.1 -> liblockfile.so.1.0
        libpciaccess.so.0 -> libpciaccess.so.0.11.1
        libexslt.so.0 -> libexslt.so.0.8.17
        libdb-5.3.so -> libdb-5.3.so
        libodfgen-0.1.so.1 -> libodfgen-0.1.so.1.0.6
        libusbmuxd.so.4 -> libusbmuxd.so.4.0.0
        libflite_cmu_grapheme_lex.so.1 -> libflite_cmu_grapheme_lex.so.2.1
        libpeas-1.0.so.0 -> libpeas-1.0.so.0.2200.0
        libbind9.so.160 -> libbind9.so.160.0.6
        libclucene-shared.so.1 -> libclucene-shared.so.2.3.3.4
        libSDL2-2.0.so.0 -> libSDL2-2.0.so.0.8.0
        liborcus-mso-0.13.so.0 -> liborcus-mso-0.13.so.0.0.0
        libtsan.so.0 -> libtsan.so.0.0.0
        libopen-rte.so.20 -> libopen-rte.so.20.10.1
        libpcrecpp.so.0 -> libpcrecpp.so.0.0.1
        libharfbuzz-icu.so.0 -> libharfbuzz-icu.so.0.10702.0
        libdrm_radeon.so.1 -> libdrm_radeon.so.1.0.1
        libwx_gtk2u_core-3.0.so.0 -> libwx_gtk2u_core-3.0.so.0.4.0
        libnvidia-opencl.so.1 -> libnvidia-opencl.so.430.26
        libXxf86dga.so.1 -> libXxf86dga.so.1.0.0
        libMagickWand-6.Q16.so.3 -> libMagickWand-6.Q16.so.3.0.0
        libsvn_fs-1.so.1 -> libsvn_fs-1.so.1.0.0
        libunistring.so.2 -> libunistring.so.2.1.0
        libdv.so.4 -> libdv.so.4.0.3
        libXft.so.2 -> libXft.so.2.3.2
        libqmi-glib.so.5 -> libqmi-glib.so.5.2.0
        libnvidia-egl-wayland.so.1 -> libnvidia-egl-wayland.so.1.1.2
        libebook-contacts-1.2.so.2 -> libebook-contacts-1.2.so.2.0.0
        libxcb-util.so.1 -> libxcb-util.so.1.0.0
        libteamdctl.so.0 -> libteamdctl.so.0.1.5
        libsodium.so.23 -> libsodium.so.23.1.0
        libdbusmenu-glib.so.4 -> libdbusmenu-glib.so.4.0.12
        libwavpack.so.1 -> libwavpack.so.1.2.0
        libavahi-core.so.7 -> libavahi-core.so.7.0.2
        librhash.so.0 -> librhash.so.0
        libjbig.so.0 -> libjbig.so.0
        libxklavier.so.16 -> libxklavier.so.16.4.0
        libIlmImfUtil-2_2.so.22 -> libIlmImfUtil-2_2.so.22.0.0
        libeot.so.0 -> libeot.so.0.0.0
        libicudata.so.60 -> libicudata.so.60.2
        libOpenGL.so.0 -> libOpenGL.so.0.0.0
        libcups.so.2 -> libcups.so.2
        libfribidi.so.0 -> libfribidi.so.0.3.6
        libmm-glib.so.0 -> libmm-glib.so.0.3.0
        libplist.so.3 -> libplist.so.3.1.0
        libgnome-games-support-1.so.3 -> libgnome-games-support-1.so.3.0.0
        libcolamd.so.2 -> libcolamd.so.2.9.6
        libhcrypto.so.4 -> libhcrypto.so.4.1.0
        libibverbs.so.1 -> libibverbs.so.1.1.17.1
        libproxy.so.1 -> libproxy.so.1.0.0
        libwebp.so.6 -> libwebp.so.6.0.2
        libdcerpc-server.so.0 -> libdcerpc-server.so.0.0.1
        libglib-2.0.so.0 -> libglib-2.0.so.0.5600.4
        libtracker-sparql-2.0.so.0 -> libtracker-sparql-2.0.so.0.3.0
        libEGL.so.1 -> libEGL.so.1.0.0
        libXv.so.1 -> libXv.so.1.0.0
        libpulse-mainloop-glib.so.0 -> libpulse-mainloop-glib.so.0.0.5
        libfastjson.so.4 -> libfastjson.so.4.2.0
        libmpi_java.so.20 -> libmpi_java.so.20.10.0
        libestr.so.0 -> libestr.so.0.0.0
        libX11.so.6 -> libX11.so.6.3.0
        libvisual-0.4.so.0 -> libvisual-0.4.so.0.0.0
        libsamba-hostconfig.so.0 -> libsamba-hostconfig.so.0.0.1
        libzeitgeist-2.0.so.0 -> libzeitgeist-2.0.so.0.0.0
        libitm.so.1 -> libitm.so.1.0.0
        libxslt.so.1 -> libxslt.so.1.1.29
        libhpipp.so.0 -> libhpipp.so.0.0.1
        libyaml-0.so.2 -> libyaml-0.so.2.0.5
        libbfd-2.30-system.so -> libbfd-2.30-system.so
        libva-drm.so.2 -> libva-drm.so.2.100.0
        libXvMC.so.1 -> libXvMC.so.1.0.0
        libcrypto.so.1.1 -> libcrypto.so.1.1
        libwebpdemux.so.2 -> libwebpdemux.so.2.0.3
        libwpd-0.10.so.10 -> libwpd-0.10.so.10.0.2
        libical-glib.so.3 -> libical-glib.so.3.0.1
        libX11-xcb.so.1 -> libX11-xcb.so.1.0.0
        libpspell.so.15 -> libpspell.so.15.2.0
        libdrm_intel.so.1 -> libdrm_intel.so.1.0.0
        libcmis-c-0.5.so.5 -> libcmis-c-0.5.so.5.0.0
        libwmf-0.2.so.7 -> libwmf-0.2.so.7.1.0
        libavahi-glib.so.1 -> libavahi-glib.so.1.0.2
        libappstream-glib.so.8 -> libappstream-glib.so.8.0.10
        liblxc.so.1 -> liblxc.so.1.4.0
        libmpi_cxx.so.20 -> libmpi_cxx.so.20.10.0
        libdaemon.so.0 -> libdaemon.so.0.5.0
        libsmbldap.so.2 -> libsmbldap.so.2
        libmca_common_sm.so.20 -> libmca_common_sm.so.20.10.1
        libtic.so.5 -> libtic.so.5.9
        libunwind.so.8 -> libunwind.so.8.0.1
        libgc.so.1 -> libgc.so.1.0.3
        libgstfft-1.0.so.0 -> libgstfft-1.0.so.0.1401.0
        libpcsclite.so.1 -> libpcsclite.so.1.0.0
        liborcus-0.13.so.0 -> liborcus-0.13.so.0.0.0
        libasan.so.3 -> libasan.so.3.0.0
        libefivar.so.1 -> libefivar.so.1.34
        libGLX.so.0 -> libGLX.so.0.0.0
        libiec61883.so.0 -> libiec61883.so.0.1.1
        libassuan.so.0 -> libassuan.so.0.8.1
        libzmq.so.5 -> libzmq.so.5.1.5
        libwx_gtk2u_stc-3.0.so.0 -> libwx_gtk2u_stc-3.0.so.0.4.0
        libgstnet-1.0.so.0 -> libgstnet-1.0.so.0.1401.0
        libmpc.so.3 -> libmpc.so.3.1.0
        libflite_cmu_us_rms.so.1 -> libflite_cmu_us_rms.so.2.1
        libv4lconvert.so.0 -> libv4lconvert.so.0.0.0
        libgdbm.so.5 -> libgdbm.so.5.0.0
        libwnck-3.so.0 -> libwnck-3.so.0.3.0
        libQt5EglFSDeviceIntegration.so.5 -> libQt5EglFSDeviceIntegration.so.5.9.5
        libpanelw.so.5 -> libpanelw.so.5.9
        libmysofa.so.0 -> libmysofa.so.0.5.1
        libapt-private.so.0.0 -> libapt-private.so.0.0.0
        libevview3.so.3 -> libevview3.so.3.0.0
        libelf.so.1 -> libelf-0.170.so
        libgstreamer-1.0.so.0 -> libgstreamer-1.0.so.0.1401.0
        libgcr-ui-3.so.1 -> libgcr-ui-3.so.1.0.0
        libssl.so.1.0.0 -> libssl.so.1.0.0
        libpulse-simple.so.0 -> libpulse-simple.so.0.1.1
        libdw.so.1 -> libdw-0.170.so
        libXrender.so.1 -> libXrender.so.1.3.0
        libclutter-1.0.so.0 -> libclutter-1.0.so.0.2600.2
        libsvn_ra_svn-1.so.1 -> libsvn_ra_svn-1.so.1.0.0
        libXau.so.6 -> libXau.so.6.0.0
        libsvn_fs_x-1.so.1 -> libsvn_fs_x-1.so.1.0.0
        libvncclient.so.1 -> libvncclient.so.1.0.0
        libcroco-0.6.so.3 -> libcroco-0.6.so.3.0.1
        libgomp.so.1 -> libgomp.so.1.0.0
        libsvn_delta-1.so.1 -> libsvn_delta-1.so.1.0.0
        libbd_fs.so.2 -> libbd_fs.so.2.0.0
        libXcursor.so.1 -> libXcursor.so.1.0.2
        liborc-test-0.4.so.0 -> liborc-test-0.4.so.0.28.0
        libflite_usenglish.so.1 -> libflite_usenglish.so.2.1
        libwx_baseu-3.0.so.0 -> libwx_baseu-3.0.so.0.4.0
        libclucene-core.so.1 -> libclucene-core.so.2.3.3.4
        libpagemaker-0.0.so.0 -> libpagemaker-0.0.so.0.0.4
        libjackserver.so.0 -> libjackserver.so.0.1.0
        libffi.so.6 -> libffi.so.6.0.4
        libcdio_cdda.so.2 -> libcdio_cdda.so.2.0.0
        libdns.so.1100 -> libdns.so.1100.1.1
        libfftw3l_threads.so.3 -> libfftw3l_threads.so.3.5.7
        libmspub-0.1.so.1 -> libmspub-0.1.so.1.0.4
        libopenal.so.1 -> libopenal.so.1.18.2
        libexiv2.so.14 -> libexiv2.so.14.0.0
        libogg.so.0 -> libogg.so.0.8.2
        libnvidia-ifr.so.1 -> libnvidia-ifr.so.430.26
        libbluetooth.so.3 -> libbluetooth.so.3.18.16
        libLLVM-9.so.1 -> libLLVM-9.so.1
        libjson-glib-1.0.so.0 -> libjson-glib-1.0.so.0.400.2
        libXmuu.so.1 -> libXmuu.so.1.0.0
        libwx_gtk2u_qa-3.0.so.0 -> libwx_gtk2u_qa-3.0.so.0.4.0
        libtheora.so.0 -> libtheora.so.0.3.10
        libgrlnet-0.3.so.0 -> libgrlnet-0.3.so.0.0.3
        libepubgen-0.1.so.1 -> libepubgen-0.1.so.1.0.0
        libXdmcp.so.6 -> libXdmcp.so.6.0.0
        librevenge-generators-0.0.so.0 -> librevenge-generators-0.0.so.0.0.4
        libpixman-1.so.0 -> libpixman-1.so.0.34.0
        libnl-route-3.so.200 -> libnl-route-3.so.200.24.0
        libsecret-1.so.0 -> libsecret-1.so.0.0.0
        libperl.so.5.26 -> libperl.so.5.26.1
        libjpeg.so.8 -> libjpeg.so.8.1.2
        libboost_thread.so.1.65.1 -> libboost_thread.so.1.65.1
        libnss3.so -> libnss3.so
        libpytalloc-util.so.2 -> libpytalloc-util.so.2.1.10
        libgphoto2_port.so.12 -> libgphoto2_port.so.12.0.0
        libvolume_key.so.1 -> libvolume_key.so.1.2.2
        libwayland-server.so.0 -> libwayland-server.so.0.1.0
        libirs.so.160 -> libirs.so.160.0.4
        libtag.so.1 -> libtag.so.1.17.0
        libgdk_pixbuf_xlib-2.0.so.0 -> libgdk_pixbuf_xlib-2.0.so.0.3611.0
        liblangtag.so.1 -> liblangtag.so.1.4.1
        libnvidia-ptxjitcompiler.so.1 -> libnvidia-ptxjitcompiler.so.430.26
        libdrm_nouveau.so.2 -> libdrm_nouveau.so.2.0.0
        libnetsnmptrapd.so.30 -> libnetsnmptrapd.so.30.0.3
        libgpm.so.2 -> libgpm.so.2
        libquadmath.so.0 -> libquadmath.so.0.0.0
        libcolord.so.2 -> libcolord.so.2.0.5
        libicutest.so.60 -> libicutest.so.60.2
        libopenjp2.so.7 -> libopenjp2.so.2.3.0
        libicutu.so.60 -> libicutu.so.60.2
        libdc1394.so.22 -> libdc1394.so.22.2.1
        libwayland-cursor.so.0 -> libwayland-cursor.so.0.0.0
        libpackagekit-glib2.so.18 -> libpackagekit-glib2.so.18.1.3
        libdbusmenu-gtk.so.4 -> libdbusmenu-gtk.so.4.0.12
        libinfinipath.so.4 -> libinfinipath.so.4.0
        libsysmetrics.so.1 -> libsysmetrics.so.1
        libSM.so.6 -> libSM.so.6.0.1
        libavahi-ui-gtk3.so.0 -> libavahi-ui-gtk3.so.0.1.4
        libflite_cmu_us_kal.so.1 -> libflite_cmu_us_kal.so.2.1
        libfabric.so.1 -> libfabric.so.1.9.3
        libflite_cmu_indic_lex.so.1 -> libflite_cmu_indic_lex.so.2.1
        libtcmalloc.so.4 -> libtcmalloc.so.4.3.0
        libbabeltrace-ctf-metadata.so.1 -> libbabeltrace-ctf-metadata.so.1.0.0
        libxcb-shape.so.0 -> libxcb-shape.so.0.0.0
        libLLVM-6.0.so.1 -> libLLVM-6.0.so.1
        libtcl8.6.so -> libtcl8.6.so.0
        libraw_r.so.16 -> libraw_r.so.16.0.0
        libnvcuvid.so.1 -> libnvcuvid.so.430.26
        libcanberra.so.0 -> libcanberra.so.0.2.5
        libsvn_auth_gnome_keyring-1.so.1 -> libsvn_auth_gnome_keyring-1.so.1.0.0
        libfftw3_omp.so.3 -> libfftw3_omp.so.3.5.7
        libXi.so.6 -> libXi.so.6.1.0
        libdatrie.so.1 -> libdatrie.so.1.3.3
        libmtp.so.9 -> libmtp.so.9.3.0
        libflite_cmu_us_kal16.so.1 -> libflite_cmu_us_kal16.so.2.1
        libcilkrts.so.5 -> libcilkrts.so.5.0.0
        libwx_gtk2u_propgrid-3.0.so.0 -> libwx_gtk2u_propgrid-3.0.so.0.4.0
        libopus.so.0 -> libopus.so.0.5.2
        libappindicator3.so.1 -> libappindicator3.so.1.0.0
        libgrilo-0.3.so.0 -> libgrilo-0.3.so.0.1.3
        libpopt.so.0 -> libpopt.so.0.0.0
        libatspi.so.0 -> libatspi.so.0.0.1
        libnvidia-encode.so.1 -> libnvidia-encode.so.430.26
        librom1394.so.0 -> librom1394.so.0.3.0
        libsvn_fs_util-1.so.1 -> libsvn_fs_util-1.so.1.0.0
        libIntelXvMC.so.1 -> libIntelXvMC.so.1.0.0
        libyelp.so.0 -> libyelp.so.0.0.0
        libmp3lame.so.0 -> libmp3lame.so.0.0.0
        libgraphite2.so.3 -> libgraphite2.so.3.0.1
        libshine.so.3 -> libshine.so.3.0.1
        libgd.so.3 -> libgd.so.3.0.5
        libgstbasecamerabinsrc-1.0.so.0 -> libgstbasecamerabinsrc-1.0.so.0.1401.0
        libwacom.so.2 -> libwacom.so.2.6.1
        libgoa-backend-1.0.so.1 -> libgoa-backend-1.0.so.1.0.0
        libswresample.so.2 -> libswresample.so.2.9.100
        libfontembed.so.1 -> libfontembed.so.1.0.0
        libnpth.so.0 -> libnpth.so.0.1.1
        libICE.so.6 -> libICE.so.6.3.0
        libxcb-xv.so.0 -> libxcb-xv.so.0.0.0
        libedata-cal-1.2.so.28 -> libedata-cal-1.2.so.28.0.0
        libgettextsrc-0.19.8.1.so -> libgettextsrc.so
        libpng16.so.16 -> libpng16.so.16.34.0
        libarchive.so.13 -> libarchive.so.13.2.2
        libgobject-2.0.so.0 -> libgobject-2.0.so.0.5600.4
        libcolord-gtk.so.1 -> libcolord-gtk.so.1.0.3
        libwebpmux.so.3 -> libwebpmux.so.3.0.1
        libxkbcommon.so.0 -> libxkbcommon.so.0.0.0
        libxcb-render-util.so.0 -> libxcb-render-util.so.0.0.0
        libvpx.so.5 -> libvpx.so.5.0.0
        libnvidia-compiler.so.430.26 -> libnvidia-compiler.so.430.26
        libpoppler.so.73 -> libpoppler.so.73.0.0
        libinput.so.10 -> libinput.so.10.13.0
        libindicator3.so.7 -> libindicator3.so.7.0.0
        libcheese.so.8 -> libcheese.so.8.0.10
        libnetsnmpmibs.so.30 -> libnetsnmpmibs.so.30.0.3
        libgstgl-1.0.so.0 -> libgstgl-1.0.so.0.1401.0
        libavformat.so.57 -> libavformat.so.57.83.100
        libcdio.so.17 -> libcdio.so.17.0.0
        libxmlsec1-nss.so.1 -> libxmlsec1-nss.so.1.2.25
        libtotem.so.0 -> libtotem.so.0.0.0
        libxcb-glx.so.0 -> libxcb-glx.so.0.0.0
        libaacs.so.0 -> libaacs.so.0.6.0
        libgeocode-glib.so.0 -> libgeocode-glib.so.0.0.0
        libgssapi.so.3 -> libgssapi.so.3.0.0
        libshotwell-plugin-dev-1.0.so.0 -> libshotwell-plugin-dev-1.0.so.0.28.4
        libdcerpc.so.0 -> libdcerpc.so.0.0.1
        libgstapp-1.0.so.0 -> libgstapp-1.0.so.0.1401.0
        libwhoopsie.so.0 -> libwhoopsie.so.0.0
        libmca_common_verbs.so.20 -> libmca_common_verbs.so.20.10.0
        libeatmydata.so.1 -> libeatmydata.so.1.1.2
        libjack.so.0 -> libjack.so.0.1.0
        libcupscgi.so.1 -> libcupscgi.so.1
        libzstd.so.1 -> libzstd.so.1.3.3
        libsmime3.so -> libsmime3.so
        libxcb.so.1 -> libxcb.so.1.1.0
        libgstvideo-1.0.so.0 -> libgstvideo-1.0.so.0.1401.0
        libdns-pkcs11.so.1100 -> libdns-pkcs11.so.1100.1.1
        libtotem-plparser.so.18 -> libtotem-plparser.so.18.1.1
        libGLdispatch.so.0 -> libGLdispatch.so.0.0.0
        libpwquality.so.1 -> libpwquality.so.1.0.2
        libtiff.so.5 -> libtiff.so.5.3.0
        libical.so.3 -> libical.so.3.0.1
        libspectre.so.1 -> libspectre.so.1.1.8
        libfreerdp2.so.2 -> libfreerdp2.so.2.0.0
        libfreetype.so.6 -> libfreetype.so.6.15.0
        libharfbuzz.so.0 -> libharfbuzz.so.0.10702.0
        libwx_gtk2u_adv-3.0.so.0 -> libwx_gtk2u_adv-3.0.so.0.4.0
        libgstrtsp-1.0.so.0 -> libgstrtsp-1.0.so.0.1401.0
        libetonyek-0.1.so.1 -> libetonyek-0.1.so.1.0.7
        libdee-1.0.so.4 -> libdee-1.0.so.4.2.1
        libasound.so.2 -> libasound.so.2.0.0
        libasn1.so.8 -> libasn1.so.8.0.0
        librevenge-stream-0.0.so.0 -> librevenge-stream-0.0.so.0.0.4
        libfftw3l.so.3 -> libfftw3l.so.3.5.7
        libxcb-icccm.so.4 -> libxcb-icccm.so.4.0.0
        libbd_utils.so.2 -> libbd_utils.so.2.1.0
        libcmis-0.5.so.5 -> libcmis-0.5.so.5.0.0
        libavcodec.so.57 -> libavcodec.so.57.107.100
        libmhash.so.2 -> libmhash.so.2.0.1
        libbdplus.so.0 -> libbdplus.so.0.1.0
        libraw1394.so.11 -> libraw1394.so.11.1.0
        libxcb-randr.so.0 -> libxcb-randr.so.0.1.0
        libgssapi_krb5.so.2 -> libgssapi_krb5.so.2.2
        libbabeltrace.so.1 -> libbabeltrace.so.1.0.0
        libdouble-conversion.so.1 -> libdouble-conversion.so.1.0
        libssh_threads.so.4 -> libssh_threads.so.4.5.0
        libssh.so.4 -> libssh.so.4.5.0
        libQt5DBus.so.5 -> libQt5DBus.so.5.9.5
        libwx_gtk2u_richtext-3.0.so.0 -> libwx_gtk2u_richtext-3.0.so.0.4.0
        libGeoIP.so.1 -> libGeoIP.so.1.6.12
        libsamba-credentials.so.0 -> libsamba-credentials.so.0.0.1
        libsvn_subr-1.so.1 -> libsvn_subr-1.so.1.0.0
        libsvn_ra-1.so.1 -> libsvn_ra-1.so.1.0.0
        libsamplerate.so.0 -> libsamplerate.so.0.1.8
        libevdocument3.so.4 -> libevdocument3.so.4.0.0
        libsemanage.so.1 -> libsemanage.so.1
        libmlx5.so.1 -> libmlx5.so.1.4.17.1
        libsvn_repos-1.so.1 -> libsvn_repos-1.so.1.0.0
        libsuitesparseconfig.so.5 -> libsuitesparseconfig.so.5.1.2
        libxcb-xinerama.so.0 -> libxcb-xinerama.so.0.0.0
        libbrotlidec.so.1 -> libbrotlidec.so.1.0.4
        libpcreposix.so.3 -> libpcreposix.so.3.13.3
        libjsoncpp.so.1 -> libjsoncpp.so.1.7.4
        libieee1284.so.3 -> libieee1284.so.3.2.2
        libnvidia-cbl.so.430.26 -> libnvidia-cbl.so.430.26
        libgeoclue-2.so.0 -> libgeoclue-2.so.0.0.0
        libedata-book-1.2.so.25 -> libedata-book-1.2.so.25.0.0
        libwx_baseu_xml-3.0.so.0 -> libwx_baseu_xml-3.0.so.0.4.0
        libcairo.so.2 -> libcairo.so.2.11510.0
        librhythmbox-core.so.10 -> librhythmbox-core.so.10.0.0
        libgcr-base-3.so.1 -> libgcr-base-3.so.1.0.0
        libexttextcat-2.0.so.0 -> libexttextcat-2.0.so.0.0.0
        liblwres.so.160 -> liblwres.so.160.0.1
        libtdb.so.1 -> libtdb.so.1.3.15
        libtcmalloc_debug.so.4 -> libtcmalloc_debug.so.4.3.0
        libnvidia-opticalflow.so.1 -> libnvidia-opticalflow.so.430.26
        libsvn_fs_base-1.so.1 -> libsvn_fs_base-1.so.1.0.0
        libwx_gtk2u_ribbon-3.0.so.0 -> libwx_gtk2u_ribbon-3.0.so.0.4.0
        libndp.so.0 -> libndp.so.0.1.0
        libGLU.so.1 -> libGLU.so.1.3.1
        liborc-0.4.so.0 -> liborc-0.4.so.0.28.0
        libgmodule-2.0.so.0 -> libgmodule-2.0.so.0.5600.4
        libGLESv1_CM.so.1 -> libGLESv1_CM.so.1.0.0
        libXss.so.1 -> libXss.so.1.0.0
        libcanberra-gtk3.so.0 -> libcanberra-gtk3.so.0.1.9
        libsvn_ra_local-1.so.1 -> libsvn_ra_local-1.so.1.0.0
        libtheoradec.so.1 -> libtheoradec.so.1.1.4
        libpoppler-glib.so.8 -> libpoppler-glib.so.8.9.0
        libhx509.so.5 -> libhx509.so.5.0.0
        libbabeltrace-lttng-live.so.1 -> libbabeltrace-lttng-live.so.1.0.0
        libgsttag-1.0.so.0 -> libgsttag-1.0.so.0.1401.0
        libgstcheck-1.0.so.0 -> libgstcheck-1.0.so.0.1401.0
        libgstbase-1.0.so.0 -> libgstbase-1.0.so.0.1401.0
        libcrypto.so.1.0.0 -> libcrypto.so.1.0.0
        libgoa-1.0.so.0 -> libgoa-1.0.so.0.0.0
        libicalss.so.3 -> libicalss.so.3.0.1
        libsamdb.so.0 -> libsamdb.so.0.0.1
        libnfnetlink.so.0 -> libnfnetlink.so.0.2.0
        libndr-standard.so.0 -> libndr-standard.so.0.0.1
        libgtk-3.so.0 -> libgtk-3.so.0.2200.30
        libnetsnmpagent.so.30 -> libnetsnmpagent.so.30.0.3
        libxmlsec1.so.1 -> libxmlsec1.so.1.2.25
        libgusb.so.2 -> libgusb.so.2.0.10
        libxcb-xkb.so.1 -> libxcb-xkb.so.1.0.0
        libxmlsec1-openssl.so.1 -> libxmlsec1-openssl.so.1.2.25
        libisns.so.0 -> libisns.so.0
        libboost_system.so.1.65.1 -> libboost_system.so.1.65.1
        libgdbm_compat.so.4 -> libgdbm_compat.so.4.0.0
        libxcb-xfixes.so.0 -> libxcb-xfixes.so.0.0.0
        libgfortran.so.4 -> libgfortran.so.4.0.0
        libedit.so.2 -> libedit.so.2.0.56
        libebook-1.2.so.19 -> libebook-1.2.so.19.1.3
        libxapian.so.30 -> libxapian.so.30.4.0
        libnetfilter_conntrack.so.3 -> libnetfilter_conntrack.so.3.6.0
        libspeechd.so.2 -> libspeechd.so.2.6.0
        libmspack.so.0 -> libmspack.so.0.1.0
        libgnutls.so.30 -> libgnutls.so.30.14.10
        libaspell.so.15 -> libaspell.so.15.2.0
        libXaw.so.7 -> libXaw7.so.7.0.0
        libcaca++.so.0 -> libcaca++.so.0.99.19
        libphonenumber.so.7 -> libphonenumber.so.7.0
        libfreehand-0.1.so.1 -> libfreehand-0.1.so.1.0.2
        libsnappy.so.1 -> libsnappy.so.1.1.7
        libaprutil-1.so.0 -> libaprutil-1.so.0.6.1
        libshotwell-authenticator.so.0 -> libshotwell-authenticator.so.0.28.4
        libpulse.so.0 -> libpulse.so.0.20.2
        libFLAC.so.8 -> libFLAC.so.8.3.0
        libnetsnmphelpers.so.30 -> libnetsnmphelpers.so.30.0.3
        libQt5Svg.so.5 -> libQt5Svg.so.5.9.5
        libdjvulibre.so.21 -> libdjvulibre.so.21.6.0
        libfontconfig.so.1 -> libfontconfig.so.1.10.1
        libfftw3f_omp.so.3 -> libfftw3f_omp.so.3.5.7
        libgnome-menu-3.so.0 -> libgnome-menu-3.so.0.0.1
        libsoup-gnome-2.4.so.1 -> libsoup-gnome-2.4.so.1.8.0
        libsamba-policy.so.0 -> libsamba-policy.so.0.0.1
        libnvoptix.so.1 -> libnvoptix.so.430.26
        libQt5Network.so.5 -> libQt5Network.so.5.9.5
        libcamel-1.2.so.61 -> libcamel-1.2.so.61.0.0
        libmpdec.so.2 -> libmpdec.so.2.4.2
        libnotify.so.4 -> libnotify.so.4.0.0
        libgpgmepp.so.6 -> libgpgmepp.so.6.5.0
        libv4l2.so.0 -> libv4l2.so.0.0.0
        libsqlite3.so.0 -> libsqlite3.so.0.8.6
        libmbim-glib.so.4 -> libmbim-glib.so.4.2.0
        libatk-1.0.so.0 -> libatk-1.0.so.0.22810.1
        libXinerama.so.1 -> libXinerama.so.1.0.0
        libompitrace.so.20 -> libompitrace.so.20.10.0
        libvorbisfile.so.3 -> libvorbisfile.so.3.3.7
        libwayland-egl.so.1 -> libwayland-egl.so.1.0.0
        libQt5PrintSupport.so.5 -> libQt5PrintSupport.so.5.9.5
        libXRes.so.1 -> libXRes.so.1.0.0
        libpangocairo-1.0.so.0 -> libpangocairo-1.0.so.0.4000.14
        libmpi.so.20 -> libmpi.so.20.10.1
        libm17n-gui.so.0 -> libm17n-gui.so.0.4.1
        libminiupnpc.so.10 -> libminiupnpc.so.10
        libsvn_fs_fs-1.so.1 -> libsvn_fs_fs-1.so.1.0.0
        libfftw3f.so.3 -> libfftw3f.so.3.5.7
        libnvidia-glcore.so.430.26 -> libnvidia-glcore.so.430.26
        libgme.so.0 -> libgme.so.0.6.2
        libcupsfilters.so.1 -> libcupsfilters.so.1.0.0
        libicalss_cxx.so.3 -> libicalss_cxx.so.3.0.1
        libneon-gnutls.so.27 -> libneon-gnutls.so.27.3.2
        libpangoxft-1.0.so.0 -> libpangoxft-1.0.so.0.4000.14
        libva.so.2 -> libva.so.2.100.0
        libnetsnmp.so.30 -> libnetsnmp.so.30.0.3
        libEGL_nvidia.so.0 -> libEGL_nvidia.so.430.26
        libfwupd.so.2 -> libfwupd.so.2.0.0
        libfontenc.so.1 -> libfontenc.so.1.0.0
        libflite_cmulex.so.1 -> libflite_cmulex.so.2.1
        libXmu.so.6 -> libXmu.so.6.2.0
        libpangoft2-1.0.so.0 -> libpangoft2-1.0.so.0.4000.14
        libatk-bridge-2.0.so.0 -> libatk-bridge-2.0.so.0.0.0
        libevdev.so.2 -> libevdev.so.2.1.20
        libmenuw.so.5 -> libmenuw.so.5.9
        libstemmer.so.0d -> libstemmer.so.0d.0.0
        libXpm.so.4 -> libXpm.so.4.11.0
        libbd_part.so.2 -> libbd_part.so.2.0.0
        libisc.so.169 -> libisc.so.169.0.1
        libjbig2dec.so.0 -> libjbig2dec.so.0.0.0
        libQt5Gui.so.5 -> libQt5Gui.so.5.9.5
        libpython2.7.so.1.0 -> libpython2.7.so.1.0
        libthai.so.0 -> libthai.so.0.3.0
        libtevent.so.0 -> libtevent.so.0.9.34
        libgutenprint.so.2 -> libgutenprint.so.2.6.0
        libimobiledevice.so.6 -> libimobiledevice.so.6.0.0
        libmenu.so.5 -> libmenu.so.5.9
        libgio-2.0.so.0 -> libgio-2.0.so.0.5600.4
        libqpdf.so.21 -> libqpdf.so.21.0.2
        libcogl-path.so.20 -> libcogl-path.so.20.4.2
        libx265.so.146 -> libx265.so.146
        libgailutil.so.18 -> libgailutil.so.18.0.1
        libpeas-gtk-1.0.so.0 -> libpeas-gtk-1.0.so.0.2200.0
        libmozjs-52.so.0 -> libmozjs-52.so.0.0.0
        libsamba-errors.so.1 -> libsamba-errors.so.1
        libxtables.so.12 -> libxtables.so.12.0.0
        libcurl-gnutls.so.4 -> libcurl-gnutls.so.4.5.0
        libtheoraenc.so.1 -> libtheoraenc.so.1.1.2
        libImath-2_2.so.12 -> libImath-2_2.so.12.0.0
        libdumbnet.so.1 -> libdumbnet.so.1.0.1
        libcares.so.2 -> libcares.so.2.2.0
        libappstream.so.4 -> libappstream.so.0.12.0
        libgom-1.0.so.0 -> libgom-1.0.so.0.1.0
        libIex-2_2.so.12 -> libIex-2_2.so.12.0.0
        libcrack.so.2 -> libcrack.so.2.9.0
        libcdio_paranoia.so.2 -> libcdio_paranoia.so.2.0.0
        libgcab-1.0.so.0 -> libgcab-1.0.so.0.0.0
        libgailutil-3.so.0 -> libgailutil-3.so.0.0.0
        libvte-2.91.so.0 -> libvte-2.91.so.0.5200.2
        libshotwell-plugin-common.so.0 -> libshotwell-plugin-common.so.0.28.4
        libgif.so.7 -> libgif.so.7.0.0
        libXxf86vm.so.1 -> libXxf86vm.so.1.0.0
        libblockdev.so.2 -> libblockdev.so.2.0.0
        libflite.so.1 -> libflite.so.2.1
        libgstrtp-1.0.so.0 -> libgstrtp-1.0.so.0.1401.0
        libmpi_usempif08.so.20 -> libmpi_usempif08.so.20.10.0
        libvorbis.so.0 -> libvorbis.so.0.4.8
        libnettle.so.6 -> libnettle.so.6.4
        libpanel.so.5 -> libpanel.so.5.9
        libavdevice.so.57 -> libavdevice.so.57.10.100
        libunwind-x86_64.so.8 -> libunwind-x86_64.so.8.0.1
        libicui18n.so.60 -> libicui18n.so.60.2
        libxatracker.so.2 -> libxatracker.so.2.5.0
        libx264.so.152 -> libx264.so.152
        libsane.so.1 -> libsane.so.1.0.27
        libsoxr.so.0 -> libsoxr.so.0.1.1
        libraptor2.so.0 -> libraptor2.so.0.0.0
        libsensors.so.4 -> libsensors.so.4.4.0
        libnvidia-ml.so.1 -> libnvidia-ml.so.430.26
        libwebkit2gtk-4.0.so.37 -> libwebkit2gtk-4.0.so.37.44.4
        libm17n.so.0 -> libm17n.so.0.4.1
        libGLX_mesa.so.0 -> libGLX_mesa.so.0.0.0
        libglut.so.3 -> libglut.so.3.9.0
        libhpdiscovery.so.0 -> libhpdiscovery.so.0.0.1
        libicalvcal.so.3 -> libicalvcal.so.3.0.1
        libtk8.6.so -> libtk8.6.so.0
        libmca_common_libfabric.so.20 -> libmca_common_libfabric.so.20.10.0
        libpsl.so.5 -> libpsl.so.5.2.0
        libsamba-util.so.0 -> libsamba-util.so.0.0.1
        libldb.so.1 -> libldb.so.1.2.3
        libsndfile.so.1 -> libsndfile.so.1.0.28
        liblber-2.4.so.2 -> liblber-2.4.so.2.10.8
        libsasl2.so.2 -> libsasl2.so.2.0.25
        libavutil.so.55 -> libavutil.so.55.78.100
        libkpathsea.so.6 -> libkpathsea.so.6.2.3
        librubberband.so.2 -> librubberband.so.2.1.0
        libtasn1.so.6 -> libtasn1.so.6.5.5
        libtalloc.so.2 -> libtalloc.so.2.1.10
        libgettextlib-0.19.8.1.so -> libgettextlib.so
        libksba.so.8 -> libksba.so.8.11.6
        libdrm_amdgpu.so.1 -> libdrm_amdgpu.so.1.0.0
        libprotobuf.so.10 -> libprotobuf.so.10.0.0
        libass.so.9 -> libass.so.9.0.2
        libboost_iostreams.so.1.65.1 -> libboost_iostreams.so.1.65.1
        libe-book-0.1.so.1 -> libe-book-0.1.so.1.0.3
        libicuuc.so.60 -> libicuuc.so.60.2
        libgpod.so.4 -> libgpod.so.4.3.2
        libgirepository-1.0.so.1 -> libgirepository-1.0.so.1.0.0
        libboost_filesystem.so.1.65.1 -> libboost_filesystem.so.1.65.1
        libgudev-1.0.so.0 -> libgudev-1.0.so.0.2.0
        libshout.so.3 -> libshout.so.3.2.0
        libgbm.so.1 -> libgbm.so.1.0.0
        libheimbase.so.1 -> libheimbase.so.1.0.0
        libkrb5.so.3 -> libkrb5.so.3.3
        libwpg-0.3.so.3 -> libwpg-0.3.so.3.0.1
        libip6tc.so.0 -> libip6tc.so.0.1.0
        libcheese-gtk.so.25 -> libcheese-gtk.so.25.1.0
        libgexiv2.so.2 -> libgexiv2.so.2.0.0
        libtcmalloc_minimal.so.4 -> libtcmalloc_minimal.so.4.3.0
        libunity-extras.so.9 -> libunity-extras.so.9.0.2
        libgstcontroller-1.0.so.0 -> libgstcontroller-1.0.so.0.1401.0
        libwx_gtk2u_aui-3.0.so.0 -> libwx_gtk2u_aui-3.0.so.0.4.0
        libtiffxx.so.5 -> libtiffxx.so.5.3.0
        libGLESv2_nvidia.so.2 -> libGLESv2_nvidia.so.430.26
        libglapi.so.0 -> libglapi.so.0.0.0
        libcolordprivate.so.2 -> libcolordprivate.so.2.0.5
        libwx_baseu_net-3.0.so.0 -> libwx_baseu_net-3.0.so.0.4.0
        libroken.so.18 -> libroken.so.18.1.0
        libxvidcore.so.4 -> libxvidcore.so.4.3
        libmpx.so.2 -> libmpx.so.2.0.1
        libdebconfclient.so.0 -> libdebconfclient.so.0.0.0
        libxkbfile.so.1 -> libxkbfile.so.1.0.2
        libedataserver-1.2.so.23 -> libedataserver-1.2.so.23.0.0
        libfftw3f_threads.so.3 -> libfftw3f_threads.so.3.5.7
        libenchant.so.1 -> libenchant.so.1.6.0
        libm17n-flt.so.0 -> libm17n-flt.so.0.4.1
        libkrb5.so.26 -> libkrb5.so.26.0.0
        libgpgme.so.11 -> libgpgme.so.11.19.0
        libkrb5support.so.0 -> libkrb5support.so.0.1
        libdrm.so.2 -> libdrm.so.2.4.0
        libwind.so.0 -> libwind.so.0.0.0
        libgstallocators-1.0.so.0 -> libgstallocators-1.0.so.0.1401.0
        libsnmp.so.30 -> libsnmp.so.30.0.3
        liblouis.so.14 -> liblouis.so.14.1.2
        libpolkit-agent-1.so.0 -> libpolkit-agent-1.so.0.0.0
        libpcap.so.0.8 -> libpcap.so.1.8.1
        libgraphene-1.0.so.0 -> libgraphene-1.0.so.0.800.0
        libpolkit-gobject-1.so.0 -> libpolkit-gobject-1.so.0.0.0
        libnssutil3.so -> libnssutil3.so
        libIlmThread-2_2.so.12 -> libIlmThread-2_2.so.12.0.0
        libspeex.so.1 -> libspeex.so.1.5.0
        libavc1394.so.0 -> libavc1394.so.0.3.0
        libmutter-2.so.0 -> libmutter-2.so.0.0.0
        libpcaudio.so.0 -> libpcaudio.so.0.0.0
        libyaml-cpp.so.0.5 -> libyaml-cpp.so.0.5.2
        libprofiler.so.0 -> libprofiler.so.0.4.8
        liblua5.3-c++.so.0 -> liblua5.3-c++.so.0.0.0
        libip4tc.so.0 -> libip4tc.so.0.1.0
        libaa.so.1 -> libaa.so.1.0.4
        libguile-2.0.so.22 -> libguile-2.0.so.22.8.1
        libhogweed.so.4 -> libhogweed.so.4.4
        libgtop-2.0.so.11 -> libgtop-2.0.so.11.0.0
        libcairo-gobject.so.2 -> libcairo-gobject.so.2.11510.0
        libopts.so.25 -> libopts.so.25.16.1
        libgrlpls-0.3.so.0 -> libgrlpls-0.3.so.0.0.0
        libsvn_wc-1.so.1 -> libsvn_wc-1.so.1.0.0
        libI810XvMC.so.1 -> libI810XvMC.so.1.0.0
        libwps-0.4.so.4 -> libwps-0.4.so.4.0.8
        libvorbisenc.so.2 -> libvorbisenc.so.2.0.11
        libao.so.4 -> libao.so.4.1.1
        libupower-glib.so.3 -> libupower-glib.so.3.0.1
        libHalf.so.12 -> libHalf.so.12.0.0
        libfftw3q_threads.so.3 -> libfftw3q_threads.so.3.5.7
        libv4l1.so.0 -> libv4l1.so.0.0.0
        libgnomekbdui.so.8 -> libgnomekbdui.so.8.0.0
        libmagic.so.1 -> libmagic.so.1.0.0
        libstdc++.so.6 -> libstdc++.so.6.0.25
        libpaper.so.1 -> libpaper.so.1.1.2
        libformw.so.5 -> libformw.so.5.9
        libwbclient.so.0 -> libwbclient.so.0.14
        libserf-1.so.1 -> libserf-1.so.1.3.0
        libndr-nbt.so.0 -> libndr-nbt.so.0.0.1
        libcuda.so.1 -> libcuda.so.430.26
        libhpmud.so.0 -> libhpmud.so.0.0.6
        libcogl.so.20 -> libcogl.so.20.4.2
        libgstpbutils-1.0.so.0 -> libgstpbutils-1.0.so.0.1401.0
        libasan.so.4 -> libasan.so.4.0.0
        libjacknet.so.0 -> libjacknet.so.0.1.0
        libgnome-desktop-3.so.17 -> libgnome-desktop-3.so.17.0.0
        libxml2.so.2 -> libxml2.so.2.9.4
        libhyphen.so.0 -> libhyphen.so.0.3.0
        libguilereadline-v-18.so.18 -> libguilereadline-v-18.so.18.0.0
        libQt5Core.so.5 -> libQt5Core.so.5.9.5
        libepoxy.so.0 -> libepoxy.so.0.0.0
        libQt5EglFsKmsSupport.so.5 -> libQt5EglFsKmsSupport.so.5.9.5
        libXext.so.6 -> libXext.so.6.4.0
        libebackend-1.2.so.10 -> libebackend-1.2.so.10.0.0
        libhwloc.so.5 -> libhwloc.so.5.7.6
        libXfont2.so.2 -> libXfont2.so.2.0.0
        libsgutils2.so.2 -> libsgutils2.so.2.0.0
        libwx_gtk2u_html-3.0.so.0 -> libwx_gtk2u_html-3.0.so.0.4.0
        liblua5.3.so.0 -> liblua5.3.so.0.0.0
        libgweather-3.so.15 -> libgweather-3.so.15.0.0
        libspeexdsp.so.1 -> libspeexdsp.so.1.5.0
        libunity.so.9 -> libunity.so.9.0.2
        libcolorhug.so.2 -> libcolorhug.so.2.0.5
        libefiboot.so.1 -> libefiboot.so.1.34
        libavresample.so.3 -> libavresample.so.3.7.0
        libdbusmenu-gtk3.so.4 -> libdbusmenu-gtk3.so.4.0.12
        libpython3.6m.so.1.0 -> libpython3.6m.so.1.0
        libcdr-0.1.so.1 -> libcdr-0.1.so.1.0.4
        libicuio.so.60 -> libicuio.so.60.2
        libjansson.so.4 -> libjansson.so.4.11.0
        libcrystalhd.so.3 -> libcrystalhd.so.3.6
        libMagickCore-6.Q16.so.3 -> libMagickCore-6.Q16.so.3.0.0
        libwebrtc_audio_processing.so.1 -> libwebrtc_audio_processing.so.1.0.0
        libwmflite-0.2.so.7 -> libwmflite-0.2.so.7.0.1
        libxshmfence.so.1 -> libxshmfence.so.1.0.0
        libmpg123.so.0 -> libmpg123.so.0.44.8
        libbd_loop.so.2 -> libbd_loop.so.2.0.0
        libical_cxx.so.3 -> libical_cxx.so.3.0.1
        libunwind-coredump.so.0 -> libunwind-coredump.so.0.0.0
        libgtk-x11-2.0.so.0 -> libgtk-x11-2.0.so.0.2400.32
        libltdl.so.7 -> libltdl.so.7.3.1
        libgdm.so.1 -> libgdm.so.1.0.0
        libgck-1.so.0 -> libgck-1.so.0.0.0
        libgmp.so.10 -> libgmp.so.10.3.2
        libotf.so.0 -> libotf.so.0.0.0
        libldap_r-2.4.so.2 -> libldap_r-2.4.so.2.10.8
        libnautilus-extension.so.1 -> libnautilus-extension.so.1.4.0
        libdbus-glib-1.so.2 -> libdbus-glib-1.so.2.3.4
        libzvbi.so.0 -> libzvbi.so.0.13.2
        libgccpp.so.1 -> libgccpp.so.1.0.3
        libsigsegv.so.2 -> libsigsegv.so.2.0.5
        libbd_part_err.so.2 -> libbd_part_err.so.2.0.0
        libcaca.so.0 -> libcaca.so.0.99.19
        libfftw3q_omp.so.3 -> libfftw3q_omp.so.3.5.7
        libwx_gtk2u_gl-3.0.so.0 -> libwx_gtk2u_gl-3.0.so.0.4.0
        libdotconf.so.0 -> libdotconf.so.0.0.1
        libQt5XcbQpa.so.5 -> libQt5XcbQpa.so.5.9.5
        libbd_crypto.so.2 -> libbd_crypto.so.2.0.0
        libwoff2common.so.1.0.2 -> libwoff2common.so.1.0.2
        libpostproc.so.54 -> libpostproc.so.54.7.100
        librsvg-2.so.2 -> librsvg-2.so.2.40.20
        libgnome-bluetooth.so.13 -> libgnome-bluetooth.so.13.0.1
        libpango-1.0.so.0 -> libpango-1.0.so.0.4000.14
        libboost_date_time.so.1.65.1 -> libboost_date_time.so.1.65.1
        libcogl-pango.so.20 -> libcogl-pango.so.20.4.2
        libnma.so.0 -> libnma.so.0.0.0
        libIlmImf-2_2.so.22 -> libIlmImf-2_2.so.22.0.0
        libwinpr2.so.2 -> libwinpr2.so.2.0.0
        libmpxwrappers.so.2 -> libmpxwrappers.so.2.0.1
        libexif.so.12 -> libexif.so.12.3.3
        libsmbclient.so.0 -> libsmbclient.so.0.2.3
        libgnome-autoar-0.so.0 -> libgnome-autoar-0.so.0.0.0
        libsvn_ra_serf-1.so.1 -> libsvn_ra_serf-1.so.1.0.0
        libbabeltrace-ctf-text.so.1 -> libbabeltrace-ctf-text.so.1.0.0
        libart_lgpl_2.so.2 -> libart_lgpl_2.so.2.3.21
/lib:
        libhandle.so.1 -> libhandle.so.1.0.3
/usr/lib:
        libnatpmp.so.1 -> libnatpmp.so.1
        libhgfs.so.0 -> libhgfs.so.0.0.0
        libBLT.2.5.so.8.6 -> libBLT.2.5.so.8.6
        libnetpbm.so.10 -> libnetpbm.so.10.0
        libvgauth.so.0 -> libvgauth.so.0.0.0
        libDeployPkg.so.0 -> libDeployPkg.so.0.0.0
        libgjs.so.0 -> libgjs.so.0.0.0
        libguestlib.so.0 -> libguestlib.so.0.0.0
        libvmtools.so.0 -> libvmtools.so.0.0.0
        libBLTlite.2.5.so.8.6 -> libBLTlite.2.5.so.8.6
/sbin/ldconfig.real: Can't create temporary cache file /etc/ld.so.cache~: Permission denied
(venv3) ron@climber2:~/fake-text-detect$ 

"
39462,ReduceLROnPlateau keeps executing lr reduction block of code after min_lr has been reached,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): through pip
- TensorFlow version (use command below): 2.2
- Python version: 3.7
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

ReduceLR will execute the part of the code that reduces LR even if lr ""equals"" min_lr. While this is not a problem if we are just dealing with LR since the value will technically never go below min_lr, it is an issue if you are trying to do any other execution in this block of code (ie. weight decay).

Given that there is a min check for this block of code, i assume it was not intended for us to enter this block of code if min_lr has already been achieved. Either get rid of the min check if we don't care about re-executing this code for no reason, or make sure this block of code is not executed if lr is at min-lr.

This issue clearly seems to be a round/precision related issue where 
we set our lr `K.set_value(self.model.optimizer.lr, new_lr)`

But then next iteration we fetch the lr
`old_lr = float(K.get_value(self.model.optimizer.lr))`

and get a slightly different value so that 
`if old_lr > self.min_lr:`

does not work.

So yes in a way this does not affect default code, and it only matter if we do custom code, but i still think this is a bug that really shouldn't happen.

**Describe the expected behavior**

Code block that is only supposed to execute if lr does not equal min_lr does not execute if lr equals min_lr

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
x = np.random.normal(size=10)
y = np.random.normal(size=10)

model = Sequential()
model.add(Dense(1))

reduce_lr = ReduceLROnPlateau(monitor='loss',
                              min_delta=0.01,
                              patience=3,
                              min_lr=0.001,
                              verbose=2)

model.compile(loss='mse', optimizer=SGD(0.01))

history = model.fit(x,
                    y,
                    epochs=25,
                    verbose=2,
                    shuffle=True,
                    batch_size=1,
                    callbacks=[reduce_lr])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39460,tf-nightly-gpu not updated?,"the lastest version I can pip install is 20200508, it hasn't been updated? I thought my pip manager broke or something. "
39459,"Deceleration of fit by 10 times, after tf.keras.models.load_model('model.h5')","I had the same problem as:
https://github.com/tensorflow/tensorflow/issues/34767

After installing tf-nightly-gpu"" and checked scenario 3: model = Model_Functional_API() + model.save('saved_model.h5'), crashes are gone.

But, fit speed after model loading from file, became very slow.
When i create model like ""modelC = Model(...)"", fit from cold start was about 4-5 min, on next fit it was 3 min.

Fit on model loaded from file show ETA above 30 min.
Recompiliing after load has no effect.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

if os.path.isfile(""model.h5""):
    modelC = tf.keras.models.load_model('model.h5')#    
else: 
    if1 = Input(shape=(tr1.shape[1], tr1.shape[2]), name='input1')
    if2 = Input(shape=(tr2.shape[1], tr2.shape[2]), name='input2')
    if3 = Input(shape=(tr3.shape[1], tr3.shape[2]), name='input3')
    if4 = Input(shape=(tr4.shape[1], tr4.shape[2]), name='input4')
    x1 = LSTM(tr1.shape[2], batch_input_shape=(batch_size,timesteps,tr1.shape[2]), return_sequences=True)(if1)
    x2 = LSTM(tr2.shape[2], batch_input_shape=(batch_size,timesteps,tr2.shape[2]), return_sequences=True)(if2)
    x3 = LSTM(tr3.shape[2], batch_input_shape=(batch_size,timesteps,tr3.shape[2]), return_sequences=True)(if3)
    x4 = LSTM(tr4.shape[2], batch_input_shape=(batch_size,timesteps,tr4.shape[2]), return_sequences=True)(if4)
    combined = concatenate([x1, x2, x3, x4])
    x = LSTM(512, return_sequences=True)(combined)
    x = TimeDistributed(Dense(tst.shape[2]*4, activation=""softmax""))(x)
    out = Dense(tst.shape[2], activation=""softmax"")(x)
    modelC = Model(inputs=[if1,if2,if3,if4], outputs=out)
    modelC.save(""model.h5"")
modelC.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-4)
,loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False,label_smoothing=0, reduction=tf.keras.losses.Reduction.AUTO )
,metrics=[tf.keras.metrics.CategoricalAccuracy()])


- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Fedora 27 (Workstation Edition)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):

tf-estimator-nightly   2.3.0.dev2020051201
tf-nightly             2.2.0.dev20200508
tf-nightly-gpu         2.2.0.dev20200508
tensorboard            2.1.1
tensorboard-plugin-wit 1.6.0.post3
tensorflow             2.1.0
tensorflow-estimator   2.1.0
tensorflow-gpu         2.1.0
tensorrt               7.0.0.11

- TensorFlow version (use command below):
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
unknown 2.1.0

- Python version:
Python 3.7.7

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
libcudnn7.x86_64                       7.6.5.33-1.cuda10.2             @nvidia-machine-learning
libcudnn7-devel.x86_64                 7.6.5.33-1.cuda10.2             nvidia-machine-learning


+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2060    Off  | 00000000:04:00.0 Off |                  N/A |
| 46%   53C    P2    38W / 160W |   4392MiB /  5934MiB |     38%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1271      G   /usr/libexec/Xorg                              9MiB |
|    0      1507      G   /usr/bin/gnome-shell                           8MiB |
|    0      7195      C   /root/anaconda3/bin/python                  4361MiB |
+-----------------------------------------------------------------------------+

- GPU model and memory:
RTX 2060 
72GB DDR4

**Describe the current behavior**
30 min on fit after loading model

**Describe the expected behavior**
3 min of fit after loading model, like when i contsruct model

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39458,Keras predict is slow on first call when using variable input shape,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Collab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla K80  11441MiB

**Describe the current behavior**

When using a variable input shape the first prediction for a new shape is slow. Following predictions for the same shape are fast. 

**Describe the expected behavior**

I would expect to see the same speed when the change in shape is small. 

**Standalone code to reproduce the issue**

This simple collab shows the problem using a ResNet50.

https://colab.research.google.com/drive/1s2-O_cRtwL_kkk2Mm-rq-xiDHdZDs5h4?usp=sharing

I notice that a warning about retracing is shown, but I don't know how to use that information to solve the problem 
"
39457,"uint16, uint32 comparisons throw errors","<em>Some datatypes such as uint16, uint32 can't be compared.</em>

**System information**
Ran on google colab.


**Describe the current behavior**
tensors with dtypes uint16, uint32 can't be compared with any other datatypes.

**Describe the expected behavior**
comparison for these datatypes should be supported/

**Standalone code to reproduce the issue**
```
import tensorflow as tf

ones = tf.ones((2, 3), dtype = tf.uint32)
zeros = tf.zeros((2, 3), dtype = tf.uint32)
tf.math.equal(ones, zeros)
```
Error log:
```
---------------------------------------------------------------------------

NotFoundError                             Traceback (most recent call last)

<ipython-input-62-2f39e3ae3672> in <module>()
      3 ones = tf.ones((2, 3), dtype = tf.uint32)
      4 zeros = tf.zeros((2, 3), dtype = tf.uint32)
----> 5 tf.math.equal(ones, zeros)

4 frames

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: Could not find valid device for node.
Node:{{node Equal}}
All kernels registered for op Equal :
  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
  device='GPU'; T in [DT_BOOL]
  device='GPU'; T in [DT_COMPLEX128]
  device='GPU'; T in [DT_COMPLEX64]
  device='GPU'; T in [DT_INT64]
  device='GPU'; T in [DT_INT16]
  device='GPU'; T in [DT_INT8]
  device='GPU'; T in [DT_INT32]
  device='GPU'; T in [DT_UINT8]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_HALF]
  device='GPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_BOOL]
  device='CPU'; T in [DT_STRING]
  device='CPU'; T in [DT_COMPLEX128]
  device='CPU'; T in [DT_COMPLEX64]
  device='CPU'; T in [DT_INT64]
  device='CPU'; T in [DT_INT32]
  device='CPU'; T in [DT_BFLOAT16]
  device='CPU'; T in [DT_INT16]
  device='CPU'; T in [DT_INT8]
  device='CPU'; T in [DT_UINT8]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_HALF]
  device='CPU'; T in [DT_FLOAT]
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_QUINT8, DT_QINT32, DT_BFLOAT16, DT_COMPLEX128, DT_HALF]
 [Op:Equal]
```

Stackverflow question: [stackoverflow](https://stackoverflow.com/questions/61751605/compare-tensor-of-unsigned-int-to-python-int/61752565#61752565)
"
39456,Keras Model.evaluate progress bar doesn't work in graph mode in TF2.2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

Calling `Model.evaluate` in graph mode doesn't produce any stdout.

**Describe the expected behavior**

A progress bar should be displayed along with the loss values, as in TF<2.2.

**Standalone code to reproduce the issue**

https://colab.research.google.com/gist/drasmuss/b4d074f13c4bed294a1403f351e4d263
"
39455,Custom layer return AttributeError during model saving,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): `conda install tensorflow`
- TensorFlow version (CPU) : 2.1.0
- Python version: 3.7.7



**Describe the current behavior**

I was working on a model and I used this below custom attention layer, After training for few epochs I tried to save my model and I'm getting an attribute error (pasted below) which I think is related to the custom attention layer.

Here is the custom attention layer used: 

```
    class AttentionWeightedAverage(Layer):

        def __init__(self, return_attention=False, **kwargs):
            self.init = initializers.get('uniform')
            self.supports_masking = True
            self.return_attention = return_attention
            super(AttentionWeightedAverage, self).__init__(** kwargs)
    
        def build(self, input_shape):
            self.input_spec = [InputSpec(ndim=3)]
            assert len(input_shape) == 3
    
            self.w = self.add_weight(shape=(input_shape[2], 1),
                                     name='{}_w'.format(self.name),
                                     initializer=self.init, trainable=True)
            super(AttentionWeightedAverage, self).build(input_shape)
    
        def call(self, h, mask=None):
            h_shape = K.shape(h)
            d_w, T = h_shape[0], h_shape[1]
            
            logits = K.dot(h, self.w)  # w^T h
            logits = K.reshape(logits, (d_w, T))
            alpha = K.exp(logits - K.max(logits, axis=-1, keepdims=True))  # exp
            
            # masked timesteps have zero weight
            if mask is not None:
                mask = K.cast(mask, K.floatx())
                alpha = alpha * mask
            
            alpha = alpha / (K.sum(alpha, axis=1, keepdims=True) + K.epsilon()) # softmax
            r = K.sum(h * K.expand_dims(alpha), axis=1)  # r = h*alpha^T
            h_star = K.tanh(r)  # h^* = tanh(r)
            if self.return_attention:
                return [h_star, alpha]
            return h_star
    
        def get_output_shape_for(self, input_shape):
            return self.compute_output_shape(input_shape)
    
        def compute_output_shape(self, input_shape):
            output_len = input_shape[2]
            if self.return_attention:
                return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]
            return (input_shape[0], output_len)
    
        def compute_mask(self, input, input_mask=None):
            if isinstance(input_mask, list):
                return [None] * len(input_mask)
            else:
                return None
```
Here is my sample architecture used:

```
        inputs = Input((shape,))
        x = Embedding(input_dim=1000, output_dim=10,
                      mask_zero=True)(inputs)
        x = Bidirectional(LSTM(10, return_sequences=True))(x)
        weights, _ = AttentionWeightedAverage(return_attention=True)(x)
        outputs = Dense(self.n_classes, activation='softmax')(weights)
        my_model= Model([inputs], [outputs])
```

After training I used my_model.save('save_models') and I got this below error, I'm posting the entire traceback related to the error,


```
    Traceback (most recent call last):
      File ""classifiers/main.py"", line 26, in <module>
        main()
      File ""classifiers/main.py"", line 18, in main
        clf.model.save(f'./classifiers/saved_models/{args.model_name}')
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\engine\network.p
        signatures, options)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\save.py"",
        signatures, options)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        save_lib.save(model, filepath, signatures, options)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\saved_model\save.py"", 
        checkpoint_graph_view)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\saved_model\signature_
        functions = saveable_view.list_functions(saveable_view.root)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\saved_model\save.py"", 
        self._serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\engine\base_laye
        .list_functions_for_serialization(serialization_cache))
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        fns = self.functions_to_serialize(serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        serialization_cache).functions_to_serialize)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        serialization_cache))
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        original_fns = _replace_child_layer_functions(layer, serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        serialization_cache).functions)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        '{}_layer_call_and_return_conditional_losses'.format(layer.name))
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        self.add_trace(*self._input_signature)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        fn.get_concrete_function(*args, **kwargs)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\saving\saved_mod
        return super(LayerCall, self).get_concrete_function(*args, **kwargs)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\eager\def_function.py""
        self._initialize(args, kwargs, add_initializers_to=initializers)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\eager\def_function.py""
        *args, **kwds))
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\eager\function.py"", li
        graph_function, _, _ = self._maybe_define_function(args, kwargs)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\eager\function.py"", li
        graph_function = self._create_graph_function(args, kwargs)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\eager\function.py"", li
        capture_by_value=self._capture_by_value),
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\framework\func_graph.p
        func_outputs = python_func(*func_args, **func_kwargs)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\eager\def_function.py""
        return weak_wrapped_fn().__wrapped__(*args, **kwds)
        return layer_call(inputs, *args, **kwargs), layer.get_losses_for(inputs)
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\classifiers\blstm_attention.py"", line 43, in 
    call
        logits = K.dot(h, self.w)  # w^T h
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 1653, in dot
        if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):
      File ""C:\Users\user\miniconda3\envs\user\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 1202, in ndim
        dims = x.shape._dims
    AttributeError: 'list' object has no attribute 'shape'
```

**Describe the expected behavior**

It suppose to save the model.

**Standalone code to reproduce the issue**
Here's a sample colab to reproduce the similar error,

https://colab.research.google.com/drive/1RDcJwpVbT6JR8_LA52r1nHPSK0w1HuY7?usp=sharing"
39454,Documentaton issue: tf.split num_or_size_splits parameter,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/split

## Description of issue (what needs changing):

> If `num_or_size_splits` is an integer, then `value` is split along the dimension axis into `num_split` smaller tensors. This requires that `value.shape[axis]` is divisible by `num_split`.

What is `num_split` here? I think this should be

> If `num_or_size_splits` is an integer, then **we call it `num_split` and** `value` is split along the dimension axis into `num_split` smaller tensors. This requires that `value.shape[axis]` is divisible by `num_split`.
"
39453,CuDNN LSTM failed with large batch size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2, 2.1
- Python version: 3.7.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.2, 10.1
- GPU model and memory: 1 x 2080Ti 11GB, 2 x 2080 8GB

**Describe the current behavior**
Sample code below fail with error:

> 2020-05-12 12:59:37.956635: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-05-12 12:59:38.245540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-05-12 12:59:39.426585: E tensorflow/stream_executor/dnn.cc:613] CUDNN_STATUS_EXECUTION_FAILED
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1847): 'cudnnRNNForwardTrainingEx( cudnn.handle(), rnn_desc.handle(), input_desc.data_handle(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.data_handle(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-05-12 12:59:39.446537: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 32, 32, 1, 5, 12928, 32]
2020-05-12 12:59:39.457993: I tensorflow/stream_executor/stream.cc:1990] [stream=0000022BBB859000,impl=0000022BBF43D210] did not wait for [stream=0000022BBB859180,impl=0000022BBF43D930]
2020-05-12 12:59:39.462903: I tensorflow/stream_executor/stream.cc:4938] [stream=0000022BBB859000,impl=0000022BBF43D210] did not memcpy host-to-device; source: 0000022BB566AF00
2020-05-12 12:59:39.467874: F tensorflow/core/common_runtime/gpu/gpu_util.cc:340] CPU->GPU Memcpy failed

If I disable GPU device or set mask_zero to False, or force to use not cudnn LTSM (via tf.keras.layers.RNN(tf.keras.layers.LSTMCell(32))(x)) it works.

I think it's somehow related with thing what I on each batch process on model not 128 (batch size) items, but 128x101 items.

**Describe the expected behavior**

It must compile without this error.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

from tensorflow.keras.layers import Input, Activation, Embedding, LSTM, Dense, Dropout, Flatten, Concatenate, Dot
from tensorflow.keras.models import Model
from tensorflow.keras.utils import Sequence

#tf.config.set_visible_devices([], 'GPU')

strategy = tf.distribute.MirroredStrategy()

words = tf.constant(((1,1,1,1,1),(1,1,1,1,1)))
products = tf.ones((10000,101), dtype=tf.int32)

test_dataset = tf.data.Dataset.from_tensor_slices(products)
test_dataset = test_dataset.batch(128, drop_remainder=True)
test_dataset = strategy.experimental_distribute_dataset(test_dataset)

def create_model(words_count):
    input_words = Input(shape=tf.TensorShape(5), dtype='int32', name='input_words')

    x = Embedding(output_dim=32, input_dim=words_count, input_length=5, mask_zero=True)(input_words)
    #x = Embedding(output_dim=32, input_dim=words_count, input_length=5, mask_zero=False)(input_words)

    x = LSTM(32)(x)
    #x = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(32))(x)

    model = Model(inputs=input_words, outputs=x)
    return model

with strategy.scope():
    model = create_model(11111)
    optimizer = tf.keras.optimizers.SGD()

print(model.summary())

@tf.function
def test_step(b_cmp):
    cmp_words = tf.gather(words, b_cmp)
    tmp = tf.reshape(cmp_words, (-1,5))
    tmp = model(tmp, training=False)
   
    # ...

    r = tf.reduce_sum(tmp)
    return r

for b_cmp in test_dataset:
    strategy.run(test_step, args=(b_cmp,))
```
"
39451,Custom metric: update_state() raises error.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.1
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 10.1/ 7.6.5
- GPU model and memory: -

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I am trying to build a custom multiclass precision and recall for a image classification task. 
- If i run the code below without keras and the @tf.function annotator (tf_run), it works fine. 
- If i'm trying to use the @tf-function annotator or use the keras model.fit function, the error below is thrown. 
- To make sure that the error is not caused by my own implementation I copied the implementation of BinaryTruePositives  from [the tf website](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Metric) into my own file and tried to run it, but it produced the same error. I know this metric doesnt make much sense here, but it's only for testing purposes.
- I tried the code below on Google Colab and there it works fine.
-  I am aware of [this issue](https://github.com/tensorflow/tensorflow/issues/30711 ), but removing the return statements had no effects.

The code to reproduce this is shown below. 

**Describe the expected behavior**
The custom metric should work like without the @tf.function annotator.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import keras

class BinaryTruePositives(tf.keras.metrics.Metric):

    def __init__(self, name='binary_true_positives', **kwargs):
        super(BinaryTruePositives, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name='tp', initializer='zeros')

    def update_state(self, y_true, y_pred, sample_weight=None):
        y_true = tf.cast(y_true, tf.bool)
        y_pred = tf.cast(y_pred, tf.bool)

        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))
        values = tf.cast(values, self.dtype)
        if sample_weight is not None:
            sample_weight = tf.cast(sample_weight, self.dtype)
            sample_weight = tf.broadcast_weights(sample_weight, values)
            values = tf.multiply(values, sample_weight)
        self.true_positives.assign_add(tf.reduce_sum(values))

    def result(self):
        return self.true_positives


class fashion_model(tf.keras.Model):
    def __init__(self):
        super(fashion_model, self).__init__()
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)

        self.flatten = tf.keras.layers.Flatten(data_format='channels_last')
        self.dense1 = tf.keras.layers.Dense(units=128, input_shape=(28 * 28,), activation='relu')
        self.out_layer = tf.keras.layers.Dense(units=10)

        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True,
            reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)

        self.train_loss = tf.keras.metrics.Mean('train_loss')
        self.train_btp = BinaryTruePositives()
        self.train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()

    def call(self, inputs, training=None, mask=None):
        x = self.flatten(inputs)
        x = self.dense1(x)
        x = self.out_layer(x)
        return x


@tf.function
def train_step(model, sample):
    images, labels = sample

    with tf.GradientTape() as tape:
        logits = model(images, training=True)
        loss = model.loss_object(y_pred=logits, y_true=labels)

    gradients = tape.gradient(loss, model.trainable_variables)
    model.optimizer.apply_gradients(grads_and_vars=zip(gradients, model.trainable_variables))

    model.train_loss(loss)
    model.train_accuracy.update_state(y_pred=logits, y_true=labels)
    model.train_btp.update_state(y_pred=tf.argmax(logits, axis=-1), y_true=labels)


def tf_run():
    fashion_mnist = keras.datasets.fashion_mnist

    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

    class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
                   'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

    train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
    train_dataset.shuffle(5000)
    train_dataset = train_dataset.batch(32, drop_remainder=True)
    model = fashion_model()

    for epoch in range(1, 10):

        for sample in train_dataset:
            train_step(model, sample)

        template = 'Epoch {}, Loss: {} , Acc: {}, BinaryTP: {} '
        print(template.format(epoch,
                              model.train_loss.result(),
                              model.train_accuracy.result(),
                              model.train_btp.result()))

        model.train_loss.reset_states()
        model.train_btp.reset_states()
        model.train_accuracy.reset_states()


def keras_run():
    fashion_mnist = keras.datasets.fashion_mnist

    (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()

    model = keras.Sequential([
        keras.layers.Flatten(input_shape=(28, 28)),
        keras.layers.Dense(128, activation='relu'),
        keras.layers.Dense(10)
    ])

    model.compile(optimizer='adam',
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                  metrics=['accuracy', BinaryTruePositives()])

    model.fit(train_images, train_labels, epochs=10)


if __name__ == ""__main__"":
    tf_run()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```python
Traceback (most recent call last):
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/Code/playground/tf_metric_issue.py"", line 192, in <module>
    main()
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/Code/playground/tf_metric_issue.py"", line 188, in main
    tf_run()
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/Code/playground/tf_metric_issue.py"", line 150, in tf_run
    model.train_step(sample)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2385, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2699, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2589, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 3207, in bound_method_wrapper
    return wrapped_fn(*args, **kwargs)
  File ""/home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in converted code:

    /home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/Code/playground/tf_metric_issue.py:130 train_step  *
        self.train_btp.update_state(y_pred=tf.argmax(logits, axis=-1), y_true=labels)
    /home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/Code/playground/tf_metric_issue.py:81 decorated  *
        update_op = update_state_fn(*args, **kwargs)
    /home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:568 __call__
        result = self._call(*args, **kwds)
    /home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py:638 _call
        return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    /home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1609 _filtered_call
        self.captured_inputs)
    /home/jakob/Dokumente/INFORMATIK/_SOMMERSEMESTER_20/Masterarbeit/pollenanalysis2.0/pollenvenv/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py:1711 _call_flat
        flat_outputs = forward_function.call(ctx, )

    TypeError: call() missing 1 required positional argument: 'args'
```"
39450,Asynchronous keras model loading,"Dear developers, 

May I ask if you could create a way to asynchronously load several Keras models at the same time? I am developing a software where I need to load approximately 50-100 models into a single python programme from local files and it takes a lot of time to do that because I can not find a way to do it in a non-consecutive way.

Let me know if I am missing something. So far, I was not able to use any kind of multithreading to achieve that due to gridlocks.

Yours sincerely,
Aleksandr Bykov
"
39449,keras.model.Model does not work with SparseTensor multiplied with Tensor in functional API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): unknown 2.1.0 (from conda)
- Python version: 3.7.7
- CUDA/cuDNN version: -
- GPU model and memory: (CPU only)

**Describe the current behavior**

Wrapping sparse operation with Lambda layer does not help in functional model if I try to multiply SparseTensor and Tensor using tf.sparse.sparse_dense_matmul.
There is an exception:
```
ValueError: Sparse ops are not supported with functional models with built-in layer wrapping. Please wrap the sparse ops in a Lambda layer like: 

        weights_mult = lambda x: tf.sparse.sparse_dense_matmul(x, weights)
        output = tf.keras.layers.Lambda(weights_mult)(input)
```

**Describe the expected behavior**

Sparse operation is working with usual Tensor if it is wrapped in Lambda layer.

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import models

def generate_sparse_matrix():
    return tf.SparseTensor(indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=[3, 4])

input_model = layers.Input(shape=(8,))
dense_layer = layers.Dense(3, activation='relu')(input_model)
sparse_dense_matmul = lambda x: tf.sparse.sparse_dense_matmul(x[0], x[1], adjoint_a=True, adjoint_b=True)
multiplied = layers.Lambda(sparse_dense_matmul)((generate_sparse_matrix(), dense_layer))
multiplied = tf.transpose(multiplied)

out_layer = layers.Dense(1, activation='sigmoid', dtype='float32')(multiplied)

model = models.Model(input_model, out_layer)

```
Colab:
https://colab.research.google.com/drive/1SEvbnO8lrMCZQdVvjyd6Tmiabi1b3_UE?usp=sharing
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-2361bce90c51> in <module>
      5 multiplied = tf.transpose(multiplied)
      6 
----> 7 out_layer = layers.Dense(1, activation='sigmoid', dtype='float32')(multiplied)
      8 
      9 model = models.Model(input_model, out_layer)


~\Anaconda3\envs\tensor\lib\site-packages\tensorflow_core\python\keras\engine\base_layer.py in __call__(self, inputs, *args, **kwargs)
    718     # framework.
    719     if build_graph and base_layer_utils.needs_keras_history(inputs):
--> 720       base_layer_utils.create_keras_history(inputs)
    721 
    722     # Clear eager losses on top level model call.

~\Anaconda3\envs\tensor\lib\site-packages\tensorflow_core\python\keras\engine\base_layer_utils.py in create_keras_history(tensors)
    185     keras_tensors: The Tensors found that came from a Keras Layer.
    186   """"""
--> 187   _, created_layers = _create_keras_history_helper(tensors, set(), [])
    188   return created_layers
    189 

~\Anaconda3\envs\tensor\lib\site-packages\tensorflow_core\python\keras\engine\base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    247               constants[i] = backend.function([], op_input)([])
    248       processed_ops, created_layers = _create_keras_history_helper(
--> 249           layer_inputs, processed_ops, created_layers)
    250       name = op.name
    251       node_def = op.node_def.SerializeToString()

~\Anaconda3\envs\tensor\lib\site-packages\tensorflow_core\python\keras\engine\base_layer_utils.py in _create_keras_history_helper(tensors, processed_ops, created_layers)
    221             'Sparse ops are not supported with functional models with built-in '
    222             'layer wrapping. Please wrap the sparse ops in a Lambda layer like'
--> 223             ': \n{lambda_example}\n'.format(lambda_example=lambda_example))
    224 
    225       # Recursively set `_keras_history`.

ValueError: Sparse ops are not supported with functional models with built-in layer wrapping. Please wrap the sparse ops in a Lambda layer like: 

        weights_mult = lambda x: tf.sparse.sparse_dense_matmul(x, weights)
        output = tf.keras.layers.Lambda(weights_mult)(input)

```"
39448,Change to on_train_batch_end causes averaged values of metrics.,"Logging batch values for the various metrics has been a very handy tool, somewhere in the transition from 1.13 to 1.14 the callback api switched from using on_batch_end, to on_train_batch_end. Along with this switch came averaged metrics.

I am trying to report my results using tensorflow in a science journal, and I significantly have to change what I am reporting because tensorflow has changed the api on a minor version change.

Is there a way to get the actual per batch metrics? 

Right now, I have different metrics for versions 1.14 and 1.13, and it is not appropriate for a professional publication where I am aiming to demonstrate something repeatable. "
39447,Has someone ported tensorflow example codes to anyother microcontroller?,Need porting information of tensorflow? I'm trying to use tensorflow example on my RX family  Microcontroller. Can someone help me?
39446,Segfault on tf.linalg.svd,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, macOS 10.14.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de 2.1.0
- Python version: 3.7.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
`tf.linalg.svd` segfaults when input shape has at least one element being 0.

Also tested on the latest [tf-2.2.0](https://colab.research.google.com/drive/1cmER2YqlgwtXANSi8YbYW8TtEnt7wSPt?usp=sharing) in Colab, and segfault still exists.
**Describe the expected behavior**
Should not segfault.
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
import tensorflow as tf
import numpy as np

tf.linalg.svd(np.random.rand(2, 0)) # segfault
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39445,"Why tf.contrib.distribute.OneDeviceStrategy is used, the Variable is defined as ResourceVariable?","I used tf 1.14. Here is the code.

```
distribution = tf.contrib.distribute.OneDeviceStrategy(""device:CPU:0"")
run_config = tf.estimator.RunConfig(
      session_config = config,
      model_dir=FLAGS.output_dir,
      train_distribute=distribution,
      save_checkpoints_steps=FLAGS.save_checkpoints_steps,
     )
```

I found that when the OneDeviceStrategy is used, the variable was defined as ResourceVariable. But when this strategy is deleted, the variable was defined as VariableV2. Why chould this happend, it is said in the official document, that when use_resource or eager mode is enabled, the variable is defined as resource variable."
39442,"MemoryError: Unable to allocate 4.00 MiB for an array with shape (512, 512, 2) and data type float64 ?","Environment:
Windows 10
Anaconda python 3.6
Spyder 3.3.0
Tensorflow 2.1.0
GPU GeForce RTX  11GB

Error during in the middle of training:

> Epoch 294/1500
>  25/100 [======>.......................] - ETA: 16s - loss: 6373.7927 - mean_io_u: 0.9811Traceback (most recent call last):
> 
>   File ""<ipython-input-1-39886f6de552>"", line 1, in <module>
>     runfile('D:/feasibilityAnalysis/RiversideProject/Code/Semantic-Segmentationtfkeras/Mytrain.py', wdir='D:/feasibilityAnalysis/RiversideProject/Code/Semantic-Segmentationtfkeras')
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 827, in runfile
>     execfile(filename, namespace)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\spyder_kernels\customize\spydercustomize.py"", line 110, in execfile
>     exec(compile(f.read(), filename, 'exec'), namespace)
> 
>   File ""D:/feasibilityAnalysis/RiversideProject/Code/Semantic-Segmentationtfkeras/Mytrain.py"", line 283, in <module>
>     initial_epoch=args.initial_epoch)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\util\deprecation.py"", line 324, in new_func
>     return func(*args, **kwargs)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 1306, in fit_generator
>     initial_epoch=initial_epoch)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 819, in fit
>     use_multiprocessing=use_multiprocessing)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 342, in fit
>     total_epochs=epochs)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 128, in run_one_epoch
>     batch_outs = execution_function(iterator)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py"", line 98, in execution_function
>     distributed_function(input_fn))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 568, in __call__
>     result = self._call(*args, **kwds)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\def_function.py"", line 599, in _call
>     return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 2363, in __call__
>     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1611, in _filtered_call
>     self.captured_inputs)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 1692, in _call_flat
>     ctx, args, cancellation_manager=cancellation_manager))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\function.py"", line 545, in call
>     ctx=ctx)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\eager\execute.py"", line 67, in quick_execute
>     six.raise_from(core._status_to_exception(e.code, message), None)
> 
>   File ""<string>"", line 3, in raise_from
> 
> ResourceExhaustedError: 2 root error(s) found.
>   (0) Resource exhausted:  MemoryError: Unable to allocate 4.00 MiB for an array with shape (512, 512, 2) and data type float64
> Traceback (most recent call last):
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\ops\script_ops.py"", line 236, in __call__
>     ret = func(*args)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 789, in generator_py_func
>     values = next(generator_state.get_iterator(iterator_id))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 881, in get
>     six.reraise(*sys.exc_info())
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\six.py"", line 703, in reraise
>     raise value
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 875, in get
>     inputs = self.queue.get(block=True).get()
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\multiprocessing\pool.py"", line 644, in get
>     raise self._value
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\multiprocessing\pool.py"", line 119, in worker
>     result = (True, func(*args, **kwds))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 663, in get_index
>     return _SHARED_SEQUENCES[uid][i]
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\keras_preprocessing\image\iterator.py"", line 65, in __getitem__
>     return self._get_batches_of_transformed_samples(index_array)
> 
>   File ""D:\feasibilityAnalysis\RiversideProject\Code\Semantic-Segmentationtfkeras\utils\data_generator.py"", line 76, in _get_batches_of_transformed_samples
>     label = one_hot(label, self.num_classes)
> 
>   File ""D:\feasibilityAnalysis\RiversideProject\Code\Semantic-Segmentationtfkeras\utils\utils.py"", line 124, in one_hot
>     heat_map = np.ones(shape=label.shape[0:2] + (num_classes,))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\numpy\core\numeric.py"", line 207, in ones
>     a = empty(shape, dtype, order)
> 
> MemoryError: Unable to allocate 4.00 MiB for an array with shape (512, 512, 2) and data type float64
> 
> 
> 	 [[{{node PyFunc}}]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
> 	 [[IteratorGetNext]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
> 	 [[metrics/mean_io_u/StatefulPartitionedCall/confusion_matrix/assert_less_1/Assert/AssertGuard/else/_34/Assert/data_4/_84]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
>   (1) Resource exhausted:  MemoryError: Unable to allocate 4.00 MiB for an array with shape (512, 512, 2) and data type float64
> Traceback (most recent call last):
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\ops\script_ops.py"", line 236, in __call__
>     ret = func(*args)
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\data\ops\dataset_ops.py"", line 789, in generator_py_func
>     values = next(generator_state.get_iterator(iterator_id))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 881, in get
>     six.reraise(*sys.exc_info())
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\six.py"", line 703, in reraise
>     raise value
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 875, in get
>     inputs = self.queue.get(block=True).get()
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\multiprocessing\pool.py"", line 644, in get
>     raise self._value
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\multiprocessing\pool.py"", line 119, in worker
>     result = (True, func(*args, **kwds))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 663, in get_index
>     return _SHARED_SEQUENCES[uid][i]
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\keras_preprocessing\image\iterator.py"", line 65, in __getitem__
>     return self._get_batches_of_transformed_samples(index_array)
> 
>   File ""D:\feasibilityAnalysis\RiversideProject\Code\Semantic-Segmentationtfkeras\utils\data_generator.py"", line 76, in _get_batches_of_transformed_samples
>     label = one_hot(label, self.num_classes)
> 
>   File ""D:\feasibilityAnalysis\RiversideProject\Code\Semantic-Segmentationtfkeras\utils\utils.py"", line 124, in one_hot
>     heat_map = np.ones(shape=label.shape[0:2] + (num_classes,))
> 
>   File ""C:\Users\Ibrahim Khalilullah\.conda\envs\TF2p1p0\lib\site-packages\numpy\core\numeric.py"", line 207, in ones
>     a = empty(shape, dtype, order)
> 
> MemoryError: Unable to allocate 4.00 MiB for an array with shape (512, 512, 2) and data type float64
> 
> 
> 	 [[{{node PyFunc}}]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
> 	 [[IteratorGetNext]]
> Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
> 
> 0 successful operations.
> 0 derived errors ignored. [Op:__inference_distributed_function_6588]
> 
> Function call stack:
> distributed_function -> distributed_function
> 

Is there any suggestion?"
39440,how to set the gpu device in tensorflow? ,"import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""1""

this command does not set to the device 1 it keep using the device 0"
39439,"Windows C++ tensorflow_cc.dll has overlapping memory address between string gpu options for ""allocator type"" and ""visible device list""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0 branched from 5b900cfe4b3b848f577315a0dde09a729f770e95
- Python version: NA
- Bazel version (if compiling from source): 0.19.2 
- GCC/Compiler version (if compiling from source): MSVC 2015
- CUDA/cuDNN version: 10.0.130, 9.2.148
- GPU model and memory: NVIDIA GP100 16Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
NA

**Describe the current behavior**

I am creating as session as follows adapted from original code 

```cpp
   std::unique_ptr<tensorflow::Session>* session;
   tensorflow::SessionOptions options;
   tensorflow::ConfigProto* config = &options.config;
   float fraction =0.8;
   int whichGPU = 0;
   int cuda_device_count=1;
   tensorflow::GraphDef graph_def;
   tensorflow::status = tensorflow::ReadBinaryProto(tensorflow::Env::Default(), ""C:\\\models\\graph.pb"", &graph_def);
   auto* device_count = options.config.mutable_device_count();
   device_count->insert({ ""GPU"", cuda_device_count });
   device_count->insert({ ""CPU"", 1 });
   options.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(fraction);
   options.config.mutable_gpu_options()->set_visible_device_list(std::to_string(whichGPU));
   session->reset(tensorflow::NewSession(options));
  (*session)->Create(graph_def);
```

which results in 

```
    70 2020-05-12 09:41:28.214176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] 
    Found device 0 with properties: 
   71 name: Quadro GP100 major: 6 minor: 0 memoryClockRate(GHz): 1.4425
   72 pciBusID: 0000:01:00.0
   73 totalMemory: 16.00GiB freeMemory: 13.28GiB
   74 2020-05-12 09:41:28.215329: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] 
Adding visible gpu devices: 0
   75 2020-05-12 09:41:28.952392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
   76 2020-05-12 09:41:28.952785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
   77 2020-05-12 09:41:28.953095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
    78 2020-05-12 09:41:28.953962: E tensorflow/core/common_runtime/gpu/gpu_process_state.cc:106] Invalid allocator type: 0
   79 2020-05-12 09:41:28.954425: E tensorflow/core/common_runtime/session.cc:64] Failed to create session: Internal: Failed to get memory allocator for TF GPU 0 with 6899999744 bytes of memory.
```

**Describe the expected behavior**

Session is created and runs on GPU 0 only using only 80% of available memory

**Standalone code to reproduce the issue**

```cpp 
#include ""tensorflow/core/protobuf/control_flow.pb.h""
#include ""tensorflow/core/protobuf/config.pb.h""
#include <iostream>

int main() {
  tensorflow::GPUOptions gpu_options;

  gpu_options.set_visible_device_list(""0"");

  std::cout << ""allocator_type "" << gpu_options.allocator_type() << std::endl; //print 0

}
```

**Other info / logs** 

Please see the following issues
https://github.com/tensorflow/tensorflow/issues/16291
https://github.com/fo40225/tensorflow-windows-wheel/issues/39

I have built my tensorflow.dll as follows:

$ENV:USE_BAZEL_VERSION=""0.19.2""
$ENV:PYTHON_BIN_PATH=C:\ProgramData\Anaconda3\python.exe
$ENV:Path += "";C:\msys64\usr\bin""
$ENV:Path += "";C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\bin""
$ENV:Path += "";C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\extras\CUPTI\libx64""
$ENV:Path += "";C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\cudnn-9.2-windows10-x64-v7.5.0.56\cuda\bin""
$ENV:BAZEL_SH = ""C:\msys64\usr\bin\bash.exe""
$ENV:CUDA_TOOLKIT_PATH=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.2\""
$ENV:TF_CUDA_VERSION=""9.2""
$ENV:CUDNN_INSTALL_PATH=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\cudnn-9.2-windows10-x64-v7.5.0.56\cuda""
$ENV:TF_CUDNN_VERSION=""7""
$ENV:TF_NCCL_VERSION=""1""
$ENV:TF_CUDA_COMPUTE_CAPABILITIES=""3.5,3.7,5.0,5.2,6.0,6.1""
$ENV:TF_CUDA_CLANG=""0""
$ENV:TF_NEED_CUDA=""1""
$ENV:TF_NEED_ROCM=""0""
$ENV:TF_NEED_OPENCL_SYCL=""0""


$params = ""configure.py"",""""
Remove-Item -Recurse -Force ""C:\Windows\system32\config\systemprofile\_bazel_SYSTEM\install\75b09cf1ac98c0ffb0534079b30efcc4""
cmd /c ""ECHO Y"" | & python.exe @params 
bazel.exe clean --expunge
bazel.exe build --copt=-nvcc_options=disable-warnings --test_tag_filters=-no_oss,-gpu,-benchmark-test,-nomac,-no_mac --announce_rc --test_timeout 300,450,1200,3600 --test_size_filters=small,medium --jobs=12 //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so 

edits have been made to the following files:

within

tensorflow/BUILD

```
`""//tensorflow:windows"": [],`
```

becomes
```
""//tensorflow:windows"": [
            ""-def:"" +  # This line must be directly followed by the exported_symbols_msvc.lds file
            ""$(location //tensorflow:tf_exported_symbols_msvc.lds)"",
        ],
```

and within 
`tf_cc_shared_object` the function of `tensorflow/BUILD`

```
    visibility = [""//visibility:public""],
    deps = [
        ""//tensorflow:tf_exported_symbols.lds"",
        ""//tensorflow:tf_version_script.lds"",
        ""//tensorflow/c:c_api"",
        ""//tensorflow/c/eager:c_api"",
```

becomes

```
    visibility = [""//visibility:public""],
    deps = [
        ""//tensorflow:tf_exported_symbols.lds"",
        ""//tensorflow:tf_exported_symbols_msvc.lds"",
        ""//tensorflow:tf_version_script.lds"",
        ""//tensorflow/c:c_api"",
        ""//tensorflow/c/eager:c_api"",
```

The contents of `tf_exported_symbols_msvc.lds` are

```
LIBRARY tensorflow_cc
EXPORTS
    ??0MetaGraphDef@tensorflow@@QEAA@XZ
    ??1MetaGraphDef@tensorflow@@UEAA@XZ
    ??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z
    ??1LogMessageFatal@internal@tensorflow@@UEAA@XZ
    ??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z
    ??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ
    ?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ
    ?NewString@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ
    ?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z
    ?SlowCopyFrom@Status@tensorflow@@AEAAXPEBUState@12@@Z
    ?_GraphDef_default_instance_@tensorflow@@3VGraphDefDefaultTypeInternal@1@A
    ?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z
    ?InitMain@port@tensorflow@@YAXPEBDPEAHPEAPEAPEAD@Z
    ?FastUInt64ToBufferLeft@strings@tensorflow@@YA_K_KPEAD@Z
    ?StrCat@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@@Z
    ?empty_string@Status@tensorflow@@CAAEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ
    ?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z
    ?InternalSwap@ConfigProto@tensorflow@@AEAAXPEAV12@@Z
    ?CopyFrom@ConfigProto@tensorflow@@QEAAXAEBV12@@Z
    ??0ConfigProto@tensorflow@@QEAA@XZ
    ?DebugString@TensorShapeRep@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ
    ??6tensorflow@@YAAEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@AEAV12@AEBVStatus@0@@Z
    ??0Status@tensorflow@@QEAA@W4Code@error@1@Vstring_view@absl@@@Z
    ?StrCat@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@00@Z
    ??0SessionOptions@tensorflow@@QEAA@XZ
    ??1ConfigProto@tensorflow@@UEAA@XZ
    ?ReadBinaryProto@tensorflow@@YA?AVStatus@1@PEAVEnv@1@AEBV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@PEAVMessageLite@protobuf@google@@@Z
    ?Default@Env@tensorflow@@SAPEAV12@XZ
    ?CheckIsAlignedAndSingleElement@Tensor@tensorflow@@AEBAXXZ
    ?_SaverDef_default_instance_@tensorflow@@3VSaverDefDefaultTypeInternal@1@A
    ?CheckTypeAndIsAligned@Tensor@tensorflow@@AEBAXW4DataType@2@@Z
    ??1Tensor@tensorflow@@QEAA@XZ
    ??0Tensor@tensorflow@@QEAA@W4DataType@1@AEBVTensorShape@1@@Z
    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@XZ
    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z
    ?DestructorOutOfLine@TensorShapeRep@tensorflow@@AEAAXXZ
    ?TfCheckOpHelperOutOfLine@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVStatus@1@PEBD@Z
    ?SlowCopyFrom@TensorShapeRep@tensorflow@@AEAAXAEBV12@@Z
    ?dim_size@?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEBA_JH@Z
    ?CheckDimsEqual@TensorShape@tensorflow@@AEBAXH@Z
    ?CheckDimsAtLeast@TensorShape@tensorflow@@AEBAXH@Z
    ??0Tensor@tensorflow@@QEAA@XZ
    ??0GraphDef@tensorflow@@QEAA@XZ
    ??1GraphDef@tensorflow@@UEAA@XZ
    ?CheckType@Tensor@tensorflow@@AEBAXW4DataType@2@@Z
    ??1NodeDef@tensorflow@@UEAA@XZ
    ??0NodeDef@tensorflow@@QEAA@AEBV01@@Z
    ?DEVICE_CPU@tensorflow@@3QEBDEB
    ?DEVICE_GPU@tensorflow@@3QEBDEB
    ?DEVICE_SYCL@tensorflow@@3QEBDEB
    ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z
    ?kDatasetGraphKey@DatasetBase@data@tensorflow@@2QBDB
    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@V?$Span@$$CB_J@absl@@@Z
    ??0?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEAA@XZ
    ??0CheckOpMessageBuilder@internal@tensorflow@@QEAA@PEBD@Z
    ??0GraphDef@tensorflow@@QEAA@XZ
    ??0LogMessageFatal@internal@tensorflow@@QEAA@PEBDH@Z
    ??0MetaGraphDef@tensorflow@@QEAA@XZ
    ??0SessionOptions@tensorflow@@QEAA@XZ
    ??0Tensor@tensorflow@@QEAA@W4DataType@1@AEBVTensorShape@1@@Z
    ?DebugString@Tensor@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ
    ?Default@Env@tensorflow@@SAPEAV12@XZ
    ?DestructorOutOfLine@TensorShapeRep@tensorflow@@AEAAXXZ
    ?ForVar2@CheckOpMessageBuilder@internal@tensorflow@@QEAAPEAV?$basic_ostream@DU?$char_traits@D@std@@@std@@XZ
    ?GetVarint32PtrFallback@core@tensorflow@@YAPEBDPEBD0PEAI@Z
    ?SlowCopyFrom@TensorShapeRep@tensorflow@@AEAAXAEBV12@@Z
    ?TfCheckOpHelperOutOfLine@tensorflow@@YAPEAV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVStatus@1@PEBD@Z
    ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z
    ?ToString@Status@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ
    ??0Tensor@tensorflow@@QEAA@XZ
    ??1CheckOpMessageBuilder@internal@tensorflow@@QEAA@XZ
    ??1ConfigProto@tensorflow@@UEAA@XZ
    ??1LogMessageFatal@internal@tensorflow@@UEAA@XZ
    ??1NodeDef@tensorflow@@UEAA@XZ
    ??1Tensor@tensorflow@@QEAA@XZ
    ?CheckDimsAtLeast@TensorShape@tensorflow@@AEBAXH@Z
    ?CheckDimsEqual@TensorShape@tensorflow@@AEBAXH@Z
    ?CheckIsAlignedAndSingleElement@Tensor@tensorflow@@AEBAXXZ
    ?CheckTypeAndIsAligned@Tensor@tensorflow@@AEBAXW4DataType@2@@Z
    ?CopyFromInternal@Tensor@tensorflow@@AEAAXAEBV12@AEBVTensorShape@2@@Z
    ?_GraphDef_default_instance_@tensorflow@@3VGraphDefDefaultTypeInternal@1@A
    ?dim_size@?$TensorShapeBase@VTensorShape@tensorflow@@@tensorflow@@QEBA_JH@Z
    ?DebugString@Tensor@tensorflow@@QEBA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@XZ
    ?StrCat@strings@tensorflow@@YA?AV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBVAlphaNum@12@0@Z
    ?CopyFrom@GraphDef@tensorflow@@QEAAXAEBV12@@Z
    ??_7ConfigProto@tensorflow@@6B@
    ??$CreateMaybeMessage@VGPUOptions@tensorflow@@$$V@Arena@protobuf@google@@CAPEAVGPUOptions@tensorflow@@PEAV012@@Z
    ??0GraphDef@tensorflow@@QEAA@AEBV01@@Z
    ?fixed_address_empty_string@internal@protobuf@google@@3V?$ExplicitlyConstructed@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@123@A
```

As documented by
https://github.com/tensorflow/tensorflow/issues/22047#issuecomment-421452033


My software is linked against `libprotobuf.lib` from https://mirror.bazel.build/github.com/google/protobuf/archive/v3.6.0.tar.gz


built as 

```batchfile
cmake -G ""Visual Studio 14 2015 Win64""  .. -DCMAKE_INSTALL_PREFIX=""%current%\protobuf-3.6.0"" -Dprotobuf_BUILD_TESTS=OFF -Dprotobuf_BUILD_SHARED_LIBS=ON -Dprotobuf_MSVC_STATIC_RUNTIME=OFF
cmake --build . --target install --config Release -- /maxcpucount:12
```

I also tried editing `tensorflow\tf_version_script.lds` to include

```
*protobuf*
```

I also tried the `TF_EXPORT` macro from `#include ""tensorflow/core/platform/macros.h""`

in
`tensorflow/core/public/session_options.h`
and
`tensorflow/core/common_runtime/session_options.cc`

as suggested by
https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows

Do you have any suggestions about how to make sure that 

the GPU options for allocator type and visible device list do not share the same memory but we still have a monolithic DLL under windows?"
39437,ModuleNotFoundError: No module named 'tensorflow.keras.applications.efficientnet',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Cannot get EfficientNet models from tf.keras.applications

**Describe the expected behavior**
from tensorflow.keras.applications.efficientnet import *
should not produce an error. All other pretrained networks can be loaded.

**Standalone code to reproduce the issue**
%tensorflow_version 2.x
from tensorflow.keras.applications.efficientnet import *

run the above in Colab with tenso

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39436,the error in multiprocessing ,"hi,dear,
I have the problem in multiprocessing ,codes
```

from multiprocessing.pool import ThreadPool
modelV3=tf.keras.applications.InceptionV3(include_top=False, pooling='avg')

def process(inputs):
    x_pred=modelV3.predict(inputs)
    return x_pred

x=np.random.randn(1,299,299,3)
y=np.random.randn(1,299,299,3)
z=np.random.randn(1,299,299,3)

pool=ThreadPool(2)
pool.map(process,[x,y,z])
```
Error:

```
Traceback (most recent call last):
  File ""D:\python36\new\xception_load_.py"", line 26, in <module>
    pool.map(process,[x,y,z])
  File ""D:\python36\lib\multiprocessing\pool.py"", line 266, in map
    return self._map_async(func, iterable, mapstar, chunksize).get()
  File ""D:\python36\lib\multiprocessing\pool.py"", line 644, in get
    raise self._value
  File ""D:\python36\lib\multiprocessing\pool.py"", line 119, in worker
    result = (True, func(*args, **kwds))
  File ""D:\python36\lib\multiprocessing\pool.py"", line 44, in mapstar
    return list(map(*args))
  File ""D:\python36\new\xception_load_.py"", line 18, in process
    x_pred=modelV3.predict(inputs)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 908, in predict
    use_multiprocessing=use_multiprocessing)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training_arrays.py"", line 723, in predict
    callbacks=callbacks)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training_arrays.py"", line 189, in model_iteration
    f = _make_execution_function(model, mode)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training_arrays.py"", line 566, in _make_execution_function
    return model._make_execution_function(mode)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2189, in _make_execution_function
    self._make_predict_function()
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2179, in _make_predict_function
    **kwargs)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 3678, in function
    return GraphExecutionFunction(inputs, outputs, updates=updates, **kwargs)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\keras\backend.py"", line 3330, in __init__
    with ops.control_dependencies([self.outputs[0]]):
  File ""D:\python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 5254, in control_dependencies
    return get_default_graph().control_dependencies(control_inputs)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 4688, in control_dependencies
    c = self.as_graph_element(c)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3607, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""D:\python36\lib\site-packages\tensorflow_core\python\framework\ops.py"", line 3686, in _as_graph_element_locked
    raise ValueError(""Tensor %s is not an element of this graph."" % obj)
ValueError: Tensor Tensor(""global_average_pooling2d_1/Mean:0"", shape=(?, 2048), dtype=float32) is not an element of this graph.
```
System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10 64
TensorFlow installed from (source or binary):pip
TensorFlow version (use command below):1.14
Python version: 3.6.8
CUDA/cuDNN version: NO
GPU model and memory: No GPU
Could you pls help me ?
thx"
39435,Initializing the variables of a tensorflow graph over and over again gets slower and slower,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04, MacOS 10.15.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1, 1.14.0
- Python version: 3.6.10
- CUDA/cuDNN version: CPU, CuDNN 9
- GPU model and memory: GeForce GTX Titan 12GB



**Describe the current behavior**

I have a graph consiting of one matrix that I initialize over and over again in a for loop using `sess.run(tf.global_variables_initializer())`.

I noticed that it takes longer and longer to initialize the variables as the iteration progresses. I haven't noticed any memory leak.

**Describe the expected behavior**

The variables initialization speed should not decrease as the iteration progresses.

**Standalone code to reproduce the issue**

Below is a fully reproducible code tested on Linux (CPU + GPU) and MacOS with TensorFlow 1.13.1 and 1.14.0:

```
import tensorflow as tf
import matplotlib.pyplot as plt
import time
from tqdm import tqdm

w = tf.get_variable('w', shape=(100, 1000))

sess = tf.Session()
times = []

for i in tqdm(range(2000)):
    s = time.time()
    sess.run(tf.global_variables_initializer())
    times.append(time.time() - s)

plt.plot(times)
plt.xlabel('iteration')
plt.ylabel('time (s)')
plt.title('Evolution of time per step')
```

![image](https://user-images.githubusercontent.com/16196950/81624193-4497d980-93fe-11ea-9934-7704d34eb44d.png)
"
39434,Upgrading from TF 2.1 to 2.2 gives 12% slowdown and 23% memory increase,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow version (use command below): 2.1.0, 2.2.0, tf-nightly
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1
- GPU model and memory: 8xTesla V100, 32GB each

**Describe the current behavior**

I'm running language modeling experiments with ALBERT, and GPU memory is at a premium due to the large batch sizes necessary. Upgrading from TF 2.1.0 to 2.2.0, I experienced OOM errors, so I ran a few benchmarks:

| | TF nightly (May 8) | TF 2.2 | TF 2.1 |
| --- | --- | --- | --- |
| Iterations/Sec | 1.63 | 1.64 | 1.86 |
| GPU Memory | 26 GB | 25 GB | 21 GB |

That's a combination of 12% speed slowdown, and 23% memory increase. I cannot upgrade until performance is matched. Are there any new experimental options, or changes I should be aware of that caused this massive performance hit? I'm using tf.function, XLA, and AMP.

It seems that fewer and fewer ops are converted to mixed-precision as we progress from TF 2.1->2.2->nightly. Is that related, and how can I restore the original behavior?

Poring over the release notes, the only thing that sticks out is:
`tf.constant` always creates CPU tensors irrespective of the current device context.


"
39432,tensorflow.keras behaves wrongly in autoencoder setup compared to keras,"Attempt to recreate the simplistic autoencoder works properly if keras (2.3.1) is used, but fails to converge if tensorflow.keras (2.3.0-tf).

The basic realisation can be found in https://gist.github.com/fedxa/45eb1a412964ddf19820fff347c5b2de

```
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
# from keras.layers import Input, Dense
# from keras.models import Model

input_img = Input(shape=(784,))
encoded = Dense(32, activation='relu')(input_img)
decoded = Dense(784, activation='sigmoid')(encoded)

autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')

from keras.datasets import mnist
import numpy as np
(x_train, _), (x_test, _) = mnist.load_data()
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))

autoencoder.fit(x_train, x_train,
                epochs=20,
                batch_size=256,
                shuffle=True,
                validation_data=(x_test, x_test))
```

If the separate keras 2.3.1 is used -- the example converges fast, if the tensorflow.keras 2.3.0-tf is used no convergence is observed (loss funciton is ~0.6 all the time and autoencoder encodes noise only).

The problem is present in google collab, on linux and MacOS with tensorflow versions 2.0, 2.1, 2.2
"
39431,TensorFlow Keras Model Hangs when .predict is Called Inside a Thread,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 19.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.7
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: RTX 2070

This is a copy of a SO question I asked, but I think it deserves bug status and hasn't yet been answered.

When trying to run a TensorFlow Keras model in a thread, it hangs when the `.predict` function is called.  

```python
import multiprocessing as mp
import tensorflow as tf
import numpy as np

def predict(data):
    a = tf.keras.Sequential([tf.keras.layers.Dense(4, input_shape=(16,))])

    return a.predict(data)

fake_data = np.zeros((100, 16))

# Works
for i in range(4):
    print(predict(fake_data).shape)

# Never finishes running
processes = []
for i in range(4):
    p = mp.Process(target=predict, args=(fake_data,))
    p.start()
    processes.append(p)

for p in processes:
    p.join()
```

I'm not sure why this happens, and I've tried wrapping the inside of `predict` with `with tf.device('/cpu:0')`, moving the TensorFlow import to inside the predict function, and calling `_make_predict_function()` with no luck.

I am aware of Keras' built-in multiprocessing with the predict function, however, this code instantiates a new Model for each prediction which is fundamentally different.

This also happens when `model.fit` is called."
39430,How to Find the Average of the Input Vectors,"In the paper ""Neural Models for Sequence Chunking"", there's a formula 2. It follows:

Chj = Average (h[i], h[i+1], ... h[i+l-1])

I first thought that it was Average Layer, but it said, ""Average(·) computes the average of the input vectors.""

So it's actually tf.math.reduce_mean, not keras.layers.Average? Or is it really Average Layer?

I tried to use reduce_mean, but it went error.

```
# Input
inputs = Input(shape=(max_length,), name=""Input"")

# Embedding
embed = Embedding(input_dim=n_words+1,
                  output_dim=embedding_size,
                  input_length=max_length,
                  name=""Embedding"")(inputs)

# Bi-LSTM
encoder = Bidirectional(LSTM(units=hidden_state_encoder_size,
                             return_sequences=True,
                             dropout=dropout_rate,
                             name=""LSTM""),
                        name=""Bi-LSTM"")

hidden_states = encoder(embed)

# Average
average = tf.math.reduce_mean(hidden_states)

# Outputs
outputs = Dense(n_tags,
                activation=""softmax"",
                name=""Output"")(average)
```

Error:

`ValueError: Input 0 is incompatible with layer Output: expected min_ndim=2, found ndim=0`

But, if I use Average layer

```
# Input
inputs = Input(shape=(max_length,), name=""Input"")

# Embedding
embed = Embedding(input_dim=n_words+1,
                  output_dim=embedding_size,
                  input_length=max_length,
                  name=""Embedding"")(inputs)

# Bi-LSTM
encoder = Bidirectional(LSTM(units=hidden_state_encoder_size,
                             return_sequences=True,
                             dropout=dropout_rate,
                             name=""LSTM""),
                        name=""Bi-LSTM"")

hidden_states = encoder(embed)

# Average
average = Average()(hidden_states)

# Outputs
outputs = Dense(n_tags,
                activation=""softmax"",
                name=""Output"")(average)
```

Error:

`ValueError: A merge layer should be called on a list of inputs.`"
39428,Error when trying to use tensorflow. Installation issue.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Home; Version: 1909; OS Build 18363.815
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not using on mobile device. Only on desktop.
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.2.0
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA. I dont have GPU and hence not explicitly installed this.
- GPU model and memory: NA. Dont have GPU.



**Describe the problem**
When loading the MNIST dataset, I get an error message saying failed to load DLL. I have just installed tensorflow and hence think this is related to how it was installed.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

The below command was used in the .py file

import tensorflow as tf

# Use MNIST handwriting dataset
mnist = tf.keras.datasets.mnist

# Prepare data for training
(x_train, y_train), (x_test, y_test) = mnist.load_data()

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/DataInOnePlace/Puneet/eDX/Intro to AI/5. Neural Networks/Lectures/src5/digits/test.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\jaisw\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39423,DLL load failed: A dynamic link library (DLL) initialization routine failed.,"Traceback (most recent call last):
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<pyshell#1>"", line 1, in <module>
    import tensorflow
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mukesh\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39421,Problem with tf.keras.metrics.Mean,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 19.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
From source
- TensorFlow version (use command below):
v2.1.0-0-ge5bf8de410 2.1.0
- Python version:
3.7
- Bazel version (if compiling from source):
? (built a while ago)
- GCC/Compiler version (if compiling from source):
? 
- CUDA/cuDNN version:
10.2
- GPU model and memory:
GeForce GTX 1080 8G

**Describe the current behavior**

The following code:

    tf.keras.metrics.Mean(name='train_loss')

results in the error:

    tensorflow.python.framework.errors_impl.InvalidArgumentError: assertion failed: [0] [Op:Assert] name: EagerVariableNameReuse

This same behaviour has been observed many times, for example:

https://stackoverflow.com/questions/61321380/gpu-out-of-memory-error-just-by-declaring-tf-keras-metrics

There is also the (same?) issue which should not have been closed:

https://github.com/tensorflow/tensorflow/issues/38518

In my case, the code was working and then began giving this error even though no code changes were made. Moreover, this persisted even when I killed all python processes first.

"
39419,Tensorflow Build from Source Raspberry Pi,"**System information**
- Windows 10 Pro Version 1909 
- Python 3.8.2
- pip 20.1
- virtualenv 20.0.20


Hello,
I am trying to follow these installation instructions https://www.tensorflow.org/install/source_rpi.

When I try to ""cross-compile the TensorFlow source code to build a Python pip package"", I get the error message:
`The command ""CI_DOCKER_EXTRA_PARAMS"" is either misspelled or could not be found`

This is the command:
`CI_DOCKER_EXTRA_PARAMS=""-e CI_BUILD_PYTHON=python3 -e CROSSTOOL_PYTHON_INCLUDE_PATH=/usr/include/python3.4"" \`

The test command for Docker works fine:
`docker run --rm hello-world`
`""Hello from Docker!
This message shows that your installation appears to be working correctly.""`

Does anyone have an idea or a tip? I suspect something is not installed properly. Is ""CI_DOCKER_EXTRA_PARAMS"" a Docker-command? What is the requirement for the OS to know or recognize this command?

"
39418,ModuleNotFoundError: No module named 'tensorflow_core.core',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution : **Debian GNU/Linux 10.x (buster) 64Bit**
- TensorFlow installed from (source or binary): **source**
- TensorFlow version (use command below): **Command not working**
- Python version: **3.7.3**
- CUDA/cuDNN version: **10.1**
- GPU model and memory: **nVidia Titan X 12GB**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Even basic commands like import tensorflow are giving me the error specified in the title, which shows to be in the __init__.py file in the traceback call.

**Describe the expected behavior**
Tensorflow should run normally

**Standalone code to reproduce the issue**
`import tensorflow as tf;`

**Other info / logs** 
```
 File ""<string>"", line 1, in <module>
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow/__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow_core/__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/home/bilank/py_envs/dev/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow_core/python/__init__.py"", line 64, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow/__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""/home/bilank/py_envs/dev/lib/python3.7/site-packages/tensorflow/__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""/home/bilank/py_envs/dev/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named 'tensorflow_core.core'

```"
39417,experimental_run_v2 throws AttributeError with MultiWorkerMirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1 and 2.2 affected

**Describe the current behavior**

Using `strategy.experimental_run_v2` (or `strategy.run` for TF 2.2) with `MultiWorkerMirroredStrategy` throws `AttributeError: 'CollectiveAllReduceExtended' object has no attribute '_cfer_fn_cache'` when passing it a tf.function

This is caused by the access at https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/mirrored_strategy.py#L743 due to `CollectiveAllReduceExtended` not calling the `super().__init__` function which creates that dictionary at https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/distribute/mirrored_strategy.py#L472

I noted that the relevant code was removed in current master by https://github.com/tensorflow/tensorflow/commit/b16d24a342c5de1384dcb9ee408a74f206d332b2 but wanted to make sure it is included in the next release or in a patch release. Also `MultiWorkerMirroredStrategy` is not mentioned in the commit, so it might be a good idea to include something like this as a test case to avoid regressions. Looking at the commit I guess this is fixed too.

**Standalone code to reproduce the issue**

```
import tensorflow as tf

strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

with strategy.scope():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10),
    ])

    model.compile(
        optimizer=tf.keras.optimizers.SGD(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])


@tf.function
def train_step(model, data, target):
    with tf.GradientTape() as tape:
        predictions = model(data, training=True)
        loss = model.loss(target, predictions)

    gradients = tape.gradient(loss, model.trainable_variables)
    model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return loss


def distributed_train_step(strategy, model, x, y):
    strategy.experimental_run_v2(train_step, args=(model, x, y))


for x, y in train_dataset:
    distributed_train_step(strategy, model, x, y)

```"
39415,TensorFlow Lite Cant convert my model to .lite version,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

2020-05-11 18:02:37.694825: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-05-11 18:02:38.353968: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-05-11 18:02:38.360550: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-05-11 18:02:38.697609: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-05-11 18:02:38.703347: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1226 nodes (-186), 1603 edges (-240), time = 90.665ms.
2020-05-11 18:02:38.708737: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 1226 nodes (0), 1603 edges (0), time = 
30.288ms.
Traceback (most recent call last):
  File ""convert_modell2.0.py"", line 8, in <module>
    tflite_model = converter.convert()
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\tensorflow_core\lite\python\lite.py"", line 983, in convert
    **converter_kwargs)
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 449, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\tensorflow_core\lite\python\convert.py"", line 200, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-05-11 18:02:44.625539: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.625766: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.625953: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.626134: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.626300: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.626537: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.626774: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.626941: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.627118: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.627320: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.627490: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.627695: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.627900: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.628117: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.628312: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.628509: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.628710: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.628936: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.629169: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.629399: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.629623: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.629857: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.630063: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.630292: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.630495: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.630701: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.630910: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.631113: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.631310: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.631531: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.631744: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.631921: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.632031: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.632229: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.632460: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.632612: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.632847: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.633076: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.633320: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.633544: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.633754: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.633972: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.634220: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.634440: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.634679: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.634906: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.635104: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.635309: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.635534: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.635733: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.635961: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.636169: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.636392: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.636618: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.636829: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.637009: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.637225: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.637423: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.637651: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.637857: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.638068: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.638294: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.638522: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.638721: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.638947: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.639155: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.639360: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.639593: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.639814: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.640019: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.640218: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.640431: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.640633: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.640855: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.641070: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.641293: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.641532: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.641705: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.641904: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.642092: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.642274: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.642461: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.642645: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.642859: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.643055: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.643261: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.643464: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.643637: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.643834: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.644026: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.644202: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.644365: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.644555: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.644745: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.644935: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.645117: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.645314: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.645508: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.645696: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.645874: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.646088: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.646297: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.646512: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.646720: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.649215: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.649471: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.649683: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.649908: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.650117: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.650336: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.650561: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.650766: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.650966: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.651152: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.651364: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.651570: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.651758: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.651947: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.652133: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.652340: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.652528: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.652718: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.652942: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.653143: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.653345: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.653547: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.653773: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.653992: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.654187: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.654383: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.654587: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.654793: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.655016: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.655223: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.655420: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.655629: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.655835: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.656076: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.656311: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.656531: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.656779: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.656994: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.657200: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.657398: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.657598: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.657815: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.658020: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.658213: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.658412: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.658654: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.658847: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.659052: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.659280: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.659442: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.659653: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.659847: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.660026: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.660235: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.660445: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.660639: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.660851: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.661026: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.661207: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.661399: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.661597: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.661791: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.661959: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.662147: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.662336: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.662534: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.662713: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.662979: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-05-11 18:02:44.663235: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.663456: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.663705: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-05-11 18:02:44.663953: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.664221: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.664490: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-05-11 18:02:44.664769: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.665027: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-05-11 18:02:44.665306: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-05-11 18:02:44.665600: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-05-11 18:02:44.665896: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-05-11 18:02:44.666157: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-05-11 18:02:44.666622: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.666853: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.667088: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.667341: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.667582: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.667814: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.668075: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.668321: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.668515: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.668740: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.668963: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.669179: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.669429: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.669642: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.669874: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.670137: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.670369: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.670607: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.670836: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.671043: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.671269: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.671491: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.671694: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.671931: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.672174: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.672403: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.672587: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.672815: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.673028: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.673258: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.673476: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.673721: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.673985: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.674198: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.674404: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.674630: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.674847: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.675055: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.675350: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-05-11 18:02:44.675615: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.675859: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.676154: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArraySizeV3
2020-05-11 18:02:44.676425: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-05-11 18:02:44.676710: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayGatherV3
2020-05-11 18:02:44.677044: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.677278: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.677514: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.677762: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.677995: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.678224: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.678494: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.678720: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.678952: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.679166: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.679407: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.679659: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.679872: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.680076: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.680268: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.680461: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.680664: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.680908: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.681132: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayV3
2020-05-11 18:02:44.681364: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.681602: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.681821: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.682066: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.682298: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.682512: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.682726: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.682942: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.683183: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.683412: I tensorflow/lite/toco/import_tensorflow.cc:193] Unsupported data type in placeholder op: 20
2020-05-11 18:02:44.683644: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayScatterV3
2020-05-11 18:02:44.683845: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.684089: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.684292: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: LoopCond
2020-05-11 18:02:44.684492: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Enter
2020-05-11 18:02:44.684691: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.684875: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.685067: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.685319: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.685500: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: Exit
2020-05-11 18:02:44.685707: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.685929: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.686191: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayReadV3
2020-05-11 18:02:44.686538: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.686776: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: TensorArrayWriteV3
2020-05-11 18:02:44.700426: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 792 operators, 1274 arrays (0 quantized)
2020-05-11 18:02:44.712374: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 720 operators, 1154 arrays (0 
quantized)
2020-05-11 18:02:44.726002: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 720 operators, 1154 arrays (0 quantized)
2020-05-11 18:02:44.750457: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 646 operators, 1085 
arrays (0 quantized)
2020-05-11 18:02:44.768568: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 646 operators, 1085 arrays (0 quantized)
2020-05-11 18:02:44.780627: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 646 operators, 1085 arrays (0 quantized)
2020-05-11 18:02:44.797189: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 37952 bytes, theoretical optimal value: 11008 bytes.
2020-05-11 18:02:44.800826: I tensorflow/lite/toco/toco_tooling.cc:454] Number of parameters: 666633
2020-05-11 18:02:44.802516: E tensorflow/lite/toco/toco_tooling.cc:481] We are continually in the process of adding support to TensorFlow Lite for more ops. It would 
be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, EXPAND_DIMS, FILL, FULLY_CONNECTED, LESS, LOGISTIC, MAXIMUM, MINIMUM, MUL, RANGE, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you 
will need custom implementations: LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.       
Traceback (most recent call last):
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\Scripts\toco_from_protos-script.py"", line 10, in <module>
    sys.exit(main())
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""E:\Users\Santiados\anaconda3\envs\tflitecon\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

TensorFlow Lite currently doesn't support control flow ops: Enter, Exit, Merge, Switch. We are working on supporting control flow ops, please see github issue at https://github.com/tensorflow/tensorflow/issues/28485. Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, EXPAND_DIMS, FILL, FULLY_CONNECTED, LESS, LOGISTIC, MAXIMUM, MINIMUM, MUL, RANGE, RESHAPE, REVERSE_V2, SHAPE, STRIDED_SLICE, SUB, SUM, TANH, TILE, TRANSPOSE, ZEROS_LIKE. Here is a list of operators for which you 
will need custom implementations: LoopCond, TensorArrayGatherV3, TensorArrayReadV3, TensorArrayScatterV3, TensorArraySizeV3, TensorArrayV3, TensorArrayWriteV3.

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
39414,Why tf data window not using in tutorials/structured_data/time_series,"Why tf data window not using in [tf tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)
I am looking for window in timeseries. 
tf data only useful if it does not need to generate window data first. But the tutorial is just using batch, shuffle and repeart which does not help on reduce data generate for training."
39413,Model.fit() process grabs generators that were not passed to it. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS, Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2.0-dev20200508
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: release 10.0, V10.0.130
- GPU model and memory: Titan RTX 24 GB

**Describe the current behavior**
Model.fit(generator_train) grabs generator_test information that was NEVER passed to it. 

**Describe the expected behavior**
Model.fit(generator_train) only works on generator_train information 

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

class GeneratorFile(tf.keras.utils.Sequence):
    def __init__(self, file_list):
        self.file_list = file_list
        self.desired_file = self.file_list[0]
        print('This should be file 1:', self.desired_file)
    def __len__(self):
        return 2
    def on_epoch_end(self):
        self.desired_file = self.file_list[1]
        print('This should be file 2:', self.desired_file)
    def __getitem__(self, item):
        return np.zeros((16, 1)), np.zeros((16,))

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(1, input_dim=1, activation=""softmax""))
model.compile(
    optimizer='Adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model_file_train = ['file1', 'file2']
model_file_test = ['file3']
generator_train = GeneratorFile(model_file_train)
generator_test = GeneratorFile(model_file_test) #THIS IS NEVER PASSED IN
model.fit(generator_train, epochs=2, initial_epoch = 0)

```

The output of the above code snippet: 

```
This should be file 1: file1
This should be file 1: file3
Epoch 1/2
2/2 [==============================] - 0s 1ms/step - loss: 15.2492 - accuracy: 0.0000e+00
This should be file 2: file2
Epoch 2/2
2/2 [==============================] - 0s 1ms/step - loss: 15.2492 - accuracy: 0.0000e+00
This should be file 2: file2
```

`file3` was NEVER passed to model.fit(), so how would the generator even be aware of that file? Granted, I'm not even sure if this is a Tensorflow issue, could just be I'm misunderstanding exactly how Model.fit() is supposed to work, but this is extremely unintuitive behavior. 

As a side note, if I comment out the line that defines `generator_test`, file3 never appears. "
39411,Categorical CrossEntropy returning wrong values,"tf.losses.CategoricalCrossentropy is returning values different from those of numpy

```
import numpy as np
import tensorflow as tf

y_pred = np.array([7.3216137e-07, 3.3240074e-11, 4.4985552e-12,
                   3.9974657e-05, 7.3216137e-07, 4.4985552e-12, 
                   4.4985552e-12, 2.9537498e-04, 8.8050038e-01,
                   1.1916277e-01])
y_true = np.zeros(10)
y_true[5] = 1
cce = tf.losses.CategoricalCrossentropy()


print(f""NUMPY {-1*np.sum(y_true*np.log(y_pred))}"")
print(f""TF {cce(y_true, y_pred)}"")
```

**System information**
- Have I written custom code: NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow installed from (source or binary): Default of Colab
- TensorFlow version (use command below): 2.2.0-rc4
- Python version: 3.6.9

**Describe the current behavior**

```
NUMPY 26.12726483737188
TF 16.11809539794922
```

**Describe the expected behavior**
```
NUMPY 26.12726483737188
TF 26.12726483737188
```

**Standalone code to reproduce the issue**
Colab Link: https://colab.research.google.com/drive/1gKb0eiKfIw4GKZo_lzDTaswWFz7gsAU0?usp=sharing
"
39410,Training a model for my own dataset using tensorflow speech commands ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Google Colab 
- TensorFlow version: Tensorflow-gpu 1.14.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Updated Google colab's CUDA and cuDNN
- GPU model and memory: Inbuilt Colab-pro GPU 



**Describe the problem**

Dear Authors, 

I am trying to train the speech recognition model for small vocabulary from the following link : https://github.com/tensorflow/tensorflow/tree/r1.9/tensorflow/examples/speech_commands

I am trying to run the following command:

!python tensorflow/examples/speech_commands/train.py --data_url= --clip_duration_ms=5000 --train_dir= /content/drive/My Drive/wuw-checkpoint --data_dir= /content/drive/My Drive/wake-up-word/ --wanted_words = up, down

I cloned into the default master file to run the same. I am getting the following error, can you please help me out in giving the right tf, CUDA and cuDNN versions that I could use to run the same?  Here's the following error I got

Traceback (most recent call last):
  File ""tensorflow/examples/speech_commands/train.py"", line 452, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""tensorflow/examples/speech_commands/train.py"", line 201, in main
    f.write('\n'.join(audio_processor.words_list))
AttributeError: 'AudioProcessor' object has no attribute 'words_list'
Thank you, I look forward to hearing from you 
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39409,"upgraded from 2.1 to 2.2 raised 'Unexpectedly high number of iterations in HLO passes, exiting fixed point loop'","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Gentoo
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0-rc2
- Python version: 3.7
- Bazel version (if compiling from source): 2.0
- GCC/Compiler version (if compiling from source): 8.4
- CUDA/cuDNN version: 10.2/7.6
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I'm using Gentoo linux and tf is installed by Gentoo's package management system(portage/emerge). After upgrading tf from 2.1.0 to 2.2.0-rc2, a new warning raised training on a same model:
`W ./tensorflow/compiler/xla/service/hlo_pass_fix.h:49] Unexpectedly high number of iterations in HLO passes, exiting fixed point loop.`
This warning might not affect training speed, but takes more GPU memory. The model with same setting works well in tf 2.1 but throws OOM in tf 2.2. It's unclear to me what this warning means and not helpful to find the problem. 

**Describe the expected behavior**
should not occupy more memory

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39408,About loading of saved_model format in tf2,"There are too many versions of tf. Before, I used a function to load the saved_model format model.

`new_model = tf.contrib.saved_model.load_keras_model(saved_model_path) new_model.summary()`

But if folder “assets” does not exist, an error will occur,and this file is not available for every model.

So which function should I use to load？

Thank you !!"
39407,tensorflow.lib missing some symbols on linking in a application,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (Woindows 10):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (r2.1):
- TensorFlow version:
- Python version:3.7
- Installed using virtualenv? pip? conda?:venv
- Bazel version (if compiling from source):0.29.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1, 7.6
- GPU model and memory: GTX1080


**Describe the problem**

I want use to tensotflow in windows 10. I have built r2.1 from source finally.
Another problem stop me again, there are some missing symbols on linking tensorflow.lib in my application.

Severity	Code	Description	Project	File	Line	Suppression State
Error	LNK2019	unresolved external symbol ""class tensorflow::Session * __cdecl tensorflow::NewSession(struct tensorflow::SessionOptions const &)"" (?NewSession@tensorflow@@YAPEAVSession@1@AEBUSessionOptions@1@@Z) 

It works in Linux. The same application does not work in windows.

Please help.
Thanks.


"
39406,Can't run tensorflow in PyCharm/ Jupyter,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip and interpreter settings
- TensorFlow version: 2.2.0
- Python version: 3.7.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: -



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Installed tensorflow via ""Interpreter Settings"" of PyCharm. Now I tried to import ""import tensorflow"" but will not works.


**Any other info / logs**
Pip freeze:
```
tensorboard==2.2.1
tensorboard-plugin-wit==1.6.0.post3
tensorflow==2.2.0
tensorflow-cpu==2.2.0
tensorflow-estimator==2.2.0
tensorflow-gpu-estimator==2.2.0
```

Error_log:
```
---------------------------------------------------------------------------

ImportError                               Traceback (most recent call last)

d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\AppData\Local\Programs\Python\Python37\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

~\AppData\Local\Programs\Python\Python37\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.


During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)

<ipython-input-4-f9575c251aea> in <module>
----> 1 import tensorflow
      2 
      3 

d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""d:\tuc\gitlab\dmc_bh8\louis_jupyter\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\...\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```
"
39405,"Unsigned int (tf.uint{32,64}) support for tf.{tile,repeat}","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `macOS Catalina 10.15.2 (19C57)`
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `2.2.0`
- Python version: `3.7.5`

**Describe the feature and the current behavior/state.**
Feature: `tf.tile` and `tf.repeat` do not fail when input is of dtype `tf.uint{32,64}`.
Current behavior: `tf.tile` and `tf.repeat` fail when input is of dtype `tf.uint{32,64}`.

**Will this change the current api? How?**
Would add new supported input type to `tf.tile` and `tf.repeat`.

**Who will benefit with this feature?**
Users that need to repeat or tile their data of type `tf.uint{32,64}`

**Sketch of a test case**
```python
import tensorflow as tf


tf.tile(tf.constant([1,2,3], dtype=tf.uint64), (3, ))
tf.tile(tf.constant([1,2,3], dtype=tf.uint32), (3, ))
tf.repeat(tf.constant([1,2,3], dtype=tf.uint64), 3, axis=0)
tf.repeat(tf.constant([1,2,3], dtype=tf.uint32), 3, axis=0)
```

**Logs for current behavior**
```
ipdb> tf.tile(tf.constant([1,2,3], dtype=tf.uint64), (3, ))
*** tensorflow.python.framework.errors_impl.UnimplementedError: TileOp : The input data type is not supported, DataType : uint64, Dimension : 1 [Op:Tile]
ipdb> tf.tile(tf.constant([1,2,3], dtype=tf.uint32), (3, ))
*** tensorflow.python.framework.errors_impl.UnimplementedError: TileOp : The input data type is not supported, DataType : uint32, Dimension : 1 [Op:Tile]
ipdb> tf.repeat(tf.constant([1,2,3], dtype=tf.uint64), 3, axis=0)
*** tensorflow.python.framework.errors_impl.UnimplementedError: TileOp : The input data type is not supported, DataType : uint64, Dimension : 2 [Op:Tile]
ipdb> tf.repeat(tf.constant([1,2,3], dtype=tf.uint32), 3, axis=0)
*** tensorflow.python.framework.errors_impl.UnimplementedError: TileOp : The input data type is not supported, DataType : uint32, Dimension : 2 [Op:Tile]
```
"
39403,tf.data.Dataset doesn't handle namedtuples properly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): true
- TensorFlow installed from (source or binary): collab notebook
- TensorFlow version (use command below): v2.2.0-rc4-0-g70087ab4f4
- Python version: 3.6.9

**Describe the current behavior**
Handling of namedtuples by datasets is not coherent: elementspec does not reflect the input type

**Describe the expected behavior**
The elementspec should reflect the structured input

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1w3vHiI2mItjL1uv8c8RzMEqATPI9v7u3?usp=sharing

"
39402,Add support for more dimensions -  ImageDataGenerator's .flow_from_directory(),"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
When using ImageDataGenerator's .flow_from_directory(), one has to specify color_mode ∈ {""grayscale"", ""rgb"", ""rgba""} (1, 3 or 4 dim).

I propose to change the color mode to an input_shape() to be able to handle more dimensions.

**Will this change the current api? How?**
Either remove the color_mode completely and handle it dynamically or let the user input the input_shape of the data.

**Who will benefit with this feature?**
When working with multiclass image segmentations and an one-hot encoded representations of the segmentation masks, it is very convenient to use the ImageDataGenerator's .flow_from_directory() for both the image and the mask, then zip them when inputted to model.fit(). 

Today, this does only work if the mask is of dim ∈ {1, 3, 4}.

One way to circumvent this is to write your own generator by inheriting from keras.utils.Sequence class, but it would be nice to have support for it in ImageDataGenerator as one then can use the built-in augmentations.

**Any Other info.**
Related question on Stackoverflow: https://stackoverflow.com/questions/60551136/how-to-use-imagedatagenerator-with-multi-label-masks-for-multi-class-image-segme"
39401,Failed to load the native TensorFlow runtime.,"(base) PS C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master> python demo_cli.py
Traceback (most recent call last):
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Произошел сбой в программе инициализации библиотеки динамической компоновки (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Произошел сбой в программе инициализации библиотеки динамической компоновки (DLL).


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39400,Failed to load the native TensorFlow runtime.,"(base) PS C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master> python demo_cli.py
Traceback (most recent call last):
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Произошел сбой в программе инициализации библиотеки динамической компоновки (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo_cli.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Users\Asteroids\Downloads\Real-Time-Voice-Cloning-master\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Asteroids\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Asteroids\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Произошел сбой в программе инициализации библиотеки динамической компоновки (DLL).


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39398,TensorFlowOpLayer messes up the TensorBoard graphs / grouping with TensorBoard,"I am not sure if this is a bug, feature request or something I do wrong. I apologize in advance. I do believe this is a non-intended behaviour, or at the very least, a missing documentation. 

As you can see in https://stackoverflow.com/questions/61594352/tensorflowoplayer-messes-up-the-tensorboard-graphs, I have asked this question and get no response. 

I copy the question from Stackoverflow to here:

This question is about TensorFlow (and TensorBoard) version 2.2 (running in tensorflow/tensorflow:2.2.0-gpu-jupyter on Ubuntu 18.04.3 LTS), but I have experienced the same issue with 2.2rc3 and 2.1. It is a continuation of the question '[Messed up TensorBoard graphs due to Python operations](https://stackoverflow.com/questions/61581114/messed-up-tensorboard-graphs-due-to-python-operations)'.

Consider the following code:

```python3
from datetime import datetime
import tensorflow as tf
from tensorflow import keras

inputs = keras.layers.Input(shape=(784, ))    

outputs = tf.zeros([32, 10], tf.float32)

for i in range(0, 3):
    x = keras.layers.Dense(32, activation='relu', name='Model/Block' + str(i) + '/relu')(inputs) 
    x = keras.layers.Dropout(0.2, name='Model/Block' + str(i) + '/dropout')(x)
    x = keras.layers.Dense(10, activation='softmax', name='Model/Block' + str(i) + '/softmax')(x)
    outputs = keras.layers.Lambda(lambda x: x[0] + x[1], name='Model/add/add' + str(i))([outputs, x])

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.summary(line_length=100, positions=[.45, .58, .67, 1.])

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255

model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=keras.optimizers.RMSprop(),
              metrics=['accuracy'])

logdir = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)
model.fit(x_train, y_train,
          batch_size=32,
          epochs=5,
          validation_split=0.2,
          callbacks=[tensorboard_callback])
```

When run, it prints the following summary:
``` 
____________________________________________________________________________________
Layer (type)                          Output Shap Param  Connected to               
====================================================================================
input_1 (InputLayer)                  [(None, 784 0                                 
____________________________________________________________________________________
Model/Block0/relu (Dense)             (None, 32)  25120  input_1[0][0]              
____________________________________________________________________________________
Model/Block0/dropout (Dropout)        (None, 32)  0      Model/Block0/relu[0][0]    
____________________________________________________________________________________
Model/Block1/relu (Dense)             (None, 32)  25120  input_1[0][0]              
____________________________________________________________________________________
Model/Block0/softmax (Dense)          (None, 10)  330    Model/Block0/dropout[0][0] 
____________________________________________________________________________________
Model/Block1/dropout (Dropout)        (None, 32)  0      Model/Block1/relu[0][0]    
____________________________________________________________________________________
Model/Block2/relu (Dense)             (None, 32)  25120  input_1[0][0]              
____________________________________________________________________________________
tf_op_layer_AddV2 (TensorFlowOpLayer) [(32, 10)]  0      Model/Block0/softmax[0][0] 
____________________________________________________________________________________
Model/Block1/softmax (Dense)          (None, 10)  330    Model/Block1/dropout[0][0] 
____________________________________________________________________________________
Model/Block2/dropout (Dropout)        (None, 32)  0      Model/Block2/relu[0][0]    
____________________________________________________________________________________
Model/add/add1 (Lambda)               (32, 10)    0      tf_op_layer_AddV2[0][0]    
                                                         Model/Block1/softmax[0][0] 
____________________________________________________________________________________
Model/Block2/softmax (Dense)          (None, 10)  330    Model/Block2/dropout[0][0] 
____________________________________________________________________________________
Model/add/add2 (Lambda)               (32, 10)    0      Model/add/add1[0][0]       
                                                         Model/Block2/softmax[0][0] 
====================================================================================
```

Note the strange entry `tf_op_layer_AddV2 (TensorFlowOpLayer)`. This kind of entry makes the TensorBoard graphs very messy. It turns out that when avoiding the use of the `tf.zeros()`, this strange `tf_op_layer_AddV2` element is not added. So the following code will not generate any `tf_op_layer` element: 

```python3
from datetime import datetime
import tensorflow as tf
from tensorflow import keras

inputs = keras.layers.Input(shape=(784, ))

x = keras.layers.Dense(32, activation='relu', name='Model/Block0/relu')(inputs) 
x = keras.layers.Dropout(0.2, name='Model/Block0/dropout')(x)
outputs = keras.layers.Dense(10, activation='softmax', name='Model/Block0/softmax')(x)

for i in range(1, 3):
    x = keras.layers.Dense(32, activation='relu', name='Model/Block' + str(i) + '/relu')(inputs) 
    x = keras.layers.Dropout(0.2, name='Model/Block' + str(i) + '/dropout')(x)
    x = keras.layers.Dense(10, activation='softmax', name='Model/Block' + str(i) + '/softmax')(x)
    outputs = keras.layers.Lambda(lambda x: x[0] + x[1], name='Model/add/add' + str(i))([outputs, x])

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.summary(line_length=84, positions=[.46, .60, .69, 1.])

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train.reshape(60000, 784).astype('float32') / 255
x_test = x_test.reshape(10000, 784).astype('float32') / 255

model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              optimizer=keras.optimizers.RMSprop(),
              metrics=['accuracy'])

logdir = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)
model.fit(x_train, y_train,
          batch_size=32,
          epochs=5,
          validation_split=0.2,
          callbacks=[tensorboard_callback])

```
There are many more complex examples where the `tf_op_layer_` elements are created. It will be appreciated if it is exaplained why they are created and how to avoid them."
39397,"Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] ","Training was going fine till 47K steps and then got Resource exhausted error. 
```
File ""/home/saini/.virtualenvs/cv/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1370, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.ResourceExhaustedError: 2 root error(s) found.
  (0) Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradients/AddN_162-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

	 [[Loss/Cast_232/_16919]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

  (1) Resource exhausted: OOM when allocating tensor with shape[32,960,10,10] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradients/AddN_162-1-TransposeNHWCToNCHW-LayoutOptimizer}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.

0 successful operations.
0 derived errors ignored.
```
OS: Ubuntu 18.04
Tensorflow: 1.14.0 GPU
CUDA: 10.0
CUDNN:: 7.6
Batch Size: 32

Follow changes i have made in default TF object detection API: 

```
model_lib.py
tf.estimator.EvalSpec(
            name=eval_spec_name,
            input_fn=eval_input_fn,
            steps=None,
            throttle_secs = 172800,
            exporters=exporter))
eval.proto
optional uint32 eval_interval_secs = 3 [default = 172800]; # default = 600
model_main.py
config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir, save_checkpoints_steps=5000)
```

"
39396,TFlite model performance issues on Snapdragon 855+ - using benchmark_model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
   
   using benchmark_model built from source (tf 2.2.0) using bazel. have a custom network model
   involving tf.matmul  (variable inputs) as target network graph for evaluation.
 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):

  Ubuntu 18.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

  OnePlus 7t (Snapdragon 855+)
  Android 10

- TensorFlow installed from (source or binary):
  source 2.2.0

- TensorFlow version (use command below):
  2.2.0

- Python version:

  3.5+

- Bazel version (if compiling from source):

  3.1.0

- GCC/Compiler version (if compiling from source):

 clang via android ndk toolchain

- CUDA/cuDNN version:

 N/A, GPU does not support the model and some ops

- CPU model and memory:

  Custom model involving series of Convolutions, BN, Matmuls.
  Model Size ~23 MB
  Max Memory usage ~65 MB

**Describe the current behavior**

Model has to be run on CPU since tf.matmul (variable inputs) operation is not supported on both GPU and Hexagon (or even via SNPE).

Average inference delay on cold start (of the phone) is low with low standard deviation (e.g., with 1000 runs). As the number of runs/iterations are increased (e.g., to 15000) for the benchmark_model script, the inference delay starts increasing and the phone gets hotter.

(pls note that higher delay for warm-up runs are ignored)

**Describe the expected behavior**

 The inference delay is expected to be fairly constant for continuous operation. The degradation in performance with time is not expected and perhaps related to thermal issues.

Any suggestions or ideas to support custom operation via GPU or Hexagon delegate would be useful.  

Recommendations of a chipset/platform with good thermal performance for sustained operations would also be helpful.


**Standalone code to reproduce the issue**

Any tflite model taking e.g. average inference execution time of 50-100ms on Snapdragon 855+ would do.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39395,add support of reduce max to tensorflow lite micro,"@tensorflow/micro

tf lite micro 2.2

issue: Didn't find op for builtin opcode 'REDUCE_MAX' version '1'

suggestion: add support of reduce max to tensorflow lite micro"
39394,Is the TFLite-Micro Library Thread-safe?,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- TensorFlow installed from (source or binary): N/A
- Tensorflow version (commit SHA if source): 2.1.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): N/A

**Describe the problem**
May I know whether the TF-Lite Micro library is thread-safe or not?
(It is not documented in the official documents nor in the source code)

Meaning if I create multiple instances of resolver/interpreter/working buffer, I can run multiple models at the same time?
(I am a little worried about the ::tflite::GetModel() function is NOT thread-safe)

Thanks for your reply!

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
39393,Is the TFLite-Micro Library Thread-safe?,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
39392,[RNN]Failed to do full integer quantization and got error: Failed to parse the model: pybind11::init(): factory function returned nullptr.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.15.3
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): tf-nightly


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
import numpy as np

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.LSTM(256, input_shape=(60, 388), activation='tanh',
                               return_sequences=True))
model.add(tf.keras.layers.Dropout(0.3))
model.add(tf.keras.layers.Dense(388, activation='softmax'))

def representative_data_gen():
  for input_value in test_ds.take(100):
    yield [input_value]
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
converter.representative_dataset = representative_data_gen
tflite_model_quant = converter.convert()
```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 51, in __init__
    _calibration_wrapper.CalibrationWrapper(model_content))
TypeError: pybind11::init(): factory function returned nullptr

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/Users/lisichao/PycharmProjects/KerasSavedModel/mnistExample.py"", line 155, in <module>
    tflite_model_quant = converter.convert()
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 611, in convert
    constants.FLOAT, True)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 316, in _calibrate_quantize_model
    calibrate_quantize = _calibrator.Calibrator(result)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 53, in __init__
    raise ValueError(""Failed to parse the model: %s."" % e)
ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.
```
**Failure details**
When I convert the TensorFlow Model without optimization, it works.
When I do post-training integer quantization without a data representative, it works.
When I provide a data representative, it fails.
The error can be reproduced from the following file.
[ReproduceTheError.zip](https://github.com/tensorflow/tensorflow/files/4607083/ReproduceTheError.zip)





"
39391,Calling model.load_weights() on model built with Sequential API throws ValueError error - TensorFlow Save and loading APIs Guide,"**System information**
- Have I written custom code: No. Following a TF Guide and adding a line of code mentioned in its comments - no custom code/implemention.
- OS Platform and Distribution: Code is running on Google Colab from Google Chrome (Version 81.0.4044.129 (Official Build) (64-bit))
- TensorFlow installed from (source or binary): Binary - TF is running in Google Colab
- TensorFlow version: 2.2.0-dev20200508 (tf.version.GIT_VERSION = v1.12.1-31489-g6047d50555)
- Python version: 3.6.9
- Bazel version: NA
- GCC/Compiler version: NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

I am running the [Save and loading APIs Guide](https://www.tensorflow.org/guide/keras/save_and_serialize) by TensorFlow and came across an error that should not be raised as per the documentation provided in the same Notebook.

I was exploring the behavior of model.load_weights('pretrained_ckpt') [in this part of the guide](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/save_and_serialize.ipynb#scrollTo=Zdnpw_6PEsAN). The code cell contains the following:

```
# Example 2: Sequential model
# Recreate the pretrained model, and load the saved weights.
inputs = keras.Input(shape=(784,), name='digits')
x = keras.layers.Dense(64, activation='relu', name='dense_1')(inputs)
x = keras.layers.Dense(64, activation='relu', name='dense_2')(x)
pretrained_model = keras.Model(inputs=inputs, outputs=x, name='pretrained')

# Sequential example:
model = keras.Sequential(
    [pretrained_model, keras.layers.Dense(5, name='predictions')])
model.summary()

pretrained_model.load_weights('pretrained_ckpt')

# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error,
# but will *not* work as expected. If you inspect the weights, you'll see that
# none of the weights will have loaded. `pretrained_model.load_weights()` is the
# correct method to call.
```

The comment at the end states that,

> ```# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error```

However, if I add a new code cell right after the mentioned cell with the following line, it throws an error.

`model.load_weights('pretrained_ckpt')
`

The error is:

```
WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.

Two checkpoint references resolved to different objects (<tensorflow.python.keras.engine.training.Model object at 0x7f4335f43ac8> and <tensorflow.python.keras.layers.core.Dense object at 0x7f4334612748>).
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-22-d50d92a829e2> in <module>()
----> 1 model.load_weights('pretrained_ckpt')

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1126     """"""
   1127     if not self.is_compatible_with(other):
-> 1128       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
   1129 
   1130   def most_specific_compatible_shape(self, other):

ValueError: Shapes (5,) and (64,) are incompatible
```

Why is this happening? Has something changed recently causing this to break? 

**Describe the expected behavior**

As the comment states, the function call must not throw an error and return a function to be used.

> ```# Warning! Calling `model.load_weights('pretrained_ckpt')` won't throw an error```

**Standalone code to reproduce the issue**
Please see the sample code provided in the notebook linked below to reproduce the issue: 
[Google Colab Notebook](https://colab.research.google.com/drive/1-ilxHnd2gdqG8NvrJ2dvj0hO_d4_gV7u?usp=sharing)

**Other info / logs**
NA
"
39390,Mississippi Coronavirus Historic Data,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:https://covidtracking.com/data/state/mississippi#historical

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## The last four days of data does not match what is posted on the Mississippi State Health Departments website of Coronavirus statistics on number of tests conducted.

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
39389,<removed>,"import tensorflow
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

print(tensorflow.__version__)

thats ALL my code
Traceback (most recent call last):
  File ""C:/Users/Anon/PycharmProjects/Imgrecn1/img.py"", line 1, in <module>
    import tensorflow
  File ""C:\Users\Anon\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Anon\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *  # pylint: disable=redefined-builtin
  File ""C:\Users\Anon\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\framework\framework_lib.py"", line 77, in <module>
    from tensorflow.python.framework.ops import Graph
  File ""C:\Users\Anon\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\framework\ops.py"", line 42, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Anon\AppData\Local\Programs\Python\Python38-32\lib\site-packages\tensorflow\python\eager\context.py"", line 50, in <module>
    DEVICE_PLACEMENT_EXPLICIT = pywrap_tensorflow.TFE_DEVICE_PLACEMENT_EXPLICIT
AttributeError: module 'tensorflow.python.pywrap_tensorflow' has no attribute 'TFE_DEVICE_PLACEMENT_EXPLICIT'

 these are all the errors. i honestly despise who ever made tensorflow
"
39388,transfer learning for speech recognition ( short word vocabulary),"Dear Authors, 

I am trying to apply transfer learning for my dataset looking into the blog posted: 
https://github.com/tensorflow/docs/blob/master/site/en/r1/tutorials/sequences/audio_recognition.md
Is there a way to download the pertained checkpoints and load it for further training? 

Thank you so much team TF"
39387,tf.keras.Model does not support `*` in call method signature.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- TensorFlow version (use command below): latest
- Python version: 3.7

**Describe the current behavior**
A tf.keras.Model will fail if the call method signature includes the `*` named argument only identifier from Python 3.7. (and if python 3.8 is supported by TF then it should also support the `/` identifier)

https://docs.python.org/3.7/reference/compound_stmts.html#function-definitions

**Describe the expected behavior**
Should be robust to the use of these indentifiers

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1-5-BygHPbpuBlOXNme2PEKsFqyfG6jvl?usp=sharing

**Other info / logs** 
N/A"
39386,"TF 2.1.0 doesn't recognize GPU, PyTorch does","**System information**
- Windows 10 home
- TensorFlow version: 2.1.0
- Python version: 3.7.7
- Installed using conda
- CUDA/cuDNN version: CUDA is 10.1, cuDNN is 7.6.5
- GPU model and memory: GeForce GTX 960

**Describe the problem**
TF 2.1.0 doesn't recognize my GPU.
I've double and triple checked the versions of CUDA, cuDNN and everything is matching.  
I have set the environment variables as required in the end of TF installation, and checked that they are exposed in my code.  
In addition, **PyTorch do recognize and able to use my GPU**.  
I also tried installing `tf-nightly`, without help.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The following prints 0:
```
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
```

"
39385,Very slow compared to my PC,"
![Screenshot (3)](https://user-images.githubusercontent.com/39881819/81509318-5490ba00-9327-11ea-990f-d35c11b67c3d.png)

Google Colaboratory notebook training speed(On GPU) is very slow compare to my PC.
Specs:- Windows 10
            GPU - Nvidia Geforce GTX 950M
            CUDA - v10.1
            CUDNN - v7.6
Per Epoch Training time on my PC(GPU) - 5-6 minutes
Per Epoch Training time on Google Colaboratory(GPU)- More than 3 hr
I am using mounted drive for providing the dataset.

please provide me a solution so i can train my model faster."
39382,Tflite Heaxgon delegate - incompatible library,"I am going through the example [here](https://www.tensorflow.org/lite/performance/hexagon_delegate#hexagon_delegate_c_api) and having trouble building the binary to due to a link error:
""skipping incompatible C:/REPOSITORIES/tflite/v2.2.0/arm64-v8a/libhexagon_nn_skel.so when searching for -lhexagon_nn_skel"". The error is repeated for libhexagon_nn_skel_v65 and libhexagon_nn_skel_v66 as well.

The device architecture is arm64-v8a.

I'm building the example as a part of a larger project which already uses TFlite on GPU successfully. I'm building using ndk-build, so the build.grade file is irrelevant. What should I replace it with? What else I might be doing wrong? 
"
39381,Upgrade protobuf-java to resolve Java9+ specific warning messages,"Please upgrade the com.google.protobuf:protobuf-java dependency to the latest version to resolve https://github.com/protocolbuffers/protobuf/issues/3781

I forced an upgrade on the application-side and it seemed to work cleanly."
39380,Can't install Tensorflow v1.15 with pip,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.15
- Python version: 3.8.1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**

I'm unable to install v1.15 (or any version older than 2.2.0) using pip

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Run command: `pip install tensorflow==1.15`

Results in the error:
```
ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4, 2.2.0)
ERROR: No matching distribution found for tensorflow==1.15
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[Verbose log](https://github.com/tensorflow/tensorflow/files/4605851/pip-log.txt)
"
39378,Java Module support,"**Describe the problem**
Applications using Java9+ benefit from libraries have Java Module (JPMS) support. This comes in two stages:

1. Ideally, add `module-info.java` to the library. Either publish a separate artifact for Java9+ or simply add `META-INF/versions/9/module-info.class` to support both versions simultaneously (multi-release jar)
2. If the library has split packages, you cannot add `module-info.java` yet in which case you can simply add `Automatic-Module-Name` to the manifest file and begin resolving the split package issue.

* Multi-release JARs
  * https://openjdk.java.net/jeps/238
  * https://blog.codefx.org/tools/multi-release-jars-multiple-java-versions/"
39377,"data_element may be type tuple, instead of tf.Tensor","https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/base_preprocessing_layer.py#L171

Is it expected that a `tf.data` Dataset should only contain features, without labels?

Concretely, `next_data()` would return a tuple if the Dataset contains separate features and labels (common when `model.fit(dataset, ...)` ). Due to the subsequent try-except, this will cause `shape` to be set to `None` (i.e. tuple has no attribute shape) and the preprocessing normalization layer will TypeError since `len(None)` etc.

Apologies if I've misunderstood."
39376,"Does tensorflow 2.X provide functions similar to ""tf.contrib.lookup.index_table_from_tensor"" in TensorFlow 1.X?","In TensorFlow 1.X, ""tf.contrib.lookup.index_table_from_tensor"" is a very useful function. But I haven't found a similar function yet in TensorFlow 2X. So does tensorflow 2.X provide functions similar to ""tf.contrib.lookup.index_table_from_tensor"" in TensorFlow 1.X?
Thank you very much."
39375,Unstack a ragged tensor stacked by the tf data api,"**System information**
- TensorFlow version (you are using): 2.2
- Are you willing to contribute it (Yes/No): yes


**Describe the feature and the current behavior/state.**
For efficiency reason when saving my structured data as a tf records i save batches of input instead of only a row. 
My dataset contains mixtures of Ragged inputs and lists. 
Each tf Example contains a batch of N rows (here 10) batched together:

```
{'test': int64_list {
   value: 1
   value: 3
   value: 9
   value: 1
 }, 'test_offset': int64_list {
   value: 0
   value: 0
   value: 0
   value: 0
   value: 0
   value: 0
   value: 3
   value: 4
   value: 4
   value: 4
   value: 4
 }}
```
That i then parse using: 
```
features =  { 
       'test': tf.io.RaggedFeature(value_key='test', dtype=tf.int64, partitions=[ 
                tf.io.RaggedFeature.RowSplits(f""test_offset"")])}
def mapper(ex):
    return tf.io.parse_single_example(ex, features)
res = dataset.map(mapper).batch(2)
```
When opening back the batches I want to batch them with another batch size for shuffling reasons. 
If I try to batch with size 2 the resulting tensor will be a ragged tensor of shape (2, None, None). 

Instead I would like to have a shape (N*2, None). I managed to get that tensor by doing : 


```
x = next(iter(re))
tf.concat([x[0], x[1]], axis=1)
```

splitting should be the same as `tf.unstack(x, axis=1)` which does not work and break on a 

> TypeError: object of type 'RaggedTensor' has no len()

**Will this change the current api? How?**
Provide a way of unstacking ragged tensors to be able to stack them properly. 

**Who will benefit with this feature?**
Users of ragged features

**Any Other info.**
google colab: 
https://colab.research.google.com/gist/tanguycdls/f52dbd48a7c17ce282c82f91cd7547a6/untitled11.ipynb
"
39374,I have encounter with the following issue;,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39373,Problematic image loading with `image_dataset_from_directory`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2.0-dev20200508
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When using `image_dataset_from_directory` to load images it drops the color of the images significantly as can be seen below:

![image](https://user-images.githubusercontent.com/22957388/81499518-d4982f00-92e9-11ea-9f0c-7a188bdf1fe1.png)



**Describe the expected behavior**

The color drop should not be there.

**Standalone code to reproduce the issue**
Colab Gist: https://colab.research.google.com/gist/sayakpaul/f4c686a65ec88fa9bb0e62f4bc32004d/scratchpad.ipynb. 

"
39372,Failed to compile TF 1.15.2 from source code,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): want install tensorflow from source
- TensorFlow version: 1.15.2
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): bazel 0.26.1
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: -
- GPU model and memory: GPU: - Memory: 4GB

**Describe the problem**

Hello, I wanted to build TensorFlow 1.15.2 from source to works with my old non AVX supported CPU( Intel Pentium(R) Dual-Core E5300 2.60GHz). I looked [TensorFlow community wheels](https://github.com/yaroslavvb/tensorflow-community-wheels/issues) but not found any TF 1.15.2 with Python 3.8.2 and  without AVX support.

I just downloaded [TF 1.15.2 source code from github release](https://github.com/tensorflow/tensorflow/archive/v1.15.2.tar.gz), Extract it, According to [here](https://www.tensorflow.org/install/source) I installed dependencies and ...

```
$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.26.1 installed.
Please specify the location of python. [Default is /usr/bin/python3]: 


Found possible Python library paths:
  /usr/local/lib/python3.8/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.8/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apache Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```
an strange error throws durring compile: /usr/bin/python not found :|
you can see in configure file I selected to use /usr/bin/python3 file so I don't know why it want that file.
`ln -s /usr/bin/python3 /usr/bin/python` silents the error but after 30 min another error occurred. I saved the details of the second error in [here](https://drive.google.com/open?id=12PtN4_7W6I1x6lS1ypT48ujmuXIecNQ8).


 


"
39370,fit function doesn't throw any error or warning if validation_data is a list instead of tuple but shows unexpected validation logs,"<em>From [this StackOverflow question](https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0/61707324#61707324), the `fit` function with validation_data passed as a list continuously shows zero accuracy and loss without any warning or error. Here is a simple model trained with validation_data that shows such behavior [colab notebook](https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing)
</em>

**System information**
Ran on Google Colab

**Describe the current behavior**
If the `validation_data` parameter is passed as a list `[X, y]`, the model shows +0.000 for all the loss and metrics. Even though in the documentation it's mentioned the validation_data should be a tuple [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), the function accepts a list too but in the `unpack_x_y_sample_weight` function the data is sent in a wrong format if it's a list without throwing any error or warning. The log finally shows +0.0000 for all the validation results. 

**N.B: If `keras` is used instead of `tensorflow.keras` the validation logs show the desired output.**

**Describe the expected behavior**
Validation metrics shouldn't show +0.0000, either the list should be cast to a tuple or a warning/error message should be invoked.

**Standalone code to reproduce the issue**
[colab notebook](https://colab.research.google.com/drive/1P8iCUlnD87vqtuS5YTdoePcDOVEKpBHr?usp=sharing)

Here is a minimal example to reproduce the behavior:
```
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
import numpy as np

x = np.random.randn(1,19,)
y = np.ones((1,1))

def make_model():
    input_vec = tf.keras.layers.Input((19,))
    final = tf.keras.layers.Dense(12, activation='relu')(input_vec)
    final = tf.keras.layers.Dense(1, activation='sigmoid')(final)

    model = tf.keras.models.Model(inputs=input_vec, outputs=final)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

model = make_model()

model.fit(x, y, batch_size=1, epochs = 10, validation_data=[x,y])
```
Log:
```
Epoch 1/10
1/1 [==============================] - 0s 61ms/step - loss: 0.8490 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 2/10
1/1 [==============================] - 0s 36ms/step - loss: 0.8296 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 3/10
1/1 [==============================] - 0s 32ms/step - loss: 0.8106 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 4/10
1/1 [==============================] - 0s 30ms/step - loss: 0.7918 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 5/10
1/1 [==============================] - 0s 31ms/step - loss: 0.7734 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 6/10
1/1 [==============================] - 0s 30ms/step - loss: 0.7554 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 7/10
1/1 [==============================] - 0s 31ms/step - loss: 0.7377 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 8/10
1/1 [==============================] - 0s 30ms/step - loss: 0.7203 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 9/10
1/1 [==============================] - 0s 34ms/step - loss: 0.7033 - accuracy: 0.0000e+00 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00
Epoch 10/10
1/1 [==============================] - 0s 30ms/step - loss: 0.6866 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00

<tensorflow.python.keras.callbacks.History at 0x7f71a41a2710>
```

**Other info / logs** 
[stackoverflow question](https://stackoverflow.com/questions/61706535/keras-validation-loss-and-accuracy-stuck-at-0/61707324#61707324)

"
39369,Unnecessary tracing in tf.functions loaded from a saved model,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Darwin-18.7.0-x86_64-i386-64bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: Python 3.6.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

A custom keras model is saved using tf.saved_model.save and provided with a signature with None in the first dimension to allow for variable batch size. When the model is loaded back and used, it does tracing for each different batch size.

**Describe the expected behavior**

Since the model is saved with a signature, the expectation is that it should not perform tracing for different batch sizes.

The issue can be avoided by specifying the signature in the tf function instead of specifying it while saving the model, as shown in the linked gist.


**Standalone code to reproduce the issue**

The issue and the workaround can be seen in -

https://gist.github.com/saswatac/40aea67d71bbac021f7280e3a0cca419



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39367,ImportError: cannot import name 'export_saved_model' from 'tensorflow.python.keras.saving.saved_model',"I tried reinstalling Tensorflow as pip install tensorflow and Keras too by pip install keras
But still i get the same ImportError

Please help me out, Thank you."
39366,How to build TensorFlow Lite and use in Qt project ?,"System information
- Android 5.1:
- Qt Creator 4.11.2:
- Android NDK 20.1.5948944:
- Android SDK 26.1.1:
- Compiler Clang Qt 5.13.2 for Android ARMv7:
- JVM java-8-oracle:

I am trying to start project:
https://mechatronicsblog.com/tensorflow-lite-integration-with-qt-and-v-play-for-multi-platform-machine-learning-apps-on-ios-and-android/

But there is an issue - Tensorflow submodule is incorrect.

I downloaded Tensorflow library from main repo by the following command:
```
git clone https://github.com/tensorflow/tensorflow.git
```

Then I updated dependencies:
```
tensorflow/lite/tools/make/download_dependencies.sh
```
Then I built the library by the following command:
```
bazel build --cxxopt='-D_GLIBCXX_USE_CXX11_ABI=0' -c opt --config=android_arm tensorflow/lite/java:libtensorflowlite_jni
```
Here's what WORKSPACE looks like:
```
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 21,
    build_tools_version = ""26.0.2"",
    path = ""/home/user/Android/SDK"",
)

android_ndk_repository(
    name = ""androidndk"",
    api_level = 20,
    path = ""/home/user/android-ndk-r20b"",
)
```
The result was successful.

Then I copied the library folder and pasted it in the root of the project folder. And edited .pro file to add libraries:
```
# TensorFlow Lite - Global
TENSORFLOW_PATH = $$PWD/tensorflow/
TFLITE_MAKE_PATH = $$TENSORFLOW_PATH/tensorflow/lite/tools/make
INCLUDEPATH +=  $$TENSORFLOW_PATH \
                $$TFLITE_MAKE_PATH/downloads/ \
                $$TFLITE_MAKE_PATH/downloads/eigen \
                $$TFLITE_MAKE_PATH/downloads/gemmlowp \
                $$TFLITE_MAKE_PATH/downloads/neon_2_sse \
                $$TFLITE_MAKE_PATH/downloads/farmhash/src \
                $$TFLITE_MAKE_PATH/downloads/flatbuffers/include

# TensorFlow Lite - Android - armv7a
android {
    QT += androidextras

    LIBS += -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite \
            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/c \
            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/core/api \
            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/kernels \
            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/kernels/internal \
            -L$$TENSORFLOW_PATH/bazel-bin/tensorflow/lite/nnapi \
            -L$$TENSORFLOW_PATH/bazel-bin/external/androidndk \
            -L$$TENSORFLOW_PATH/bazel-bin/external/farmhash_archive \
            -L$$TENSORFLOW_PATH/bazel-bin/external/fft2d \
            -L$$TENSORFLOW_PATH/bazel-bin/external/flatbuffers \
            -L$$TENSORFLOW_PATH/bazel-bin/external/flatbuffers/src \
            -L$$TENSORFLOW_PATH/bazel-bin/external/ruy/ruy \
            -L$$TENSORFLOW_PATH/bazel-bin/external/ruy/ruy/profiler \
            -lallocation.pic -larena_planner.pic -larena_planner.pic -lminimal_logging.pic \
            -lsimple_memory_arena.pic -lstring_util.pic -lutil.pic \
            -lapi.pic -lbuiltin_op_kernels.pic -lbuiltin_ops.pic -lcpu_backend_context.pic -lcpu_backend_gemm.pic -leigen_support.pic \
            -lkernel_util.pic -llstm_eval.pic -laudio_utils.pic -lkernel_utils.pic -lneon_tensor_utils.pic \
            -lportable_tensor_utils.pic -ltensor_utils.pic -lquantization_util.pic -ltranspose_utils.pic \
            -lfarmhash.pic -lfft2d.pic -lflatbuffers.pic \
            -lallocator.pic -lapply_multiplier.pic -lblocking_counter.pic -lblock_map.pic -lcontext.pic -lcontext_get_ctx.pic \
            -lctx.pic -ldetect_arm.pic -ldetect_x86.pic -lhave_built_path_for_avx2.pic -lhave_built_path_for_avx512.pic \
            -lhave_built_path_for_avxvnni.pic -lhave_built_path_for_sse42.pic -lkernel_arm.pic -lkernel_avx2.pic \
            -lkernel_avx512.pic -lkernel_avxvnni.pic -lkernel_sse42.pic -lpack_arm.pic -lpack_avx2.pic -lpack_avx512.pic \
            -lpack_avxvnni.pic -lpack_sse42.pic -lprepacked_cache.pic -lthread_pool.pic -ltrace.pic -ltrmul.pic \
            -ltune.pic -lwait.pic -linstrumentation.pic -lnnapi_implementation.pic -lnnapi_util.pic
}
```
Then I built it up.
Errors:
```
tensorflow/lite/util.cc:47: error: undefined reference to 'TfLiteIntArrayCreate'
tensorflow/lite/util.cc:47: error: undefined reference to 'TfLiteIntArrayCreate'
tensorflow/lite/kernels/activations.cc:265: error: undefined reference to 'TfLiteIntArrayCopy'
tensorflow/lite/kernels/activations.cc:291: error: undefined reference to 'TfLiteIntArrayCopy'
tensorflow/lite/kernels/activations.cc:380: error: undefined reference to 'TfLiteIntArrayCopy'
tensorflow/lite/kernels/activations.cc:612: error: undefined reference to 'TfLiteIntArrayCopy'
tensorflow/lite/kernels/activations.cc:729: error: undefined reference to 'TfLiteTypeGetName'
tensorflow/lite/kernels/activations.cc:757: error: undefined reference to 'TfLiteTypeGetName'
tensorflow/lite/kernels/activations.cc:840: error: undefined reference to 'TfLiteTypeGetName'
tensorflow/lite/kernels/activations.cc:1084: error: undefined reference to 'TfLiteTypeGetName'
tensorflow/lite/kernels/arg_min_max.cc:40: error: undefined reference to 'TfLiteIntArrayCreate'
tensorflow/lite/kernels/arg_min_max.cc:40: error: undefined reference to 'TfLiteIntArrayCreate'
tensorflow/lite/kernels/basic_rnn.cc:104: error: undefined reference to 'TfLiteIntArrayFree'
tensorflow/lite/kernels/basic_rnn.cc:110: error: undefined reference to 'TfLiteIntArrayEqual'
tensorflow/lite/kernels/basic_rnn.cc:120: error: undefined reference to 'TfLiteIntArrayEqual'
tensorflow/lite/kernels/basic_rnn.cc:133: error: undefined reference to 'TfLiteIntArrayEqualsArray'
tensorflow/lite/kernels/basic_rnn.cc:144: error: undefined reference to 'TfLiteIntArrayEqualsArray'
tensorflow/lite/kernels/basic_rnn.cc:157: error: undefined reference to 'TfLiteIntArrayEqualsArray'
tensorflow/lite/kernels/basic_rnn.cc:168: error: undefined reference to 'TfLiteIntArrayEqualsArray'
tensorflow/lite/kernels/batch_matmul.cc:108: error: undefined reference to 'TfLiteIntArrayFree'
tensorflow/lite/kernels/bidirectional_sequence_lstm.cc:527: error: undefined reference to 'TfLiteIntArrayFree'
tensorflow/lite/kernels/bidirectional_sequence_lstm.cc:527: error: undefined reference to 'TfLiteIntArrayFree'
tensorflow/lite/kernels/bidirectional_sequence_lstm.cc:630: error: undefined reference to 'TfLiteIntArrayEqual'
tensorflow/lite/kernels/bidirectional_sequence_lstm.cc:642: error: undefined reference to 'TfLiteIntArrayEqual'
./tensorflow/lite/kernels/internal/reference/densify.h:36: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, TfLiteSparsity const&)'
./tensorflow/lite/kernels/internal/reference/densify.h:38: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'
./tensorflow/lite/kernels/internal/reference/densify.h:36: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<signed char>::FormatConverter(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, TfLiteSparsity const&)'
./tensorflow/lite/kernels/internal/reference/densify.h:38: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<signed char>::SparseToDense(signed char const*)'
tensorflow/lite/kernels/embedding_lookup_sparse.cc:178: error: undefined reference to 'TfLiteTensorRealloc'
tensorflow/lite/kernels/expand_dims.cc:105: error: undefined reference to 'TfLiteTensorRealloc'
./tensorflow/lite/kernels/internal/reference/sparse_ops/fully_connected.h:35: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::FormatConverter(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, TfLiteSparsity const&)'
./tensorflow/lite/kernels/internal/reference/sparse_ops/fully_connected.h:37: error: undefined reference to 'tflite::optimize::sparsity::FormatConverter<float>::SparseToDense(float const*)'
tensorflow/lite/kernels/if.cc:85: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'
tensorflow/lite/kernels/if.cc:92: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'
tensorflow/lite/kernels/if.cc:85: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'
tensorflow/lite/kernels/if.cc:92: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'
tensorflow/lite/kernels/if.cc:155: error: undefined reference to 'tflite::impl::Subgraph::Invoke()'
tensorflow/lite/kernels/reshape.cc:156: error: undefined reference to 'TfLiteTensorRealloc'
tensorflow/lite/kernels/while.cc:152: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'
tensorflow/lite/kernels/while.cc:170: error: undefined reference to 'tflite::impl::Subgraph::AllocateTensors()'
tensorflow/lite/kernels/while.cc:57: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'
tensorflow/lite/kernels/while.cc:267: error: undefined reference to 'tflite::impl::Subgraph::Invoke()'
tensorflow/lite/kernels/while.cc:292: error: undefined reference to 'tflite::impl::Subgraph::Invoke()'
tensorflow/lite/kernels/while.cc:57: error: undefined reference to 'tflite::impl::Subgraph::ResizeInputTensor(int, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&)'
tensorflow/lite/kernels/register.cc:34: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:35: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:36: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'
tensorflow/lite/kernels/register.cc:38: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:39: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'
tensorflow/lite/kernels/register.cc:41: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'
tensorflow/lite/kernels/register.cc:43: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int, int)'
tensorflow/lite/kernels/register.cc:52: error: undefined reference to 'tflite::MutableOpResolver::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:291: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:294: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:295: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'
tensorflow/lite/kernels/register.cc:297: error: undefined reference to 'tflite::MutableOpResolver::AddCustom(char const*, TfLiteRegistration const*, int)'
external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include/unordered_map:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
./tensorflow/lite/kernels/register.h:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
/home/sergey/FelgoProjects/TensorFlowLiteQtVPlay/tensorflow//bazel-bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):register.cc:vtable for tflite::ops::builtin::BuiltinOpResolver: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'
/home/sergey/FelgoProjects/TensorFlowLiteQtVPlay/tensorflow//bazel-bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):register.cc:vtable for tflite::ops::builtin::BuiltinOpResolver: error: undefined reference to 'tflite::MutableOpResolver::FindOp(char const*, int) const'
/home/sergey/FelgoProjects/TensorFlowLiteQtVPlay/tensorflow//bazel-bin/tensorflow/lite/kernels/libbuiltin_ops.pic.a(register.pic.o):register.cc:typeinfo for tflite::ops::builtin::BuiltinOpResolver: error: undefined reference to 'typeinfo for tflite::MutableOpResolver'
../../Felgo/Felgo/android_armv7/include/QtQml/qqml.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'
../../Felgo/Felgo/android_armv7/include/QtQml/qqml.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'
../TensorFlowLiteQtVPlay/auxutils.h:0: error: undefined reference to 'vtable for AuxUtils'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../../Felgo/Felgo/android_armv7/include/QtCore/qmetatype.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'
../../Felgo/Felgo/android_armv7/include/QtCore/qmetatype.h:0: error: undefined reference to 'ObjectsRecogFilter::staticMetaObject'
../TensorFlowLiteQtVPlay/objectsrecogfilter.h:0: error: undefined reference to 'vtable for ObjectsRecogFilter'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/tensorflowthread.h:0: error: undefined reference to 'vtable for TensorflowThread'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/tensorflowthread.h:0: error: undefined reference to 'vtable for WorkerTF'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/tensorflow/tensorflow/lite/mutable_op_resolver.h:0: error: undefined reference to 'vtable for tflite::MutableOpResolver'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
/home/sergey/android-ndk-r20b/sources/cxx-stl/llvm-libc++/include/memory:2339: error: undefined reference to 'tflite::FlatBufferModel::~FlatBufferModel()'
/home/sergey/android-ndk-r20b/sources/cxx-stl/llvm-libc++/include/memory:2339: error: undefined reference to 'tflite::impl::Interpreter::~Interpreter()'
main.o:main.cpp:vtable for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'ObjectsRecogFilter::metaObject() const'
main.o:main.cpp:vtable for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'ObjectsRecogFilter::qt_metacast(char const*)'
main.o:main.cpp:vtable for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'ObjectsRecogFilter::qt_metacall(QMetaObject::Call, int, void**)'
main.o:main.cpp:typeinfo for QQmlPrivate::QQmlElement<ObjectsRecogFilter>: error: undefined reference to 'typeinfo for ObjectsRecogFilter'
../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:16: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'
../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:0: error: undefined reference to 'vtable for ObjectsRecogFilter'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:161: error: undefined reference to 'ObjectsRecogFilter::runTensorFlow(QImage)'
../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:300: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'
../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:248: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'
../TensorFlowLiteQtVPlay/objectsrecogfilter.cpp:276: error: undefined reference to 'ObjectsRecogFilter::initializedChanged(bool const&)'
../TensorFlowLiteQtVPlay/tensorflowthread.cpp:16: error: undefined reference to 'WorkerTF::results(int, QStringList, QList<double>, QList<QRectF>, double)'
../TensorFlowLiteQtVPlay/tensorflowthread.cpp:17: error: undefined reference to 'WorkerTF::finished()'
../TensorFlowLiteQtVPlay/tensorflowthread.cpp:0: error: undefined reference to 'vtable for TensorflowThread'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/tensorflowthread.cpp:47: error: undefined reference to 'TensorflowThread::results(int, QStringList, QList<double>, QList<QRectF>, double)'
../TensorFlowLiteQtVPlay/tensorflowthread.h:0: error: undefined reference to 'vtable for WorkerTF'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:109: error: undefined reference to 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:118: error: undefined reference to 'tflite::impl::InterpreterBuilder::InterpreterBuilder(tflite::FlatBufferModel const&, tflite::OpResolver const&)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:121: error: undefined reference to 'tflite::impl::InterpreterBuilder::operator()(std::__ndk1::unique_ptr<tflite::impl::Interpreter, std::__ndk1::default_delete<tflite::impl::Interpreter> >*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:128: error: undefined reference to 'tflite::impl::Interpreter::UseNNAPI(bool)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:131: error: undefined reference to 'tflite::impl::Interpreter::SetNumThreads(int)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:136: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:166: error: undefined reference to 'tflite::impl::InterpreterBuilder::~InterpreterBuilder()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:166: error: undefined reference to 'tflite::impl::InterpreterBuilder::~InterpreterBuilder()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:272: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'
../TensorFlowLiteQtVPlay/tensorflow/tensorflow/lite/stderr_reporter.h:0: error: undefined reference to 'vtable for tflite::StderrReporter'
/home/sergey/android-ndk-r20b/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: the vtable symbol may be undefined because the class is missing its key function
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::DefaultErrorReporter()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::impl::Interpreter::Interpreter(tflite::ErrorReporter*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:321: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:324: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:327: error: undefined reference to 'tflite::impl::Interpreter::SetInputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:328: error: undefined reference to 'tflite::impl::Interpreter::SetOutputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:337: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:340: error: undefined reference to 'tflite::impl::Interpreter::AddNodeWithParameters(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, char const*, unsigned int, void*, TfLiteRegistration const*, int*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:341: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:354: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::DefaultErrorReporter()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:316: error: undefined reference to 'tflite::impl::Interpreter::Interpreter(tflite::ErrorReporter*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:321: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:324: error: undefined reference to 'tflite::impl::Interpreter::AddTensors(int, int*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:327: error: undefined reference to 'tflite::impl::Interpreter::SetInputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:328: error: undefined reference to 'tflite::impl::Interpreter::SetOutputs(std::__ndk1::vector<int, std::__ndk1::allocator<int> >)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:337: error: undefined reference to 'tflite::MutableOpResolver::FindOp(tflite::BuiltinOperator, int) const'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:340: error: undefined reference to 'tflite::impl::Interpreter::AddNodeWithParameters(std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, std::__ndk1::vector<int, std::__ndk1::allocator<int> > const&, char const*, unsigned int, void*, TfLiteRegistration const*, int*)'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:341: error: undefined reference to 'tflite::impl::Interpreter::AllocateTensors()'
../TensorFlowLiteQtVPlay/tensorflowlite.cpp:354: error: undefined reference to 'tflite::impl::Interpreter::Invoke()'
../TensorFlowLiteQtVPlay/tensorflow/tensorflow/lite/interpreter.h:178: error: undefined reference to 'tflite::impl::Interpreter::SetTensorParametersReadWrite(int, TfLiteType, char const*, unsigned int, int const*, TfLiteQuantizationParams, bool, unsigned int, int const*)'
tensorflow/lite/string_util.cc:108: error: undefined reference to 'TfLiteTensorReset'
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [libTensorFlowLiteQtVPlay.so] Error 1
```
"
39365,interpreter.allocate_tensors() Failing for MNIST model with LocallyConnected2D layer,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **macOS 10.15.4**
- TensorFlow installed from (source or binary): **from pip**
- TensorFlow version (or GitHub SHA if from source): **2.2.0-rc4**

**Command used to run the converter or code if you’re using the Python API**
[colab.research.google.com](https://colab.research.google.com/drive/1GFvL0LrCYEwsLVBxjxF3hVeraGDZgJ41?usp=sharing) **_WORKS FINE ON COLAB BUT BREAKS ON MY MACHINE_**

```
def convert_keras_to_tflite_and_verify(keras_model):
  converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)
  tflite_model = converter.convert()
  interpreter = tf.lite.Interpreter(model_content=tflite_model)
  interpreter.allocate_tensors()  # ERRORS OUT
```

**The output from the converter invocation**

```
2020-05-09 22:05:21.516092: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-05-09 22:05:21.516150: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-05-09 22:05:21.580927: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-05-09 22:05:21.580949: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-05-09 22:05:21.580954: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-05-09 22:05:23.603879: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-05-09 22:05:23.603950: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-05-09 22:05:23.856983: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-05-09 22:05:23.857001: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 3693 nodes (-25), 4297 edges (-25), time = 124.184ms.
2020-05-09 22:05:23.857005: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 3693 nodes (0), 4297 edges (0), time = 67.772ms.
Allocating tensors
Traceback (most recent call last):
  File ""main.py"", line 122, in <module>
    convert_keras_to_tflite_and_verify(mnist_model)
  File ""main.py"", line 68, in convert_keras_to_tflite_and_verify
    interpreter.allocate_tensors()
  File ""/Users/xx/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 242, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/Users/xx/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 110, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (1331 != 121)Node number 3040 (RESHAPE) failed to prepare.
```

**Also, please include a link to the saved model or GraphDef**

The model can be found here [colab.research.google.com](https://colab.research.google.com/drive/1GFvL0LrCYEwsLVBxjxF3hVeraGDZgJ41?usp=sharing)

**Failure details**
The `interpreter.allocate_tensors()` method works fine on the colab notebook but fails for me locally on my machine for the exact same model

Background - I am trying to add a basic attention branch to a CNN model from https://keras.io/examples/mnist_cnn/. 
Replacing the `LocallyConnected2D` layer from the attention branch with `Conv2D` makes everything work fine.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39363,bazel build; encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swif,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: source
- TensorFlow version: 1.14.0
- Python version:3.7.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version: 0.24.1
- trying to build tensorflow cpu not gpu 

I followed the intructions and based my versions found here: https://www.tensorflow.org/install/source#tested_build_configurations



**Describe the problem**
When i run the command
``` bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package ``` it gives the following error: 

```
Starting local Bazel server and connecting to it...
ERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_swift/releases/download/0.9.0/rules_swift.0.9.0.tar.gz] to /home/domingo_cm/.cache/bazel/_bazel_domingo_cm/f08b2f8b39197d5a57f7b557c17f0caf/external/build_bazel_rules_swift/rules_swift.0.9.0.tar.gz: Unknown host: github.com
ERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': java.io.IOException: Error downloading [https://github.com/bazelbuild/rules_swift/releases/download/0.9.0/rules_swift.0.9.0.tar.gz] to /home/domingo_cm/.cache/bazel/_bazel_domingo_cm/f08b2f8b39197d5a57f7b557c17f0caf/external/build_bazel_rules_swift/rules_swift.0.9.0.tar.gz: Unknown host: github.com
INFO: Elapsed time: 14.145s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. git clone https://github.com/tensorflow/tensorflow.git
2. cd tensorflow
3. git checkout r1.14
4. ./configure
5. (assuing bazel is installed ) bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
I am running under a corporate proxy network if it gives any importance
Also, git version is 2.17.1 (latest in Ubuntu 18.04)
"
39362,Example create_sine_model.ipynb disappreared ,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source):  I921176f6a8dc4c76bd45e6a508548d3b1936f89d
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): all

**Describe the problem**
The create_sine_model.ipynb was deleted from the examples. 
There is afaik no new place mentioned, where it can be found.
It is btw the introductory example in Pete Wardens TinyML book. 

**Please provide the exact sequence of commands/steps when you ran into the problem**
Ich checked the github location which is suggested in the book and could't find the example. By searching the last commit, I noticed that it was deleted recently.
"
39361,Issue with Python3.8 and Ubuntu 20.04,"@tensorflow/micro

**System information**
- Ubuntu 20.04 arm64
- TensorFlow installed from (source or binary): https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl
- Tensorflow version (commit SHA if source): 2.1.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33, etc.): Raspberry Pi 3

**Describe the problem**
Please make support for Python 3.8 on Ubuntu 20.04 with Raspberry Pi 3 arm64. I try to install TF Lite with Python 3.8, but it isn't a good version of TF Lite and I read that the TF 2.2.0 works with Python 3.8, but this version I cannot find on the page: https://www.tensorflow.org/lite/guide/python. Thanks.

**Please provide the exact sequence of commands/steps when you ran into the problem**
pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0.post1-cp37-cp37m-linux_aarch64.whl

"
39360,[Docs] Inconsistencies with Keras Applications preprocess_input,"The latest [keras.io](https://keras.io) website includes a whole bunch of great examples and tutorials that could help users get started with deep learning, transfer learning and combines all the goodness that comes in around with the TF 2.x support for keras.

1. https://keras.io/guides/transfer_learning/
2. https://www.tensorflow.org/tutorials/images/transfer_learning

## Description of documentation issue:
One of the major setbacks that I observed was brought in by the transfer learning examples portraying use of data augmentation involving the newly added `Resize()` and `Rescaling()` (rescaling the data from a range of 0-255 to 0.0-1.0, specific case for Xception in [1.](https://keras.io/guides/transfer_learning/) and -1.0-+1.0, specific case for MobileNetV2 in [2.](https://www.tensorflow.org/tutorials/images/transfer_learning)) layers from `tf.keras.layers.experimental` as per the latest keras RFC on Preprocessing Layers API. This is indeed a well planned transition and beneficial for users to switch to tf.data API instead of the `keras.preprocessing.image.ImageDataGenerator` in favour of performance and reducing training bottlenecks.

A majority of the users still are using the ImageDataGenerator class with pre-trained models from keras-applications instead of the new `tf.keras.preprocessing.image_dataset_from_directory` or tf.data API(s) directly. Most likely, with the use of the `preprocessing_function` argument to ImageDataGenerator objects.

eg.
```python3
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.resnet_50 import preprocess_input
datagen = image.ImageDataGenerator(preprocessing_function=preprocess_input)
```

The new documentation encourages use of tf.data API(s) exclusively which lacks consistency with this type of legacy image preprocessing used. As a result of various pre-trained models of keras_applications being ported from a variety of frameworks apart from TensorFlow, each specific keras_applications model requires it's own way of preprocessing images. The currently available ones being: 
https://github.com/tensorflow/tensorflow/blob/de5b0cfd434c7a9f848ab100b70a6be16e48280b/tensorflow/python/keras/applications/imagenet_utils.py#L72-L83
This makes it necessary for each of the pre-trained ConvNet(s) that form of a part of the keras_applications repo, be documented relating to which pre-trained model requires which type of preprocessing whatsoever and specifically how the same can be achieved using the newer Preprocessing Layers API or directly by chaining it with `ds.map()`.

A workaround in this regard would be to:
- update the newly created [keras-io repo](https://github.com/keras-team/keras-io) to document preprocessing method for each pre-trained model. [*please mention in comments if a corresponding Issue has to be filed in that repo*]
- introduce changes to `tf.keras` and the `tf.keras.layers.experimental.preprocessing` for creating new classes that can help serve this goal
- adding some documentation in place to explicitly specify use of respective `preprocess_input` with `ds.map`

/cc: @fchollet Is there any RFC in this regard proposed by the TensorFlow community to unify legacy preprocess_input functions with the newer Preprocessing Layers API?"
39359,ImportError: DLL load failed: The specified module could not be found.,"Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""G:\Final year project\python\Emotion-recognition-master\train_emotion_classifier.py"", line 5, in <module>
    from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\DELL\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>> 
"
39358,The trainable_variables of the subclass of tf.keras.Model is empty.,"class MaskMean(tf.keras.layers.Layer):
    def __init__(self, trainable=True):
        super(MaskMean, self).__init__()
        self.dropout = tf.keras.layers.Dropout(0.1, trainable=trainable)

class Predict(tf.keras.layers.Layer):
    def __init__(self, trainable=True):
        super(Predict, self).__init__()
        self.dense_layer = tf.keras.layers.Dense(512, activation='relu', name=""full connection"", trainable=trainable)
        self.predict = tf.keras.layers.Dense(1, name=""prediction"", trainable=trainable)

class Regulation(tf.keras.Model):
    def __init__(self, trainable=True):
        super(Regulation, self).__init__()
        # self.embedding = TFBertModel.from_pretrained(r'E:\googleCloudDisk\model\Bert\cased\base')
        # self.dropout = tf.keras.layers.Dropout(0.1, name='dropout')
        # self.inmediate = tf.keras.layers.Dense(512, name=""full connection"")
        # self.predict = tf.keras.layers.Dense(1, name=""prediction"")
        self.mask_mean_layer = MaskMean(trainable=trainable)
        self.predict = Predict(trainable=trainable)

Rmodel = Regulation(trainable=True)

But the Rmodel.trainable_variables() is empty.
How to define a subclass model of tf.keras.Model to make its' variables can be trainable and could return by Rmodel.trainable_variables()?"
39357,ImportError: DLL load failed: The specified module could not be found.,"I'm trying work with MINIST dataset with tensorflow
but it keep failed to load about the DLL file

**System information**
- WINDOWS 10
- TensorFlow installed : Anaconda (by Prompt)
- TensorFlow version: 2.1.0
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: Prompt pip
- GPU model and memory: NVIDIA GTX950 4G

ImportError: Traceback (most recent call last):
  File ""C:\Users\pc\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\pc\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\pc\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\pc\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\pc\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

I've tried uninstall and reinstall all the pip, tensorflow, pillow
with different kinds of CMD / Conda / Prompt
and it just didn't work and really can't figure out wheres the problem.
"
39351,tf.strings.join not allowed in graph,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 19.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2
- Python version:3.7.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When writing a function that is mapped on a `tf.data.Dataset`, using `tf.strings.join()` yields the following error
```
OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did not convert this function. Try decorating it directly with @tf.function.
```
Even when using the `@tf.function`

**Describe the expected behavior**
Should work when calling the `@tf.function` decorator

**Standalone code to reproduce the issue**
```
@tf.function
def process_path(path):
    path = tf.strings.split(path, ""_"")[:3]
    path = tf.strings.join(path, ""_"")
    return path

path_ds = tf.data.Dataset.list_files(...)
path_ds = path_ds.map(process_path)
```
"
39347,I used from keras_applications.inception_v3 import InceptionV3 but it showed the following error.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39346,Cannot import EfficientNetB0 from tensorflow.keras.applications,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 (docker container)
- TensorFlow installed from (source or binary): binary (using pip) 
- TensorFlow version (use command below):  v2.2.0-rc4-8-g2b96f3662b 2.2.0 (I installed version 2.2.0 from pypi)
- Python version: 3.7.7

**Describe the current behavior**
I cannot import EfficientNetB0 from tensorflow.keras.applications
When listing the module, using `print(dir(tensorflow.keras.applications))`, I can see other applications (like MobileNetV2, ...), but not efficient net or dense net
 This is weird, since here (https://github.com/tensorflow/tensorflow/tree/v2.2.0/tensorflow/python/keras/applications) it's present.


**Describe the expected behavior**
`from tensorflow.keras.applications import EfficientNetB0` should work

"
39345,Failed to import tensorflow with Winpython64-3.7.7.0 (Windows 10),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Just tried to import tensorflow
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
Winpython64-3.7.7.0
- **TensorFlow version (use command below)**:
tensorflow_cpu 2.1.0
- **Python version**:
Winpython64-3.7.7.0
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
Coral Edge TPU
- **Exact command to reproduce**:

import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)




### Describe the problem
Failed to import tensorflow (see logs)

### Source code / logs
import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\ptpython\repl.py"", line 148, in _execute
    code = compile_with_flags(line, ""eval"")
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\ptpython\repl.py"", line 135, in compile_with_flags
    dont_inherit=True,
  File ""<stdin>"", line 1
    import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
         ^
SyntaxError: invalid syntax

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Traceback (most recent call last):
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\ptpython\repl.py"", line 148, in _execute
    code = compile_with_flags(line, ""eval"")
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\ptpython\repl.py"", line 135, in compile_with_flags
    dont_inherit=True,
  File ""<stdin>"", line 1
    import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
         ^
SyntaxError: invalid syntax

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""E:\Software\Coral\WPy64-3770\python-3.7.7.amd64\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
39344,Gradient of outputs with respect to inputs inside a custom loss function,"I want to write a custom loss function for a Multilayer Perceptron network in Keras. The loss has two components: first is the regular 'mse' and the second is element wise gradients of output with respect to input features. Let x be the input with 2 features (size: number of samples X 2) and y the output with single output (size: number of samples X 1). I am denoting derivative of each output sample with first feature of each sample as `$\frac{dy[:]}{dx[:,0]}$`

Similarly, I want to compute the following expression inside the loss function:

`$$r[:] = y[:] \frac{dy[:]}{dx[:,0]} - x[:,1] \frac{d^2y[:]}{dx[:,0]^2}$$`

and take the mean square of the r vector. The total loss being the sum of the regular 'mse' and mean square of r vector.

This is a minimal, reproducible example of the code I tried:
```
def custom_loss_envelop(model_inputs, model_outputs):
    def custom_loss(y_true,y_pred):
        mse_loss = keras.losses.mean_squared_error(y_true, y_pred)
        print()
        print(model_inputs); print()
        print(model_outputs); print()
        dy_dx = keras.backend.gradients(model_outputs, tf.gather(model_inputs, [0], axis=1))
        print(dy_dx); print()
        d2y_dx2 = keras.backend.gradients(dy_dx, tf.gather(model_inputs, [0], axis=1))
        print(d2y_dx2); print()

        r = tf.multiply(model_outputs, tf.gather(dy_dx, [0], axis=1)) - tf.multiply(tf.gather(model_inputs, [1], axis=1), tf.gather(d2y_dx2, [0], axis=1)) # y*dy_dx[0] - x[1]*d2y_dx[0]2

        r = keras.backend.mean(keras.backend.square(r))
        loss = mse_loss + r
        return loss
    return custom_loss

nx=100;
inputs_train=np.random.uniform(0,1,(nx,2)); outputs_train=np.random.uniform(0,1,(nx,1))
inputs_val=np.random.uniform(0,1,(int(nx/2),2)); outputs_val=np.random.uniform(0,1,(int(nx/2),1))
n_hidden_units=50; l2_reg_lambda=0; learning_rate=0.001; dropout_factor=0.0; epochs=3

model = keras.Sequential();
model.add(keras.layers.Dense(n_hidden_units, activation='relu', input_shape=(inputs_train.shape[1],), kernel_regularizer=keras.regularizers.l2(l2_reg_lambda))); #first hidden layer
model.add(keras.layers.Dropout(dropout_factor)); model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(n_hidden_units, activation='relu', kernel_regularizer = keras.regularizers.l2(l2_reg_lambda)));
model.add(keras.layers.Dropout(dropout_factor)); model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(n_hidden_units, activation='relu', kernel_regularizer = keras.regularizers.l2(l2_reg_lambda)));
model.add(keras.layers.Dropout(dropout_factor)); model.add(keras.layers.BatchNormalization())
model.add(keras.layers.Dense(outputs_train.shape[1], activation='linear'));
optimizer1 = keras.optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=True)

model.compile(loss=custom_loss_envelop(model.inputs, model.outputs), optimizer=optimizer1, metrics=['mse'])

model.fit(inputs_train, outputs_train, batch_size=100, epochs=epochs, shuffle=True, validation_data=(inputs_val,outputs_val), verbose=1)
```

Here, I have generated training and validation samples randomly. I am getting the tensor shapes as follows: model_inputs: `[<tf.Tensor 'dense_input:0' shape=(None, 2) dtype=float32>]`, model_outputs: `[<tf.Tensor 'dense_3/Identity:0' shape=(None, 1) dtype=float32>]` and dy_dx: `[None]`. The first 2 are as expected but the derivative should also be of shape `(None, 1)` which it is not. Hence, I get `AttributeError: 'NoneType' object has no attribute 'op'` error in the line `d2y_dx2 = keras.backend.gradients(dy_dx, tf.gather(model_inputs, [0], axis=1))`

Any help is appreciated either to fix this issue or with alternate solution."
39343,Is it normal that every worker print the same loss and metric values when using multiworker distributed stragety?,"**Descriptions**

I've created a Keras model, and trained it with multiworker distributed strategy using `tf2.2.0`.

The training data I used is dataset gengerated from numpy array. And according to the [shard policy](https://www.tensorflow.org/api_docs/python/tf/data/experimental/DistributeOptions), the dataset will be auto sharded by `DATA policy`. That is to say every worker will handle different portion of the whole dataset.

After training, I notice that all the workers print the same loss and metric values just like they all trained with the identical data. But according to my knowledge, if every worker train with different data, the loss and metric values of them should be different. Is it because the shard policy didn't take effect?

I also know that multi-worker uses all-reduce communication method to keep variables in sync. So, the values printed are calculated after all-reduce? This could explain the same values maybe.

**Questions:**

1. Is this normal? And why?
2. How to check(or make sure) that different workers use different data portion for training(or the shard policy take effect)?
3. Are the values printed for all workers after all-reduce operation or just for that single worker?
4. When will all-reduce execute, on every batch end or epoch end?
5. When I use just numpy array for training, does the shard policy work? Or every worker will use the whole data for its training?
6. In general, dataset for every worker's fit method is same. When different, training will success as well, does shard policy work at this time?

**Code snippets:**

```python
class ThreeLayerMLP(keras.Model):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.dense_1 = layers.Dense(64, activation='relu', name='dense_1')
        self.dense_2 = layers.Dense(64, activation='relu', name='dense_2')
        self.pred_layer = layers.Dense(10, name='predictions')

    def call(self, inputs):
        x = self.dense_1(inputs)
        x = self.dense_2(x)
        return self.pred_layer(x)


def main(argv):
    del argv  # Unused args
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    BATCH_SIZE_PER_REPLICA = 64
    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
    print('Number of devices: %d' % strategy.num_replicas_in_sync)

    with strategy.scope():
        model = model = ThreeLayerMLP(name='3_layer_mlp')
        model.compile(
            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
            optimizer=keras.optimizers.RMSprop())

    log_dir = FLAGS.logs
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir,
                                                          histogram_freq=1,
                                                          update_freq='batch')

    np.random.seed(0)
    x_train, y_train = (np.random.random(
        (60000, 784)), np.random.randint(10, size=(60000, 1)))
    x_test, y_test = (np.random.random(
        (10000, 784)), np.random.randint(10, size=(10000, 1)))

    # options = tf.data.Options()
    # options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    train_dataset = train_dataset.shuffle(1024).batch(BATCH_SIZE)


    model.fit(
        train_dataset,
        epochs=5,
        steps_per_epoch=10,
        callbacks=tensorboard_callback)

    model_dir = FLAGS.logs + '/models/' + str(task_index)
    model.save(model_dir)


if __name__ == '__main__':
    app.run(main)
```

I'm cofused by these questions for long time. I will appreciate your help very much.

Thanks~"
39342,<Bug> while fit the tensor flow mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>
https://colab.research.google.com/drive/1Okqfq5UXFttj_atsmVqwooDwVibLtriq?usp=sharing
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39341,sparse_categorical_crossentropy metrics bug?,"**System information**
- Have I written custom code : No
- OS Platform and Distribution: macOS 10.15.4 (Reproduce on Colab)
- TensorFlow installed from (source or binary): from pip 
- TensorFlow version (use command below): tensorflow==2.2.0
- Python version: python 3.7.3 

TensorFlow version : v2.2.0-rc4-8-g2b96f3662b 2.2.0
**Describe the current behavior**
Trying to train with below code , however it shows ValueError.
```
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=[""accuracy""])

model.fit(train_ds, epochs=100, validation_data=valid_ds, steps_per_epoch=100, use_multiprocessing=False)
```
show below errors
```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-33-1c452d509fd9> in <module>()
----> 1 model.fit(train_ds, epochs=100, validation_data=valid_ds, steps_per_epoch=100, use_multiprocessing=False)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    966           except Exception as e:  # pylint:disable=broad-except
    967             if hasattr(e, ""ag_error_metadata""):
--> 968               raise e.ag_error_metadata.to_exception(e)
    969             else:
    970               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
        outputs = self.distribute_strategy.run(
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:543 train_step  **
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:391 update_state
        self._build(y_pred, y_true)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:322 _build
        self._metrics, y_true, y_pred)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1118 map_structure_up_to
        **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1214 map_structure_with_tuple_paths_up_to
        *flat_value_lists)]
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1213 <listcomp>
        results = [func(*args, **kwargs) for args in zip(flat_path_list,
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py:1116 <lambda>
        lambda _, *values: func(*values),  # Discards the path arg.
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:421 _get_metric_objects
        return [self._get_metric_object(m, y_t, y_p) for m in metrics]
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:421 <listcomp>
        return [self._get_metric_object(m, y_t, y_p) for m in metrics]
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:442 _get_metric_object
        y_t_rank = len(y_t.shape.as_list())
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_shape.py:1173 as_list
        raise ValueError(""as_list() is not defined on an unknown TensorShape."")

    ValueError: as_list() is not defined on an unknown TensorShape.
```
**Describe the expected behavior**
Expect the model start training.
```
Epoch 1/100
100/100 [==============================] - 22s 223ms/step - loss: 0.4050 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.1670 - val_sparse_categorical_accuracy: 0.9400
Epoch 2/100
 30/100 [========>.....................] - ETA: 13s - loss: 0.4082 - sparse_categorical_accuracy: 0.8763
```

**Standalone code to reproduce the issue**
**Reproducible Colab**
https://drive.google.com/open?id=1VjmF4OlPddPVbDG0iMNYg4lWjuis9H5f


**Other info / logs**

Although the bugs above, i can train by changing the metrics from the code 
[""accuracy""] to [tf.keras.metrics.SparseCategoricalAccuracy()]


"
39340,"TF 2.2.0 fails to build (Ubuntu Linux): ""this rule is missing dependency declarations""","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0 (2b96f3662b)
- Python version: Python 3.8.2
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): GCC 9.3.0 (Ubuntu 9.3.0-10ubuntu2)
- CUDA/cuDNN version: CUDA 10.2 / cuDNN 7.6.5 (but seems irrelevant for this issue)
- GPU model and memory: N/A

**Describe the problem**
TensorFlow 2.2.0 fails to build from source, complaining about ""missing dependency declarations"".
The precise error differs somewhat, depending on whether CUDA support is activated in the `configure` script.

When not activating CUDA, the following error occurs:

```
ERROR: /home/michael/devel/tensorflow/tensorflow/python/BUILD:797:1: undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_checkpoint_reader.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/util/py_checkpoint_reader_wrapper.cc':
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /home/michael/devel/tensorflow/tensorflow/python/tools/BUILD:82:1 undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_checkpoint_reader.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/util/py_checkpoint_reader_wrapper.cc':
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__ufunc_api.h'
```

With CUDA support activated, the same type of failure occurs, but at a different stage:

```
  ERROR: /home/michael/devel/tensorflow/tensorflow/python/BUILD:606:1: undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_tf_session.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/client/tf_session_wrapper.cc':
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/michael/devel/tensorflow/tensorflow/python/tools/BUILD:225:1 undeclared inclusion(s) in rule '//tensorflow/python:_pywrap_tf_session.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/client/tf_session_wrapper.cc':
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/bin/external/local_config_python/python_include/numpy/__ufunc_api.h'
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Given a working Bazel installation (2.0.0), the following commands lead to the error:

```
cd ~/devel/tensorflow
git clean -fxd
git checkout v2.2.0

python3 -m venv ~/.virtualenvs/tf_dev
source ~/.virtualenvs/tf_dev/bin/activate
pip install -U pip six numpy wheel setuptools mock 'future>=0.17.1'
pip install -U keras_applications --no-deps
pip install -U keras_preprocessing --no-deps

./configure
# (see below for output)

bazel build --config=opt -c opt //tensorflow/tools/pip_package:build_pip_package
```

Output from `configure` (in the CUDA support case; default options were used, except for CUDA support):
```
  (tf_dev) michael@the-beast ➜  ~/devel/tensorflow git:(2b96f3662b) ./configure                                                                                        
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /home/michael/.virtualenvs/tf_dev/bin/python]: 


Found possible Python library paths:
  /home/michael/.virtualenvs/tf_dev/lib/python3.8/site-packages
Please input the desired Python library path to use.  Default is [/home/michael/.virtualenvs/tf_dev/lib/python3.8/site-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Found CUDA 10.2 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include
Found cuDNN 7 in:
    /usr/local/cuda/lib64
    /usr/local/cuda/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 5.2,5.2]: 5.2


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

I can attach a fuill build log, if necessary. Due to the asynchronous build nature, it might be hard to interpret, though. The above error messages are the only obvious notices w.r.t. the problem at hand."
39339,"AutoGraph and tf.function are not working for TPU: Resubmitting this issue, because no reply so far for previous thread.","It seems that TPU only supports keras fit() function, but unable to use functions from tf.function and autographs.

Tensorflow version 2.1
python version 3.6

Issue can be reproduced in colab.

Here's the link to gist.
https://gist.github.com/saahiluppal/0bf79c27a15eaf6a72b25322eae6b6aa

Thanks."
39338,TypeError: 'DatasetV1Adapter' object is not subscriptable,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow version (use command below):1.15.0 (I have to use tf 1.x rather than tf 2.x)
- Python version: python3.7
- CUDA/cuDNN version: 10.0
- GPU model and memory: Nvidia 8G
**Error**
```
Traceback (most recent call last):
  File ""/home/frank/PycharmProjects/reconstruction_NN/my_test.py"", line 77, in <module>
    train_script(model,example,opt)
  File ""/home/frank/PycharmProjects/reconstruction_NN/my_test.py"", line 54, in train_script
    output = model(example['my_x'])
TypeError: 'DatasetV1Adapter' object is not subscriptable
```

**Full Code**
```
import tensorflow as tf
from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, Conv2D, Reshape, MaxPool2D, Average, Dropout, Concatenate, \
    Add, Maximum, Layer, Activation, Conv1D, TimeDistributed, GlobalAvgPool2D
import numpy as np
tf.compat.v1.enable_eager_execution()
print(tf.__version__)
print(tf.executing_eagerly())


class Test(tf.keras.Model):
    def __init__(self,attention_sz,dropout_rt, name=None):
        super(Test, self).__init__(name=name)
        # here we define the layer:
        self.fc = Dense(attention_sz,input_dim = attention_sz ,activation='relu')
        self.fc2 = Dense(attention_sz, activation='relu')
        self.fc3 = Dense(1, activation='sigmoid')

        self.dp = Dropout(dropout_rt,input_shape=(attention_sz,))
        self.dp2 = Dropout(dropout_rt,input_shape=(attention_sz,))


    def call(self, inp):
        # here we get the segmentation and pose
        with tf.device('/gpu:0'):
            print(""~~~~~~~~~~~"")
            x = self.fc(inp)
            print(x.shape)
            z = self.dp(x)
            print(z.shape)
            x = self.fc2(z)
            print(x.shape)
            z = self.dp2(x)
            print(z.shape)
            y = self.fc3(z)
            print(y.shape)
        return y # here z is the weight.


    # here we overwrite the method.
    def save(self, checkpoint_path):
        print(""Saving model..."")
        self.save_weights(checkpoint_path)
        print(""Model saved"")
    def load(self, checkpoint_path):
        print(""Loading model checkpoint {} ...\n"".format(checkpoint_path))
        self.load_weights(checkpoint_path)
        print(""Model loaded"")

checkpoint_path = ""saved_model/""


def train_script(model, example, optimizer):
    with tf.GradientTape() as tape:
        output = model(example['my_x'])
        loss = tf.reduce_mean(tf.abs(output - example['my_y']))
    variables = model.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    return loss

if __name__ == '__main__':
    model = Test(1024, 0.05)

    x = np.round(np.random.normal(1.75, 0.2, size=(10000, 1024)), 2)
    x2 = np.round(np.random.normal(100.75, 0.2, size=(10000, 1024)), 2)
    labels = np.zeros((10000, 1))
    labels2 = np.ones((10000, 1))
    x_t = np.row_stack((x, x2))
    labels = np.row_stack((labels, labels2))
    x_t = tf.convert_to_tensor(x_t)
    labels = tf.convert_to_tensor(labels)

    example = tf.data.Dataset.from_tensor_slices(
        dict(my_x=x_t, my_y=labels)).repeat().batch(2)

    opt = tf.keras.optimizers.Adam(0.1)
    train_script(model,example,opt)

```"
39337,Memory alignment of C + + member variables,"I'm using tensorflow for image recognition。Now it's been crashing
The logs are as follows：
 *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2020-05-09 15:50:16.279 8343-8343/? A/DEBUG: Build fingerprint: 'Android/msm8953_32/msm8953_32:7.1.2/N2G47H/howard04261732:user/test-keys'
2020-05-09 15:50:16.279 8343-8343/? A/DEBUG: Revision: '0'
2020-05-09 15:50:16.279 8343-8343/? A/DEBUG: ABI: 'arm'
2020-05-09 15:50:16.279 8343-8343/? A/DEBUG: pid: 8134, tid: 8134, name: recycling  >>> recycling <<<
2020-05-09 15:50:16.279 8343-8343/? A/DEBUG: signal 7 (SIGBUS), code 1 (BUS_ADRALN), fault addr 0x8baaf4bd
2020-05-09 15:50:16.279 8343-8343/? A/DEBUG:     r0 be9f370c  r1 be9f370c  r2 00024bfc  r3 00000000
2020-05-09 15:50:16.280 8343-8343/? A/DEBUG:     r4 00000000  r5 00000000  r6 8b493000  r7 be9f36b0
2020-05-09 15:50:16.280 8343-8343/? A/DEBUG:     r8 a1837010  r9 00000000  sl 00000000  fp 00024c00
2020-05-09 15:50:16.280 8343-8343/? A/DEBUG:     ip 8baaf4bd  sp be9f3644  lr 00093000  pc 8e9bc67e  cpsr 600d0030
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG: backtrace:
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #00 pc 0008167e  /data/app/com.pekon.recycling-1/lib/arm/libtensorflowlite_jni.so
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #01 pc 000838cb  /data/app/com.pekon.recycling-1/lib/arm/libtensorflowlite_jni.so
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #02 pc 000801c3  /data/app/com.pekon.recycling-1/lib/arm/libtensorflowlite_jni.so
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #03 pc 000f7a39  /data/app/com.pekon.recycling-1/lib/arm/libtensorflowlite_jni.so
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #04 pc 000f9fc7  /data/app/com.pekon.recycling-1/lib/arm/libtensorflowlite_jni.so
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #05 pc 00007be3  /data/app/com.pekon.recycling-1/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+26)
2020-05-09 15:50:16.282 8343-8343/? A/DEBUG:     #06 pc 00a253f3  /data/app/com.pekon.recycling-1/oat/arm/base.odex (offset 0x9d1000)"
39336,Distributed Training on multiple nodes - process gets stuck after initializing grpc channel,"**System information**
- TF v.2.1
- Linux-based HPC
- Snakemake workflow manager
- Slurm as scheduler
- TensorFlow installed from (source or binary): virtual conda environment
- Python version: 3.7.6
- CUDA/cuDNN version: 10.1
- GPU model and memory: GeForce GTX 980 computeCapability: 5.2,  coreClock: 1.2405GHz coreCount: 16 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 208.91GiB/s


**Describe the current behavior**
Hi there,  
My aim is to train a neural net on multiple nodes and GPUs on a HPC. Therefore, I am using TF's `MultiWorkerMirroredStrategy` and the `SlurmClusterResolver` to get the configuration of my nodes and to set the TF_CONFIG variable.  
  
However, when trying to connect to the cluster using:  
```
tf.config.experimental_connect_to_cluster(resolver,
                                            job_name = 'worker',
                                            task_index = cfg['task']['index'],
                                            protocol = 'grpc')
```  
the process gets stuck (please see the log below).  
When I `ssh` into the nodes I see my processes there but they are all sleeping. GPUs aren't used either.  
  
Has anyone experience with this kind of setup and can provide help?  
  
If I don't use the `experimental_connect_to_cluster` every node is executing the job independently, instead of working together.  
I know that TF_CONFIG should be set before calling the `MultiWorkerMirroredStrategy` but this caused a `RunTimeError`.  
I also played around with the position of GPU initialization in the code - this doesn't seem to have an effect.  
  
Note that I am using the most recent version of the `SlurmClusterResolver`, I basically copied the script from the GitHub and call it using `slurm_cluster_resolver.SlurmClusterResolver`

**Standalone code to reproduce the issue**
```
import tensorflow as tf

tf.keras.backend.clear_session()
# print all tensor allocations
tf.debugging.set_log_device_placement(True)
tf.random.set_seed(42)

# instantiate strategy at program startup to prevent RuntimeError
print(""-------------------------------------------------------------------------"")

print(""DEFINE DISTRIBUTED TRAINING STRATEGY"")
# only RING communication uses grpc protocols

multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL)


import os
import sys
import json
import gc
import numpy as np
import pandas as pd



print(""-------------------------------------------------------------------------"")

print('CLUSTER CONFIGURATION')

def set_tf_config(resolver, environment=None):
    """"""Set the TF_CONFIG env variable from the given cluster resolver""""""
    cfg = {
        'cluster': resolver.cluster_spec().as_dict(),
        'task': {
            'type': resolver.get_task_info()[0],
            'index': resolver.get_task_info()[1],
        },
        'rpc_layer': resolver.rpc_layer,
    }
    if environment:
        cfg['environment'] = environment
    os.environ['TF_CONFIG'] = json.dumps(cfg)

    return cfg


# there must be one GPU for every task
resolver = slurm_cluster_resolver.SlurmClusterResolver(port_base = 11214, gpus_per_task=1, tasks_per_node=2)

print(""-------------------------------------------------------------------------"")

cfg = set_tf_config(resolver)
tf.print(cfg)
print(cfg)


print('CONNECT TO CLUSTER')

tf.config.experimental_connect_to_cluster(resolver,
                                            job_name = 'worker',
                                            task_index = cfg['task']['index'],
                                            protocol = 'grpc')

print(""-------------------------------------------------------------------------"")

# doesn't matter if at the beginning or not
print(""GPU CONFIGURATION"")
# allow memory growth

gpus = tf.config.experimental.list_physical_devices('GPU')

print('INITIALIZE GPUs')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

if gpus:
    tf.config.experimental.set_visible_devices(gpus, 'GPU')
    print(gpus)

print(""-------------------------------------------------------------------------"")



```
**Other info / logs**
```
2020-05-09 08:40:42.220277: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op StringFormat in device /job:localhost/replica:0/task:0/device:CPU:0
2020-05-09 08:40:42.220277: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op StringFormat in device /job:localhost/replica:0/task:0/device:CPU:0
2020-05-09 08:40:42.220607: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op PrintV2 in device /job:localhost/replica:0/task:0/device:CPU:0
2020-05-09 08:40:42.220639: I tensorflow/core/common_runtime/eager/execute.cc:573] Executing op PrintV2 in device /job:localhost/replica:0/task:0/device:CPU:0
{'cluster': {'worker': ['dge10:11214',
                'dge10:11215',
                'dge12:11214',
                'dge12:11215',
                'dge13:11214',
                'dge13:11215',
                'dge14:11214',
                'dge14:11215',
                'dge15:11214',
                'dge15:11215',
                'dge9:11214',
                'dge9:11215']},
'rpc_layer': 'grpc',
'task': {'index': 5, 'type': 'worker'}}
{'cluster': {'worker': ['dge10:11214',
                'dge10:11215',
                'dge12:11214',
                'dge12:11215',
                'dge13:11214',
                'dge13:11215',
                'dge14:11214',
                'dge14:11215',
                'dge15:11214',
                'dge15:11215',
                'dge9:11214',
                'dge9:11215']},
'rpc_layer': 'grpc',
'task': {'index': 4, 'type': 'worker'}}
2020-05-09 08:40:42.223059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-09 08:40:42.223095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-09 08:40:42.223113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]
2020-05-09 08:40:42.223157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102]
2020-05-09 08:40:42.226758: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> dge10:11214, 1 -> dge10:11215, 2 -> dge12:11214, 3 -> dge12:11215, 4 -> localhost:11214, 5 -> dge13:11215, 6 -> dge14:11214, 7 -> dge14:11215, 8 -> dge15:11214, 9 -> dge15:11215, 10 -> dge9:11214, 11 -> dge9:11215}
2020-05-09 08:40:42.226828: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:300] Initialize GrpcChannelCache for job worker -> {0 -> dge10:11214, 1 -> dge10:11215, 2 -> dge12:11214, 3 -> dge12:11215, 4 -> dge13:11214, 5 -> localhost:11215, 6 -> dge14:11214, 7 -> dge14:11215, 8 -> dge15:11214, 9 -> dge15:11215, 10 -> dge9:11214, 11 -> dge9:11215}
```
"
39334,value error raised whern calling export_saved_model() with estimator model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS
- TensorFlow version (use command below): TensorFlow 2.0 CPU | v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: python3.6
- CUDA/cuDNN version: None
- GPU model and memory: None

## Value Error raised whern calling `export_saved_model()` with estimator model
I am trying to export my trained estimator model for serving. The estimator model was converted from keras model with `tf.keras.estimator.model_to_estimator`, and I have successfully trained and evaluated it. However, when I am trying to save the model to local disk for serving,  a bug just bothered me... the codes and the logs are as listed below: 

## codes
```python
model_estimator = tf.keras.estimator.model_to_estimator(keras_model=model)
# ...
model_estimator.train(input_fn=dataset_func, steps=STEPS_PER_EPOCH)
model_estimator.evaluate(input_fn=dataset_func, steps=STEPS_PER_EPOCH)
# ...
model_estimator.export_saved_model(model_path, serving_input_fn) # bug comes from here
```
## log for Error info
```python
INFO:tensorflow:Calling model_fn.
[2020-05-09 13:20:56] - estimator.py[line:1147] - INFO: Calling model_fn.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    470                 preferred_dtype=default_dtype,
--> 471                 as_ref=input_arg.is_ref)
    472             if input_arg.number_attr and len(

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)
   1364             preferred_dtype=preferred_dtype,
-> 1365             ctx=ctx))
   1366   return ret

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx, accept_composite_tensors)
   1270           ""Tensor conversion requested dtype %s for Tensor with dtype %s: %r"" %
-> 1271           (dtype.name, value.dtype.name, value))
   1272     return value

ValueError: Tensor conversion requested dtype int64 for Tensor with dtype float32: <tf.Tensor 'no_mask_4_115/no_mask_4/Identity:0' shape=(None, 1) dtype=float32>

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-76-01ac33b3b8f1> in <module>
----> 1 model_estimator.export_saved_model(model_path, serving_input_fn)

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in export_saved_model(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, experimental_mode)
    733         as_text=as_text,
    734         checkpoint_path=checkpoint_path,
--> 735         strip_default_attrs=True)
    736 
    737   def experimental_export_all_saved_models(

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _export_all_saved_models(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)
    856             builder, input_receiver_fn_map, checkpoint_path,
    857             save_variables, mode=ModeKeys.PREDICT,
--> 858             strip_default_attrs=strip_default_attrs)
    859         save_variables = False
    860 

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _add_meta_graph_for_mode(self, builder, input_receiver_fn_map, checkpoint_path, save_variables, mode, export_tags, check_variables, strip_default_attrs)
    929           labels=getattr(input_receiver, 'labels', None),
    930           mode=mode,
--> 931           config=self.config)
    932 
    933       export_outputs = export_lib.export_outputs_for_mode(

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)
   1146 
   1147     logging.info('Calling model_fn.')
-> 1148     model_fn_results = self._model_fn(features=features, **kwargs)
   1149     logging.info('Done calling model_fn.')
   1150 

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in model_fn(features, labels, mode)
    286         features=features,
    287         labels=labels,
--> 288         optimizer_config=optimizer_config)
    289     model_output_names = []
    290     # We need to make sure that the output names of the last layer in the model

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/keras.py in _clone_and_build_model(mode, keras_model, custom_objects, features, labels, optimizer_config)
    225       in_place_reset=(not keras_model._is_graph_network),
    226       optimizer_iterations=global_step,
--> 227       optimizer_config=optimizer_config)
    228 
    229   if sample_weight_tensors is not None:

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in clone_and_build_model(model, input_tensors, target_tensors, custom_objects, compile_clone, in_place_reset, optimizer_iterations, optimizer_config)
    632         clone = clone_model(model, input_tensors=input_tensors)
    633     else:
--> 634       clone = clone_model(model, input_tensors=input_tensors)
    635 
    636     if all([isinstance(clone, Sequential),

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in clone_model(model, input_tensors, clone_function)
    420   else:
    421     return _clone_functional_model(
--> 422         model, input_tensors=input_tensors, layer_fn=clone_function)
    423 
    424 

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/models.py in _clone_functional_model(model, input_tensors, layer_fn)
    193   input_tensors, output_tensors, created_layers = (
    194       network.reconstruct_from_config(model_config,
--> 195                                       created_layers=created_layers))
    196   metrics_names = model.metrics_names
    197   model = Model(input_tensors, output_tensors, name=model.name)

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in reconstruct_from_config(config, custom_objects, created_layers)
   1850       if layer in unprocessed_nodes:
   1851         for node_data in unprocessed_nodes.pop(layer):
-> 1852           process_node(layer, node_data)
   1853 
   1854   input_tensors = []

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py in process_node(layer, node_data)
   1797       if not isinstance(input_tensors, dict) and len(flat_input_tensors) == 1:
   1798         input_tensors = flat_input_tensors[0]
-> 1799       output_tensors = layer(input_tensors, **kwargs)
   1800 
   1801       # Update node index map.

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    845                     outputs = base_layer_utils.mark_as_return(outputs, acd)
    846                 else:
--> 847                   outputs = call_fn(cast_inputs, *args, **kwargs)
    848 
    849             except errors.OperatorNotAllowedInGraphError as e:

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/merge.py in call(self, inputs)
    180         return y
    181     else:
--> 182       return self._merge_function(inputs)
    183 
    184   @tf_utils.shape_type_conversion

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/layers/merge.py in _merge_function(self, inputs)
    392 
    393   def _merge_function(self, inputs):
--> 394     return K.concatenate(inputs, axis=self.axis)
    395 
    396   @tf_utils.shape_type_conversion

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py in concatenate(tensors, axis)
   2706     return sparse_ops.sparse_concat(axis, tensors)
   2707   else:
-> 2708     return array_ops.concat([to_dense(x) for x in tensors], axis)
   2709 
   2710 

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py in concat(values, axis, name)
   1429           dtype=dtypes.int32).get_shape().assert_has_rank(0)
   1430       return identity(values[0], name=name)
-> 1431   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
   1432 
   1433 

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py in concat_v2(values, axis, name)
   1255   _attr_N = len(values)
   1256   _, _, _op = _op_def_lib._apply_op_helper(
-> 1257         ""ConcatV2"", values=values, axis=axis, name=name)
   1258   _result = _op.outputs[:]
   1259   _inputs_flat = _op.inputs

/usr/local/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)
    497                                 (prefix, dtype.name))
    498               else:
--> 499                 raise TypeError(""%s that don't all match."" % prefix)
    500             else:
    501               raise TypeError(

TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, float32, float32, float32, float32, float32, float32, float32, int64, int64, int64, int64, int64, int64, int64, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, float32, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, float32, float32, float32, float32, float32, float32, float32, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, float32, float32, float32, float32, float32, float32, float32, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64, int64] that don't all match.
```

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39333,master build error(windows10),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 1909
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master 2.2.0
- Python version: 3.8.2
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): Visual Studio 2019  compiler
- CUDA/cuDNN version: 10.2/7.6.5
- GPU model and memory: RTX2080Ti GDDR6 11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
bazel build

**Describe the expected behavior**
success as always

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
1. ./configure ( and set to user cuda 10.2 and cudnn 7.6.5 )
2. bazel build --define=no_tensorflow_py_deps=true --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Execution platform: @local_execution_config_platform//:platform
.\tensorflow/core/kernels/cuda_sparse.h(39): error: identifier ""cusparseDnMatDescr_t"" is undefined

.\tensorflow/core/kernels/cuda_sparse.h(40): error: identifier ""cusparseSpMatDescr_t"" is undefined

.\tensorflow/core/kernels/cuda_sparse.h(41): error: identifier ""cusparseSpMMAlg_t"" is undefined

3 errors detected in the compilation of ""C:/Users/ALAN-W~1/AppData/Local/Temp/nvcc_inter_files_tmp_dir/tmp4sztyoaw/kernels_gpu.cu.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```"
39332,AdditiveAttention score calculation,"Bahdanau's paper specifies that the score is calculated as:

`score(st,hi)=v⊤atanh(Wa[st;hi]) `

While the actualy implemention's score is calcuted as:
```
    return math_ops.reduce_sum(
        scale * math_ops.tanh(q_reshaped + k_reshaped), axis=-1)
```

The explaination:
```

 1. Reshape `query` and `value` into shapes `[batch_size, Tq, 1, dim]`
     and `[batch_size, 1, Tv, dim]` respectively.
  2. Calculate scores with shape `[batch_size, Tq, Tv]` as a non-linear
     sum: `scores = tf.reduce_sum(tf.tanh(query + value), axis=-1)`
  3. Use scores to calculate a distribution with shape
     `[batch_size, Tq, Tv]`: `distribution = tf.nn.softmax(scores)`.
  4. Use `distribution` to create a linear combination of `value` with
     shape `batch_size, Tq, dim]`:
     `return tf.matmul(distribution, value)`.
```

Why such a difference? 

The Tensorflow's attention tutorial is more consistent with the paper:
https://www.tensorflow.org/tutorials/text/nmt_with_attention#restore_the_latest_checkpoint_and_test

"
39331,How to extract_sub_graph in TF2?,"My goal is to get partial model (`tf.Graph`) as feature extractor.

In TF1, the following utility can be used to get partial TF graph: https://www.tensorflow.org/api_docs/python/tf/compat/v1/graph_util/extract_sub_graph, how do I achieve same functionality given this util is deprecated?


```
[20200521 15:34:24 WARNING (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
```"
39329,CategoricalCrossentropy Label Smoothing Bug for tensors with dim > 2,"The flag ""label_smoothing"" in tf.keras.losses.CategoricalCrossentropy does not behave as expected for tensors with dimension > 2. When this parameter is set to non zero value it can on occasion cause severe learning instability and divergence. Looking at the source code the current code is as follows (found [here](https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/losses.py#L1521)):

```
def _smooth_labels():
    num_classes = math_ops.cast(array_ops.shape(y_true)[1], y_pred.dtype)
    return y_true * (1.0 - label_smoothing) + (label_smoothing / num_classes)
```
When the y_true tensor has dimension > 2 (for example when using a U-net architecture to classify each pixel of an image), the num_classes will be incorrect.
I think the correct way to look up the num_classes is to get the size of dim=-1:

`num_classes = math_ops.cast(array_ops.shape(y_true)[-1], y_pred.dtype)`"
39328,Build failure in tensorflow/tensorflow:devel docker container on MacOS Catalina 10.15.4,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.4 Docker 19.03.8 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source from within docker container of image tensorflow/tensorflow:devel 
- TensorFlow version: Release 2.0.1
- Python version:3.6.9
- Installed using virtualenv? pip? conda?: //tensorflow/tools/lib_package:libtensorflow
- Bazel version (if compiling from source): 3.0.0
- GCC/Compiler version (if compiling from source):  7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A Docker Host GPU is AMD Radeon R9 M295X 4 GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Tried to perform a build of tensorflowlib from within docker container using the following sequence of actions outlined below. Unable to diagnose source of build failure.....File /usr/share/doc/gcc-7/README.Bugs outlined in stack trace is not available in the docker cotainer provided by image tensorflow/tensorflow:devel

$ docker run -it -w /tensorflow_src -v $PWD/lib:/mnt -e HOST_PERMS=""$(id -u):$(id -g)"" \
    tensorflow/tensorflow:devel bash

$ git pull
$ ./configure #accept default python paths and no to all other questions
$  bazel build --config=opt --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/lib_package:libtensorflow


Received the following error.....

INFO: From Compiling tensorflow/core/framework/function_handle_cache.cc:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:22,
                 from ./tensorflow/core/framework/attr_value_util.h:24,
                 from ./tensorflow/core/framework/function.h:30,
                 from ./tensorflow/core/framework/function_handle_cache.h:20,
                 from tensorflow/core/framework/function_handle_cache.cc:15:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;
                                         ^
INFO: From Compiling tensorflow/cc/framework/cc_op_gen.cc:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:22,
                 from ./tensorflow/core/framework/attr_value_util.h:24,
                 from tensorflow/cc/framework/cc_op_gen.cc:25:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;
                                         ^
INFO: From Compiling tensorflow/core/ops/nn_ops.cc:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:22,
                 from ./tensorflow/core/framework/attr_value_util.h:24,
                 from ./tensorflow/core/framework/node_def_util.h:22,
                 from ./tensorflow/core/framework/shape_inference.h:21,
                 from ./tensorflow/core/framework/common_shape_fns.h:20,
                 from tensorflow/core/ops/nn_ops.cc:18:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;
                                         ^
INFO: From Compiling tensorflow/core/framework/function.cc:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:22,
                 from ./tensorflow/core/framework/attr_value_util.h:24,
                 from ./tensorflow/core/framework/function.h:30,
                 from tensorflow/core/framework/function.cc:16:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;
                                         ^
INFO: From Compiling tensorflow/core/util/batch_util.cc:
In file included from ./third_party/eigen3/unsupported/Eigen/CXX11/FixedPoint:41:0,
                 from ./tensorflow/core/framework/numeric_types.h:24,
                 from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/tensor.h:22,
                 from ./tensorflow/core/util/batch_util.h:18,
                 from tensorflow/core/util/batch_util.cc:16:
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:30:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 20> Packet32q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:31:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 21> Packet16q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:32:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 22> Packet32q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:33:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 23> Packet16q8i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:34:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 25> Packet16q8u;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:35:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 26> Packet8q16i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:36:41: warning: ignoring attributes on template argument '__m256i {aka __vector(4) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m256i, 27> Packet8q32i;
                                         ^
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX2.h:37:41: warning: ignoring attributes on template argument '__m128i {aka __vector(2) long long int}' [-Wignored-attributes]
 typedef eigen_packet_wrapper<__m128i, 28> Packet4q32i;
                                         ^
ERROR: /tensorflow_src/tensorflow/compiler/xla/service/BUILD:260:1: C++ compilation of rule '//tensorflow/compiler/xla/service:hlo_evaluator' failed (Exit 4)
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/lib_package:libtensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 16423.036s, Critical Path: 155.94s
INFO: 4629 processes: 4629 local.
FAILED: Build did NOT complete successfully

"
39326,No module named 'tensorflow.python.types' when building estimator from master branch,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Lunux Ubuntu 16.04
- TensorFlow installed from (source or binary):
Pip install tensorflow==2.2.0
- TensorFlow version:
2.2.0
- Python version:
3.6
- Installed using virtualenv? pip? conda?:
pip
- Bazel version (if compiling from source):
2.0.0 & 0.25.0
- GCC/Compiler version (if compiling from source):
5.4.0

**Describe the problem**
I'm trying to compile the estimator binary from the tensorflow/estimator master branch, using the bazel commands that listed on the readme page. The building failed with the error:

```
File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/9e73ac3ed666d141725dcd5fe404d502/sandbox/linux-sandbox/2/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/python/estimator/api/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/model_fn.py"", line 29, in <module>
    from tensorflow.python.types import core
ModuleNotFoundError: No module named 'tensorflow.python.types'
```

I tried to clean the directory and checkout the `esitimator/r2.2` branch, built with the same environment and command. That build was passed with no error.
Could someone suggest why master branch is having this import issue? Thanks

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
git clone https://github.com/tensorflow/estimator.git
pip uninstall -y tensorflow
pip install tensorflow==2.2.0
bazel build //tensorflow_estimator/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
/home/ubuntu/estimator/tensorflow_estimator/python/estimator/api/BUILD:38:1: Executing genrule //tensorflow_estimator/python/estimator/api:estimator_python_api_gen_compat_v1 failed (Exit 1) bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
Traceback (most recent call last):
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/9e73ac3ed666d141725dcd5fe404d502/sandbox/linux-sandbox/2/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/python/estimator/api/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/api/create_python_api_wrapper.py"", line 26, in <module>
    from tensorflow_estimator.python.estimator import estimator_lib  # pylint: disable=unused-import
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/9e73ac3ed666d141725dcd5fe404d502/sandbox/linux-sandbox/2/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/python/estimator/api/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow_estimator.python.estimator.canned.baseline import BaselineClassifier
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/9e73ac3ed666d141725dcd5fe404d502/sandbox/linux-sandbox/2/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/python/estimator/api/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/baseline.py"", line 59, in <module>
    from tensorflow_estimator.python.estimator import estimator
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/9e73ac3ed666d141725dcd5fe404d502/sandbox/linux-sandbox/2/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/python/estimator/api/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/estimator.py"", line 51, in <module>
    from tensorflow_estimator.python.estimator import model_fn as model_fn_lib
  File ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/9e73ac3ed666d141725dcd5fe404d502/sandbox/linux-sandbox/2/execroot/org_tensorflow_estimator/bazel-out/host/bin/tensorflow_estimator/python/estimator/api/create_tensorflow_estimator.python.estimator_api_1_estimator_python_api_gen_compat_v1.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/model_fn.py"", line 29, in <module>
    from tensorflow.python.types import core
ModuleNotFoundError: No module named 'tensorflow.python.types'
Target //tensorflow_estimator/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/ubuntu/estimator/tensorflow_estimator/tools/pip_package/BUILD:18:1 Executing genrule //tensorflow_estimator/python/estimator/api:estimator_python_api_gen_compat_v1 failed (Exit 1) bash failed: error executing command /bin/bash -c ... (remaining 1 argument(s) skipped)

Use --sandbox_debug to see verbose messages from the sandbox
INFO: Elapsed time: 5.025s, Critical Path: 1.66s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```"
39323,"InaccessibleTensorError: The tensor 'Tensor(""Tile:0"", shape=(None, 3), dtype=float32)' cannot be accessed here: it is defined in another function or code block.","**System information**
- Have I written custom code: Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS
- Mobile device: No
- TensorFlow installed from: binary
- TensorFlow version: 2.1.0
- Python version: 3.6.9
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the current behavior**
Throws the error.
**Describe the expected behavior**
Returns a tensor of shape (2, 6, 3), e.g.
```
tf.Tensor(
[[[0.4200933  0.51168334 0.13771784]
  [0.4200933  0.51168334 0.13771784]
  [0.4200933  0.51168334 0.13771784]
  [0.31555724 0.80608404 0.38079023]
  [0.31555724 0.80608404 0.38079023]
  [0.22353566 0.7539935  0.28550136]]

 [[0.3753245  0.10351241 0.61035573]
  [0.51126313 0.4842764  0.5390732 ]
  [0.1071049  0.8601215  0.69413567]
  [0.1071049  0.8601215  0.69413567]
  [0.         0.         0.        ]
  [0.         0.         0.        ]]], shape=(2, 6, 3), dtype=float32)
```
**Standalone code to reproduce the issue**
```
import tensorflow as tf
@tf.function(input_signature=[
        tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, None, 1), dtype=tf.int32),
    ])
def some_fz(x, dims):
    batch_size = tf.shape(x)[0]
    seq_len = tf.shape(x)[1]
    dims = tf.cast(tf.math.round(dims), tf.int32)
    new_lengths = tf.reduce_sum(dims, axis=1)
    max_dim = tf.math.reduce_max(new_lengths)
    pad_sizes = max_dim - new_lengths
    new_batch = []
    x = tf.expand_dims(x, axis=-2)
    for i in tf.range(batch_size):
        tensor_list = []
        for j in tf.range(seq_len):
            tiled = tf.tile(x[i][j], [dims[i][j][0], 1])
            tensor_list.append(tiled)
        new_tensor = tf.concat(tensor_list, axis=0)  # breaks here
        new_tensor = tf.pad(new_tensor, [[0,pad_sizes[i][0]], [0,0]])
        new_batch.append(new_tensor)
    return tf.stack(new_batch)

if __name__ == '__main__':
    random = tf.random.uniform([2, 3, 3])
    vector = tf.constant([[[3], [2], [1]], [[1], [1], [2]]])
    out = some_fz(random, vector)
```

**Other info / logs** 
```
Traceback (most recent call last):
  File ""/Users/fcardina/Library/Preferences/PyCharmCE2019.2/scratches/scratch_2.py"", line 58, in <module>
    out = some_fz(random, new_dimension)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
tensorflow.python.framework.errors_impl.InaccessibleTensorError: in converted code:

    /Users/fcardina/Library/Preferences/PyCharmCE2019.2/scratches/scratch_2.py:47 some_fz  *
        new_tensor = tf.concat(tensor_list, axis=0)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:1516 concat
        return identity(values[0], name=name)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/ops/array_ops.py:267 identity
        ret = gen_array_ops.identity(input, name=name)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_array_ops.py:3829 identity
        ""Identity"", input=input, name=name)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/framework/op_def_library.py:742 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:591 _create_op_internal
        inp = self.capture(inp)
    /Users/fcardina/anaconda3/envs/ttsTF/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py:641 capture
        % (tensor, tensor.graph, self))

    InaccessibleTensorError: The tensor 'Tensor(""Tile:0"", shape=(None, 3), dtype=float32)' cannot be accessed here: it is defined in another function or code block. Use return values, explicit Python locals or TensorFlow collections to access it. Defined in: FuncGraph(name=while_body_59, id=5613077840); accessed from: FuncGraph(name=while_body_37, id=5612708472).
```


I tried to declare the list outside of the first loop, but it won't access the elements then. Doing tensor_list[i] would throw 
```
TypeError: list indices must be integers or slices, not Tensor
```

I tried what is suggested here:
https://github.com/tensorflow/tensorflow/issues/37512#issuecomment-600776581
But neither solution works. The first needs the lists to have the same size. The second complains that tf.autograph.experimental.set_loop_options has no shape_invariants argument."
39322,C++ compilation of rule '//tensorflow/core/kernels:sparse_reduce_op' failed (Exit 1),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Debian GNU/Linux 9.12 (stretch)
- TensorFlow version: 2.1
- Python version: 3.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
I'm trying to build tensorflow from source, it keeps running for several hours then the build fails and I get the error:

```
ERROR: /home/emadboctor/tensorflow/tensorflow/core/kernels/BUILD:5398:1: C++ compilation of rule '//tensorflow/core/kernels:sparse_reduce_op' failed (Exit 1)
    In file included from external/eigen_archive/unsupported/Eigen/CXX11/Tensor:104:0,
                     from ./third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                     from ./tensorflow/core/framework/numeric_types.h:20,
                     from ./tensorflow/core/framework/allocator.h:26,
                     from ./tensorflow/core/framework/op_kernel.h:24,
                     from tensorflow/core/kernels/sparse_reduce_op.cc:20:
    external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h: In static member function 'static void std::_Function_handler<void(_ArgTypes ...), _Functor>::_M_invoke(const std::_Any_data&, _ArgTypes&& ...) [with _Functor = Eigen::internal::TensorExecutor<Expression, Eigen::ThreadPoolDevice, Vectorizable, Tiling>::run(const Expression&, const Eigen::ThreadPoolDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >; bool Vectorizable = true; Eigen::internal::TiledEvaluation Tiling = (Eigen::internal::TiledEvaluation)0u]::<lambda(Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, (Eigen::internal::TiledEvaluation)0u>::StorageIndex, Eigen::internal::TensorExecutor<const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<std::complex<float>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorReductionOp<Eigen::internal::SumReducer<std::complex<float> >, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorMap<Eigen::Tensor<std::complex<float>, 1, 1, long int>, 0, Eigen::MakePointer>, Eigen::MakePointer> >, Eigen::ThreadPoolDevice, true, (Eigen::internal::TiledEvaluation)0u>::StorageIndex)>; _ArgTypes = {long int, long int}]':
    external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorReduction.h:806:9: internal compiler error: in emit_move_insn, at expr.c:3547
             values[i] = internal::InnerMostDimReducer<Self, Op>::reduce(*this, firstIndex + i * num_values_to_reduce,
             ^~~~~~
    Please submit a full bug report,
    with preprocessed source if appropriate.
    See <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.
    Target //tensorflow/tools/pip_package:build_pip_package failed to build
    Use --verbose_failures to see the command lines of failed build steps.
    ERROR: /home/emadboctor/tensorflow/tensorflow/tools/pip_package/BUILD:65:1 C++ compilation of rule '//tensorflow/core/kernels:sparse_reduce_op' failed (Exit 1)
    INFO: Elapsed time: 8153.055s, Critical Path: 926.03s
    INFO: 8566 processes: 8566 local.
    FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
apt update && apt install -y \
        build-essential \
        libc-ares-dev \
        libjpeg-dev \
        openjdk-8-jdk \
        gcc \
        g++ \
        python3-pip \
```

    pip3 install six numpy wheel setuptools mock && \
        pip3 install keras_applications --no-deps && \
        pip3 install keras_preprocessing --no-deps

    sudo apt-get update
    sudo apt-get -y upgrade
    wget https://dl.google.com/go/go1.13.3.linux-amd64.tar.gz
    sudo tar -xvf go1.13.3.linux-amd64.tar.gz
    sudo mv go /usr/local
    export GOROOT=/usr/local/go
    export GOPATH=$HOME/Projects/Proj1
    export PATH=$GOPATH/bin:$GOROOT/bin:$PATH
    go get github.com/bazelbuild/bazelisk
    mkdir -p ~/bin
    ln -s $(go env GOPATH)/bin/bazelisk ~/bin/bazel
    export PATH=$HOME/bin:$PATH
    git clone https://github.com/tensorflow/tensorflow
    cd tensorflow
    ./configure

Configuration:

    emadboctor@reg-build-vm:~/tensorflow$ ./configure 
    You have bazel 3.0.0 installed.
    Please specify the location of python. [Default is /usr/bin/python3]: /usr/bin/python3
    
    
    Found possible Python library paths:
      /usr/local/lib/python3.5/dist-packages
      /usr/lib/python3/dist-packages
    Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]
    /usr/local/lib/python3.5/dist-packages
    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
    No OpenCL SYCL support will be enabled for TensorFlow.
    
    Do you wish to build TensorFlow with ROCm support? [y/N]: n
    No ROCm support will be enabled for TensorFlow.
    
    Do you wish to build TensorFlow with CUDA support? [y/N]: n
    No CUDA support will be enabled for TensorFlow.
    
    Do you wish to download a fresh release of clang? (Experimental) [y/N]: y
    Clang will be downloaded and used to compile tensorflow.
    
    Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -config=mkl
    
    
    Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
    Not configuring the WORKSPACE for Android builds.
    
    Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
    	--config=mkl         	# Build with MKL support.
    	--config=monolithic  	# Config for mostly static monolithic build.
    	--config=ngraph      	# Build with Intel nGraph support.
    	--config=numa        	# Build with NUMA support.
    	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
    	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
    Preconfigured Bazel build configs to DISABLE default on features:
    	--config=noaws       	# Disable AWS S3 filesystem support.
    	--config=nogcp       	# Disable GCP support.
    	--config=nohdfs      	# Disable HDFS support.
    	--config=nonccl      	# Disable NVIDIA NCCL support.
    Configuration finished
    emadboctor@reg-build-vm:~/tensorflow$

Then:

    bazel build -c opt \
                --define=grpc_no_ares=true  \
                --linkopt=""-lrt"" \
                --linkopt=""-lm"" \
                --host_linkopt=""-lrt"" \
                --host_linkopt=""-lm"" \
                --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}"" \
                --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both \
                --copt=-w \
                --jobs=26 \
                //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39320,Breakpoints do not stop inside tf.function,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7
- CUDA/cuDNN version: CUDA 10.1 / cuDNN 7
- GPU model and memory: GeForce RTX 2060 6GB GDDR6


**Describe the current behavior**

If in the code from below you set a breakpoint in the line `print('Dummy function')` it will not stop.
```
import tensorflow as tf

def read_tfrecord(x):
    print('Dummy function')
    return x

dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
print(dataset)
dataset = dataset.map(lambda x: read_tfrecord(x))
```

**Describe the expected behavior**

The code execution should stop at that line and you should be able to debug that function.


**Standalone code to reproduce the issue**

```
import tensorflow as tf

def read_tfrecord(x):
    print('Dummy function')
    return x

dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
print(dataset)
dataset = dataset.map(lambda x: read_tfrecord(x))
```


**Other info / logs** Include any logs or source code that would be helpful to

First I though it was a problem with the debugger that I was using so I created an issue there (https://github.com/jupyterlab/debugger/issues/435).

Then they said it might be to the underlying debugger that it was using (ptvsd / debugpy) since the problem was also present in Visual Studio Code, and both debuggers use the same, so I created an issue there as well (https://github.com/microsoft/debugpy/issues/228).

And they now point out that it might be related to how tensorflow is build, so maybe the problem comes from Tensorflow and the way it creates the threads, read comment: https://github.com/microsoft/debugpy/issues/228#issuecomment-624908204

"
39317,tfjs-node does not work with Node14,"This is due to the new N-API-Version '6' which is missing from package.json.

The library builds apparently without issues (as N-API '5'), but the install.js script fails to symlink or copy tensorflow.dll (because folder for N-API '6' does not exist).
"
39316,"bug report:All feature_columns must be _FeatureColumn instances. Given: SequenceNumericColumn(key='volume', shape=(1,), default_value=0.0, dtype=tf.float32, normalizer_fn=None)","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install 
- TensorFlow version (use command below): tensorflow-gpu 2.2.0 and 2.1.0
- Python version:python3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:8g

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
my code is:
# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow import feature_column
from tensorflow.keras.experimental import SequenceFeatures

volume = feature_column.sequence_numeric_column('volume')
lstm_columns = [volume]
features = tf.io.parse_example(..., features=feature_column.make_parse_example_spec(lstm_columns))

sequence_feature_layer = SequenceFeatures(lstm_columns) #返回特征列的长度和 tensor
sequence_input, sequence_length = sequence_feature_layer(features)



**Describe the current behavior**

Traceback (most recent call last):
  File ""/home/zy/PycharmProjects/keras-tcn-master/brain_LD.py"", line 104, in <module>
    features = tf.io.parse_example(..., features=feature_column.make_parse_example_spec(lstm_columns))
  File ""/home/zy/anaconda3/envs/tf1/lib/python3.6/site-packages/tensorflow_core/python/feature_column/feature_column.py"", line 806, in make_parse_example_spec
    'Given: {}'.format(column))
ValueError: All feature_columns must be _FeatureColumn instances. Given: SequenceNumericColumn(key='volume', shape=(1,), default_value=0.0, dtype=tf.float32, normalizer_fn=None)

 it looks like a bug ? can you fix it?




**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39315,Failed to convert .pb file to rflite,"**System information**
Ubuntu 16.04
TensorFlow 2.1.0 installed from anaconda

**Command used to run the converter or code if you’re using the Python API**

```
    import tensorflow as tf

    import numpy as np
    import torch

    def load_pb(path_to_pb):
        with tf.compat.v1.gfile.GFile(path_to_pb, 'rb') as f:
            graph_def = tf.compat.v1.GraphDef()
            graph_def.ParseFromString(f.read())
        with tf.compat.v1.Graph().as_default() as graph:
            tf.compat.v1.import_graph_def(graph_def, name='')
        return graph, graph_def

    tf_graph, graph_def = load_pb('./model120.pb')
    sess = tf.compat.v1.Session(graph=tf_graph)

    # Show tensor names in graph
    for op in tf_graph.get_operations():
        print(op.values())

    output_tensor = tf_graph.get_tensor_by_name('test_output:0')
    input_tensor = tf_graph.get_tensor_by_name('test_input:0')

    dummy_input = np.zeros(shape=(1, 3, 672, 672)).astype(np.float32)

    output = sess.run(output_tensor, feed_dict={input_tensor: dummy_input})

    print('>==============================<')
    print(output.shape)
    print(output)
    print('<==============================>')

    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph('/home/marat/OCR/yolo3/model120.pb', ['test_input'], ['test_output'])
    tflite_model = converter.convert()
    open(""model120.tflite"", ""wb"").write(tflite_model)
```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""/home/marat/OCR/yolo3/torch2tflite.py"", line 93, in <module>
    tflite_model = converter.convert()
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/python/lite.py"", line 1007, in convert
    **converter_kwargs)
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 457, in toco_convert_impl
    enable_mlir_converter=enable_mlir_converter)
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/python/convert.py"", line 203, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-05-08 16:44:59.629678: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/marat/anaconda3/lib
2020-05-08 16:44:59.629746: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/home/marat/anaconda3/lib
2020-05-08 16:44:59.629754: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2020-05-08 16:45:00.211158: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-08 16:45:00.216250: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3407935000 Hz
2020-05-08 16:45:00.216513: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55eb58a38390 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-08 16:45:00.216527: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-08 16:45:00.218143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-08 16:45:00.220636: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-05-08 16:45:00.220672: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: saturn
2020-05-08 16:45:00.220678: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: saturn
2020-05-08 16:45:00.220727: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 440.64.0
2020-05-08 16:45:00.220762: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 440.64.0
2020-05-08 16:45:00.220768: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 440.64.0
2020-05-08 16:45:00.240198: I tensorflow/lite/toco/import_tensorflow.cc:659] Converting unsupported operation: If
2020-05-08 16:45:00.240265: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted

Current thread 0x00007fc41d0f2700 (most recent call first):
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 56 in execute
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/absl/app.py"", line 250 in _run_main
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/absl/app.py"", line 299 in run
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/python/platform/app.py"", line 40 in run
  File ""/home/marat/anaconda3/envs/cexp/lib/python3.7/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 93 in main
  File ""/home/marat/anaconda3/envs/cexp/bin/toco_from_protos"", line 8 in <module>
Aborted (core dumped)
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/open?id=12CN_l_VlAFFCt28lWvF9xqVfRAxY1M8b
```

**Failure details**

No information at all about exact converter failure reason

```
2020-05-08 16:54:52.542355: F tensorflow/lite/toco/import_tensorflow.cc:114] Check failed: attr.value_case() == AttrValue::kType (1 vs. 6)
Fatal Python error: Aborted
```
Its seems thath problem with this code com from pytorch `F.interpolate `"
39314,[Regression] batch_begin/end callbacks no longer get batch number and size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- TensorFlow installed from (source or binary): Bianry
- TensorFlow version (use command below): 2.2.0

**Describe the current behavior**

The callbacks are documented at https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_train_batch_begin as `logs: dict. Has keys batch and size representing the current batch number and the size of the batch.`

The same used to be the case for `on_train_batch_end` in 2.1 but this seems to have been dropped for unknown reason. In 2.1 the `on_train_batch_begin` had a `logs[`size`]=1` which was plain wrong but the value in `on_train_batch_end` was correct so that could be used.

However in 2.2 the `size` log has been removed from `on_train_batch_end` causing existing code to break.

Furthermore contrary to the documentation for neither callback the size is included

**Describe the expected behavior**

The size should be correctly reported to both callbacks for them to use.

**Standalone code to reproduce the issue**

Instead of coming up with a MWE I quote the code directly:

https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/training.py#L847 does not even pass a `logs` parameter so that is clearly wrong

https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/training.py#L855 uses the result of the `train_function` obtained at https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/keras/engine/training.py#L848 which does not contain batch number or size either."
39313,Use Eager execution or decorate this function with @tf.function.,"
hi,dear
When I define a class,then use map, got the error:
```
class Func(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super(Func, self).__init__(**kwargs)

    def build(self, input_shape):
        super(Func, self).build(input_shape)

    def call(self, x):
        return x    

x=tf.constant([1,2,3])
with tf.Session() as sess:
    inputs=list(map(Func(),x))
    print(sess.run(inputs))
```
I have tried the 
`tf.compat.v1.disable_eager_execution()`
but no use
Could you pls help me ?
thx


**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., win10 64bit):
- TensorFlow installed from (pip):
- TensorFlow version (use command below):1.15
- Python version:3.6.8
- CUDA/cuDNN version:No
- GPU model and memory:No


"
42802,How to translate the content under tensorflow.org/resources/learn-ml?,"Some resources under tensorflow.org/resources/learn-ml like this one https://developers.google.com/machine-learning/crash-course are useful and are already translated to multiple languages, therefore, I want to translate them into Arabic. However, their source code isn't in this repository and I couldn't find it in other repositories. 

So, could explain the procedure to translate them? 

Thanks."
39312,Saving model in Single Writer Multiple Reader (SWMR) mode,"Hello, Can I set SWMR mode when I'm saving my model to h5 file? I need a concurrent reading of an HDF5 file while it is being written from another training process. It's useful in multi-agent reinforcement learning. Thanks.
"
39309,Bazel build fails on MacOS Catalina - Unable to find numpy package despite PYTHON_LIB_PATH set to the directory containing the numpy package,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: RELEASE.md => Release 2.0.1 building from source
- Python version: 3.6.6
- Installed using virtualenv? pip? conda?: pyenv
- Bazel version (if compiling from source): 3.0.0
- GCC/Compiler version (if compiling from source): 4.2.1 (Apple clang version 11.0.3 (clang-1103.0.32.59))
- CUDA/cuDNN version: N/A
- GPU model and memory: AMD Radeon R9 M295X 4 GB graphics


**Describe the problem**
Trying to build from source as documented [here](https://www.tensorflow.org/install/source) fails for me on MacOS Cataline 10.15.4...the bazel build process cannot find numpy package even though a listing of the PYTHON_LIB_PATH shows the numpy package is present:

``` bash
$ ls /Users/simon/Library/Python/3.6/lib/python/site-packages

Keras_Applications-1.0.8.dist-info/  easy_install.py                      keras_applications/                  libpasteurize/                       numpy/                               pkg_resources/                       six-1.14.0.dist-info/                wheel-0.34.2.dist-info/
Keras_Preprocessing-1.1.0.dist-info/ future/                              keras_preprocessing/                 mock/                                numpy-1.18.4.dist-info/              setuptools/                          six.py
__pycache__/                         future-0.18.2.dist-info/             libfuturize/                         mock-4.0.2.dist-info/                past/                                setuptools-46.1.3.dist-info/         wheel/
```
I am using Python 3.6.6 via pyenv.

Salient output from bazel build process included below. Full error log from build process included at the the end of report.

``` bash
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
	File ""/Users/simon/src/libs/third-party/tensorflow/third_party/py/python_configure.bzl"", line 213
		_get_numpy_include(<2 more arguments>)
	File ""/Users/simon/src/libs/third-party/tensorflow/third_party/py/python_configure.bzl"", line 187, in _get_numpy_include
		execute(repository_ctx, <3 more arguments>)
	File ""/Users/simon/src/libs/third-party/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
Is numpy installed?

PYTHON_BIN_PATH=/Users/simon/.pyenv/versions/3.6.6/bin/python3 --action_env

PYTHON_LIB_PATH=/Users/simon/Library/Python/3.6/lib/python/site-packages
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
$ pyenv version system
3.6.6 (set by /Users/simon/.pyenv/version)

$ pip install -U --user pip six numpy wheel setuptools mock 'future>=0.17.1'
$ pip install -U --user keras_applications --no-deps
$ pip install -U --user keras_preprocessing --no-deps

# required python packages exist as shown in stdout log from pip install
Requirement already up-to-date: pip in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages (20.1)
Requirement already up-to-date: six in /Users/simon/Library/Python/3.6/lib/python/site-packages (1.14.0)
Requirement already up-to-date: numpy in /Users/simon/Library/Python/3.6/lib/python/site-packages (1.18.4)
Requirement already up-to-date: wheel in /Users/simon/Library/Python/3.6/lib/python/site-packages (0.34.2)
Requirement already up-to-date: setuptools in /Users/simon/Library/Python/3.6/lib/python/site-packages (46.1.3)
Requirement already up-to-date: mock in /Users/simon/Library/Python/3.6/lib/python/site-packages (4.0.2)
Requirement already up-to-date: future>=0.17.1 in /Users/simon/Library/Python/3.6/lib/python/site-packages (0.18.2)
Requirement already up-to-date: keras_applications in /Users/simon/Library/Python/3.6/lib/python/site-packages (1.0.8)
Requirement already up-to-date: keras_preprocessing in /Users/simon/Library/Python/3.6/lib/python/site-packages (1.1.0)

$ ./configure
Please specify the location of python. [Default is /Users/simon/.pyenv/versions/3.6.6/bin/python3]:  /Users/simon/.pyenv/versions/3.6.6/bin/python3 
Please input the desired Python library path to use.  Default is [/Users/simon/.pyenv/versions/3.6.6/lib/python3.6/site-packages] /Users/simon/Library/Python/3.6/lib/python/site-packages
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
Do you wish to build TensorFlow with ROCm support? [y/N]: n
Do you wish to build TensorFlow with CUDA support? [y/N]: n
Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -march=core2 -Wno-sign-compare
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Do you wish to build TensorFlow with iOS support? [y/N]: n

$  bazel build --config=opt --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/lib_package:libtensorflow

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

When run the above build steps, error message displayed because numpy package was not found.

``` bash
bazel build --config=opt --config=monolithic --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow/tools/lib_package:libtensorflow
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=298
INFO: Reading rc options for 'build' from /Users/simon/src/libs/third-party/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/simon/src/libs/third-party/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /Users/simon/src/libs/third-party/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Users/simon/.pyenv/versions/3.6.6/bin/python3 --action_env PYTHON_LIB_PATH=/Users/simon/Library/Python/3.6/lib/python/site-packages --python_path=/Users/simon/.pyenv/versions/3.6.6/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:opt in file /Users/simon/src/libs/third-party/tensorflow/.tf_configure.bazelrc: --copt=-march=core2 --copt=-Wno-sign-compare --host_copt=-march=native --define with_default_optimizations=true
INFO: Found applicable config definition build:monolithic in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --define framework_shared_object=false
INFO: Found applicable config definition build:noaws in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --define=no_aws_support=true
INFO: Found applicable config definition build:nogcp in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --define=no_gcp_support=true
INFO: Found applicable config definition build:nohdfs in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --define=no_hdfs_support=true
INFO: Found applicable config definition build:nonccl in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --define=no_nccl_support=true
INFO: Found applicable config definition build:macos in file /Users/simon/src/libs/third-party/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
INFO: Call stack for the definition of repository 'local_config_python' which is a python_configure (rule definition at /Users/simon/src/libs/third-party/tensorflow/third_party/py/python_configure.bzl:294:20):
 - <builtin>
 - /Users/simon/src/libs/third-party/tensorflow/tensorflow/workspace.bzl:104:5
 - /Users/simon/src/libs/third-party/tensorflow/WORKSPACE:19:1
INFO: Call stack for the definition of repository 'local_execution_config_python' which is a local_python_configure (rule definition at /Users/simon/src/libs/third-party/tensorflow/third_party/py/python_configure.bzl:275:26):
 - <builtin>
 - /Users/simon/src/libs/third-party/tensorflow/third_party/toolchains/remote_config/rbe_config.bzl:158:5
 - /Users/simon/src/libs/third-party/tensorflow/third_party/toolchains/remote_config/configs.bzl:6:5
 - /Users/simon/src/libs/third-party/tensorflow/tensorflow/workspace.bzl:93:5
 - /Users/simon/src/libs/third-party/tensorflow/WORKSPACE:19:1
ERROR: An error occurred during the fetch of repository 'local_execution_config_python':
   Traceback (most recent call last):
	File ""/Users/simon/src/libs/third-party/tensorflow/third_party/py/python_configure.bzl"", line 213
		_get_numpy_include(<2 more arguments>)
	File ""/Users/simon/src/libs/third-party/tensorflow/third_party/py/python_configure.bzl"", line 187, in _get_numpy_include
		execute(repository_ctx, <3 more arguments>)
	File ""/Users/simon/src/libs/third-party/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
		fail(<1 more arguments>)
Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ModuleNotFoundError: No module named 'numpy'
Is numpy installed?
INFO: Call stack for the definition of repository 'com_google_protobuf' which is a tf_http_archive (rule definition at /Users/simon/src/libs/third-party/tensorflow/third_party/repo.bzl:134:19):
 - <builtin>
 - /Users/simon/src/libs/third-party/tensorflow/tensorflow/workspace.bzl:559:5
 - /Users/simon/src/libs/third-party/tensorflow/WORKSPACE:19:1
Internal error thrown during build. Printing stack trace: java.lang.NullPointerException: //tensorflow/tools/lib_package:libtensorflow BuildConfigurationValue.Key[e4e94f4998e8c26abbc5eb2bf9d33e94aa6f63e0a9af8b8373f7ec758ef26105] false
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)
	at com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)
	at com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)
	at com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)
	at com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)
	at com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)
	at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)
	at com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)
	at com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)
	at com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)
	at io.grpc.Context$1.run(Context.java:595)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)

INFO: Elapsed time: 3.854s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (16 packages loaded, 36 targets configured)
    currently loading: tensorflow ... (3 packages)
    Fetching @local_config_cc_toolchains; fetching
Internal error thrown during build. Printing stack trace: java.lang.NullPointerException: //tensorflow/tools/lib_package:libtensorflow BuildConfigurationValue.Key[e4e94f4998e8c26abbc5eb2bf9d33e94aa6f63e0a9af8b8373f7ec758ef26105] false
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)
	at com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)
	at com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)
	at com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)
	at com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)
	at com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)
	at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)
	at com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)
	at com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)
	at com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)
	at io.grpc.Context$1.run(Context.java:595)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.base/java.lang.Thread.run(Unknown Source)
java.lang.NullPointerException: //tensorflow/tools/lib_package:libtensorflow BuildConfigurationValue.Key[e4e94f4998e8c26abbc5eb2bf9d33e94aa6f63e0a9af8b8373f7ec758ef26105] false
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:895)
	at com.google.devtools.build.skyframe.DelegatingWalkableGraph.getDirectDeps(DelegatingWalkableGraph.java:121)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.assertSaneAnalysisError(SkyframeBuildView.java:773)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.processErrors(SkyframeBuildView.java:616)
	at com.google.devtools.build.lib.skyframe.SkyframeBuildView.configureTargets(SkyframeBuildView.java:454)
	at com.google.devtools.build.lib.analysis.BuildView.update(BuildView.java:404)
	at com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.runAnalysisPhase(AnalysisPhaseRunner.java:213)
	at com.google.devtools.build.lib.buildtool.AnalysisPhaseRunner.execute(AnalysisPhaseRunner.java:124)
	at com.google.devtools.build.lib.buildtool.BuildTool.buildTargets(BuildTool.java:145)
	at com.google.devtools.build.lib.buildtool.BuildTool.processRequest(BuildTool.java:290)
	at com.google.devtools.build.lib.runtime.commands.BuildCommand.exec(BuildCommand.java:95)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.execExclusively(BlazeCommandDispatcher.java:564)
	at com.google.devtools.build.lib.runtime.BlazeCommandDispatcher.exec(BlazeCommandDispatcher.java:208)
	at com.google.devtools.build.lib.server.GrpcServerImpl.executeCommand(GrpcServerImpl.java:603)
	at com.google.devtools.build.lib.server.GrpcServerImpl.lambda$run$2(GrpcServerImpl.java:659)
	at io.grpc.Context$1.run(Context.java:595)
FAILED: Build did NOT complete successfully (16 packages loaded, 36 targets configured)
    currently loading: tensorflow ... (3 packages)
    Fetching @local_config_cc_toolchains; fetching
```"
39308,Extension of the Data API `take` method to accept percent values,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
The Tensorflow Data API defines the [`take` method](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) which makes it possible to process a certain number of dataset elements. Currently it seems to be only possible to specify an absolute number of elements to be consumed (example: 32). It would be a nice addition if the take method could also receive a percentage value (0.0 - 1.0) that permits the consumption of a certain percentage of the dataset (exmaple: 0.2 for 20% of the dataset). This would be especially helpful for experiments that require an incremental increase in the amount of training data.

**Will this change the current api? How?**
If the signature of the take method is extended to accept float as well as integer values, a fundamental change to the API should not be necessary.

**Who will benefit with this feature?**
This would be especially helpful for experiments that require an incremental increase in the amount of training data (for example in active learning scenarios)."
39307,Golang Tensorflow v2.2.0 fails install,"**System information**
- OS Platform and Distribution (Macos catalina 10.15.4):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Following https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.mdhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md

`go generate github.com/tensorflow/tensorflow/tensorflow/go/op`
fails with `failed to process ""api_def_VarHandleOp.pbtxt"": Attribute allowed_devices not defined in base api for VarHandleOp
exit status 1
go/src/github.com/tensorflow/tensorflow/tensorflow/go/op/generate.go:18: running ""go"": exit status 1`

`go test github.com/tensorflow/tensorflow/tensorflow/go` passes 

building or running go code using tensorflow then proceeds to fail with the following output 
`go: finding github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto latest
go: finding github.com/tensorflow/tensorflow/tensorflow/go/core latest
go: finding github.com/tensorflow/tensorflow/tensorflow/go latest
go: finding github.com/tensorflow/tensorflow/tensorflow latest
build command-line-arguments: cannot load github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto: cannot find module providing package github.com/tensorflow/tensorflow/tensorflow/go/core/core_protos_go_proto`

the code run is the example that prints tf library version from https://www.tensorflow.org/install/lang_go

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39306,Unable to use small_bert for tensorflow lite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina Version 10.15.3
- TensorFlow installed from (source or binary): tensorflow-2.2.0-cp37-cp37m-macosx_10_11_x86_64.whl
- TensorFlow version (or github SHA if from source): 2.2.0-dev20200507

Following the demo notebook https://www.tensorflow.org/lite/tutorials/model_maker_text_classification, I wanted to change the BertClassifierModelSpec to use a smaller bert model on tensorflow hub

```
model_spec = BertClassifierModelSpec(uri='https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2/1')

train_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'train'), model_spec=model_spec, class_labels=['pos', 'neg'])

test_data = TextClassifierDataLoader.from_folder(os.path.join(data_path, 'test'), model_spec=model_spec, is_training=False, shuffle=False)

model = text_classifier.create(train_data, model_spec=model_spec, epochs=1)
```

which led to the following error:
```
INFO:tensorflow:Retraining the models...
INFO:tensorflow:Retraining the models...
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-22-dfbd96b901f3> in <module>()
----> 1 model = text_classifier.create(train_data, model_spec=model_spec, epochs=1)

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow_hub/keras_layer.py in _get_callable(self)
    294     if self._signature not in self._func.signatures:
    295       raise ValueError(""Unknown signature %s in %s (available signatures: %s).""
--> 296                        % (self._signature, self._handle, self._func.signatures))
    297     f = self._func.signatures[self._signature]
    298     if not callable(f):

ValueError: Unknown signature default in https://tfhub.dev/google/small_bert/bert_uncased_L-2_H-128_A-2/1 (available signatures: _SignatureMap({'tokenization_info': <ConcreteFunction pruned() at 0x7F8969733860>, 'mlm': <ConcreteFunction pruned(mlm_positions, segment_ids, input_mask, input_ids) at 0x7F8969701F98>, 'tokens': <ConcreteFunction pruned(input_ids, input_mask, segment_ids) at 0x7F8969095DA0>})).
```"
39305,control_dependencies with assert_equal,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/control_dependencies

## Description of issue (what needs changing):
Do we need to mention the debug use case in https://www.tensorflow.org/api_docs/python/tf/debugging/assert_equal#returns ?
### Clear description
We declare in the note
> Note: In TensorFlow 2 with eager and/or Autograph, you should not require this method, as code executes in the expected order. Only use tf.control_dependencies when working with v1-style code or in a graph context such as inside Dataset.map.
But there is any direct reference to the `assert_equal` use case

For example, why should someone use this method? How is it useful?
Take a look at the issue [here](https://github.com/tensorflow/addons/issues/1794)"
39304,What happen to the tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb ?,"
"
39303,CUBLAS_STATUS_NOT_INITIALIZED error when running without GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1.243

**Describe the current behavior**

When running the stock example on a system with CUDA but without CUDA visible devices (e.g. no GPUs allowed in job, GPUs busy, disabled cgroups) Tensorflow throws an error `failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED`

**Describe the expected behavior**

Fallback to CPU execution as done when no GPU/CUDA was present at all

**Standalone code to reproduce the issue**
```
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax'),
])

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
```

**Other info / logs**

Again this happens on a node that has GPUs and CUDA properly setup but inside a job which makes the GPUS unavailable. This can be verified by `tf.config.list_physical_devices('GPU')` which comes up empty.

So the issue is the TF tries to use cublas without checking for GPUs first.

This worked in in TF 2.1 so this is a regression!
"
39302,There is not `python/ops/gen_spectral_ops.py` in github,"When I have to learn how to compute mfcc, there is a question in compute fft.

The question is not finding 'python/ops/gen_spectral_ops.py' in github, how could I read it? 

However,  there is a 'python/ops/gen_spectral_ops.py' in tf1.14.0 in my computer, is the same file?"
39301,[RNN] tflite converted but unable to use on Android,"**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.
[Colab Notebook]()

**Failure details**
I am using tensorflow.compat.v1 TFLiteConverter because I want to enable GPU delegate and the new TFLiteConverter doesn't provide any options to set your input shape to make the tensor static-sized. The model is converted successfully, but when run on Android, I get the following error:

```
E/AndroidRuntime: FATAL EXCEPTION: Thread-5
    Process: com.example.android.testreadfile, PID: 20335
    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Encountered unresolved custom op: TensorListReserve.
    Node number 3 (TensorListReserve) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegates(NativeInterpreterWrapper.java:342)
        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:82)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:63)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:237)
        at com.example.android.testreadfile.ClassificationClient.loadModel(ClassificationClient.java:164)
        at com.example.android.testreadfile.ClassificationClient.load(ClassificationClient.java:143)
        at com.example.android.testreadfile.MainActivity$2$1.run(MainActivity.java:547)
        at java.lang.Thread.run(Thread.java:764)
```

I am trying to build a text classifier using RNN network architecture and I am currently using this example code as a template to load tflite model and classify text. [Android Text Classifier](https://github.com/tensorflow/examples/tree/master/lite/examples/text_classification/android)

After that I follow the tutorial from [https://www.tensorflow.org/lite/performance/gpu](https://www.tensorflow.org/lite/performance/gpu) to enable GPU delegate. 


P/s: is there any chance that the new TfLiteConverterV2 will allow us to set the input shape when converting?"
39300,"TFLite-Micro Operation Support for ""Dropout""","@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
N/A
- TensorFlow installed from (source or binary):
N/A
- Tensorflow version (commit SHA if source):
N/A
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):
N/A

**Describe the problem**
Hi, may I know the plan to support ""Dropout"" operator in TFLite-Micro?
Thanks for your help!

**Please provide the exact sequence of commands/steps when you ran into the problem**

"
39299,libtensorflow v2.2.0 from source reports as v2.1.0,"**System information**
- OS Platform and Distribution (macos Catalina 10.15.4 & Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: 2.2.0
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): For mac os Apple clang version 11.0.0
- CUDA/cuDNN version: Not used
- GPU model and memory: Not used



**Describe the problem**
Following this guide: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/lib_package/README.md
to build libtensorflow after 2.2.0 release 

And running the hello world version example, still results in a version 2.1.0 print. 

**Any other info / logs**
I have tried on macos from the tag v2.2.0 and on ubuntu from both master branch and tag v2.2.0 all of them reports as being v2.1.0 when running the example code.
"
39298,Custom optimizer that behaves differently depending on the shapes of weights,"I am attempting to create a custom TensorFlow optimizer (`tf.keras.optimizers.Optimizer`) which treats weights of different shapes differently.

For example, please consider a simple convolutional neural network with the following shape of weights (and biases):

```
(3, 3, 3, 16)
(16,)
(3, 3, 16, 16)
(16,)
(2704, 64)
(64,)
(64, 10)
(10,)
```
At the beginning of the method _resource_apply_dense(self, grad, var), I would like to transform `var` of different shapes all into 2 dimensional, and then perform some other operations.

The following is the simplified logic of the desired transformation behavior:

```python
def custom_train_step(var):
    if tf.rank(var) == 1:
        # Case1
        return tf.expand_dims(input=var, axis=0)
    elif tf.rank(var) == 2:
        # Case2
        return tf.transpose(a=var)
    elif tf.rank(var) == 4:
        # Case3
        var = tf.transpose(a=var, perm=[3,0,1,2])
        return tf.reshape(var, shape=[var.shape[0], -1])
    else:
        # Case4 omitted
        pass
```
However, this will not work when `ndim(var)<4`, because it seems that when TensorFlow constructs its computation graph, all 4 branches are traced including Case3. Consequently, 1d and 2d `var`, during tracing, will also be passed to `tf.transpose(a=var, perm=(3,0,1,2))`, which will result in an error:

`ValueError: Dimension must be 1 but is 4 for '{{node transpose}} = Transpose[T=DT_FLOAT, Tperm=DT_INT32](transpose/ReadVariableOp, Const)' with input shapes: [16], [4].`

(The error occurred when var is a bias tensor of shape (16,))

I've tried directly writing the conditional statements using `tf.cond`, `tf.case` and `tf.switch_case`, but the error stayed. I understand this is perhaps because that `custom_train_step(var)` is polymorphic which made it necessary for retracing, but I can't think of a way to avoid such behavior by improving the code. (Please note again that I probably can not write 4 branches in separate methods and decorate each of them with `tf.function`, because this is supposed to be called inside a `tf.keras` training loop. Please correct me if I am wrong.)

I would like to know if there is a workaround to achieve what I described above, or is this type of behavior not yet supported by Tensorflow?

Any help and suggestions would be appreciated. Thanks!"
39296,"InvalidArgumentError: Unable to find the relevant tensor remote_handle: Op ID: 3940, Output num: 0","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39294,resolve tensorflow issues ,"https://github.com/geet2019/rgn
sir I wanted to implement this code. but his instruction is to have python 2.7 and TensorFlow in between 1.4 and 1.11
doubt :
Can I able to execute in Python 3.7 and in TensorFlow 2.1?
whether I am able to execute all functions in TensorFlow <1.11 in TensorFlow 2.1. If not, is there any possible way to do so?
As you know ""https://github.com/geet2019/rgn "" this a GitHub project ? is there any way to execute it in GitHub desktop itself. If so, can you please provide me with help as early as possible.

Looking forward to a quick response from you that helps me a lot.
thank you so much for your valuable support for my work.
"
39293,How to convert my custom model(.pb) with uint8 to float16?,"I trained my custom model using [Tensorflow Object Dection API](https://github.com/tensorflow/models/tree/master/research/object_detection).

I used [ssd_resnet50_v1_fpn](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync.config).

To accelerate the process of inference, I'm planning to use TensorRT.

So, I converted my model to onnx (.pb -> .onnx), using [tf2onnx](https://github.com/onnx/tensorflow-onnx).

However, TensorRT throws an error, ```""Unsupported onnx data type: uint8 (2)""``` 

Could you help me with converting my graph?"
39292,updation issue,"https://github.com/geet2019/rgn
sir I wanted to implement this code. but his instruction is to have python 2.7 and TensorFlow in between 1.4 and 1.11
doubt :
Can I able to execute in Python 3.7 and in TensorFlow 2.1?
whether I am able to execute all functions in TensorFlow <1.11 in TensorFlow 2.1. If not, is there any possible way to do so?
As you know ""https://github.com/geet2019/rgn "" this a GitHub project ? is there any way to execute it in GitHub desktop itself. If so, can you please provide me with help as early as possible.

Looking forward to a quick response from you that helps me a lot.
thank you so much for your valuable support for my work.
"
39291,tensorflow updation issues ,"https://github.com/geet2019/rgn 
 sir I wanted tto implement this code . but his instrcution is to have python 2.7 and tenorflow in between 1.4 and 1.11 
doubt :
Can I able to execute in python 3.7 and in tensorflow 2.1 ? 
"
39290,TensorFlow Lite bazel ios build fails,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 10.15.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2(latest - cloned yesterday)
- Python version: 2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.0.0
- GCC/Compiler version (if compiling from source): clang 11.0.3
- CUDA/cuDNN version:
- GPU model and memory:

I've made a code that uses `tensorflowlite_c` and `libtensorflowlite_gpu_delegate.so` in android. And it's build was successful and works well in Android devices.
So I wanted to build the same libraries for ios.
But building `tensorflowlite_c` fails when I'm trying to build for ios.

I've installed the latest XCode from the AppStore, and commandline tools works fine.


### configure
```
➜  tensorflow git:(master) ./configure                                          
You have bazel 3.0.0 installed.
Please specify the location of python. [Default is /Library/Frameworks/Python.framework/Versions/3.7/bin/python3]: 


Found possible Python library paths:
  /Applications/root_v6.19.02/lib
  /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages
Please input the desired Python library path to use.  Default is [/Applications/root_v6.19.02/lib]
/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: -D_GLIBCXX_USE_CXX11_ABI=0 -O3


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Do you wish to build TensorFlow with iOS support? [y/N]: y
iOS support will be enabled for TensorFlow.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```

### build
```
➜  tensorflow git:(master) bazel build -c opt --config=ios //tensorflow/lite/c:tensorflowlite_c
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=210
INFO: Reading rc options for 'build' from /Users/yonggyulee/tf/tf6/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /Users/yonggyulee/tf/tf6/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /Users/yonggyulee/tf/tf6/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 --action_env PYTHON_LIB_PATH=/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages --python_path=/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 --config=xla --action_env TF_CONFIGURE_IOS=1
INFO: Found applicable config definition build:v2 in file /Users/yonggyulee/tf/tf6/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /Users/yonggyulee/tf/tf6/tensorflow/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:ios in file /Users/yonggyulee/tf/tf6/tensorflow/.bazelrc: --apple_platform_type=ios --apple_bitcode=embedded --copt=-fembed-bitcode --copt=-Wno-c++11-narrowing
INFO: Found applicable config definition build:macos in file /Users/yonggyulee/tf/tf6/tensorflow/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14
INFO: Analyzed target //tensorflow/lite/c:tensorflowlite_c (64 packages loaded, 1589 targets configured).
INFO: Found 1 target...
INFO: Deleting stale sandbox base /private/var/tmp/_bazel_yonggyulee/85757772b8505423f8a25bcdc8f80746/sandbox
ERROR: /private/var/tmp/_bazel_yonggyulee/85757772b8505423f8a25bcdc8f80746/external/fft2d/BUILD.bazel:27:1: C++ compilation of rule '@fft2d//:fft2d' failed (Exit 1)
clang: error: invalid version number in '-mmacosx-version-min=13.4'
Target //tensorflow/lite/c:tensorflowlite_c failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.723s, Critical Path: 0.13s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```
"
39289,tf.Module.name_scope overrides parent scopes,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: macOS 10.15.5
- Mobile device if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.8
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**
The `name_scope` of a `tf.Module` instance has a forward slash appended to the name, which causes the scope to disregard any parent scopes. Using `with tf.Module.name_scope` and `with tf.name_scope(mymod.name_scope.name):` both result in this issue. 

**Describe the expected behavior**
Each of the following should have the same outcome:
- `with tf.Module.name_scope:`
- `with tf.name_scope(tf.Module.name_scope.name):`
- `with tf.name_scope(tf.Module.name):`

**Standalone code to reproduce the issue**
```python
import tensorflow as tf


def my_test_mod_scopes():
    mymod = tf.Module(name='mymod')
    with tf.name_scope('first_scope'):
         with mymod.name_scope:
             tf.summary.scalar('scalar1', 84.9)
         with tf.name_scope(mymod.name):
             tf.summary.scalar('scalar2', 74.3)
         with tf.name_scope(mymod.name_scope.name):
             tf.summary.scalar('scalar3', 79.7)

with tf.summary.create_file_writer('./logs').as_default():
    tf.summary.experimental.set_step(0)
    my_test_mod_scopes()
```
![Screen Shot 2020-05-07 at 21 43 20](https://user-images.githubusercontent.com/31281983/81361269-e7bdbb80-90ab-11ea-84b9-4d41f84250b8.png)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39287,Using tf.nn.softmax with tensor of dtype `int321 throws exception - works fine with float32,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`
- OS Platform and Distribution: `Arch Linux`, but I'm actually using [collab](https://colab.research.google.com/) .
- TensorFlow installed from (source or binary): Using Google Collab.
- TensorFlow version (use command below): `Tensorflow 2.x`, setup using `%tensorflow_version 2.x` in the notebook
- Python version: `3.6.9`

**Describe the current behavior**

Error is thrown when a tensor is used in the `tf.nn.softmax` with `dtype` of `int32`, as seen below,
![image](https://user-images.githubusercontent.com/41635766/81356987-02b02000-90ec-11ea-9d77-de3a3509559d.png)

However, in contrast, when the `int32` are converted to a `float32`, like below, the code works, 
![image](https://user-images.githubusercontent.com/41635766/81357051-24a9a280-90ec-11ea-91c6-c4e4b3a5ecfa.png)

**Describe the expected behavior**

I should get back the array of values like I have with `float32`.

**Standalone code to reproduce the issue**

```
%tensorflow_version 2.x
import tensorflow as tf

tensor = tf.constant( [[10, 2], [1, 1]] )
print(tf.nn.softmax(tensor))
```

**Other info / logs**
```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-26-dff233aef206> in <module>()
      1 tensor = tf.constant( [[10, 2], [1, 1]] )
----> 2 print(tf.nn.softmax(tensor))

4 frames
/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

NotFoundError: Could not find valid device for node.
Node:{{node Softmax}}
All kernels registered for op Softmax :
  device='XLA_GPU'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
  device='CPU'; T in [DT_HALF]
  device='GPU'; T in [DT_DOUBLE]
  device='GPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_HALF]
  device='XLA_GPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_BFLOAT16, DT_HALF]
 [Op:Softmax]
```

## Other Info:

Another similar issue exist(ed) [here](https://github.com/tensorflow/tensorflow/issues/33685). 

## Possible Solution

`tf.nn.softmax` should automatically promote `intx` to `floatx` in the function itself, or generate an error that `intx` can't be used, where `x` is either `64` or `32` bits.

Thank you."
39284,DLL load failed: ,"raceback (most recent call last):
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Le module spécifié est introuvable.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/user/PycharmProjects/QBOImport/Clients_Analysis/Himalaya_Anlys.py"", line 9, in <module>
    import keras
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\user\PycharmProjects\QBOImport\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\user\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Le module spécifié est introuvable.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Windows 10

- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:3,7


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39280,undefined reference to `__cudaInitModule' when compiling run_hlo_module in debug mode,"Build is configured with CUDA.

When compiling run_hlo_module as follows:
 
`
bazel build --config=dbg -j 24 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/compiler/xla/tools:run_hlo_module
`

it results in the following error:

```
ERROR: /home/baarts/tensorflow/tensorflow/tensorflow/compiler/xla/tools/BUILD:96:1: Linking of rule '//tensorflow/compiler/xla/tools:replay_computation_gpu' failed (Exit 1)
bazel-out/k8-dbg/bin/external/nccl_archive/libdevice_a.a(sum_f64_reduce_scatter.cu.pic.o): In function `__nv_init_managed_rt_with_module(void**)':
/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_runtime.h:264: undefined reference to `__cudaInitModule'
bazel-out/k8-dbg/bin/external/nccl_archive/libdevice_a.a(sum_f64_reduce.cu.pic.o): In function `__nv_init_managed_rt_with_module(void**)':
/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_runtime.h:264: undefined reference to `__cudaInitModule'
bazel-out/k8-dbg/bin/external/nccl_archive/libdevice_a.a(sum_f64_all_reduce.cu.pic.o): In function `__nv_init_managed_rt_with_module(void**)':
/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_runtime.h:264: undefined reference to `__cudaInitModule'
bazel-out/k8-dbg/bin/external/nccl_archive/libdevice_a.a(sum_f32_reduce_scatter.cu.pic.o): In function `__nv_init_managed_rt_with_module(void**)':
/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_runtime.h:264: undefined reference to `__cudaInitModule'
bazel-out/k8-dbg/bin/external/nccl_archive/libdevice_a.a(sum_f32_reduce.cu.pic.o): In function `__nv_init_managed_rt_with_module(void**)':
/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_runtime.h:264: undefined reference to `__cudaInitModule'
bazel-out/k8-dbg/bin/external/nccl_archive/libdevice_a.a(sum_f32_all_reduce.cu.pic.o):/usr/local/cuda-10.2/bin/../targets/x86_64-linux/include/crt/host_runtime.h:264: more undefined references to `__cudaInitModule' follow
collect2: error: ld returned 1 exit status
Target //tensorflow/compiler/xla/tools:replay_computation_gpu failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 65.233s, Critical Path: 55.19s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```

the rule that generates libdevice_a.a includes host_runtime.h which contains the following code:

```
static char __nv_init_managed_rt_with_module(void **handle)
{
  return __cudaInitModule(handle);
}
```

Because this is  a debug compilation, this function is not removed, and pulls in a reference to __cudaInitModule. This function is missing from tensorflow/stream_executor/cuda/cudart_stub.cc

The same error is there when compiling replay_computation_gpu and interactive_graphviz"
39277,ImageAugmentation using tf.keras.preprocessing.image.ImageDataGenerator and tf.datasets: model.fit() is running infinitely,"**What I need help with / What I was wondering**
I am facing issue while running the fit() function in TensorFlow(v 2.2.0-rc4) with augmented images(using ImageDataGenerator) passed as a dataset. The fit() function is running infinitely without stopping.

**What I've tried so far**
I tried it with the default code which was shared in Tensorflow documentation. 

Please find the code snippet below:

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential, Model 
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D 
from tensorflow.keras.layers import Input, Dense

flowers = tf.keras.utils.get_file(
    'flower_photos',
    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',
    untar=True)

img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)

images, labels = next(img_gen.flow_from_directory(flowers))

print(images.dtype, images.shape)
print(labels.dtype, labels.shape)

train_data_gen = img_gen.flow_from_directory(
                    batch_size=32, 
                    directory=flowers,
                    shuffle=True,
                    target_size=(256, 256),
                    class_mode='categorical')

ds = tf.data.Dataset.from_generator(lambda: train_data_gen,
                     output_types=(tf.float32, tf.float32),
                     output_shapes=([32, 256, 256, 3],
                                    [32, 5])
                     )

ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

it = iter(ds)
batch = next(it)
print(batch)

def create_model():
  model = Sequential()
  model.add(Conv2D(32, (3, 3), activation='relu', input_shape=images[0].shape))
  model.add(Conv2D(32, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.5))
  model.add(Conv2D(64, (3, 3), activation='relu'))
  model.add(Conv2D(64, (3, 3), activation='relu'))
  model.add(MaxPooling2D(pool_size=(2, 2)))
  model.add(Dropout(0.5))
  model.add(Flatten())
  model.add(Dense(64, activation='relu'))
  model.add(Dropout(0.5))
  model.add(Dense(5, activation='softmax'))
  return model

model = create_model()
model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=[""accuracy""])
model.fit(ds, verbose=1,  batch_size= 32, epochs =1)

This last line of code - fit() is running infinitly without stopping. I had also tried passing steps_per_epoch = total_no_of_train_records/batch_size.

**It would be nice if...**
I would like you to confirm whethere this is a bug in the tensorflow datasets package and in which release will this be fixed.


**Environment information**
* System: Google colaborator
* Python version: v3.6.9
* `tensorflow version: v2.2.0-rc4
"
39276,Missing intermediate input node in TF Lite convert,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Debian 9.12 stretch
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 2.1.0


**Command used to run the converter or code if you’re using the Python API**

```
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    model_file,
    input_arrays=[input_name], 
    output_arrays=[output_name],
```


**The output from the converter invocation**

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
~/.virtualenvs/tf-lite/lib/python3.7/site-packages/tensorflow_core/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    496       try:
--> 497         results = c_api.TF_GraphImportGraphDefWithResults(
    498             graph._c_graph, serialized, options)  # pylint: disable=protected-access

InvalidArgumentError: Node 'batchnorm_13/mul_1': Unknown input node 'Add_2'
```

**Also, please include a link to the saved model or GraphDef**

[Saved Model GDRIVE link](https://drive.google.com/file/d/1fWaW4snOOeBBb4gUTqBqPPyG6riwR94Y/view?usp=sharing)

**Failure details**

In the graph, these are batch normalisation operations that cannot be removed, since they follow an Add operation.  This part of the graph is:

```

A --
    --> Add --> BatchNorm --> ...
B --               ^
                   |
                  BN params 
```

[This issue](https://github.com/tensorflow/tensorflow/issues/23627) might make me think that Batch Norm is not supported.  

However a very similar model I'm using features Add layers followed by BatchNorm, and is successfully exported.

I'm trying to figure out the source of this issue.  Is there anything I should be looking at that might help me pin down the cause?"
39275,TF Lite Benchmark does not compile on aarch64(RK3399),"**System information**
- OS Platform and Distribution: Debian 9.12
- Mobile device: development board of RK3399 (Based on Big.Little architecture, it integrates dual-core Cortex-A72and quad-core Cortex-A53 with separate NEON coprocessor)
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.5.3
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 6.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: Mali T860 MP4



**Describe the problem**

`bazel build -c opt tensorflow/lite/tools/benchmark:benchmark_model` cannot successfully build the binary of benchmark.


**Any other info / logs**

ERROR: /media/linaro/9aced46d-1704-4d54-8f78-7bb9c3cc8def/tmp/_bazel_linaro/d6df223f3277e5d3a224cafaa5ad64da/external/XNNPACK/BUILD.bazel:1910:1: C++ compilation of rule '@XNNPACK//:asm_ukernels' failed (Exit 1)
external/XNNPACK/src/f32-igemm/gen/6x8-minmax-aarch64-neonfma-ios.S:1:0: error: unknown value 'armv8.2-a+fp16' for -march
 // Auto-generated file. Do not edit!

Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 152.931s, Critical Path: 58.65s
INFO: 81 processes: 81 local.
FAILED: Build did NOT complete successfully

Please let me know if anything else is needed to compile the benchmark on aarch64."
39274,FailedPreconditionError on tf.estimator.BaselineClassifier(),"tf: 2.2.0-rc4

- Same code works fun on **tf.compat.v1.estimator.BaselineClassifier()**.
- But raise ""FailedPreconditionError: GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element."" on **tf.estimator.BaselineClassifier()**.
- Input dataset is ""tensorflow.python.data.ops.dataset_ops.BatchDataset""

`baseline_estimator = tf.estimator.BaselineClassifier(n_classes = 2)
baseline_estimator.train(input_fn = lambda : make_dataset(train_df, y_train))`  
[colab-link](https://colab.research.google.com/drive/1MbQXSMgsuRuoPgoZtBtjzB96KneiidWW?usp=sharing)"
39273,"hi, i have been installing , uninstalling and reinstalling the tensorflow, this time DLL Load failed . kindly help","runfile('C:/Users/acer/anaconda3/Lib/chk.py', wdir='C:/Users/acer/anaconda3/Lib')
Traceback (most recent call last):

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""C:\Users\acer\anaconda3\Lib\chk.py"", line 2, in <module>
    import tensorflow as tf

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\acer\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Error in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x0000000003BE9A88>> (for post_execute):
Traceback (most recent call last):

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""C:\Users\acer\anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 538, in post_execute_hook
    _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])

  File ""C:\Users\acer\anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 184, in filename_and_mtime
    if not hasattr(module, '__file__') or module.__file__ is None:

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\acer\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked

  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\acer\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\acer\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\acer\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39272,embedding projector - mouse enter/out tooltip incorrect,"See attached animated gif for example.

![hover](https://user-images.githubusercontent.com/5237151/81316791-08f5bc00-905a-11ea-8fd4-0c1540db9f21.gif)
"
39271,error while building libtensorflowlite.so,"**System information**
- OS Platform and Distribution: Ubuntu 18.04 running as a docker container
- Mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 2.7.17
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 2.0.1
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: None
- GPU model and memory: None

I followed the official [documentation](https://www.tensorflow.org/lite/guide/android) to build c++ libraries of tensorflow-lite locally, but build of libtensorflowlite.so fails with the following error:

/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/ruy/ruy/BUILD:270:1: C++ compilation of rule '@ruy//ruy:detect_arm' failed (Exit 1)
external/ruy/ruy/detect_arm.cc:59:10: error: use of undeclared identifier 'getauxval'
  return getauxval(AT_HWCAP) & kLocalHwcapAsimddp;

I ran bazel using the following command:

bazel build -c opt --config=android_arm64 --config=v2 --cxxopt='--std=c++14' //tensorflow/lite:libtensorflowlite.so  

Some environment vars:

ANDROID_HOME = /usr/lib/android-sdk
ANDROID_SDK_HOME=/usr/lib/android-sdk
ANDROID_SDK_API_LEVEL=23
ANDROID_NDK_HOME=/usr/lib/android-sdk/ndk/17.2.4988734
ANDROID_NDK_API_LEVEL=17

Build tools version: 27.0.1

What am I doing wrong?

"
39270,train_on_batch fails with MirroredStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 0.29.1
- GCC/Compiler version (if compiling from source): 8.3.0
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX1080Ti

**Describe the current behavior**

When running `train_on_batch` on a model created under a distribution strategy such as MirroredStrategy the error `ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.` is thrown

Putting the `train_ob_batch` into the strategy scope does not change that error. However when doing so with a model containing a batch normalization layer (e.g. Resnet50) it throws a different error: `RuntimeError: `add_update` was called in a cross-replica context. This is not expected. If you require this feature, please file an issue.` (the other error location is not reached in that case as it bails out with the above)

**Describe the expected behavior**

`train_on_batch` should work

**Standalone code to reproduce the issue**
```
import tensorflow as tf

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0
    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

    model = tf.keras.models.Sequential([
        tf.keras.layers.Flatten(input_shape=(28, 28)),
        tf.keras.layers.Dense(128, activation='relu'),
        tf.keras.layers.Dense(10),
    ])

    model.compile(
        optimizer=tf.keras.optimizers.SGD(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=['accuracy'])

for x, y in train_dataset:
    model.train_on_batch(x, y)
```

**Other info / logs** Include any logs or source code that would be helpful to
Full traceback:
```
Traceback (most recent call last):
  File ""tf_issue_train_on_batch.py"", line 24, in <module>
    model.train_on_batch(x, y)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1078, in train_on_batch
    standalone=True)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 433, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:305 train_on_batch  *
        outs, total_loss, output_losses, masks = (
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:273 _process_single_batch
        model.optimizer.apply_gradients(zip(grads, trainable_weights))
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:444 apply_gradients
        kwargs={""name"": name})
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1949 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1956 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:485 _distributed_apply
        scope_name), distribution.extended.colocate_vars_with(var):
    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__
        return next(self.gen)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4112 _colocate_with_for_gradient
        with self.colocate_with(op, ignore_existing):
    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__
        return next(self.gen)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4161 colocate_with
        op = _op_to_colocate_with(op, self)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:6548 _op_to_colocate_with
        if hasattr(v, ""handle"") and isinstance(v.handlTraceback (most recent call last):
  File ""tf_issue_train_on_batch.py"", line 24, in <module>
    model.train_on_batch(x, y)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1078, in train_on_batch
    standalone=True)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py"", line 433, in train_on_batch
    output_loss_metrics=model._output_loss_metrics)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 568, in __call__
    result = self._call(*args, **kwds)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 615, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 497, in _initialize
    *args, **kwds))
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2389, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2703, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py"", line 2593, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 978, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py"", line 439, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in converted code:

    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:305 train_on_batch  *
        outs, total_loss, output_losses, masks = (
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py:273 _process_single_batch
        model.optimizer.apply_gradients(zip(grads, trainable_weights))
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:444 apply_gradients
        kwargs={""name"": name})
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1949 merge_call
        return self._merge_call(merge_fn, args, kwargs)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/distribute_lib.py:1956 _merge_call
        return merge_fn(self._strategy, *args, **kwargs)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:485 _distributed_apply
        scope_name), distribution.extended.colocate_vars_with(var):
    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__
        return next(self.gen)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4112 _colocate_with_for_gradient
        with self.colocate_with(op, ignore_existing):
    /sw/installed/Python/3.7.4-GCCcore-8.3.0/lib/python3.7/contextlib.py:112 __enter__
        return next(self.gen)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:4161 colocate_with
        op = _op_to_colocate_with(op, self)
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:6548 _op_to_colocate_with
        if hasattr(v, ""handle"") and isinstance(v.handle, Tensor):
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py:720 handle
        raise ValueError(""`handle` is not available outside the replica context""

    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.

```e, Tensor):
    /sw/installed/TensorFlow/2.1.0-fosscuda-2019b-Python-3.7.4/lib/python3.7/site-packages/tensorflow_core/python/distribute/values.py:720 handle
        raise ValueError(""`handle` is not available outside the replica context""

    ValueError: `handle` is not available outside the replica context or a `tf.distribute.Strategy.update()` call.

"
39269,Unintended tf.distribute.ReplicaContext.merge_call behavior on TPU,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 & Colab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0rc3
- Python version: 3
- CUDA/cuDNN version: 7.6
- GPU model and memory: GTX1080Ti

**Describe the current behavior**
The argument in merge_fn is the original input tensor.

**Describe the expected behavior**
According to the doc: ""merge_fn: Function that joins arguments from threads that are given as PerReplica"".

**Standalone code to reproduce the issue**
```
import os

import tensorflow as tf

if 'COLAB_TPU_ADDR' in os.environ:
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
  strategy = tf.distribute.experimental.TPUStrategy(resolver)
else:
  strategy = tf.distribute.MirroredStrategy()


@tf.function
def step_fn():
  v = tf.zeros([10])

  def merge_fn(strategy, keys):
    print(keys)

  context = tf.distribute.get_replica_context()
  context.merge_call(merge_fn, args=[v])


strategy.run(step_fn)
```

**Other info / logs** 

On a two GPU machine, the output is
```
PerReplica:{
  0: Tensor(""zeros:0"", shape=(10,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:0),
  1: Tensor(""replica_1/zeros:0"", shape=(10,), dtype=float32, device=/job:localhost/replica:0/task:0/device:GPU:1)
}
```
On a Colab TPU, the output is
```
Tensor(""zeros:0"", shape=(10,), dtype=float32)
```"
39268,Inconsistency in MirroredStrategy evaluation results for batch dependent metrics,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): colab (and from pip locally)
- TensorFlow version (use command below): 2.2.0-rc4 (but also locally on 2.1)
- Python version: 3.6.9 (and 3.6.8 locally) 
- CUDA/cuDNN version: 10.1
- GPU model and memory: Quadro P50000 locally, colab otherwise

**Describe the current behavior**

When I use `MirroredStrategy` for model evaluation with whole-batch dependent metrics, there is some inconsistency in the metrics returned.
This has to do with the fact that the batches are separated before being sent to the different devices and the metrics are computed on each device before being averaged on the master device.

**Describe the expected behavior**

I would like the metrics evaluation to be independent of whether I use `MirroredStrategy` or not.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1_Lohjz7qSjF7cGKCMO7F7-lIBDMTYJw5?usp=sharing

You can test that the metric computation is otherwise consistent by changing the flag to `False`.


```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense


# logical device separation
physical_devices = tf.config.list_physical_devices('GPU')

if True:
  tf.config.set_logical_device_configuration( 
      physical_devices[0], 
      [tf.config.LogicalDeviceConfiguration(memory_limit=8000), 
       tf.config.LogicalDeviceConfiguration(memory_limit=8000)])


# my full batch dependent loss
def my_loss(y_true, y_pred):
    return tf.reduce_max(tf.abs(y_true - y_pred))

# my toy model
mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.ReductionToOneDevice())
with mirrored_strategy.scope():
    model_distributed = Sequential(Dense(10))
    model_distributed.compile(loss=my_loss)

# my toy data
x = tf.random.normal([32, 10])
y = tf.random.normal([32, 10])

# my experiments
metrics = model_distributed.evaluate(x, y)
print(metrics)

y_pred = model_distributed.predict(x)
print(my_loss(y, y_pred))
```

**Other info / logs**

A sample output from the previous code would for example be:
```
1/1 [==============================] - 0s 1ms/step - loss: 4.1214
4.121423721313477
tf.Tensor(4.1544814, shape=(), dtype=float32)
```

An obvious solution is to not use `evaluate` but `predict` and iterate myself over the dataset (in my real use case I use a dataset and NCCL) computing the metrics myself. But I am then losing some nice properties of `evaluate` like the callbacks and I have to compute manually potentially a range of metrics.

Maybe this isn't a bug but in which case it would be nice to have a warning in the docs. I also would like to know if there is a way to still use `evaluate` maybe with a custom `cross_device_ops`."
39267,CUDA_ERROR_LAUNCH_FAILED when training RNNs,"I'm getting the following error when training LSTMs or GRUs

I tensorflow/stream_executor/stream.cc:1868 [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not wait for [stream=0000019AB6A73AF0,impl=0000019ACE30D4B0]
I tensorflow/stream_executor/stream.cc:4816 [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not memcpy host-to-device; source: 0000019F7DB15080
I tensorflow/stream_executor/stream.cc:1868] [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not wait for [stream=0000019AB6A73AF0,impl=0000019ACE30D4B0]
I tensorflow/stream_executor/stream.cc:4816] [stream=0000019AB6A744F0,impl=0000019ACE30D210] did not memcpy host-to-device; source: 0000019A943627C0
E tensorflow/stream_executor/stream.cc:332] Error recording event in stream: error recording CUDA event on stream 0x19ad7cd0600: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure; not marking stream as bad, as the Event object may be at fault. Monitor for further errors.
E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
[I 13:52:21.664 LabApp] KernelRestarter: restarting kernel (1/5), keep random ports

The problem occurs only on the newly bought machine, not on the previously used one, although both have the same setup with same tensorflow and CUDA versions:

Intel Core i9 9960X
Nvidia Quadro RTX 6000
Windows 10
Tensorflow 2.0
CUDA 10.1
cuDNN 7.6.5

Also, all users properly initialize the GPU in the jupyter notebooks to allow for multiple notebooks being trained with

gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(enable=True,device=gpus[0])

I would appreciate any help, since I haven't found any solution for this on the web.

Thanks!



"
39265,Tensorflow release 2.2.0 not found via `pip install`,"**tldr;** pip does not find the new release 2.2.0 published yesterday

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Not Applicable
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu Server 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Not Applicable
- Python version: 3.8.2
- PIP version: 20.1
- CUDA/cuDNN version: Not Applicable (10.2)
- GPU model and memory: Not Applicable (RTX 2080 Ti)

**Describe the current behavior**
From the change log and the Github releases I noticed that Tensorflow 2.2.0 was released and that I could upgrade the Tensorflow version currently used in my project (2.2.0rc4). Sadly, `pip install` failed to install the update as it was unable to find release 2.2.0. (Clearing the cache of pip didn't help.) The PyPi website also says latest version is the rc4, not the [2.2.0 release](https://pypi.org/project/tensorflow/#history).

**Describe the expected behavior**
Well, I would like to be able to install Tensorflow 2.2.0 over `pip`.

**Standalone code to reproduce the issue**
`pip install tensorflow==2.2.0`

**Other info / logs**
```
ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0  (from versions: 2.2.0rc1, 2.2.0rc2, 2.2.0rc3, 2.2.0rc4)
ERROR: No matching distribution found for tensorflow==2.2.0 (from -r projects/hohl.thesis/requirements.txt (line 3))
```"
39264,E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): msvc
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10, cudnn 7.4
- GPU model and memory: gtx 1650



**Describe the problem**
E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR

**Provide the exact sequence of commands / steps that you executed before running into the problem**



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39263,Remove GCC_HOST_COMPILER_PREFIX as it may be out of sync with GCC_HOST_COMPILER_PATH,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.2.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 7.3.0


**Describe the problem**

The file https://github.com/tensorflow/tensorflow/blob/1588f45ee56860d247a1c26ea228cb3721b4bf1b/third_party/gpus/cuda_configure.bzl has a documented environment variable `GCC_HOST_COMPILER_PATH` to set the path (or name) of a GCC host compiler. However it also uses an undocumented variable `GCC_HOST_COMPILER_PREFIX` to get the folder where the gcc binary resides (guessing from name) defaulting to `/usr/bin` if it isn't set:
https://github.com/tensorflow/tensorflow/blob/1588f45ee56860d247a1c26ea228cb3721b4bf1b/third_party/gpus/cuda_configure.bzl#L1028-L1030

I have 2 problems with that:
- It is undocumented and hence hard to set right if you don't know for sure what it is
- The default is wrong when the (documented!) `GCC_HOST_COMPILER_PATH`  is used

This leads to issues such as 

```
 external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/k8-opt/bin/external/protobuf_archive/js_embed -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-opt/bin/external/protobuf_archive/js_embed-2.params)
/usr/bin/ld.gold: error: /software/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable
/usr/bin/ld.gold: error: /software/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable
/usr/bin/ld.gold: error: bazel-out/k8-opt/bin/external/protobuf_archive/_objs/js_embed/embed.o: unsupported reloc 42 against global symbol std::ios_base::Init::~Init()
/software/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42
/software/software/GCCcore/7.3.0/lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42
bazel-out/k8-opt/bin/external/protobuf_archive/_objs/js_embed/embed.o:embed.cc:function _GLOBAL__sub_I_main: error: unsupported reloc 42
collect2: error: ld returned 1 exit status
```

As reported at https://github.com/easybuilders/easybuild-easyconfigs/pull/7800#issuecomment-471447493

I hence propose to either completely remove that variable in favor of deriving its value from `GCC_HOST_COMPILER_PATH` or properly documenting it with a better default.

It does not seem to be required at all so it's likely best to just remove it. This should have been done by #34218 but for some reason that merge was reverted with a very misleading commit title: https://github.com/tensorflow/tensorflow/commit/f0571998d0195b5b243cf409a64d5fa17bd44d43

@gunan @mihaimaruseac please take a look what went wrong"
39262,Tensorflow v2.2.0 build failure at the final step on macOS 10.13.6 with CUDA enabled,"I've been building [wheel packages of Tensorflow with CUDA support for macOS](https://github.com/TomHeaven/tensorflow-osx-build/releases) for a while. Usually, TF will be built successfully with some patches on sources and bazel config files. 

However, I met a troublesome issue building Tensorflow v2.2.0 at the final step: The compiler complained about **Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE** when loading _pywrap_tensorflow_internal.so. The details are as follows:

### System information
- **Mac OS 10.13.6**:
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 2.2.0
- **Python version**: 3.7.0
- **Bazel version (if compiling from source)**: 2.0.0
- **GCC/Compiler version (if compiling from source)**: AppleClang++ 9.0
- **CUDA/cuDNN version**: 10.0/7.4
- **GPU model and memory**: Nvidia Titan V
- **Exact command to reproduce**:
```
bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --config=nonccl --config=monolithic --verbose_failures //tensorflow/tools/pip_package:build_pip_package
```
- **Build Failure Info**:
```
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /Volumes/Data/libraries/tensorflow/tensorflow/python/keras/api/BUILD:117:1: Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE
  Referenced from: /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 10): Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE
  Referenced from: /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /private/var/tmp/_bazel_tomheaven/561821a038e9c8d51ab53646fb4bd33f/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/keras/api/create_tensorflow.python_api_keras_python_api_gen_compat_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /Volumes/Data/libraries/tensorflow/tensorflow/python/tools/BUILD:82:1 Executing genrule //tensorflow/python/keras/api:keras_python_api_gen_compat_v1 failed (Exit 1)
INFO: Elapsed time: 0.967s, Critical Path: 0.31s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
```
"
39261,"Im triying to use rasa inside Anaconda and using ""rasa init"" command to create a rasa project I get errors!","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Im triying to use rasa inside Anaconda and using ""rasa init"" command to create a rasa project I get errors
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 home with Anaconda
 
- TensorFlow version (use command below): tensorflow-estimator==2.1.0 (Anaconda virtualenv)
- Python version: python-3.6.10
- GPU model and memory: dell x-64, 4GB, i3

**(rasa) C:\Users\carlo>rasa init**

Traceback (most recent call last):
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\carlo\anaconda3\envs\rasa\Scripts\rasa.exe\__main__.py"", line 7, in <module>
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\rasa\__main__.py"", line 82, in main
    set_log_level(log_level)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\rasa\utils\common.py"", line 71, in set_log_level
    update_tensorflow_log_level()
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\rasa\utils\common.py"", line 112, in update_tensorflow_log_level
    import tensorflow as tf
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\importlib\__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\users\carlo\anaconda3\envs\rasa\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: Uma rotina de inicialização da biblioteca de vínculo dinâmico (DLL) falhou.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.




"
39260,Cross Compile tensorflow for arm using a different python version ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Host linux ubuntu 18.04 amd64 , Target : linux ubuntu 18.04 arm 32 bit 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1.0
- Python version:  3.6.9
- Installed using virtualenv? pip? conda?:  python venv
- Bazel version (if compiling from source):  4.0.0
- GCC/Compiler version (if compiling from source):  arm-linux-gnueabihf-gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04
- CUDA/cuDNN version: no
- GPU model and memory: no 

Hi,
i am cross compiling tensorflow ( version 2.1.0 ) since i need the new delegate features for my hardware accelerator. 
I tried to install directly tensorflow lite from the official binaries for linux arm 32 however when i run a model i get the ""illegal instruction"" which is probably due to different floating point support of the processor and/or avx instructions (those ideas come from other github issue)
My board is the Pynq Z2 from tul with arm processor on 32 bit architecture running a linux based OS. I am following the procedure for building  tensorflow from source for the rasberry ( i changed some of the compiler flags for correctly match my architecture) however i cannot change the python version of the wheel ( i need a cp36 instead of a cp35 ) from the build process. Is there any workaround?

If you are wondering why i did not install python3.5, it is becouse there may be some incompatibility with the pynq package. 

PS i tried to change brutally the name of the wheel, from cp35 to cp36, but when i import tensorflow into the python enviroment i get the incompatibility error of the two version.
"
39259,TensorFlowLite app's camera doesn't focus on objects - produces blurry images,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No.**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10+ and Samsung Galaxy A70 and Samsung Galaxy S7 Edge (I tried on many diff. devides)
- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/examples.git
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I'm trying to achieve this image quality:
[https://i.stack.imgur.com/tPfdg.jpg](https://i.stack.imgur.com/tPfdg.jpg)

**Describe the expected behavior**
I'm achieving this image quality (currently):
[https://i.stack.imgur.com/qD8uh.jpg](https://i.stack.imgur.com/qD8uh.jpg)

**Standalone code to reproduce the issue**
All the info. can be found in the following link:
[https://stackoverflow.com/questions/61611050/tensorflows-app-camera-images-are-blurry](https://stackoverflow.com/questions/61611050/tensorflows-app-camera-images-are-blurry)

Also, I've added most of it here to. (see below)

Set up the working directory

git clone https://github.com/tensorflow/examples.git
Open the project with Android Studio

Open a project with Android Studio by taking the following steps:

Open Android Studio. After it loads select "" Open an existing Android Studio project"".

In the file selector, choose examples/lite/examples/image_classification/android from your working directory to load the project.

In `LegacyCameraConnectionFragment.java`, in the function `onSurfaceTextureAvailable`

Add in line 90~ this code is added to turn on the flash all the time (it's off by default):

```
List<String> flashModes = parameters.getSupportedFlashModes();
if (flashModes.contains(android.hardware.Camera.Parameters.FLASH_MODE_TORCH))
{
  parameters.setFlashMode(parameters.FLASH_MODE_TORCH);
}
```


Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

It's simple, please take a cup or any other object, which you can put your phone over it, put a small object into it, for example like a gum, or some sort of a pill w/ text in it, and see if the image is blurry or not w/ TensorFlow's app.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39257,"""OverflowError: cannot serialize a bytes object larger than 4 GiB"" when using model.fit([...], use_multiprocessing=True) on custom generator","Hi,

I'm trying to train a model consisting of some GRU layers on data stored in a large numpy array (~18gb) on 2 GPUs using the MirroredStrategy.

My system:
* AMD Ryzen Threadripper 3960X 24-Core Processor 
* 64 GB RAM
* two NVIDIA GeForce RTX 2070 SUPER with 8192MiB each
* Win 10 (unfortunately, the ASrock Creator TRX40 motherboard we bought is currently incompatible with Linux, wtf...)
* TF 2.1.0 installed from binary (anaconda)
* Python 3.7.7
* CUDA Version 10.2.89

This is my code:

```python
import tensorflow as tf 
import numpy as np 

train_tokens_X = np.zeros((1066673, 61, 69), dtype=np.float32)
train_tokens_X[:] = np.eye(69)[:61,:]

train_target = np.zeros((1066673, 3943), dtype=np.float32)
train_target[:,2] = 1

valid_tokens_X= np.zeros((133366, 61, 69), dtype=np.float32)
valid_tokens_X[:] = np.eye(69)[:61,:]

valid_target= np.zeros((133366, 3943), dtype=np.float32)
valid_target[:,2] = 1

output_size = 3943
max_id = 68
batch_size = 256

class Sequencer(tf.keras.utils.Sequence):

    def __init__(self, x_set, y_set, batch_size):
        self.x, self.y = x_set, y_set
        self.batch_size = batch_size

    def __len__(self):
        return math.ceil(len(self.x) / self.batch_size)

    def __getitem__(self, idx):
        batch_x = self.x[idx * self.batch_size:(idx + 1) *
        self.batch_size]
        batch_y = self.y[idx * self.batch_size:(idx + 1) *
        self.batch_size]

        return np.array(batch_x), np.array(batch_y)


train_generator = Sequencer(train_tokens_X, train_target, batch_size)
valid_generator = Sequencer(valid_tokens_X, valid_target, batch_size)

mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())

with mirrored_strategy.scope():
    model = keras.models.Sequential([
        keras.layers.GRU(128, return_sequences=True, input_shape=[ None, max_id+1], use_bias=False),
        keras.layers.GRU(128, return_sequences=True, use_bias=False),
        keras.layers.GRU(128, use_bias=False),
        keras.layers.Flatten(),
        keras.layers.Dense(output_size, activation=""softmax"")
    ])
    model.compile(loss=[focal_loss_umbertogriffo.categorical_focal_loss(alpha=.25, gamma=2)], optimizer=""adam"", metrics=['accuracy'])

history = model.fit(train_generator, validation_data=valid_generator, epochs=25, callbacks = callbacks, max_queue_size=10, workers=2, use_multiprocessing=True)
```

and there it crashes with:

```
Exception in thread Thread-12:
Traceback (most recent call last):
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\threading.py"", line 926, in _bootstrap_inner
    self.run()
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\threading.py"", line 870, in run
    self._target(*self._args, **self._kwargs)
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 844, in _run
    with closing(self.executor_fn(_SHARED_SEQUENCES)) as executor:
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\site-packages\tensorflow_core\python\keras\utils\data_utils.py"", line 823, in pool_fn
    initargs=(seqs, None, get_worker_id_queue()))
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\context.py"", line 119, in Pool
    context=self.get_context())
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\pool.py"", line 176, in __init__
    self._repopulate_pool()
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\pool.py"", line 241, in _repopulate_pool
    w.start()
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\process.py"", line 112, in start
    self._popen = self._Popen(self)
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\context.py"", line 322, in _Popen
    return Popen(process_obj)
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\popen_spawn_win32.py"", line 89, in __init__
    reduction.dump(process_obj, to_child)
  File ""C:\Users\KI\anaconda3\envs\tensorflow_test\lib\multiprocessing\reduction.py"", line 60, in dump
    ForkingPickler(file, protocol).dump(obj)
OverflowError: cannot serialize a bytes object larger than 4 GiB
```

If trained without the `use_multiprocessing ` flag, training works seamlessly, but is very slow due to the generator overhead (I guess) with only ~10% load on each GPU and also ~10% on the CPU.

If trained without the using the generator at all (putting the data directly into model.fit), model.fit crashes due to memory issues:

`2020-05-07 12:46:43.785479: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 16823566556 exceeds 10% of system memory.`

...so I have to use a generator for memory efficiency.

I also tried to use a tf.data pipeline but the same `exceeds 10% of system memory` arises, unfortunately. 

What can I do here?"
39256,save tensorflow estimator based custom model for inference ,"thanks for the example codes. Experimented with Text Classification for character_rnn (https://github.com/tensorflow/tensorflow/blob/671baf080238025da9698ea980cd9504005f727c/tensorflow/examples/learn/text_classification_character_rnn.py).

How can i write a serving_input_fn for it ? I want to save and restore this model

extended the code to save but getting error, please help
`from tensorflow.contrib.learn.python.learn.utils import input_fn_utils` 
`feature_spec = {""feature"":tf.FixedLenFeature([100],tf.int64)}`
`serving_input_fn = input_fn_utils.build_parsing_serving_input_fn(feature_spec)`
 and than
`classifier.export_savedmodel(export_dir_base='model', serving_input_receiver_fn=serving_input_fn)`

and getting this error

TypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'feature': <tf.Tensor 'ParseExample/ParseExample:0' shape=(?, 100) dtype=int64>}. Consider casting elements to a supported type."
39253,[TF Lite C API] TfLiteInterpreterGetOutputTensor returns nullpointer in second iteration,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): GNU/Linux aarch64
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): gcc version 7.4.0 (Ubuntu/Linaro 7.4.0-1ubuntu1~18.04.1)

**Describe the current behavior**
I am trying to use the C API for TF Lite on a custom ARM board. I built `libtensorflowlite_c.so` with `select_tf_ops` enabled via bazel:

```
bazel build --config=monolithic \
--define=with_select_tf_ops=true \
-c opt //tensorflow/lite/c:tensorflowlite_c \
--config noaws
```

I tried to use the API as described as in `tensorflow/lite/c/c_api.h`, but instead of just doing a single inference, I wanted to have cyclic evaluation of my inputs with the model. To do this, I simply put the code there in a for-loop:

```
#include ""tensorflow/lite/c/c_api.h""
#include <stdio.h>
#include <stdlib.h>


int main (int argc, char* argv[]) {

   for(int i = 0; i < 3; i++)
    {
        printf(""Iteration: %d\n"", i);

        float input[49] = { 0.0 };
        TfLiteModel* model = TfLiteModelCreateFromFile(""model.tflite"");
        TfLiteInterpreterOptions* options = TfLiteInterpreterOptionsCreate();
        TfLiteInterpreterOptionsSetNumThreads(options, 2);
        TfLiteInterpreter* interpreter = TfLiteInterpreterCreate(model, options);
        TfLiteInterpreterAllocateTensors(interpreter);

        TfLiteTensor* input_tensor = TfLiteInterpreterGetInputTensor(interpreter, 0);
        TfLiteTensorCopyFromBuffer(input_tensor, input, 49 * sizeof(float));

        TfLiteInterpreterInvoke(interpreter);

        const TfLiteTensor* output_tensor = TfLiteInterpreterGetOutputTensor(interpreter, 14);

        float output[49];
        TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));

        printf(""Output: \n\n"");
        for (int j = 0; j < 49; j++) {
            printf(""%d: %f\n"", j, output[j]);
        }

        TfLiteInterpreterDelete(interpreter);
        TfLiteInterpreterOptionsDelete(options);
        TfLiteModelDelete(model);
    }
    return 0;
}
```
The first iteration runs fine and returns something. But on the second iteration, I get a SegFault when calling `TfLiteTensorCopyToBuffer(output_tensor, output, 49 * sizeof(float));`. Reason for this is that the previous function `TfLiteInterpreterGetOutputTensor` returns a nullpointer.

**Describe the expected behavior**
I expected to run this multiple times without any problems, as I destroy all old instances of variables at the end of the for-loop and thus start a fresh interpreter everytime. Obviously, this is not the case.

Can somebody tell me what is going wrong here? I am currently building the debug version of the shared object, but this will take some time. Any help is appreciated!"
39251,Code gets stuck when using mirrored strategy,"**System information:**

Have I written custom code : Yes
OS Platform and Distribution : Linux Ubuntu 18.04
TensorFlow installed from : binary (install from pip)
TensorFlow version (use command below): 2.1.0
Python version: 3.6.8
CUDA/cuDNN version: Cuda Toolkit 10.1 / cuDNN 7.6.4
GPU model and memory: 2 Tesla V 100 16 GB each

**Question:**


I would like to store different variables and run different tensorflow ops on different devices (GPU/CPU). I want to do this because my model cannot fit on a single GPU. I am trying to train embeddings. I want to place recurring words on all GPUs and non common words on different devices (as a part of preprocessing I do all this) and transfer sentences to respective GPUs. 


For the this question I am not pasting my actual code. I am writing a snippet which is close to what I want to do. When I run the code, I do not get any error and neither does it execute successfully. I think the problem is that this code gets stuck (may be deadlock on GPU). 

Below is the snippet. For the sake of simplicity say I want to do  `y = x * w1 + x * w2 + 2 * w3`, where `w1`, `w2` and `w3` are trainable variables. I place `w1` on `GPU0` and `w2` on `GPU1`. Instead of doing the `2 * w3` on a single device say I want to split it and perform on 2 devices (to see how mirrored strategy works). I perform respective computations on respective GPUs. First of all is that a correct way to do it (using mirrored scope to assign devices to variables). Secondly if it is wrong why does tensorflow not throw any error, why is it getting stuck and why is it showing 100% GPU usage?

If i change all the device/scopes to a single GPU it executes (in seconds) successfully but I don't want that. I want to place some variables on single devices and some on both and perform computations as I illustrated below to speed up processing.
```
import os

#os.environ['TF2_BEHAVIOR'] = '1'
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

#tf.compat.v1.disable_eager_execution()

tf.config.set_soft_device_placement(False)
tf.debugging.set_log_device_placement(True)
import numpy as np

class ProdLayer(tf.keras.layers.Layer):
    def __init__(self, name):
        super(ProdLayer, self).__init__()
        self.w = tf.keras.backend.variable(0.01, name='var_'+ name)


    def call(self, x):
        return x * self.w 

class SumLayer(tf.keras.layers.Layer):
    def __init__(self):
        super(SumLayer, self).__init__()
        
    def call(self, x1, x2):
        return x1 + x2
    

mirrored_strategy = tf.distribute.MirroredStrategy(devices=[""/gpu:0"", ""/gpu:1""])

class EmbeddingModel(tf.keras.Model):
    def __init__(self):
        super(EmbeddingModel, self).__init__()
        
        # place w1 on GPU 0 and create the layer
        with tf.device('/gpu:0'):
            self.L1 = ProdLayer('w1')
        
        # place w2 on GPU 0 and create the layer        
        with tf.device('/gpu:1'):
            self.L2 = ProdLayer('w2')
        
        # place w3 on both GPU0 and GPU1 using mirrored scope. Can I do this?
        with mirrored_strategy.scope():
            self.w3 = tf.keras.backend.variable(0.01, name='var_w3')
            
        # may be do this on CPU? But for now let it perform this on GPU0
        with tf.device('/gpu:0'):
            self.L3 = SumLayer()
        
    
    def call(self, input_layer):
        # w1 is on GPU0, w2 is on GPU1 and w3 is placed using mirrored scope on both GPUs
        # y1 = w1 * x + w3
        with tf.device('/gpu:0'):
            y1 = self.L1(input_layer) + self.w3

        # y2 = w2 * x + w3
        with tf.device('/gpu:1'):
            y2 = self.L2(input_layer) + self.w3

        # y = y1 + y2 (i.e. w1 * x + w2 * x + 2 * w3) 
        with tf.device('/gpu:0'):
            y_hat = self.L3(y1, y2)
            
        return y_hat
    
    
def myLoss(y, y_hat):
    return tf.reduce_sum((y_hat-y) * (y_hat-y))

with mirrored_strategy.scope():
    model = EmbeddingModel()
    model.compile(tf.optimizers.Adam(lr=0.0001), 
                      loss=myLoss)

train_dataset = np.random.choice(100, size=(1000,))
model.fit(x=train_dataset.astype(np.float32), y=train_dataset.astype(np.float32), 
       batch_size=1000, 
       epochs=10, shuffle=False, verbose=True)

```




"
39250,When I run Interpreter.run() method I get: 'DataType error: cannot resolve DataType of [[Ljava.lang.Float;' ,"This is the code :
    fun estimateTheAction(detectedSkeletons: ArrayList<Float>): String {
             var output = Array(1) {FloatArray(4)}
             val arrayInput: Array<Float> = detectedSkeletons.toTypedArray()
             //var output = mapOf(1 to ""person_jumping"", 2 to ""person_kicking"", 3 to ""person_running"", 4 to ""person_walking"")
             var input = arrayOf(arrayInput)
             try {
                      interpreter?.run(input, output)
             }catch (e: Exception ){
                      e.printStackTrace()
        }
        //var index = output[0].toString()
        return ""actionName[output[0][0]]
    }


Complete Error:
W/System.err: java.lang.IllegalArgumentException: DataType error: cannot resolve DataType of [[Ljava.lang.Float;
W/System.err:     at org.tensorflow.lite.Tensor.dataTypeOf(Tensor.java:199)
        at org.tensorflow.lite.Tensor.throwIfTypeIsIncompatible(Tensor.java:257)
        at org.tensorflow.lite.Tensor.getInputShapeIfDifferent(Tensor.java:162)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:132)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:275)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:249)
       ....."
39249,Maybe a bug,"Hi,

I am not sure if it is bug. However, the code uses iterator as the advice using iterator from the previous same error. But it does not work. When I tried saving model, `save_path = saver.save(sess, ckpt_saver_file_final, global_step)`, I get the error.

```
Error:
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.
Fatal Python error: Aborted
```

Tensorflow: 1.8-cuda
Python: 3.6
GPU :  Tesla V100-SXM2-16GB
Train and Model Code:

```
Error Trace ;
[libprotobuf FATAL external/protobuf_archive/src/google/protobuf/message_lite.cc:68] CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.
terminate called after throwing an instance of 'google::protobuf::FatalException'
  what():  CHECK failed: (byte_size_before_serialization) == (byte_size_after_serialization): tensorflow.GraphDef was modified concurrently during serialization.
Fatal Python error: Aborted

Thread 0x00002aabae642700 (most recent call first):
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/threading.py"", line 295 in wait
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/queue.py"", line 164 in get
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/summary/writer/event_file_writer.py"", line 159 in run
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/threading.py"", line 916 in _bootstrap_inner
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lang/Python/3.6.4-foss-2018a/lib/python3.6/threading.py"", line 884 in _bootstrap

Current thread 0x00002aaaaaaea900 (most recent call first):
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3169 in _as_graph_def
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3228 in as_graph_def
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1766 in export_meta_graph
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1728 in save
  File ""/mnt/lscratch/users/wma/ncc/inst2vec/inst2vec_embedding.py"", line 417 in train_skip_gram
  File ""/mnt/lscratch/users/wma/ncc/inst2vec/inst2vec_embedding.py"", line 546 in train_embeddings
  File ""train_inst2vec.py"", line 60 in main
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/absl/app.py"", line 251 in _run_main
  File ""/opt/apps/resif/data/production/v1.1-20180716/default/software/lib/TensorFlow/1.8.0-foss-2018a-Python-3.6.4-CUDA-9.1.85/lib/python3.6/site-packages/absl/app.py"", line 300 in run
  File ""train_inst2vec.py"", line 74 in <module>
/var/lib/slurmd/job1799931/slurm_script: line 21: 217538 Aborted                 python train_inst2vec.py
```

Code:
```
########################################################################################################################
# Training embeddings
########################################################################################################################
def train_skip_gram(V, data_folder, data_folders, dataset_size, reverse_dictionary,
                    param, valid_examples, log_dir, vocab_metada_file, embeddings_pickle,
                    ckpt_saver_file, ckpt_saver_file_init, ckpt_saver_file_final,
                    restore_variables):
    """"""
    Train embeddings (Skip-Gram model)
    :param V: vocabulary size
    :param data_folder: string containing the path to the parent directory of raw data sub-folders
    :param data_folders: list of sub-folders containing pre-processed LLVM IR code
    :param dataset_size: number of data pairs in total in the training data set
    :param reverse_dictionary: [keys=statement index, values=statement]
    :param param: parameters of the inst2vec training
    :param valid_examples: statements to be used as validation examples (list of indices)
    :param log_dir: logging directory for Tensorboard output
    :param vocab_metada_file: vocabulary metadata file for Tensorboard
    :param embeddings_pickle: file in which to pickle embeddings
    :param ckpt_saver_file: checkpoint saver file (intermediate states of training)
    :param ckpt_saver_file_init: checkpoint saver file (initial state of training)
    :param ckpt_saver_file_final: checkpoint saver file (final state of training)
    :param restore_variables: boolean: whether to restore variables from a previous training
    :return: embeddings matrix
    """"""
    ####################################################################################################################
    # Extract parameters from dictionary ""param""
    N = param['embedding_size']
    mini_batch_size = param['mini_batch_size']
    num_sampled = param['num_sampled']
    num_epochs = param['num_epochs']
    learning_rate = param['learning_rate']
    l2_reg_scale = param['beta']
    freq_print_loss = param['freq_print_loss']
    step_print_neighbors = param['step_print_neighbors']
    context_width = param['context_width']

    ####################################################################################################################
    # Set up for analogies
    analogies, analogy_types, n_questions_total, n_questions_relevant = i2v_eval.load_analogies(data_folder)
    folder_evaluation = embeddings_pickle.replace('.p', '') + 'eval'
    if not os.path.exists(folder_evaluation):
        os.makedirs(folder_evaluation)
    analogy_evaluation_file = os.path.join(folder_evaluation, ""analogy_results"")

    config = None
    options = None
    metadata = None
    if FLAGS.profile:
        options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        metadata = tf.RunMetadata()
    if FLAGS.xla:
        config = tf.ConfigProto()
        config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1

    ####################################################################################################################
    # Read data using Tensorflow's data API
    data_files = get_data_pair_files(data_folders, context_width)
    print('\ttraining with data from files:', data_files)
    with tf.name_scope(""Reader"") as scope:

        random.shuffle(data_files)
        dataset_raw = tf.data.FixedLengthRecordDataset(filenames=data_files,
                                                       record_bytes=8)  # <TFRecordDataset shapes: (), types: tf.string>
        dataset = dataset_raw.map(record_parser)
        dataset = dataset.shuffle(int(1e5))
        dataset_batched = dataset.apply(tf.contrib.data.batch_and_drop_remainder(mini_batch_size))
        dataset_batched = dataset_batched.prefetch(int(100000000))
        iterator = dataset_batched.make_initializable_iterator()
        saveable_iterator = tf.contrib.data.make_saveable_from_iterator(iterator)
        next_batch = iterator.get_next()  # Tensor(""Shape:0"", shape=(2,), dtype=int32)

    ####################################################################################################################
    # Tensorflow computational graph
    # Placeholders for inputs
    with tf.name_scope(""Input_Data"") as scope:
        train_inputs = next_batch[:, 0]
        train_labels = tf.reshape(next_batch[:, 1], shape=[mini_batch_size, 1], name=""training_labels"")

    # (input) Embedding matrix
    with tf.name_scope(""Input_Layer"") as scope:
        W_in = tf.Variable(tf.random_uniform([V, N], -1.0, 1.0), name=""input-embeddings"")

        # Look up the vector representing each source word in the batch (fetches rows of the embedding matrix)
        h = tf.nn.embedding_lookup(W_in, train_inputs, name=""input_embedding_vectors"")

    # Normalized embedding matrix
    with tf.name_scope(""Embeddings_Normalized"") as scope:
        normalized_embeddings = tf.nn.l2_normalize(W_in, name=""embeddings_normalized"")

    # (output) Embedding matrix (""output weights"")
    with tf.name_scope(""Output_Layer"") as scope:
        if FLAGS.softmax:
            W_out = tf.Variable(tf.truncated_normal([N, V], stddev=1.0 / math.sqrt(N)), name=""output_embeddings"")
        else:
            W_out = tf.Variable(tf.truncated_normal([V, N], stddev=1.0 / math.sqrt(N)), name=""output_embeddings"")

        # Biases between hidden layer and output layer
        b_out = tf.Variable(tf.zeros([V]), name=""nce_bias"")

    # Optimization
    with tf.name_scope(""Optimization_Block"") as scope:
        # Loss function
        if FLAGS.softmax:
            logits = tf.layers.dense(inputs=h, units=V)
            onehot = tf.one_hot(train_labels, V)
            loss_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(labels=onehot, logits=logits)
        else:
            loss_tensor = tf.nn.nce_loss(weights=W_out,
                                         biases=b_out,
                                         labels=train_labels,
                                         inputs=h,
                                         num_sampled=num_sampled,
                                         num_classes=V)
        train_loss = tf.reduce_mean(loss_tensor, name=""nce_loss"")

        # Regularization (optional)
        if l2_reg_scale > 0:
            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, W_in)
            tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, W_out)
            regularizer = tf.contrib.layers.l2_regularizer(l2_reg_scale)
            reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)
            reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)
            loss = train_loss + reg_term
        else:
            loss = train_loss

        # Optimizer
        if FLAGS.optimizer == 'adam':
            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)
        elif FLAGS.optimizer == 'nadam':
            optimizer = tf.contrib.opt.NadamOptimizer(learning_rate=learning_rate).minimize(loss)
        elif FLAGS.optimizer == 'momentum':
            global_train_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=""global_step"")
            # Passing global_step to minimize() will increment it at each step.
            optimizer = (
                tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss, global_step=global_train_step)
            )
        else:
            raise ValueError('Unrecognized optimizer ' + FLAGS.optimizer)

    if FLAGS.optimizer != 'momentum':
        global_train_step = tf.Variable(0, trainable=False, dtype=tf.int32, name=""global_step"")

    ####################################################################################################################
    # Validation block
    with tf.name_scope(""Validation_Block"") as scope:
        valid_dataset = tf.constant(valid_examples, dtype=tf.int32, name=""validation_data_size"")
        valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)
        cosine_similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)

    ####################################################################################################################
    # Summaries
    with tf.name_scope(""Summaries"") as scope:
        tf.summary.histogram(""input_embeddings"", W_in)
        tf.summary.histogram(""input_embeddings_normalized"", normalized_embeddings)
        tf.summary.histogram(""output_embeddings"", W_out)
        tf.summary.scalar(""nce_loss"", loss)

        analogy_score_tensor = tf.Variable(0, trainable=False, dtype=tf.int32, name=""analogy_score"")
        tf.summary.scalar(""analogy_score"", analogy_score_tensor)

    ####################################################################################################################
    # Misc.
    restore_completed = False
    init = tf.global_variables_initializer()        # variables initializer
    summary_op = tf.summary.merge_all()             # merge summaries into one operation

    ####################################################################################################################
    # Training
    with tf.Session(config=config) as sess:

        # Add TensorBoard components
        writer = tf.summary.FileWriter(log_dir)  # create summary writer
        writer.add_graph(sess.graph)
        gvars = [gvar for gvar in tf.global_variables() if 'analogy_score' not in gvar.name]
        saver = tf.train.Saver(gvars, max_to_keep=5)  # create checkpoint saver
        config = projector.ProjectorConfig()  # create projector config
        embedding = config.embeddings.add()  # add embeddings visualizer
        embedding.tensor_name = W_in.name
        embedding.metadata_path = vocab_metada_file  # link metadata
        projector.visualize_embeddings(writer, config)  # add writer and config to projector

        # Set up variables
        if restore_variables:   # restore variables from disk
            restore_file = tf.train.latest_checkpoint(log_dir)
            assert restore_file is not None, ""No restore file found in folder "" + log_dir
            assert os.path.exists(restore_file + "".index""), \
                ""Trying to restore Tensorflow session from non-existing file: "" + restore_file + "".index""
            init.run()
            saver.restore(sess, restore_file)
            print(""\tVariables restored from file"", ckpt_saver_file, ""in TensorFlow "")

        else:  # save the computational graph to file and initialize variables

            graph_saver = tf.train.Saver(allow_empty=True)
            init.run()
            graph_saver.save(sess, ckpt_saver_file_init, global_step=0, write_meta_graph=True)
            tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable_iterator)
            print(""\tVariables initialized in TensorFlow"")

        # Compute the necessary number of steps for this epoch as well as how often to print the avg loss
        num_steps = int(math.ceil(dataset_size / mini_batch_size))
        step_print_loss = int(math.ceil(num_steps / freq_print_loss))
        print('\tPrinting loss every ', step_print_loss, 'steps, i.e.', freq_print_loss, 'times per epoch')

        ################################################################################################################
        # Epoch loop
        epoch = 0
        global_step = 0
        while epoch < int(num_epochs):
            print('\n\tStarting epoch ', epoch)
            sess.run(iterator.initializer)      # initialize iterator

            # If restoring a previous training session, set the right training epoch
            if restore_variables and not restore_completed:
                epoch = int(math.floor(global_train_step.eval() / (dataset_size / mini_batch_size)))
                global_step = global_train_step.eval()
                print('Starting from epoch', epoch)

            ############################################################################################################
            # Loop over steps (mini batches) inside of epoch
            step = 0
            avg_loss = 0
            while True:

                try:

                    # Print average loss every x steps
                    if step_print_loss > 0 and step % int(step_print_loss) == 0:    # update step with logging

                        # If restoring a previous training session, set the right training epoch
                        if restore_variables and not restore_completed:
                            restore_completed = True

                        # Write global step
                        if FLAGS.optimizer != 'momentum':
                            global_train_step.assign(global_step).eval()

                        # Perform an update
                        # print('\tStarting local step {:>6}'.format(step))  # un-comment for debugging
                        [_, loss_val, train_loss_val, global_step] = sess.run(
                            [optimizer, loss, train_loss, global_train_step], options=options,
                            run_metadata=metadata)
                        assert not np.isnan(loss_val), ""Loss at step "" + str(step) + "" is nan""
                        assert not np.isinf(loss_val), ""Loss at step "" + str(step) + "" is inf""
                        avg_loss += loss_val

                        if step > 0:
                            avg_loss /= step_print_loss

                        analogy_score = i2v_eval.evaluate_analogies(W_in.eval(), reverse_dictionary, analogies,
                                                                    analogy_types, analogy_evaluation_file,
                                                                    session=sess, print=i2v_eval.nop)
                        total_analogy_score = sum([a[0] for a in analogy_score])
                        analogy_score_tensor.assign(total_analogy_score).eval()  # for tf.summary

                        [summary, W_in_val] = sess.run([summary_op, W_in])

                        if FLAGS.savebest is not None:
                            filelist = [f for f in os.listdir(FLAGS.savebest)]
                            scorelist = [int(s.split('-')[1]) for s in filelist]
                            if len(scorelist) == 0 or total_analogy_score > sorted(scorelist)[-1]:
                                i2v_utils.safe_pickle(W_in_val, FLAGS.savebest + '/' + 'score-' +
                                                      str(total_analogy_score) + '-w.p')

                        # Display average loss
                        print('{} Avg. loss at epoch {:>6,d}, step {:>12,d} of {:>12,d}, global step {:>15} : {:>12.3f}, analogies: {})'.format(
                            str(datetime.now()), epoch, step, num_steps, global_step, avg_loss, str(analogy_score)))
                        avg_loss = 0

                        # Pickle intermediate embeddings
                        i2v_utils.safe_pickle(W_in_val, embeddings_pickle)

                        # Write to TensorBoard
                        saver.save(sess, ckpt_saver_file, global_step=global_step, write_meta_graph=False)
                        writer.add_summary(summary, global_step=global_step)

                        if FLAGS.profile:
                            fetched_timeline = timeline.Timeline(metadata.step_stats)
                            chrome_trace = fetched_timeline.generate_chrome_trace_format()
                            with open('timeline_step_%d.json' % step, 'w') as f:
                                f.write(chrome_trace)

                        if step > 0 and FLAGS.extreme:
                            sys.exit(22)

                    else:   # ordinary update step
                        [_, loss_val] = sess.run([optimizer, loss])
                        avg_loss += loss_val

                    # Compute and print nearest neighbors every x steps
                    if step_print_neighbors > 0 and step % int(step_print_neighbors) == 0:
                        print_neighbors(op=cosine_similarity, examples=valid_examples, top_k=6,
                                        reverse_dictionary=reverse_dictionary)

                    # Update loop index (steps in epoch)
                    step += 1
                    global_step += 1

                except tf.errors.OutOfRangeError:

                    # We reached the end of the epoch
                    print('\n\t Writing embeddings to file ', embeddings_pickle)
                    i2v_utils.safe_pickle([W_in.eval()], embeddings_pickle)                   # WEIRD!
                    epoch += 1      # update loop index (epochs)
                    break           # from this inner loop

        ################################################################################################################
        # End of training:
        # Print the nearest neighbors at the end of the run
        if step_print_neighbors == -1:
            print_neighbors(op=cosine_similarity, examples=valid_examples, top_k=6,
                            reverse_dictionary=reverse_dictionary)

        # Save state of training and close the TensorBoard summary writer
        save_path = saver.save(sess, ckpt_saver_file_final, global_step)
        writer.add_summary(summary, global_step)
        writer.close()

        return W_in.eval()
```


"
39248,.bazelversion and configure.py conflicts,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu 18.04.3
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):  source master
- TensorFlow version: master 2.1.0
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.2/7.6.5
- GPU model and memory: GTX1080Ti GDDR5X 11GB



**Describe the problem**

.bazelversion and configure.py. bazel version requirements are different from each other,
causing error.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
.configure

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

\.configure
```
Traceback (most recent call last):
  File ""./configure.py"", line 1557, in <module>
    main()
  File ""./configure.py"", line 1369, in main
    current_bazel_version = check_bazel_version(_TF_MIN_BAZEL_VERSION,
  File ""./configure.py"", line 482, in check_bazel_version
    curr_version = run_shell(
  File ""./configure.py"", line 159, in run_shell
    output = subprocess.check_output(cmd, stderr=stderr)
  File ""/home/wmind/anaconda3/lib/python3.8/subprocess.py"", line 411, in check_output
    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,
  File ""/home/wmind/anaconda3/lib/python3.8/subprocess.py"", line 512, in run
    raise CalledProcessError(retcode, process.args,
subprocess.CalledProcessError: Command '['bazel', '--batch', '--bazelrc=/dev/null', 'version']' returned non-zero exit status 1.
```

bazel --version
```
ERROR: The project you're trying to build requires Bazel 3.0.0 (specified in /home/wmind/repo/tensorflow/.bazelversion), but it wasn't found in /usr/bin.

You can install the required Bazel version via apt:
  sudo apt update && sudo apt install bazel-3.0.0

If this doesn't work, check Bazel's installation instructions for help:
  https://docs.bazel.build/versions/master/install-ubuntu.html
```"
39247,Support CUDA 10.2,"**System information**
- TensorFlow version (you are using): 2.2.0-rc4
- Are you willing to contribute it (Yes/No): no

**Describe the feature and the current behavior/state.**
tensorflow has hardcoded the dll name of CUDA and tries to detect version 10.1, but version 10.2 is the current.

```W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll ```

The current name is cudart64_102.dll

**Will this change the current api? How?**
no

**Who will benefit with this feature?**
all new users

**Any Other info.**
-
"
39246,GPU support for tf.image.resize with antialias=True,"**System information**
- TensorFlow version 2.1.0
- Python 3.7
- CUDA 10.1
- Ubuntu 18.04

**Describe the feature and the current behavior/state.**

Right now `tf.image.resize` returns a CPU Tensor after being called with `antialias=True`.

```python
import tensorflow as tf
with tf.device('/device:GPU:0'):
    for antialias in (True, False):
        x = tf.random.uniform((64, 32, 32, 1), maxval=1)
        x = tf.image.resize(images, (28, 28), antialias=antialias)
        print(antialias, x.device)
```

Prints:

```
True /job:localhost/replica:0/task:0/device:CPU:0
False /job:localhost/replica:0/task:0/device:GPU:0
```

When antialiasing is enabled, a CPU Tensor is returned. When it is disabled, a GPU tensor is returned.

**Will this change the current api? How?**

No.

**Who will benefit with this feature?**

Anyone who wants to rescale images without severe resampling artifacts.
"
39240,ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.,"I train faster RCNN for medical image classification in Google Colab. I get the following error and dont know how to fix it. This is the github page I use: https://github.com/you359/Keras-FasterRCNN/blob/master/README.md

```
Traceback (most recent call last):
  File ""train_frcnn.py"", line 145, in <module>
    shared_layers = nn.nn_base(img_input, trainable=True)
  File ""/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/resnet.py"", line 189, in nn_base
    x = FixedBatchNormalization(axis=bn_axis, name='bn_conv1')(x)
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 75, in symbolic_fn_wrapper
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 463, in __call__
    self.build(unpack_singleton(input_shapes))
  File ""/content/drive/My Drive/FasterRCNN/FASTER_RCNN_1.1/FASTER_RCNN/keras_frcnn/FixedBatchNormalization.py"", line 30, in build
    trainable=False)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 279, in add_weight
    weight = K.variable(initializer(shape, dtype=dtype),
  File ""/usr/local/lib/python3.6/dist-packages/keras/initializers.py"", line 46, in __call__
    return K.constant(1, shape=shape, dtype=dtype)
  File ""/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py"", line 649, in constant
    value, dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py"", line 970, in constant
    return constant_op.constant(value, dtype=dtype, shape=shape, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 262, in constant
    allow_broadcast=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 291, in _constant_impl
    return _eager_fill(shape.as_list(), t, ctx)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 52, in _eager_fill
    dims = convert_to_eager_tensor(dims, ctx, dtypes.int32)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor
    return ops.EagerTensor(value, ctx.device_name, dtype)
**ValueError: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.**
```

I could not find any threat that can help to fix this issue in the Google Colab. Thanks for any input. "
39238,Tensorflow with gpflow ,"Since tensorflow probability does not have a way of creating a custom kernel I decided to create a custom kernel on a tensorflow compatible library called gpflow and use that. 

I have found it necessary to manually add attributes to ensure that the kernel works but unfortunately i am getting the following error

'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'feature_ndims'

How would i go about adding this feature_ndims attribute to my kernel such that this goes away? "
39237,tf.keras.preprocessing.image.random_rotation makes a translation instead of rotation,"**System information**
standard tesorflow docker, tensorflow/tensorflow:2.1.0-py3-jupyter
v2.1.0-rc2-17-ge5bf8de 2.1.0

**Describe the current behavior**
![elephant](https://user-images.githubusercontent.com/26264000/81228345-f167f600-8fbb-11ea-8722-d25dcda78a0c.jpg)

![descarga](https://user-images.githubusercontent.com/26264000/81228466-1a888680-8fbc-11ea-8801-5e9061f489b5.png)


**Describe the expected behavior**

it should rotate the image 90 degrees

**Standalone code to reproduce the issue**
![elephant](https://user-images.githubusercontent.com/26264000/81228345-f167f600-8fbb-11ea-8722-d25dcda78a0c.jpg)

```python
import tensorflow as tf
import matplotlib.pyplot as plt
def show(im,im2): 
    plt.subplot(1,2,1) 
    plt.imshow(im) 
    plt.subplot(1,2,2) 
    plt.imshow(im2) 
    plt.show() 
    
img = tf.io.read_file('elephant.jpg')
img = tf.image.decode_jpeg(img, channels=0)/255
img = tf.image.resize(img, (320,320))
show(img,tf.keras.preprocessing.image.random_rotation(img,90))
```
if I try 
```python
show(img,tf.keras.preprocessing.image.random_rotation(img, rg=30,row_axis=0,col_axis=1,channel_axis=2))
```
I got the following error

```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-21-42509f257892> in <module>()
     11 img = tf.image.resize(img, (180,320))
     12 #np.rollaxis(img, 2, 0)
---> 13 show(img,tf.keras.preprocessing.image.random_rotation(img, rg=30,row_axis=1,col_axis=0,channel_axis=2))

2 frames
/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py in random_rotation(x, rg, row_axis, col_axis, channel_axis, fill_mode, cval, interpolation_order)
     56     x = apply_affine_transform(x, theta=theta, channel_axis=channel_axis,
     57                                fill_mode=fill_mode, cval=cval,
---> 58                                order=interpolation_order)
     59     return x
     60 

/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py in apply_affine_transform(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)
    321         transform_matrix = transform_matrix_offset_center(
    322             transform_matrix, h, w)
--> 323         x = np.rollaxis(x, channel_axis, 0)
    324         final_affine_matrix = transform_matrix[:2, :2]
    325         final_offset = transform_matrix[:2, 2]

<__array_function__ internals> in rollaxis(*args, **kwargs)

/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py in rollaxis(a, axis, start)
   1272     axes.remove(axis)
   1273     axes.insert(start, axis)
-> 1274     return a.transpose(axes)
   1275 
   1276 

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'transpose'

```

[](https://colab.research.google.com/drive/1x8ySj1tt5zA8woEU6NUIH0AJqmYDgIop#scrollTo=sqxsXx6kxde2)
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


"
39236,TF Hub BERT model output names mismatch with the singature.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 2.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

```
# model is from: https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/1

saved_model_dir = ""path/to/bert_en_cased_L-12_H-768_A-12/1""
loaded_model = tf.saved_model.load(saved_model_dir)
concrete_func = loaded_model.signatures[""serving_default""]
prediction = concrete_func(**inputs)
```

Expected output names:
```
concrete_fn.outputs
Out[4]: 
[<tf.Tensor 'Identity:0' shape=(None, 768) dtype=float32>,
 <tf.Tensor 'Identity_1:0' shape=(None, None, 768) dtype=float32>]
```

**However, actual prediction returns a dict with different output names.**

```
prediction.keys()
Out[5]: [u'bert_model_1', u'bert_model']
```

**Describe the expected behavior**

Expecting the prediction output has same name as `concrete_func.outputs`."
39235,Attension Layer Tensorflow TypeError: Cannot iterate over a tensor with unknown first dimension,"I am trying to apply attention Layer to CRNN Econder and Decoder
but there is an error cant understand what it means?



Use distribution to create a linear combination of value with shape batch_size, Tq, dim]: return tf.matmul(distribution, value).
    cnn = MaxPooling2D(pool_size=(1,2), strides=(1,2), padding=""valid"")(cnn)
    shape = cnn.get_shape()
    nb_units = shape[2] * shape[3]

    gru = Reshape((shape[1], nb_units))(cnn)
    gru, forward_h, forward_c, backward_h, backward_c = Bidirectional \
    (LSTM
     (nb_units,
      dropout=0.2,
      return_sequences=True,
      return_state=True,
      recurrent_activation='relu',
      recurrent_initializer='glorot_uniform'))(gru)

    state_h = tf.keras.layers.Concatenate()([forward_h, backward_h])
    state_c = tf.keras.layers.Concatenate()([forward_c, backward_c])
    gru, attention_weights = Attention(32)([gru, state_h])
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-10-1b228a635105> in <module>()
      4 # note: `learning_rate=None` will get architecture default value
      5 model = HTRModel(architecture=arch, input_size=input_size, vocab_size=dtgen.tokenizer.vocab_size)
----> 6 model.compile(learning_rate=0.001)
      7 
      8 # save network summary

C:\Users\a.abdallah\Desktop\handwritten-text-recognition\src\network\model.py in compile(self, learning_rate)
    134 
    135         # define inputs, outputs and optimizer of the chosen architecture
--> 136         outs = self.architecture(self.input_size, self.vocab_size + 1, learning_rate)
    137         inputs, outputs, optimizer = outs
    138 

C:\Users\a.abdallah\Desktop\handwritten-text-recognition\src\network\model.py in abdo(input_size, d_model, learning_rate)
    571 
    572     
--> 573     gru, attention_weights = Attention(32)([gru, state_h])
    574 
    575     

c:\users\a.abdallah\anaconda3\envs\handwritten\lib\site-packages\tensorflow_core\python\framework\ops.py in __iter__(self)
    546     if shape[0] is None:
    547       raise TypeError(
--> 548           ""Cannot iterate over a tensor with unknown first dimension."")
    549     for i in xrange(shape[0]):
    550       yield self[i]

TypeError: Cannot iterate over a tensor with unknown first dimension.
"
39234,Symbol not found during bazel build,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX mojave 10.14.6 18.7.0 Darwin Kernel Version 18.7.0 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source commit 70087ab4f46a4bebaacce1023cd12bd9c655e159 (HEAD -> rc2.2, tag: v2.2.0-rc4, origin/r2.2) 
- TensorFlow version: 2.2
- Python version:Python 3.6.10 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): bazel 2.0.0 
- GCC/Compiler version (if compiling from source):Target: x86_64-apple-darwin18.7.0
- CUDA/cuDNN version: NA
- GPU model and memory:NA



**Describe the problem**
When trying to bazel build --config=opt --copt=-g //tensorflow/tools/pip_package:build_pip_package , I get this error. 
```
ImportError: dlopen(/private/var/tmp/_bazel_vikumar/03ac0190026ff3e87deaafab7f769284/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: __ZN10tensorflow4data12experimental14SnapshotReader33kSnappyReaderInputBufferSizeBytesE
  Referenced from: /private/var/tmp/_bazel_vikumar/03ac0190026ff3e87deaafab7f769284/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /private/var/tmp/_bazel_vikumar/03ac0190026ff3e87deaafab7f769284/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_tf_python_api_gen_v2.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so
```

I explicitly tried changing snapshot_util.h(where kSnappyReaderInputBufferSizeByte is defined) to force recompile, but still fails with same above error. 
```
index a2df3ccec1..0488136b2b 100644
--- a/tensorflow/core/kernels/data/experimental/snapshot_util.h
+++ b/tensorflow/core/kernels/data/experimental/snapshot_util.h
@@ -70,6 +70,7 @@ class SnapshotReader {
   // The reader input buffer size is deliberately large because the input reader
   // will throw an error if the compressed block length cannot fit in the input
   // buffer.
+   static constexpr const int v =1; 
```



My tf.configure.bazelrc file contents
```
(tb2-2) 38f9d370d4a3:tensorflow vikumar$ cat .tf_configure.bazelrc 
build --action_env PYTHON_BIN_PATH=""/Users/vikumar/anaconda3/envs/tb2-2/bin/python""
build --action_env PYTHON_LIB_PATH=""/Users/vikumar/anaconda3/envs/tb2-2/lib/python3.6/site-packages""
build --python_path=""/Users/vikumar/anaconda3/envs/tb2-2/bin/python""
build --config=xla
build:opt --copt=-g
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-gpu,-nomac,-no_mac,-v1only
build --action_env TF_CONFIGURE_IOS=""0""
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel clean --expunge 
./configure
bazel build --config=opt --copt=-g //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39232,Upgrade/Provide Docker Images based on Ubuntu 20.04 LTS,Upgrade/Provide Docker Images based on Ubuntu 20.04 LTS
39230,tf.keras.losses.categorical_crossentropy and binary_crossentropy (and other losses) only works for channels_last layout networks,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip3
- TensorFlow version (use command below): v2.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

Looks like in a bizarre api design the constructor of neither `tf.keras.losses.categorical_crossentropy` nor `tf.keras.losses.binary_crossentropy` take an axis parameter as input. An axis specifies along which dimensions one wants to calculate these losses. Current implementation simply assumes last axis. Without a way to specify a different axis in the constructor, these can only be used for networks where `image_data_layout` is set to `channels_last`. `channels_first` network are out of luck because they require different axis. 

Also inconsistent api design is evident in `tf.keras.backend.categorical_crossentropy`. This backend version of categorical_crossentropy does that an axis as an input in its constructor. Though one can use this as a loss function as a workaround, it still does not solve the binary cross entropy problem. And also results in an overall inconsistent api.

The `tf.keras.losses.categorical_crossentropy` is actually calling the backend version but since axis was an optional parameter, it gets set by default to -1 all the time. See [this line in the losses.py code](https://github.com/tensorflow/tensorflow/blob/e5bf8de410005de06a7ff5393fafdf832ef1d4ad/tensorflow/python/keras/losses.py#L971)

**Describe the expected behavior**

 `tf.keras.losses.categorical_crossentropy` and `tf.keras.losses.binary_crossentropy` (and may be all other loss functions) should take an axis parameter as input. So that these losses can be used in channels_first setups.

**Standalone code to reproduce the issue**
None needed. See the documentation of these loss functions and see the source code link above.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39229,map_fn doesn't work with empty lists,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.2.0-rc4-0-g70087ab4f4
- Python version: 3.6.9

**Describe the current behavior**
map_fn doesn't support empty lists

**Describe the expected behavior**
It should return an empty list

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
fn = lambda x: x
tf.map_fn(fn, [])

# additionally, this works:
tf.map_fn(fn, np.array([1.]))
# but not this eventhough [1.] is not a scalar.
tf.map_fn(fn, [1.])
```"
39228,Cannot build TFLite with OpenCL SYCL support,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow version: master (May 6th, 2020)
- Bazel version (if compiling from source): 2.0.0

**Describe the problem**
Cannot build TFLite with OpenCL SYCL support. Not with GCC (version 7.5.0). Not with Clang (version 10.0.0).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Using GCC (version 7.5.0)
```
terminal: python3 configure.py
Extracting Bazel installation...
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/local/lib/python3.6/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.6/dist-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y
OpenCL SYCL support will be enabled for TensorFlow.

Please specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]:


Please specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]:


Do you wish to build TensorFlow with ComputeCPP support? [Y/n]: y
ComputeCPP support will be enabled for TensorFlow.

Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: /opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu


Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
terminal: bazel build -c opt --config=sycl //tensorflow/lite:libtensorflowlite.so
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [sycl]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=275
INFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.6/dist-packages --python_path=/usr/bin/python3 --config=xla --config=sycl --action_env HOST_CXX_COMPILER=/usr/bin/g++ --action_env HOST_C_COMPILER=/usr/bin/gcc --action_env TF_NEED_COMPUTECPP=1 --action_env COMPUTECPP_TOOLKIT_PATH=/opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /media/ssd512/tensorflow-master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /media/ssd512/tensorflow-master/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1
INFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1
INFO: Found applicable config definition build:linux in file /media/ssd512/tensorflow-master/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /media/ssd512/tensorflow-master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):
 - /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_toolchains/repositories/repositories.bzl:37:9
 - /media/ssd512/tensorflow-master/WORKSPACE:37:1
ERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: @local_config_sycl//crosstool:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule
ERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'
ERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:5:1: Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'
ERROR: /media/ssd512/tensorflow-master/tensorflow/lite/BUILD:574:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis failed
INFO: Elapsed time: 11.888s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (24 packages loaded, 53 targets configured)
```
Using Clang (version 10.0.0)
```
terminal: python3 configure.py
Extracting Bazel installation...
You have bazel 2.0.0 installed.
Please specify the location of python. [Default is /usr/bin/python3]:


Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: y
OpenCL SYCL support will be enabled for TensorFlow.

Please specify which C++ compiler should be used as the host C++ compiler. [Default is /usr/bin/g++]: /opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang++


Please specify which C compiler should be used as the host C compiler. [Default is /usr/bin/gcc]: /opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang


Do you wish to build TensorFlow with ComputeCPP support? [Y/n]: y
ComputeCPP support will be enabled for TensorFlow.

Please specify the location where ComputeCpp for SYCL 1.2 is installed. [Default is /usr/local/computecpp]: /opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu


Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]:


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
terminal: bazel build -c opt --config=sycl //tensorflow/lite:libtensorflowlite.so
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [sycl]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=275
INFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from /media/ssd512/tensorflow-master/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3/dist-packages --python_path=/usr/bin/python3 --config=xla --config=sycl --action_env HOST_CXX_COMPILER=/opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang++ --action_env HOST_C_COMPILER=/opt/clang+llvm-10.0.0-x86_64-linux-gnu-ubuntu-18.04/bin/clang --action_env TF_NEED_COMPUTECPP=1 --action_env COMPUTECPP_TOOLKIT_PATH=/opt/ComputeCpp-CE-2.0.0-x86_64-linux-gnu --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file /media/ssd512/tensorflow-master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /media/ssd512/tensorflow-master/.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1
INFO: Found applicable config definition build:sycl in file /media/ssd512/tensorflow-master/.bazelrc: --crosstool_top=@local_config_sycl//crosstool:toolchain --define=using_sycl=true --action_env TF_NEED_OPENCL_SYCL=1
INFO: Found applicable config definition build:linux in file /media/ssd512/tensorflow-master/.bazelrc: --copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /media/ssd512/tensorflow-master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Call stack for the definition of repository 'io_bazel_rules_docker' which is a git_repository (rule definition at /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_tools/tools/build_defs/repo/git.bzl:195:18):
 - /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/bazel_toolchains/repositories/repositories.bzl:37:9
 - /media/ssd512/tensorflow-master/WORKSPACE:37:1
ERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: @local_config_sycl//crosstool:cc-compiler-local: missing value for mandatory attribute 'toolchain_config' in 'cc_toolchain' rule
ERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:12:1: Target '@local_config_sycl//crosstool:empty' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:cc-compiler-local'
ERROR: /home/lotte/.cache/bazel/_bazel_lotte/984900b31b80e1f2c94e5989b5f952f7/external/local_config_sycl/crosstool/BUILD:5:1: Target '@local_config_sycl//crosstool:cc-compiler-local' contains an error and its package is in error and referenced by '@local_config_sycl//crosstool:toolchain'
ERROR: /media/ssd512/tensorflow-master/tensorflow/lite/BUILD:574:1: every rule of type cc_binary implicitly depends upon the target '@local_config_sycl//crosstool:toolchain', but this target could not be found because of: Target '@local_config_sycl//crosstool:toolchain' contains an error and its package is in error
ERROR: Analysis of target '//tensorflow/lite:libtensorflowlite.so' failed; build aborted: Analysis failed
INFO: Elapsed time: 12.664s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (23 packages loaded, 55 targets configured)
```"
39227,FailedPreconditionError:  Error while reading resource variable _AnonymousVar555 from Container: localhost.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow version (use command below):2.2.0-rc-4


**Describe the current behavior**

> FailedPreconditionError:  Error while reading resource variable _AnonymousVar555 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar555/N10tensorflow3VarE does not exist.
> 	 [[node MatMul_388/ReadVariableOp (defined at <ipython-input-86-300e1b0d20e1>:30) ]] [Op:__inference_keras_scratch_graph_21594]
> 
> Function call stack:
> keras_scratch_graph

At building Model


**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1IYvi-iIM2gcTVBPhE2K2qjTVUURyZvIi

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39226,Inputs mismatch in model.predict does not raise Error,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Catalina 10.15.4
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.5



**Describe the current behavior**
I wrote a simple model that takes 1 input array and outputs another. I produce then 2 input arrays, one with the right dimension and the other with wrong dimensions. If you feed to model.predict the 2 input arrays, the correct one first, the model simply takes the first array and the run goes fine. The second input is completely ignored.

**Describe the expected behavior**
I would expect an error like ""You are giving two input arrays to a model that expects one""

**Standalone code to reproduce the issue**

```
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
import numpy as np


def generator(input_dim, output_dim):
        # Input
        g_input = Input(input_dim, name='noise_input')

        # Dense block
        x = Dense(32)(g_input)

        # Final layer
        g_output = Dense(output_dim)(x)

        G = Model(g_input, g_output, name='Generator')
        return G


input_dim = 6
output_dim = 2

gen = generator(input_dim, output_dim)

sample_size = 5

noise_data_1 = np.random.normal(0, 1, size=(sample_size, input_dim))
noise_data_2 = np.random.normal(0, 1, size=(sample_size, 1))


out = gen.predict([noise_data_1, noise_data_2])

print(out)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39224,Use tf.data and tf.keras in multitasking (each task uses a different custom loss),"

**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.0
- Python version:3.6


**description**
**step 1)** I used tf.keras to build a single-input dual-output multi-task model, each output loss is customized (by inheriting tf.keras.losses.Loss).
<img width=""174"" alt=""image"" src=""https://user-images.githubusercontent.com/15835439/81172546-069b4f80-8fd1-11ea-881a-93fecf39992f.png"">
**step 2)** Construct a data set through tf.data-(image, (label1, label2)).
**step 3)** 
<img width=""359"" alt=""image"" src=""https://user-images.githubusercontent.com/15835439/81172981-d7d1a900-8fd1-11ea-97c9-85933f1deaa4.png"">
<img width=""176"" alt=""image"" src=""https://user-images.githubusercontent.com/15835439/81172995-df914d80-8fd1-11ea-9852-aff980257594.png"">
**bug**

I found that the two outputs of the network were sent to the corresponding loss functions, but the corresponding label1 and label2 in tf.data were not sent to the two loss functions
 

"
39223,Binary classification using Keras always give wrong predictions: The acc is always 0.5,"Hi~ I am using Keras to make a simple binary classification. And I am using TF as backend.

I checked:

- data shuffle: I set the param in model.fit() shuffle = True
- network structure: The NN take a vector with 1024 elements and makes a prediction 0 or 1.

ENV: 

- tensorflow 1.13.2 
- Keras 2.2.4
- Keras-applications 1.0.8
- Keras-Processing 1.1.0
- Tensorflow-estimator 1.13.0
- Ubuntu 16.04 
- python3


But the output is still wrong. The acc is always 0.5.

Some people ran my code and get the acc 0.9857 after 1 epoch.
See here:
![image](https://user-images.githubusercontent.com/22954901/81170557-79a2c700-8fcd-11ea-978e-72e2bc192f95.png)

[https://stackoverflow.com/questions/61633602/binary-classification-using-keras-always-give-wrong-predictions-the-acc-is-alwa?noredirect=1#comment109021786_61633602](https://stackoverflow.com/questions/61633602/binary-classification-using-keras-always-give-wrong-predictions-the-acc-is-alwa?noredirect=1#comment109021786_61633602)


```
import tensorflow as tf
from tensorflow.keras.layers import Input, Flatten, Dense, Lambda, Conv2D, Reshape, MaxPool2D, Average, Dropout, Concatenate, \
    Add, Maximum, Layer, Activation, Conv1D, TimeDistributed, GlobalAvgPool2D
import numpy as np


class Test(tf.keras.Model):
    def __init__(self,attention_sz,dropout_rt, name=None):
        super(Test, self).__init__(name=name)
        # here we define the layer:
        self.fc = Dense(attention_sz,input_dim = attention_sz ,activation='relu')
        self.fc2 = Dense(attention_sz, activation='relu')
        self.fc3 = Dense(1, activation='sigmoid')

        self.dp = Dropout(dropout_rt,input_shape=(attention_sz,))
        self.dp2 = Dropout(dropout_rt,input_shape=(attention_sz,))


    def call(self, inp):
        # here we get the segmentation and pose
        with tf.device('/gpu:0'):
            print(""~~~~~~~~~~~"")
            x = self.fc(inp)
            print(x.shape)
            z = self.dp(x)
            print(z.shape)
            x = self.fc2(z)
            print(x.shape)
            z = self.dp2(x)
            print(z.shape)
            y = self.fc3(z)
            print(y.shape)
        return y 

if __name__ == '__main__':
    model  = Test(1024, 0.05)
    model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    x = np.round(np.random.normal(1.75, 0.2, size=(10000, 1024)), 2)
    x2 = np.round(np.random.normal(100.75, 0.2, size=(10000, 1024)), 2)
    labels = np.zeros((10000, 1))
    labels2 = np.ones((10000, 1))

    x_t = np.row_stack((x, x2))
    labels = np.row_stack((labels,labels2))
    print(x_t.shape)
    print(labels.shape)
    model.fit(x_t, labels, shuffle=True, epochs=10, batch_size=32)
    x = np.round(np.random.normal(1.75, 0.2, size=(1, 1024)), 2)
    y = np.round(np.random.normal(100.75, 0.2, size=(1, 1024)), 2)
    res = model.predict(x)
    print(res)
    print(res.shape)
    res = model.predict(y)
    print(res)
    print(res.shape)

```
output:
```
WARNING:tensorflow:From /home/frank/Desktop/mesh-py3/my_venv/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
2020-05-06 19:00:58.440615: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-06 19:00:58.616327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-06 19:00:58.617158: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55201b0 executing computations on platform CUDA. Devices:
2020-05-06 19:00:58.617175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2080, Compute Capability 7.5
2020-05-06 19:00:58.636996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz
2020-05-06 19:00:58.637508: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x558add0 executing computations on platform Host. Devices:
2020-05-06 19:00:58.637523: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2020-05-06 19:00:58.637876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2080 major: 7 minor: 5 memoryClockRate(GHz): 1.095
pciBusID: 0000:01:00.0
totalMemory: 7.77GiB freeMemory: 7.06GiB
2020-05-06 19:00:58.637892: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-05-06 19:00:58.639694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-06 19:00:58.639708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-05-06 19:00:58.639713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-05-06 19:00:58.639923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6868 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)
Epoch 1/10
2020-05-06 19:00:59.495123: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
20000/20000 [==============================] - 3s 148us/sample - loss: 8.0497 - acc: 0.4997
Epoch 2/10
20000/20000 [==============================] - 2s 98us/sample - loss: 8.0590 - acc: 0.5000
Epoch 3/10
20000/20000 [==============================] - 2s 99us/sample - loss: 8.0590 - acc: 0.5000
Epoch 4/10
20000/20000 [==============================] - 2s 80us/sample - loss: 8.0590 - acc: 0.5000
Epoch 5/10
20000/20000 [==============================] - 2s 81us/sample - loss: 8.0590 - acc: 0.5000
Epoch 6/10
20000/20000 [==============================] - 2s 80us/sample - loss: 8.0590 - acc: 0.5000
Epoch 7/10
20000/20000 [==============================] - 2s 89us/sample - loss: 8.0590 - acc: 0.5000
Epoch 8/10
20000/20000 [==============================] - 2s 83us/sample - loss: 8.0590 - acc: 0.5000
Epoch 9/10
20000/20000 [==============================] - 2s 78us/sample - loss: 8.0590 - acc: 0.5000
Epoch 10/10
20000/20000 [==============================] - 2s 79us/sample - loss: 8.0590 - acc: 0.5000
[[0.]]
(1, 1)
[[0.]]
(1, 1)

Process finished with exit code 0

```
Thanks in advance!"
39222,Typos in source code docs,"[Here](https://github.com/tensorflow/tensorflow/blob/fedc6d951faa73936a1154d6507d54240614d416/tensorflow/python/eager/backprop.py#L532) a minor typo in the source code: **rturns** ==> **returns**
"
39221,Error in load/save models with FeatureLayer when refitting or reloading,"Hello,

My model has FeatureLayer and i want do flow like this:

1. Create, fit model  on basic data and save.
2. Load model and fit on new data and save updated model.
3. Do 2 step periodically e.g. 1 time in week.

When I do model  first time, it's no problem, also I can use it in tensorflow-serving and it works.
Then I can load this model, fit on new data and save updated model. But when I try to load updated model i have an error:

` raise ValueError('Passing a dictionary input to a Sequential Model '
ValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.`

In tf v 2.0.0 problem is when load model:
```
model = tf_load.load_internal(path, loader_cls=KerasObjectLoader)
File ""/usr/lib64/python3.6/site-packages/tensorflow_core/python/saved_model/load.py"", line 541, in load_internal
export_dir)
File ""/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 103, in __init__
self._finalize()
File ""/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 132, in _finalize
node._set_inputs(inputs)
File ""/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2697, in _set_inputs
inputs = self._set_input_attrs(inputs)
File ""/usr/lib64/python3.6/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
result = method(self, *args, **kwargs)
File ""/usr/lib64/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py"", line 2731, in _set_input_attrs
raise ValueError('Passing a dictionary input to a Sequential Model '
ValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.
```
I tested it and it's not problem with fitting model but with save or load, becouse when I only load first model and save as new model (without any interference in model) the error occurs when i try load updated model.

In version 2.1.0 its not error when load updated model, but when I try to fit. Final error is the same in both cases
```
 File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 819, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 235, in fit
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 593, in _process_training_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 706, in _process_inputs
    use_multiprocessing=use_multiprocessing)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\data_adapter.py"", line 702, in __init__
    x = standardize_function(x)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py"", line 660, in standardize_function
    standardize(dataset, extract_tensors_from_dataset=False)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2346, in _standardize_user_data
    all_inputs, y_input, dict_inputs = self._build_model_with_inputs(x, y)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2572, in _build_model_with_inputs
    self._set_inputs(cast_inputs)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2647, in _set_inputs
    inputs = self._set_input_attrs(inputs)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\training\tracking\base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""C:\Users\kglapiak\AppData\Local\Continuum\anaconda3\envs\tf_test2\lib\site-packages\tensorflow_core\python\keras\engine\training.py"", line 2681, in _set_input_attrs
    raise ValueError('Passing a dictionary input to a Sequential Model '
ValueError: Passing a dictionary input to a Sequential Model which doesn't have FeatureLayer as the first layer is an error.
```


I save model by mehod:
`model.save('original_model',save_format='tf')`

Example of model:
```
feature_columns = []

categorical = tf.feature_column.categorical_column_with_vocabulary_list(
        key='f1', vocabulary_list=['x1','x2','x3'],
        num_oov_buckets=0)
one_hot = feature_column.indicator_column(categorical)
feature_columns.append(one_hot)

categorical = tf.feature_column.categorical_column_with_vocabulary_list(
        key='f2', vocabulary_list=['x1','x2','x3','x4','x5','x6'],
        num_oov_buckets=5)
embedding = feature_column.embedding_column(categorical, dimension=10)
feature_columns.append(embedding)

feature_layer = tf.keras.layers.DenseFeatures(feature_columns)

model = tf.keras.Sequential([
  feature_layer,
  layers.Dense(20, activation='relu'),
  layers.Dense(1,activation='softsign')
])
    
model.compile(optimizer='adam',
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=['accuracy'])


def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    dataframe = dataframe.copy()
    labels = dataframe.pop('target')
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size)
    return ds

train = df_to_dataset(train)
val = df_to_dataset(val)

model.fit(train,
          validation_data=val,
          epochs=2)
```

I tested it on conda and pip distributions, on tf versions 2.0.0 and 2.1.0 and on Windows and Centos and several versions of Python and all time is the same problem.


- OS Platform and Distribution (Windows, Centos):
- TensorFlow version (2.0.0,2.1.0 also pip and conda distridutions ):
- Python version: several versions from  3.6.5 to 3.7.1
"
39219,How to release CPU memory after session.close() java api,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (linux centOS 7):
- TensorFlow installed from (source or binary):
- TensorFlow version (1.14.0):
- javaversion:java8

I use java to load tensorflow model，When I update the model，I close session and graph like that：session.close() & graph.close(), 
But the memory is not released，How could I release CPU memory timely to avoid OOM error please?
Thanks!"
39218,TF 2 libraries for Arduino IDE,"@tensorflow/micro

The arduino library managers only provide tf 1.14 and 1.15 build-in libraries and hope tensorflow team could provide the tf 2 libraries for Arduino IDE."
39217,DLL load failed,"(comp1) E:\Computer Vision\Projects\face-mask-detector>python train_mask_detector.py --dataset dataset
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_mask_detector.py"", line 5, in <module>
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\ProgramData\Anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
39215,TF: Model SubCassing | AttributeError: 'NoneType' object has no attribute 'compile',"**System information**
- System: Kaggle Kernel
- TensorFlow version: 2.1.0
- Python version: 3.6.6
- TPU model and memory: TPU V3-8


I am trying to implement sub-classing for model definition but face a following error:

```python
def my_model(Model):
    def __init__(self, dim):
        super(my_model, self).__init__(**kwargs)
        self.efnet  = efn.EfficientNetB0(input_shape=(dim, 3), include_top = False, weights = 'imagenet')
        self.gap    = L.GlobalAveragePooling2D()
        self.bn     = L.BatchNormalization()
        self.denseA = L.Dense(784, activation='relu', name = 'dense_A')
        self.out    = L.Dense(1, activation='sigmoid')
    
    def call(self, inputs):
        x     = self.efnet(inputs)
        x_gap = self.gap(x)
        bn    = self.bn(x_gap)
        den_A = self.denseA(bn)
        drop  = self.drop(den_A)
        return self.out(drop)

dim = (124,124)
model = my_model((dim)
```

I understand I don't load the model properly, but I think it should load in this way. I am trying on this on Kaggle tpu with a proper strategy scheme (though I didn't mention that syntax in the above snippet). Any catch?"
39214,Node name obfuscation in TF 2.X,"**System information**
- TensorFlow version (you are using): **2.2.0-rc3**
- Are you willing to contribute it (Yes/No): **Yes**

**Describe the feature and the current behavior/state.**
It seems like the [Graph Transform Tool](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms) is now deprecated (https://github.com/tensorflow/tensorflow/issues/33352) in TF 2.X and Grappler will take care of graph optimization. Unfortunately, [obfuscate_names](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#obfuscate_names) is missing and we could not find an alternative way for node name obfuscation in TF 2.X.

**Will this change the current api? How?**
Not sure. Basically there are 3 possible approaches:

1. Grappler: AFAIK, saving a `saved_model` is in fact saving a snapshot of a Grappler-optimized concrete function. So it might be possible to add a new option to control the behavior?

2. Graph Transform Tool: Reviving Graph Transform Tool and support TF 2.X. However, I don't think this is a good idea since most graph optimizations are now handled by Grappler, and `saved_model` is now the de facto standard format. There is a PR https://github.com/tensorflow/tensorflow/pull/28099 working on `saved_model` support for Graph Transform Tool, but I'm not sure if it is compatible with TF 2.X.

3. Workarounds: Manually obfuscate names *after* training/debugging and *before* saving a model. This approach will add a lot of boilerplate code and global vars, and is way less flexible comparing to 1 & 2. Most importantly, changing `name`s of ops cannot strip the `/` namespaces added by Keras layers, which will expose the hierarchical structure of the model.

**Who will benefit with this feature?**
Developers who build proprietary on-premise softwares, or embed models in mobile apps.

AOT compilation might not work for every situation, node name obfuscation could at least provide a certain degree of protection against reverse engineering."
39212,Full screen camera needed for pose estimation androie,Provide code for full screen camera (now only half screen camera ) shown on pose estimation android .Please provide the code for change to full screen 
39211,Full screen camera needed for tensor flow pose estimation android,Provide full screen camera for pose estimation android
39210,[tf-dbg] source_utils_test fails in Python 3.8,"TF version hash: 47ee0d08b22e79b84eb442ccb75ef8e4ccd5ec07

See the following test log:
```
Test output for //tensorflow/python/debug:source_utils_test:
/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/random_ops.py:287: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
  minval_is_zero = minval is 0  # pylint: disable=literal-comparison
/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/random_ops.py:288: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
  maxval_is_one = maxval is 1  # pylint: disable=literal-comparison
/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:84: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
  if (default_value.shape.ndims is not 0
/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/ops/ragged/ragged_batch_gather_with_default_op.py:85: SyntaxWarning: ""is not"" with a literal. Did you mean ""!=""?
  and default_value.shape.ndims is not 1):
Running tests under Python 3.8.2: /usr/bin/python3.8
[ RUN      ] GuessIsTensorFlowLibraryTest.testDebuggerExampleFilePathReturnsFalse
[       OK ] GuessIsTensorFlowLibraryTest.testDebuggerExampleFilePathReturnsFalse
[ RUN      ] GuessIsTensorFlowLibraryTest.testFileInPythonKernelsPathReturnsTrue
[       OK ] GuessIsTensorFlowLibraryTest.testFileInPythonKernelsPathReturnsTrue
[ RUN      ] GuessIsTensorFlowLibraryTest.testGuessedBaseDirIsProbablyCorrect
[       OK ] GuessIsTensorFlowLibraryTest.testGuessedBaseDirIsProbablyCorrect
[ RUN      ] GuessIsTensorFlowLibraryTest.testNonPythonFileRaisesException
[       OK ] GuessIsTensorFlowLibraryTest.testNonPythonFileRaisesException
[ RUN      ] GuessIsTensorFlowLibraryTest.testSourceUtilModuleReturnsTrue
[       OK ] GuessIsTensorFlowLibraryTest.testSourceUtilModuleReturnsTrue
[ RUN      ] GuessIsTensorFlowLibraryTest.testUnitTestFileReturnsFalse
[       OK ] GuessIsTensorFlowLibraryTest.testUnitTestFileReturnsFalse
[ RUN      ] GuessIsTensorFlowLibraryTest.test_session
WARNING:tensorflow:From /usr/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
W0506 00:11:11.384973 140211243161344 deprecation.py:317] From /usr/lib/python3.8/contextlib.py:83: TensorFlowTestCase.test_session (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.
Instructions for updating:
Use `self.session()` or `self.cached_session()` instead.
[       OK ] GuessIsTensorFlowLibraryTest.test_session
[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceList
[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceList
[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceListWithNodeNameFilter
[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceListWithNodeNameFilter
[ RUN      ] ListSourceAgainstDumpTest.testGenerateSourceListWithPathRegexFilter
[  SKIPPED ] ListSourceAgainstDumpTest.testGenerateSourceListWithPathRegexFilter
[ RUN      ] ListSourceAgainstDumpTest.test_session
[  SKIPPED ] ListSourceAgainstDumpTest.test_session
[ RUN      ] SourceHelperTest.testAnnotateDumpedTensorsGivesCorrectResult
2020-05-06 00:11:11.473613: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-05-06 00:11:11.489470: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2500005000 Hz
2020-05-06 00:11:11.497351: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28121b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-06 00:11:11.497393: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/autograph/utils/testing.py:21: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
[       OK ] SourceHelperTest.testAnnotateDumpedTensorsGivesCorrectResult
[ RUN      ] SourceHelperTest.testAnnotateSubsetOfLinesGivesCorrectResult
[       OK ] SourceHelperTest.testAnnotateSubsetOfLinesGivesCorrectResult
[ RUN      ] SourceHelperTest.testAnnotateWholeValidSourceFileGivesCorrectResult
[  FAILED  ] SourceHelperTest.testAnnotateWholeValidSourceFileGivesCorrectResult
[ RUN      ] SourceHelperTest.testAnnotateWithStackTopGivesCorrectResult
[  FAILED  ] SourceHelperTest.testAnnotateWithStackTopGivesCorrectResult
[ RUN      ] SourceHelperTest.testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError
[       OK ] SourceHelperTest.testCallingAnnotateSourceOnUnrelatedSourceFileDoesNotError
[ RUN      ] SourceHelperTest.testCallingAnnotateSourceWithoutPythonGraphRaisesException
[       OK ] SourceHelperTest.testCallingAnnotateSourceWithoutPythonGraphRaisesException
[ RUN      ] SourceHelperTest.testLoadNonexistentNonParPathFailsWithIOError
[       OK ] SourceHelperTest.testLoadNonexistentNonParPathFailsWithIOError
[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileInParFileFailsRaisingIOError
[       OK ] SourceHelperTest.testLoadingPythonSourceFileInParFileFailsRaisingIOError
[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileInParFileSucceeds
[       OK ] SourceHelperTest.testLoadingPythonSourceFileInParFileSucceeds
[ RUN      ] SourceHelperTest.testLoadingPythonSourceFileWithNonAsciiChars
[       OK ] SourceHelperTest.testLoadingPythonSourceFileWithNonAsciiChars
[ RUN      ] SourceHelperTest.test_session
[       OK ] SourceHelperTest.test_session
======================================================================
ERROR: testAnnotateWholeValidSourceFileGivesCorrectResult (__main__.SourceHelperTest)
testAnnotateWholeValidSourceFileGivesCorrectResult (__main__.SourceHelperTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/debug/lib/source_utils_test.py"", line 159, in testAnnotateWholeValidSourceFileGivesCorrectResult
    source_annotation[self.u_init_line_number])
KeyError: 116

======================================================================
ERROR: testAnnotateWithStackTopGivesCorrectResult (__main__.SourceHelperTest)
testAnnotateWithStackTopGivesCorrectResult (__main__.SourceHelperTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/bazel-buildfarm/default/operations/cf843b16-8c97-4552-8d55-fd16fe09d4f4/bazel-out/k8-opt/bin/tensorflow/python/debug/source_utils_test.runfiles/org_tensorflow/tensorflow/python/debug/lib/source_utils_test.py"", line 181, in testAnnotateWithStackTopGivesCorrectResult
    source_annotation[self.u_init_line_number])
KeyError: 116

----------------------------------------------------------------------
Ran 22 tests in 1.288s

FAILED (errors=2, skipped=4)
```

cc @caisq "
39209,download_dependencies.sh failure: Usage: download_and_extract URL DIR,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Description:	Ubuntu 18.04.4 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
cloned today
- Python version:

ii  python         2.7.15~rc1-1 amd64        interactive high-level object-ori

- Installed using virtualenv? pip? conda?:
no

- Bazel version (if compiling from source):
none
- GCC/Compiler version (if compiling from source):
ii  python         2.7.15~rc1-1 amd64        interactive high-level object-ori

- CUDA/cuDNN version:
none
- GPU model and memory:
inxi -G
Graphics:  Card: Advanced Micro Devices [AMD/ATI] Cayman PRO [Radeon HD 6950]
           Display Server: x11 (X.Org 1.20.5 ) driver: radeon
           Resolution: 1920x1080@60.00hz
           OpenGL: renderer: AMD CAYMAN (DRM 2.50.0 / 5.3.0-51-generic, LLVM 9.0.0)
           version: 4.3 Mesa 19.2.8



**Describe the problem**
trying to build deepbacksub which depends on tensorflow lite.

build fails with:
./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR

with ""set -x -v"" added to 2nd line of script, we see it is the eigen package, which is the first one.
download_and_extract ""${EIGEN_URL}"" ""${DOWNLOADS_DIR}/eigen""
+ download_and_extract '' tensorflow/lite/tools/make/downloads/eigen
+ local 'usage=Usage: download_and_extract URL DIR'
./download_dependencies.sh: line 60: 1: Usage: download_and_extract URL DIR


cd /usr/local/src/deepbacksub/tensorflow
grep -o 'http.*bitbucket.org/eigen/eigen/get/.*tar\.gz' ""tensorflow/workspace.bzl"" | grep -v mirror.tensorflow | head -n1
[nada]
 cat tensorflow/workspace.bzl | fgrep -i eigen
        name = ""eigen_archive"",
        build_file = clean_dep(""//third_party:eigen.BUILD""),
        patch_file = clean_dep(""//third_party/eigen3:gpu_packet_math.patch""),
        strip_prefix = ""eigen-4e696901f873a2347f76d931cf2f701e31e15d05"",
            ""https://storage.googleapis.com/mirror.tensorflow.org/gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz"",
            ""https://gitlab.com/libeigen/eigen/-/archive/4e696901f873a2347f76d931cf2f701e31e15d05/eigen-4e696901f873a2347f76d931cf2f701e31e15d05.tar.gz"",



**Provide the exact sequence of commands / steps that you executed before running into the problem**



sudo apt-get install libopencv-dev opencv-doc
cd /usr/local/srcsudo mkdir deepbacksubcd deepbacksubsudo git clone -b v2.1.0 https://github.com/tensorflow/tensorflow
# NEXT LINE FAILS:
sudo ./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR
sudo  ./tensorflow/tensorflow/lite/tools/make/build_lib.sh
sudo git clone https://github.com/floe/deepbacksub.git
sudo make all
sudo make install

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

sudo git clone -b v2.1.0  https://github.com/tensorflow/tensorflow/
Cloning into 'tensorflow'...
remote: Enumerating objects: 278, done.
remote: Counting objects: 100% (278/278), done.
remote: Compressing objects: 100% (244/244), done.
remote: Total 891232 (delta 104), reused 133 (delta 32), pack-reused 890954
Receiving objects: 100% (891232/891232), 523.35 MiB | 3.49 MiB/s, done.
Resolving deltas: 100% (723322/723322), done.
Note: checking out 'e5bf8de410005de06a7ff5393fafdf832ef1d4ad'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by performing another checkout.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -b with the checkout command again. Example:

  git checkout -b <new-branch-name>

Checking out files: 100% (19161/19161), done.
sudo ./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh
./tensorflow/tensorflow/lite/tools/make/download_dependencies.sh: line 59: 1: Usage: download_and_extract URL DIR


"
39208,Inability to use tf.Session(),"Hi, I am new to tensorflow and I am still learning on what to do for printing. I've seen that I should be using tf.Session() but whenever I do that(like Sess = tf.Session() or just tf.Session()) i get the error >>

module 'tensorflow' has no attribute 'Session'

I've learned that I should also use tf.compat.v1 but it doesn't provide the same for my lessons as i cannot use .run or any of the things that session uses. I also use tf.print() or print() which is standard but doesn't run the same as .Session(). Are there any alternatives to run the same thing in Tensorflow 2.1.0 or if there was a way, is there a page that can help me do so?"
39207,Distributed TensorFlow scaling efficiency with CUDA_VISIBLE_DEVICES,"Hello! I'm seeing a strange dip in performance when I run distributed training on a subset of available GPUs using CUDA_VISIBLE_DEVICES and TensorFlow 1.x. I included a case using TensorFlow Docker images and the tf_cnn_benchmarks. Is this expected behavior?

Thank you for your help!

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No (TF CNN benchmarks + official TF docker images)**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04.3 within container (CentOS 7.3 outside, Singularity 3.5)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): **official TF Docker container: tensorflow/tensorflow:1.15.2-gpu-py3.sif**
- TensorFlow version (use command below): **1.15.2**
- Python version: **3.6.9** 
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: **10**
- GPU model and memory: **GTX 1080Ti - 11GB x8**

**Describe the current behavior**

On a node with 8 GPUs, distributed scaling efficiency on 4 GPUs is significantly (~15%) worse when setting `CUDA_VISIBLE_DEVICE=<subset of nodes>` compared to keeping CUDA_VISIBLE_DEVICES unset or setting it to all physical GPUs, as seen in the graph below (using the scripts in the next section):

![CUDA scaling with different CUDA_VISIBLE_DEVICES](https://user-images.githubusercontent.com/814638/81127747-341ec500-8f0d-11ea-814c-f44a60c3fa6f.png)

In these runs, I set `CUDA_VISIBLE_DEVICES=0,…,(n-1)` in the dotted case (the TensorFlow default order) and `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7` in the solid case. According to `nvidia-smi`, the same GPUs are working in both cases.

This pattern is also present for different NVIDIA hardware (seen in 11GB GTX 1080Ti, 11GB RTX 2080Ti, and 32GB V100 nodes), different GPU interconnects (seen in PCIe, NVLink, and DMA-only), different all-reduce strategies (seen in NCCL and xring), and different versions of TensorFlow (seen in 1.15.2, 1.14.0, and 1.13.2). I also get these worse performance results for any subset of nodes (e.g. `CUDA_VISIBLE_DEVICES=1,2,3,4,5,6,7` for `--num_gpus=2`), not just the neat case above.

**Describe the expected behavior**

I'd expect that distributed training on 4 nodes with `CUDA_VISIBLE_DEVICES=0,1,2,3` would have similar performance to the same run with `CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7`.

**Standalone code to reproduce the issue**

Setting up the environment:
```bash
$ singularity pull docker://tensorflow/tensorflow:1.15.2-gpu-py3
$ git clone tensorflow/benchmarks
$ cd benchmarks && git checkout cnn_tf_v1.15_compatible
```

1 GPU:
```bash
$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \
    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
      --num_gpus 1 --tf_random_seed 4321 --device gpu \
      --variable_update replicated --all_reduce_spec nccl \
      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \
      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64

#=> average total images/sec: ~220

$ CUDA_VISIBLE_DEVICES=0 \
    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \
    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
      --num_gpus 1 --tf_random_seed 4321 --device gpu \
      --variable_update replicated --all_reduce_spec nccl \
      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \
      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64

#=> average total images/sec: ~220
```

4 GPUs:
```bash
$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \
    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
      --num_gpus 4 --tf_random_seed 4321 --device gpu \
      --variable_update replicated --all_reduce_spec nccl \
      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \
      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64

#=> average total images/sec: ~840 (~95% scaling efficiency)

$ CUDA_VISIBLE_DEVICES=0,1,2,3 \   # or any other subset of 4-7 GPUs
    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \
    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
      --num_gpus 4 --tf_random_seed 4321 --device gpu \
      --variable_update replicated --all_reduce_spec nccl \
      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \
      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64

#=> average total images/sec: ~700 (~80% scaling efficiency)
```

8 GPUs:
```bash
$ CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
    singularity exec --nv ../tensorflow_1.15.2-gpu-py3.sif \
    python3 scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py \
      --num_gpus 8 --tf_random_seed 4321 --device gpu \
      --variable_update replicated --all_reduce_spec nccl \
      --local_parameter_device cpu --nodistortions --gradient_repacking 1 \
      --model resnet50 --optimizer momentum --data_name imagenet --batch_size 64

#=> average total images/sec: ~1580 (~90% scaling efficiency)
```

**Other info / logs**"
39206,Calling custom op changes data type,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.15.0-rc1-16779-g605ebe703f 2.1.0
- Python version: 3.8.2
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 10.2.89 / 7.6.5
- GPU model and memory: GTX 1070 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I have a custom op defined as follows
```
REGISTER_OP(""Mean2D"")
    .Attr(""T: {float, double}"")
    .Attr(get_data_format())
    .Attr(""alpha: {float, double} "")
    .Input(""img: T"")
    .Input(""kernel: T"")
    .Output(""out: T"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        // ... snip ...
    });
```

When I call this op like this
```
# Code  
disp[1] = ops.mean2d(img=disp[1], kernel=self.gaussian(self.params[""blur_sigma""]), alpha=float(self.params[""blur_threshold""]))
```

I get this error
```
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py"", line 685, in build
    self.call(x, **kwargs)
  File ""/media/RAID/Projects/PhD/HVSnet/model/structure.py"", line 372, in call
    disp[1] = ops.mean2d(img=disp[1], kernel=self.gaussian(self.params[""blur_sigma""]), alpha=alpha)
  File ""<string>"", line 68, in mean2d
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 692, in _apply_op_helper
    attr_value.type = _MakeType(value, attr_def)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 180, in _MakeType
    _SatisfiesTypeConstraint(i, attr_def, param_name=attr_def.name)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 57, in _SatisfiesTypeConstraint
    raise TypeError(
TypeError: Value passed to parameter 'alpha' has DataType int8 not in list of allowed values: float32, float64
```

**Describe the expected behavior**
I would expect that parameter `alpha` would maintain its `float` data type 

"
39205,Tensorflow writes events file in TMPDIR with unbounded size,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not mobile
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.15.0-rc1-11276-gc9f7f636eb 2.1.0
- Python version: 3.6.9
- Bazel version (if compiling from source): 2.1.1
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory: GTX 1080Ti 11GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I have a custom keras model and I am using the fit api to train. I am NOT using the tensorboard callback. During a training an events file in generated in my `/tmp` directory that slowly grows in size until all remaining space on the hard drive is consumed, causing training to crash as it is not able to write to this file any longer.

```
-rw-r--r-- 1 bidski bidski 28G May  6 10:00 /tmp/tmp_50ztys5/events.out.tfevents.1587776979.bidski-alien.30810.513.v2
```

I once tried deleting the file once it got too large, but this resulted in the training process hanging. Why is this file even created?

**Describe the expected behavior**
I would expect a tempfile to 
  1. not consume the entire hard drive
  1. not cause training to crash when writing to the file fails

"
39203,Ubtunu:14.04 Docker Build Terminates with fatal error: pybind11/pybind11.h: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
### OS Platform and Distribution 
- [x] Ubuntu 14.04 arm32
### Mobile device?
- [ ] Nope 
### TensorFlow installed from (source or binary):
- [ ] Source
### TensorFlow version:
- [ ] 2.1.0 
### Python version:
- [ ] python3.4
### Installed using
- [ ] Grabbed it from Git
### Bazel Version?
### GCC/Compiler version (if compiling from source):
n/a
### CUDA/cuDNN version:
n/a
### GPU model and memory:
n/a

### Describe the problem
Trying to use a Docker container to build a clean tflite build from the scripts specified in:
```
/tensorflow/tensorflow/lite/tools/pip_package/
```

### Provide the exact sequence of commands / steps that you executed before running into the problem
```
$ tensorflow/lite/tools/make/download_dependencies.sh
$ tensorflow/lite/tools/pip_package/build_pip_package.sh
```
### Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is what I'm getting right after running the second command
```bash
+++ dirname tensorflow/lite/tools/pip_package/build_pip_package.sh
++ cd tensorflow/lite/tools/pip_package
++ pwd
+ SCRIPT_DIR=/tensorflow/tensorflow/lite/tools/pip_package
+ PYTHON=python3
+ VERSION_SUFFIX=
+ export TENSORFLOW_DIR=/tensorflow/tensorflow/lite/tools/pip_package/../../../..
+ TENSORFLOW_DIR=/tensorflow/tensorflow/lite/tools/pip_package/../../../..
+ TENSORFLOW_LITE_DIR=/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite
++ grep '_VERSION = ' /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/tools/pip_package/setup.py
++ cut -d= -f2
++ sed 's/[ '\''-]//g'
+ TENSORFLOW_VERSION=2.1.0
+ export PACKAGE_VERSION=2.1.0
+ PACKAGE_VERSION=2.1.0
+ BUILD_DIR=/tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ rm -rf /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ mkdir -p /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ cp -r /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/debian /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/setup.py /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/MANIFEST.in /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ cp /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter.py /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3/tflite_runtime
+ echo '__version__ = '\''2.1.0'\'''
++ git -C /tensorflow/tensorflow/lite/tools/pip_package/../../../.. describe
+ echo '__git_version__ = '\''0.6.0-85035-g8e29dc7'\'''
+ cd /tensorflow/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3
+ case ""${TENSORFLOW_TARGET}"" in
+ [[ -n '' ]]
+ python3 setup.py bdist bdist_wheel
running bdist
running bdist_dumb
running build
running build_py
running build_ext
make: Entering directory `/tensorflow'
make: Nothing to be done for `all'.
make: Leaving directory `/tensorflow'
building 'tflite_runtime._pywrap_tensorflow_interpreter_wrapper' extension
creating build
creating build/temp.linux-armv7l-3.4
creating build/temp.linux-armv7l-3.4/interpreter_wrapper
arm-linux-gnueabihf-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -I/tensorflow/tensorflow/lite/tools/pip_package/../../../.. -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package -I/usr/lib/python3/dist-packages/numpy/core/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/absl -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/local/include/python3.4 -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/include/python3.4m -I/usr/include/python3.4m -c interpreter_wrapper/interpreter_wrapper.cc -o build/temp.linux-armv7l-3.4/interpreter_wrapper/interpreter_wrapper.o --std=c++11
cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ [enabled by default]
interpreter_wrapper/interpreter_wrapper.cc: In member function 'std::string tflite::interpreter_wrapper::InterpreterWrapper::TensorName(int) const':
interpreter_wrapper/interpreter_wrapper.cc:317:56: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (!interpreter_ || i >= interpreter_->tensors_size() || i < 0) {
                                                        ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorType(int) const':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:327:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorSize(int) const':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:345:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorSizeSignature(int) const':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:360:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorSparsityParameters(int) const':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:380:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorQuantization(int) const':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:391:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::TensorQuantizationParameters(int) const':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:398:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::SetTensor(int, PyObject*)':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:432:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::NodeInputs(int) const':
interpreter_wrapper/interpreter_wrapper.cc:56:37: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->nodes_size() || i < 0) {         \
                                     ^
interpreter_wrapper/interpreter_wrapper.cc:511:3: note: in expansion of macro 'TFLITE_PY_NODES_BOUNDS_CHECK'
   TFLITE_PY_NODES_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'PyObject* tflite::interpreter_wrapper::InterpreterWrapper::NodeOutputs(int) const':
interpreter_wrapper/interpreter_wrapper.cc:56:37: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->nodes_size() || i < 0) {         \
                                     ^
interpreter_wrapper/interpreter_wrapper.cc:521:3: note: in expansion of macro 'TFLITE_PY_NODES_BOUNDS_CHECK'
   TFLITE_PY_NODES_BOUNDS_CHECK(i);
   ^
interpreter_wrapper/interpreter_wrapper.cc: In member function 'std::string tflite::interpreter_wrapper::InterpreterWrapper::NodeName(int) const':
interpreter_wrapper/interpreter_wrapper.cc:530:54: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (!interpreter_ || i >= interpreter_->nodes_size() || i < 0) {
                                                      ^
interpreter_wrapper/interpreter_wrapper.cc: In function 'PyObject* tflite::interpreter_wrapper::{anonymous}::CheckGetTensorArgs(tflite::Interpreter*, int, TfLiteTensor**, int*)':
interpreter_wrapper/interpreter_wrapper.cc:48:39: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   if (i >= interpreter_->tensors_size() || i < 0) {                         \
                                       ^
interpreter_wrapper/interpreter_wrapper.cc:556:3: note: in expansion of macro 'TFLITE_PY_TENSOR_BOUNDS_CHECK'
   TFLITE_PY_TENSOR_BOUNDS_CHECK(tensor_index);
   ^
In file included from /usr/lib/python3/dist-packages/numpy/core/include/numpy/ufuncobject.h:327:0,
                 from /tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/python/interpreter_wrapper/numpy.h:52,
                 from interpreter_wrapper/interpreter_wrapper.cc:36:
/usr/lib/python3/dist-packages/numpy/core/include/numpy/__ufunc_api.h: At global scope:
/usr/lib/python3/dist-packages/numpy/core/include/numpy/__ufunc_api.h:241:1: warning: 'int _import_umath()' defined but not used [-Wunused-function]
 _import_umath(void)
 ^
arm-linux-gnueabihf-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -g -fstack-protector --param=ssp-buffer-size=4 -Wformat -Werror=format-security -D_FORTIFY_SOURCE=2 -fPIC -I/tensorflow/tensorflow/lite/tools/pip_package/../../../.. -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package -I/usr/lib/python3/dist-packages/numpy/core/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/make/downloads/absl -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/local/include/python3.4 -I/tensorflow/tensorflow/lite/tools/pip_package/../../../../tensorflow/lite/tools/pip_package/gen/usr/include/python3.4m -I/usr/include/python3.4m -c interpreter_wrapper/interpreter_wrapper_pybind11.cc -o build/temp.linux-armv7l-3.4/interpreter_wrapper/interpreter_wrapper_pybind11.o --std=c++11
cc1plus: warning: command line option '-Wstrict-prototypes' is valid for C/ObjC but not for C++ [enabled by default]
interpreter_wrapper/interpreter_wrapper_pybind11.cc:16:31: fatal error: pybind11/pybind11.h: No such file or directory
 #include ""pybind11/pybind11.h""
                        
```


"
39199,Breaking changes related to infinite generators missing in TF 2.2 release notes,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:

https://github.com/tensorflow/tensorflow/releases
https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly

I have spent the better part of a working day on tracking down several infinite loops using TF 2.2.0. In summary, the following two pieces of code run fine in TF 2.1.0, and run in infinite loops in TF 2.2.0:

## Issue 1

```
""""""Bug 1.""""""
import numpy as np
from tensorflow.keras.applications import vgg16
from tensorflow.keras.preprocessing.image import ImageDataGenerator

N = 9
data = ImageDataGenerator().flow(np.empty((N, 224, 224, 3)), np.empty((N, 1000)))

model = vgg16.VGG16()
model.compile(optimizer=""adam"", loss=""categorical_crossentropy"")
model.fit(
    x=data,
    epochs=2,
    verbose=2,  # 1
    # steps_per_epoch=N,
)

# https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly
# ""When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument.""
# This is a breaking change that should be part of the release notes.
```
This code auto-selects `steps_per_epoch=1` (""Train for 1 steps"") in TF 2.1.0, which is correct (default `batch_size` is 32, `len(train_flow)` is 1). In TF 2.2.0, it's an infinite loop that is hard to debug. After setting `verbose=1`, one can see that something is in fact happening, with the number of steps per epoch growing without limits; from there, looking at the docs, one notices that `steps_per_epoch` is required (""When passing an infinitely repeating dataset, you must specify the steps_per_epoch argument.""). But it's hard to recognize that breaking change from the release notes. (It's even hard to know that the `ImageDataGenerator().flow()` does indeed repeat infinitely, since it does have a length.)

## Issues 2 and 3

```
""""""Bug 2.""""""
import numpy as np
from tensorflow.keras.applications import vgg16
from tensorflow.keras.preprocessing.image import ImageDataGenerator

N = 9
data = ImageDataGenerator().flow(np.empty((N, 224, 224, 3)), np.empty((N, 1000)))

model = vgg16.VGG16()
model.compile(optimizer=""adam"", loss=""categorical_crossentropy"")
model.fit(
    x=data,
    epochs=2,
    verbose=1,
    # steps_per_epoch=N,
    validation_data=data,
    ## validation_steps=N,
)

# https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=nightly
# ""Note that `validation_data` does not support all the data types that are supported in `x`, eg, dict, generator [...]""
# This is a breaking change that should be part of the release notes.
```
Similarly, this code auto-selects `steps_per_epoch=1` and `validation_steps=1` (""Train for 1 steps, validate for 1 steps"") in TF 2.1.0, which is correct (default `batch_size` is 32, `len(val_flow)` is 1). In TF 2.2.0, it's again an infinite loop that is hard to debug, even if one sets `verbose=1` **and** `steps_per_epoch=N` (since there is no verbosity on the validation part). Again, looking at the docs, one notices that `validation_steps` is required (""In the case of an infinitely repeated dataset, it will run into an infinite loop.""). But, again, it's hard to recognize that breaking change from the release notes. (Again, it's even hard to know that the `ImageDataGenerator().flow()` does indeed repeat infinitely, since it does have a length.)

Finally, the nightly docs say that ""`validation_data` does not support all the data types that are supported in `x`, eg, dict, generator [...]"". As you can see above, using a `ImageDataGenerator` still works if you adhere to the other requirements. If use of any generator is disencouraged from now on, this is a breaking change that needs to go into the release notes in my opinion."
39198,Error while reading resource variable _AnonymousVar46 from Container: localhost.,"I have this code: (all variables are initialized)
`def train(models, X_train, noise_plot, dir_result=""D:\gan\result"", epochs=10000, batch_size=100):
        combined, discriminator, generator = models
        nlatent_dim = noise_plot.shape[1]
        half_batch  = int(batch_size / 2)
        history = []
        for epoch in range(epochs):

            # ---------------------
            #  Train Discriminator
            # ---------------------

            # Select a random half batch of images
            idx = np.random.randint(0, X_train.shape[0], half_batch)
            imgs = X_train[idx]
            noise = get_noise(half_batch, nlatent_dim)

            # Generate a half batch of new images
            gen_imgs = generator.predict(noise)

            
            # Train the discriminator q: better to mix them together?
            d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))
            d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))
            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)


            # ---------------------
            #  Train Generator
            # ---------------------

            noise = get_noise(batch_size, nlatent_dim)

            # The generator wants the discriminator to label the generated samples
            # as valid (ones)
            valid_y = (np.array([1] * batch_size)).reshape(batch_size,1)
            
            # Train the generator
            g_loss = combined.train_on_batch(noise, valid_y)

            history.append({""D"":d_loss[0],""G"":g_loss})
            
            if epoch % 100 == 0:
                # Plot the progress
                print (""Epoch {:05.0f} [D loss: {:4.3f}, acc.: {:05.1f}%] [G loss: {:4.3f}]"".format(
                    epoch, d_loss[0], 100*d_loss[1], g_loss))
            if epoch % int(epochs/100) == 0:
                plot_generated_images(noise_plot,
                                      path_save=dir_result+""/image_{:05.0f}.png"".format(epoch),
                                      titleadd=""Epoch {}"".format(epoch))
            if epoch % 1000 == 0:
                plot_generated_images(noise_plot,
                                      titleadd=""Epoch {}"".format(epoch))
                        
        return(history)
history = train(models=_models, X_train=X_train, noise_plot=noise, dir_result=dir_result,epochs=10000, batch_size=100)`
- OS: Windows 10

- TensorFlow version: 2.1.0
- Python version: 3.7


And I have some problems:
`FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-13-7e6cdc16ab87> in <module>
     72 _models = combined, discriminator, generator
     73 
---> 74 history = train(models=_models, X_train=X_train, noise_plot=noise, dir_result=dir_result,epochs=10000, batch_size=100)
     75 end_time = time.time()
     76 print(""-""*10)

<ipython-input-13-7e6cdc16ab87> in train(models, X_train, noise_plot, dir_result, epochs, batch_size)
     27 
     28             # Train the discriminator q: better to mix them together?
---> 29             d_loss_real = discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))
     30             d_loss_fake = discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))
     31             d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

C:\Anaconda3\envs\tf\lib\site-packages\keras\engine\training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1512             ins = x + y + sample_weights
   1513         self._make_train_function()
-> 1514         outputs = self.train_function(ins)
   1515 
   1516         if reset_metrics:

C:\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\keras\backend.py in __call__(self, inputs)
   3725         value = math_ops.cast(value, tensor.dtype)
   3726       converted_inputs.append(value)
-> 3727     outputs = self._graph_fn(*converted_inputs)
   3728 
   3729     # EagerTensor.numpy() will often make a copy to ensure memory safety.

C:\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   1549       TypeError: For invalid positional/keyword argument combinations.
   1550     """"""
-> 1551     return self._call_impl(args, kwargs)
   1552 
   1553   def _call_impl(self, args, kwargs, cancellation_manager=None):

C:\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py in _call_impl(self, args, kwargs, cancellation_manager)
   1589       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(
   1590           list(kwargs.keys()), list(self._arg_keywords)))
-> 1591     return self._call_flat(args, self.captured_inputs, cancellation_manager)
   1592 
   1593   def _filtered_call(self, args, kwargs):

C:\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1690       # No tape is watching; skip to running the function.
   1691       return self._build_call_outputs(self._inference_function.call(
-> 1692           ctx, args, cancellation_manager=cancellation_manager))
   1693     forward_backward = self._select_forward_and_backward_functions(
   1694         args,

C:\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    543               inputs=args,
    544               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545               ctx=ctx)
    546         else:
    547           outputs = execute.execute_with_cancellation(

C:\Anaconda3\envs\tf\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     65     else:
     66       message = e.message
---> 67     six.raise_from(core._status_to_exception(e.code, message), None)
     68   except TypeError as e:
     69     keras_symbolic_tensors = [

C:\Anaconda3\envs\tf\lib\site-packages\six.py in raise_from(value, from_value)

FailedPreconditionError:  Error while reading resource variable _AnonymousVar46 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar46/class tensorflow::Var does not exist.
	 [[node mul_46/ReadVariableOp (defined at C:\Anaconda3\envs\tf\lib\site-packages\keras\backend\tensorflow_backend.py:3009) ]] [Op:__inference_keras_scratch_graph_2593]

Function call stack:
keras_scratch_graph`
![image](https://user-images.githubusercontent.com/20381972/81101401-bf508880-8f16-11ea-80fc-773a8a6f0390.png)
![image](https://user-images.githubusercontent.com/20381972/81101421-caa3b400-8f16-11ea-979b-1af07cd77b96.png)
"
39197,Very slow recording of TFRecordWriter with tf.data.Dataset.shard(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0-dev20200412
- Python version: 3.6.10

**Describe the current behavior**
I'm trying to convert my dataset to multiple *.tfrecord files. My initial idea was to compile a `tf.data.Dataset` with all data of a certain class, and then split it into multiple datasets using `tf.data.Dataset.shard(...)`, which produced terrible recording to disk speed. 

As a workaround i split and compiled the data first into multiple `tf.data.Dataset` and recorded them as it is, which produced much better results.

Code to reproduce: https://gist.github.com/theonekeyg/68b3df515ce872d70611f92a857895a1
The exact data that i used can be obtained from this Kaggle competition: https://www.kaggle.com/c/alaska2-image-steganalysis/data

To be more specific, with `tf.data.Dataset.shard(...)` approach, to process 3000 samples it took ~35 minutes, and ~1.2 minutes with second approach.

Is that an expected behavior, that `ShardDataset` takes so long to compile/write to .tfrecord?"
39196,Got small output value error between .h5 model and .pb model,"I tried both on `tf-gpu1.4+keras2.1.3` and on `tf-gpu1.12+keras2.2.4` and the problem always happens.

The problem is:  **After I converted the keras.application.ResNet50() model into freeze graph model in .pb format, I feed in the same picture into the converted .pb model but the output value changes just a little.**

Below is the codes, which prints the first 10 element of the ResNet output vector , and also freeze the graph to output pb model file:
```
from tensorflow.python.framework.graph_util_impl import convert_variables_to_constants
from keras.preprocessing import image
from keras.applications.resnet50 import preprocess_input, ResNet50
import keras.backend as K
K.set_learning_phase(0)

img = image.load_img('images/34rews.jpg', target_size=(224, 224))
x = image.img_to_array(img)
x = np.expand_dims(x, axis=0)
x_input = preprocess_input(x)

net_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')
sess = K.get_session()
preds = sess.run(net_model.get_output_at(0), feed_dict={net_model.get_input_at(0): x_input})
print('before convert to pb :', np.array(preds).squeeze()[:10])

output_name0 = net_model.get_output_at(0).op.name  # 'global_average_pooling2d_1/Mean'
constant_graph = convert_variables_to_constants(sess, sess.graph_def, [output_name0])

with tf.gfile.GFile('saved_model_constant.pb', 'wb') as f:
    f.write(constant_graph.SerializeToString())
```

and the print log is : 
```
before convert to pb : [**0.99536467** 0.31807986 2.0998483  0.9077819  0.10606026 0.93215793
 0.04187933 0.10000334 1.1727284  1.0535308 ]
```

Then we predict the same image through the pb file generated by above codes:
```
def test_constant(pb_dir, img_path='images/34rews.jpg'):
    img = image.load_img(img_path, target_size=(224, 224))
    x = image.img_to_array(img)
    x = np.expand_dims(x, axis=0)
    x = preprocess_input(x)

    from tensorflow.python.platform import gfile
    with tf.Session() as sess:
        with gfile.FastGFile(pb_dir, 'rb') as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())

        result = tf.import_graph_def(graph_def, return_elements=[""global_average_pooling2d_1/Mean:0""], name='')
        preds = sess.run(result, feed_dict={sess.graph.get_tensor_by_name('input_1:0'):x})
        print('using pb file:', np.array(preds).squeeze()[:10])
```

The output printing log is:
```
using pb file: [**0.99536514** 0.3180797  2.0998483  0.90778273 0.10606024 0.9321572
 0.04187941 0.10000295 1.1727289  1.0535315 ]
```

I can clearly find the extreme small value error of the predict vector, between the original keras model and the pb model after using the freeze graph method.
e.g. The first element value of the resnet output vector using original keras model is **0.99536467**, but the output one is **0.99536514** using the converted pb file. 
I wonder why there is such a small value error? It may not cause big accuracy error but it is really strange!"
39195,Possible bug with rejection sampling,"This is about the [official TensorFlow tutorial on input pipelines](https://www.tensorflow.org/guide/data). In the [section](https://www.tensorflow.org/guide/data#rejection_resampling) on rejection sampling, the very last cell, namely
```
for features, labels in balanced_ds.take(10):
    print(labels.numpy())
```
produces a set of labels which are all zeros, indicating that the dataset hasn't been balanced.

**System information**
- Windows 10
- Python 3.7.4 under conda 4.8.3
- tensorflow-datasets 1.3.0
- tensorflow-estimator 2.0.1
- tensorflow-gpu 2.1.0 
- tensorflow-gpu-estimator 2.1.0
- gast 0.2.2        "
39189,ValueError: Please initialize `TimeDistributed` layer with a `Layer` instance. You passed: <mrcnn.model1.BatchNorm object at 0x7f0b39c36588>,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 19.10):
- TensorFlow installed from (binary):
- Libraries: 
     - Keras==2.1.6
     - tensorflow==1.14.0
     - tensorflow-model-optimization==0.3.0


**My code snippet:**

```
from mrcnn import model as modellib

custom={        
    'DepthwiseConv2D': keras.applications.mobilenet.DepthwiseConv2D,
    'PyramidROIAlign':modellib.PyramidROIAlign,
    'DetectionLayer':modellib.DetectionLayer,
    'ProposalLayer': modellib.ProposalLayer,
    'BatchNorm':modellib.BatchNorm,
    'relu6':relu6,
    'tf':tf,
}

tflite_converter = tf.lite.TFLiteConverter.from_keras_model_file(model_path, 
                                                    custom_objects=custom)
```

**Output Error:**

```
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-0cdc241e349b> in <module>
     15 
     16 tflite_converter = tf.lite.TFLiteConverter.from_keras_model_file(model_path, 
---> 17                                                     custom_objects=custom)
     18 
     19 tflite_converter.optimizations = [tf.lite.Optimize.DEFAULT]

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/lite/python/lite.py in from_keras_model_file(cls, model_file, input_arrays, input_shapes, output_arrays, custom_objects)
    745     _keras.backend.clear_session()
    746     _keras.backend.set_learning_phase(False)
--> 747     keras_model = _keras.models.load_model(model_file, custom_objects)
    748     sess = _keras.backend.get_session()
    749 

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile)
    144       h5py is not None and (
    145           isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):
--> 146     return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
    147 
    148   if isinstance(filepath, six.string_types):

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model_from_hdf5(filepath, custom_objects, compile)
    210     model_config = json.loads(model_config.decode('utf-8'))
    211     model = model_config_lib.model_from_config(model_config,
--> 212                                                custom_objects=custom_objects)
    213 
    214     # set weights

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)
     53                     '`Sequential.from_config(config)`?')
     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 55   return deserialize(config, custom_objects=custom_objects)
     56 
     57 

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
     87       module_objects=globs,
     88       custom_objects=custom_objects,
---> 89       printable_module_name='layer')

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    190             custom_objects=dict(
    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 192                 list(custom_objects.items())))
    193       with CustomObjectScope(custom_objects):
    194         return cls.from_config(cls_config)

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in from_config(cls, config, custom_objects)
   1119     # First, we create all layers and enqueue nodes to be processed
   1120     for layer_data in config['layers']:
-> 1121       process_layer(layer_data)
   1122     # Then we process nodes in order of layer depth.
   1123     # Nodes that cannot yet be processed (if the inbound node

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py in process_layer(layer_data)
   1103       from tensorflow.python.keras.layers import deserialize as deserialize_layer  # pylint: disable=g-import-not-at-top
   1104 
-> 1105       layer = deserialize_layer(layer_data, custom_objects=custom_objects)
   1106       created_layers[layer_name] = layer
   1107 

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
     87       module_objects=globs,
     88       custom_objects=custom_objects,
---> 89       printable_module_name='layer')

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    190             custom_objects=dict(
    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 192                 list(custom_objects.items())))
    193       with CustomObjectScope(custom_objects):
    194         return cls.from_config(cls_config)

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py in from_config(cls, config, custom_objects)
     84     layer = deserialize_layer(
     85         config.pop('layer'), custom_objects=custom_objects)
---> 86     return cls(layer, **config)
     87 
     88 

~/anaconda3/envs/unet/lib/python3.6/site-packages/tensorflow/python/keras/layers/wrappers.py in __init__(self, layer, **kwargs)
    149       raise ValueError(
    150           'Please initialize `TimeDistributed` layer with a '
--> 151           '`Layer` instance. You passed: {input}'.format(input=layer))
    152     super(TimeDistributed, self).__init__(layer, **kwargs)
    153     self.supports_masking = True

ValueError: Please initialize `TimeDistributed` layer with a `Layer` instance. You passed: <mrcnn.model1.BatchNorm object at 0x7f0b39c36588>

```


**Failure details:**
It' not even loading model from keras file. It's a custom build MRCNN model so there is some kind of issue with layers, but I am not sure.


**Error source:**
Error generated by this [file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/wrappers.py). Kindly if any one knows the solution post it here."
39186,tf.data.experimental.make_csv_dataset modifies mutable variables passed to it,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow installed from (source or binary): docker
- TensorFlow version (use command below): TF 2.1.0

**Describe the current behavior**

tf.data.experimental.make_csv_dataset modifies passed variables in place. So if you call
`tf.data.experimental.make_csv_dataset(file_pattern, batch_size, select_columns=columns_to_use)`
the variable `columns_to_use` is changed. It's in line 463 https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/python/data/experimental/ops/readers.py#L463 but it may happen to other variables passed. Specifically the list sent to `select_columns=` is replaced by a list of the indices of those columns in the file to read.

**Describe the expected behavior**

A function should never modify the mutable objects passed to it. This is only ever appropriate for methods of a class."
39185,Failed to find bogomips on s390x architecture,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): s390x Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):  v2.2.0-rc4-0-g70087ab4f4 2.2.0-rc4
- Python version: Python 3.6.9
- Bazel version (if compiling from source): Build label: 2.0.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When running test cases on s390x architecture following Warning message is logged:

`W tensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency`

**Describe the expected behavior**
CPU frequency on s390x architecture should be correctly detected.

**Standalone code to reproduce the issue**
Run the following test case:
```
root@3cefe6d659f6:/home/tensorflow# python tensorflow/python/data/experimental/kernel_tests/optimization/shuffle_and_repeat_fusion_test.py
Running tests under Python 3.6.9: /usr/bin/python
[ RUN      ] ShuffleAndRepeatFusionTest.testShuffleAndRepeatFusion
2020-05-05 13:11:16.071244: W tensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

It seems like `tensorflow/core/platform/profile_utils/cpu_utils.cc`  is not parsing `bogomips` line correctly on s390x: 
```
root@3cefe6d659f6:/home# cat /proc/cpuinfo | grep bogomips
bogomips per cpu: 3033.00
```
This patch should fix it:
```
diff --git a/tensorflow/core/platform/profile_utils/cpu_utils.cc b/tensorflow/core/platform/profile_utils/cpu_utils.cc
index 587c97875a..b22123a804 100644
--- a/tensorflow/core/platform/profile_utils/cpu_utils.cc
+++ b/tensorflow/core/platform/profile_utils/cpu_utils.cc
@@ -88,6 +88,8 @@ static ICpuUtilsHelper* cpu_utils_helper_instance_ = nullptr;
      defined(__ppc__) && (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__))
     retval = sscanf(line.c_str(), ""clock              : %lfMHz"", &cpu_freq);
     freq_factor = 1.0;
+#elif defined(__s390x__)
+    retval = sscanf(line.c_str(), ""bogomips per cpu: %lf"", &cpu_freq);
 #else
     retval = sscanf(line.c_str(), ""bogomips : %lf"", &cpu_freq);
 #endif
```"
39184,"InvalidArgumentError: PartialTensorShape: Incompatible shapes during merge: [1,2] vs. [2,2]","**System information**  
-OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
-Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
-TensorFlow installed from (source or binary): NVIDIA (https://docs.nvidia.com/deeplearning/frameworks/tensorflow-release-notes)
-TensorFlow version (use command below): 2.1
-Python version: 3.6.9


Hello, I am running my NN model and it runs perfectly some epochs and suddenly it breaks showing up the next issues (sometimes one and sometimes the other one).

_**InvalidArgumentError**:  PartialTensorShape: Incompatible shapes during merge: [1,2] vs. [2,2]
	 [[node TensorArrayV2Stack/TensorListStack (defined at /workspace/code/model_NN.py:268) ]] [Op:__forward_p2d_fluoro_prediction_117955]

Errors may have originated from an input operation.
Input Source operations connected to node TensorArrayV2Stack/TensorListStack:
 while (defined at /workspace/code/model_NN.py:261)

Function call stack:
p2d_fluoro_prediction_

_**InvalidArgumentError:** 2 root error(s) found.
  (0) Invalid argument:  Tried to set a tensor with incompatible shape at a list index. Item element shape: [2,2] list shape: [1,2]
	 [[{{node while/body/_1/TensorArrayV2Write/TensorListSetItem}}]]
  (1) Invalid argument:  Tried to set a tensor with incompatible shape at a list index. Item element shape: [2,2] list shape: [1,2]
	 [[{{node while/body/_1/TensorArrayV2Write/TensorListSetItem}}]]
	 [[while/loop_body_control/_24/_13]]
0 successful operations.
0 derived errors ignored. [Op:__forward_p2d_fluoro_prediction_1262956]

Function call stack:
p2d_fluoro_prediction -> p2d_fluoro_prediction_


For me, this is a strange behavior because each time it breaks after a different epoch.  I am running all my dataset in each epoch so it is not a problem of the data. I have try adding this line tf.config.threading.set_inter_op_parallelism_threads(1) like this, but adain it does not work:

```
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
              tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), ""Physical GPUs,"", len(logical_gpus), ""Logical GPUs"")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
tf.keras.backend.set_floatx('float32')

context._context = None
context._create_context()

tf.config.threading.set_inter_op_parallelism_threads(1)
```


The function which cause the problem is the following one and it works well.

```
@tf.function    
    def p2d_fluoro_prediction(self,heat_map):
        points = tf.TensorArray(tf.int64, size=self.nb_points,dynamic_size=True)
        for i in tf.range(self.nb_points):
            hm = heat_map[:,:,i]
            heat_max = tf.reduce_max(hm)
            point = tf.where(hm[:,:]==heat_max)
            point = tf.reverse(point, [1])
            points = points.write(i,point)          
        points = points.stack()    
        return tf.cast(tf.squeeze(points,axis = 1),dtype ='float32')
```

I hope you can help me with this issue. Because I do not if it is a problem of my program or is an internal problem in tensorflow. Thank you in advance.


"
39182,Can not concat RaggedTensor in custom keras layer.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
2.1.0
- TensorFlow version (use command below):
v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version:
3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
10.1
- GPU model and memory:
GTX2080 8GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
raise exception
**Describe the expected behavior**
not raise exception
**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.models import Model
import tensorflow.keras.layers as layers

MAX_LEN=20 

lookuptable = tf.lookup.StaticVocabularyTable(tf.lookup.TextFileInitializer(""vocab.txt"", tf.string, 0, tf.int64, 1, delimiter="" ""), num_oov_buckets=1)

input_encoding_string = layers.Input(dtype=tf.string,shape=1)

def custom_tokenizer(input_tensor_string, width=MAX_LEN):
    ragged_tensor = tf.strings.split(input_tensor_string)
    words_index = tf.ragged.map_flat_values(lookuptable.lookup,ragged_tensor)
    rt = words_index[-width:]  # Truncate rows to have at most `width` items
    pad_row_lengths = width - rt.row_lengths()
    pad_values = tf.zeros([(width * rt.nrows()) - tf.size(rt, tf.int64)], rt.dtype)
    padding = tf.RaggedTensor.from_row_lengths(pad_values, pad_row_lengths)
    return tf.concat([padding, rt], axis=1).to_tensor()

def custom_tokenizer_shape(shapes):
    return (shapes[0], MAX_LEN)


processed_input = layers.Lambda(custom_tokenizer, output_shape=custom_tokenizer_shape)(input_encoding_string)
tm = Model(input_encoding_string, processed_input)
```
the vocab.txt is just a word index map like :
```
a 1
b 2
c 3
d 4
```

Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
TypeError                                 Traceback (most recent call last)
~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    542     try:
--> 543       str_values = [compat.as_bytes(x) for x in proto_values]
    544     except TypeError:

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in <listcomp>(.0)
    542     try:
--> 543       str_values = [compat.as_bytes(x) for x in proto_values]
    544     except TypeError:

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/compat.py in as_bytes(bytes_or_text, encoding)
     86     raise TypeError('Expected binary or unicode string, got %r' %
---> 87                     (bytes_or_text,))
     88

TypeError: Expected binary or unicode string, got tf.RaggedTensor(values=Tensor(""lambda_24_1/zeros:0"", shape=(None,), dtype=int64), row_splits=Tensor(""lambda_24_1/RaggedFromRowLengths/concat:0"", shape=(None,), dtype=int64))

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    411               preferred_dtype=default_dtype,
--> 412               as_ref=input_arg.is_ref)
    413           if input_arg.number_attr and len(

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in internal_convert_n_to_tensor(values, dtype, name, as_ref, preferred_dtype, ctx)
   1381             preferred_dtype=preferred_dtype,
-> 1382             ctx=ctx))
   1383   return ret

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1313     if ret is None:
-> 1314       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1315

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    316   _ = as_ref
--> 317   return constant(v, dtype=dtype, name=name)
    318

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in constant(value, dtype, shape, name)
    257   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 258                         allow_broadcast=True)
    259

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    295           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 296           allow_broadcast=allow_broadcast))
    297   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    546                       ""Contents: %s. Consider casting elements to a ""
--> 547                       ""supported type."" % (type(values), values))
    548     tensor_proto.string_val.extend(str_values)

TypeError: Failed to convert object of type <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'> to Tensor. Contents: tf.RaggedTensor(values=Tensor(""lambda_24_1/zeros:0"", shape=(None,), dtype=int64), row_splits=Tensor(""lambda_24_1/RaggedFromRowLengths/concat:0"", shape=(None,), dtype=int64)). Consider casting elements to a supported type.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py in concat(values, axis, name)
   1516       return identity(values[0], name=name)
-> 1517   return gen_array_ops.concat_v2(values=values, axis=axis, name=name)
   1518

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py in concat_v2(values, axis, name)
   1125   _, _, _op, _outputs = _op_def_library._apply_op_helper(
-> 1126         ""ConcatV2"", values=values, axis=axis, name=name)
   1127   _result = _outputs[:]

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    439             else:
--> 440               raise TypeError(""%s that don't all match."" % prefix)
    441           else:

TypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [<NOT CONVERTIBLE TO TENSOR>, <NOT CONVERTIBLE TO TENSOR>] that don't all match.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-74-900126414798> in <module>
----> 1 l(input_encoding_string)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    771                     not base_layer_utils.is_in_eager_or_tf_function()):
    772                   with auto_control_deps.AutomaticControlDependencies() as acd:
--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)
    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    775                     # circular dependencies.

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py in call(self, inputs, mask, training)
    844     with backprop.GradientTape(watch_accessed_variables=True) as tape,\
    845         variable_scope.variable_creator_scope(_variable_creator):
--> 846       result = self.function(inputs, **kwargs)
    847     self._check_variables(created_variables, tape.watched_variables())
    848     return result

<ipython-input-63-7e27594b3fb4> in custom_tokenizer(input_tensor_string, width)
     10     tf.print('pad',padding.dtype)
     11     tf.print('pad',rt.dtype)
---> 12     return tf.concat([padding, rt], axis=1).to_tensor()
     13

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in wrapper(*args, **kwargs)
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a
    183       # TypeError, when given unexpected types.  So we need to catch both.
--> 184       result = dispatch(wrapper, *args, **kwargs)
    185       if result is not OpDispatcher.NOT_SUPPORTED:
    186         return result

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py in dispatch(op, *args, **kwargs)
     99   """"""
    100   for dispatcher in getattr(op, DISPATCH_ATTR):
--> 101     result = dispatcher.handle(args, kwargs)
    102     if result is not OpDispatcher.NOT_SUPPORTED:
    103       return result

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_dispatch.py in handle(self, args, kwargs)
    251   def handle(self, args, kwargs):
    252     if self.is_supported(args, kwargs):
--> 253       return self._ragged_op(*args, **kwargs)
    254     else:
    255       return self.NOT_SUPPORTED

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_concat_ops.py in concat(values, axis, name)
     68     values = [values]
     69   with ops.name_scope(name, 'RaggedConcat', values):
---> 70     return _ragged_stack_concat_helper(values, axis, stack_values=False)
     71
     72

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/ops/ragged/ragged_concat_ops.py in _ragged_stack_concat_helper(rt_inputs, axis, stack_values)
    159       ndims = rt.shape.ndims
    160     else:
--> 161       rt.shape.assert_has_rank(ndims)
    162
    163   out_ndims = ndims if (ndims is None or not stack_values) else ndims + 1

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py in assert_has_rank(self, rank)
    988     """"""
    989     if self.rank not in (None, rank):
--> 990       raise ValueError(""Shape %s must have rank %d"" % (self, rank))
    991
    992   def with_rank(self, rank):

ValueError: Shape (None, None, None) must have rank 2

```"
39180,TensorFlow build is failing on Bazel CI due to Eigen update,"https://buildkite.com/bazel/tensorflow/builds/4927
The build is failing on Linux and macOS with
```
./third_party/eigen3/unsupported/Eigen/CXX11/src/FixedPoint/PacketMathAVX512.h:432:16: error: could not convert 'Eigen::internal::pfirst<__vector(2) long long int>(_mm_min_epi32(res.Eigen::internal::eigen_packet_wrapper<__vector(2) long long int, 0>::operator __vector(2) long long int&(), _mm_shuffle_epi32(res.Eigen::internal::eigen_packet_wrapper<__vector(2) long long int, 0>::operator __vector(2) long long int&(), ((((0 << 6) | (0 << 4)) | (0 << 2)) | 1))))' from 'Eigen::internal::unpacket_traits<__vector(2) long long int>::type {aka __vector(2) long long int}' to 'Eigen::QInt32'
   return pfirst(
          ~~~~~~^
       _mm_min_epi32(res, _mm_shuffle_epi32(res, _MM_SHUFFLE(0, 0, 0, 1))));
```

It's weird we don't see the failure on TF CI.

A bisect shows 510f0f9a6c79d5e2f29c52911a3819fbdb61a6e0 is the culprit

/cc @gunan"
39179,tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input). tflite for android,"I have converted a [mask-rcnn model](https://github.com/matterport/Mask_RCNN/releases/tag/v2.0) to deploy it on android. I was able to load the keras weight, freeze the model and convert it to .tflite model using **tflite 1.13 toco** using [this script](https://gist.github.com/bmabir17/754a6e0450ec4fd5e25e462af949cde6). This model contains ResizeNearestNeighbor, Stack, and TensorFlowShape tf_ops. So i had to use 
`converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,tf.lite.OpsSet.SELECT_TF_OPS]`
Because of this i have already add the tflite build with select-tf-ops like the following
```java
dependencies {
    implementation('org.tensorflow:tensorflow-lite:0.0.0-nightly'){ changing = true }
    implementation('org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly'){ changing = true }
    implementation('org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'){ changing = true }
    implementation('org.tensorflow:tensorflow-lite-support:0.0.0-nightly'){ changing = true }
}
```


Now i am trying to use this converted model for inference in android.
```java
    protected Classifier(Activity activity, Device device, int numThreads) throws IOException {
        tfliteModel = FileUtil.loadMappedFile(activity, getModelPath());
        switch (device) {
            case GPU:
                // TODO: Create a GPU delegate instance and add it to the interpreter options

                break;
            case CPU:
                break;
        }
        tfliteOptions.setNumThreads(numThreads);
        tflite = new Interpreter(tfliteModel, tfliteOptions);

        // Loads labels out from the label file.
        labels = FileUtil.loadLabels(activity, getLabelPath());
        // Todo change output as multiple output for maskrcnn
        // Reads type and shape of input and output tensors, respectively.
        int imageTensorIndex = 0;
        int[] imageShape = tflite.getInputTensor(imageTensorIndex).shape(); // {1, height, width, 3}
        imageSizeY = imageShape[1];
        imageSizeX = imageShape[2];
        DataType imageDataType = tflite.getInputTensor(imageTensorIndex).dataType();
        int probabilityTensorIndex = 0;
        int[] probabilityShape =
                tflite.getOutputTensor(probabilityTensorIndex).shape(); // {1, NUM_CLASSES}
        DataType probabilityDataType = tflite.getOutputTensor(probabilityTensorIndex).dataType();
        // Creates the input tensor.
        inputImageBuffer = new TensorImage(imageDataType);

        // Creates the output tensor and its processor.
        outputProbabilityBuffer = TensorBuffer.createFixedSize(probabilityShape, probabilityDataType);

        // Creates the post processor for the output probability.
        probabilityProcessor = new TensorProcessor.Builder().add(getPostprocessNormalizeOp()).build();

        LOGGER.d(""Created a Tensorflow Lite Image Classifier."");
    }
    public List<Recognition> InferImage(final Bitmap bitmap) {
        int sensorOrientation=0;
        inputImageBuffer = loadImage(bitmap, sensorOrientation);
        tflite.run(inputImageBuffer.getBuffer(), outputProbabilityBuffer.getBuffer().rewind());

        // Gets the map of label and probability
        Map<String, Float> labeledProbability =
                new TensorLabel(labels, probabilityProcessor.process(outputProbabilityBuffer))
                        .getMapWithFloatValue();
        // Gets top-k results.
        return getTopKProbability(labeledProbability);
    }
```
[Full Source Code](https://gist.github.com/bmabir17/4ba3df476b5e27c38a4979e0e627f275)
### But the line with tflite.run throws the following error
`tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input).`

Detailed Logcat
```
2020-05-05 16:46:05.199 8663-8755/co.chowagiken.tflite_android E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: co.chowagiken.tflite_android, PID: 8663
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/gather.cc:80 0 <= axis && axis < NumDimensions(input) was not true.
    Node number 222 (GATHER) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:154)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:314)
        at org.tensorflow.lite.Interpreter.run(Interpreter.java:275)
        at co.chowagiken.tflite_android.Classifier.InferImage(Classifier.java:196)
        at co.chowagiken.tflite_android.ClassifierActivity$1.run(ClassifierActivity.java:93)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:201)
        at android.os.HandlerThread.run(HandlerThread.java:65)

```

#### Converted Model
[mask_rcnn_coco_1024_quantize.tflite](https://drive.google.com/file/d/1K12f-NCQy52XmMqvnqMsgEQQA7PTTgRq/view?usp=sharing)
"
39178,"Something wrong with ""predict_on_batch""","<em>
Performance with predict_on_batch when using not determined input size is not good enough on tf.2.2.0. In my experiments with TensorFlow version 2.1 the prediction speed is for flexible input the same as fixed input, but for tf.2.2 prediction, speed is dramatically lower. Moreover, about 10 days ago, when I used nightly build the performance was good.
</em>

**System information**
- OS Platform: Linux Ubuntu 20.04
- TensorFlow installed from: pip
- TensorFlow version: 2.2 nightly. v1.12.1-31119-gce72f093cb 2.2.0-dev20200504
- Python version: 3.6
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: GTX1080 TI, 11Gb

**Describe the current behavior**
**for v2.1.0-rc2-17-ge5bf8de 2.1.0**

- **_Fixed input_**

```
from tensorflow.keras.applications.resnet50 import ResNet50
from tqdm import tqdm
import numpy as np


net = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)
for n in tqdm(range(1000)):
    a = net.predict_on_batch(np.random.randn(1, 224, 224, 3))

output:
100%|██████████| 1000/1000 [00:13<00:00, 74.74it/s]
```

- **_Flexible input_**

```
from tensorflow.keras.applications.resnet50 import ResNet50
from tqdm import tqdm
import numpy as np

net = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)
for n in tqdm(range(1000)):
    a = net.predict_on_batch(np.random.randn(1, np.random.randint(112, 224), 224, 3))
    
output:
100%|██████████| 1000/1000 [00:14<00:00, 68.15it/s] 
```

**for tf.2.2 nightly build (v1.12.1-31119-gce72f093cb 2.2.0-dev20200504)**

- **_Fixed input_**

```
from tensorflow.keras.applications.resnet50 import ResNet50
from tqdm import tqdm
import numpy as np

net = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)
for n in tqdm(range(1000)):
    a = net.predict_on_batch(np.random.randn(1, 224, 224, 3))
    
output:
100%|██████████| 1000/1000 [00:26<00:00, 38.24it/s]
```

- **_Flexible input_**

```
from tensorflow.keras.applications.resnet50 import ResNet50
from tqdm import tqdm
import numpy as np

net = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)
for n in tqdm(range(1000)):
    a = net.predict_on_batch(np.random.randn(1, np.random.randint(112, 224), 224, 3))

output:
  1%|          | 6/1000 [00:47<2:07:54,  7.72s/it]
```

upd:
for current stable version: v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2 
result is same:

- **_Flexible input_**

```
from tensorflow.keras.applications.resnet50 import ResNet50
from tqdm import tqdm
import numpy as np

net = ResNet50(input_shape=(None, 224, 3), weights=None, include_top=False)
for n in tqdm(range(1000)):
    a = net.predict_on_batch(np.random.randn(1, np.random.randint(112, 224), 224, 3))

output:
  0%|          | 4/1000 [00:31<2:12:39,  7.99s/it]
```"
39177,"tf.keras.layers.experimental.preprocessing.RandomXXX API is missing ""nearest"" as an option for fill_mode.","**System information**
- TensorFlow version (you are using): 2.2.0-dev20200504
- Are you willing to contribute it (Yes/No): Yes (but really depends on the technicality)

**Describe the feature and the current behavior/state.**
tf.keras.layers.experimental.preprocessing.RandomTranslation's argument ""fill_mode"" should accept the value of ""nearest"". This should be available to all relevant data augmentation experimental API that will inherit from Layer. fill_mode of ""nearest"" is the default for tf.keras.preprocessing.image.ImageDataGenerator. 

**Will this change the current api? How?**
This is about adding an extra fill_mode option for ""nearest"". It should be there since this is the default for the older Keras API (for data augmentation). This will allow apple to apple comparison to reproduce results.

**Who will benefit with this feature?**


**Any Other info.**
"
39176,Tensorflow lite conversion problem. Zero dimensions output node after conversion,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colaboratory
- TensorFlow installed from (source or binary): Colaboratory default
- TensorFlow version (or github SHA if from source):  2.2.0-rc3, Keras version: 2.3.0-tf


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
tflite_model = tf.keras.models.load_model('saved_model/my_model')
converter = tf.lite.TFLiteConverter.from_keras_model(tflite_model)
tflite_save = converter.convert()
open(""greek_smart_reply_model.tflite"", ""wb"").write(tflite_save)
```
**the output from keras model**

```
print(new_model.get_input_shape_at(0))
print(new_model)
for node in model.outputs:
  print(node)

(None, 40)
<tensorflow.python.keras.engine.sequential.Sequential object at 0x7f21efbcb240>
Tensor(""dense/Identity:0"", shape=(None, 3), dtype=float32)

```

**The output from the converter invocation**

```
interpreter = tf.lite.Interpreter('greek_smart_reply_model.tflite')
interpreter.allocate_tensors()
interpreter.get_tensor_details()

Output node:
{'dtype': numpy.float32,
  'index': 34,
  'name': 'Identity',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
   'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32)},
  'shape': array([], dtype=int32),
  'shape_signature': array([], dtype=int32),
  'sparsity_parameters': {}}

-----------Shape is empty array and should be [1,3]
```

**Also, please include a link to the saved model or GraphDef**

```
https://drive.google.com/open?id=1cAUPhmL4OsGUStsLcdFxYoX6c4L7u5aE
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results: Despite the fact that keras model predicts fine in colaboratory and has exactly output [1,3] when I convert it in .tflite the output node has zero dimensions:

Output node:
'dtype': numpy.float32,
  'index': 34,
  'name': 'Identity',
  'quantization': (0.0, 0),
  'quantization_parameters': {'quantized_dimension': 0,
   'scales': array([], dtype=float32),
   'zero_points': array([], dtype=int32)},
  'shape': array([], dtype=int32),
  'shape_signature': array([], dtype=int32),
  'sparsity_parameters': {}}

AND the same is stated when I load it inside android application:
2020-05-05 09:45:16.036 28527-28748/com.george.fs_korinthias E/INPUT_TENSOR_WHOLE: [1, 40]
2020-05-05 09:45:16.036 28527-28748/com.george.fs_korinthias E/INPUT_DATA_TYPE: FLOAT32
2020-05-05 09:45:16.037 28527-28748/com.george.fs_korinthias E/OUTPUT_TENSOR_SHAPE: []
2020-05-05 09:45:16.037 28527-28748/com.george.fs_korinthias E/OUTPUT_DATA_TYPE: FLOAT32

When I state output of interpreter as [1,3] android gives:

2020-05-05 09:55:07.570 29331-29421/com.george.fs_korinthias E/INPUT_TENSOR_WHOLE: [1, 40]
2020-05-05 09:55:07.571 29331-29421/com.george.fs_korinthias E/INPUT_DATA_TYPE: FLOAT32
2020-05-05 09:55:07.571 29331-29421/com.george.fs_korinthias E/OUTPUT_TENSOR_SHAPE: []
2020-05-05 09:55:07.571 29331-29421/com.george.fs_korinthias E/OUTPUT_DATA_TYPE: FLOAT32
2020-05-05 09:55:07.571 29331-29421/com.george.fs_korinthias E/DigitClassifier: Initialized TFLite interpreter.
2020-05-05 09:55:07.989 29331-29331/com.george.fs_korinthias E/MainActivity_classifier: Error to setting up smart reply classifier.
    java.lang.IllegalArgumentException: Cannot copy between a TensorFlowLite tensor with shape [] and a Java object with shape [1, 3].

Thank you in advance!!!!!




**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39175,ImportError: No module named utils using google colab,"Hi i am using google colab to run object_detection_tutorial.ipynb from https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10.I am using tensorflow cpu and anaconda prompt.I have added the utils module to path in anaconda prompt 
set PYTHONPATH=C:\tensorflow2\models;C:\tensorflow2\models\research;C:\tensorflow2\models\research\slim;C:\tensorflow2\models\research\object_detection\utils.Yet this issue is still continues

ImportErrorTraceback (most recent call last)
<ipython-input-3-aa270cd948af> in <module>()
----> 1 from utils import label_map_util
      2 
      3 from utils import visualization_utils as vis_util

ImportError: No module named utils
"
39174,C API Release,"Not sure if this is the right place to ask this, but I was wondering why the C API precompiled binaries have not been released for TF 2.x yet. Are you planning to release them soon? And I also have the same question for the java proto library.

cc @alextp "
39173,tf.keras.metrics.MeanIoU API is practically unusable without a threshold,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**

tf.keras.metrics.MeanIoU's constructor implementation does not take a threshold or list of thresholds as input argument. This is not only inconsistent the API used by other metrics (e.g. tf.keras.metrics.TruePositives, tf.keras.metrics.FalseNegatives) but also renders the API practically unusable because the outputs (i.e. predictions) from a network would generally be probability values in range from 0 to 1 and not a perfect 0 or 1 values. Hence, unless the constructor takes thresholds as argument and applies it to predictions before computing IoU, it is practically useless. It would always end up showing 0.5, or 0.25 or whatever the baseline random guess IOU happens to be in a given problem. 

**Describe the expected behavior**

tf.keras.metrics.MeanIoU constructor should take threshold values as input and also apply those before computing the IoU.

**Standalone code to reproduce the issue**
None required because the docs https://www.tensorflow.org/api_docs/python/tf/keras/metrics/MeanIoU proves the point where it only shows a example where preds are already binary values. 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39170,"""shuffle_and_repeat_fusion"" optimizer content incorrect on s390x arch (big-endian)","- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - s390x Ubuntu 18.04
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below):  v2.2.0-rc4-0-g70087ab4f4 2.2.0-rc4
- Python version: Python 3.6.9
- Bazel version (if compiling from source): Build label: 2.0.0- (@non-git)
Build target: bazel-out/s390x-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When running Tensorflow test cases on s390x, several hundred testcase fail with the following error:

/==========================/
[ RUN      ] 
...
ShuffleAndRepeatFusionTest.testShuffleAndRepeatFusion_test_mode_eager_tfapiversion_2
2020-04-30 19:20:21.809298: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at optimize_dataset_op.cc:66 : Internal: Tried to register a dataset optimizer that doesn't exist:
/==========================/

**Describe the expected behavior**
""shuffle_and_repeat_fusion"" optimizer should be correctly identified on s390x

**Standalone code to reproduce the issue**
On s390x system, run the following test case:

""python tensorflow/python/data/experimental/kernel_tests/optimization/shuffle_and_repeat_fusion_test.py""

The problem seems to occur when `tensorflow/core/kernels/data/optimize_dataset_op.cc` tries to OptimizeDatasetOp::MakeDataset for ""shuffle_and_repeat_fusion"" optimizer.  On s390x arch, optimization content is incorrect:

On s390x:
/================================/
(gdb) p optimizations
$3 = std::vector of length 1, capacity 1 = {{tstr_ = {u = {smll = {**size = 0 '\000',**
          str = ""\000\000\000\000\000\000e\000\000\000\000\000\000\000/\000\000\000\000\003\266;`""}, large = {
          size = 101, cap = 47, ptr = 0x3b63b60 ""shuffle_and_repeat_fusion""}, offset = {size = 0, offset = 101,
          count = 0}, view = {size = 101, ptr = 0x2f <error: Cannot access memory at address 0x2f>}, raw = {
          raw = ""\000\000\000\000\000\000\000e\000\000\000\000\000\000\000/\000\000\000\000\003\266;`""}}}}}
/================================/

On x86:

/===============================/
(gdb) p optimizations
$65 = {tstr_ = {u = {smll = {**size = 101 'e'**,
        str = ""\000\000\000\000\000\000\000/\000\000\000\000\000\000\000`\250\202\004\000\000\000""}, large = {size = 101,
        cap = 47, ptr = 0x482a860 ""shuffle_and_repeat_fusion""}, offset = {size = 101, offset = 0, count = 47}, view = {
        size = 101, ptr = 0x2f <error: Cannot access memory at address 0x2f>}, raw = {
        raw = ""e\000\000\000\000\000\000\000/\000\000\000\000\000\000\000`\250\202\004\000\000\000""}}}}
/===============================/

As we can see, **size**  is incorrectly extracted on s390x.  It is very likely that optimization contents need to be endian sensitive.  Specifically positioning of ""000e"" looks suspect.

I looked at shuffle_and_repeat_fusion.cc to get an idea but couldn't nail down where this is set.

Any pointers appreciated.

Thanks."
39169,No gradients provided for any variable,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch linux + Gnome
- TensorFlow version: tf-nightly==2.2.0.dev20200504 from `pip install tf-nightly`
- Python version: 3.8.2

**Describe the current behavior**
When running the training on the localhost the error occurs:

```
Epoch 1/1000
Traceback (most recent call last):
  File ""main.py"", line 155, in <module>
    h = model.fit(x=dtgen.next_train_batch(),
  File ""/home/arthur/Code/handwritten-text-recognition/src/network/model.py"", line 166, in fit
    out = self.model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs, verbose=verbose,
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 72, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 921, in fit
    tmp_logs = train_function(iterator)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 695, in __call__
    result = self._call(*args, **kwds)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 737, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 616, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2902, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3232, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3111, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 528, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:631 train_function  *
        return step_function(self, iterator)
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:621 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:952 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2292 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2651 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:614 run_step  **
        outputs = model.train_step(data)
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:581 train_step
        _minimize(self.distribute_strategy, tape, self.optimizer, loss,
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1946 _minimize
        gradients = optimizer._aggregate_gradients(zip(gradients,  # pylint: disable=protected-access
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:554 _aggregate_gradients
        filtered_grads_and_vars = _filter_grads(grads_and_vars)
    /home/arthur/Code/handwritten-text-recognition/.venv/lib/python3.8/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:1251 _filter_grads
        raise ValueError(""No gradients provided for any variable: %s."" %

    ValueError: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'gated_conv2d/kernel:0', 'gated_conv2d/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'gated_conv2d_1/kernel:0', 'gated_conv2d_1/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'gated_conv2d_2/kernel:0', 'gated_conv2d_2/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'bidirectional/forward_lstm/lstm_cell_1/kernel:0', 'bidirectional/forward_lstm/lstm_cell_1/recurrent_kernel:0', 'bidirectional/forward_lstm/lstm_cell_1/bias:0', 'bidirectional/backward_lstm/lstm_cell_2/kernel:0', 'bidirectional/backward_lstm/lstm_cell_2/recurrent_kernel:0', 'bidirectional/backward_lstm/lstm_cell_2/bias:0', 'dense/kernel:0', 'dense/bias:0', 'bidirectional_1/forward_lstm_1/lstm_cell_4/kernel:0', 'bidirectional_1/forward_lstm_1/lstm_cell_4/recurrent_kernel:0', 'bidirectional_1/forward_lstm_1/lstm_cell_4/bias:0', 'bidirectional_1/backward_lstm_1/lstm_cell_5/kernel:0', 'bidirectional_1/backward_lstm_1/lstm_cell_5/recurrent_kernel:0', 'bidirectional_1/backward_lstm_1/lstm_cell_5/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0'].
```

**Describe the expected behavior**
This issue didn't happen in previous versions, but I'm having to use python 3.8, thus the tf-nightly version. I also noticed that in the collab the code works, with the TF 2.1 version and python 3.7.

In addition, I found issues suggesting using:
````
with tf.GradientTape as tape:
[...]
````

However, the examples I saw use a customized training function (`train_step()`). In my case, I only use the standard `fit()` function, which is enough for the context.

Maybe I'm not sure how to use this in new version, or maybe it's an issue. Anyway, if anyone can help, thank you very much.

**Standalone code to reproduce the issue**
[Project code](https://github.com/arthurflor23/handwritten-text-recognition) and [model class](https://github.com/arthurflor23/handwritten-text-recognition/blob/master/src/network/model.py)"
39167,tf.io.gfile.listdir is inconsistent between GCS dir and local dir - adds trailing slashes,"
**System information**
- TensorFlow version (use command below): v2.0.0-rc2-26-g64c3d38 2.0.0
- Python version: 3.6

**Describe the current behavior**

When listing the directory items for a GCS dir, listdir returns names with trailing slashes. 
When listing the directory items for a local dir, listdir returns names without trailing slashes. 

**Describe the expected behavior**

The behavior needs to be consistent.

**Standalone code to reproduce the issue**

```
tensorflow.io.gfile.listdir('gs://bucket/dir')
>>> ['eval/', 'train/']

tensorflow.io.gfile.listdir('..')
>>> ['eval', 'train']
```"
39165,Invalid Argument Error with tf.math.add_n,"I am having an issue with the tf.add_n()/tf.math.add_n() command. I keep on getting an error no matter how I change it. I am using Tensorflow 2.1.0, while using Jupyter Notebook, and am still new to using TensorFlow. Here is my code and the errors that have been produced. It seems like a simple fix but I have no clue what to do.

import tensorflow as tf

a = tf.constant(6, name = 'constant_a')
b = tf.constant(3, name = 'constant_b')
c = tf.constant(10, name = 'constant_c')
d = tf.constant(15, name = 'constant_d')

mul = tf.multiply(a,b, name=""mul"")
div = tf.math.divide(c,d, name=""div"")
addn = tf.math.add_n([mul,div], name=""addn"") 
     
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-48-c8061239fcf2> in <module>
      5 #division operation which divides c by d and have the name ""div""
      6 #tf.add_n sums up the element in an array
----> 7 addn = tf.math.add_n([mul,div], name=""addn"")

c:\users\galvi\anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\util\dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

c:\users\galvi\anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\math_ops.py in add_n(inputs, name)
   3051       return array_ops.identity(values, name=name)
   3052     return values
-> 3053   return gen_math_ops.add_n(inputs, name=name)
   3054 
   3055 

c:\users\galvi\anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\ops\gen_math_ops.py in add_n(inputs, name)
    410         pass  # Add nodes to the TensorFlow graph.
    411     except _core._NotOkStatusException as e:
--> 412       _ops.raise_from_not_ok_status(e, name)
    413   # Add nodes to the TensorFlow graph.
    414   if not isinstance(inputs, (list, tuple)):

c:\users\galvi\anaconda3\envs\tensorflow\lib\site-packages\tensorflow_core\python\framework\ops.py in raise_from_not_ok_status(e, name)
   6604   message = e.message + ("" name: "" + name if name is not None else """")
   6605   # pylint: disable=protected-access
-> 6606   six.raise_from(core._status_to_exception(e.code, message), None)
   6607   # pylint: enable=protected-access
   6608 

c:\users\galvi\anaconda3\envs\tensorflow\lib\site-packages\six.py in raise_from(value, from_value)

InvalidArgumentError: cannot compute AddN as input #1(zero-based) was expected to be a int32 tensor but is a double tensor [Op:AddN] name: addn
"
39164,Build error for Android quickstart - Toolchain missing,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.1
- Python version: 3.8
- Bazel version (if compiling from source):  3.1.0


**Describe the problem**

I want to build a shared library that contains the C API for TensorflowLite.
Building a .dll with `bazel build -c opt //tensorflow/lite/c:tensorflowlite_c`  was no problem. My goal now is to build the C API on Windows for Android to get a an .so and i was running into some problems, while i followed the instructions on ../g3doc/guide/android.md  under ""Build TensorFlow Lite locally""

I am missing some toolchains and i do not know how to get them or how to set the correct Path. According to the tutorial i guess i probably should have them. Maybe the sdk/ndk paths are not set correctly since the ./configure file didn't react as expected (described further down).

**Build command:**
`bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/java:tensorflow-lite`

**ERROR message:**
D:\ComputerVision\tensorflow-master3\tensorflow-master>bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain //tensorflow/lite/java:tensorflow-lite
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from d:\computervision\tensorflow-master3\tensorflow-master\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/python.exe
INFO: Reading rc options for 'build' from d:\computervision\tensorflow-master3\tensorflow-master\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=v2
INFO: Reading rc options for 'build' from d:\computervision\tensorflow-master3\tensorflow-master\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/lib/site-packages --python_path=C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:v2 in file d:\computervision\tensorflow-master3\tensorflow-master\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file d:\computervision\tensorflow-master3\tensorflow-master\.bazelrc: --action_env=TF_ENABLE_XLA=1 --define=with_xla_support=true
INFO: Found applicable config definition build:windows in file d:\computervision\tensorflow-master3\tensorflow-master\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\computervision\tensorflow-master3\tensorflow-master\.bazelrc: --define framework_shared_object=false
ERROR: C:/users/nutzer/_bazel_nutzer/cncesu4u/external/local_config_cc/BUILD:47:1: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'x86'
ERROR: Analysis of target '//tensorflow/lite/java:tensorflow-lite' failed; build aborted: Analysis of target '@local_config_cc//:toolchain' failed
INFO: Elapsed time: 4.042s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (41 packages loaded, 395 targets configured)



**Steps executed before running into the problem**

I followed the instructions on ../g3doc/guide/android.md  under ""Build TensorFlow Lite locally"". I already have Bazel, sdk and ndk installed. So i ran ./configure.py but were not asked to to interactively configure the `./WORKSPACE` for Android builds, which seemed wrong according to the guide. Here is the ./configure promt:

`C:\Users\Nutzer\AppData\Local\Programs\Python\Python38\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\Nutzer\AppData\Local\Programs\Python\Python38\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:

Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.`


To fix the `./configure` problem i already tried:

1) Setting `ANDROID_SDK_HOME` and`ANDROID_NDK_HOME` in cmd (the highest API level should automatically be used right?)

2) Manually adding the env entries in `.tf_configure.bazelrc` so it looked like this:

`build --action_env PYTHON_BIN_PATH=""C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/lib/site-packages""
build --action_env ANDROID_NDK_HOME=""C:/Users/Nutzer/AppData/Local/Android/NDK/android-ndk-r17c""
build --action_env ANDROID_NDK_API_LEVEL=""21""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""28.0.3""
build --action_env ANDROID_SDK_API_LEVEL=""29""
build --action_env ANDROID_SDK_HOME=""C:/Users/Nutzer/AppData/Local/Android/Sdk""
build --python_path=""C:/Users/Nutzer/AppData/Local/Programs/Python/Python38/python.exe""
build --config=xla
build:opt --copt=/arch:AVX
build:opt --define with_default_optimizations=true
build --define=override_eigen_strong_inline=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""`

3) Adding the following sniped to the `WORKSPACE`:

android_sdk_repository(
    name = ""androidsdk"", # Required. Name *must* be ""androidsdk"".
    path = ""C:/Users/Nutzer/AppData/Local/Android/Sdk"", # Optional. Can be omitted if `ANDROID_HOME` environment variable is set.
)
android_ndk_repository(
    name = ""androidndk"", # Required. Name *must* be ""androidndk"".
    path = ""C:/Users/Nutzer/AppData/Local/Android/NDK/android-ndk-r17c"", # Optional. Can be omitted if `ANDROID_NDK_HOME` environment variable is set.
)


**Thank You for Your time and expertise!**
"
39163,Cannot make padded batches from dataset made from ragged tensor slices,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.2.0-rc4
- Python version: 3.7.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**

I can create a dataset from a ragged tensor using [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices), but I cannot make a padded batch from it with [`tf.data.Dataset.padded_batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch).

**Describe the expected behavior**

If the API is meant to support ragged tensors, then the dataset should allow me to make a padded batch from the ragged tensor slices. Otherwise, the API could be restricted to disallow ragged tensors.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
a = tf.ragged.stack([[1, 2], [3, 4, 5], [6], [7, 8, 9, 10]])
dataset = tf.data.Dataset.from_tensor_slices(a)
print(dataset)
# <TensorSliceDataset shapes: (None,), types: tf.int32>
for it in dataset:
    print(it.numpy())
# [1 2]
# [3 4 5]
# [6]
# [ 7  8  9 10]
batches = dataset.padded_batch(batch_size=2, padded_shapes=[5])
# TypeError: ('Padded batching of components of type ', <class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensorSpec'>, ' is not supported.')
```

Compare with a similar working example with a dataset taken from the documentation of [`tf.data.Dataset.from_generator`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator):

```python
import tensorflow as tf
import itertools
def gen():
    for i in itertools.count(1):
        yield (i, [1] * i)
dataset = tf.data.Dataset.from_generator(gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))
print(dataset)
# <FlatMapDataset shapes: ((), (None,)), types: (tf.int64, tf.int64)>
for it1, it2 in dataset.take(3):
    print(it1.numpy(), it2.numpy())
# 1 [1]
# 2 [1 1]
# 3 [1 1 1]
batches = dataset.padded_batch(batch_size=2, padded_shapes=([], [10]))
for it1, it2 in batches.take(3):
    print(it1.numpy(), it2.numpy())
# [1 2] [[1 0 0 0 0 0 0 0 0 0]
#  [1 1 0 0 0 0 0 0 0 0]]
# [3 4] [[1 1 1 0 0 0 0 0 0 0]
#  [1 1 1 1 0 0 0 0 0 0]]
# [5 6] [[1 1 1 1 1 0 0 0 0 0]
#  [1 1 1 1 1 1 0 0 0 0]]
```


**Other info / logs** NA"
39162,Support for TensorList crossing the XLA/TF boundary is not implemented,"A colab notebook to reproduce the issue: https://colab.research.google.com/drive/1O-ht27_h4d6qf_65nmK52qLTrMyDLMVr

I am trying to implement a simple RNN compiled with XLA. The code works without XLA, but when I try to compile one tf function with XLA and I get a strange error:

```
UnimplementedError:  Support for TensorList crossing the XLA/TF boundary is not implemented
	 [[node dummy_name/StatefulPartitionedCall (defined at <ipython-input-12-b36d3a96b1c7>:30) ]] [Op:__inference_simple_train_5054]

Errors may have originated from an input operation.
Input Source operations connected to node dummy_name/StatefulPartitionedCall:
 data (defined at <ipython-input-15-3f954a2dcc35>:38)
```

I am not sure what goes wrong when experimental_compile is added and if thats an expected behavior?"
39160,Issue in google codelabs handwritten digit classifier code.,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/index.html?index=..%2F..index#7

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

There is an error in the code of step 4 of the codelabs.
get a null safety error when implementing the code

"
39157,Inference problem using a tflite model java.lang.IllegalArgumentException: Invalid output Tensor index: 1,"[Input and output shape][1]
[Java exception][2]
how did you solve this exception please
java.lang.IllegalArgumentException: Invalid output Tensor index: 1.
i converted a yolov3-tiny model
i changed the NUM_DETECTION into 2535 (NUM_DETECTION=2535) because the input shape is (1,416,416,6) and the output shape is (1,2535,6).
I ve trained the model on license plates so it can detect them. like i said i've worked with the yolov3-tiny version with darknet, so i converted it to pb file then tflite file so i can use in an android app and detect plates in real time


  [1]: https://i.stack.imgur.com/Rhw4R.png
  [2]: https://i.stack.imgur.com/2ekGP.jpg"
39154,Unbundling sqlite doesn't work with TF_SYSTEM_LIBS,"Unable to use sqlite using TF_SYSTEM_LIBS

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 7.7 ppc64le
- TensorFlow installed from (source or binary): Source 
- TensorFlow version: 2.2.0-rc4 tag
- Python version: 3.6/3.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 2.0.0
- GCC/Compiler version (if compiling from source): 7.2
- CUDA/cuDNN version: 10.2/7.6.5

**Describe the problem**
I'm facing an error while using TF_SYSTEM_LIBS=org_sqlite in my environment. I have correctly set PREFIX, INCLUDEDIR and LIBDIR to point to the installed location of sqlite with proper suffixes include and lib to PREFIX. In third_party/systemlibs/sqlite.BUILD, I've also created a genrule to create the symlinks of sqlite's headers which works correctly but somehow my TF's build still fails for not able to find libsqlite.so. For this I've also explicitly set LD_LIBRARY_PATH to point to the location of libsqlite.so. Another attempt was to add --linkopt in the bazel build command but still no luck. Kindly provide some thoughts on this.

Also, I believe setting PREFIX, INCLUDEDIR and LIBDIR variables should suffice for TF to locate the headers and libraries for any external dependency, instead of creating symlinks to them or setting any additional variables like LD_LIBRARY_PATH.


**Provide the exact sequence of commands / steps that you executed before running into the problem**
Build flags
```
build --action_env TF_SYSTEM_LIBS=""org_sqlite""
build:linux --define=PREFIX=""$SYSTEM_LIBS_PREFIX""
build:linux --define=LIBDIR=""$SYSTEM_LIBS_PREFIX/lib""
build:linux --define=INCLUDEDIR=""$SYSTEM_LIBS_PREFIX/include""
build --action_env LD_LIBRARY_PATH=""$LD_LIBRARY_PATH:$SYSTEM_LIBS_PREFIX/lib""
``` 
where SYSTEM_LIBS_PREFIX is the path where sqlite is installed in my environment.

Bazel build command - 
```
bazel --bazelrc=$SRC_DIR/tensorflow/tensorflow.bazelrc build \
    --config=opt \
    --config=numa \
    --curses=no \
    --linkopt=""-L${SQLITE_LIB}"" \
    //tensorflow/tools/pip_package:build_pip_package
```

BUILD file changes for sqlite in thirdparty/systemlibs/sqlite.BUILD
```
--- a/third_party/systemlibs/sqlite.BUILD
+++ b/third_party/systemlibs/sqlite.BUILD
@@ -1,12 +1,31 @@
 licenses([""unencumbered""])  # Public Domain

+HEADERS = [
+   ""sqlite3.h"",
+   ""sqlite3ext.h"",
+]
+
 # Production build of SQLite library that's baked into TensorFlow.
 cc_library(
     name = ""org_sqlite"",
+    hdrs = HEADERS,
+    includes = ["".""],
     linkopts = [""-lsqlite3""],
     visibility = [""//visibility:public""],
 )

+genrule(
+    name = ""link_headers"",
+    outs = HEADERS,
+    message = ""Printing value of $$(INCLUDEDIR)"",
+    cmd = """"""
+      for i in $(OUTS); do
+        i=$${i##*/}
+        ln -sf $(INCLUDEDIR)/$$i $(@D)/$$i
+      done
+    """""",
+)
```
**Any other info / logs**
```
[[ERROR: [/tmp/tmpTfBuild/work/tensorflow/python/BUILD:5796:1: Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)
/tmp/tmpTfBuild/_build_env/bin/../lib/gcc/powerpc64le-conda_cos7-linux-gnu/7.2.0/../../../../powerpc64le-conda_cos7-linux-gnu/bin/ld: cannot find -lsqlite3
Target //tensorflow/tools/pip_package:build_pip_package failed to build
[ERROR: [/tmp/tmpTfBuild/work/tensorflow/tools/pip_package/BUILD:62:1 Linking of rule '//tensorflow/python:_pywrap_tensorflow_internal.so' failed (Exit 1)
```"
39152,TensorFlow can't be build for PS4 using Orbis LLVM compiler,"I tried to build TensorFlow for PS4. I added custom toolchain to bazel (and tested it on simple c++ project), but when I started to build Tensorflow I got a lot of errors.

Is it possible to resolve it? Current TensorFlow code can't get license for PS4 publishing.

```
[0 / 1,100] [Prepa] Creating source manifest for //tensorflow:tensorflow.dll
ERROR: C:/users/user/_bazel_user/6p42r4kl/external/com_google_protobuf/BUILD:295:1: C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1)
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:24:
external/com_google_protobuf/src\google/protobuf/arena.h:541:15: error: use of typeid requires enabling RTTI
    AllocHook(RTTI_TYPE_ID(T), n);
              ^
external/com_google_protobuf/src\google/protobuf/arena.h:194:30: note: expanded from macro 'RTTI_TYPE_ID'
#define RTTI_TYPE_ID(type) (&typeid(type))
                             ^
external/com_google_protobuf/src\google/protobuf/arena.h:604:15: error: use of typeid requires enabling RTTI
    AllocHook(RTTI_TYPE_ID(T), n);
              ^
external/com_google_protobuf/src\google/protobuf/arena.h:194:30: note: expanded from macro 'RTTI_TYPE_ID'
#define RTTI_TYPE_ID(type) (&typeid(type))
                             ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:251:3: error: no type named 'uintptr_t' in namespace 'std'; did you mean simply 'uintptr_t'?
  std::uintptr_t aliasing_ = kNoAliasing;
  ^~~~~
toolchain/orbis/target/include\sys/_types/_uintptr_t.h:13:22: note: 'uintptr_t' declared here
typedef __uintptr_t             uintptr_t;
                                ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:225:38: error: no type named 'uintptr_t' in namespace 'std'; did you mean simply 'uintptr_t'?
        aliasing_ = reinterpret_cast<std::uintptr_t>(flat.data()) -
                                     ^~~~~
toolchain/orbis/target/include\sys/_types/_uintptr_t.h:13:22: note: 'uintptr_t' declared here
typedef __uintptr_t             uintptr_t;
                                ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:226:38: error: no type named 'uintptr_t' in namespace 'std'; did you mean simply 'uintptr_t'?
                    reinterpret_cast<std::uintptr_t>(buffer_);
                                     ^~~~~
toolchain/orbis/target/include\sys/_types/_uintptr_t.h:13:22: note: 'uintptr_t' declared here
typedef __uintptr_t             uintptr_t;
                                ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:336:40: error: cannot initialize object parameter of type 'google::protobuf::internal::EpsCopyInputStream' with an expression of type 'google::protobuf::internal::ParseContext'
  bool Done(const char** ptr) { return DoneWithCheck(ptr, group_depth_); }
                                       ^~~~~~~~~~~~~
external/com_google_protobuf/src\google/protobuf/parse_context.h:337:51: error: cannot initialize object parameter of type 'google::protobuf::internal::EpsCopyInputStream' with an expression of type 'google::protobuf::internal::ParseContext'
  bool DoneNoSlopCheck(const char** ptr) { return DoneWithCheck(ptr, -1); }
                                                  ^~~~~~~~~~~~~
external/com_google_protobuf/src\google/protobuf/parse_context.h:359:33: error: cannot initialize object parameter of type 'google::protobuf::internal::EpsCopyInputStream' with an expression of type 'google::protobuf::internal::ParseContext'
    if (PROTOBUF_PREDICT_FALSE(!ConsumeEndGroup(tag))) return nullptr;
                                ^~~~~~~~~~~~~~~
external/com_google_protobuf/src\google/protobuf/port_def.inc:217:53: note: expanded from macro 'PROTOBUF_PREDICT_FALSE'
#define PROTOBUF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                                    ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:472:8: error: no type named 'uint32_t' in namespace 'std'; did you mean simply 'uint32_t'?
  for (std::uint32_t i = 0; i < 4; i++) {
       ^~~~~
toolchain/orbis/target/include\sys/_types/_uint32_t.h:13:20: note: 'uint32_t' declared here
typedef __uint32_t      uint32_t;
                        ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:475:25: error: no type named 'uint64_t' in namespace 'std'; did you mean simply 'uint64_t'?
    res += (static_cast<std::uint64_t>(tmp) - 2) << (14 * (i + 1) - 1);
                        ^~~~~
toolchain/orbis/target/include\sys/_types/_uint64_t.h:13:21: note: 'uint64_t' declared here
typedef __uint64_t              uint64_t;
                                ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:476:36: error: no member named 'int16_t' in namespace 'std'
    if (PROTOBUF_PREDICT_TRUE(std::int16_t(tmp) >= 0)) {
                              ~~~~~^
external/com_google_protobuf/src\google/protobuf/port_def.inc:206:55: note: expanded from macro 'PROTOBUF_PREDICT_TRUE'
#define PROTOBUF_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))
                                                      ^
In file included from external/com_google_protobuf/src/google/protobuf/compiler/code_generator.cc:39:
In file included from external/com_google_protobuf/src\google/protobuf/compiler/plugin.pb.h:26:
In file included from external/com_google_protobuf/src\google/protobuf/generated_message_table_driven.h:34:
In file included from external/com_google_protobuf/src\google/protobuf/map.h:49:
In file included from external/com_google_protobuf/src\google/protobuf/map_type_handler.h:34:
external/com_google_protobuf/src\google/protobuf/parse_context.h:575:14: error: cannot initialize object parameter of type 'google::protobuf::internal::EpsCopyInputStream' with an expression of type 'google::protobuf::internal::ParseContext'
  auto old = PushLimit(ptr, size);
             ^~~~~~~~~
external/com_google_protobuf/src\google/protobuf/parse_context.h:691:7: error: cannot initialize object parameter of type 'google::protobuf::internal::EpsCopyInputStream' with an expression of type 'google::protobuf::internal::ParseContext'
      ctx->SetLastTag(tag);
      ^~~
13 errors generated.
Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 196.531s, Critical Path: 2.13s
INFO: 5 processes: 5 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

```
INFO: Analyzed target //tensorflow:tensorflow (164 packages loaded, 13275 targets configured).
INFO: Found 1 target...
[0 / 975] [Prepa] BazelWorkspaceStatusAction stable-status.txt
ERROR: C:/users/user/_bazel_user/6p42r4kl/external/com_google_protobuf/BUILD:295:1: C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1)
external/com_google_protobuf/src/google/protobuf/compiler/subprocess.cc:41:10: fatal error: 'signal.h' file not found
#include <signal.h>
         ^~~~~~~~~~
1 error generated.
Target //tensorflow:tensorflow failed to build
ERROR: D:/workspace/tensorflow-build/target/tensorflow/tensorflow/c/BUILD:120:1 C++ compilation of rule '@com_google_protobuf//:protoc_lib' failed (Exit 1)

```

What parts of TF can we rewrite to get appropriate PS4 lib?"
39149,Rank-k cholesky up/downdates,"**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**

**Problem statement:** Given a positive-definite matrix `A` (n by n) with known Cholesky factor `L` (lower triangular), and another matrix `V` (n by k), compute the Cholesky up/down-dates of `A` with respect to `VV.T`, namely find *lower-triangular matrices* `M` and `K` such that `M M.T = A + V V.T` and  `K K.T = A - V V.T` (assuming the second is positive definite).

In my code I do this the costly way, by computing `A +/- V V.T` and then calling `tf.linalg.cholesky` on it. This takes O(n^3) time whereas this computation can be done in O(kn^2) time. This speedup is especially important as datasets grow (as n grows). For examples of algorithms that achieve this complexity see this [report by Seeger](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.585.5275&rep=rep1&type=pdf) and this [report by Walder](https://arxiv.org/abs/1011.1173). LINPACK implementations for [updates](http://www.netlib.no/netlib/linpack/dchud.f) and [downdates](http://www.netlib.no/netlib/linpack/dchdd.f) already exist and are [being used in R](https://www.rdocumentation.org/packages/SamplerCompare/versions/1.3.0/topics/chud).

**Currently:** The operator `tf.linalg.LinearOperatorLowRankUpdate` can be used to perform low-rank updates (i.e. compute `A +/- V V.T` from `A`) but what we are interested in is the Cholesky of `A +/- V V.T`. Using `tf.linalg.cholesky` computes the factor afresh, and does not exploit the fact that we know `L`.

**Desired feature:** Can we make a `tf.linalg.cholesky_rank_k_update` method which performs up/down-dates in O(kn^2) time?

**Will this change the current api? How?**
This could involve a new method as part of `tf.linalg`. However, I'm not sure what could be the best way to integrate this new functionality. Input and guidance on this and any other points of this request would be much appreciated!

**Who will benefit with this feature?**
Potentially many users in the ML community, working in applications which require this operation. I intend to use this feature for Gaussian Processes, but it would also be useful for many other applications in linear-algebraic and statistical problems. For example, if you want to up/down-date the Cholesky of large covariance matrix by a lower-rank matrix, for example to sample data from the up/down-dated Gaussian, you would benefit quite a bit from this speedup.
"
39148,How to enable C++ API usage from AAR builds (and integrate into gradle project)?,"
**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: Samsung Galaxy S10
- TensorFlow installed from: pip
- TensorFlow version: 2.1.0
- Python version: 3.5.2
- Installed using: virtualenv / pip
- Bazel version (if compiling from source): 3.0.0
- GCC/Compiler version (if compiling from source): 5.4.0

**Describe the problem**

How to enable C++ API usage from AAR builds (and integrate into gradle project)?
I'm trying to run a simple inference from the C++ API... what am I missing? I've tried following the guides at the TF website, and the Github repositories (minimal / image classification) but I'm missing the part where the TFLite library joins the Android Studio NDK.

Is there a way to use C++ API the same way I've been using the Java API via gradle? (kindly see attached comment by @jdduke)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

https://www.tensorflow.org/lite/guide/android
https://www.tensorflow.org/lite/guide/inference
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/minimal/minimal.cc
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/label_image/label_image.cc
https://www.tensorflow.org/lite/guide/build_arm64

**Any other info / logs**
_Originally posted by @jdduke in https://github.com/tensorflow/tensorflow/issues/34489#issuecomment-558261882_"
39147,Possible to get layer from graph?,"I seek to access a layer's attributes from within a model's optimizer; I suppose one way to do so is via the Graph, which is accessible using e.g. `K.get_session()` (Keras/tf.Keras). However, I only find ops and tensors using `.get_operations()` or `._nodes_by_name` - not layer instances themselves. Is it possible to access layers via the default (or other) Graph? [Mirror SO](https://stackoverflow.com/questions/61592020/get-layer-from-graph-tensorflow-keras)

<hr>

**Example**: fetch `recurrent_regularizer` of the LSTM layer without accessing `model` (except `model.optimizer`):


```python
from keras.layers import Input, LSTM
from keras.models import Model
from keras.regularizers import l2

ipt = Input(shape=(120, 4))
out = LSTM(60, recurrent_regularizer=l2(0), name='lstm_1')(ipt)
model = Model(ipt, out)
model.compile('adam', loss='mse')
```
```python
>> print(vars(model.layers[1].cell.recurrent_regularizer))
{'l1': array(0., dtype=float32), 'l2': array(1.e-04, dtype=float32)}
```"
39146,Loss over a padded and masked sequence,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 19.10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de
- Python version: 3.7

**Describe the current behavior**
The data I'm working on is a collection of sequences of different length. I have padded all the sequences to the same length and written an LSTM model that uses masks to ignore the padded part of the data.

The loss at each step is given by the sum of the losses at all points in the sequences divided by the total length of the sequences, original + padded part.

**Describe the expected behavior**
I expected the loss to be given by the sum of the losses at all points in the sequences divided by the total length **of the original sequences**, discarding the padded part.

Is this the intended behavior? And are the two fundamentally equivalent, from the point of view of the backprop algorithm?

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import LSTM

# Config
N = 32
T = 10
n = 2
mask_value = -1.
tf.random.set_seed(1)
np.random.seed(1)

# Data creation
X = np.ones((N, T, n)) * mask_value
Y = np.ones((N, T, 1)) * mask_value
for i in range(N):
    l = np.random.randint(1, T)
    value = np.random.random([l, n])
    X[i, :l] = value
    Y[i, :l] = np.array([sum(v) > 0.5 * n for v in value])[:, None]


class MyModel(Model):
    def __init__(self, n, mask_value, *args, **kwargs):
        super().__init__(name='MyModel', *args, **kwargs)
        self.mask_value = mask_value
        self.n = n
        self.LSTM = LSTM(self.n, return_sequences=True, activation='linear')
        return

    def call(self, inputs, training=None, mask=None):
        mask = tf.cast(tf.reduce_sum(inputs - self.mask_value, axis=-1), tf.bool)
        x = self.LSTM(inputs, mask=mask)
        return x


model = MyModel(n, mask_value)
model.build(input_shape=(N, T, n))
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy'],
)
model.summary()

mask = 1 - tf.cast(tf.reduce_all(tf.equal(X, mask_value), axis=-1), tf.float32)
loss_unmasked = tf.reduce_mean(tf.keras.losses.binary_crossentropy(Y, model.predict(X)))
loss_masked_1 = tf.reduce_sum(tf.keras.losses.binary_crossentropy(Y, model.predict(X)) * mask) / tf.reduce_sum(mask)
loss_masked_2 = tf.reduce_sum(tf.keras.losses.binary_crossentropy(Y, model.predict(X)) * mask) / (N * T)
print(f""model.evaluate(X, Y): {model.evaluate(X, Y)[0]:.2f}\n""
      f""loss_unmasked       : {loss_unmasked:.2f}\n""
      f""loss_masked_1       : {loss_masked_1:.2f}\n""
      f""loss_masked_2       : {loss_masked_2:.2f}""
      )
```
"
39145,TFLiteConverter,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39144,experimental_compile regression in 2.2.rc4,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 19.3
- TensorFlow installed from: pip
- TensorFlow version 2.2.0-rc4 / v2.2.0-rc3-33-g70087ab4f4
- Python version: 3.6 

The following code worked with `experimental_compile=True` in 2.1, but causes an error in 2.2.rc4:

**Standalone code to reproduce the issue**
```
import tensorflow as tf

@tf.function(experimental_compile=True)
def process_line(line):
    return tf.strings.split(line, "" "")

text = tf.data.Dataset.from_tensor_slices([""1 2""])
text = text.map(process_line)

for x in text:
    print(x)

# this one works in neither 2.1 nor 2.2rc4
# process_line(""1 2"")
```

**Other info / logs**
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Function invoked by the following node is not compilable: {{node PartitionedCall}} = PartitionedCall[Tin=[DT_STRING], Tout=[DT_STRING], _XlaMustCompile=true, _read_only_resource_inputs=[], config="""", config_proto=""\n\007\n\003GPU\020\000\n\007\n\003CPU\020\0012\002J\0008\001"", executor_type="""", f=__inference_process_line_84[]](args_0).
Uncompilable nodes:
PartitionedCall: could not instantiate call: '__inference_process_line_84'
	Stacktrace:
		Node: PartitionedCall, function: 

	 [[PartitionedCall]] [Op:MakeIterator]
```
"
39143,Empty profile overview page in Tensorboard,"**System information**
- Stock Tensorflow example
- Linux Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version (tf-nightly-gpu): v1.12.1-28892-g90dd57bb43 2.2.0-dev20200405
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: GTX 1080 8G

**Describe the current behavior**
I have been running the tutorial as given at https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras. I have installed the exact same TF version as used in the tutorial, the matching nightly Tensorboard version (2.3.0a20200405), and latest tensorboard_plugin_profile. 

The profile page displays correctly, but I'm getting only zeroes, and empty graph 
![image](https://user-images.githubusercontent.com/4252621/80950251-ca5fc780-8df5-11ea-8028-18438b4fced2.png)

Down the ""Performance summary"" panel, I'm also getting the message: ""WARNING: No step markers observed and hence the step time is actually unknown. This may happen if your profiling duration is shorter than the step time. In that case, you may try to profile longer.""

**Describe the expected behavior**
When opening Tensorboard, I was expecting non-zero numbers in the ""Performance Summary"" and ""Step-time Graph"" panels as shown in the figure from the tutorial (https://github.com/tensorflow/tensorboard/blob/master/docs/images/profiler_overview_page_bad_ip.png?raw=1). 

**Standalone code to reproduce the issue**
The mentioned tutorial as-is.
"
39142,Adding Tab press for showing available methods and autocompletion for objects,"**Describe the feature and the current behavior/state.**
Pressing the `tab` button does not show methods or autocomplete already defined objects when using Jupyter notebooks.

I tried this solution [https://stackoverflow.com/questions/54281719/how-to-get-code-completion-in-a-jupyter-notebook-running-in-docker](url). It didn't do anything when I tried it though.

`%config IPCompleter.greedy=True`

Example of how it works in Jupyter notebook when installed with conda.
```
import pandas as pd
pd.<tab_here>   # should show all availabe methods e.g. pd.read_csv(), pd.read_excel(), etc.
```

**System information**
I used the following below and the feature was missing. I don't know if it has been addressed or fixed in the new versions.
- TensorFlow Docker Image:  tensorflow/tensorflow:2.1.0-gpu-py3-jupyter
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- GPU model and memory: 1080Ti
- Are you willing to contribute it (Yes/No): No because I don't know how to implement it.

**Will this change the current api? How?**
I don't think so. It's just an add-on I think.

**Who will benefit with this feature?**
The showing of methods is a behavior that works when installing jupyter from conda and I would say is a standard feature in Jupyter notebooks. I've never seen anybody that couldn't hit tab and pull up methods or had autocomplete. It helps the user so they can see what options they have to call. This can save a massive amount of time and help the user be much more efficient."
39141,-,-
39140,Can't get nbextensions to work using Tensorflow docker gpu image,"**Describe the problem**
I am using a Jupyter gpu docker image on Ubuntu 18.04 LTS and I can't get nbextensions to work. I made a Dockerfile adding the nbextensions and I see them available on my screen but when I enable them nothing happens on my Jupyter Notebook. This would made my productivity if I could get these nbextensions to work. 


**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: Tensorflow Docker image
- GPU model and memory: 1080Ti

Here is the Docker file that I build my image
```
FROM tensorflow/tensorflow:2.1.0-gpu-py3-jupyter

RUN pip install --upgrade pip
RUN pip install jupyter_contrib_nbextensions
RUN jupyter contrib nbextension install --system
RUN pip install pandas 
RUN pip install matplotlib
RUN pip install scikit-learn
RUN pip install opencv-python 
```

Then I build it 
`docker build -t tf21 .`

Then run it 
`sudo docker run -u $(id -u):$(id -g) --gpus all --rm -it -p 8888:8888 -v $(pwd):/tf/notebook tf21`

From here I open the jupyter in a browser and click on the Nbextensions tab and uncheck 
`disable configuration for nbextensions without explicit compatibility (they may break your notebook environment, but can be useful to show for nbextension development)` 
Then I start checking the extensions I would like to use.

When I open my notebook, none of my selections work.

I'm not sure if this helps but this is what outputs on my terminal when I click on the extensions I want.

```
[E 07:15:56.931 NotebookApp] 500 PATCH /api/config/notebook (172.17.0.1) 1.44ms referer=http://127.0.0.1:8888/tree/notebooks
[E 07:15:57.742 NotebookApp] Uncaught exception PATCH /api/config/notebook (172.17.0.1)
    HTTPServerRequest(protocol='http', host='127.0.0.1:8888', method='PATCH', uri='/api/config/notebook', version='HTTP/1.1', remote_ip='172.17.0.1')
    Traceback (most recent call last):
      File ""/usr/local/lib/python3.6/dist-packages/tornado/web.py"", line 1697, in _execute
        result = method(*self.path_args, **self.path_kwargs)
      File ""/usr/local/lib/python3.6/dist-packages/tornado/web.py"", line 3174, in wrapper
        return method(self, *args, **kwargs)
      File ""/usr/local/lib/python3.6/dist-packages/notebook/services/config/handlers.py"", line 29, in patch
        section = self.config_manager.update(section_name, new_data)
      File ""/usr/local/lib/python3.6/dist-packages/notebook/services/config/manager.py"", line 34, in update
        return self.write_config_manager.update(section_name, new_data)
      File ""/usr/local/lib/python3.6/dist-packages/notebook/config_manager.py"", line 132, in update
        self.set(section_name, data)
      File ""/usr/local/lib/python3.6/dist-packages/notebook/config_manager.py"", line 109, in set
        self.ensure_config_dir_exists()
      File ""/usr/local/lib/python3.6/dist-packages/notebook/config_manager.py"", line 66, in ensure_config_dir_exists
        os.makedirs(self.config_dir, 0o755)
      File ""/usr/lib/python3.6/os.py"", line 210, in makedirs
        makedirs(head, mode, exist_ok)
      File ""/usr/lib/python3.6/os.py"", line 220, in makedirs
        mkdir(name, mode)
    PermissionError: [Errno 13] Permission denied: '/.jupyter'
[W 07:15:57.742 NotebookApp] Unhandled error
[E 07:15:57.742 NotebookApp] {
      ""Host"": ""127.0.0.1:8888"",
      ""User-Agent"": ""Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0"",
      ""Accept"": ""application/json, text/javascript, */*; q=0.01"",
      ""Accept-Language"": ""en-US,en;q=0.5"",
      ""Accept-Encoding"": ""gzip, deflate"",
      ""Content-Type"": ""application/json"",
      ""X-Xsrftoken"": ""2|92e82d6a|8f936aeda218e6eed7af91384caf3a27|1588574338"",
      ""X-Requested-With"": ""XMLHttpRequest"",
      ""Content-Length"": ""51"",
      ""Origin"": ""http://127.0.0.1:8888"",
      ""Dnt"": ""1"",
      ""Connection"": ""keep-alive"",
      ""Referer"": ""http://127.0.0.1:8888/tree/notebooks"",
      ""Cookie"": ""username-127-0-0-1-8888=\""2|1:0|10:1588575705|23:username-127-0-0-1-8888|44:MTJkNzE1ZmJhZWJlNDQzZjlhZmI2ZTM5MDIxMjFjMGE=|a976c227287b6c47a53e2e8460d1df1e8da047bb0ad73f418665f60dcf55fe7f\""; _xsrf=2|92e82d6a|8f936aeda218e6eed7af91384caf3a27|1588574338""
    }
```

Does anybody know how I can get this to work? I think it has something to do with the `/.jupyter` Permission denied.

By the way, you guys are doing an awesome job! TF2 is so much easier to use than before."
39139,module 'tensorflow' has no attribute 'keras',"Specifications:

win10 os
python 3.6
tensorflow 2.1.0

While loading the trained model, I used:
tf.keras.models.load_model

But it resulted in error, 
AttributeError: module 'tensorflow' has no attribute 'keras'"
39138,can't save stacked lstm,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
the following model cause error. The error message is ""RuntimeError: Unable to create link (name already exists)""

```python
#!/usr/bin/python3
import tensorflow as tf;

def lstm():
  inputs = tf.keras.Input((25,256));
  results = tf.keras.layers.RNN([tf.keras.layers.LSTMCell(units = 512) for i in range(2)])(inputs);
  return tf.keras.Model(inputs = inputs, outputs = results);

if __name__ == ""__main__"":
  m = lstm();
  m.save('lstm.h5');
```

but model without stacking lstm can be serialized successfully.

```python
#!/usr/bin/python3
import tensorflow as tf;

def lstm():
  inputs = tf.keras.Input((25,256));
  results = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(units = 512))(inputs);
  return tf.keras.Model(inputs = inputs, outputs = results);

if __name__ == ""__main__"":
  m = lstm();
  m.save('lstm.h5');
```

**Describe the expected behavior**

the serialization should be processed without problem.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3
import tensorflow as tf;

def lstm():
  inputs = tf.keras.Input((25,256));
  results = tf.keras.layers.RNN([tf.keras.layers.LSTMCell(units = 512) for i in range(2)])(inputs);
  return tf.keras.Model(inputs = inputs, outputs = results);

if __name__ == ""__main__"":
  m = lstm();
  m.save('lstm.h5');
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

>Traceback (most recent call last):
  File ""<stdin>"", line 3, in <module>
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/network.py"", line 1008, in save
    signatures, options)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/save.py"", line 112, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 109, in save_model_to_hdf5
    save_weights_to_hdf5_group(model_weights_group, model_layers)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/saving/hdf5_format.py"", line 631, in save_weights_to_hdf5_group
    param_dset = g.create_dataset(name, val.shape, dtype=val.dtype)
  File ""/home/xieyi/.local/lib/python3.6/site-packages/h5py/_hl/group.py"", line 139, in create_dataset
    self[name] = dset
  File ""/home/xieyi/.local/lib/python3.6/site-packages/h5py/_hl/group.py"", line 373, in __setitem__
    h5o.link(obj.id, self.id, name, lcpl=lcpl, lapl=self._lapl)
  File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper
  File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper
  File ""h5py/h5o.pyx"", line 202, in h5py.h5o.link
RuntimeError: Unable to create link (name already exists)
"
39136,Dataset.unbatch() sets cardinality to -2 even when batch size is known,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64 (1909)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.7.7
- CUDA/cuDNN version: Using CPU
- GPU model and memory: Using CPU

**Describe the current behavior**
Doing `Dataset.unbatch()` on dataset with known batch size resets cardinality to -2 (unknown).

**Describe the expected behavior**
When batch size of dataset is known, it should set cardinality to `batch_size * cardinality`.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
ds = tf.data.Dataset.range(10) # shape=()
ds = ds.batch(2, drop_remainder=True) # shape=(2,)
print(tf.data.experimental.cardinality(ds)) # 5
ds = ds.unbatch() # shape=()
print(tf.data.experimental.cardinality(ds)) # Should be 10, but is -2 (unknown)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
Although cardinality is currently experimental, it is used when traning keras model."
39135,mlir graph optimization is not built into tf-nightly,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: tf-nightly
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the problem**

Commit 248bc00 (@ezhulenev ) enables MLIR graph optimizations. However, it looks like this is not built as part of the tf-nightly. As a result, even though the following two methods are available:
```
tf.config.experimental.enable_mlir_graph_optimization
tf.config.experimental.disable_mlir_graph_optimization
```

There is no effect by toggle on/off the above two commands with tf-nightly installed.

From the commit, if the commit is carried in tf-nightly, then with 
```
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'

import tensorflow as tf
...
...
```

we should see something like below (base on the code in commit 248bc00):

```
  if (!config_proto.experimental().enable_mlir_graph_optimization()) {
    VLOG(1) << ""Skipping MLIR Graph Optimization Pass""
            << "", session flag not enabled"";
    return Status::OK();
  }
```


But the logs does not show the above contents.

Is there any way (e.g., pass certain build options) so that the generated pip wheel could carry the MLIR graph optimizations?




"
39132,tensorflow-gpu: Could not load dynamic library 'libcudart.so.10.1',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
```python
import tensorflow as tf
print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
```
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ArchLinux 64bits
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: tensorflow-gpu 2.2.0rc4
- **Python version**: 3.8.2
- **CUDA/cuDNN version**: 10.2.89-5 (from Archlinux repo)

### Describe the problem
TensorFlow doesn't use my GPU because there is a bug while trying to load ""libcudart.so.10.1"". My system has libcudart.so.10.2 installed
All the other libraries load fine since they look for libcu***.so.10 and not 10.1

### Source code / logs
```bash
Python 3.8.2 (default, Apr  8 2020, 14:31:25) 
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
>>> print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
2020-05-03 22:17:42.422067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-03 22:17:42.471297: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 22:17:42.472040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.683GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-05-03 22:17:42.472169: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-05-03 22:17:42.473728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-03 22:17:42.475267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-03 22:17:42.475499: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-03 22:17:42.477078: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-03 22:17:42.478053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-03 22:17:42.481446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-03 22:17:42.481478: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
Num GPUs Available:  0

```

Here is the CUDA lib I have installed:
```bash
/opt/cuda/doc/man/man7/libcudart.7
/opt/cuda/doc/man/man7/libcudart.so.7
/opt/cuda/targets/x86_64-linux/lib/libcudart.so
/opt/cuda/targets/x86_64-linux/lib/libcudart.so.10
/opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.2
/opt/cuda/targets/x86_64-linux/lib/libcudart.so.10.2.89
/opt/cuda/targets/x86_64-linux/lib/libcudart_static.a

```
"
39130,pip Install Error: don't could find version that satisfies the requirement tensorflow,"hi, I have win10 and I have tried install through cmd tensorflow with:
pip install tensorflow
pip install tensorflow-cpu
pip3 install --upgrade tensorflow
 
All these pip gave me the same error:

![image](https://user-images.githubusercontent.com/63893710/80923804-cbd8b380-8d53-11ea-8b1f-af41f380daab.png)

what's happened? What I need do to fix it?

"
39128,tensorflow in python,"1.  i tried import tnsorflow , and get alot of warning messemges 

2. after import, i runed this code : 

```
a = tf.Variable(1, name = ""a"")
b = tf.Variable(2, name = ""b"")
f = a+b
print(f)
```

but this was my put put : 
Tensor(""add:0"", shape=(), dtype=int32)

can someone know what is the problem ? and what to do ? 

*** i did :  pip   install --upgrade tensorflow
but it doesnt  #helped....







"
39122,"Error when trying to run on TPU - ""Failed to connect to all addresses""","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, but it used to work. 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.2.0-dev20200503
- Python version: 3.7.3

**Describe the current behavior**
Hi,
I'm trying to train a BERT-based model on a TPU (using TensorFlow models). It used to work perfectly a week ago, and I haven't changed my code since then, but now whenever I'm trying to run it (on a GCP VM instance connected to a TPU node) I get this:

```
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 196, in _create_device_dataset
    ds = _ReincarnatedPerDeviceGenerator(prototype_ds, incarnation_id)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 181, in __init__
    **self._flat_structure)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2070, in generator_dataset
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6810, in raise_from_not_ok_status
2020-05-03 15:47:19.234435: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: failed to
 connect to all addresses
Additional GRPC error information:
{""created"":""@1588520839.208445209"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588520839.208443301"",""description"":""f
ailed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1588520839.208445209"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588520839.208443301"",""description"":""f
ailed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]} [Op:GeneratorDataset]
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py"", line 2256, in async_wait
    context().sync_executors()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py"", line 652, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1588520839.208445209"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588520839.208443301"",""description"":""f
ailed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f6cc39591e0>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 539, in __del__
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1224, in delete_iterator
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6810, in raise_from_not_ok_status
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1588520839.208445209"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588520839.208443301"",""description"":""f
ailed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]} [Op:DeleteIterator]
Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f6cc39591e0>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 539, in __del__
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1224, in delete_iterator
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6810, in raise_from_not_ok_status
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1588520839.208445209"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588520839.208443301"",""description"":""f
ailed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]} [Op:DeleteIterator]
Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f6cc39591e0>
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 539, in __del__
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 1224, in delete_iterator
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 6810, in raise_from_not_ok_status
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1588520839.208445209"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588520839.208443301"",""description"":""f
ailed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]} [Op:DeleteIterator]
2020-05-03 15:47:19.384278: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op 
ID: 144, Output num: 0
Additional GRPC error information:
{""created"":""@1588520839.384194484"",""description"":""Error received from peer ipv4:10.71.22.42:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handl
e: Op ID: 144, Output num: 0"",""grpc_status"":3}
```

Any ideas about what could have happened?"
39121,TF 2.2 - Train_step output control,"**System information**
- TensorFlow version (you are using): 2.2 rc1
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
TF 2.2 allows modifying train_step() and make_train_function() to leverage the advantages of fit() with a lot of customization. However, due to train_step() being a tf.function, there is no way to extract variables from it (e.g. the model's output during training), other than by adding it to the *outputs* dictionary which goes directly to the logs. By customizing make_train_function() it is possible to do that even with multiple GPUs (concatenating accross devices), but adding that output to the logs creates a domino of problems when using callbacks.

Below is an example of mine where the model's outputs are stored with key *output* in the dict, concatenated accross devices, and returned to the dict, which is stored to the logs during each step in fit().

```
# In make_train_function()
...
def train_function(iterator):
    data = next(iterator)
    output_dict = self.distribute_strategy.run(
        self.train_step, args=(data,))
    m_output = output_dict.pop('output')
    m_output = reduce_per_replica(
        m_output, self.distribute_strategy, reduction='concat')
    output_dict = reduce_per_replica(
        output_dict, self.distribute_strategy, reduction='first')
    output_dict['output'] = m_output
    return output_dict
...

# Then in fit (without changes)
...
tmp_logs = train_function(iterator)
if not data_handler.inferred_steps:
  context.async_wait()
logs = tmp_logs
...
```

My suggestion would be changing the fit() function so that instead of assigning the output of train_function() to *tmp_logs*, it allows for additional outputs from train_step(), stored in a distinct variable which can then be accessed via callbacks. Below I have modified train_function() and fit() to return a list that would allow storing desired outputs in a separate variable.

```
def train_function(iterator):
    data = next(iterator)
    output_dict = self.distribute_strategy.run(
        self.train_step, args=(data,))
    output_dict = reduce_per_replica(
        output_dict, self.distribute_strategy, reduction='first')
    return output_dict, None

#OR with user modifications to get some output

def train_function(iterator):
    data = next(iterator)
    output_dict = self.distribute_strategy.run(
        self.train_step, args=(data,))
    m_output = output_dict.pop('output')
    m_output = reduce_per_replica(
        m_output, self.distribute_strategy, reduction='concat')
    output_dict = reduce_per_replica(
        output_dict, self.distribute_strategy, reduction='first')
    return output_dict, m_output

# Then in fit
...
tmp_logs, tmp_step_output = train_function(iterator)
if not data_handler.inferred_steps:
  context.async_wait()
logs, step_output = tmp_logs, tmp_step_output
...
```
With this modification and by giving all callbacks access to *step_output* users can customise train_function() and get whatever they want outside the training graph. These modifications would give them absolute freedom over it.

**Will this change the current api? How?**
As far as I can see the only changes would be the above code snippets in fit() and make_train_function(), and then adding *step_output* to the default inputs for callbacks.

**Who will benefit with this feature?**
Everyone. Other than model outputs it can be used to store gradients, and intermediate outcomes. It would allow for much better prototyping with minimal changes to the API.

**Any Other info.**
"
39120,TensorFlow Lite C++ API setup,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Ubuntu 20.04
- TensorFlow installed from (source or binary): installed from binary. (also have cloned using git clone)
- TensorFlow version: 2.1.0
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: conda



I want to setup TensorFlow Lite (TFLite) for C++ development. I searched google for it but didn't find anything. Tensorflow have posted regarding C++ development([documentation](https://www.tensorflow.org/lite/api_docs/cc)) but there's nothing about installation instruction. As of May 3, 2020 it seems to be in experimental mode. Though tensorflow has developed few [models](https://www.tensorflow.org/lite/models) out of it and build Android/iOS apps from it. I want to try those models on my own in C++. But here's a problem, to access header files required for C++  there is no direct support in tensorflow pip installation. This githubRepo of tensorflow need to be cloned which contains header files. Even after that to access some third-party header files that repository needs to be build. And I don't know how to build it. It'd be helpful if anyone working on TFLite C++ API shares how it can be done?


I want to include following header file to my C++ code. 
`#include ""tensorflow/lite/model.h""`
But I got error as `cannot open source file ""flatbuffers/flatbuffers.h""`. I searched complete repository but there isn't anything as `flatbuffers.h`. But I found a folder **flatbuffers** which contains some build files. I think if I can build those files I will be able to load header files I need. But I don't know how to build them.


Note : Just to be clear I want tensorflow lite C++ installation not tensorflow C api

Thank you.
"
39118,Can't run Tensorflow-Lite in NDK,"
**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: Samsung Galaxy S10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2
- Python version: 3.5.2
- Bazel version (if compiling from source): 3.0.0
- GCC/Compiler version (if compiling from source): 5.4.0



**Describe the problem**

I can build TFLite from binaries but I think it's not building correctly, or maybe only partially.
I can include the AAR / SO files into Android Studio, but I can't seem to use TFLite successfully in the NDK.

This should be a very basic task - I just want to run a simple inference in Android's NDK.
What am I missing here?
I tried to follow the basic examples from https://www.tensorflow.org/ or this repository's c++ examples, but the TFLite library does not seems to respond.

More information available at the StackOverflow post:
https://stackoverflow.com/questions/61523713/how-to-run-tensorflow-lite-in-android-studio-ndk
"
39116, Installation broken - Tensorflow 2.1 Cuda 10.1 Ubuntu 18.04,"Hi,

I'm struggling with the same issue as [here](https://github.com/tensorflow/tensorflow/issues/36121) when trying to install TF 2.1 with Cuda 10.1. I followed the [instructions](https://www.tensorflow.org/install/gpu) for Ubuntu 18.04 twice on two different machines, worked like a charm a few weeks ago, and now on this one I get stuck as described above when trying to install `cuda-10-1` (the other libraries can be installed, and `nvidia-smi` shows the gpus):
```
$ sudo apt-get install cuda-10-1
The following packages have unmet dependencies:
 cuda-10-1 : Depends: cuda-runtime-10-1 (>= 10.1.243) but it is not going to be installed
             Depends: cuda-demo-suite-10-1 (>= 10.1.243) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
```
When trying to install deps:
```
$ sudo apt-get install cuda-runtime-10-1
The following packages have unmet dependencies:
 cuda-runtime-10-1 : Depends: cuda-drivers (>= 418.87) but 410.0-0~dummy18.04 is to be installed
E: Unable to correct problems, you have held broken packages.
```
Finally:
```
$ sudo apt-get install cuda-drivers
cuda-drivers is already the newest version (410.0-0~dummy18.04).
```
I read the other thread but I'm not sure how @igorhoogerwoord  managed to go beyond this. I'd also avoid installing `tf-nightly` for now as @HardRockDude did if I can...

Thanks in advance for any help!"
39115,conv2d calculation results are inconsistent with pytorch,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information** 
- Have I written custom code (as opposed to using a stock
example script provided in TensorFlow): 
- OS Platform and Distribution (e.g.,
Linux Ubuntu 16.04): 
- TensorFlow installed from (source or
binary): - TensorFlow version (use command below): 
- Python version: 3.6.5
- CUDA/cuDNN version: - GPU model and memory:10.1

**Describe the current behavior**
For some specific data, the calculation results of tensorflow and pytorch are inconsistent
**Describe the expected behavior**
The calculation results of tensorflow and pytorch are consistent or the difference is small
**Standalone code to reproduce the issue** 

        if target_interface == 'conv1':
            weights_torch = torch.from_numpy(np.full((11, 11, 3, 1), 1, np.float64).transpose((3, 2, 0, 1)))
            output_pytorch_cpu = torch.from_numpy(
                F.conv2d(torch.from_numpy(input_pytorch.numpy().transpose((0, 3, 1, 2))), weights_torch, padding=0,
                         stride=4).numpy().transpose((0, 2, 3, 1)))
**Other info / logs** 
tensorflow_output:
31.42857361,31.4285183,31.42857361
69.24489594,69.24489594,59.3673439
100.89792633,100.89792633,100.89792633
133.26530457,133.26530457,133.26530457
60.22449875,60.22449875,60.22449875
187.46939087,187.46939087,187.46939087
205.918396,205.918396,205.918396
96.42860413,96.42860413,96.42860413
61.42854309,61.42854309,61.42854309
148.89811707,148.89811707,148.89811707
96.28572083,96.28572083,96.28572083
83.02045441,83.02045441,83.02045441
87.89794922,87.89794922,87.89794922
135.12240601,135.12240601,135.12240601


pytorch_output
31.428574,31.428518,31.428574
59.367344,69.244896,69.244896
100.89793,100.89793,100.89793
133.2653,133.2653,133.2653
60.2245,60.2245,60.2245
187.46939,187.46939,187.46939
205.9184,205.9184,205.9184
96.428604,96.428604,96.428604
61.428543,61.428543,61.428543
148.89812,148.89812,148.89812
96.28572,96.28572,96.28572
83.020454,83.020454,83.020454
87.89795,87.89795,87.89795
135.1224,135.1224,135.1224

 The version of tensorflow is 1.90. The error operators are conv2d () and avg_pooling (). Thank you.

This error is a fact, and it can be reproduced by using a large number of random tensors as input to conv2d () or avg_pooling (). Thank you."
39114,Using dynamic=True in a keras Metric constructor should run the metric methods in eager mode,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2.0rc4
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

Currently, when using `dynamic=True` in a Metric constructor, it has no effect. All the methods are run in graph mode. Since this feature is [already present in keras Layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer#arguments_20), and that a Metric is a subclass of Layer, one would expect a metric to run in eager mode if `dynamic=True` is provided.

**Will this change the current api? How?**

No change to the API. dynamic=True is already accepted in a Metric constructor. The change would be in behavior only. The flag should trigger the eager mode. Currently it doesn't do anything.

**Who will benefit with this feature?**

Users who want to make custom metrics which need eager mode and want to distribute them.

**Any Other info.**

Here is a minimal notebook reproducing the problem. With this feature implemented, the error should not happen:

https://colab.research.google.com/drive/11EuSPoyedceHySz_uxxqjZJbU2uatpfk

"
39113,cannot download micro speech pre-trained model ,"@tensorflow/micro

**System information**
windows 10 enterprise 
firefox 75.0

**Describe the problem**
when trying to download micro speech pre-trained model an xml with the following error is received:

`Anonymous caller does not have storage.objects.get access to download.tensorflow.org/models/tflite/micro/speech_commands_2020_04_13.zip`

**Please provide the exact sequence of commands/steps when you ran into the problem**
1. open readme.md: [https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech/train#trained-models](url)
2. click the download link: [https://storage.googleapis.com/download.tensorflow.org/models/tflite/micro/speech_commands_2020_04_13.zip](url)
3. get following error:
![image](https://user-images.githubusercontent.com/61374608/80912183-db500080-8d43-11ea-9710-386a7c45f179.png)

"
39112,TensorflowLite convertion Fatal error,"**System information**
- Windows Platform
- TensorFlow installed from (source or binary):source
- TensorFlow version (or github SHA if from source):1.15

i first build toco
`bazel run -c opt tensorflow/lite/toco`

then
`toco --graph_def_file=object_detection/inference_graph_for_tflite/tflite_graph.pb \ --output_file=object_detection/inference_graph_for_tflite/detect.tflite \ --input_shapes=1,300,300,3 \ --input_arrays=normalized_input_image_tensor \ --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \ --allow_custom_ops`

result
```Fatal Python error: Aborted```

```
Current thread 0x00003744 (most recent call first):
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52 in execute
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\site-packages\absl\app.py"", line 250 in _run_main
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\site-packages\absl\app.py"", line 299 in run
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40 in run
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89 in main
  File ""C:\Users\Acer\Anaconda3\envs\tensorflowX\Scripts\toco_from_protos.exe\__main__.py"", line 7 in <module>
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\runpy.py"", line 85 in _run_code
  File ""c:\users\acer\anaconda3\envs\tensorflowx\lib\runpy.py"", line 193 in _run_module_as_main
```

I was thinking that the error must be because of the issue i just post here:
```
https://github.com/tensorflow/tensorflow/issues/39111
```


"
39111,tensorflow build from source c++,"**System information**
- Windows Platform
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.15
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):2.0.0

Build Toco using bazel run at the end error show up, is this necessary, or it is ok to leave it be?

```ERROR: C:/tensorflow/tensorflow/core/framework/BUILD:591:1: C++ compilation of rule '//tensorflow/core/framework:bfloat16'```

here it is,

```ERROR: C:/tensorflowexpire/tensorflow/core/framework/BUILD:591:1: C++ compilation of rule '//tensorflow/core/framework:bfloat16' failed (Exit 2)
c:\users\acer\_bazel_acer\q66yhmmw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1028): error C2061: syntax error: identifier 'Kind'
c:\users\acer\_bazel_acer\q66yhmmw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1134): note: see reference to class template instantiation 'Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>' being compiled
c:\users\acer\_bazel_acer\q66yhmmw\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1037): error C2061: syntax error: identifier 'Kind'
Target //tensorflow/lite/toco:toco failed to build
INFO: Elapsed time: 1135.067s, Critical Path: 33.35s
INFO: 508 processes: 508 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully```
"
39110,MultiWorkerMirroredStrategy doesn't work without memory_growth,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Container image `tensorflow/tensorflow:2.1.0-gpu-py3` (tf-nightly-gpu installed)
- TensorFlow version (use command below): 2.2.0-dev20200501
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):gcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
- CUDA/cuDNN version: release 10.1, V10.1.243, but I can't find cuDNN libraries using a command `cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2`
- GPU model and memory: (NVIDIA Tesla T4(16G) * 4) * 2 nodes on GCP
- kubeflow 1.0.2 (kubernetes 1.15.11, tf-operator 1.0)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/4569678/tf_env.txt)

You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
TensorFlow version:  `v1.12.1-31004-g203aa8b634 2.2.0-dev20200501`

**Describe the current behavior**
if memory_growth of each GPU doesn't set to true as below
```
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    except RuntimeError as e:
        print(e)
```

Out of memory errors occur
```
2020-05-03 02:05:59.765039: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1686] Adding visible gpu devices: 0, 1, 2, 3
2020-05-03 02:05:59.769309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1085] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-03 02:05:59.769338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1091]      0 1 2 3
2020-05-03 02:05:59.769346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 0:   N Y N N
2020-05-03 02:05:59.769350: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 1:   Y N N N
2020-05-03 02:05:59.769354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 2:   N N N Y
2020-05-03 02:05:59.769359: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1104] 3:   N N Y N
2020-05-03 02:05:59.770642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.774976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.779921: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.785132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.789412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.793147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:0 with 13297 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
2020-05-03 02:05:59.793269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.801442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:1 with 13297 MB memory) -> physical GPU (device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5)
2020-05-03 02:05:59.801564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.817011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:2 with 13297 MB memory) -> physical GPU (device: 2, name: Tesla T4, pci bus id: 0000:00:06.0, compute capability: 7.5)
2020-05-03 02:05:59.817149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-05-03 02:05:59.831155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1230] Created TensorFlow device (/job:worker/replica:0/task:4/device:GPU:3 with 13323 MB memory) -> physical GPU (device: 3, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5)
2020-05-03 02:05:59.834444: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> dist-worker-0.default.svc:2222, 1 -> dist-worker-1.default.svc:2222, 2 -> dist-worker-2.default.svc:2222, 3 -> dist-worker-3.default.svc:2222, 4 -> localhost:2222}
2020-05-03 02:05:59.834974: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:390] Started server with target: grpc://localhost:2222
WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your
local data directory. If you'd instead prefer to read directly from our public
GCS bucket (recommended if you're running on GCP), you can instead pass
`try_gcs=True` to `tfds.load` or set `data_dir=gs://tfds-data/datasets`.

Dl Completed...: 100%|██████████| 4/4 [00:00<00:00,  7.41 file/s]2020-05-03 02:06:02.069644: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 12.99G (13943597824 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.072746: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 11.69G (12549237760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.075769: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 10.52G (11294313472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.078466: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 9.47G (10164881408 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.081365: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 8.52G (9148393472 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.084128: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 7.67G (8233553920 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.086712: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 6.90G (7410198528 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.089443: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 6.21G (6669178368 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.092258: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.59G (6002260480 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.094925: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.03G (5402034176 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:02.097778: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.53G (4861830656 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
.
.
.
.
2020-05-03 02:06:12.431390: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 5.0K (5120 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.433206: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.5K (4608 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.435134: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.2K (4352 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.437051: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 4.0K (4096 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.438725: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.8K (3840 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.440388: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.5K (3584 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.442053: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.2K (3328 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.444054: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 3.0K (3072 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.445745: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.8K (2816 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.447422: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.5K (2560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.449275: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.451169: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.453220: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.454900: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.456610: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.458291: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.460314: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.462209: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
2020-05-03 02:06:12.464132: I tensorflow/stream_executor/cuda/cuda_driver.cc:764] failed to allocate 2.2K (2304 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory
.
.
.
```

- nvidia-smi info
```
## Node 1
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   64C    P0    31W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   66C    P0    31W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |
| N/A   64C    P0    29W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   64C    P0    30W /  70W |  14220MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     11553      C   python                                     14209MiB |
|    1     11553      C   python                                     14209MiB |
|    2     11553      C   python                                     14209MiB |
|    3     11553      C   python                                     14209MiB |
+-----------------------------------------------------------------------------+


## Node 2
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla T4            On   | 00000000:00:04.0 Off |                    0 |
| N/A   59C    P0    28W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla T4            On   | 00000000:00:05.0 Off |                    0 |
| N/A   64C    P0    29W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  Tesla T4            On   | 00000000:00:06.0 Off |                    0 |
| N/A   68C    P0    32W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
^C|   3  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |
| N/A   66C    P0    30W /  70W |  15103MiB / 15109MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     18804      C   python                                     13541MiB |
|    0     18917      C   python                                      1013MiB |
|    0     18972      C   python                                       243MiB |
|    0     19285      C   python                                       295MiB |
|    1     18804      C   python                                       241MiB |
|    1     18917      C   python                                       301MiB |
|    1     18972      C   python                                     13537MiB |
|    1     19285      C   python                                      1013MiB |
|    2     18804      C   python                                       301MiB |
|    2     18917      C   python                                      1013MiB |
|    2     18972      C   python                                       243MiB |
|    2     19285      C   python                                     13535MiB |
|    3     18804      C   python                                       251MiB |
|    3     18917      C   python                                     13589MiB |
|    3     18972      C   python                                       239MiB |
|    3     19285      C   python                                      1013MiB |
+-----------------------------------------------------------------------------+
```


**Describe the expected behavior**
Most of all, I would like to know the reason why MultiWorkerMirroredStrategy doesn't work without `memory_growth`
and whether it is normal operation or not.
if it adversely affects performance, the strategy should work without memory_growth as the example code [here](https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras)

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

- My simple code
```
import os, json
os.environ['NCCL_DEBUG'] = 'INFO'
import tensorflow_datasets as tfds
import tensorflow as tf
tfds.disable_progress_bar()

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)

    except RuntimeError as e:
        print(e)


strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(communication=tf.distribute.experimental.CollectiveCommunication.NCCL) # NCCL vs RING

BATCH_SIZE = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE * 2
BUFFER_SIZE = 10000

print('Number of devices: {}'.format(strategy.num_replicas_in_sync))

def make_datasets_unbatched():
  # Scaling MNIST data from (0, 255] to (0., 1.]
  def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255
    return image, label

  datasets, info = tfds.load(name='mnist',
                            with_info=True,
                            as_supervised=True)

  return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE)

def build_and_compile_cnn_model():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])
  model.compile(
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=['accuracy'])
  return model

with strategy.scope():
  train_datasets = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE).repeat()
  options = tf.data.Options()
  options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
  train_datasets = train_datasets.with_options(options)
  multi_worker_model = build_and_compile_cnn_model()

multi_worker_model.fit(x=train_datasets, epochs=3, steps_per_epoch=5)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
39109,TensorFlow Lite Metadata duplication,"@lu-wang-g @petewarden @aselle

`ModelMetadata` defines another `description` property when one is already defined on `Model`.

Would it make sense to encode model level properties like `author`, `name`, `license`, `version` as `Metadata` properties on the model itself instead of defining them in `ModelMetadata`. That way all TensorFlow Lite models could carry such information as part of the model without relying on model_schema.fbs for decoding (similar to [metadata_props](https://github.com/onnx/onnx/blob/master/docs/IR.md) in ONNX).

See lutzroeder/netron#481"
39108,tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot serialize protocol buffer of type tensorflow.GraphDef as the serialized size (2643020353bytes) would be larger than the limit (2147483647 bytes),tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot serialize protocol buffer of type tensorflow.GraphDef as the serialized size (2643020353bytes) would be larger than the limit (2147483647 bytes)
39107,Implementation of custom operations in the layers,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**:
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
**Problem Description: If we want to implement our own custom operation in place of dot products there is no such provision to date. Right now I want to experiment with the Conv2d layer with the Coefficient of variation operation instead of Dot products.** 
Reference Stack Overflow question seeking for same kind of help:
https://stackoverflow.com/questions/47968757/how-to-make-a-customized-tf-nn-conv2d-of-tensorflow

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
39103,WARNING:tensorflow:Skipping full serialization of Keras model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.1.0
- Python version: 3.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: Nvidia K80 12GB

**Describe the current behavior**
I am trying to save Encoder and Decoder models using tf.saved_model.save as a part of a sequence to sequence problem. This models can be saved fine on my windows laptop, however when running on a remote server with GPU I get the following warning when saving:
WARNING:tensorflow:Skipping full serialization of Keras model <seq2seq_model.Encoder object at 0x7f4f2c2b53c8>, because its inputs are not defined.
WARNING:tensorflow:Skipping full serialization of Keras model <seq2seq_model.Decoder object at 0x7f4f2c1d5eb8>, because its inputs are not defined.

Whenever I try to then load the models using tf.saved_model.load() and then do some inference, I get TypeError: '_UserObject' object is not callable. This doesn't happen on if the saving was performed on my laptop.

**Describe the expected behavior**
The models should be saved without a warning, then loading the model and performing inference should work. I'm not sure why, when the model is saved on my laptop, I dont get a warning and it works fine.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional, Dropout

DROPOUT = 0.0
LSTM_DIM = 512

class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding, n_units, batch_size, use_segment_embedding, segment_embedding_dim):
        super(Encoder, self).__init__()
        self.n_units = n_units
        self.batch_size = batch_size
        
        self.embedding = embedding
        
        # segment embedding are used so that this model can better distinguish between persona and message segments
        # pad segment vectors with 0's exactly like word vectors
        if use_segment_embedding:
            # segment_embedding_dim must be the same as output_dim of word embedding
            self.segment_embedding = Embedding(3, segment_embedding_dim, trainable=True, mask_zero=True, name=""segment_embedding"")
        else:
            # use a zero segment embedding which will have no effect on the model
            self.segment_embedding = Embedding(3, segment_embedding_dim, weights=[np.zeros((3, segment_embedding_dim))], trainable=False, mask_zero=True, name=""segment_embedding"")
        
        self.lstm1 = LSTM(n_units, return_sequences=True, return_state=True, recurrent_initializer=""glorot_uniform"", name=""enc_lstm1"")
        
    @tf.function
    def call(self, inputs):
        input_utterence, segment_tokens, initial_state = inputs
        input_embed = self.embedding(input_utterence)
        segment_embed = self.segment_embedding(segment_tokens)
        
        combined_embed = tf.add(input_embed, segment_embed)
        
        encoder_states, h1, c1 = self.lstm1(combined_embed, initial_state=[initial_state, initial_state])
        
        return encoder_states, h1, c1
    
    def create_initial_state(self):
        return tf.zeros((self.batch_size, self.n_units))


class Decoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding, n_units, batch_size):
        super(Decoder, self).__init__()
        self.batch_size = batch_size
        self.n_units = n_units
        
        self.embedding = embedding
        
        self.lstm1 = LSTM(n_units, return_sequences=True, return_state=True, recurrent_initializer=""glorot_uniform"", name=""dec_lstm1"")
        
        self.dropout = Dropout(DROPOUT)
        
        # attention
        # Ct(s) = V tanh(W1 hs + W2 ht)
        # where hs is encoder state at timestep s and ht is the previous
        # decoder timestep (which is at timestep t - 1)
        self.W1 = Dense(n_units)
        self.W2 = Dense(n_units)
        self.V  = Dense(1)
        
        # from_logits=True in loss function, it will apply the softmax there for us
        self.out_dense1 = Dense(vocab_size)
    
    @tf.function
    def call(self, inputs):
        input_word, encoder_outputs, is_training, hidden = inputs
        h1, c1 = hidden
        
        # ------ Attention ------ #
        # => (batch_size, 1, n_units)
        decoder_state = tf.expand_dims(h1, 1)
        
        # score shape => (batch_size, src_timesteps, 1)
        score = self.V(
            tf.nn.tanh(self.W1(encoder_outputs) + self.W2(decoder_state)) )
        
        attn_weights = tf.nn.softmax(score, axis=1)
        
        # context vector is a weighted sum of attention weights with encoder outputs
        context_vec = attn_weights * encoder_outputs
        # => (batch_size, n_units)
        context_vec = tf.reduce_sum(context_vec, axis=1)
        # ------ ------ #
        
        input_embed = self.embedding(input_word)
        
        # feed context vector as input into LSTM at current timestep
        input_embed = tf.concat([tf.expand_dims(context_vec, 1), input_embed], axis=-1)
        
        decoder_output, h1, c1 = self.lstm1(input_embed, initial_state=[h1, c1])
        
        # (batch_size, 1, n_units) => (batch_size, n_units)
        decoder_output = tf.reshape(decoder_output, (-1, decoder_output.shape[2]))
        
        decoder_output = self.dropout(decoder_output, training=is_training)
        decoder_output = self.out_dense1(decoder_output)
        
        return decoder_output, attn_weights, h1, c1

vocab_size = 100

embedding_matrix = Embedding(vocab_size, 300, trainable=True, mask_zero=True, name=""tied_embedding"")

encoder = Encoder(vocab_size, embedding_matrix, LSTM_DIM, 64, True, 300)
decoder = Decoder(vocab_size, embedding_matrix, LSTM_DIM, 64)

enc_state = enc_state = encoder.create_initial_state()
out, h1, c1 = encoder([tf.random.uniform((64, 10), 0, 99),
        tf.random.uniform((64, 10), 0, 2),
        enc_state])

decoder([tf.random.uniform((64, 1), 0, 99), out, True, [h1, c1]])

encoder_fn = ""seq2seq_encoder""
decoder_fn = ""seq2seq_decoder""
decoder_states_spec = [
            tf.TensorSpec(shape=[None, LSTM_DIM], dtype=tf.float32, name='h1'), 
            tf.TensorSpec(shape=[None, LSTM_DIM], dtype=tf.float32, name='c1')]
        
tf.saved_model.save(encoder, encoder_fn , signatures=encoder.call.get_concrete_function(
        [
            tf.TensorSpec(shape=[None, None], dtype=tf.int32, name='input_utterence'),
            tf.TensorSpec(shape=[None, None], dtype=tf.int32, name='segment_tokens'),
            tf.TensorSpec(shape=[None, 512], dtype=tf.float32, name=""initial_state"")
        ]
))
    
tf.saved_model.save(decoder, decoder_fn, signatures=decoder.call.get_concrete_function(
        [
            tf.TensorSpec(shape=[None, None], dtype=tf.int32, name='input_word'), 
            tf.TensorSpec(shape=[None, None, LSTM_DIM], dtype=tf.float32, name=""encoder_output""),
            tf.TensorSpec(shape=[], dtype=tf.bool, name=""is_training""),
            decoder_states_spec
        ]
))

encoder = tf.saved_model.load(encoder_fn)
decoder = tf.saved_model.load(decoder_fn)

enc_state = enc_state = tf.zeros((64, LSTM_DIM))
out, h1, c1 = encoder([tf.random.uniform((64, 10), 0, 99),
        tf.random.uniform((64, 10), 0, 2),
        enc_state])

decoder([tf.random.uniform((64, 1), 0, 99), out, True, [h1, c1]])
```

Again the above code does run fine on my windows laptop, but not on the remote server (it's google deep learning vm)
"
39102,Tensorflow installation issues,"**System information**
- OS Platform and Distribution - Windows 10
- TensorFlow version: latest
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?: - virtualenv

Greetings,

I hope this is the correct place to submit an inquiry of this nature, if it is not, please forgive my confusion & please point me in the right direction. I greatly appreciate your time & consideration.

I am new to Python & Tensorflow. I've done some coding with C in the past, mostly when I was in college. I am determined to learn Python & to utilize both Python & Tensorflow for AI & Machine Learning purposes.

I've had difficulties in getting Tensorflow to install properly. I started by installing the latest version of Python which didn't seem to like my attempts at installing Tensorflow, I then went with Python 3.6.4-amd64. I installed that, created a fresh directory for my environments, then installed pip & virtual env, then created a virtual environment to setup with Tensorflow.

One of the confusing issues I keep encountering is that when I install pip & virtualenv, and eventually Tensorflow, it keeps sending it by default to C:\user\username\appdata\roaming\python etc, my question is, how do I prevent it from doing that? I am trying to install in the direct being utilized in the command prop, I call up the fresh directory I created for my virtual environment, then activate the virtual environment, and no matter what I do it keeps sending all new install files into the appdata/roaming user directory sub folders.

This is causing the incredibly annoying issue of making it impossible for me to proceed with utilizing Tensorflow because I get nothing but errors on missing files, path directory etc etc. I even tried manually moving some of the files over to the virtual environment directory and that worked in some cases, but did not solve the overall problem.

Okay, now that I've made it painfully apparent how much of an uneducated newbie I am with all of this, may someone please give me some advice. The first step is admitting you need help, and I clearly do as I've spent several hours with my eyes glued to various articles and tutorials that have left me with more questions than answers. I truly appreciate any help you're willing to provide. Just a loner trying to figure this all out & increase my knowledge along the way. Thanks for your time,
"
39101,TFLite Interpreter fails to load quantized model on iPhone,"**Similar issue: https://github.com/tensorflow/tensorflow/issues/28163**

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock MobilenetV2
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: iPhone 5s
- TensorFlow installed from (source or binary): pip install tensorflow==2.1
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.4
- CUDA/cuDNN version: 10.1
- GPU model and memory: Geforce GTX 1650, 4 GB

**Describe the current behavior**
I have created a new tflite model based on MobilenetV2.
tf.keras.applications.MobileNetV2(input_shape=[SIZE, SIZE, 3], include_top=False)

It works well without quantization using CPU on iOS. I should say that TensorFlow team did a great job, many thanks.

Unfortunately there is a problem with latency.  I have read TF documentation related to optimization (post trainig quantization) and workerd with dynamic range quantization.

I executed the following python code:

```
tflite_models_dir = pathlib.Path(""/tmp/mnist_tflite_models/"")
tflite_models_dir.mkdir(exist_ok=True, parents=True)
converter = tf.lite.TFLiteConverter.from_saved_model('C:\Work\Python\NN\MobileNet_v2_128')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quant_model = converter.convert()
tflite_model_quant_file = tflite_models_dir/""MobileNet_v2_128_quant.tflite""
tflite_model_quant_file.write_bytes(tflite_quant_model)
```

After this model was added to XCode project on MAC. Pod file contains the following framework:
_pod 'TensorFlowLite', '~> 1.13.1'_

**Error:**
tflite::InterpreterBuilder return this error: ""Didn't find op for builtin opcode 'CONV_2D' version '2'""

**Describe the expected behavior**
I suppose this should work faster without errors.

**Other info / logs** 
I also tried to use float16 quantization.

Python code:
```
converter = tf.lite.TFLiteConverter.from_saved_model('C:\Work\Python\NN\MobileNet_v2_128')
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_quant_model = converter.convert()
open(""MobileNet_v2_128_qua_float16.tflite"", ""wb"").write(tflite_quant_model)
```

In swift code I used MetalDelegate

With _pod 'TensorFlowLiteSwift', '0.0.1-nightly'_
I have no errors, but model doesn’t work

With _pod 'TensorFlowLiteSwift', '2.1.0'_
I have the following error:

2020-05-01 21:36:13.578369+0300 TFL Segmentation[6367:330410] Initialized TensorFlow Lite runtime. 2020-05-01 21:36:20.877393+0300 TFL Segmentation[6367:330397] Execution of the command buffer was aborted due to an error during execution. Caused GPU Hang Error (IOAF code 3)

Is it possible to use MobilenetV2.tflite quantized model in XCode Swift project?

Best regards,
Dmitriy"
39100,Python Keras model in C++,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win10 and Ubuntu 18
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): yes
- TensorFlow version (use command below): 2.01
- Python version: 3.7
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
If I set names in Keras and want to use the trained model in C++ then there seem to be no names for the nodes, so I can't set inputs or get outputs.

**Standalone code to reproduce the issue**
Python Code on the Windows 10 machine:

```
import tensorflow as tf
from tensorflow.keras import layers

model = tf.keras.Sequential()

model.add(layers.Input(2, name='INPUT'))
# Adds a densely-connected layer with 64 units to the model:
model.add(layers.Dense(8, name='layer1', input_shape=(2,), activation='tanh'))
# Add another:
model.add(layers.Dense(4, name='layer2', input_shape=(8,), activation='sigmoid'))
# Add an output layer with 1 output unit:
model.add(layers.Dense(1, name='outputlayer', input_shape=(4,), activation='sigmoid'))

model.compile(optimizer=tf.keras.optimizers.Adam(0.01),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=['accuracy'])
model.summary()

print(model.inputs[0].name)
import numpy as np

data = np.array([
    [0, 0],
    [1, 0],
    [0, 1],
    [1, 1]
])
labels = np.array([
    [0],
    [1],
    [1],
    [0]
])

model.fit(data, labels, epochs=500)
tf.keras.models.save_model(model, 'model')
```
C++ code on the linux machine:

```
tensorflow::Tensor inputs(tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 2}));
auto input_mapped = inputs.tensor<float, 2>();
input_mapped(0, 0) = 0.0;
input_mapped(0, 1) = 1.0;

std::vector<std::pair<std::string, tensorflow::Tensor>> inp = {{""INPUT"", inputs}};

std::vector<tensorflow::Tensor> outputs;
std::vector<std::string> output_names({""output""});

status = model.session->Run(inp, output_names, {}, &outputs);

```
What is the problem here? Also if I set names for the layers in Keras, save the model, load it again than the names are replaced with others. And if I use these other names in the C++ application it doesn't work.

Here are the error message:

2020-05-02 13:38:31.256880: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:502] model_pruner failed: Internal: Could not find node with name 'outputlayer'

Invalid argument: Tensor INPUT:0, specified in either feed_devices or fetch_devices was not found in the Graph


"
39099,[Regression] Cloud TPU crashes with  UnavailableError: failed to connect to all addresses,"- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tf-nightly/ 2.2.0-dev20200502
- Python version: 3.7
- TPU model : TPU v3-8 with tf-nightly

**Describe the current behavior**
I have been using TPU v3-8's with tf-nightly without any issues, however I started  receiving this after upgrading the VM and TPU v3 from 2.2.0-dev20200429 to latest tf-nightly ie. 2.2.0.dev20200502.

Edit [1] : The notebook runs fine on colab with `2.2.0-rc3`
Edit [2] :  Not fixed in `2.2.0.dev20200504` aswell

I cannot post my codebase here since it is pretty big, and since the official guide for tpu also triggers the same error, I have attached it here.

**Standalone code to reproduce the issue**
[notebook to reproduce the bug](https://gist.github.com/srihari-humbarwadi/c9bd4b2dec4666252a8d6319ddd87060)

**Other info / logs**
```
UnavailableError                          Traceback (most recent call last)
<ipython-input-10-8b804771207f> in <module>
      1 model.fit(train_dataset,
      2           epochs=5,
----> 3           validation_data=test_dataset)

~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
     70   def _method_wrapper(self, *args, **kwargs):
     71     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
---> 72       return method(self, *args, **kwargs)
     73 
     74     # Running inside `run_distribute_coordinator` already.

~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
    893       data_handler._initial_epoch = (  # pylint: disable=protected-access
    894           self._maybe_load_initial_epoch_from_ckpt(initial_epoch))
--> 895       for epoch, iterator in data_handler.enumerate_epochs():
    896         self.reset_metrics()
    897         callbacks.on_epoch_begin(epoch)

~/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/data_adapter.py in enumerate_epochs(self)
   1153     """"""Yields `(epoch, tf.data.Iterator)`.""""""
   1154     with self._truncate_execution_to_epoch():
-> 1155       data_iterator = iter(self._dataset)
   1156       for epoch in range(self._initial_epoch, self._epochs):
   1157         if self._insufficient_data:  # Set by `catch_stop_iteration`.

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __iter__(self)
    706     worker_iterators = _create_iterators_per_worker(self._cloned_datasets,
    707                                                     self._input_workers,
--> 708                                                     enable_legacy_iterators)
    709     if enable_legacy_iterators:
    710       iterator = DistributedIteratorV1(self._input_workers, worker_iterators,

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in _create_iterators_per_worker(worker_datasets, input_workers, enable_legacy_iterators)
   1368       if tf2.enabled() and not enable_legacy_iterators:
   1369         iterator = _SingleWorkerOwnedDatasetIterator(worker_datasets[i], worker,
-> 1370                                                      worker_devices)
   1371       else:
   1372         iterator = _SingleWorkerDatasetIterator(worker_datasets[i], worker,

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, worker, devices, components, element_spec)
   1225         raise ValueError(error_message)
   1226       super(_SingleWorkerOwnedDatasetIterator, self).__init__(dataset, worker,
-> 1227                                                               devices)
   1228 
   1229   def _make_iterator(self):

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in __init__(self, dataset, worker, devices)
   1071     self._devices = devices
   1072     self._element_spec = dataset.element_spec
-> 1073     self._make_iterator()
   1074 
   1075   def _make_iterator(self):

~/tf/lib/python3.7/site-packages/tensorflow/python/distribute/input_lib.py in _make_iterator(self)
   1235     with ops.device(self._worker):
   1236       self._iterator = multi_device_iterator_ops.OwnedMultiDeviceIterator(
-> 1237           self._dataset, self._devices, source_device=host_device)
   1238 
   1239   @property

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py in __init__(self, dataset, devices, max_buffer_size, prefetch_buffer_size, source_device, components, element_spec)
    560                                       incarnation_id, prefetch_buffer_size,
    561                                       experimental_slack)
--> 562           iterator = iter(ds)
    563           self._device_iterators.append(iterator)
    564           iterator_handles.append(iterator._iterator_resource)  # pylint: disable=protected-access

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py in __iter__(self)
    407     """"""
    408     if context.executing_eagerly() or ops.inside_function():
--> 409       return iterator_ops.OwnedIterator(self)
    410     else:
    411       raise RuntimeError(""__iter__() is only supported inside of tf.function ""

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in __init__(self, dataset, components, element_spec, job_token)
    602           context.context().device_spec.device_type != ""CPU""):
    603         with ops.device(""/cpu:0""):
--> 604           self._create_iterator(dataset)
    605       else:
    606         self._create_iterator(dataset)

~/tf/lib/python3.7/site-packages/tensorflow/python/data/ops/iterator_ops.py in _create_iterator(self, dataset)
    628               output_shapes=self._flat_output_shapes))
    629       if self._job_token is None:
--> 630         gen_dataset_ops.make_iterator(ds_variant, self._iterator_resource)
    631       else:
    632         gen_experimental_dataset_ops.make_data_service_iterator(

~/tf/lib/python3.7/site-packages/tensorflow/python/ops/gen_dataset_ops.py in make_iterator(dataset, iterator, name)
   2950       return _result
   2951     except _core._NotOkStatusException as e:
-> 2952       _ops.raise_from_not_ok_status(e, name)
   2953     except _core._FallbackException:
   2954       pass

~/tf/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6808   message = e.message + ("" name: "" + name if name is not None else """")
   6809   # pylint: disable=protected-access
-> 6810   six.raise_from(core._status_to_exception(e.code, message), None)
   6811   # pylint: enable=protected-access
   6812 

~/tf/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

UnavailableError: failed to connect to all addresses
Additional GRPC error information:
{""created"":""@1588419621.643701261"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3937,""referenced_errors"":[{""created"":""@1588419621.215578031"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]} [Op:MakeIterator]
```
@jhseu   "
42801,!head {train_file_path}报错,"'head' 不是内部或外部命令，也不是可运行的程序
或批处理文件。"
39098,Feature request for supporting  stateful RNN with tf.distribute.Strategy,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
   2.2.0-rc3
- Are you willing to contribute it (Yes/No):
No


**Describe the feature and the current behavior/state.**
 Currently RNNs with stateful=True not yet supported with tf.distribute.Strategy.
 So, my feature request is to make it supported.
**Will this change the current api? How?**
Can't say.

**Who will benefit with this feature?**
Everyone from the community is going to benefit from this because training RNN models takes a lot of time to train. Therefore, we trained our model via TPUs with distributed strategy. For example, in Shakespear's texts generation project, we've to use stateful RNNs. So, if this feature is going to be added then we can train our model via TPUs and with distributive strategy.


"
39097,Inconsistent Result between CPU build and GPU build,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS Catalina 10.15.2 / Linux Ubuntu 16
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MacBook Air (Retina, 13-inch, 2019)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.1.0-rc2-17-ge5bf8de410 2.1.0
- Python version: 3.6.6
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  10.1
- GPU model and memory: RTX 2080 ti 11GB x4


**Describe the current behavior**
Test runs on laptop, 
```
WARNING:tensorflow:From /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
WARNING:tensorflow:From /Users/zijingzhang/Library/Python/3.6/lib/python/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
Train for 3 steps, validate for 3 steps

1/3 [=========>....................] - ETA: 4s - loss: 0.6949 - accuracy: 0.5000
3/3 [==============================] - 3s 961ms/step - loss: 0.6948 - accuracy: 0.5000 - val_loss: 0.6943 - val_accuracy: 0.5000
ok

----------------------------------------------------------------------
Ran 1 test in 282.183s

OK
```
but when deployed on server-GPU/CI, it shows error:
```
venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op
         raise ValueError(str(e))
     ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.
```
**Describe the expected behavior**
Consistent result between laptop and server.
**Standalone code to reproduce the issue**
### Setup
./data/sample.csv
```csv
id,comment_text,lang,toxic
0,""Este usuario ni siquiera llega al rango de    hereje   . Por lo tanto debería ser quemado en la barbacoa para purificar su alma y nuestro aparato digestivo mediante su ingestión.    Skipe linkin 22px   Honor, valor, leltad.      17:48 13 mar 2008 (UTC)"",es,0
1,""Il testo di questa voce pare esser scopiazzato direttamente da qui. Immagino possano esserci problemi di copyright, nel fare cio ."",it,0
2,""Vale. Sólo expongo mi pasado. Todo tiempo pasado fue mejor, ni mucho menos, yo no quisiera retroceder 31 años a nivel particular. Las volveria a pasar putas.Fernando "",es,1
3,""Bu maddenin alt başlığı olarak  uluslararası ilişkiler  ile konuyu sürdürmek ile ilgili tereddütlerim var.Önerim siyaset bilimi ana başlığından sonra siyasal yaşam ve toplum, siyasal güç, siyasal çatışma, siyasal gruplar, çağdaş ideolojiler, din, siyasal değişme, kamuoyu, propaganda ve siyasal katılma temelinde çoğulcu siyasal sistemler.Bu alt başlıkların daha anlamlı olacağı kanaatindeyim."",tr,0
4,""Belçika nın şehirlerinin yanında ilçe ve beldelerini yaparken sanırım Portekizi örnek alacaksın. Ben de uzak gelecekte(2-3 yıl) bu tip şeyler düşünüyorum. Tabii futbol maddelerinin hakkından geldikten sonra..    daha önce mesajlarınızı görmüştüm, hatta anon bölümünü bizzat kullanıyordum   sözünü anlamadım??  tanışmak bugüneymiş gibi bir şey eklemeyi düşündüm ama vazgeçtim. orayı da silmeyi unuttum. boşverin Kıdemli   +    "",tr,0
5,""güzel, zaten kaynaklandırması zor subjektif kategoriler bunlar. bazı maddelerden çıkartmak, kiminden çıkartıp kiminde bırakmak, çıkartılanları yerine iade etmek yerine Kategori:Milletlere karşı ırkçı duygular dakilerin tamamına el atmak gerekiyor.    kibele    "",tr,0
6,""No es mala idea. De hecho, yo estaba pensando descolgarme ya del reto mensual, pero esto vuelve a ilusionarme. El problema es que contabilizar los artículos a mano es muy tedioso, así que habría que disponer de una herramienta tipo escaladix que lo hiciera automáticamente. Y tampoco estaría mal disponer de resultados parciales cada tres meses o similar. Yo me apunto ) . Saludos de "",es,0
7,""Kod hatalarını düzeltmişsiniz,elinize sağlık çok teşekkürler.Önceki sürümleri araştırdım.13 Haziran 2010 da Kullanıcı:Türk Süvarisi nin yaptığı şu değişikliğe kadar 107 kaynak var ve benim görebildiğim kadarıyla kaynaklarda hata yok.Türk Süvarisi nin değişikliğinden sonra kaynak sayısı 111 e çıkıyor ve kaynak hataları oluşuyor.Bana Türk Süvarisi hatalı bir şekilde yeni kaynaklar eklemiş gibi geldi.Siz ne dersiniz?Yardımcı olursanız çok sevinirim.Bu arada İran ı KM adayı göstermeyi düşünüyorum ve bu nedenle maddeyi düzenlemeye çalışıyorum.Vikipedi:Madde incelemesi/İran/arşiv1 de siz de görüşlerinizi belirtirseniz maddenin gelişimi adına katkısı olur diye düşünüyorum.İngiltere Portalı nda sizin ve diğer vikipedistlerin bahsettiği yorumsal ifadeleri ve yazım hatalarını düzelttim.İsterseniz portala tekrar göz atın.Sevgiler, iyivikiler   Âkhilleus       mesaj    "",tr,0
```
### Reproduction

#### requirements.txt
```txt
tensorflow-hub
tensorflow_text>=2.0.0rc0
```

#### USE.py
```
import tensorflow_hub as hub
import tensorflow_text

module_url = 'https://tfhub.dev/google/universal-sentence-encoder-multilingual/3'


def embed_text(input):
    model = hub.load(module_url)
    return model(input)

```

#### Model.py
```
import tensorflow as tf


def build():
    inputs = tf.keras.Input(shape=(512,), name=""use_input"")
    hl1 = tf.keras.layers.Dense(
        963, activation=tf.nn.relu, name='hidden_layer_1')(inputs)
    hl2 = tf.keras.layers.Dense(
        369, activation=tf.nn.relu,  name='hidden_layer_2')(hl1)
    hl3 = tf.keras.layers.Dense(
        69, activation=tf.nn.relu,  name='hidden_layer_3')(hl2)
    hl4 = tf.keras.layers.Dense(
        3, activation=tf.nn.relu,  name='hidden_layer_4')(hl3)
    outputs = tf.keras.layers.Dense(
        2, activation=tf.nn.softmax, name='prediction')(hl4)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model

```
#### DataProcessing.py
```
import functools

import numpy as np
import tensorflow as tf
import USE

LABEL_COLUMN = 'toxic'
LABELS = [0, 1]


def get_dataset(file_path, batch_size=3, **kwargs):
    dataset = tf.data.experimental.make_csv_dataset(
        file_path,
        # Artificially small to make examples easier to show.
        batch_size=batch_size,
        label_name=LABEL_COLUMN,
        na_value=""?"",
        num_epochs=1,
        ignore_errors=True,
        **kwargs)
    return dataset


def show_batch(dataset):
    for batch, label in dataset.take(1):
        for key, value in batch.items():
            print(key)
            print(value)
            # print(""{:20s}: {}"".format(key, value.numpy()))
        print(label)


def embed_batch(dataset):
    embeddings = []
    for batch, label in dataset.take(1):
        for key, value in batch.items():
            # print(key)
            # print(value)
            for comment in value:
                # print(comment)
                emb = USE.embed_text(comment)
                emb = emb.numpy()
                # print(emb)
                embeddings.append(emb)

            # print(""{:20s}: {}"".format(key, value.numpy()))
        # print(label)
        # print(embeddings)
        # print(dataset)
        return embeddings, label

# show_batch(raw_train_data)
# print(filtered_dataset)
# show_batch(filtered_dataset)

```
#### Training.py

```python
import Model
import DataProcessing
import tensorflow as tf


def fit(training_file_path='./data/sample.csv', validation_file_path=""./data/sample.csv"", batch_size=3,
        SELECT_COLUMNS=['toxic', 'comment_text'], model_saved='test'):
    training_dataset = DataProcessing.get_dataset(
        training_file_path, select_columns=SELECT_COLUMNS, batch_size=batch_size)
    validation_dataset = DataProcessing.get_dataset(
        training_file_path, select_columns=SELECT_COLUMNS, batch_size=batch_size)
    useEmbedding, labels = DataProcessing.embed_batch(training_dataset)
    dataset = tf.data.Dataset.from_tensor_slices((useEmbedding, labels))
    useEmbedding, labels = DataProcessing.embed_batch(validation_dataset)
    validation = tf.data.Dataset.from_tensor_slices((useEmbedding, labels))
    model = Model.build()
    model.summary()
    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='binary_crossentropy',
                  metrics=['accuracy'])
    model.save('./model/'+model_saved)
    return model.fit(dataset, validation_data=validation)

Training.fit()
```
#### Command
```
pip install -r requirements.txt && python Training.py
```
**Other info / logs** 

```
======================================================================
 ERROR: test_fit (test_training.TestTraining)
 ----------------------------------------------------------------------
 Traceback (most recent call last):
   File ""/builds/zzj0402/project/test/test_training.py"", line 8, in test_fit
     self.assertTrue(Training.fit(), ""Training should run."")
   File ""/builds/zzj0402/project/Training.py"", line 21, in fit
     return model.fit(dataset, validation_data=validation)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper
     return method(self, *args, **kwargs)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 848, in fit
     tmp_logs = train_function(iterator)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
     result = self._call(*args, **kwds)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
     self._initialize(args, kwds, add_initializers_to=initializers)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 506, in _initialize
     *args, **kwds))
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
     graph_function = self._create_graph_function(args, kwargs)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
     capture_by_value=self._capture_by_value),
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
     func_outputs = python_func(*func_args, **func_kwargs)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
     return weak_wrapped_fn().__wrapped__(*args, **kwds)
   File ""/builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
     raise e.ag_error_metadata.to_exception(e)
 ValueError: in user code:
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *
         outputs = self.distribute_strategy.run(
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **
         return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica
         return self._call_for_each_replica(fn, args, kwargs)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica
         return fn(*args, **kwargs)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py:533 train_step  **
         y, y_pred, sample_weight, regularization_losses=self.losses)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/compile_utils.py:213 __call__
         batch_dim = array_ops.shape(y_t)[0]
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:984 _slice_helper
         name=name)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py:1150 strided_slice
         shrink_axis_mask=shrink_axis_mask)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py:10179 strided_slice
         shrink_axis_mask=shrink_axis_mask, name=name)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
         attrs=attr_protos, op_def=op_def)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal
         compute_device)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal
         op_def=op_def)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1817 __init__
         control_input_ops, op_def)
     /builds/zzj0402/project/venv/lib/python3.6/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op
         raise ValueError(str(e))
     ValueError: slice index 0 of dimension 0 out of bounds. for '{{node strided_slice}} = StridedSlice[Index=DT_INT32, T=DT_INT32, begin_mask=0, ellipsis_mask=0, end_mask=0, new_axis_mask=0, shrink_axis_mask=1](Shape, strided_slice/stack, strided_slice/stack_1, strided_slice/stack_2)' with input shapes: [0], [1], [1], [1] and with computed input tensors: input[1] = <0>, input[2] = <1>, input[3] = <1>.
```"
39096,"Problem in importing Tensorflow: ""ImportError: DLL load failed while importing""","I have a problem in importing TensorFlow. My system is protected in ProgramData. So, I used the virtual environment and installed TensorFlow in my user environment. The TensorFlow is installed, but can not be imported. I checked other websites and related issues, but non of them solved my issue.

The versions of the python: Python 3.8.2
The version of TensorFlow:  1.13.1
The versions of the pip: pip 20.1
CPU: has avx instruction
Environment: virtual environment installed from ""https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/20/conda/""
Error: ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors
for some common reasons and solutions. Include the entire stack trace
above this error message when asking for help.

Thanks."
39094,Add Bluetooth support to send Box Locations array,"**System information**
- Tensorflow Lite

**Describe the feature and the current behavior/state.**

Add support to send box locations over Bluetooth

**Who will benefit with this feature?**

Developers on automation projects
"
39093,Keras Custom Layer could not receive `training` parameter,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, custom test script. Provided in the description section.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Not on mobile

- TensorFlow installed from (source or binary):
binary version from pip: tensorflow==1.14.0

- TensorFlow version (use command below):
`v1.14.0-rc1-22-gaf24dc91b5 1.14.0`

- Python version:
`Python 3.5.6 :: Anaconda, Inc.`

- Bazel version (if compiling from source):
```
Build label: 0.25.2
Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Fri May 10 20:47:48 2019 (1557521268)
Build timestamp: 1557521268
Build timestamp as int: 1557521268
```

- GCC/Compiler version (if compiling from source):
Not compiled from Source - `gcc version 7.4.0 (Ubuntu 7.4.0-1ubuntu1~16.04~ppa1)`

- CUDA/cuDNN version:
Not using GPU version TF

- GPU model and memory:
Not using GPU version TF

**Describe the current behavior**

Tested with an example script:

```
import tensorflow as tf

import numpy as np
from tensorflow.python.ops import math_ops
from tensorflow.python import keras

class MyLayer(keras.layers.Layer):

    def call(self, inputs, training=None):
        # Expecting training to be set
        if training is not None:
            self.add_loss(math_ops.reduce_sum(inputs))

        return inputs


inputs = keras.Input((3,))
layer = MyLayer()
outputs = layer(inputs)
model = keras.Model(inputs, outputs)
model.compile('sgd', 'mse', run_eagerly=False)
loss = model.fit(np.ones((2, 3)), np.ones((2, 3)))

print(loss.history)
```

**Describe the expected behavior**

The print out loss should not 0, instead it should be ""6"", since during training, `self.add_loss` would run.

The behavior is more based on tests in tests in TF source code:
https://github.com/tensorflow/tensorflow/blob/v1.14.0/tensorflow/python/keras/engine/base_layer_test.py#L861

And online doc: https://www.tensorflow.org/guide/keras/custom_layers_and_models#privileged_training_argument_in_the_call_method

**Standalone code to reproduce the issue**
Pasted above.

**Other info / logs**
Above script output:

```
$: python3 test_layer.py
WARNING:tensorflow:Entity <bound method MyLayer.call of <__main__.MyLayer object at 0x7faaf7402fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MyLayer.call of <__main__.MyLayer object at 0x7faaf7402fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4
2020-05-01 17:38:01.840747: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-05-01 17:38:01.863084: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz
2020-05-01 17:38:01.864264: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55efb64ae370 executing computations on platform Host. Devices:
2020-05-01 17:38:01.864288: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-05-01 17:38:01.879314: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.
2/2 [==============================] - 0s 33ms/sample - loss: 0.0000e+00
{'loss': [0.0]}
```
Loss is ""0"" as the training branch of code didn't run.
Also, if enabled `eager_execution` in the code, the print out is more reasonable:

```
import tensorflow as tf
tf.enable_eager_execution()

import numpy as np
from tensorflow.python.ops import math_ops
from tensorflow.python import keras

class MyLayer(keras.layers.Layer):

    def call(self, inputs, training=None):
        # Expecting training to be set
        if training is not None:
            self.add_loss(math_ops.reduce_sum(inputs))

        return inputs


inputs = keras.Input((3,))
layer = MyLayer()
outputs = layer(inputs)
model = keras.Model(inputs, outputs)
model.compile('sgd', 'mse', run_eagerly=True)
loss = model.fit(np.ones((2, 3)), np.ones((2, 3)))

print(loss.history)

# Print out is ""6"" as training branch works.
```
"
39092,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.   Failed to load the native TensorFlow runtime.,"- OS Platform: Windows 10 64-bit
- TensorFlow version: 2.0.0
- Python version: 3.7.6
- pip version: 20.1

Installed the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017, and 2019 from
https://support.microsoft.com/en-us/help/2977003/the-latest-supported-visual-c-downloads
and updated using the file: https://aka.ms/vs/16/release/vc_redist.x64.exe 
as mentioned in Step 1 from https://www.tensorflow.org/install/pip#windows_1 

- Using anaconda: 
I installed tensorflow CPU only version from

pip install tensorflow==2.0.0

To test the successful installation I used the following command:
python -c ""import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))""

Output:

(base) C:\Users\chandru>python -c ""import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))""
Traceback (most recent call last):
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\chandru\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\chandru\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 98, in <module>
    from tensorflow_core import *
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\chandru\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\chandru\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\chandru\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\chandru\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


#Please guide me through the installation process. I'm not able to use the TensorFlow backend for Keras model which gives me the above same error."
39091,Autograph fails to convert the eager mode code to graph,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):2.1.0
- Python version:3.7.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A




I wanted to write a simple layer that would work on the output of tf.experiment.make_csv_dataset and i could use to impute the missing values in numeric dtypes with batch mean, maintain a moving mean to be used at test time, create embeddings for categorical columns and keep the dimensions dependent on the predefined list of unique values. 


Below is the code i wrote:

```
import tensorflow as tf
import pandas as pd
import numpy as np
from tensorflow.keras import layers
from tensorflow import feature_column
class NUM_TO_DENSE(layers.Layer):
    def __init__(self,num_cols):
        super().__init__()
        self.keys = num_cols
        self.keys_all = self.keys+[str(i)+'__nullcol' for i in self.keys]
    def build(self,input_shape):
        def create_moving_mean_vars():
            return tf.Variable(initial_value=0.,shape=(),dtype=tf.float32,trainable=False)
        self.moving_means_total = {t:create_moving_mean_vars() for t in self.keys}
        self.layer_global_counter = tf.Variable(initial_value=0.,shape=(),dtype=tf.float32,trainable=False)

    def call(self,inputs, training = True):
        null_cols = {k:tf.math.is_finite(inputs[k]) for k in self.keys}
        current_means = {}
        def compute_update_current_means(t):
            current_mean = tf.math.divide_no_nan(tf.reduce_sum(tf.where(null_cols[t],inputs[t],0.),axis=0),\
                                  tf.reduce_sum(tf.cast(tf.math.is_finite(inputs[t]),tf.float32),axis=0))
            self.moving_means_total[t].assign_add(current_mean)
            return current_mean
        
        if training:
            current_means = {t:compute_update_current_means(t) for t in self.keys}
            outputs = {t:tf.where(null_cols[t],inputs[t],current_means[t]) for t in self.keys}
            outputs.update({str(k)+'__nullcol':tf.cast(null_cols[k],tf.float32) for k in self.keys})
            self.layer_global_counter.assign_add(1.)
        else:
            outputs = {t:tf.where(null_cols[t],inputs[t],(self.moving_means_total[t]/self.layer_global_counter))\
                       for t in self.keys}
            outputs.update({str(k)+'__nullcol':tf.cast(null_cols[k],tf.float32) for k in self.keys})
        return outputs


class PREPROCESS_MONSOON(layers.Layer):
    def __init__(self,cat_cols_with_unique_values,num_cols):
        '''cat_cols_with_unqiue_values: (dict) {'col_cat':[unique_values_list]}
        num_cols: (list) [num_cols_name_list]'''
        super().__init__()
        self.cat_cols = cat_cols_with_unique_values
        self.num_cols = num_cols
    def build(self,input_shape):
        self.ntd = NUM_TO_DENSE(self.num_cols)
        self.num_colnames = self.ntd.keys_all
        self.ctd = {k:layers.DenseFeatures\
                    (feature_column.embedding_column\
                     (feature_column.categorical_column_with_vocabulary_list\
                      (k,v),tf.cast(tf.math.ceil(tf.math.log(tf.cast(len(self.cat_cols[k]),tf.float32))),tf.int32).numpy()))\
                   for k,v in self.cat_cols.items()}
        self.cat_colnames = [i for i in self.cat_cols]
        self.dense_colnames = self.num_colnames+self.cat_colnames
    def call(self,inputs,training=True):
        dense_num_d = self.ntd(inputs,training=training)
        dense_cat_d = {k:self.ctd[k](inputs) for k in self.cat_colnames}
        
        dense_num = tf.stack([dense_num_d[k] for k in self.num_colnames],axis=1)
        dense_cat = tf.concat([dense_cat_d[k] for k in self.cat_colnames],axis=1)
        dense_all = tf.concat([dense_num,dense_cat],axis=1)
        return dense_all
```
creating data to test this
```
    mnist = tf.keras.datasets.mnist

    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train, x_test = x_train / 255.0, x_test / 255.0

    x_train_ = pd.DataFrame(x_train.reshape(60000,-1),columns = ['col_'+str(i) for i in range(28*28)])
    x_test_ = pd.DataFrame(x_test.reshape(10000,-1),columns = ['col_'+str(i) for i in range(28*28)])
    x_train_['col_cat1'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]
    x_test_['col_cat1'] = [np.random.choice(['a','b','c','d','e','f','g','h','i','j']) for i in range(x_test_.shape[0])]
    x_train_['col_cat2'] = [np.random.choice(['a','b','c','d','e','f','g','h','i']) for i in range(x_train_.shape[0])]
    x_test_['col_cat2'] = [np.random.choice(['a','b','c','d','e','f','g','h','i','j']) for i in range(x_test_.shape[0])]
    x_train_[np.random.choice([True,False],size = x_train_.shape,p=[0.05,0.95]).reshape(x_train_.shape)] = np.nan
    x_test_[np.random.choice([True,False],size = x_test_.shape,p=[0.05,0.95]).reshape(x_test_.shape)] = np.nan
    x_train_.to_csv('data/x_train.csv',index=False)
    x_test_.to_csv('data/x_test.csv',index=False)
```
getting one batch of created in ram
```
cdtypes = pd.read_csv('data/x_train.csv',nrows=2).dtypes
xtb = tf.data.experimental.make_csv_dataset('data/x_train.csv',32,header=True,prefetch_buffer_size=1,
                                           column_defaults=[np.nan if i == (float or int) else '__missing__' for i in cdtypes])
for i in xtb:
    break
dd = pd.read_csv('data/x_train.csv',nrows=2).head()
num_cols = [i for i in dd.columns if i not in ['col_cat1','col_cat2']]
cat_cols = [i for i in dd.columns if i in ['col_cat1','col_cat2']]

col_cat1_unique = ['a','b','c','d','e','f','g','h','i']
col_cat2_unique = ['a','b','c','d','e','f','g','h','i']

col_cat_unique = [col_cat1_unique,col_cat2_unique]

catcoldict = {k:v for k,v in zip(cat_cols,col_cat_unique)}
```
testing it:
this works: 
```
pm = PREPROCESS_MONSOON(catcoldict,num_cols)
pm(i)
```
this works with a bug report
```
pm = PREPROCESS_MONSOON(catcoldict,num_cols)
@tf.function
def p(i):
    return pm(i)

p(i)
output: (along with the expected preprocessed batch)
WARNING:tensorflow:AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unexpected indent (<unknown>, line 10)
WARNING: AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unexpected indent (<unknown>, line 10)
WARNING:tensorflow:AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unexpected indent (<unknown>, line 10)
WARNING: AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458a0ec50>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unexpected indent (<unknown>, line 10)
```

this fails
```
pm = PREPROCESS_MONSOON(catcoldict,num_cols)

inputs = tf.keras.Input(shape=(None,786))
x = pm(inputs)
output:
WARNING:tensorflow:AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458aa3a90>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unexpected indent (<unknown>, line 10)
WARNING: AutoGraph could not transform <bound method NUM_TO_DENSE.call of <__main__.NUM_TO_DENSE object at 0x7f6458aa3a90>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: unexpected indent (<unknown>, line 10)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-78-64c553138beb> in <module>
      2 
      3 inputs = tf.keras.Input(shape=(None,786))
----> 4 x = pm(inputs)
      5 # x = tf.keras.layers.Dense(500,tf.keras.layers.ReLU(100.,0.01,0.))
      6 # output = tf.keras.layers.Dense(10,tf.keras.layers.Softmax())

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    771                     not base_layer_utils.is_in_eager_or_tf_function()):
    772                   with auto_control_deps.AutomaticControlDependencies() as acd:
--> 773                     outputs = call_fn(cast_inputs, *args, **kwargs)
    774                     # Wrap Tensors in `outputs` in `tf.identity` to avoid
    775                     # circular dependencies.

~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    235       except Exception as e:  # pylint:disable=broad-except
    236         if hasattr(e, 'ag_error_metadata'):
--> 237           raise e.ag_error_metadata.to_exception(e)
    238         else:
    239           raise

TypeError: in converted code:

    <ipython-input-66-936477fe8a70>:62 call  *
        dense_num_d = self.ntd(inputs,training=training)
    /home/nitin/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py:773 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    <ipython-input-66-936477fe8a70>:20 call
        null_cols = {k:tf.math.is_finite(inputs[k]) for k in self.keys}
    <ipython-input-66-936477fe8a70>:20 <dictcomp>
        null_cols = {k:tf.math.is_finite(inputs[k]) for k in self.keys}
    /home/nitin/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:862 _slice_helper
        _check_index(s)
    /home/nitin/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:752 _check_index
        raise TypeError(_SLICE_TYPE_ERROR + "", got {!r}"".format(idx))

    TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got 'col_0'
```"
39090,Problem with tfjs.converters.save_keras_model,"Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 224, 224, 32)      896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 112, 112, 32)      0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 112, 112, 64)      18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 56, 56, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 56, 56, 64)        36928     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 28, 28, 64)        0         
_________________________________________________________________
flatten (Flatten)            (None, 50176)             0         
_________________________________________________________________
dense (Dense)                (None, 512)               25690624  
_________________________________________________________________
dense_1 (Dense)              (None, 7)                 3591      
=================================================================
Total params: 25,750,535
Trainable params: 25,750,535
Non-trainable params: 0
_________________________________________________________________

This is a summary of the model that I made and trained, it's fairly simple, but when I use tensorflowjs to save it to a folder to be used as a javascript model in a browser, when I try to load the model, the console always returns ""Provided weight data has no target variable: conv2d_3/kernel"".  As can be seen, there is no conv2d_3 layer in my model, and I have no idea why the converter makes this 4th conv2d layer.  I tried passing false into the strict argument when using tf.loadLayersModel function, but it's like that pass is ignored.
Here's how I wrote the function call:  const net = await tf.loadLayersModel(""path to json file"", strict=false).  Am I doing it wrong?  I really need this figured out as this is a part of my senior project for my university.  Any help is appreciated, thank you."
39088,It is not possible to train the trainable parameters of the RandomFourierFeatures keras layer in eager mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes; minimal working example provided
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux-5.3.0-46-generic-x86_64-with-Ubuntu-18.04-bionic
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0.dev20200501
- Python version: 3.7.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
It is not possible to train the ""trainable"" parameters of the RandomFourierFeatures keras layer, when using eager execution.

**Describe the expected behavior**
It should be possible to train the ""trainable"" parameters of the RandomFourierFeatures keras layer, even when using eager execution.

**Standalone code to reproduce the issue**
import tensorflow as tf
from tensorflow_core.python.keras.layers import RandomFourierFeatures

fourier_features = RandomFourierFeatures(
    1,
    kernel_initializer='gaussian',
    scale=1.0,
    trainable=True,
    dtype=tf.float64
)

input = tf.keras.Input(shape=(1,), dtype=tf.float64, name='input')
output = fourier_features(input)
model = tf.keras.Model(inputs=input, outputs=output)
model.compile(loss='mean_squared_error')

model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]), epochs=1)


**Other info / logs**
The call to fit throws the following error:
ValueError: No gradients provided for any variable: ['random_fourier_features/random_features_scale:0'].
1/1 [==============================] - 0s 17ms/sample
"
39084,Improve training runtime reporting during .fit() for different modes.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
2.0

- Are you willing to contribute it (Yes/No):
yes


**Describe the feature and the current behavior/state.**
The request is to improve the real-time timing reporting across the .fit() train variations.  The format, units, and behavior differ currently

## Baseline
""standard reporting"" looks like this:
```
Epoch 9/100
3200/3200 [==============================] - 61s 19ms/sample - loss: 1.4259e-07 - accuracy: 1.0000
```
Note the implicit ""per epoch"" in 61s.  Also that number sometimes is ""ETA"" and then switches to ""per epoch"" once that epoch is over.  Note also that unit time is per sample.  And note that time is in milliseconds.

## Two worker tf.distribute
### Worker 1:
```
Epoch 1/100
3200/3200 [==============================] - 209s 65ms/sample - loss: 0.1447 - accuracy: 0.9873
Epoch 2/100
 480/3200 [===>..........................] - ETA: 2:44 - loss: 8.5048e-06 - accuracy: 1.0000
```
### Worker 2:
```
Epoch 1/100
  32/3200 [..............................] - ETA: 15:14 - loss: 0.7334 - accuracy: 0.66  64/3200 [..............................] - ETA: 9:05 - loss: 5.8474 - accuracy: 0.718  96/3200 [..............................] - ETA: 7:01 - loss: 3.9763 - accuracy: 0.809 128/3200 [>.............................] - ETA: 5:59 - loss: 3.1775 - accuracy: 0.802 160/3200 [>.............................] - ETA: 5:22 - loss: 2.5869 - accuracy: 0.829
```
#### My belief:  Worker 2 is not honoring the 'verbose' setting of 1 which I set in the chief worker.
The request is one way or another make the reporting the same for each worker of the same type.

## dataset versus numpy
The output of .fit() depends on the training data
Case A: Input and output are numpy arrays
Case B: Input is a dataset.
The following is for dataset:
```
Epoch 1/100
     17/Unknown - 41s 2s/step - loss: 0.6328 - accuracy: 0.9323
```
, the previous examples were for numpy inputs.
NOTE 1:  The units are now per step instead of per sample
NOTE 2: How many examples are in a ""step"" for a 2 worker job?  Is it batch_size or batch_size/# of workers?  What should it be?
NOTE 3:  There is only one significant digit ""2"".  There should be a fixed number of significant digits or maybe an absolute precision (i.e. always show to tenth of millisecond) or a combination of the two.

Anything to improve the reporting is appreciated.  I suggest:

1) Be sure that all workers comply with the ""verbose"" setting properly.  This is speculation as to the problem on my part.
2) Either report /step or /example, or both.  But be consistent.
3) Don't mix ""ETA"" with ""/epoch"".  Maybe show both always.  If ETA is unknown, then ok to report unknown.
4) Figure out a good rule for showing data.  One rule might be: Always show to 1/10th of a unit regardless of unit (so if a time is written in seconds, show number rounded to nearest 0.1 second, if time is in ms, then show in 0.1 ms.  Then consider significant digits.
5) Consider how time should be reported when using tf.distribute (eg reporting when running 1 worker versus reporting when running two workers.  I don't have suggestions for this one.


**Will this change the current api? How?**
no

**Who will benefit with this feature?**
Users who wish to compare runtime between various training modes
**Any Other info.**
"
39083,CNN made using tf.keras yields different and worse accuracy if compared to the same CNN built using keras,"**System information**
- Have I written custom code: I'm using the CNN Mnist example from the [keras documentation](https://keras.io/examples/mnist_cnn/) 
- OS Platform and Distribution: Linux (Google Colab)
- TensorFlow installed from (source or binary): Colab
- TensorFlow version: 2.2.0-rc3
- Python version: 3.6.9
- CUDA/cuDNN version: Colab gpu default
- GPU model and memory: Colab gpu default

**Describe the current behavior**
If I train a simple CNN on the mnist dataset following the [Keras Mnist CNN example](https://keras.io/examples/mnist_cnn/) I get different accuracy depending on whether I use `tf.keras` or `keras`.

**Describe the expected behavior**
I think that the accuracy should be the same.

**Standalone code to reproduce the issue**
You can find the code reproducing this possible bug in a colab notebook here: https://colab.research.google.com/drive/1bLOyOt7tJqh2m-4rNdb_t-FZC4xX7DJI

This is the first issue I open in this repository, so I hope that the information I have provided is clear enough. 
Thank you for your work with Tensorflow!
"
39082,google.protobuf.message.DecodeError With Multi-GPU and Mirrored Strategy on 2.2rc3 (and 2.2rc4),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): `python3 -m pip install tensorflow-gpu==2.2rc3` (and 2.2rc4)
- TensorFlow version (use command below): `v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3`
- Python version: `3.6.9`
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **CUDA - 10.1, cuDNN 7.6.4**
- GPU model and memory: **eight Tesla V100-SXM2-16GB**
ouput of `nvidia-smi topo -m`

```
	GPU0	GPU1	GPU2	GPU3	GPU4	GPU5	GPU6	GPU7	mlx5_0	mlx5_1	mlx5_2	CPU Affinity
GPU0	 X 	NV1	NV1	NV2	NV2	SYS	SYS	SYS	PIX	PHB	SYS	0-19,40-59
GPU1	NV1	 X 	NV2	NV1	SYS	NV2	SYS	SYS	PIX	PHB	SYS	0-19,40-59
GPU2	NV1	NV2	 X 	NV2	SYS	SYS	NV1	SYS	PHB	PIX	SYS	0-19,40-59
GPU3	NV2	NV1	NV2	 X 	SYS	SYS	SYS	NV1	PHB	PIX	SYS	0-19,40-59
GPU4	NV2	SYS	SYS	SYS	 X 	NV1	NV1	NV2	SYS	SYS	PIX	20-39,60-79
GPU5	SYS	NV2	SYS	SYS	NV1	 X 	NV2	NV1	SYS	SYS	PIX	20-39,60-79
GPU6	SYS	SYS	NV1	SYS	NV1	NV2	 X 	NV2	SYS	SYS	PHB	20-39,60-79
GPU7	SYS	SYS	SYS	NV1	NV2	NV1	NV2	 X 	SYS	SYS	PHB	20-39,60-79
mlx5_0	PIX	PIX	PHB	PHB	SYS	SYS	SYS	SYS	 X 	PHB	SYS	
mlx5_1	PHB	PHB	PIX	PIX	SYS	SYS	SYS	SYS	PHB	 X 	SYS	
mlx5_2	SYS	SYS	SYS	SYS	PIX	PIX	PHB	PHB	SYS	SYS	 X 	

```

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

When running the following:

```
strategy = tf.distribute.MirroredStrategy()
run_config = tf.estimator.tpu.RunConfig(
      ...
      train_distribute=strategy
  )
train_estimator = tf.estimator.tpu.TPUEstimator(
        ...
        config=run_config)
train_estimator.train(...)
```

The following exception occurs, when at least one visible GPU is not connected via NVLink. 

```
INFO:tensorflow:training_loop marked as finished
I0501 17:10:16.417816 139720608089920 error_handling.py:115] training_loop marked as finished
WARNING:tensorflow:Reraising captured error
W0501 17:10:16.418111 139720608089920 error_handling.py:149] Reraising captured error
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Parsing Inputs...
Traceback (most recent call last):
  File ""main.py"", line 427, in <module>
    tf.app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""main.py"", line 283, in main
    FLAGS.train_batch_size))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3083, in train
    rendezvous.raise_errors()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors
    six.reraise(typ, value, traceback)
  File ""/usr/local/lib/python3.6/dist-packages/six.py"", line 703, in reraise
    raise value
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3078, in train
    saving_listeners=saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 349, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1180, in _train_model
    return self._train_model_distributed(input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1242, in _train_model_distributed
    self._config._train_distribute, input_fn, hooks, saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1356, in _actual_train_model_distributed
    saving_listeners)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1514, in _train_with_estimator_spec
    log_step_count_steps=log_step_count_steps) as mon_sess:
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 604, in MonitoredTrainingSession
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1038, in __init__
    stop_grace_period_secs=stop_grace_period_secs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 737, in __init__
    h.begin()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/basic_session_run_hooks.py"", line 563, in begin
    self._summary_writer = SummaryWriterCache.get(self._checkpoint_dir)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer_cache.py"", line 63, in get
    logdir, graph=ops.get_default_graph())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py"", line 372, in __init__
    super(FileWriter, self).__init__(event_writer, graph, graph_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py"", line 84, in __init__
    self.add_graph(graph=graph, graph_def=graph_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/summary/writer/writer.py"", line 194, in add_graph
    true_graph_def = graph.as_graph_def(add_shapes=True)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3136, in as_graph_def
    result, _ = self._as_graph_def(from_version, add_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3053, in _as_graph_def
    graph.ParseFromString(compat.as_bytes(data))
google.protobuf.message.DecodeError: Error parsing message
```

However when limiting this to GPUs connected via NVLink (i.e. `CUDA_VISIBLE_DEVICES=""0,1,2,3""` or `CUDA_VISIBLE_DEVICES=""4,5,6,7""`) this error does not occur. It does occur even when 2 GPUs are given that aren't NVLink connected (i.e. `CUDA_VISIBLE_DEVICES=""0,5""`).

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

I was unable to reproduce using a minimal example, however the code is a [distributed fork of google brain's efficientdet](https://github.com/fsx950223/automl).

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39081,Broken URL in Fairness Indicators documentation,"On this documentation [page](https://www.tensorflow.org/tfx/guide/fairness_indicators), the URL linked at the end of the paragraph under ""Data"" is broken.

<img width=""719"" alt=""Screen Shot 2020-05-01 at 10 22 37"" src=""https://user-images.githubusercontent.com/20053362/80825676-b6b62600-8b95-11ea-90bd-84d7e64f0b2f.png"">

It directs to https://www.tensorflow.org/tfx/guide/bit.ly/fairness-indicators-guidance, but that leads to an Error 404 page."
39080,About website link 404 not found in README,"Could you add the website link in the following url：
https://github.com/tensorflow/tensorflow/tree/r1.13/tensorflow/contrib/quantize

![image](https://user-images.githubusercontent.com/35229624/80811968-97e97b00-8bf9-11ea-9752-8b65e886e8d5.png)
the link marked in blue is 404 not found.

Thank you!

"
39079,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Windows Version	10.0.18362 Build 18362

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  NA
- TensorFlow installed from (source or binary):  pip3 install keras and tensorflow, confirmed using Anaconda Navigator 1.9.12
- TensorFlow version (use command below):  2.1.0 based on Anaconda, commands below for TF 1.0 and 2.0 both failed with same error: ImportError: DLL load failed: The specified module could not be found.
- Python version:   Python 3.7.6
- Bazel version (if compiling from source):  NA
- GCC/Compiler version (if compiling from source):  NA
- CUDA/cuDNN version:  CUDAtoolkit 10.0.130 and cuDNN 7.6.5 were installed later, but did not resolve the issue/bug
- GPU model and memory:  Intel(R) HD Graphics 620 with 1 GB RAM


You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
from keras.models import Sequential

python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
OR
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

caused the same problems, but starting with:
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  **File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>**
    from tensorflow_core import *

Line 101 shows up below as well, and the rest of the error continues similarly: lines 40, 50, 44, 127, 49, 74, 58, 28, 24, 242, 342



Using TensorFlow backend.
ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

ERROR:root:Internal Python error in the inspect module.
Below is the traceback from this internal error.

Traceback (most recent call last):
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Max\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Max\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Max\anaconda3\lib\site-packages\IPython\core\interactiveshell.py"", line 3331, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-9c5e0a19b646>"", line 1, in <module>
    from keras.models import Sequential
  File ""C:\Users\Max\anaconda3\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Max\anaconda3\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Max\anaconda3\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Max\anaconda3\lib\site-packages\keras\backend\__init__.py"", line 1, in <module>
    from .load_backend import epsilon
  File ""C:\Users\Max\anaconda3\lib\site-packages\keras\backend\load_backend.py"", line 90, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Max\anaconda3\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  **File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>**
    from tensorflow_core import *
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\Max\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Max\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Max\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Max\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


**Describe the expected behavior**

I'm not sure.  I'm new to Python using Keras and Tensorflow.  

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

from keras.models import Sequential
OR
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
OR
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[Error messages - LSTM RNN, Keras Tensorflow.txt](https://github.com/tensorflow/tensorflow/files/4564109/Error.messages.-.LSTM.RNN.Keras.Tensorflow.txt)
"
39078,TPU usage while running a model using Transfer Learning ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Google colab version
- TensorFlow version (use command below):2.2.0rc3
- Python version:3.6.9
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):No
- CUDA/cuDNN version:No
- GPU model and memory: TPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Showing this error while usingTransfer learning and TPU together **
 `
FailedPreconditionError: Error while reading resource variable efficientnet-b0/stem/conv2d/kernel_31710 from Container: worker. This could mean that the variable was uninitialized. Not found: Resource worker/efficientnet-b0/stem/conv2d/kernel_31710/N10tensorflow3VarE does not exist.`

**The model should start fitting the data**


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/14Gx2l195vQaCC_GWoyhgW_Qnwsfk5vGN#scrollTo=2CVARbH2K01F&line=1&uniqifier=1

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
39076,LSTM/GRU performance discrepancy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora32/Colab/Ubuntu 19.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0rc4/2.2.0rc3/2.1.0
- Python version: 3.7.6/3.7.4/3.6.9
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 970 4GB/Colab GPU/NVIDIA Tesla P100 16GB

**Describe the current behavior**
I'm experimenting with models that require the state of LSTMs to be processed after after each time step. So I tried unrolling the LSTM layer in a loop, iterating over the time steps. That's were I noticed a huge increase in time per training step. Uppon further investigation I noticed that this is also the case in some other configuration featuring LSTMs/GRUs. Different versions of TensorFlow are effected differently. See below:

Some results:
|    | Colab | Compute Cloud | Local |
| ------------- | ------------- | ------------- | ------------- |
| Keras Sequential with LSTMs  | 17ms/step  | 15ms/step | 17ms/step |
| Keras Sequential with GRUs  | 17ms/step  | 14ms/step | 16ms/step |
| [Custom Model](https://www.tensorflow.org/guide/keras/custom_layers_and_models) with LSTMs  | 17ms/step  | 14ms/step | 17ms/step |
| Custom Model with GRUs  | 88ms/step  | 14ms/step | 40ms/step |
| Custom Model with LSTMs in loop  | 120ms/step  | 173ms/step | 81ms/step |
| Custom Model with GRUs in loop  | 107ms/step  | 153ms/step | 66ms/step 
| Custom Model with compat LSTMs  | 19ms/step  | 15ms/step | 17ms/step |
| Custom Model with compat GRUs  | 20ms/step  | 15ms/step | 17ms/step |
| Custom Model with compat LSTMs in loop  | 87ms/step  | 108ms/step | 62ms/step |
| Custom Model with compat GRUs in loop  | 78ms/step  | 86ms/step | 49ms/step |

Colab: TF 2.2.0rc3, Python 3.6.9, with GPU
Compute Cloud: TF 2.1.0, Python 3.7.4, Cuda 10.1, Nvidia Tesla P100, Ubuntu 19.04
Local: TF 2.2.0rc4, Python 3.7.6, Cuda 10.1, Nvidia GTX 970, Fedora 32

![index](https://user-images.githubusercontent.com/64591330/80797451-28946c80-8ba2-11ea-9196-b526c4767bd6.png)

**Describe the expected behavior**
Time per step should be more or less similar.

**Standalone code to reproduce the issue** 
[Colab Notebook](https://colab.research.google.com/drive/19-JZo_iJwsoR2gqpXtuN_sbBkEyQtxx4)
"
39075,ForwardAccumulator fails with `experimental_run_functions_eagerly(True)`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos Catalina
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  `2.2.0rc4`
- Python version: `3.7.5`
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the current behavior**
Running the examples in [tf.ForwardAccumulator docs](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) fail with `RecursionError: maximum recursion depth exceeded` when running with `tf.config.experimental_run_functions_eagerly(True)`.

**Describe the expected behavior**
Running the examples in [tf.ForwardAccumulator docs](https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator) with `tf.config.experimental_run_functions_eagerly(True)` work the same way as when running with `tf.config.experimental_run_functions_eagerly(False)`.

**Standalone code to reproduce the issue**
This is the standard example from https://www.tensorflow.org/api_docs/python/tf/autodiff/ForwardAccumulator, with just the `experimental_run_functions_eagerly(True)` call added.

```python
import tensorflow as tf

tf.config.experimental_run_functions_eagerly(True)


v = tf.Variable([1., 2.])
with tf.autodiff.ForwardAccumulator(
    v,
    # The ""vector"" in Hessian-vector product.
    tf.constant([1., 0.])) as acc:
  with tf.GradientTape() as tape:
    y = tf.reduce_sum(v ** 3.)
  backward = tape.gradient(y, v)
backward  # gradient from backprop

acc.jvp(backward)  # forward-over-backward Hessian-vector product
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
...
    self._push_tape()
  File ""/Users/hartikainen/conda/envs/policy-evaluation/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 849, in _push_tape
    watch_accessed_variables=self._watch_accessed_variables)
  File ""/Users/hartikainen/conda/envs/policy-evaluation/lib/python3.7/site-packages/tensorflow/python/eager/tape.py"", line 48, in push_new_tape
    return Tape(tape)
RecursionError: maximum recursion depth exceeded
```

"
39074,Loading model with add_loss fails,"**System information**
- Have I written custom code: Yes. Minimal Failing Example given below.
- OS Platform and Distribution: MacOs Mojave Version 10.14.6. Also tested on Linux Ubuntu 18.04.
- TensorFlow installed from: binary
- TensorFlow version (use command below): 2.1.0 (v2.1.0-rc2-17-ge5bf8de410 2.1.0)
- Python version: 3.7.7
- CPU execution only.

**Describe the current behavior**

Model loading using `tf.keras.models.load_model` does not work for models with custom layers that add losses with `self.add_loss`. 

In the example below, I create a one-layer model with a custom layer. The layer adds a dummy loss using `self.add_loss`. This is the only loss of the model and there is no other loss passed to `model.compile('adam')`, which is intentional. The model compiles correctly and can successfully be stored to disk in `SavedModel` format with `model.save('my_model')`.
```python
import numpy as np
import tensorflow as tf


class CustomLayer(tf.keras.layers.Layer):
    """"""Imaginary Layer that adds a custom loss in the call""""""

    def __init__(self, a):
        super().__init__()
        self.var = tf.Variable(a, name='var_a')

    def call(self, inputs, training=False):
        output = tf.reduce_sum(inputs * self.var, axis=-1)
        self.add_loss(tf.reduce_mean(output))
        return output


def get_model(input_dim: int) -> tf.keras.Model:
    layer = CustomLayer(0.1)
    inputs = tf.keras.Input((input_dim,), name=""inputs"")
    outputs = layer(inputs)
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    model.compile(optimizer='adam')
    return model


if __name__ == ""__main__"":
    num_data = 100
    X, Y = np.random.randn(num_data, 1), np.random.randn(num_data, 1)

    model = get_model(input_dim=X.shape[-1])
    model.summary()
    print(""model.losses"", model.losses)

    model.save('my_model')
    reconstructed_model = tf.keras.models.load_model('my_model')  # ~~ breaks 
    reconstructed_model.summary()

    # Let's check:
    np.testing.assert_allclose(
        model.predict(X),
        reconstructed_model.predict(X)
    )
```
***PROBLEM:*** The issue arises when loading the model. When the load method tries to compile the model the program terminates with the following stack trace:
```bash
Traceback (most recent call last):
  File ""save_model.py"", line 36, in <module>
    reconstructed_model = tf.keras.models.load_model('my_model')
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py"", line 150, in load_model
    return saved_model_load.load(filepath, compile)
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/saved_model/load.py"", line 99, in load
    training_config))
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 446, in compile
    self._compile_weights_loss_and_weighted_metrics()
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1592, in _compile_weights_loss_and_weighted_metrics
    self.total_loss = self._prepare_total_loss(masks)
  File ""/Users/vincent/miniconda3/envs/gpflux/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py"", line 1691, in _prepare_total_loss
    raise ValueError('The model cannot be compiled '
ValueError: The model cannot be compiled because it has no loss to optimize.
```
So it looks like the model can not be reconstructed/compiled because there is no loss specified. The original model, however, did have a loss:
```python
print(""model.losses"", model.losses)
>>> model.losses [<tf.Tensor 'custom_layer/Mean:0' shape=() dtype=float32>]
```
Interestingly, if we load the model without compiling we get
```python
reconstructed_model = tf.keras.models.load_model('my_model', compile=False)
print(""reconstructed_model.losses"", reconstructed_model.losses)
>>> reconstructed_model.losses []
```
which indicates that the losses indeed aren't correctly loaded into the reconstructed model. Not compiling the model, however, is not an option as the `model.predict` doesn't work as long as the model is not compiled.

***Additional info:***
1) Passing the source code of the layer in the load method results in the same crash:
```python
reconstructed_model = tf.keras.models.load_model('my_model', custom_objects={""CustomLayer"": CustomLayer})
```
However, in my use-case I don't have access to the source code at load time. So this solution would not fit my needs (but is also doesn't work).

2) Interestingly, the program terminates correctly when the original model does not get compiled. Again, this doesn't fit my use-case.

**Describe the expected behavior**

According to the docs, the Keras API `save_model()` and the `SavedModel` format supports the saving and loading of models that add losses using `add_loss` in their `call` method. 

Many thanks for the support.
 
"
39072,Support Ragged inputs in a Dense layer.,"**System information**
- TensorFlow version (you are using): **TF 2.1, 2.2.0-rc4, TF nightly**
- Are you willing to contribute it (Yes/No): **Yes**

**Describe the feature and the current behavior/state.**

Currently, `tf.keras.layers.Dense` does not support RaggedTensor inputs.
https://github.com/tensorflow/tensorflow/blob/203aa8b6341b30abef0e24edb549c11c7966229e/tensorflow/python/keras/layers/core.py#L1180
However, if the ragged dimension is not the last, it would be extremely useful if it did.

For example, if you want to do sequence labeling, you pass a ragged tensor with differently length sequences through an Embedding layer, then through a RNN, and then you would need to add the final classification Dense layer, which is currently not straightforward.

Therefore, I propose to use something like `RaggedTensor.with_flat_values` on the output of the Dense layer applied to `RaggedTensor.flat_values`.

**Will this change the current api? How?**

No new methods; the Dense layer would only add support for ragged tensors.

**Who will benefit with this feature?**

I think anybody using ragged tensors."
39071,ImportError: DLL load failed: The specified module could not be found,"Hello. My OS is windows10, Python version is 3.7 (Anaconda Python). I install tensorflow using `pip install tensorflow`. After installing when i use `import tensorflow`, i am getting following too long error:

import tensorflow
Traceback (most recent call last):

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""<ipython-input-1-d6579f534729>"", line 1, in <module>
    import tensorflow

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\dell\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Error in callback <bound method AutoreloadMagics.post_execute_hook of <autoreload.AutoreloadMagics object at 0x000001AE0977D088>> (for post_execute):
Traceback (most recent call last):

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)

  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)

  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)

ImportError: DLL load failed: The specified module could not be found.


During handling of the above exception, another exception occurred:

Traceback (most recent call last):

  File ""C:\Users\dell\anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 538, in post_execute_hook
    _, pymtime = self._reloader.filename_and_mtime(sys.modules[modname])

  File ""C:\Users\dell\anaconda3\lib\site-packages\IPython\extensions\autoreload.py"", line 184, in filename_and_mtime
    if not hasattr(module, '__file__') or module.__file__ is None:

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\dell\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 953, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import

  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load

  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked

  File ""<frozen importlib._bootstrap>"", line 677, in _load_unlocked

  File ""<frozen importlib._bootstrap_external>"", line 728, in exec_module

  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 42, in <module>
    from . _api.v2 import audio

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\_api\v2\audio\__init__.py"", line 10, in <module>
    from tensorflow.python.ops.gen_audio_ops import decode_wav

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\ops\gen_audio_ops.py"", line 9, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)

  File ""C:\Users\dell\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\dell\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\dell\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

"
39070,Passing call arguments to individual layers of a model,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
There does not seem to be a way to pass call argument to individual layers of a model.
For example I have been unable to find a way to pass initial_state call argument to the lstm layers inside a model. I explored multiple pathways and all failed.
          

**Will this change the current api? How?** 

**Who will benefit with this feature?** everyone

**Any Other info.** Seems to be a simple fix
"
39069,Ability to exclude columns from tf.data.experimental.CsvDataset,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):2.0
- Are you willing to contribute it (Yes/No): No (beginner level python skills)



**Describe the feature and the current behavior/state.**
There is an existing ""select_cols"" parameter; it would be useful to provide an ""exclude_cols"" parameter (possibly mutually exclusive) so that when you have 200+ columns in a CSV, but a small number of them are useful only to a human reader, you can exclude them from the dataset.

**Will this change the current api? How?**
It would only add an additional optional parameter

**Who will benefit with this feature?**
People wanting to train neural nets on CSVs full of multi-column data, e.g. to identify musical instruments from frequency spectrum

**Any Other info.**
"
39068,ImportError: DLL load failed: The specified module could not be found.,"From CMD Run as Administrator.

1) C:\windows\system32>pip install --upgrade pip
2) C:\windows\system32>**pip install tensorflow**
3) C:\windows\system32>python
>>> import tensorflow as ts

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""C:\Users\sprod\anaconda3\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\sprod\anaconda3\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\sprod\anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\sprod\anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
**ImportError: DLL load failed: The specified module could not be found.**"
39067,"Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64.","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:1.14
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.2/7.6
- GPU model and memory:4G



**Describe the problem**
On windows10 systems, the call to the tensorflow-cpu version using Java runs successfully, but replacing libtensorflow_jni with libtensorflow_jni_gpu will report an error in Can't find dependent libraries?
cpu:successful
     maven:dependency
		libtensorflow and libtensorflow_jni
gpu:error
     maven:dependency
		libtensorflow and libtensorflow_jni-gpu
     Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.         at org.tensorflow.NativeLibrary.load(NativeLibrary.java:77)         at org.tensorflow.TensorFlow.init(TensorFlow.java:66)         at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)         at org.tensorflow.Graph.<clinit>(Graph.java:479)         at HelloTensorFlow.main(HelloTensorFlow.java:8)
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
39066,Float 16 quantization error,"Hi 

I get the following error - 


`""tflite.py"", line 5, in <module>
    converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]
AttributeError: module 'tensorflow_core._api.v2.lite' has no attribute 'constants'`

while trying the following code to quantize my .pb file to float 16 

```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_saved_model('out/')
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.lite.constants.FLOAT16]
tflite_quant_model = converter.convert()
```

Tensorflow version 2.1.0, OS - Mac "
39065,ModuleNotFoundError: No module named 'tensorflow.contrib',"Tryint to iter through the dataset 
using tensorflow 2.0 

```
def make_dataset(X_data,y_data,n_splits):
    
    def gen():
        for train_index, test_index in KFold(n_splits).split(X_data):
            X_train, X_test = X_data[train_index], X_data[test_index]
            y_train, y_test = y_data[train_index], y_data[tests_index]
            yield X_train, y_train,X_test,y_test
    return tf.data.Dataset.from_generator(gen,(tf.float32,tf.float32),(tf.TensorShape([40,40,9]), tf.TensorShape([40,40,1])))
dataset= make_dataset(X,y,5)

import tensorflow as tf
import tensorflow.contrib.eager as tfe
for X_train, y_train,X_test,y_test in tfe.Iterator(dataset):
    print(X_train.shape)

```


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-21-950ccb8e9a23> in <module>
----> 1 for X_train, y_train,X_test,y_test in trt.Iterator(dataset):
      2     print(X_train.shape)

AttributeError: module 'tensorflow.python.compiler.tensorrt.trt_convert' has no attribute 'Iterator'
time: 17.4 ms"
39064,Error in Keras Save with TF Lookup,"When using the save method from a tf.keras model (or tf.saved_model.save), an error appears that exporting the model fails due an untracked `tf.Variable`. [Here is a Colab replicating the issue](https://colab.research.google.com/drive/1PCbiubCnzUszW6mB8f8wtft7YFVZlkaB).

```python
import tensorflow as tf

# Create dummy model
model = tf.keras.Sequential([
    tf.keras.Input(shape=(1,), dtype=tf.float32),
    tf.keras.layers.Dense(1)
])

# Create Simple Signature Function
def outer_fn(filepath):
    @tf.function
    def inner_fn(b):
        text_init = tf.lookup.TextFileInitializer(filename=filepath, 
                            key_dtype=tf.string, key_index=0,
                            value_dtype=tf.int64, value_index=tf.lookup.TextFileIndex.LINE_NUMBER
                            )
        table = tf.lookup.StaticHashTable(initializer=text_init, default_value=0)
        return table.lookup(b)
    return inner_fn

# Signature
signatures = {
    'example_signature_1':
        outer_fn(filepath='./test_file').get_concrete_function(
            tf.TensorSpec(shape=[None], dtype=tf.string, name='ex_sig_1')
            ),
}

# Model Save [ERROR - Untracked Tensor]
model.save('.', signatures=signatures)

>>> AssertionError: Tried to export a function which references untracked object Tensor(""164:0"", shape=(), dtype=resource).TensorFlow objects (e.g. tf.Variable) captured by functions must be tracked by assigning them to an attribute of a tracked object or assigned to an attribute of the main object directly.
```
This is assuming a simple text file (here named 'test_file') like this:
```
here_is_one_row
here_is_another_row
```

To attempt to fix this, I (1) tried putting the tf.lookup outside the function, (2) created a tf.keras subclassed layer to use the `config` method - neither of which succeeded. Is there any way to save a `tf.lookup` object that does not appear in the documentation?
"
39063,how to perform cross validation with this type of dataset?,"```
import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

inp = Input(shape=(3,))
output = Dense(1, activation='sigmoid')(inp)
model = Model(inp, output)
model.compile(optimizer=Adam(1e-2), loss='binary_crossentropy')

def simple_generator():
    while True:
        yield [0.5, 0.2, -0.3], 0.0
        yield [-0.5, 0.3, -0.1], 1.0
        
dataset = tf.data.Dataset.from_generator(simple_generator,
                                         output_types=(tf.float32,
                                                       tf.float32))
dataset = dataset.batch(4).prefetch(1)

model.fit(dataset)
```

How to perform cross validation with tf.data.Dataset.From_generator ? "
39061,Windows build error: 'mlir::FoldingHook': 'value' is not a valid template type argument for parameter 'ConcreteType',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 10.0.18363
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.1 (SHA-1 is c878390581fc817564f8ebe1f4237d0cbd225f14)
- Python version: 3.7
- Installed using virtualenv? pip? conda?: N/A?
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): (BAZEL_VS: C:\Program Files (x86)\Microsoft Visual Studio\2019\BuildTools)
- CUDA/cuDNN version: CUDA 10.1, cuDNN 7.6.5
- GPU model and memory: RTX 2080 (8GB)

I'm following this guide https://github.com/sitting-duck/stuff/tree/master/ai/tensorflow/build_tensorflow_1.14_source_for_Windows and advice here https://github.com/tensorflow/tensorflow/issues/23542 with some adjustments to build Tensorflow 2.1 for Windows.

I ran `python configure.py` and this ended up being my `.tf_configure.bazelrc`:

    build --action_env PYTHON_BIN_PATH=""C:/Python37/python.exe""
    build --action_env PYTHON_LIB_PATH=""C:/Python37/lib/site-packages""
    build --python_path=""C:/Python37/python.exe""
    build --config=xla
    build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1""
    build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""7.5""
    build --config=cuda
    build:opt --copt=/arch:AVX2
    build:opt --define with_default_optimizations=true
    build --define=override_eigen_strong_inline=true
    test --flaky_test_attempts=3
    test --test_size_filters=small,medium
    test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial
    test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu
    test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only
    test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only
    build --action_env TF_CONFIGURE_IOS=""0""

Then I did

`bazel build --config=cuda --copt=-nvcc_options=disable-warnings tensorflow:tensorflow.dll`

I've done this twice and got the same error during the build. Pardon the mess...

ERROR: C:/users/SOMEUSER/_bazel_SOMEUSER/dktb5wq4/external/llvm-project/mlir/BUILD:75:1: C++ compilation of rule '@llvm-project//mlir:IR' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/SOMEUSER/_bazel_SOMEUSER/dktb5wq4/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Python37/python.exe
    SET PYTHON_LIB_PATH=C:/Python37/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\SOMEU~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=7.5
    SET TF_ENABLE_XLA=1
    SET TF_NEED_CUDA=1
    SET TMP=C:\Users\SOMEU~1\AppData\Local\Temp
  C:/Python37/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/DialectSymbolRegistry /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /DNDEBUG /w /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /std:c++14 /Fobazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_objs/IR/Module.o /c external/llvm-project/mlir/lib/IR/Module.cpp
Execution platform: @local_execution_config_platform//:platform
external/llvm-project/mlir/include\mlir/IR/OpDefinition.h(1185): error C2923: 'mlir::FoldingHook': 'value' is not a valid template type argument for parameter 'ConcreteType'
external/llvm-project/llvm/include\llvm/ADT/STLExtras.h(1281): note: see declaration of 'value'
external/llvm-project/mlir/include\mlir/IR/Module.h(36): note: see reference to class template instantiation 'mlir::Op<mlir::ModuleOp,mlir::OpTrait::ZeroOperands,mlir::OpTrait::ZeroResult,mlir::OpTrait::IsIsolatedFromAbove,mlir::OpTrait::PolyhedralScope,mlir::OpTrait::SymbolTable,mlir::OpTrait::SingleBlockImplicitTerminator<mlir::ModuleTerminatorOp>::Impl,mlir::SymbolOpInterface::Trait>' being compiled
external/llvm-project/mlir/include\mlir/IR/OpDefinition.h(1187): error C2955: 'mlir::FoldingHook': use of class template requires template argument list
external/llvm-project/mlir/include\mlir/IR/OpDefinition.h(273): note: see declaration of 'mlir::FoldingHook'
Target //tensorflow:tensorflow.dll failed to build
INFO: Elapsed time: 1466.809s, Critical Path: 96.50s
INFO: 3656 processes: 3656 local.
FAILED: Build did NOT complete successfully

The first time the build stopped entirely. The second time, which is right now, the same error reported, but it's still running. I see
`[10,544 / 12,151] Compiling tensorflow/core/kernels/scatter_op.cc; 5820s local`

Is there any hope that this will finish? What can be done to fix the error? Thanks for reading.
Update: I noticed my CPU wasn't working hard at all while waiting for the second round, so I decided to terminate it.
"
39060,XLA function crashed when called under GradientTape,"Hello,

**Describe the current behavior**

I have a function compiled with XLA (tf.function + experimental_compile=True) which writes the processed output to the TensorArray. When called inside GradientTape it crashes. Without experimental_compile=True everything works fine.

**Describe the expected behavior**
function does not crash

**Standalone code to reproduce the issue**
Colab link to reproduce the crash: https://colab.research.google.com/drive/1Hh7fFkWVC8YCPqTtkQXDNVB8elQS8rq0#scrollTo=MpeMlEwGje0n
"
