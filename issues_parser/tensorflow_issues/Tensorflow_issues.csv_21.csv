Issue Number,Issue Title,Issue Body
43264,Helm chart for TF2+GPU,Would like to see a helm chart for TF2 w/ GPUs be made avail. 
43263,TF 2.3 - loading of saved_model from disk with ragged=True input is slower than ragged=False,"**System information**
Google Colab Notebook with TF 2.3

**Describe the current behavior**
Loading saved model with input `tf.keras.layers.Input(shape=[None], dtype=tf.int64, ragged=True)` is 5-10x slower than `tf.keras.layers.Input(shape=[None], dtype=tf.int64, ragged=False)`

**Describe the expected behavior**
The above should not impact performance

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/10ZXvdbp1lf8X2LLtemcHr2rX692n2jv9?usp=sharing

**Other info / logs**
The above is achieved with a custom model implementation. However, when I used the model from one of the Tensorflow tutorials I could not replicate the issue.

On top of that, we were able to replicate this issue with tensorflow serving where loading of the saved_model + first prediction was 5-10x slower than model without ragged tensors."
43262,Unexpected behaviour for model.evaluate inside keras Callback,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9

**Describe the current behavior**
Inside a `keras.callbacks.Callback`, `self.model.evaluate` returns results for the `validation_data` regardless of what is passed in. The issue seems to not be present if `validation_data` is None.

**Describe the expected behavior**
`self.model.evaluate` should evaluate what is passed in.

**Standalone code to reproduce the issue**
[Colab notebook
](https://colab.research.google.com/drive/1RHrGba3sQ3iw9Ju0_m3WRFC5dog4WG6k?usp=sharing) which illustrates the issue.
"
43261,Self-attention on word embeddings using half-precision with mask_zero set to True crashes training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-41557-gae0a324182 2.4.0-dev20200915
- Python version: 3.6.9
- CUDA/cuDNN version: Bug reproducible without GPU (but the same bug occurs on GPU)
- GPU model and memory: Bug reproducible without GPU (but the same bug occurs on GPU)

**Describe the current behavior**

When applying self-attention to word embeddings with half-precision, if mask_zero is set to True, then training crashes. If it is set to False, then training completes without crashing.

**Describe the expected behavior**

Train with mask_zero set to True without crashing.

**Standalone code to reproduce the issue**

Reproduced bug in tf-nightly here:
https://colab.research.google.com/drive/1lzG0SonIWiqrnCLxzxQO-s2e-fYNoG1c?usp=sharing

**Other info / logs**

When mask_zero is set to True, the error is:

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
   1135 
-> 1136   def binary_op_wrapper(x, y):
   1137     with ops.name_scope(None, op_name, [x, y]) as name:

13 frames
ValueError: Tensor conversion requested dtype float32 for Tensor with dtype float16: <tf.Tensor 'attention_19/MatMul:0' shape=(None, None, None) dtype=float16>

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    504 
    505             # If we did not match an allowed dtype, try again with the default
--> 506             # dtype. This could be because we have an empty tensor and thus we
    507             # picked the wrong type.
    508             if inferred is not None and inferred.dtype in allowed_list:

TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float16 of argument 'x'.
"
43259,Importing Tensorflow after using pyinstaller (ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.14
- Python version: 3.7.8
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None (CPU)
- GPU model and memory: None



**Describe the problem**
I have installed tensorflow using conda and it works fine in the conda environment. I converted my app into binary file using pyinstaller. While running .exe file, it throws ImportError

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Used pyinstaller to convert my flask app to .exe file
First issue encountered: No module named tensorflow
Copied the tensorflow folder from the conda environment and pasted it in directory where app.exe is located.
Now facing the Import Error
Is there something else, I need to copy to the working directory or a path to set?

**Any other info / logs**
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""R:\armorfinal\cheque_locate_ws\dist\app\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""R:\armorfinal\cheque_locate_ws\dist\app\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""R:\armorfinal\cheque_locate_ws\dist\app\tensorflow\python\pywrap_tensorflow.py"", line 59, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""R:\armorfinal\cheque_locate_ws\dist\app\tensorflow\python\pywrap_tensorflow_internal.py"", line 35, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""R:\armorfinal\cheque_locate_ws\dist\app\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Apurva\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Apurva\AppData\Local\Programs\Python\Python38\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

"
43258," ValueError: No gradients provided for any variable: ['embedding_2/embeddings:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0'].","I am training model in colab. The model I'm trying to build is siamese network for text similarity. 
the loss function which I used finds internally the negative sample.
**Explaination of loss function::
kindly, watch this two small videos where the lecturer explains about the loss function I used::**
https://www.coursera.org/lecture/sequence-models-in-nlp/computing-the-cost-i-T4Ylj
https://www.coursera.org/lecture/sequence-models-in-nlp/computing-the-cost-ii-qXOjN
**the dataset can be found here::**
[https://www.kaggle.com/c/quora-question-pairs/data](url)
**my colab notebook::**
[https://colab.research.google.com/drive/1NCUxSS9fiuLpPd2hOSxKv5QKH2IUuhX2?usp=sharing](url)

```
sub_model=tf.keras.models.Sequential([Embedding(vocab_size,300,input_length=79), 
Bidirectional(LSTM(79,return_sequences=True)),
Bidirectional(LSTM(79,return_sequences=True)),
tf.keras.layers.GlobalAveragePooling1D(),
tf.keras.layers.Dense(units=158)])
ins1=Input((79,),name='input1')
ins2=Input((79,),name='input2')
x1=sub_model(ins1)
x2=sub_model(ins2)
norm1=tf.keras.layers.Layer(lambda x: tf.math.l2_normalize(x,axis=1))(x1)
norm2=tf.keras.layers.Layer(lambda x: tf.math.l2_normalize(x,axis=1))(x2)

model=Model([ins1,ins2],[norm1,norm2])
```
the loss function used is 
```

def get_tripletloss(y_pred1,y_pred2):
    y_pred=tf.matmul(y_pred1,y_pred2,transpose_b=True) ##getting y_pred of (batch,batch)
    batch=y_pred.get_shape().as_list()[0] ##getting batch_size
    alpha_matrix=tf.cast(tf.reshape(tf.repeat(0.2,repeats=batch),shape=(batch,1)),dtype=tf.float32) #making alpha matrix of 0.2's
    diag_part=tf.cast(tf.reshape(tf.linalg.diag_part(y_pred),shape=(batch,1)),dtype=tf.float32) ##taking diag_part
    diagonal_matrix=tf.cast(tf.linalg.diag(tf.linalg.diag_part(y_pred)),dtype=tf.float32) ## making as diagonal_matrix
    sim_an=tf.reshape(tf.reduce_mean(tf.cast(y_pred,dtype=tf.float32)-diagonal_matrix,axis=1),shape=(batch,1)) ## getting only off-diagonal
    sim_an=tf.cast(sim_an,dtype=tf.float32)-diag_part+alpha_matrix ## getting sim_an-sim_ap+alpha
    sim_an=tf.maximum(sim_an,tf.cast(tf.zeros((batch,1)),dtype=tf.float32)) # getting max(loss,0)
    loss1=tf.keras.backend.mean(sim_an) ##final_loss1
    ##########
    y_pred=tf.where(y_pred<diag_part,y_pred,tf.cast(0.000001,dtype=tf.float32))##made to small number where off-diagonal elements are getter than diagonal and also diagonal elements.
    sim_an2=tf.reshape(tf.reduce_max(y_pred,axis=1),shape=(batch,1)) ##getting max value(closet_negative) 
    loss2=tf.cast(sim_an2,dtype=tf.float32)-diag_part+alpha_matrix ##getting sim_an-sim_ap+alpha
    loss2=tf.maximum(loss2,tf.cast(tf.zeros((batch,1)),dtype=tf.float32)) ## max(loss2,0)
    loss2=tf.keras.backend.mean(loss2)
    loss=loss1+loss2
    return loss
```
and the training loop is 
```
dataset=tf.data.Dataset.from_tensor_slices((seq1,seq2))
dataset=dataset.shuffle(149263)
dataset=dataset.batch(29,drop_remainder=True)
dataset=dataset.prefetch(tf.data.experimental.AUTOTUNE)

@tf.function
def get_grads(v1,v2):
    with tf.GradientTape() as tape:
        loss=get_tripletloss(v1,v2)
        grads=tape.gradient(loss,model.trainable_variables)
    return loss,grads
loss_per_batch=[]
loss_per_epoch=[]
optimizer=tf.keras.optimizers.Adam()
for j in range(2):
    training_batches=seq1.shape[0]//29
    for i in range(training_batches):
        data=next(iter(dataset))
        v1,v2=model(data)
        loss,grads=get_grads(v1,v2)
        loss_per_batch.append(loss)
        optimizer.apply_gradients(zip(grads,model.trainable_variables))
    loss_per_epoch.append(np.mean(loss_per_batch))
    loss_per_batch=[]
    print(f""finsihed {j+1} epoch got loss of {loss_per_epoch[j]}"")
```
the error is ::
```
ValueError                                Traceback (most recent call last)
<ipython-input-98-24aaeac0e75b> in <module>()
      9         loss,grads=get_grads(v1,v2)
     10         loss_per_batch.append(loss)
---> 11         optimizer.apply_gradients(zip(grads,model.trainable_variables))
     12     loss_per_epoch.append(np.mean(loss_per_batch))
     13     loss_per_batch=[]

1 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
    511       ValueError: If none of the variables have gradients.
    512     """"""
--> 513     grads_and_vars = _filter_grads(grads_and_vars)
    514     var_list = [v for (_, v) in grads_and_vars]
    515 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py in _filter_grads(grads_and_vars)
   1269   if not filtered:
   1270     raise ValueError(""No gradients provided for any variable: %s."" %
-> 1271                      ([v.name for _, v in grads_and_vars],))
   1272   if vars_with_empty_grads:
   1273     logging.warning(

ValueError: No gradients provided for any variable: ['embedding_2/embeddings:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/recurrent_kernel:0', 'bidirectional_4/forward_lstm_4/lstm_cell_13/bias:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/recurrent_kernel:0', 'bidirectional_4/backward_lstm_4/lstm_cell_14/bias:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/recurrent_kernel:0', 'bidirectional_5/forward_lstm_5/lstm_cell_16/bias:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/recurrent_kernel:0', 'bidirectional_5/backward_lstm_5/lstm_cell_17/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0'].
```"
43257,MobileNetV3 Small/Large not fully quantized after full integer post-training quantization runs with no errors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A (used Google Colaboratory)
- TensorFlow version (using the command in the template): v1.12.1-39569-gcb34190201 2.4.0-dev20200818

**Describe the current behavior**
The model is not fully quantized after full integer quantization even though no error occurs during the quantization. 

**Describe the expected behavior**
I tried using the quantized version in the example Android app but it crashed every time so I wanted to see if the quantized version worked on Coral devices, but then it failed to compile with the error ""Model is not fully quantized."".

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1B-VbTOCpmGhgv5adOpnGn68VARo30bF9?usp=sharing
"
43256,mAP drops crazily when replacing Conv with Depthwise_Conv,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): package manager
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
    converter = tf.lite.TFLiteConverter.from_keras_model(model_yolo)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_data_gen
    
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8
    
    tflite_model_quant = converter.convert()
```

**The output from the converter invocation**

```
# Nothing. Successfully converted to tflite model.
```

**Also, please include a link to the saved model or GraphDef**

```
# Model Definition file see attachment
[mobilenet_v2_yolo_v3.zip](https://github.com/tensorflow/tensorflow/files/5229629/mobilenet_v2_yolo_v3.zip)

```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing huge decrease in accuracy


**Any other info / logs**
I am doing object detection using MoblieNet Yolo v3.
For backbone I use mobilenet v2. I almost copy and paste from official keras implementation. The problem seems happens in YOLO head (I called it ""_make_last_layers"" in my code).
For the first time, I use  
`_make_last_layers_1()`
as my YOLO head.  After converted to tflite model, mAP only drops around 1~4% compared to original model.

For the second time, I use
`_make_last_layers_2()`
as my YOLO head.  After converted to tflite model, mAP drops around **15~19%** compared to original model.

So the difference between _make_last_layers_1() and  _make_last_layers_2() is I try to use the combination of Depthwise_Conv+Conv to replace pure Conv so that I can reduce tons of parameters(23M -> 6.8M)

So...any idea why this structure causes such great drops in accuracy?"
43255,Failed to load the native TensorFlow runtime.,"Hi,

I am using GeForce GTX 1660 Ti 6GB GPU on windows and executed following command.
from tensorflow.python.client import device_lib

Error:
Traceback (most recent call last):
  File ""C:\Users\Sudhakar\anaconda3\envs\keras_tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Sudhakar\anaconda3\envs\keras_tf\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Sudhakar\anaconda3\envs\keras_tf\lib\site-packages\tensorflow\python\__init__.py"", line 39, in <module>
    from tensorflow.python import pywrap_tensorflow as _pywrap_tensorflow
  File ""C:\Users\Sudhakar\anaconda3\envs\keras_tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Sudhakar\anaconda3\envs\keras_tf\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
43254,TextVectorization not working on TPU with tf-nightly.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- TensorFlow version (use command below):tf-nightly:2.4.0-dev20200915
- Using TPU: Yes

I am trying to run TextVectorization on TPU

```
def get_vectorize_layer(texts, vocab_size, max_seq, special_tokens=['[MASK]']):

  vectorize_layer = preprocessing.TextVectorization(
    max_tokens=vocab_size,
    output_mode='int',
    standardize = None,
    output_sequence_length=max_seq)
  
  vectorize_layer.adapt(texts)

  vocab = vectorize_layer.get_vocabulary()
  vocab = vocab[2:vocab_size-len(special_tokens)] + ['[MASK]']
  vectorize_layer.set_vocabulary(vocab)
  return vectorize_layer


vectorize_layer = get_vectorize_layer(data.text.values.tolist(), 20000, 196, special_tokens=['[MASK]'])
```
 Error:

> NotFoundError: 'OptimizeDatasetV2' is neither a type of a primitive operation nor a name of a function registered in binary running on n-86c78cbc-w-0. Make sure the operation or function is registered in the binary running in this process.

Tensorflow: 2.4.0-dev20200915
Reference Notebook: https://colab.research.google.com/drive/1QiQmA2M2WkwsjiHHxBhPM0Jw3_LBJnGV?usp=sharing"
43253,"setting verbosity to 0 does not suppress keras debug messages, no way to turn them off","Using latest version(s) from Colab

Impossible to suppress the ""Found X images belonging to N classes."" that results from Keras flow_from_directory method"
43252,tf.saved_model.save very slow with second-order tf.autodiff.ForwardAccumulator,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b
- GPU model and memory: No gpu
4xIntel(R) Core(TM) i7-8650U CPU @ 1.90GHz
32 GB RAM

**Describe the current behavior**

Saving a tf.module involving second-order tf.autodiff.ForwardAccumulator takes too much time; 1 hour for the example below

**Describe the expected behavior**

Saving the graph in the example should take few seconds

**Standalone code to reproduce the issue**

```python
import os
import tensorflow as tf
import time

class Issue_fwd(tf.Module):

    @tf.function(input_signature=[tf.TensorSpec([None, 1], tf.float64)] * 3 +
                                 [tf.TensorSpec([None, 3], tf.float64)] +
                                 [tf.TensorSpec([1, None], tf.float64)] * 4)
    def f(self, x1, x2, x3, c, v1, v2, v3, v4):

        with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1_2, \
                tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2_2:

            with tf.autodiff.ForwardAccumulator(x1, tf.ones_like(x1)) as fwd_acc_x1, \
                 tf.autodiff.ForwardAccumulator(x2, tf.ones_like(x2)) as fwd_acc_x2, \
                 tf.autodiff.ForwardAccumulator(x3, tf.ones_like(x3)) as fwd_acc_x3:

                p = tf.concat([x1, x2, x3], axis=1)
                pe = tf.transpose(a=p[:, :, None], perm=[0, 2, 1])
                ce = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
                r = tf.reduce_sum(input_tensor=tf.square(ce - pe), axis=2)
                G = tf.exp(-r / 2)

                p = tf.reduce_sum(input_tensor=G * v1, axis=1, keepdims=True)
                b = tf.reduce_sum(input_tensor=G * v2, axis=1, keepdims=True)
                u = tf.reduce_sum(input_tensor=G * v3, axis=1, keepdims=True)
                w = tf.reduce_sum(input_tensor=G * v4, axis=1, keepdims=True)

            dpdx = fwd_acc_x1.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdx = fwd_acc_x1.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudx = fwd_acc_x1.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdx = fwd_acc_x1.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dpdz = fwd_acc_x2.jvp(p, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dbdz = fwd_acc_x2.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudz = fwd_acc_x2.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdz = fwd_acc_x2.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

            dbdt = fwd_acc_x3.jvp(b, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dudt = fwd_acc_x3.jvp(u, unconnected_gradients=tf.UnconnectedGradients.ZERO)
            dwdt = fwd_acc_x3.jvp(w, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2ud2x = fwd_acc_x1_2.jvp(dudx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2ud2z = fwd_acc_x2_2.jvp(dudz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2wd2x = fwd_acc_x1_2.jvp(dwdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2wd2z = fwd_acc_x2_2.jvp(dwdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        d2bd2x = fwd_acc_x1_2.jvp(dbdx, unconnected_gradients=tf.UnconnectedGradients.ZERO)
        d2bd2z = fwd_acc_x2_2.jvp(dbdz, unconnected_gradients=tf.UnconnectedGradients.ZERO)

        return dudx, dudz, dudt, dwdx, dwdz, dwdt, dbdx, dbdz, dbdt, dpdx, dpdz,  d2ud2x, d2ud2z, d2wd2x, d2wd2z, d2bd2x, d2bd2z,


f = Issue_fwd()
saving_path = 'save_path'
os.makedirs(saving_path, exist_ok=True)

start_time = time.clock()
tf.saved_model.save(f, saving_path)
delta_time = time.clock() - start_time
print('saving took {:f} seconds'.format(delta_time))
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))

```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
saving took 3942.697280 seconds
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
```

"
43250,get_output_shapes does not return the correct dimensions for a Dataset,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Using docker container with base image `tensorflow/tensorflow:2.3.0-jupyter`
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I am using AutoKeras to train to train a neural net for image regression, but it is crashing because `tensorflow.compat.v1.data.get_output_shapes(dataset)` function used within AutoKeras is returning shapes that have `None` for each of the dimension lengths rather than an integer. According to [this TensorFlow issue](https://github.com/tensorflow/tensorflow/issues/30774), this function has been replaced by `tf.data.experimental.get_structure`, but both of these return None in their output. I have confirmed that all the tensors in my dataset are the same shape and I have run my Dataset against these functions both with my original Dataset and with the Dataset AutoKeras creates from it and None is returned in both cases as shown in my code snippet below.

**Describe the expected behavior**

Either `tensorflow.compat.v1.data.get_output_shapes(dataset)` or `tf.data.experimental.get_structure` would return the correct shape of elements in a Dataset where all elements have the same shape.


**Standalone code to reproduce the issue**
This snippet contrasts the output of `tensorflow.compat.v1.data.get_output_shapes(dataset)`, `tf.data.experimental.get_structure`, and a custom function that returns the shape of the first element in a Dataset.
```python
def get_first_element_shape(dataset):
    for thing in dataset:
        if isinstance(thing, tuple):
            return tuple(element.shape for element in thing)
        else:
            return thing.shape

print(tf.compat.v1.data.get_output_shapes(train_dataset))
print(tf.data.experimental.get_structure(train_dataset))
print(get_first_element_shape(train_dataset))

x = train_dataset.map(lambda a, b: a)
print('\nX dataset (Created by AutoKeras by separating the input values from the output values in the training dataset)')
print(tf.compat.v1.data.get_output_shapes(x))
print(tf.data.experimental.get_structure(x))
print(get_first_element_shape(x))
```

This outputs:
```
(TensorShape([None, None, None]), TensorShape([1]))
(TensorSpec(shape=(None, None, None), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None))
(TensorShape([600, 600, 3]), TensorShape([1]))

X dataset (Created by AutoKeras by separating the input values from the output values in the training dataset)
(None, None, None)
TensorSpec(shape=(None, None, None), dtype=tf.float32, name=None)
(600, 600, 3)
```
"
43248,Class weights issue with sparse data from tf.data.dataset where y.shape.rank is None,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.2
-   **Python version**: 3.6.7
-   **CUDA/cuDNN version**: 10.2
-   **GPU model and memory**: RTX 2070, 32
-   **Exact command to reproduce**:

Sorry, this is my first time submitting an issue, this is a small issue with a simple fix but I'm not sure if what I added is a long-term solution. Unfortunately, I can't divulge the full code, but I can explain the steps I took to reproduce the error. I have a simple 3-layer deep model and a tf.dataset generate like so:

```
#model instantiated above
model.compile(
            loss = 'sparse_categorical_crossentropy',
            metrics=['sparse_categorical_accuracy'],
            optimizer=tf.keras.optimizers.Adam(learning_rate=0.01)
            )

class generator:
    def __call__(self, file, batch_size): #file is a parquet file
        actual_file = file.decode(""utf-8"") 
        df = pd.read_parquet(actual_file)
        df = df.astype(""float16"")
        df[df.columns[df.shape[1]-1]] = df[df.columns[df.shape[1]-1]].astype(int)
        filefinish = False
        fileIndex = 0
        fileEnd = df.shape[0]
        fileX = df.shape[1]
        while not filefinish:
            if fileIndex + batch_size >= fileEnd:
                yield df.iloc[fileIndex:fileEnd, 0:fileX-1].values, df.iloc[fileIndex:fileEnd, fileX-1:fileX].values.reshape(fileEnd-fileIndex)
                filefinish = True
            else:
                yield df.iloc[fileIndex:fileIndex+batch_size, 0:fileX-1].values, df.iloc[fileIndex:fileIndex+batch_size, fileX-1:fileX].values.reshape(batch_size)
                fileIndex += batch_size

training_files_dir = [LIST OF TRAINING FILES]
BATCH_SIZE, EPOCHS = 4096, 10

training_generator = tf.data.Dataset.from_tensor_slices(training_files_dir)
training_generator = training_generator.interleave(lambda filename: tf.data.Dataset.from_generator(
        generator(), 
        output_types=(tf.float16, tf.int8),
        args=(filename, BATCH_SIZE,)), num_parallel_calls=tf.data.experimental.AUTOTUNE, cycle_length=2)
training_generator = training_generator.repeat(EPOCHS)

model.fit(x=training_generator, epochs=EPOCHS, steps_per_epoch=EPOCH_STEPS, validation_steps=VALIDATION_STEPS, validation_data=validation_generator, class_weight=class_weights, callbacks=all_callbacks, verbose=1)
```

### Describe the problem
Using class weights on fit function and sparse data from tf.data produces the error below as y.shape.rank is None. I added an if statement to data_adapter.py to fix it, but I'm not sure if this is a sustainable fix to leave in for the future.

```
if y.shape.rank != None:
        if y.shape.rank > 2:
          raise ValueError(""`class_weight` not supported for ""
                                                  ""3+ dimensional targets."")
```

### Source code / logs
```
Traceback (most recent call last):
  File ""c:/Users/Harry Wang/Desktop/fnma-deep-mortage-risk/fnma_dnn/main.py"", line 448, in <module>
    model._model.fit(x=training_generator, epochs=EPOCHS, steps_per_epoch=EPOCH_STEPS, validation_steps=VALIDATION_STEPS, validation_data=validation_generator, class_weight=class_weights, callbacks=all_callbacks, verbose=1)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 66, in _method_wrapper 
    return method(self, *args, **kwargs)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 815, in fit
    model=self)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 1117, in __init__  
    dataset = dataset.map(_make_class_weight_map_fn(class_weight))
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 1621, in map
    return MapDataset(self, map_func, preserve_cardinality=True)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3981, in __init__
    use_legacy_function=use_legacy_function)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3221, in __init__       
    self._function = wrapper_fn.get_concrete_function()
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 2532, in get_concrete_function
    *args, **kwargs)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 2496, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\eager\function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3214, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3156, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 262, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 492, in converted_call
    return _call_unconverted(f, args, kwargs, options)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 346, in _call_unconverted
    return f(*args, **kwargs)
  File ""C:\Users\Harry Wang\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\keras\engine\data_adapter.py"", line 1248, in _class_weights_map_fn
    if y.shape.rank > 2:
TypeError: '>' not supported between instances of 'NoneType' and 'int'
```"
43247,Feature request: MLIR-based TFLite converter support for 16bit conv2d,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.1
-   **TensorFlow installed from (source or binary)**: source 
-   **TensorFlow version (use command below)**: 2.4.0, with git hash 4a896124e344adcf94e30ed335b59900b578e53e
-   **Python version**: 3.6.9
-   **Bazel version (if compiling from source)**: 3.1.0

### Describe the problem
I have tf.quantization.fake_quant_with_min_max_args+ tf.conv2d, and use tf.lite.TFLiteConverter to convert it into tflite quantized operator. This works for me if I set both input and weight to 8 bits. But the same process doesn't work for 16 bit activation.

I realize I could optionally use the old and soon to be deprecated TOCO converter (by setting converter.experimental_new_converter=False and converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8]). This does generate tflite but that's not well legalized into one tfl.conv2d with native tfl.qint16 input and output tensors.
"
43245,when i am running the sample tutorial code of basic text classification i just cannot download a file named imdb_word_index.json  ,"when Downloading the url ""https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json"" the program run into the method of ""self._sslobj.do_handshake()"" in file of ssl.py and raise an exception of ""TimeoutError: [WinError 10060]"" . yet i dont know how to solve this problem ,so please help me to out ,thanks "
43244,I am not able to install tensorflow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): command prompt
- TensorFlow version: 2.0
- Python version:3.7.3
- Installed using virtualenv? pip? conda?: pip

**Describe the problem**
I am unable to install tensorflow from my command prompt using pip

**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install tensorflow==2.2.0rc4
pip install tensorflow

 **ERROR**
`ERROR: Could not find a version that satisfies the requirement tensorflow==2.2.0rc4 (from versions: none)
ERROR: No matching distribution found for tensorflow==2.2.0rc4`"
43243,Broadcasting not working for divide and divide_no_nan,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary):pip binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: No GPU
- GPU model and memory: No GPU


**Describe the current behavior**
Broadcasting not working for divide and divide_no_nan ops. I have 2 tensors, of shape [5,140,280,3,2] and [5,140,280,3]. Attempting to divide with tf.math.divide_no_nan(A, B) yields the following error: 
`tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [5,140,280,3,2] vs. [5,140,280,3] [Op:DivNoNan]`
I have also tested the regular tf.math.divide() op and it yields the same error.
**Describe the expected behavior**
With broadcast behavior I would expect to be able to easily divide A by B, with B being broadcast to fit the innermost dimension of A.
**Standalone code to reproduce the issue**
a = tf.ones([5,140,280,3,2])
b = tf.ones([5,140,280,3])
c = tf.math.divide_no_nan(a, b)
**Other info / logs** None"
43242,Model Input & Output does not match. ,"**System information**
Hello everyone,
I am working on a project to implement a prediction algorithm, which is to be implemented for a microcontroller. 
Therefore, I have not installed the complete library, instead I have downloaded the most recent repository. Then I built the c++ project on ""S32 Design Studio"" and run it on the laptop with Windows 10(haven't started to compile on microcontroller yet)
I have used the tf-nightly : 
```
!pip install tf-nightly
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()
```
**the tf keras model which was as below: **
```
n_epoch = 1
n_batch = 1
n_neurons= 25
time_ev = 6

# design network
model = Sequential()
model.add(tf.keras.layers.Input(batch_input_shape=(n_batch,time_ev, 3), name='input')) # nbatch, num_feature, time
model.add(tf.keras.layers.Flatten())
model.add(Dense(1, activation=tf.keras.activations.relu))
model.compile(loss='mean_squared_error', optimizer='adam')
```
and I have converted it as below:
```
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

```
```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:2289: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1376: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`layer.updates` will be removed in a future version. '
INFO:tensorflow:Assets written to: /tmp/tmpztyddjcu/assets
INFO:tensorflow:Assets written to: /tmp/tmpztyddjcu/assets
```
Then I have downloaded the model as a .cc file because I could not use the function to read .tflite file: 

```
# Define paths to model files
import os

MODELS_DIR = 'models/'
if not os.path.exists(MODELS_DIR):
    os.mkdir(MODELS_DIR)
MODEL_TF = MODELS_DIR + 'model.pb'
MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'
MODEL_TFLITE = MODELS_DIR + 'model.tflite'
MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'

# # Save the model to disk
open(MODEL_NO_QUANT_TFLITE, ""wb"").write(tflite_model)

# Install xxd if it is not available
!apt-get update && apt-get -qq install xxd
# Convert to a C source file
!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_TFLITE_MICRO}
# Update variable names
REPLACE_TEXT = MODEL_NO_QUANT_TFLITE.replace('/', '_').replace('.', '_')
!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}

files.download(""models/model.cc"") 
```

Finally I have this small script just to import the model and build the interpreter:

```

#include <string.h>

#include ""model.h""

#include ""../../tensorflow-lite/tensorflow/lite/micro/all_ops_resolver.h""
#include ""../../tensorflow-lite/tensorflow/lite/micro/kernels/micro_ops.h""
#include ""../../tensorflow-lite/tensorflow/lite/micro/micro_error_reporter.h""
#include ""../../tensorflow-lite/tensorflow/lite/micro/micro_interpreter.h""
#include ""../../tensorflow-lite/tensorflow/lite/micro/micro_mutable_op_resolver.h""
#include ""../../tensorflow-lite/tensorflow/lite/micro/micro_optional_debug_tools.h""
#include ""../../tensorflow-lite/tensorflow/lite/version.h""

// Globals, used for compatibility with Arduino-style sketches.
namespace {
tflite::ErrorReporter* error_reporter;
//const tflite::Model* model ;
tflite::MicroInterpreter* interpreter = nullptr;

// Create an area of memory to use for input, output, and intermediate arrays.
// Minimum arena size, at the time of writing. After allocating tensors
// you can retrieve this value by invoking interpreter.arena_used_bytes().
constexpr size_t allocator_buffer_size = 2096 /* optimal arena size at the time of writting. */
			+ 16 /* alignment */ + 100 /* some headroom */;

uint8_t allocator_buffer[allocator_buffer_size];
} // namespace

int main() {

	const tflite::Model* model = ::tflite::GetModel(g_model);

	if (model->version() == TFLITE_SCHEMA_VERSION) {
		puts(""Model provided is supported.\n"");
		//fprintf(stderr, ""Model provided is supported.\n"");

	}

	tflite::MicroErrorReporter micro_error_reporter;
	error_reporter = &micro_error_reporter;
	static tflite::AllOpsResolver resolver;
        static tflite::MicroInterpreter static_interpreter(model, resolver, allocator_buffer, allocator_buffer_size, error_reporter);
        interpreter = &static_interpreter;
	if (interpreter != nullptr) {
		using namespace std;
		puts(""interpreter is not empty\n"");
	}

    TFLITE_MINIMAL_CHECK(interpreter->Invoke() == kTfLiteOk);
    printf(""\n\n=== Post-invoke Interpreter State ===\n"");
    tflite::PrintInterpreterState(interpreter);

	return 0;

}

```

When I run the code, I obtain this in the console : 

```
Model provided is supported.

interpreter is not empty


=== Post-invoke Interpreter State ===
Interpreter has 0 tensors and 2 nodes
Inputs: 0
Outputs: 5


Node   0 Operator Builtin Code  22 RESHAPE
  Inputs: 0 2
  Outputs: 4
Node   1 Operator Builtin Code   9 FULLY_CONNECTED
  Inputs: 4 3 1
  Outputs: 5
```

Isn't this wrong? Input dimension should be (1,6,3) and output should be (1)
I have tested it on python and the tflite_model works as expected,however, when I convert it to a matrix and import with ""model.cc"" is it deteriorated ? I could not set an input matrix with dimension (1,6,3) . 

Thank you in advance."
43239,Model conversion fails with cryptic error when feeding sample during post-training quantization,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): Nightly 2.4.0-dev20200914


**Command used to run the converter or code if you’re using the Python API**

```python
import numpy as np
import tensorflow as tf
import cv2

def representative_dataset_gen(input_image_shape, num_samples_to_generate=100): # dummy generator
  h, w, *_ = input_image_shape
  for _ in range(num_samples_to_generate):
    im = np.random.random([w,h,3])
    yield [im.astype(np.float32)]


# Convert the model
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model') # from the attached zip file
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                                       tf.lite.OpsSet.SELECT_TF_OPS]
converter.inference_input_type = tf.int8
#converter.inference_output_type = tf.int8
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = lambda : representative_dataset_gen([256, 1024, 3], 5)
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
2020-09-15 15:51:43.315246: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:315] Ignored output_format.
2020-09-15 15:51:43.315283: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:318] Ignored drop_control_dependency.
2020-09-15 15:51:43.315515: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: models/runs/mobilenet_v2_kitti_1024x256/tflite/saved_model
2020-09-15 15:51:43.373628: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }
2020-09-15 15:51:43.373665: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: models/runs/mobilenet_v2_kitti_1024x256/tflite/saved_model
2020-09-15 15:51:43.373707: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-09-15 15:51:43.560665: I tensorflow/cc/saved_model/loader.cc:190] Restoring SavedModel bundle.
2020-09-15 15:51:44.001062: I tensorflow/cc/saved_model/loader.cc:174] Running initialization op on SavedModel bundle at path: models/runs/mobilenet_v2_kitti_1024x256/tflite/saved_model
2020-09-15 15:51:44.187305: I tensorflow/cc/saved_model/loader.cc:261] SavedModel load for tags { serve }; Status: success: OK. Took 871790 microseconds.
see current operation: %126 = ""tfl.concatenation""(%125#0, %125#1, %125#2, %125#3) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>
2020-09-15 15:51:46.541754: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %126 = ""tfl.concatenation""(%125#0, %125#1, %125#2, %125#3) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>

see current operation: %129 = ""tfl.concatenation""(%128#0, %128#1, %128#2, %128#3) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>
2020-09-15 15:51:46.541786: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %129 = ""tfl.concatenation""(%128#0, %128#1, %128#2, %128#3) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>

see current operation: %140 = ""tfl.concatenation""(%133, %137, %135, %139) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>
2020-09-15 15:51:46.541823: W tensorflow/compiler/mlir/lite/tf_to_tfl_flatbuffer.cc:138] see current operation: %140 = ""tfl.concatenation""(%133, %137, %135, %139) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>, tensor<100x1xf32>) -> tensor<100x4xf32>

2020-09-15 15:51:46.618251: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
INFO: TfLiteFlexDelegate delegate: 3 nodes delegated out of 143 nodes with 1 partitions.

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 724, in convert
    return super(TFLiteSavedModelConverterV2,
  File ""/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 648, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 474, in _calibrate_quantize_model
    return calibrate_quantize.calibrate_and_quantize(
  File ""/home/slr/anaconda3/envs/tf_nightly/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 94, in calibrate_and_quantize
    self._calibrator.FeedTensor(sample)
IndexError: _Map_base::at
```

**Also, please include a link to the saved model or GraphDef**

[saved_model.zip](https://github.com/tensorflow/tensorflow/files/5225443/saved_model.zip)

**Failure details**
When converting the model in FP32 mode, the model converts and works correctly.
When attempting post-training-quantization, a cryptic error message is raised with no information regarding what exactly went wrong and making it impossible to debug any further.

"
43238,Interpreter AllocateTensors() does not finish or report error,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): Source - Arduino TensorFlowLite version 2.1.0-ALPHA(not precompiled)
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): SparkFun Edge

**Describe the problem**

Allocating a model does not finish either with a error or success. Before I added the SOFTMAX and RESHAPE ops I received at least messages through the report serial at 9600. Now `Serial.print(""Allocating Tensors ...\n"");` is printing and nothing happens as if it `TfLiteStatus allocate_status = interpreter->AllocateTensors();` is stuck or an error message is not arriving. I use the biggest arena size I got via decreasing until the SRAM overflow error disappeared. The used model is included at the end. It was created from the [quantized version of mobilenet v1](http://download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_0.25_128_quant.tgz) via the notebook [here](https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/train_hello_world_model.ipynb#scrollTo=HPSFmDL7pv2L).

I am new to microcontrollers and ML.

**Please provide the exact sequence of commands/steps when you ran into the problem**
```
namespace {
	tflite::ErrorReporter* error_reporter = nullptr;
	const tflite::Model* model = nullptr;
	tflite::MicroInterpreter* interpreter = nullptr;
	TfLiteTensor* input = nullptr;

	// An area of memory to use for input, output, and intermediate arrays.
	constexpr int kTensorArenaSize = 270 * 1024;
	static uint8_t tensor_arena[kTensorArenaSize];
} 

void setup() {

	Serial.begin(460800);
	do {
		delay(500);
	} while (!Serial);

	static tflite::MicroErrorReporter micro_error_reporter;
	error_reporter = &micro_error_reporter;

	Serial.print(""Tensorflow setup...\n"");

	Serial.print(""Loading model ...\n"");

	model = tflite::GetModel(g_object_detect_model_data);
	if (model->version() != TFLITE_SCHEMA_VERSION) {
		Serial.printf(""Model provided is schema version %d not equal to supported version %d."",
			model->version(), TFLITE_SCHEMA_VERSION);
		return;
	}

	Serial.print(""Model loaded ...\n"");

	Serial.print(""Building interpreter ...\n"");

	static tflite::MicroMutableOpResolver<5> micro_op_resolver;

	micro_op_resolver.AddBuiltin(
		tflite::BuiltinOperator_DEPTHWISE_CONV_2D,
		tflite::ops::micro::Register_DEPTHWISE_CONV_2D());

	micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_CONV_2D,
		tflite::ops::micro::Register_CONV_2D());

	micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_AVERAGE_POOL_2D,
		tflite::ops::micro::Register_AVERAGE_POOL_2D());

	micro_op_resolver.AddBuiltin(tflite::BuiltinOperator_SOFTMAX,
		tflite::ops::micro::Register_SOFTMAX());

	micro_op_resolver.AddBuiltin(
		tflite::BuiltinOperator_RESHAPE,
		tflite::ops::micro::Register_RESHAPE());

	static tflite::MicroInterpreter static_interpreter(
		model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);
	interpreter = &static_interpreter;

	Serial.print(""Interpreter built ...\n"");

	Serial.print(""Allocating Tensors ...\n"");

	TfLiteStatus allocate_status = interpreter->AllocateTensors();

	if (allocate_status != kTfLiteOk) {
		Serial.print(""AllocateTensors() failed\n"");
		return;
	}

	Serial.print(""Tensors allocated ...\n"");

	input = interpreter->input(0);

	Serial.print(""Tensorflow setup finished.\n"");

}
```

[model.zip](https://github.com/tensorflow/tensorflow/files/5224464/model.zip)
"
43236,Tensorflow 2.2 and 2.3 not detecting GPU with CUDA 10.1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.3 and 2.2
- Python version: 3.6
- Installed using virtualenv? pip? conda?: venv and pip
- GCC/Compiler version (if compiling from source): 7.5
- CUDA/cuDNN version: 10.1
- GPU model and memory: K80



**Describe the problem**
After installing tensorflow, GPU is not detected and getting error: 'Cannot open dynamic library **libcublas.so.10**'.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. All the steps are followed from the official tensorflow page as it is: https://www.tensorflow.org/install/gpu and https://www.tensorflow.org/install/pip.
2. Also, i have to install cuda-toolkit separately.
3. Finally added CUDA-10.1 path in bashrc file.

**How I fix the problem**:

I started with a clean VM on Azure with nothing installed. Then followed the tensorflow guides (above) to install NVIDIA-Driver, CUDA 10.1, cuDNN, cuda-toolkit and tensorflow.

After all these steps, my local folder had two cuda folders (don't know why):
/usr/local/cuda-10.1/lib64/
/usr/localo/cuda-10.2/lib64/

**The error which I was getting was for dynamic library 'libcublas.so.10'. And this file was not present in folder 'cuda-10.1', but instead it was present in 'cuda-10.2' (note, that i have installed everything in venv)**

**I have to manually copy all the files (including files inside the 'stubs' folder). And then it works.**

This site also mention this issue, where they say that with CUDA 10.1, some of the libraries are installed differently - https://forums.developer.nvidia.com/t/cublas-for-10-1-is-missing/71015/4 (the steps here are when you install libraries at system level and not venv).

Expected Behaviour:
Either tensorflow should automatically refer to the missing dynamic libraries or mention how to fix this in Install Set up. 

Note: The errors are similar when you install CUDA 10.2, it's just the dynamic library version are different."
43235,libtensorflow_inference.so: protobuf failed to link __android_log_write and dl_iterate_phdr,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 (1809)
- TensorFlow installed from (source or binary): Source
- TensorFlow version: Master
- Python version: 3.7.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): 4.9.x
- CUDA/cuDNN version: /
- GPU model and memory: /

**Describe the [problem**]
I'm trying to build the libtensorflow_inference.so stack for android arm64-v8a (Samsung A10) in order to use a pre trained model and an interial sensor for human activity recognition but for some reason it fails to link __android_log_write and dl_iterate_phdr. 

I found a couple of similar issue within tensorflow (e.g. #29658 ) and within the protobuf library (e.g. [#2719](https://github.com/protocolbuffers/protobuf/issues/2719)) but so far i had no luck with fixing this problem

I tried the lite stack bevor and its build worked perfectly fine but since lite appears to be focused on speech/image recognition it's a bit limited for our purpose because some of the models (boosted trees in particular) are not supported (correct me if im wrong :)). 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
bazel build //tensorflow/tools/android/inference_interface:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=arm64-v8a --verbose_failures -s
```
**Any other info / logs**
I had to make a couple of changes in order to get it that far using the clang compiler. Here is my current config:
[bazelrc.txt](https://github.com/tensorflow/tensorflow/files/5222985/bazelrc.txt)

The build log from my last attempt: 
```
SUBCOMMAND: # @com_google_protobuf//:protobuf [action 'Compiling external/com_google_protobuf/src/google/protobuf/message.cc', configuration: 84e089be1534a4a8f56cc6899fb88395476bdcb929d979ef8f6aa36da34d3e83, execution platform: @local_execution_config_platform//:platform]
cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow
  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2
    SET ANDROID_NDK_API_LEVEL=21
    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944
    SET ANDROID_SDK_API_LEVEL=30
    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/python.exe
    SET PYTHON_LIB_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -fpic -isystemexternal/androidndk/ndk/sysroot/usr/include/aarch64-linux-android -D__ANDROID_API__=21 -no-canonical-prefixes -Wno-invalid-command-line-argument -Wno-unused-command-line-argument -funwind-tables -fstack-protector-strong -fno-addrsig -Werror=return-type -Werror=int-to-pointer-cast -Werror=pointer-to-int-cast -Werror=implicit-function-declaration -O2 -g -DNDEBUG -MD -MF bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf/message.pic.d -frandom-seed=bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf/message.pic.o -fPIC -iquote external/com_google_protobuf -iquote bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/arm64-v8a-opt/bin/external/zlib -isystem external/com_google_protobuf/src -isystem bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/arm64-v8a-opt/bin/external/zlib -w -D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI -experimental=preprocessor -std=c++14 -DHAVE_PTHREAD -DHAVE_ZLIB -Woverloaded-virtual -Wno-sign-compare -Wno-unused-function -Wno-write-strings --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64 -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++/include -isystem external/androidndk/ndk/sources/cxx-stl/llvm-libc++abi/include -isystem external/androidndk/ndk/sources/android/support/include -isystemexternal/androidndk/ndk/sysroot/usr/include -c external/com_google_protobuf/src/google/protobuf/message.cc -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protobuf/message.pic.o
ERROR: C:/users/usr/_bazel_usr/hv3mexld/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1): clang failed: error executing command
  cd C:/users/usr/_bazel_usr/hv3mexld/execroot/org_tensorflow
  SET ANDROID_BUILD_TOOLS_VERSION=30.0.2
    SET ANDROID_NDK_API_LEVEL=21
    SET ANDROID_NDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk/ndk/20.1.5948944
    SET ANDROID_SDK_API_LEVEL=30
    SET ANDROID_SDK_HOME=C:/Users/usr/AppData/Local/Android/Sdk
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/python.exe
    SET PYTHON_LIB_PATH=C:/Users/usr/Anaconda3/envs/tensorflow/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
  external/androidndk/ndk/toolchains/llvm/prebuilt/windows-x86_64/bin/clang -o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/protoc bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/_objs/protoc/main.o bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotoc_lib.a -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf.lo -Wl,-no-whole-archive -Wl,-whole-archive bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo -Wl,-no-whole-archive bazel-out/arm64-v8a-opt/bin/external/zlib/libzlib.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++_static.a external/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a/libc++abi.a -DEBUG -OPT=REF -OPT=ICF -static-libgcc -gcc-toolchain external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64 -target aarch64-none-linux-android -Lexternal/androidndk/ndk/sources/cxx-stl/llvm-libc++/libs/arm64-v8a -no-canonical-prefixes -Wl,-z,relro -Wl,--gc-sections --sysroot=external/androidndk/ndk/platforms/android-21/arch-arm64
Execution platform: @local_execution_config_platform//:platform
bazel-out/arm64-v8a-opt/bin/external/com_google_protobuf/libprotobuf_lite.lo(common.o): In function `google::protobuf::internal::DefaultLogHandler(google::protobuf::LogLevel, char const*, int, std::__ndk1::basic_string<char, std::__ndk1::char_traits<char>, std::__ndk1::allocator<char> > const&)':
C:\users\usr\_bazel_usr\hv3mexld\execroot\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:149: undefined reference to `__android_log_write'
C:\users\usr\_bazel_usr\hv3mexld\execroot\org_tensorflow/external/com_google_protobuf/src/google/protobuf/stubs/common.cc:157: undefined reference to `__android_log_write'
external/androidndk/ndk/toolchains/aarch64-linux-android-4.9/prebuilt/windows-x86_64/lib/gcc/aarch64-linux-android/4.9.x\libgcc.a(unwind-dw2-fde-dip.o): In function `_Unwind_Find_FDE':
/usr/local/google/buildbot/src/android/gcc/toolchain/build/../gcc/gcc-4.9/libgcc/unwind-dw2-fde-dip.c:485: undefined reference to `dl_iterate_phdr'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/tools/android/inference_interface:libtensorflow_inference.so failed to build
INFO: Elapsed time: 116.019s, Critical Path: 27.02s
INFO: 234 processes: 234 local.
FAILED: Build did NOT complete successfully
```
Thank you in advance"
43234,tf2.3 c++库编译失败,"cenots 8
gcc 8.3.1

is_not_gcc failed: error executing command 
  (cd /home/docker_data/.bazel_cache_2_3/7a9fafc24832c7560e0a6e1a434658e0/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \
    CUDNN_INSTALL_PATH=/usr/local/cuda-10.2 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib:/usr/local/cuda-10.2/extras/CUPTI/lib64: \
    NCCL_INSTALL_PATH=/usr \
    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/targets/x86_64-linux/lib:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages \
    TENSORRT_INSTALL_PATH=/usr/local/cuda-10.2 \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_PATHS=/usr/local/cuda-10.2 \
    TF_CUDA_VERSION=10.2 \
    TF_CUDNN_VERSION=7 \
    TF_ENABLE_XLA=1 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_TENSORRT_VERSION=7 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.3.0-2.params)
Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/libkernels.pic.lo(mat_mul_op.pic.o): In function `tensorflow::CSRMatMulGPUOp<float>::Compute(tensorflow::OpKernelContext*)':
mat_mul_op.cc:(.text._ZN10tensorflow14CSRMatMulGPUOpIfE7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow14CSRMatMulGPUOpIfE7ComputeEPNS_15OpKernelContextE]+0x14ba): undefined reference to `tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/libkernels.pic.lo(sparse_mat_mul_op.pic.o): In function `tensorflow::CSRSparseMatMulGPUOp<Eigen::GpuDevice, float>::Compute(tensorflow::OpKernelContext*)':
sparse_mat_mul_op.cc:(.text._ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE]+0x8ee): undefined reference to `tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
sparse_mat_mul_op.cc:(.text._ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow20CSRSparseMatMulGPUOpIN5Eigen9GpuDeviceEfE7ComputeEPNS_15OpKernelContextE]+0x97e): undefined reference to `tensorflow::functor::CSRSparseMatrixTranspose<Eigen::GpuDevice, float>::operator()(tensorflow::OpKernelContext*, bool, tensorflow::CSRSparseMatrix const&, tensorflow::CSRSparseMatrix*)'
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 37.074s, Critical Path: 36.43s
INFO: 0 processes.
FAILED: Build did NOT complete successfully"
43233,Usage of MethodNameUpdater() function,"@florence27 can you please report a new issue along with the complete  code to reproduce the error that is not working and cc me on it as well. 
Thanks, 
Goldie

_Originally posted by @goldiegadde in https://github.com/tensorflow/tensorflow/issues/34968#issuecomment-692168824_

As already mentioned in issue #34968 I'd like to use the `MethodNameUpdater()` function in order to change the method name in the signature from predict to classify. Unfortunately, the following code is not running, due to an `AttributeError`:

```
import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(4, input_shape=(FEATURE_SIZE,)))
model.add(tf.keras.layers.Dense(1))
optim = tf.keras.optimizers.Adam()
model.compile(loss='mean_squared_error', optimizer=optim, metrics=['mae'])

x, y = load_features(train_files, scaler)
model.fit(x, y, epochs=num_epochs, callbacks=callbacks)

job_dir = '/path/to/job/'
version = '00000123'
export_path = os.path.join(job_dir, version)

tf.keras.models.save_model(
                model,
                export_path,
                overwrite=True,
                include_optimizer=True,
                save_format=None,
                options=None,
            )

updater = tf.compat.v1.saved_model.builder.MethodNameUpdater(export_path)
updater.replace_method_name(signature_key=""bar"", method_name=""classify"", tags=""serve"")
updater.save(export_path)
```

This is the exact error I'm getting: `AttributeError: module 'tensorflow._api.v2.compat.v1.saved_model.builder' has no attribute 'MethodNameUpdater'`. The `load_features()` function takes the training files and a `MinMaxScaler()` as inputs and returns `np.arrays` with input features and corresponding labels. My setup is the following: 

OS Platform: macOS Catalina, Version 10.15.5
Tensorflow Version: 2.3.0
Python Version: 3.7.9

Any help with this would be appreciated!"
43232,Didn't find op for builtin opcode 'CONV_2D' version '5',"Hi, I'm trying to run inference of a custom trained mobilenet_v2_coco17_320x320_tpu-8 model on a Raspberry pi. I have succesfully trained, saved and converted the model to tf.lite. However, when trying to run the inttepreter on a Raspberry pi 4, I'm getting the following error: ValueError: Didn't find op for builtin opcode 'CONV_2D' version '5' Registration failed.
But when running the interpreter on google colab notebook it works just fine.    

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google colab
- TensorFlow installed from (source or binary): tf-nightly on colab. tf-2.2 on Rapsberry pi


**Provide the text output from tflite_convert**

```
converter = tf.lite.TFLiteConverter.from_saved_model('/content/drive/My Drive/Deteccion_escaleras/models/research/Mobilenet_entrenada/saved_model',signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.experimental_new_converter = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
tflite_model = converter.convert()

with tf.io.gfile.GFile('/content/drive/My Drive/training_mobilenet/model_3.tflite', 'wb') as f:
  f.write(tflite_model)
  print(""Model saved"")
```
Code use for training,evaluation,conversion:
https://colab.research.google.com/drive/1M7SBWcA_gRe4xEww3q8QYdyblV8Ic2Te?usp=sharing


**Any other info / logs**

Generated model files (saved_model.pb & model.tflite): https://drive.google.com/drive/folders/1-JHyeg268jQWyXF_AO348SNN8fgU1TCT?usp=sharing
"
43230,Models loaded from files do not perform as before saving,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS (GNU/Linux 4.15.0-45-generic x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7.5 
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version:  CUDA Version: 10.1 
- GPU model and memory:   NVIDIA-SMI 430.50       Driver Version: 430.50      

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I created a semi-supervised GAN model, based on: https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/. It works with MNIST data.
During the training the model reaches >90% accuracy, and gets saved in a file.
However, when I try to load the saved model and run it on the same data, it performs at random (~10% accuracy).
I tried both SavedModel and H5 formats, but I get the same results. 

**Describe the expected behavior**
I would have expected the model loaded from the file to result in same performance as the model before saving.

**Standalone code to reproduce the issue**

Collab notebook for training: https://colab.research.google.com/drive/1-FnGgEhCPwEjyQHiSWLuwTtqRVb-frpT?usp=sharing
Collab notebook applying the model: https://colab.research.google.com/drive/1JDeMO1e3Z7RVbdW-QEC2V0yWbRtSz0jx?usp=sharing 
(Note: the code is meant to run on a machine, not in Collab)

"
43229,Confused which version TensorfFlow CPU supports,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10/Ubuntu 18.04/Linux Mint
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/a
- TensorFlow installed from (source or binary): Source
- TensorFlow version: v 1.x to  2.3.0
- Python version: 3.6 to 3.8
- Installed using virtualenv? pip? conda?: virtualenv, pip & conda
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: N/a
- GPU model and memory: N/a

**Describe the problem**

In terms of the OS Platform, I have used 3 different OS such as Windows 10, Ubuntu 18.04 and Linux Mint.

I am having a persistent problem in installing TensorFlow GPU before because the NVIDIA Driver cannot be installed/continued due to incompatible versions. Thus, I gave up on it, and now am only interested in installing TensorFlow CPU.

I used to having problems when installing TensorFlow GPU when it always says 'DLL failed to load when ......' and 'Importerror no name image_preprocessing'.

**My issue** is whatever I issue 'import tensorflow as tf' in Python 3.6 to Python 3.8 with TF 1.1+ to 2.3, respectively, the following message always appears (see logs).

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Python = 3.6 to 3.8
Tensorflow = 1.1+ to 2.3.0

**Any other info / logs**

```
(tensor0) root@opwrd:/home/user# python
Python 3.8.5 (default, Sep  4 2020, 07:30:14) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Illegal instruction (core dumped)
```

**Is there any simpler way of installing TensorFlow CPU without concerning about its version/getting errors?** I have been working on this for days.

I seriously look forward to hearing from the TensorFlow team about this issue. I am needing to install TensorFlow to perform performance benchmarks on my object detection models, but this cannot be done without having TensorFlow in my machine.

Highly appreciated your answers/advice."
43228,"""Function call stack: _dist_train_step"" error on tensorflow2 object detection api","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to StackOverflow.

If you are reporting a vulnerability, please use the dedicated reporting process.

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Every time I try to train the object detection models (any model, I tried ssd, faster rcnn, efficientdet..) by tf2 model train script (which is model_main_tf2.py), I got this error message every 1500~2000 steps.

image

I ran the notebook in google colab, and I got the colab pro upgrade either. (I thought this issue might be the memory issue on GPU or TPU)

I just followed the object detection api todo ..
Anyone can help..?
I also put a link to my colab notebook too.

https://colab.research.google.com/drive/1Xg5NKpipLTUCCyTFDYYCC_xAkPRTUyCc?usp=sharing"
43227,"""Function call stack: _dist_train_step"" error on tensorflow2 object detection api","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Every time I try to train the object detection models (any model, I tried ssd, faster rcnn, efficientdet..) by tf2 model train script (which is model_main_tf2.py), I got this error message every 1500~2000 steps.

![image](https://user-images.githubusercontent.com/70259589/93154006-87b3e180-f73d-11ea-90c7-ddc11707d7f2.png)

I ran the notebook in google colab, and I got the colab pro upgrade either. (I thought this issue might be the memory issue on GPU or TPU)

I just followed the object detection api todo ..
Anyone can help..?
I also put a link to my colab notebook too. 

https://colab.research.google.com/drive/1Xg5NKpipLTUCCyTFDYYCC_xAkPRTUyCc?usp=sharing
"
43224,keras layers API documentation not sorted properly,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/keras/layers

## Description of issue (what needs changing):

Very minor issue, but the overview list of all layer classes is not consistently sorted alphabetically, though it very much feels like it is intended to (e.g. the GRU layer is listed above the GaussianDropout layer). In the sidebar it is sorted correctly alphabetically. I personally nearly overlooked the presence of the GRU layer because they are differently sorted in the sidebar and overview.

Unfortunately could I not find the API documentation in the tensorflow/docs repository, which is why I am opening the issue instead of offering a pull request. If you point me to the API documentation source can I gladly take care of that."
43223,OP_REQUIRES failed : Not found: No algorithm worked!,"**System information**
- OS Platform and Distribution:  Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip install)
- TensorFlow version (use command below): v1.12.1-41444-g9396e98574 2.4.0-dev20200913
- Python version: 3.8.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0 and 8.0
- GPU model and memory: GTX 2060

**Describe the current behavior**
The script crashes at prediction.

**Describe the expected behavior**
The script shall compute prediction

**Standalone code to reproduce the issue**
```
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Sequential
import numpy as np

model = Sequential([
    ResNet50(include_top=False, weights=""imagenet"", input_shape=(224, 224, 3), pooling=""avg""),
    Dropout(0.5),
    Dense(64, activation='relu'),
])

X = np.zeros((1, 224, 224, 3))
print(model.predict(X))
```

Note that if adding the line `tf.config.set_visible_devices([], 'GPU')`, this problem disappear

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
2020-09-14 21:29:32.163353: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-14 21:29:32.931061: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-09-14 21:29:32.931534: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-14 21:29:32.953860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:32.954284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s
2020-09-14 21:29:32.954301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-14 21:29:32.956135: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-09-14 21:29:32.957003: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-14 21:29:32.957172: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-14 21:29:32.959170: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-14 21:29:32.959632: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-09-14 21:29:32.959728: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-09-14 21:29:32.959814: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:32.960277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:32.960669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-14 21:29:32.960919: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-14 21:29:32.961335: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2020-09-14 21:29:32.961418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:32.961827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:09:00.0 name: GeForce RTX 2060 computeCapability: 7.5
coreClock: 1.68GHz coreCount: 30 deviceMemorySize: 5.79GiB deviceMemoryBandwidth: 312.97GiB/s
2020-09-14 21:29:32.961841: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-14 21:29:32.961854: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-09-14 21:29:32.961864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-14 21:29:32.961873: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-14 21:29:32.961882: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-14 21:29:32.961892: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-09-14 21:29:32.961900: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-09-14 21:29:32.961949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:32.962375: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:32.962759: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-14 21:29:32.962779: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-14 21:29:33.321292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-14 21:29:33.321321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-14 21:29:33.321325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-14 21:29:33.321485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:33.321815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:33.322113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-14 21:29:33.322399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5352 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2060, pci bus id: 0000:09:00.0, compute capability: 7.5)
2020-09-14 21:29:34.367791: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 1)
2020-09-14 21:29:34.384838: I tensorflow/core/platform/profile_utils/cpu_utils.cc:108] CPU Frequency: 3593095000 Hz
2020-09-14 21:29:34.803301: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-09-14 21:29:35.075322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-09-14 21:29:35.505290: W tensorflow/core/framework/op_kernel.cc:1774] OP_REQUIRES failed at conv_ops.cc:1114 : Not found: No algorithm worked!
Traceback (most recent call last):
  File ""report_bug.py"", line 13, in <module>
    print(model.predict(X))
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 102, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1584, in predict
    tmp_batch_outputs = self.predict_function(iterator)
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 787, in __call__
    result = self._call(*args, **kwds)
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 853, in _call
    return self._concrete_stateful_fn._call_flat(
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1919, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 556, in call
    outputs = execute.execute(
  File ""/opt/jupyterhub/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!
         [[node sequential/resnet50/conv1_conv/Conv2D (defined at report_bug.py:13) ]] [Op:__inference_predict_function_6649]

Function call stack:
predict_function
```
"
43219,LR Custom Scheduler not working TF 2.3,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04.5 LTS

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: GTX 1050 

Rest of the information attached 
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/5220028/tf_env.txt)


**Describe the current behavior**
Returns an error when using the fit function. see the log below.

**Describe the expected behavior**
Decays the lr during training. works by just passing schdule object to optimizer lr argument.

![lr_schedule](https://user-images.githubusercontent.com/17620536/92232869-b671c280-eec8-11ea-9f20-9c274cadcddc.png)

**Standalone code to reproduce the issue**
```
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
  def __init__(self, warmup_steps=1e4):
    super().__init__()

    self.warmup_steps = tf.cast(warmup_steps, tf.float32)
    
  def __call__(self, step):
    step = tf.cast(step, tf.float32)
    m = tf.maximum(self.warmup_steps, step)
    m = tf.cast(m, tf.float32)
    lr = tf.math.rsqrt(m)
    
    return lr

```
```
learning_rate_fn = CustomSchedule()
optimizer = tf.keras.optimizer.Adam(learning_rate_fn)

model.compile(optimizer=optimizer, ...)
model.fit(dataset, epochs=1)
```
### Traceback
**TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of <function Model.make_train_function.<locals>.train_function at 0x7fdf2c2b9c80>, found return value of type <class '__main__.CustomSchedule'>, which is not a Tensor.**


```
Epoch 1/50
/home/ml/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    547     try:
--> 548       str_values = [compat.as_bytes(x) for x in proto_values]
    549     except TypeError:

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py in <listcomp>(.0)
    547     try:
--> 548       str_values = [compat.as_bytes(x) for x in proto_values]
    549     except TypeError:

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/util/compat.py in as_bytes(bytes_or_text, encoding)
     86     raise TypeError('Expected binary or unicode string, got %r' %
---> 87                     (bytes_or_text,))
     88 

TypeError: Expected binary or unicode string, got <__main__.CustomSchedule object at 0x7f02bc039a20>

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in convert(x)
    941         try:
--> 942           x = ops.convert_to_tensor_or_composite(x)
    943         except (ValueError, TypeError):

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_or_composite(value, dtype, name)
   1621   return internal_convert_to_tensor_or_composite(
-> 1622       value=value, dtype=dtype, name=name, as_ref=False)
   1623 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor_or_composite(value, dtype, name, as_ref)
   1660         as_ref=as_ref,
-> 1661         accepted_result_types=(Tensor, composite_tensor.CompositeTensor))
   1662 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1498     if ret is None:
-> 1499       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1500 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    337   _ = as_ref
--> 338   return constant(v, dtype=dtype, name=name)
    339 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    263   return _constant_impl(value, dtype, shape, name, verify_shape=False,
--> 264                         allow_broadcast=True)
    265 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    281           value, dtype=dtype, shape=shape, verify_shape=verify_shape,
--> 282           allow_broadcast=allow_broadcast))
    283   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape, allow_broadcast)
    551                       ""Contents: %s. Consider casting elements to a ""
--> 552                       ""supported type."" % (type(values), values))
    553     tensor_proto.string_val.extend(str_values)

TypeError: Failed to convert object of type <class '__main__.CustomSchedule'> to Tensor. Contents: <__main__.CustomSchedule object at 0x7f02bc039a20>. Consider casting elements to a supported type.

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-22-8c9c3cff7892> in <module>
      1 epochs_done = 0
      2 model.fit(train_dataset, epochs=50, callbacks=callbacks, 
----> 3           validation_data=validation_dataset, initial_epoch=epochs_done)

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    989       # TensorArrays and `None`s.
    990       func_outputs = nest.map_structure(convert, func_outputs,
--> 991                                         expand_composites=True)
    992 
    993       check_mutation(func_args_before, func_args, original_func)

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

~/anaconda3/envs/hugging/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py in convert(x)
    946               ""must return zero or more Tensors; in compilation of %s, found ""
    947               ""return value of type %s, which is not a Tensor."" %
--> 948               (str(python_func), type(x)))
    949       if add_control_dependencies:
    950         x = deps_ctx.mark_as_return(x)

TypeError: To be compatible with tf.eager.defun, Python functions must return zero or more Tensors; in compilation of <function Model.make_train_function.<locals>.train_function at 0x7f02601460d0>, found return value of type <class '__main__.CustomSchedule'>, which is not a Tensor.

```"
43212,tensorflow model to MLIR,"I want convert tensorflow model to MLIR .

I saved tensorflow model using model.save() as saved_model.pb.


How can lower the convert model to MLIR (affine/std dialect)?
What are the steps to do so?

Thank You"
43211,Converter Error : for converting saved-model to tflite ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (or github SHA if from source): 2.3.0 , also installed tf-nightly


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```
import tensorflow as tf

saved_model_dir = 'C:\\Users\\diksh\\Desktop\\Fine_Tuned_Model\\saved_model'


model = tf.saved_model.load(saved_model_dir)
model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 256, 256, 3])
tf.saved_model.save(model, ""saved_model_updated"", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])
# Convert
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir='saved_model_updated', signature_keys=['serving_default'])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()

## TFLite Interpreter to check input shape
interpreter = tf.lite.Interpreter(model_content=tflite_model)
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test the model on random input data.
input_shape = input_details[0]['shape']
print(input_shape)

**The output from the converter invocation**
Exception                                 Traceback (most recent call last)
~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    198                                                  debug_info_str,
--> 199                                                  enable_mlir_converter)
    200       return model_str

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     37       debug_info_str,
---> 38       enable_mlir_converter)
     39 

Exception: <unknown>:0: error: loc(""Func/StatefulPartitionedCall/input/_0""): requires all operands and results to have compatible element types
<unknown>:0: note: loc(""Func/StatefulPartitionedCall/input/_0""): see current operation: %1 = ""tf.Identity""(%arg0) {_class = [""loc:@Func/StatefulPartitionedCall/StatefulPartitionedCall/input/_702""], device = """"} : (tensor<1x256x256x3x!tf.quint8>) -> tensor<1x256x256x3xui8>


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
<ipython-input-4-b50eec57af9d> in <module>
     11 converter.optimizations = [tf.lite.Optimize.DEFAULT]
     12 converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
---> 13 tflite_model = converter.convert()
     14 
     15 ## TFLite Interpreter to check input shape

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\lite.py in convert(self)
   1074         Invalid quantization parameters.
   1075     """"""
-> 1076     return super(TFLiteConverterV2, self).convert()
   1077 
   1078 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\lite.py in convert(self)
    898 
    899     return super(TFLiteFrozenGraphConverterV2,
--> 900                  self).convert(graph_def, input_tensors, output_tensors)
    901 
    902 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\lite.py in convert(self, graph_def, input_tensors, output_tensors)
    631         input_tensors=input_tensors,
    632         output_tensors=output_tensors,
--> 633         **converter_kwargs)
    634 
    635     calibrate_and_quantize, flags = quant_mode.quantizer_flags(

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    572       input_data.SerializeToString(),
    573       debug_info_str=debug_info_str,
--> 574       enable_mlir_converter=enable_mlir_converter)
    575   return data
    576 

~\AppData\Roaming\Python\Python37\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--> 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(""Func/StatefulPartitionedCall/input/_0""): requires all operands and results to have compatible element types
<unknown>:0: note: loc(""Func/StatefulPartitionedCall/input/_0""): see current operation: %1 = ""tf.Identity""(%arg0) {_class = [""loc:@Func/StatefulPartitionedCall/StatefulPartitionedCall/input/_702""], device = """"} : (tensor<1x256x256x3x!tf.quint8>) -> tensor<1x256x256x3xui8>

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43210,clone_model doesn't work properly. Dimensions of inputs should match.,"**System information**
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.1
- Python version: 3.7:
- GCC/Compiler version (if compiling from source):
- GPU model and memory: tesla v100

**Describe the current behavior**
I am training my custom models and I have used the following code to clone and train a model for domain adaptation tasks. Suddenly this stopped working. I am training on a cluster and using this line of code for 4 months without a problem. 

```
model.fit(train_dataset)
model_target = tf.keras.models.clone_model(model)
model_target.set_weights(model.get_weights())

for layer in model_target.layers:
        layer._name = layer.name + str(""_target"")

model_target.compile(optimizer=tf.keras.optimizers.Adam(0.0001),
                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                      metrics=[""accuracy""])

model_target.fit(train_dataset)
```
It produces the error :

```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-46-095ac6db4568> in <module>
     15                       metrics=[""accuracy""])
     16 print(""second"")
---> 17 model_target.fit(train_dataset)

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in evaluate(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)
   1377             with trace.Trace('TraceContext', graph_type='test', step_num=step):
   1378               callbacks.on_test_batch_begin(step)
-> 1379               tmp_logs = test_function(iterator)
   1380               if data_handler.should_sync:
   1381                 context.async_wait()

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    838         # Lifting succeeded, so variables are initialized and we can run the
    839         # stateless function.
--> 840         return self._stateless_fn(*args, **kwds)
    841     else:
    842       canon_args, canon_kwds = \

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2827     with self._lock:
   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2830 
   2831   @property

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1846                            resource_variable_ops.BaseResourceVariable))],
   1847         captured_inputs=self.captured_inputs,
-> 1848         cancellation_manager=cancellation_manager)
   1849 
   1850   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1922       # No tape is watching; skip to running the function.
   1923       return self._build_call_outputs(self._inference_function.call(
-> 1924           ctx, args, cancellation_manager=cancellation_manager))
   1925     forward_backward = self._select_forward_and_backward_functions(
   1926         args,

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    548               inputs=args,
    549               attrs=attrs,
--> 550               ctx=ctx)
    551         else:
    552           outputs = execute.execute_with_cancellation(

~/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError:  ConcatOp : Dimensions of inputs should match: shape[0] = [5,16384] vs. shape[1] = [20,8192]
	 [[node functional_1/features_target/concat (defined at <ipython-input-46-095ac6db4568>:17) ]] [Op:__inference_test_function_125643]

Function call stack:
test_function
```

the main model trains without any problem on the same dataset, so the dataset can not be the problem. 
I am making the main model using the following code:


```
input_range = Input(input_shape_range, name=""input_range"")
    input_doppler = Input(input_shape_doppler, name=""input_doppler"")

    r = Conv2D(filters=8, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c1_r"",
               data_format=""channels_first"")(input_range)
    r = Conv2D(filters=16, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c2_r"",
               data_format=""channels_first"")(r)
    r = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=""Valid"", name=""m1_r"",
                  data_format=""channels_first"")(r)
    r = Conv2D(filters=16, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c3_r"",
               data_format=""channels_first"")(r)
    r = Conv2D(filters=32, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c4_r"",
               data_format=""channels_first"")(r)
    r = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=""Valid"", name=""m2_r"",
                  data_format=""channels_first"")(r)

    features_range = Flatten(name=""flatten_r"")(r)

    d = Conv2D(filters=8, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c1_d"",
               data_format=""channels_first"")(input_doppler)
    d = Conv2D(filters=16, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c2_d"",
               data_format=""channels_first"")(d)
    d = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=""Valid"", name=""m1_d"",
                  data_format=""channels_first"")(d)
    d = Conv2D(filters=16, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c3_d"",
               data_format=""channels_first"")(d)
    d = Conv2D(filters=32, kernel_size=(3, 3), padding=""same"", activation=""relu"", name=""c4_d"",
               data_format=""channels_first"")(d)
    d = MaxPool2D(pool_size=(2, 2), strides=(2, 2), padding=""Valid"", name=""m2_d"",
                  data_format=""channels_first"")(d)
    features_doppler = Flatten(name=""flatten_d"")(d)

    features = tf.keras.layers.concatenate([features_range, features_doppler], name=""features"" ,axis=1)

    fc_1 = Dense(256, activation=""relu"", use_bias=True, name=""fc_1"",
                 kernel_regularizer= tf.keras.regularizers.l2(0.01))(features)
    drop_1 = Dropout(0.2)(fc_1)
    fc_2 = Dense(128, activation=""relu"", use_bias=True, name=""fc_2"",
                 kernel_regularizer= tf.keras.regularizers.l2(0.01))(features)
    drop_2 = Dropout(0.2)(fc_2)
    fc_3 = Dense(64, activation=""relu"", use_bias=True, name=""fc_3"",
                 kernel_regularizer= tf.keras.regularizers.l2(0.01))(drop_2)
    drop_3 = Dropout(0.2)(fc_3)
    out = Dense(5, use_bias=True, name=""out"")(drop_3)

    model = tf.keras.Model(inputs=[input_range, input_doppler], outputs=out)
```"
43208,Question about TFLite GPU delegate,"I was trying to build TFLite GPU delegate on ARM device such as Raspberry pi 4.
After reading the guide document, I found that GPU delegate only uses Bazel build system and it only supports Android and iOS.

My question is that:
1) Is there any plan to support Makefile or CMake to build TFLite GPU delegate?

2) If I'd like to use TFLite GPU delegate on other Linux dist such as Ubuntu ARM, can I do that? If possible, how?

3) Is there any roadmap or plan when Raspberry pi 4 GPU will be a supported TFLite GPU delegate?


Thank you in advance.

**System information**
- OS Platform and Distribution: Linux dist instead of Android.
- Mobile device: Raspberry pi 4 or other ARM device.
- TensorFlow build from source
- TensorFlow version: latest 
"
43207,RuntimeError: Quantization not yet supported for op: %,"**System information**
- OS Platform and Distribution (Linux Ubuntu 18.04):
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): TF 2.3.0


**Command used to run the converter or code if you’re using the Python API**
```
def representative_data_gen():
    dataset_list = os.listdir(data_dir)
    num_calibration_images = 100
    norm_factor = 255.0
    for i in range(num_calibration_images):
        image_name = next(iter(dataset_list))
 
        image = cv2.imread(os.path.join(data_dir, image_name), 1)
        image = image.astype(np.float32)
        image = image/norm_factor

        image = tf.expand_dims(image, 0)
        yield [image]


converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.target_spec.supported_types = [tf.int8]

# These set the input and output tensors to uint8 (added in r2.3)
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

converter.allow_custom_ops = True
tflite_model = converter.convert()
```

** Model definition:
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

def conv2d_block_3layers(input_tensor, n_filters, kernel_size=3, dropout=0.2, 
                         batchnorm=True, activation=True):
    # first layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(input_tensor)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    # second layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(x)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    x = layers.Dropout(dropout)(x)
    # third layer
    x = layers.Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),
                      padding = 'same')(x)
    if batchnorm:
        x = layers.BatchNormalization()(x)
    if activation:
        x = layers.Activation('relu')(x)
    return x

def UNET_v2(nClasses=25, input_height=288, input_width=224, n_filters=64, dropout=0.2, 
            batchnorm=True, activation=True):
  
    img_input = layers.Input(shape=(input_height, input_width, 3))  

    c1 = conv2d_block_3layers(img_input, n_filters * 1, kernel_size=3, batchnorm = batchnorm, activation=activation)
    p1 = layers.MaxPooling2D((2, 2))(c1) 

    c2 = conv2d_block_3layers(p1, n_filters * 2, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p2 = layers.MaxPooling2D((2, 2))(c2) 

    c3 = conv2d_block_3layers(p2, n_filters * 4, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p3 = layers.MaxPooling2D((2, 2))(c3) 

    c4 = conv2d_block_3layers(p3, n_filters * 4, kernel_size=3, batchnorm = batchnorm,  activation=activation)
    p4 = layers.MaxPooling2D((2, 2))(c4) 

    c5 = conv2d_block_3layers(p4, n_filters = n_filters * 8, kernel_size=3, batchnorm = batchnorm, activation=activation)
    p5 = layers.Dropout(dropout)(c5) 

    up6  = layers.Conv2DTranspose(n_filters * 4, kernel_size=(3,3), strides=(2,2), padding=""same"")(p5)
    # up6  = layers.UpSampling2D()(p5) 
    m6 = layers.Concatenate(axis=3)([up6, c4])
    c6 = conv2d_block_3layers(m6, n_filters * 4, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up7 = layers.Conv2DTranspose(n_filters * 4, kernel_size=(3,3), strides=(2,2), padding=""same"")(c6)
    # up7  = layers.UpSampling2D()(c6) 
    m7 = layers.Concatenate(axis=3)([up7, c3])
    c7 = conv2d_block_3layers(m7, n_filters * 4, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up8 = layers.Conv2DTranspose(n_filters * 2, kernel_size=(3,3), strides=(2,2), padding=""same"")(c7)
    # up8  = layers.UpSampling2D()(c7) 
    m8  = layers.Concatenate(axis=3)([up8, c2])
    c8 = conv2d_block_3layers(m8, n_filters * 2, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    up9 = layers.Conv2DTranspose(n_filters * 1, kernel_size=(3,3), strides=(2,2), padding=""same"")(c8)
    # up9  = layers.UpSampling2D()(c8) 
    m9 = layers.Concatenate(axis=3)([up9, c1])
    c9 = conv2d_block_3layers(m9, n_filters * 1, kernel_size = 3, batchnorm = batchnorm, activation=activation)

    outputlayer = tf.keras.layers.Conv2D(filters=nClasses, kernel_size=1, activation=""softmax"")(c9)
    
    model = tf.keras.Model(inputs=img_input, outputs=outputlayer)
    model.summary(line_length=124)
    return model
```
 
**The output from the converter invocation**
```
Traceback (most recent call last):
  File ""keras_to_tflite.py"", line 105, in <module>
    tflite_model = converter.convert()
  File ""/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 831, in convert
    self).convert(graph_def, input_tensors, output_tensors)
  File ""/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/lite.py"", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""/home/ths/anaconda3/envs/py37_tf23/lib/python3.7/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 98, in calibrate_and_quantize
    np.dtype(activations_type.as_numpy_dtype()).num)
RuntimeError: Quantization not yet supported for op: %

```



The model architecture uses Conv2D, batchnormalization, dropout, MaxPooling2D, Conv2DTranspose, Concatenate layers. Is the operation % used in any of the mentioned layers? The % operation is not used at all.  
"
43206,Architecture of Tensorflow model Zoo ,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
https://github.com/tensorflow/models/tree/master/official

## Description of issue:
Where i can find full architecture of all the pre-trained model of Tensorflow object detection api. And also the difference between different version. For example: Difference between SSD MobileNet V2 FPNLite 640x640 and SSD MobileNet v2 320x320???

"
43205,Sparse tensor error message when applying constraint to dense variable,"
**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): Conda
- TensorFlow version: I tried versions 2.2.0, 2.3.0, 1.15, and 1.14
- Python version: 3.6 and 3.8.3
- CUDA/cuDNN version: CUDA 10.1 and cuDNN 7.6 for TF 2.3.0 and 2.2.0, CUDA 10.0 and cuDNN 7.4 for TF 1.15 and 1.14
- GPU model and memory: GeForce GTX TITAN X (12 GB)

**Describe the current behavior**

I am trying to impose a constraint on a trainable variable. 
However, at some point in my pipeline, I am applying a tf.gather operation to the variable that I am constraining, which is causing a runtime error.
The error message (see below) says that a constraint function cannot be used on a sparse variable.
However, my variable is not sparse.
When I am excluding the tf.gather operation, it works without any errors.

**Describe the expected behavior**

The variable should be constrained to a specific range without any errors.

**Standalone code to reproduce the issue**

Colab: https://colab.research.google.com/drive/1Cv-ftL1E0tpC7-1sqO3AMNQ9kSto8m4_?usp=sharing#scrollTo=RY1T16oqz9yo

This is not working:
```python
import tensorflow as tf
import numpy as np

tf.compat.v1.disable_v2_behavior()

y = tf.Variable(name='y',
                initial_value=np.random.rand(5, 2) * 2,
                dtype=tf.float32,
                constraint=lambda x: tf.clip_by_value(x, clip_value_min=-1.0, clip_value_max=1.0), 
                )

yy = tf.gather(y, axis=0, indices=[0, 1, 2])
loss = tf.reduce_sum(yy)
step = tf.compat.v1.train.AdamOptimizer(learning_rate=0.1).minimize(loss=loss, var_list=[y])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    y_, _ = sess.run(fetches=[y, step])
    print(y_)
```


This is working (just removed the tf.gather operation):
```python
import tensorflow as tf
import numpy as np

tf.compat.v1.disable_v2_behavior()

y = tf.Variable(name='y',
                initial_value=np.random.rand(5, 2) * 2,
                dtype=tf.float32,
                constraint=lambda x: tf.clip_by_value(x, clip_value_min=-1.0, clip_value_max=1.0), 
                )

loss = tf.reduce_sum(y)
step = tf.compat.v1.train.AdamOptimizer(learning_rate=0.1).minimize(loss=loss, var_list=[y])

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    y_, _ = sess.run(fetches=[y, step])
    print(y_)`
```

**Other info / logs** 
```python
RuntimeError                              Traceback (most recent call last)
<ipython-input-13-1e1802616fbe> in <module>()
loss = tf.reduce_sum(yy)
step = tf.compat.v1.train.AdamOptimizer(learning_rate=0.1).minimize(loss=loss, var_list=[y])
/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py in update_op(self, optimizer, g)
if self._v.constraint is not None:
 raise RuntimeError(
""Cannot use a constraint function on a sparse variable."")
return optimizer._resource_apply_sparse_duplicate_indices(
g.values, self._v, g.indices)
RuntimeError: Cannot use a constraint function on a sparse variable.
```"
43203,tf-mlir-translate is not coming in binaries,"I built mlir using : bazel build --config opt tensorflow/compiler/mlir/...

But tf-mlir-translate is not there in the tensorflow/bazel-bin/tensorflow/compiler/mlir/tensorflow/


Thank You"
43202,running TF operations on mobile GPU - error conversion to dynamic graph yields issues while running on mobile GPU,"this happens to me as well.

any progress with it?

@pranshugo Your model has TF ops (converted with SELECT_TF) currently this introduces dynamic shape in the runtime. It's on our radar to fix SELECT option to avoid dynamic shape during inference.

_Originally posted by @karimnosseir in https://github.com/tensorflow/tensorflow/issues/38036#issuecomment-685021311_"
43201,micro speech example not working with STM32F769,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): f2c9a930a3f9f8dc0b7904f1d490b2665979d768
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Mbed OS (STM32F769)

**Describe the problem**
I'm trying to build the micro_speech application for STM32F769, following the readme file (which refers instead to STM32F746), but each iteration of the program has the same final scores for each word possible: `[64 64 64 64]`, so the `is_new_command` variable remains always false and there's no prediction.

**Please provide the exact sequence of commands/steps when you ran into the problem**
I followed the same identical steps listed on the readme, but changing the target:
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""CMSIS disco_f769ni"" generate_micro_speech_mbed_project`

`tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed`

`mbed config root .`

`mbed deploy`

`python -c 'import fileinput, glob;
for filename in glob.glob(""mbed-os/tools/profiles/*.json""):
  for line in fileinput.input(filename, inplace=True):
    print line.replace(""\""-std=gnu++98\"""",""\""-std=c++11\"", \""-fpermissive\"""")'`

`mbed compile -m DISCO_F769NI -t GCC_ARM`

In order to compile fine I also had to:
- Modify `tensorflow/lite/micro/examples/micro_speech/CMSIS/Makefile.inc` as explained [here](https://github.com/tensorflow/tensorflow/pull/36444/files)
- Manually copy `arm_math_f16.h` inside `tensorflow\lite\micro\tools\make\gen\mbed_cortex-m4\prj\micro_speech\mbed\third_party\cmsis\CMSIS\DSP\Include` since it wasn't there 
- Add `BSP_DISCO_F769NI` and `LCD_DISCO_F769NI`

The last thing I tried was following the steps explained [here](https://github.com/tensorflow/tensorflow/issues/42918), then use the cmsis-nn flag, but with my board the proposed solution seems not to work.

Thanks in advance for your help and suggestions.

"
43200,tf.keras.callbacks.TensorBoard histogram summary breaks on tf.bool layer weights,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NixOS 20.03 (irrelevant, as tensorflow is installed over pip)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0 (applies to HEAD as well)
- Python version: 3.7.7
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Tensorboard callback crashes with InvalidArgumentError in an attempt to save a histogram for tf.bool layer variable.

**Describe the expected behavior**
The most simple fix would be to skip over non-trainable weights / not-supported types in [_log_weights](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/callbacks.py#L2227).

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import tensorflow_addons as tfa
import numpy as np

themodel = tf.keras.Sequential([
    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1)) # breaks due to tf.bool [_initialzed](https://github.com/tensorflow/addons/blob/v0.11.2/tensorflow_addons/layers/wrappers.py#L105) layer weight
    #tf.keras.layers.Dense(1) # works fine
])

batch_size=1
x = np.zeros([batch_size, 10])
y = np.zeros([batch_size, 1])

themodel.compile(optimizer='sgd', loss='binary_crossentropy')
themodel.fit(x, y, batch_size=batch_size, epochs=1, callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp',histogram_freq=1,update_freq='epoch')])

```

**Other info / logs**

```
  File "".../.venv/lib/python3.7
/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute              
    inputs, attrs, num_outputs)                                                            tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bool is
 not in the list of allowed values: float, double, int32, uint8, int16, int8, int64, bfloat
16, uint16, half, uint32, uint64
        ; NodeDef: {{node WriteHistogramSummary}}; Op<name=WriteHistogramSummary; signature
=writer:resource, step:int64, tag:string, values:T -> ; attr=T:type,default=DT_FLOAT,allowe
d=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, DT_INT8, DT_INT64, DT_BFLOAT16, DT_UI
NT16, DT_HALF, DT_UINT32, DT_UINT64]; is_stateful=true> [Op:WriteHistogramSummary]
```"
43195,tf.keras SparseCategoricalCrossentropy with sample_weight on TPU: Error DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04(Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): TF-2.3.0
- Colab TPU: Yes

Objective: I am trying to using SparseCategoricalCrossentropy with *sample_weight* argument on **TPU** to train Mask Language Model.

Note: Connecting TPU with tf.distribute.cluster_resolver.TPUClusterResolver()

Loss function Looks like this:

```
def masked_sparse_categorical_crossentropy(y_true, y_pred, sample_weight):

  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(
    reduction=tf.keras.losses.Reduction.NONE,
      from_logits=True
  )
  return loss_fn(y_true, y_pred, sample_weight=sample_weight)

```

When I'm running on CPU and GPU the code runs fine. But when I am running on TPU, I am getting an error **on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation'**

```
(0) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broadcastable_is_valid_shape_has_valid_nonscalar_shape_true_195924_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}){{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}
	 [[broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape]]
	 [[broadcast_weights/assert_broadcastable/is_valid_shape]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_18335038187613310273/_6]]
	 [[tpu_compile_succeeded_assert/_18335038187613310273/_6/_265]]
  (1) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broadcastable_is_valid_shape_has_valid_nonscalar_shape_true_195924_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}){{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}
	 [[broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape]]
	 [[broadcast_weights/assert_broadcastable/is_valid_shape]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_18335038187613310273/_6]]
	 [[tpu_compile_succeeded_assert/_18335038187613310273/_6/_237]]
  (2) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broadcastable_is_valid_shape_has_valid_nonscalar_shape_true_195924_const_0[] on XLA_TPU_JIT: DenseToDenseSetOperation (No registered 'DenseToDenseSetOperation' OpKernel for XLA_TPU_JIT devices compatible with node {{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}){{node broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape/has_invalid_dims/DenseToDenseSetOperation}}
	 [[broadcast_weights/assert_broadcastable/is_valid_shape/has_valid_nonscalar_shape]]
	 [[broadcast_weights/assert_broadcastable/is_valid_shape]]
	TPU compilation failed
	 [[tpu_compile_succeeded_assert/_18335038187613310273/_6]]
	 [[tpu_compile_succeeded_assert/_18335038187613310273/_6/_251]]
  (3) Invalid argument: {{function_node __inference_train_function_196061}} Compilation failure: Detected unsupported operations when trying to compile graph broadcast_weights_assert_broa ... [truncated]
```

Please help me to resolve this.

Regards,
Ankur "
43194,tf.keras.applications.ResNet50 does not work properly ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ""Red Hat Enterprise Linux Server"" v7.6
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- CUDA/cuDNN version: 10.1
- GPU model and memory: Tesla v100 32G

Hello everyone,
I have just tried to use pre-trained ResNet50 with tf.keras.applications.ResNet50(**args). The thing is happening for me is that after downloading weights, nothing else happens!! even after a couple of hours it does not go any further!! Any idea? what may be the casue of this? Any solution?

Many thanks for your help in advance.

I use as follow:

ENCODER_BASE = tf.keras.applications.ResNet50(include_top=False,input_shape=(None,None,3), weights='imagenet')

![image](https://user-images.githubusercontent.com/54047164/93038849-62659b80-f689-11ea-8d8e-b634407469a7.png)

"
43193,W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"I got this issue when I run import tensorflow as tf in command line

>>> import tensorflow as tf
2020-09-13 17:53:34.644899: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-13 17:53:34.649089: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

I use win10 and install the CUDA and cuDNN
But I don't understand why this still happened.
![image](https://user-images.githubusercontent.com/68514251/93029699-06e4d080-f5eb-11ea-9948-a0133f0f8603.png)
![image](https://user-images.githubusercontent.com/68514251/93029705-19f7a080-f5eb-11ea-9dc2-3087ff1379fe.png)
"
43192,ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):win10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):binary
- TensorFlow version:2.2.0
- Python version:3.8.5
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:10.1
- GPU model and memory:nvidia940m, 4 gb



**Describe the problem**  I am confident that I've installed Cuda and cudnn properly and added path in the environment variable, but still, I am getting this error

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I am following their step: https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html#tensorflow-object-detection-api-installation
When I tried for ""python object_detection/builders/model_builder_tf2_test.py"" I am facing this error

**Any other info / logs**  (tensorflow1) C:\tensorflow1\models\research>python object_detection/builders/model_builder_tf2_test.py
Traceback (most recent call last):
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/model_builder_tf2_test.py"", line 21, in <module>
    import tensorflow.compat.v1 as tf
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Pranab\Anaconda3\envs\tensorflow1\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43190,Tensorflow hub USE model issue,"This is worse. What documentation they have is also failing? Tensorflow, please help.

import tensorflow as tf
import tensorflow_hub as hub

# Create graph and finalize (finalizing optional but recommended).
g = tf.Graph()
with g.as_default():
  # We will be feeding 1D tensors of text into the graph.
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module(""use_model"")
  embedded_text = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Create session and initialize.
session = tf.Session(graph=g)
session.run(init_op)

result = session.run(embedded_text, feed_dict={text_input: [""Hello world""]})
print(result.shape)



2020-09-13 19:57:18.597868: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found
2020-09-13 19:57:18.605416: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\native_module.py:55: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\saved_model_module.py:44: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\saved_model_module.py:45: The name tf.saved_model.constants.MAIN_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.MAIN_OP_KEY instead.

WARNING:tensorflow:From endpoints.py:33: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\module.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\resolver.py:458: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\native_module.py:92: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\saved_model_lib.py:80: The name tf.saved_model.constants.SAVED_MODEL_FILENAME_PB is deprecated. Please use tf.saved_model.SAVED_MODEL_FILENAME_PB instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\saved_model_lib.py:221: The name tf.saved_model.constants.ASSETS_KEY is deprecated. Please use tf.saved_model.ASSETS_KEY instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\saved_model_lib.py:55: The name tf.saved_model.constants.VARIABLES_DIRECTORY is deprecated. Please use tf.saved_model.VARIABLES_DIRECTORY instead.

WARNING:tensorflow:From C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\saved_model_lib.py:56: The name tf.saved_model.constants.VARIABLES_FILENAME is deprecated. Please use tf.saved_model.VARIABLES_FILENAME instead.

Traceback (most recent call last):
  File ""endpoints.py"", line 34, in <module>
    embed = hub.Module(""use_model"")
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\module.py"", line 144, in __init__
    self._spec = as_module_spec(spec)
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\module.py"", line 33, in as_module_spec
    return load_module_spec(spec)
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\module.py"", line 58, in load_module_spec
    return registry.loader(path)
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\registry.py"", line 42, in __call__
    return impl(*args, **kwargs)
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\native_module.py"", line 115, in __call__
    return _ModuleSpec(saved_model_handler, checkpoint_filename)
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\native_module.py"", line 296, in __init__
    saved_model_handler, _SUPPORTED_COLLECTIONS)
  File ""C:\Users\jay.timbadia\black\lib\site-packages\tensorflow_hub\native_module.py"", line 708, in check_collections_are_supported
    "" as appropriate."" % list(unsupported))
ValueError: Unsupported collections in graph: ['saved_model_main_op']
Use hub.create_module_spec(..., drop_collections=[...]) as appropriate."
43189,Tensorflow 2.3 not detecting gpu (cuda 10.0),"**System information**
- OS Platform and Distribution:Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version:2.3
- Python version:3.6.9
- Installed using:pip
- CUDA/cuDNN version:10.0/ 7.6.5
- GPU model and memory: gtx 1050 ti, 4GB


I'm not able to figure out why tensorflow is not detecting gpu.
As soon as I import tf in cmd it shows this warning. 

_**>>> import tensorflow as tf**_
2020-09-13 19:49:51.205462: W tensorflow/stream_executor/platform/default/dso_lo
ader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcuda
rt.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRAR
Y_PATH: /usr/local/cuda-10.2/lib64
2020-09-13 19:49:51.205488: I tensorflow/stream_executor/cuda/cudart_stub.cc:29]
 Ignore above cudart dlerror if you do not have a GPU set up on your machine.

I tried checking with 
**_>>> tf.test.gpu_device_name()_**
but it gives an empty string ' '

Its not a dependency issue as tf 2 supports cuda 10

And one more weird stuff, when I run **_'nvidia-smi'_** on cmd prompt it shows cuda version as 11 but when I run **_'cat /usr/local/cuda/version.txt'_** it shows version as 10.0.130
"
43187,Adding HDFS connection cache on tensorflow side can improve performance by 20+ times,"This is a issue from TaiJi AI platform in Tencent.

**System information**
- OS Platform and Distribution : Linux version 4.14.105-1-tlinux3-0010
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1（we use）and the latest version also has this problem
- Python version: 3.6
- C++ version: 11
- hadoop version: 2.8

**test conclusion**
After testing, it is found that adding hdfs connection cache on the tensorflow side can improve performance by 20+ times.

All HDFS APIs that rely on [HDFS Connect](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L160) have this performance problem. Here is the test result of [the HDFS stat API](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553). [The HDFS stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) with connection cache takes about 458 microseconds, but [The HDFS stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) without connection cache takes about 12114 microseconds, so it can be concluded that adding hdfs connection cache on the tensorflow side can improve performance by **20+ Times**. For details, please refer to the test results below.

**solution**: [patch-1](https://github.com/tensorflow/tensorflow/pull/43188)

**test code**
```
#include <sys/time.h>
#include <iostream>

int64_t getCurrentTime() {
  struct timeval tv;
  gettimeofday(&tv, NULL);
  return tv.tv_sec * 1000000 + tv.tv_usec;
}

Status HadoopFileSystem::Stat(const string& fname, TransactionToken* token,
                              FileStatistics* stats) {
  int64_t start = getCurrentTime();
  .......................
  int64_t end = getCurrentTime();
  std::cout << ""HadoopFileSystem::Stat use time "" << (end - start) << "" µs"" << std::endl;
  return Status::OK();
}
```

**logs** 
[stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) without connection cache
![image](https://user-images.githubusercontent.com/70072713/93019627-bed5a600-f60a-11ea-8dc6-1c9b83c4d18f.png)


[stat](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L553) with connection cache
![image](https://user-images.githubusercontent.com/70072713/93019596-98b00600-f60a-11ea-8e17-b919a250c386.png)
"
43186,error when trying to modify code to train  on tpu's.,"I am training model on colab. Already raised issue in googlecolab/colabtools .But, no answers.

I am working on kaggle dataset [https://www.kaggle.com/c/severstal-steel-defect-detection/data]. Its a segmentation task. In which label(masks) are not given directly, they gave encoded pixels. we need to transform them back to image. So, Its a multilabel segmentation. for each class of the image from the encoded pixels I converted them to image and saved in four files(four classes) also has some images in which there are no labels for them I made simply np.zeros(). In the custom data generator my aim is to for every image get those mask images stack them and finally get mask as (x,y,4).
below is the code of the custom data generator::
```
class DataGenerator(tf.keras.utils.Sequence):
    def __init__(self,batch_size,dataframe,x_col,y_col,number_classes,dimensions=[256,1600,3],shuffle=True):
        self.batch=batch_size
        self.df=dataframe
        self.x=x_col
        self.y=y_col
        self.classes=number_classes
        self.dim=dimensions
        self.indexes=self.df.index.tolist()
        self.shuffle=shuffle
        self.index_of_indexes=np.arange(len(self.indexes))
        self.on_epoch_end()
        self.n=0
        self.max=self.__len__()
    def __len__(self):
        return int(np.floor(len(self.indexes)/self.batch))
    def on_epoch_end(self):
        if self.shuffle==True:
            np.random.shuffle(self.index_of_indexes)
    def __next__(self):
      if self.n>=self.max:
        self.n=0
      result = self.__getitem__(self.n)
      self.n += 1
      return result
    def __getitem__(self,index):
        temp_index_of_indexes=self.index_of_indexes[index*self.batch:(index+1)*self.batch]
        temp_indexes=[self.indexes[i] for i in temp_index_of_indexes]
        X=np.empty((self.batch,self.dim[0],self.dim[1],self.dim[2]))
        Y=np.empty((self.batch,self.dim[0],self.dim[1],self.classes))
        for i,id_ in enumerate(temp_indexes):
            image_name=str(self.df.loc[id_,self.x])
            classes_list=np.array(self.df.loc[id_,self.y])
            shape=[self.dim[0],self.dim[1]]
            X[i,],Y[i,]=self.get_data(image_name,classes_list,shape)
        return X,Y
    def get_data(self,image_name,classes_list,shape):
        for i,c in enumerate(classes_list):
            if i==0 and c==1:
                file=image_name.split('.')[0]+'.npy'
                path='/content/severstal-steel-defect-detection/temp1/'+file
                channel1=np.load(path)
                channel1=channel1/255.0
            elif i==0 and c==0:
                channel1=np.zeros((shape[0],shape[1]))
            elif i==1 and c==1:
                file=image_name.split('.')[0]+'.npy'
                path='/content/severstal-steel-defect-detection/temp2/'+file
                channel2=np.load(path)
                channel2=channel2/255.0
            elif i==1 and c==0:
                channel2=np.zeros((shape[0],shape[1]))
            elif i==2 and c==1:
                file=image_name.split('.')[0]+'.npy'
                path='/content/severstal-steel-defect-detection/temp3/'+file
                channel3=np.load(path)
                channel3=channel3/255.0
            elif i==2 and c==0:
                channel3=np.zeros((shape[0],shape[1]))
            elif i==3 and c==1:
                file=image_name.split('.')[0]+'.npy'
                path='/content/severstal-steel-defect-detection/temp4/'+file
                channel4=np.load(path)
                channel4=channel4/255.0
            elif i==3 and c==0:
                channel4=np.zeros((shape[0],shape[1]))
        path='/content/severstal-steel-defect-detection/train_images/'+image_name
        image=load_img(path,target_size=(shape[0],shape[1],3))
        image=img_to_array(image)
        image=image/255.0
        mask=np.stack([channel1,channel2,channel3,channel4],axis=-1)
        image=tf.cast(image,dtype=tf.float32)
        mask=tf.cast(mask,dtype=tf.float32)  
        return image,mask
```
I am using unet as my model. Also, when I am using gpu its working fine. But, taking a lot of time. So, I wanted to use TPU's.
So, the datagenerator which I created is working is fine. I also made my model as a function and my loss function amd metric I mentioned them in my tpu_strategy.scope() only. I am not understanding whats my error::
```
batch1=4 * tpu_strategy.num_replicas_in_sync
batch2=2 * tpu_strategy.num_replicas_in_sync


with tpu_strategy.scope():
  training_model=custom_model()
  def soft_dice_loss(y_true,pred):
    y_true=K.flatten(y_true)
    pred=K.flatten(pred)
    intersec=(2*K.sum(y_true*pred))+1e-9
    deno=K.sum(y_true**2)+K.sum(pred**2)+1e-9
    return 1-K.mean(intersec/deno)
  def soft_dice_coeff(y_true,pred):
    y_true=K.flatten(y_true)
    pred=K.flatten(pred)
    intersec=(2*K.sum(y_true*pred))+1e-9
    deno=K.sum(K.abs(y_true))+K.sum(K.abs(pred))+1e-9
    return K.mean(intersec/deno)
  training_model.compile(
      optimizer='Adam',
      loss=soft_dice_loss,
      metrics=[soft_dice_coeff])
```
```
Epoch 1/100
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Iterator.get_next_as_optional()` instead.
---------------------------------------------------------------------------
UnavailableError                          Traceback (most recent call last)
<ipython-input-57-1b0974d37f1f> in <module>()
     13                               dimensions=[256, 1600, 3],
     14                               shuffle=True)
---> 15 training_model.fit(train_generator,epochs=100,steps_per_epoch=train_image_csv.shape[0]//4,validation_data=valid_generator,validation_steps=valid_image_csv.shape[0]//2)

14 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1101               logs = tmp_logs  # No error, now safe to assign to logs.
   1102               end_step = step + data_handler.step_increment
-> 1103               callbacks.on_train_batch_end(end_step, logs)
   1104         epoch_logs = copy.copy(logs)
   1105 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
    438     """"""
    439     if self._should_call_train_batch_hooks:
--> 440       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
    441 
    442   def on_test_batch_begin(self, batch, logs=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    287       self._call_batch_begin_hook(mode, batch, logs)
    288     elif hook == 'end':
--> 289       self._call_batch_end_hook(mode, batch, logs)
    290     else:
    291       raise ValueError('Unrecognized hook: {}'.format(hook))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)
    307       batch_time = time.time() - self._batch_start_time
    308 
--> 309     self._call_batch_hook_helper(hook_name, batch, logs)
    310 
    311     if self._check_timing:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)
    340       hook = getattr(callback, hook_name)
    341       if getattr(callback, '_supports_tf_logs', False):
--> 342         hook(batch, logs)
    343       else:
    344         if numpy_logs is None:  # Only convert once.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
    959 
    960   def on_train_batch_end(self, batch, logs=None):
--> 961     self._batch_update_progbar(batch, logs)
    962 
    963   def on_test_batch_end(self, batch, logs=None):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)
   1014     if self.verbose == 1:
   1015       # Only block async when verbose = 1.
-> 1016       logs = tf_utils.to_numpy_or_python_type(logs)
   1017       self.progbar.update(self.seen, list(logs.items()), finalize=False)
   1018 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)
    535     return t  # Don't turn ragged or sparse tensors to NumPy.
    536 
--> 537   return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    538 
    539 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    633 
    634   return pack_sequence_as(
--> 635       structure[0], [func(*x) for x in entries],
    636       expand_composites=expand_composites)
    637 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)
    531   def _to_single_numpy_or_python_type(t):
    532     if isinstance(t, ops.Tensor):
--> 533       x = t.numpy()
    534       return x.item() if np.ndim(x) == 0 else x
    535     return t  # Don't turn ragged or sparse tensors to NumPy.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)
   1061     """"""
   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1065 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1029       return self._numpy_internal()
   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1032 
   1033   @property

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnavailableError: 9 root error(s) found.
  (0) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1599836582.583992584"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1599836582.583991302"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_24/_190]]
  (1) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1599836582.583992584"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1599836582.583991302"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[Cast_1/_62]]
  (2) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1599836582.583992584"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1599836582.583991302"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[Cast_1/_32]]
  (3) Unavailable: {{function_node __inference_train_function_14623}} failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1599836582.583992584"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1599836582.583991302"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
	 [[IteratorGetNextAsOptional]]
	 [[strided_slice_86/_250]]
  (4) Unavailable: {{function_node __inference_train_function_14623}} failed to con ... [truncated]
```"
43184,Keras on_epoch_end() callback output happens BEFORE completion of epoch output with verbose=1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): modified TF example code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Win10
- TensorFlow version (use command below): 2.3.0

**Describe the current behavior**
when training Model using `Model.fit()` with `verbose=1 `(for progress bar) and `keras.callbacks.Callback()` with `on_epoch_end() `function I observe that callback output happens BEFORE completion of the epoch progress output, like this:
```
Epoch 1/2
 63/125 [==============>...............] - ETA: 0s - loss: 43.2549 - mean_absolute_error: 4.6003
CALLBACK MESSAGE ON END EPOCH 0
125/125 [==============================] - 0s 794us/step - loss: 37.6690 - mean_absolute_error: 4.6575
```

**Describe the expected behavior**
callback output should happen AFTER completion of the epoch progress output:
```
Epoch 1/2
125/125 [==============================] - 0s 900us/step - loss: 36.9897 - mean_absolute_error: 4.8264
CALLBACK MESSAGE ON END EPOCH 0
```

**Standalone code to reproduce the issue**
[see gist here](https://colab.research.google.com/gist/poedator/05d7c2f1ac4cc5de65706896113b8e4d/custom_callback.ipynb)"
43183,Learning Rate finder for optimizers,"**System information**
- TensorFlow version: 2.3
- Are you willing to contribute it: Yes


**Describe the feature and the current behavior/state.**
--
I am currently reading the book ""Deep Learning"" by Ian Goodfellow, Yoshua Bengio and Aaron Courville. In this book they propose a formula for finding the best learning rate  under certain constraints. The formula is: 
 `e* = g.T*g / g.T*H*g`
where g is the gradient and H is the Hessian matrix at X(0).
Out of interest i implemented a short script for a random loss function with two variables x,y and a random data point X(0) = {x:1,y:1}. This gives me in this arbitrary example an optimal learning rate of `0.171232876712329`.
[OptimalLR.pdf](https://github.com/tensorflow/tensorflow/files/5213736/OptimalLR.pdf)

Unfortunately i do not have the in depth tensorflow knowledge to play with lets say the Adam Optimizer to try it out there. One would need to get the loss function, take a data point (i assume mini batches work as well) and apply the formula to it and see if it evaluates a valid learning rate. Then run test with the default 0.01 learning rate versus the calculated learning rate and see if the calculated one performs significantly better which would justify the extra calculation. 

I would apprechiate feedback on whether that is possible or whether i am missing something obvious as a reason why that is not possible. For example is this calculation even feasible for deep neural networks?




**Who will benefit with this feature?**
--

If this algorithm is possible one could replace the fixed default learning rate of 0.01 to a calculated one which potentially improves the performance of neural networks regarding the loss function.
"
43182,How to check the tensor value(this is different from issue 27519),"I want to check the tensor value in a specific layer and I find that tensor.numpy represents the value in eager mode. However, there is tf.placeholder in my code and tf.placeholder is not compatible with eager mode so I have to disable eager mode. Then how to check the value in this case? 

In brief, I want to see a data(eg. 1x32x32 image) going through the neural network and check all activation between every layer. It doesn't need to be training mode, inference mode is fine. Much appreciated!"
43181,cuda,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
43179,Issue with Training CenterNet Resnet50 V2 Keypoints 512x512 ,"Hello i am trying to train  CenterNet Resnet50 V2 Keypoints 512x512  with hand dataset 'https://www.robots.ox.ac.uk/~vgg/data/hands/downloads/hand_dataset.tar.gz'  but facing issue with label_map.pbxt and don't know how to resolve this issue. 

**Errors i got during training:**  
File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py"", line 991, in _build_center_net_model
    kp_params = keypoint_proto_to_params(task, keypoint_map_dict)
  File ""/usr/local/lib/python3.6/dist-packages/object_detection/builders/model_builder.py"", line 809, in keypoint_proto_to_params
    label_map_item = keypoint_map_dict[kp_config.keypoint_class_name]
KeyError: '/m/01g317'"
43177,custom BLAS,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No):Yes (WOULD LOVE IT!)



**Describe the feature and the current behavior/state.**
After a good amount of profiling on some of the codes [here](https://github.com/dragen1860/TensorFlow-2.x-Tutorials). I found out that TensorFlow uses 'mkldnn_sgemm' and Tensorflow with ""MKL-DNN support"" (--config=mkl during build time) uses 'mkldnn::impl::cpu::avx_gemm_f32::sgemm_nocopy_driver' for **MATRIX MULTIPLICATION**. What I wanted was that somehow every time Tensorflow does matrix multiplication we make TensorFlow do these matmul operations with user defined functions. Maybe with the help of .so file?  

**Will this change the current api? How?**
NO

**Who will benefit with this feature?**
researchers and engineers who are working on TensorFlow from a systems point of view 

**Any Other info.**
"
43175,While training fasterRCNN with Tensoflow Object Detection API 2.(using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.),"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.10
- CUDA/cuDNN version: 10.1.243 / 7.6.5
- GPU model and memory: 1080ti 11GB

Command used to run training:
    python3 model_main_tf2.py     --pipeline_config_path=training/pipeline.config     --model_dir=training     --num_train_steps=10000     --sample_1_of_n_eval_examples=1     --alsologtostderr

Traceback:
2020-09-12 14:20:06.611317: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-12 14:20:07.902280: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-12 14:20:07.932835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:07.933417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.721GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-12 14:20:07.933439: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-12 14:20:07.934581: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-12 14:20:07.935660: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-12 14:20:07.935852: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-12 14:20:07.937035: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-12 14:20:07.937660: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-12 14:20:07.940093: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-12 14:20:07.940227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:07.940860: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:07.941301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-12 14:20:07.941543: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-12 14:20:07.945801: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 4200000000 Hz
2020-09-12 14:20:07.946049: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56548f8ff7c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-12 14:20:07.946060: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-12 14:20:08.017594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:08.028832: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x565490ae2c00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-12 14:20:08.028848: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-09-12 14:20:08.029003: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:08.029293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.721GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-12 14:20:08.029314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-12 14:20:08.029341: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-12 14:20:08.029354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-12 14:20:08.029367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-12 14:20:08.029379: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-12 14:20:08.029392: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-12 14:20:08.029405: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-12 14:20:08.029445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:08.029749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:08.030020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-12 14:20:08.030039: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-12 14:20:08.314944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-12 14:20:08.314974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-12 14:20:08.314995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-12 14:20:08.315166: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:08.315535: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-12 14:20:08.315849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9866 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0912 14:20:08.317715 140151783593792 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: 10000
I0912 14:20:08.320816 140151783593792 config_util.py:552] Maybe overwriting train_steps: 10000
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0912 14:20:08.320905 140151783593792 config_util.py:552] Maybe overwriting use_bfloat16: False
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0912 14:20:08.454587 140151783593792 dataset_builder.py:83] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
W0912 14:20:08.457228 140151783593792 deprecation.py:323] From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:100: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
WARNING:tensorflow:From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0912 14:20:08.479154 140151783593792 deprecation.py:323] From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py:175: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:AutoGraph could not transform <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f775eae7940>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0912 14:20:08.549343 140151783593792 ag_logging.py:146] AutoGraph could not transform <bound method TfExampleDecoder.decode of <object_detection.data_decoders.tf_example_decoder.TfExampleDecoder object at 0x7f775eae7940>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Constant'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:AutoGraph could not transform <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f775eab3400> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
W0912 14:20:08.632742 140151783593792 ag_logging.py:146] AutoGraph could not transform <function train_input.<locals>.transform_and_pad_input_data_fn at 0x7f775eab3400> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING:tensorflow:From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W0912 14:20:08.635260 140151783593792 deprecation.py:323] From /home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py:201: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
Traceback (most recent call last):
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 584, in converted_call
    converted_f = conversion.convert(target_entity, program_ctx)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 119, in convert
    entity, program_ctx.options, program_ctx, custom_vars)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 412, in transform_function
    extra_locals)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 373, in _transformed_factory
    nodes, ctx = self._transform_function(fn, user_context)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 339, in _transform_function
    node = self.transform_ast(node, context)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/conversion.py"", line 70, in transform_ast
    node = activity.resolve(node, ctx, None)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 705, in resolve
    return ActivityAnalyzer(context, parent_scope).visit(node)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/transformer.py"", line 445, in visit
    result = super(Base, self).visit(node)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/ast.py"", line 253, in visit
    return visitor(node)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 575, in visit_FunctionDef
    node = self._visit_arg_annotations(node)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 551, in _visit_arg_annotations
    node = self._visit_arg_declarations(node)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/pyct/static_analysis/activity.py"", line 556, in _visit_arg_declarations
    node.args.posonlyargs = self._visit_node_list(node.args.posonlyargs)
AttributeError: 'arguments' object has no attribute 'posonlyargs'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""model_main_tf2.py"", line 110, in main
    record_summaries=FLAGS.record_summaries)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/model_lib_v2.py"", line 526, in train_loop
    train_dataset_fn)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 1128, in experimental_distribute_datasets_from_function
    dataset_fn, options)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/mirrored_strategy.py"", line 503, in _experimental_distribute_datasets_from_function
    self._container_strategy())
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py"", line 136, in get_distributed_datasets_from_function
    strategy)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py"", line 1183, in __init__
    dataset_fn))
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/distribute/input_lib.py"", line 1767, in _create_datasets_per_worker_with_input_context
    dataset = dataset_fn(ctx)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/model_lib_v2.py"", line 521, in train_dataset_fn
    input_context=input_context)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py"", line 838, in train_input
    reduce_to_frame_fn=reduce_to_frame_fn)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py"", line 196, in build
    batch_size, input_reader_config)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/builders/dataset_builder.py"", line 175, in dataset_map_fn
    fn_to_map, num_parallel_calls=num_parallel_calls)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 2564, in map_with_legacy_function
    use_legacy_function=True))
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 4084, in __init__
    use_legacy_function=use_legacy_function)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3339, in __init__
    self._function.add_to_graph(ops.get_default_graph())
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 544, in add_to_graph
    self._create_definition_if_needed()
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 376, in _create_definition_if_needed
    self._create_definition_if_needed_impl()
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 407, in _create_definition_if_needed_impl
    capture_resource_var_by_value=self._capture_resource_var_by_value)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/function.py"", line 969, in func_graph_from_py_func
    outputs = func(*func_graph.inputs)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3331, in wrapper_fn
    ret = _wrapper_helper(*args)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 3299, in _wrapper_helper
    ret = autograph.tf_convert(func, ag_ctx)(*nested_args)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 255, in wrapper
    return converted_call(f, args, kwargs, options=options)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 591, in converted_call
    return _fall_back_unconverted(f, args, kwargs, options, e)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 398, in _fall_back_unconverted
    return _call_unconverted(f, args, kwargs, options)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py"", line 339, in _call_unconverted
    return f(*args, **kwargs)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py"", line 819, in transform_and_pad_input_data_fn
    tensor_dict=transform_data_fn(tensor_dict),
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py"", line 249, in transform_input_data
    out_tensor_dict = data_augmentation_fn(out_tensor_dict)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/inputs.py"", line 577, in augment_input_data
    include_dense_pose=include_dense_pose))
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/core/preprocessor.py"", line 4591, in preprocess
    results = func(*args, **params)
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/object_detection/core/preprocessor.py"", line 4093, in random_square_crop_by_scale
    if ymin == 0:
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 877, in __bool__
    self._disallow_bool_casting()
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 490, in _disallow_bool_casting
    self._disallow_in_graph_mode(""using a `tf.Tensor` as a Python `bool`"")
  File ""/home/convsys/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 479, in _disallow_in_graph_mode
    "" this function with @tf.function."".format(task))
tensorflow.python.framework.errors_impl.OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.

Config File
`

model {
  faster_rcnn {
    num_classes: 33
    image_resizer {
      keep_aspect_ratio_resizer {
        min_dimension: 800
        max_dimension: 1333
        pad_to_max_dimension: true
      }
    }
    feature_extractor {
      type: 'faster_rcnn_resnet101_keras'
    }
    first_stage_anchor_generator {
      grid_anchor_generator {
        scales: [0.25, 0.5, 1.0, 2.0]
        aspect_ratios: [0.5, 1.0, 2.0]
        height_stride: 16
        width_stride: 16
      }
    }
    first_stage_box_predictor_conv_hyperparams {
      op: CONV
      regularizer {
        l2_regularizer {
          weight: 0.0
        }
      }
      initializer {
        truncated_normal_initializer {
          stddev: 0.01
        }
      }
    }
    first_stage_nms_score_threshold: 0.0
    first_stage_nms_iou_threshold: 0.7
    first_stage_max_proposals: 300
    first_stage_localization_loss_weight: 2.0
    first_stage_objectness_loss_weight: 1.0
    initial_crop_size: 14
    maxpool_kernel_size: 2
    maxpool_stride: 2
    second_stage_box_predictor {
      mask_rcnn_box_predictor {
        use_dropout: false
        dropout_keep_probability: 1.0
        fc_hyperparams {
          op: FC
          regularizer {
            l2_regularizer {
              weight: 0.0
            }
          }
          initializer {
            variance_scaling_initializer {
              factor: 1.0
              uniform: true
              mode: FAN_AVG
            }
          }
        }
      }
    }
    second_stage_post_processing {
      batch_non_max_suppression {
        score_threshold: 0.0
        iou_threshold: 0.6
        max_detections_per_class: 100
        max_total_detections: 100
      }
      score_converter: SOFTMAX
    }
    second_stage_localization_loss_weight: 2.0
    second_stage_classification_loss_weight: 1.0
  }
}

train_config: {
  batch_size: 4
  num_steps: 200000
  optimizer {
    momentum_optimizer: {
      learning_rate: {
        cosine_decay_learning_rate {
          learning_rate_base: 0.01
          total_steps: 200000
          warmup_learning_rate: 0.0
          warmup_steps: 5000
        }
      }
      momentum_optimizer_value: 0.9
    }
    use_moving_average: false
  }
  gradient_clipping_by_norm: 10.0
  fine_tune_checkpoint_version: V2
  fine_tune_checkpoint: ""/media/convsys/c924758e-a96b-40c5-86de-4d47f53b16db/models-master/research/object_detection/faster_rcnn_resnet101_v1_800x1333_coco17_gpu-8/checkpoint/ckpt-0""
  fine_tune_checkpoint_type: ""classification""
  data_augmentation_options {
    random_horizontal_flip {
    }
  }

  data_augmentation_options {
    random_adjust_hue {
    }
  }

  data_augmentation_options {
    random_adjust_contrast {
    }
  }

  data_augmentation_options {
    random_adjust_saturation {
    }
  }

  data_augmentation_options {
     random_square_crop_by_scale {
      scale_min: 0.6
      scale_max: 1.3
    }
  }
}

train_input_reader: {
  label_map_path: ""training/fashion.pbtxt""
  tf_record_input_reader {
    input_path: ""training/train.record""
  }
}

eval_config: {
  metrics_set: ""coco_detection_metrics""
  use_moving_averages: false
  batch_size: 1;
}

eval_input_reader: {
  label_map_path: ""training/fashion.pbtxt""
  shuffle: false
  num_epochs: 1
  tf_record_input_reader {
    input_path: ""training/test.record""
  }
}`
"
43174,NotFoundError:  No algorithm worked! when using Conv2D,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux fully updated
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Installed from pacman
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: Cuda is 11.0, cuDNN is 8.0.2
- GPU model and memory: RTX 2070 Super


**Describe the current behavior**

I am running the following code: https://github.com/davidADSP/GDL_code/blob/tensorflow_2/02_03_deep_learning_conv_neural_network.ipynb

entry 11, when it does the fit, it fails. It returns:

NotFoundError:  No algorithm worked!
	 [[node functional_1/conv2d/Conv2D (defined at <ipython-input-7-10b06c61fca5>:1) ]] [Op:__inference_train_function_2021]

Function call stack:
train_function

This used to work ok previously, 1-2 months ago. I was just testing some other conv2d today, and it failed, so I went to test this example which I know it worked, and it also fails

**Describe the expected behavior**

The code should work

**Standalone code to reproduce the issue**
See the previous link. It's a github jupyter code

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-7-10b06c61fca5> in <module>
----> 1 model.fit(x_train
      2           , y_train
      3           , batch_size=32
      4           , epochs=10
      5           , shuffle=True

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    838         # Lifting succeeded, so variables are initialized and we can run the
    839         # stateless function.
--> 840         return self._stateless_fn(*args, **kwds)
    841     else:
    842       canon_args, canon_kwds = \

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   2827     with self._lock:
   2828       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2829     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2830 
   2831   @property

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs, cancellation_manager)
   1841       `args` and `kwargs`.
   1842     """"""
-> 1843     return self._call_flat(
   1844         [t for t in nest.flatten((args, kwargs), expand_composites=True)
   1845          if isinstance(t, (ops.Tensor,

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1921         and executing_eagerly):
   1922       # No tape is watching; skip to running the function.
-> 1923       return self._build_call_outputs(self._inference_function.call(
   1924           ctx, args, cancellation_manager=cancellation_manager))
   1925     forward_backward = self._select_forward_and_backward_functions(

/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    543       with _InterpolateFunctionError(self):
    544         if cancellation_manager is None:
--> 545           outputs = execute.execute(
    546               str(self.signature.name),
    547               num_outputs=self._num_outputs,

/usr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

NotFoundError:  No algorithm worked!
	 [[node functional_1/conv2d/Conv2D (defined at <ipython-input-7-10b06c61fca5>:1) ]] [Op:__inference_train_function_2021]

Function call stack:
train_function
"
43173,"subclass of tf.keras.Model throws ""NotImplementedError"" when fit() with custom data","TF 2.3.0 tested on Windows and in Colab.

There is an error that comes out only when certain type of model (subclass of tf.keras.Model) is combined with certain custom dataloaders (Python generator and subclass of tf.keras.utils.Sequence). Yet same loaders work OK with simple Functional Keras model. 

the error is ""**NotImplementedError: When subclassing the `Model` class, you should implement a `call` method**""

See Colab reproduction [here.](https://colab.research.google.com/drive/19hbGTk-RAjdHiUfM-cgDPMNTyu5FRIQf?usp=sharing)"
43172,Random NaN loss when using float16 dtype and batch size of 1,"**System information**
- OS Platform and Distribution: Google Colab
- TensorFlow version (use command below): tf-nightly (2.4.0-dev20200911)

**Reproducing code**
see [https://colab.research.google.com/drive/1miE61h7rlSJL0xIG4yNdVx05lVxT-oKb?usp=sharing](https://colab.research.google.com/drive/1miE61h7rlSJL0xIG4yNdVx05lVxT-oKb?usp=sharing)


````
import math
import numpy as np
import tensorflow as tf

print(tf.__version__)

x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

loss_function = tf.keras.losses.BinaryCrossentropy()

for i in range(2000):
    if i % 100 == 0:
        print(""Iteration {}"".format(i))

    model = tf.keras.Sequential([
        tf.keras.layers.Dense(units=1, dtype=tf.float16)])

    model.compile(optimizer='sgd', loss=loss_function)
    model.fit(x=x, y=y, epochs=1, batch_size=1, verbose=0)

    loss_result = loss_function(y, model(x))

    if math.isnan(loss_result):
        raise RuntimeError('NAN Error in iteration {}'.format(i))
````

**Behavior description**

Seemingly non-deterministic occurence of a NaN result when calculating loss of a very simple Dense Model. The NaN loss seems to happen randomly and can occur on the 60th or 600th iteration. In the supplied Google colab code it happened in the 343rd iteration. The bug only seems to occur using a dtype of float16 and batch_size of 1. Debugging the error lead me to see that the models producing a NaN loss seem to have been initialized with a NaN bias and kernel, though I couldn't get to the bottom of why.

**Background**

I already opened this issue before in the following ticket: [https://github.com/tensorflow/tensorflow/issues/38457](https://github.com/tensorflow/tensorflow/issues/38457)

The assigned Tensorflower closed the issue after supplying a hotfix, with the note that I shall reopen the issue if I am not satisfied with the solution. As the supplied solution is only a hotfix, though does not solve the underlying issue even in the current tf nightly (see the Google colab above in which I tested the current nightly) am I not satisfied with the solution, particularly since the issue is very easily reproducible by setting the dtype to float16 and batch_size to 1. The ticket was closed with the request to reopen the issue, if not satisfied, which I understood as the request to create a new issue as I am obviously no TF contributor and can't reopen issues..."
43171,OpenCL,I understand TensorFlow only supports CUDA. What would need to be done to add in OpenCL support?
43170,Segfault with tf.linalg.cholesky,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Mint 20
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary (PyPI)
- TensorFlow version (use command below): 2.4.0-dev20200911
- Python version: 3.7.6 (Anaconda)
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: CUDA 11.0, cuDNN 8.0.3.33 (NVIDIA Ubuntu 20.04 repo)
- GPU model and memory: GeForce GTX 1650 3.82GiB

**Describe the current behavior**
Calling `tf.linalg.cholesky` results in Python aborting (SIGABRT). 

**Describe the expected behavior**
The Cholesky decomposition succeeds.

**Standalone code to reproduce the issue**

```Python
python -c ""import tensorflow as tf; tf.linalg.cholesky(tf.eye(6))""
```

**Other info / logs**
Log attached: [tf_error_log.txt](https://github.com/tensorflow/tensorflow/files/5212288/tf_error_log.txt)

"
43169,DLL load failed: A dynamic link library (DLL) initialization routine failed.,"**System information**
I am using Tensorflow on Windows 10 2004
Tensorflow installed from PIP (I don't know if that's what you are asking for, sorry)
Tensorflow version is 1.15
Python version is 3.7 (64-bit)

**Describe the problem**
It says _""DLL load failed: A dynamic link library (DLL) initialization routine failed.""_

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I spent many hours installing necessary packages for [this repository](https://github.com/CorentinJ/Real-Time-Voice-Cloning), plus any extra packages that it asked for.

**Any other info / logs**
> Traceback (most recent call last):
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""E:\Python\lib\imp.py"", line 242, in load_module
>     return load_dynamic(name, filename, file)
>   File ""E:\Python\lib\imp.py"", line 342, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
> 
> During handling of the above exception, another exception occurred:
> 
> Traceback (most recent call last):
>   File ""E:\a\b\Real-Time-Voice-Cloning-master\demo_cli.py"", line 4, in <module>
>     from synthesizer.inference import Synthesizer
>   File ""E:\a\b\Real-Time-Voice-Cloning-master\synthesizer\inference.py"", line 1, in <module>
>     from synthesizer.tacotron2 import Tacotron2
>   File ""E:\a\b\Real-Time-Voice-Cloning-master\synthesizer\tacotron2.py"", line 3, in <module>
>     from synthesizer.models import create_model
>   File ""E:\a\b\Real-Time-Voice-Cloning-master\synthesizer\models\__init__.py"", line 1, in <module>
>     from .tacotron import Tacotron
>   File ""E:\a\b\Real-Time-Voice-Cloning-master\synthesizer\models\tacotron.py"", line 1, in <module>
>     import tensorflow as tf
>   File ""E:\Python\lib\site-packages\tensorflow\__init__.py"", line 99, in <module>
>     from tensorflow_core import *
>   File ""E:\Python\lib\site-packages\tensorflow_core\__init__.py"", line 28, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
>   File ""E:\Python\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
>     module = self._load()
>   File ""E:\Python\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
>     module = _importlib.import_module(self.__name__)
>   File ""E:\Python\lib\importlib\__init__.py"", line 127, in import_module
>     return _bootstrap._gcd_import(name[level:], package, level)
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
>     from tensorflow.python import pywrap_tensorflow
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
>     raise ImportError(msg)
> ImportError: Traceback (most recent call last):
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
>     from tensorflow.python.pywrap_tensorflow_internal import *
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
>     _pywrap_tensorflow_internal = swig_import_helper()
>   File ""E:\Python\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
>     _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
>   File ""E:\Python\lib\imp.py"", line 242, in load_module
>     return load_dynamic(name, filename, file)
>   File ""E:\Python\lib\imp.py"", line 342, in load_dynamic
>     return _load(spec)
> ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.
> 
> 
> Failed to load the native TensorFlow runtime.
> 
> See https://www.tensorflow.org/install/errors
> 
> for some common reasons and solutions.  Include the entire stack trace
> above this error message when asking for help."
43168,Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary: `libtensorflow_jni-cpu-windows-x86_64-1.14.0/tensorflow_jni.dll`
- TensorFlow version: 1.14.0

**Describe the problem**
2020-09-12 01:46:00.374893: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Model load took 21ms, TensorFlow version: 1.14.0
Successfully loaded model from the input stream

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Go to: https://www.tensorflow.org/install/lang_java and download libtensorflow.jar and Java Native Interface (JNI) file, save it to project root;
2. Load tensorflow through `System.load (""./tensorflow_jni.dll"")`
"
43165,SyncBatchNormalization has NaN losses with channels-first format,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04 running in Docker container (host is 18.04)
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.0 and 2.3.0
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.29.1-1.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: Various, e.g. Nvidia TITAN X

**Describe the current behavior**
When using the experimental SyncBatchNormalization layer with the channels-first (NCHW or batch-channel-height-width) format, the output of the layer is NaN.

**Describe the expected behavior**
The output of the layer should be valid, as occurs with the default channels-last format, or with regular BatchNormalization.

**Standalone code to reproduce the issue**
This [Colab notebook](https://colab.research.google.com/drive/1zeLVlrWwohsmGJnEOQaFsb_gzH6xOFo1?usp=sharing) contains several models that demonstrate the issue. A Keras model with NCHW format, and a Keras model with SyncBatchNorm, both train correctly. However, a Keras model with _both_ NCHW format and SyncBatchNorm immediately fails with NaN loss. Similarly, an Estimator model with NCHW and SyncBatchNorm fails with NaN loss.

**Other info / logs**
I think the input tensor size seems to be related to the error. I tried a smaller tensor (10x10x10) before and it didn't cause the issue. And a medium-size tensor (I think 30x30x30) trained for a few steps and then encountered this issue. I'd have to double-check on the current code, but I think there's something there.

In the example, the loss is NaN. I believe that the output of the layer is already NaN (I'm not sure if every single value is NaN or just some of the values). I had some tests where I printed out the sum of the layer outputs etc., and if I recall correctly, immediately after the first SyncBatchNorm layer they were already NaN. This happened on the second and later iterations, but not the first iteration, so I suspect there may be some issue with backpropagation rather than with the forward step. For example, hypothetically if it happened that backpropagation in the first step caused the moving variance to go to 0 (or infinity), I think we would see behavior like this.

You should be able to replicate all these tests fairly easily with some simple print functions etc., but if you need any more information please let me know and I can provide it.

The loss function doesn't seem to matter - I can use various loss functions and still trigger the NaN - but the Conv2D at the beginning may matter; it seems like it works fine if I remove that layer or if I change it to channels-last.

If I run on Colab without a GPU, I get the following error:

```
InvalidArgumentError:  Conv2DCustomBackpropFilterOp only supports NHWC.
	 [[node gradient_tape/sequential/conv2d/Conv2D/Conv2DBackpropFilter (defined at <ipython-input-3-11fc99b50ff4>:18) ]] [Op:__inference_train_function_654]
```

I wonder if this might be related to the problem?"
43164,Model produces three different outcomes for the same input depending on how I measure it,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.7

**Describe the current behavior**
When I take the loss from the history object or the result from the evaluate function on the training data or from a numpy calculation of mean squared error I receive different results.
This is an output of my code below:
```
4.520166488224531 
4.409448146820068 
4.139582633972168
```

**Describe the expected behavior**
I would receive the same results for all different evaluation calculations.
Like:
```
4.520166488224531 
4.520166488224531 
4.520166488224531 
```
**Standalone code to reproduce the issue**

You can find a colab notebook with the issue here: 
https://colab.research.google.com/gist/ichitaka/c97ce36d57184ba154c8840802a8e01d

```
from tensorflow.keras.layers import Input, Dense, Activation
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import RMSprop
import numpy as np
x = np.random.normal(1, 2, 10)
y = np.random.normal(1, 2, 10)

model = Sequential()
model.add(Input(1))
model.add(Dense(10))
model.add(Activation(""sigmoid""))
model.add(Dense(1))
model.summary()


rmsprop = RMSprop(lr=0.01)
model.compile(loss='mean_squared_error', optimizer=rmsprop)
hist = model.fit(x=x,y=y)
_ = model.evaluate(x, y, verbose=0)
pred = model.predict(x)
print(np.mean((y-pred)**2), '\n', hist.history['loss'][0], '\n', _)
```"
43162,custom xception input size broadcasting incorrectly,"## System information:
Red Hat Enterprise Linux Server release 7.7 (Maipo)
Tensorflow: tensorflow gpu 2.1
GPU: Nvidia V100
Python: 3.6.10
GCC: 7.3.0
CUDA: 10.1 (I think)
Running through slurm

## The Error log:
```
ValueError: could not broadcast input array from shape (850,550,3) into shape (850,550,3,3)
```

## model.summary()
This model is the stock xception with a custom top (global max, dense layer, and softmax) used for image classification. 
```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_2 (InputLayer)            [(None, 850, 550, 3) 0
__________________________________________________________________________________________________
block1_conv1 (Conv2D)           (None, 424, 274, 32) 864         input_2[0][0]
__________________________________________________________________________________________________
block1_conv1_bn (BatchNormaliza (None, 424, 274, 32) 128         block1_conv1[0][0]
__________________________________________________________________________________________________
block1_conv1_act (Activation)   (None, 424, 274, 32) 0           block1_conv1_bn[0][0]

...
```

## Relevant Code

A runnable colab gist can be found [here](https://colab.research.google.com/gist/auchtopus/3fdb4fdecd4a4754d10951e942ecddd1/issue_43162.ipynb)
```
!git clone https://github.com/auchtopus/flowering_toy_dataset
import numpy as np
import pandas as pd
import tensorflow
from tensorflow import keras
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.models import load_model
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.applications import Xception
from tensorflow.keras.preprocessing import image
from tensorflow.keras.layers import Flatten, Dense, Input, GlobalAvgPool2D
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
import matplotlib.pyplot as plt
import random
import os
import time
from datetime import datetime
from IPython.display import SVG
from PIL import ImageFile
ImageFile.LOAD_TRUNCATED_IMAGES = True


# verify gpus
print(tensorflow.config.list_physical_devices('GPU'))


FAST_RUN = False
IMAGE_HEIGHT=850
IMAGE_WIDTH=550
IMAGE_CHANNELS=3
IMAGE_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS)

input_tensor_def = Input(shape=(IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANNELS))  # unused for now


NAME = f""{datetime.today().strftime('%Y-%m-%d')}-xception_850_flowering_keras_xception""


model_core = Xception(weights = None, include_top = False, input_shape = IMAGE_SIZE)

model_head = model_core.output
model_head = GlobalAvgPool2D()(model_head)
model_head = Flatten()(model_head)
model_head = Dense(512, activation = 'relu')(model_head)
model_head = Dense(256, activation = 'relu')(model_head)
model_head = Dense(2, activation = 'softmax')(model_head)

model = Model(inputs = model_core.input, outputs = model_head)

model.compile(Adam(lr=.00005), loss='categorical_crossentropy', metrics=['accuracy'])

print(model.summary())

earlystop = EarlyStopping(patience=20)


filepath=f""/content/model/model.hdf5""
if not os.path.isdir(f""/content/model""):
  os.makedirs(f""/content/model"")
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')

callbacks = [earlystop, checkpoint]


# hard coded for now, replace later
nb_train_samples = 20
# nb_validation_samples = 
batch_size=4

train_path = '/content/flowering_toy_dataset/images'
# valid_path = '/content/NEVP_phenology_unscored_20191206/images'
train_datagen = ImageDataGenerator(
    rotation_range=15,
    rescale=1./255,
    shear_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True,
    width_shift_range=0.1,
    height_shift_range=0.1
)
train_generator = train_datagen.flow_from_directory(
    train_path, target_size=IMAGE_SIZE, class_mode='categorical', classes=['flowering', 'not_flowering'], batch_size=batch_size)

# validation_datagen = ImageDataGenerator(rescale=1./255)
# validation_generator = validation_datagen.flow_from_directory(
#     valid_path, target_size=IMAGE_SIZE, class_mode='categorical', classes=['Flowering', 'Not_Flowering'], batch_size=batch_size)

# print(nb_validation_samples//batch_size)

epochs=3 if FAST_RUN else 500
history = model.fit_generator(
    train_generator,
    epochs=epochs,
    steps_per_epoch=nb_train_samples//batch_size,
    callbacks=callbacks
)

```
## My thoughts
I feel like normally with broadcasting errors, it's generally related to the sizes of the images, or it fails to broadcast from a 3-dim input tensor to the 4-dim (batch, height ,width,channels) tensor. However, here, it just seems like the code has forgotten about the existence of the batch dimension and confused height for batches, width for height, and channels for both width and channels. I have double checked my code, but to my (admittedly very limited) knowledge, everything looks okay. "
43157,`validation_split` support for RaggedTensors,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No):No



**Describe the feature and the current behavior/state.**
Call to tf.keras.Model.fit with validation_split=0.2 produces:

ValueError: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor'>]

**Will this change the current api? How?**
No.
**Who will benefit with this feature?**
Users of RaggedTensor
**Any Other info.**
This only occurs when validation_split parameter is being used."
43154,tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018DDF556708> and will run it as-is.,"WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018DDF556708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018DDF556708> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: 'arguments' object has no attribute 'posonlyargs'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert"
43152,ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`,"ImportError: Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-0f2507b3ca9f> in <module>
     16 
     17 
---> 18 from keras.models import Sequential
     19 from keras.layers import Dense, Dropout, BatchNormalization, Activation
     20 from keras import regularizers

C:\ProgramData\Anaconda3\lib\site-packages\keras\__init__.py in <module>
      4 except ImportError:
      5     raise ImportError(
----> 6         'Keras requires TensorFlow 2.2 or higher. '
      7         'Install TensorFlow via `pip install tensorflow`')
      8 

ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`

"
43151,Using Hidden layers output in the loss function ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): *
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: *
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.3 / tf-nightly
- Python version: 3.6-3.7-3.8
- Bazel version (if compiling from source): *
- GCC/Compiler version (if compiling from source): *
- CUDA/cuDNN version: *
- GPU model and memory: *

**Describe the current behavior**

Hidden layers output of the model cannot be accessed outside of the function building code.
 
**Describe the expected behavior**

To be able to use hidden layers output in my loss function.

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1laEpykHax2QbAV4SB-8Srwfh9SvmAt4B?usp=sharing

**Other info / logs** 

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: dense_6/Relu:0

During handling of the above exception, another exception occurred:

_SymbolicException                        Traceback (most recent call last)
9 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     72       raise core._SymbolicException(
     73           ""Inputs to eager execution function cannot be Keras symbolic ""
---> 74           ""tensors, but found {}"".format(keras_symbolic_tensors))
     75     raise e
     76   # pylint: enable=protected-access

**_SymbolicException:** Inputs to eager execution function cannot be Keras symbolic tensors, but found [<tf.Tensor 'dense_6/Relu:0' shape=(None, 128) dtype=float32>]

When i used `tf.config.run_functions_eagerly(True)`

I got my loss being equal to  **0.0000e+00**
"
43150,"Make failed with C++ API used like external library. Error: static assertion failed: std::string is no longer a scalar type, use tensorflow::tstring","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 7.5.0
- GPU model and memory:
 
I'm trying to use TensorFlow c++ like external library with cmake to load a model trained in python, I use the example of label image https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image. I did it with past version of tensorflow v1.14.0, bazel 0.24.1, protobuf 3.7.x, Eigen 3.3.7, but when I updated the version of Tensorflow 2.3.0 with its dependencies (bazel 3.1.0, protobuf 3.9.2, Eigen 3.3.x (according to workspace.bzl)) there is the next error:

`[ 50%] Building CXX object CMakeFiles/main.dir/main.cc.o
In file included from /usr/local/include/tensorflow/cc/framework/ops.h:21:0,
                 from /usr/local/include/tensorflow/cc/ops/const_op.h:19,
                 from /home/nae/ML/LoadModel/src/main.cc:41:
/usr/local/include/tensorflow/core/framework/tensor.h: In instantiation of ‘typename tensorflow::TTypes<T>::Scalar tensorflow::Tensor::scalar() [with T = std::__cxx11::basic_string<char>; typename tensorflow::TTypes<T>::Scalar = Eigen::TensorMap<Eigen::TensorFixedSize<std::__cxx11::basic_string<char>, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer>]’:
/home/nae/ML/LoadModel/src/main.cc:109:26:   required from here
/usr/local/include/tensorflow/core/framework/tensor.h:878:3: error: static assertion failed: std::string is no longer a scalar type, use tensorflow::tstring
   static_assert(
`


**Any other info / logs**
CMakeLists.txt:

cmake_minimum_required(VERSION 3.10)


project(LoadModel)
find_package(Tensorflow REQUIRED)
find_package(Protobuf REQUIRED)
find_package(Eigen3 REQUIRED)

set(INCLUDE_DIRS ${Tensorflow_INCLUDE_DIRS}
		 ${EIGEN3_INCLUDE_DIRS}
                 ${Protobuf_INCLUDE_DIRS}
)
set(MODULE_LIBS ${Tensorflow_LIBRARIES}
		${EIGEN3_LIBRARIES}
                ${Protobuf_LIBRARIES}
)

add_executable(main ./main.cc)
target_include_directories(main PUBLIC ${INCLUDE_DIRS})
target_link_libraries(main ${MODULE_LIBS})


Example label images

 https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/label_image"
43149,Error while using ModelCheckpoint callback in tf.keras when creating checkpoints.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 20.04 LTS (GNU/Linux 4.4.0-18362-Microsoft x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Source
- TensorFlow version (use command below):2.3.0
- Python version:3.8.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I am trying to create checkpoints while training the model using ModelCheckpoint callback. It is giving the following error
`TypeError: get_config() missing 1 required positional argument: 'self'`

**Describe the expected behavior**
I tried saving the model using save_model func. It works fine. I think there is some issue with the callback. 

**Standalone code to reproduce the issue**
Link to the gist:
[https://gist.github.com/Gokul-S-Kumar/0a355ca70e44953f3af40693e38bc2b1](url)

**Other info / logs** 
Link to full description of the error, in case useful:
[https://gist.github.com/Gokul-S-Kumar/b478a8fba470626aca51d4e09331388b](url)

PS:- I may be committing a simple mistake, if yes please help in pointing it out so that I can rectify it. "
43147,Cannot import name 'image_preprocessing',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.12
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1/7.6
- GPU model and memory:

**Describe the current behavior**
I am currently struggling to proceed due to having an unexpected error message containing 'ImportError: cannot import name 'image_preprocessing''.

I am doing a project related to Object Detection (credits to: EdjeElectronics). Upon installing Tensorflow, Keras, CUDA, cudnn, Visual Studio Microsoft Redistribution 2019, I tried to do testing 'python ~\object_detection\builders\model_builder_tf2_test.py' to see if everything is correctly installed. However, I keep receiving this error.

**Describe the expected behavior**
```
(tensor-gpu) PS C:\Users\User\tensorflow11\models\research> python object_detection\builders\model_builder_tf2_test.py
2020-09-11 12:40:46.813673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
Traceback (most recent call last):
  File ""object_detection\builders\model_builder_tf2_test.py"", line 24, in <module>
    from object_detection.builders import model_builder
  File ""C:\Users\User\tensorflow11\models\research\object_detection\builders\model_builder.py"", line 65, in <module>
    from object_detection.models import ssd_efficientnet_bifpn_feature_extractor as ssd_efficientnet_bifpn
  File ""C:\Users\User\tensorflow11\models\research\object_detection\models\ssd_efficientnet_bifpn_feature_extractor.py"", line 33, in <module>
    from official.vision.image_classification.efficientnet import efficientnet_model
  File ""C:\Users\User\tensorflow11\models\official\vision\image_classification\efficientnet\efficientnet_model.py"", line 37, in <module>
    from official.vision.image_classification import preprocessing
  File ""C:\Users\User\tensorflow11\models\official\vision\image_classification\preprocessing.py"", line 25, in <module>
    from official.vision.image_classification import augment
  File ""C:\Users\User\tensorflow11\models\official\vision\image_classification\augment.py"", line 31, in <module>
    from tensorflow.python.keras.layers.preprocessing import image_preprocessing as image_ops
ImportError: cannot import name 'image_preprocessing'
```

Appreciate if experienced TF developers to state where/what I am doing wrong.

Thank you."
43146,[EfficientNet] Understanding architectural implementation decision for the Squeeze and Excitation phase,"The following question refers to ""Squeeze and Excitation"" block from the EfficientNet implementation (https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/applications/efficientnet.py#L517-L540), lines 478 - 498.

Concretely, ""Squeeze and Excite"" current implementation makes use of two Conv2D blocks: one for filters reduction at line 483 and the other one for filters expanding at line 491.

Giving the fact that the input in this two Conv2D layers is just an array of features reshaped into a 3D tensor (of shape (1, 1, nr_features) according to line 482), the operations that this layers are doing is just a standard one-shot matrix multiplication followed by and activation (as in exactly what a Dense layer does by default).

Therefore, my question is why the implementation uses a Conv2D instead of a regular Dense layer? Is it because of easier code understanding/maintenance? I guess the reason could not be speed execution since both Dense and Conv2D make use of optimized matrix multiplication.

_Thanks, would really appreciate if this question can be answered as I can't find any docs that address this question._"
43145,Data augmentation on TfRecords for multichannel images ,"I am working with multichannel medical Nifti images which I convert to TfRecords and create a tf.dataset to pass later on a multichannel 3D CNN.  The shape of the images is (61,73,61,2) which means the image has 2 channels,  is there a way to do data augmentation on the fly when I read the TfRecords and increase the dataset size?  How can I apply the augmentation across all channels? "
43143,"Discrepancy between loss when called from `tf.keras.losses.BinaryCrossentropy()(true, pred)` and verbose output from model.fit() or hand-computation","I'm struggling to understand the behavior of `tf.keras.losses.BinaryCrossentropy()(true, pred)` -- I can't reproduce it's behavior from first principles.  Here's a MWE with a very simple two-output loss:

```
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import Dense
from tensorflow.keras import Model, Input

np.random.seed(8675309)
X = np.random.normal(size = 1000).reshape(100,10)
B = np.random.normal(size = 20).reshape(10,2)
Y = np.sin(X @ B ) @ np.array([1,0,0,-1]).reshape(2,2) >0

i = Input(10)
l = Dense(5, activation = 'relu')(i)
o1 = Dense(1, name = ""one"", activation = 'sigmoid')(l)
o2 = Dense(1, name = ""two"", activation = 'sigmoid')(l)
m = Model(i, [o1, o2])

loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)

m.compile(optimizer=tf.keras.optimizers.Adam(1e-4),
              loss={'one': tf.keras.losses.BinaryCrossentropy(from_logits=False),
                    'two': tf.keras.losses.BinaryCrossentropy(from_logits=False)})

m.fit(X, [Y[:,0], Y[:,1]],
      batch_size=1000,
      epochs=3,
      verbose=1)
pred = m.predict(X)
loss = loss_object(Y, pred)
```

which gives the following output:

```
Train on 100 samples
Epoch 1/3
100/100 [==============================] - 1s 10ms/sample - loss: 1.5362 - one_loss: 0.8195 - two_loss: 0.7167
Epoch 2/3
100/100 [==============================] - 0s 47us/sample - loss: 1.5359 - one_loss: 0.8193 - two_loss: 0.7166
Epoch 3/3
100/100 [==============================] - 0s 35us/sample - loss: 1.5357 - one_loss: 0.8192 - two_loss: 0.7166


The loss from the loss object is 0.7730987071990967
```

The losses don't match.  To see what's going on, I'll compute entropy by hand:

```
P = np.concatenate([pred[0], pred[1]], axis = 1)
YY = np.concatenate([Y, 1-Y], axis = 1)
PP = np.concatenate([P, 1-P], axis = 1)

(YY*np.log(PP)).sum(axis=1).mean()
Out[47]: -1.5354833577014506

(YY[:,[0,2]]*np.log(PP[:,[0,2]])).sum(axis=1).mean()
Out[48]: -0.8189752248860895

(YY[:,[1,3]]*np.log(PP[:,[1,3]])).sum(axis=1).mean()
Out[49]: -0.7165081328153611
```

The results match the verbose output, but not the output from the loss object.  (Aside from a fairly large discrepancy in the third-ish decimal place???)

Would appreciate an explaination of this behavior.  

I have looked into the loss code on github, and it confuses me further:  

 ```
Example subclass implementation:
  ```python
  class MeanSquaredError(Loss):
    def call(self, y_true, y_pred):
      y_pred = tf.convert_to_tensor_v2(y_pred)
      y_true = tf.cast(y_true, y_pred.dtype)
      return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1)
  ```
Converting a list to a tensor concatenates it along the first dimension, irrespective of the shape of the other input.  The rest of the github page doesn't make it clear how the entropy is actually calculated from that, or at least I can't see it.  binary cross entropy appears to be a super class, with the actual implementation somewhere else.

```"
43142,keras Sequential model get empty metrics_names,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina 0.15.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip3 install --upgrade tensorflow
- TensorFlow version (use command below): 2.3.0
- Python version:
```
Python 3.7.5 (default, Oct 25 2019, 10:52:18)
[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin
```
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the current behavior**
```python
def build_model():
    model = keras.Sequential()
    model.add(keras.layers.Embedding(vocab_size, 64))
    model.add(keras.layers.GlobalAveragePooling1D())
    model.add(keras.layers.Dense(128, activation='relu'))
    model.add(keras.layers.Dense(128, activation='relu'))
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dense(1))
    optimizer = tf.keras.optimizers.RMSprop(0.01)
    model.compile(loss='mse',
                  optimizer=optimizer,
                  metrics=['mae', 'mse'])
    return model


model = build_model()
print(model.metrics_names)  #  print []
```

**Describe the expected behavior**
should print out: ['loss', 'mae', 'mse']
tensorflow 2.1.0 print correct"
43141,Tensorflow 2.3.0 TfLite converter adds ExpandDims,"I have a problem with new version of Tensorflow. I try to deploy my model to embedded system. I have trained Keras model and want to convert it to *.tflite format. Tensorflow 2.2.0 generates .tflite model which is accepted by tflite/micro on embedded system. However Tensorflow 2.3.0 adds some ExpandDims layer at the beggining of the network, and ExpandDims can not e resolved on the tf/lite/micro side. 
Convertion using TF 2.3.0:
![image](https://user-images.githubusercontent.com/58625554/92916599-f3403b00-f42d-11ea-9562-03e6aca59bb0.png)

Convertion using TF 2.2.0:
![image](https://user-images.githubusercontent.com/58625554/92916775-1b2f9e80-f42e-11ea-9ba9-672af0d6281f.png)

We want to move our project from tf 2.2 to tf 2.3 ut for now it is impossible because of this problem.
Here is the code used for convertion:
`        converter = tf.lite.TFLiteConverter.from_keras_model(LOADED_KERAS_MODELl)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]

        quantized_and_pruned_tflite_model = converter.convert()

        quantized_and_pruned_tflite_file = 'model_lite_pruned_microchip.tflite'

        with open(quantized_and_pruned_tflite_file, 'wb') as f:
            f.write(quantized_and_pruned_tflite_model)
        print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)`

Tools versions:
_Python 3.8.5,
Tensorflow 2.2.0/2.3.0
tensorflow-model-optimization 0.4.1_

"
43140,AttributeError: 'int' object has no attribute 'op' in using customized model layers,"Hi there, 

I am trying to build a model with a customized layer in Tensorflow. However, I always got the error when I initialize the customized model.

The error message I got is 'AttributeError: 'int' object has no attribute 'op' '.

Traceback (most recent call last):

  File ""C:\Users\wgw\.spyder-py3\Training_601_Model_Subclass_Layers.py"", line 85, in <module>
    my_model = MyModel(64,12,name='my_custom_model')

  File ""C:\Users\wgw\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 167, in __init__
    super(Model, self).__init__(*args, **kwargs)

  File ""C:\Users\wgw\anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 173, in __init__
    self._init_graph_network(*args, **kwargs)

  File ""C:\Users\wgw\anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 456, in _method_wrapper
    result = method(self, *args, **kwargs)

  File ""C:\Users\wgw\anaconda3\lib\site-packages\tensorflow\python\keras\engine\network.py"", line 254, in _init_graph_network
    base_layer_utils.create_keras_history(self._nested_outputs)

  File ""C:\Users\wgw\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py"", line 186, in create_keras_history
    _, created_layers = _create_keras_history_helper(tensors, set(), [])

  File ""C:\Users\wgw\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py"", line 212, in _create_keras_history_helper
    op = tensor.op  # The Op that created this Tensor.

AttributeError: 'int' object has no attribute 'op'

"
43138,tensorflow 1.14 gpu  is not  in 2060S 2080TI," import tensorflow as tf
/home/sl/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
/home/sl/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
>>> tf.test.is_gpu_available()
2020-09-11 17:33:59.857951: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-09-11 17:33:59.873846: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1
2020-09-11 17:33:59.961881: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-11 17:33:59.962263: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc4d8a9e40 executing computations on platform CUDA. Devices:
2020-09-11 17:33:59.962281: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): GeForce RTX 2060 SUPER, Compute Capability 7.5
2020-09-11 17:33:59.983750: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3600000000 Hz
2020-09-11 17:33:59.984247: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fc4d9d5990 executing computations on platform Host. Devices:
2020-09-11 17:33:59.984263: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>
2020-09-11 17:33:59.984404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-11 17:33:59.984701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2060 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.665
pciBusID: 0000:01:00.0
2020-09-11 17:33:59.984785: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:33:59.984839: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:33:59.984887: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:33:59.984935: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:33:59.984983: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:33:59.985029: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:33:59.987699: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-11 17:33:59.987719: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
2020-09-11 17:33:59.987739: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-11 17:33:59.987748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-11 17:33:59.987754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
False
>>> from tensorflow.python.client import device_lib
>>> print(device_lib.list_local_devices())
2020-09-11 17:35:02.273857: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-11 17:35:02.274176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: 
name: GeForce RTX 2060 SUPER major: 7 minor: 5 memoryClockRate(GHz): 1.665
pciBusID: 0000:01:00.0
2020-09-11 17:35:02.274268: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:35:02.274317: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:35:02.274363: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:35:02.274407: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:35:02.274450: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:35:02.274494: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory
2020-09-11 17:35:02.274511: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7
2020-09-11 17:35:02.274518: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...
2020-09-11 17:35:02.274530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-11 17:35:02.274536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 
2020-09-11 17:35:02.274542: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N 
[name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
"
43135,Mac c++ tensorflow_cpu segmentation fault,"I build tensorflow 1.12 c++ on Mac without gpu. I can load .meta and .ckpt of model. When I run session->run() ，program raise segmentation error. I can get some node output results in middle but I could not get   the last outputs results.  Thank you very much for your help.
My environment is:
   
 MacOS 10.15.4, tensorflow 1.12, cuda(None), bazel 0.15.2, clang version 11.0.3

`
    MetaGraphDef graphdef; //Graph Definition for current model

    string checkpointPath = ""../static/model_ldh"";
    Status status_load = ReadBinaryProto(Env::Default(), ""../static/model_ldh.meta"", &graphdef);
    if (!status_load.ok()) {
        std::cout << ""ERROR: Loading model failed...""<< std::endl;
        std::cout << status_load.ToString() << ""\n"";
        return -1;
    }
    //create session
    SessionOptions options;
    auto session = NewSession(options);
    if (session == nullptr) {
        std::cout << ""ERROR: Creating graph in session failed..."" << std::endl;
        return -1;
    }
    //add graph into session
    Status status = session->Create(graphdef.graph_def());
    if (!status.ok()) {
        throw runtime_error(""Error creating graph: "" + status.ToString());
    }
    // load weights
    Tensor checkpointPathTensor(DT_STRING, TensorShape());
    checkpointPathTensor.scalar<std::string>()() = checkpointPath;
    status = session->Run(
            {{ graphdef.saver_def().filename_tensor_name(), checkpointPathTensor },},
            {},
            {graphdef.saver_def().restore_op_name()},
            nullptr);
    if (!status.ok()) {
        throw runtime_error(""Error loading checkpoint from "" + checkpointPath + "": "" + status.ToString());
    }

    string image_path = ""../static/test.jpg"";
    int input_height = 112;
    int input_width = 96;
    int input_mean = 127;
    int input_std = 128;
    std::vector<Tensor> resized_tensors;
    Status read_tensor_status =
            ReadTensorFromImageFile(image_path, input_height, input_width, input_mean,
                                    input_std, &resized_tensors); // load image . shape is [1, 112, 96, 3]
    if (!read_tensor_status.ok()) {
        LOG(ERROR) << read_tensor_status;
        cout<<""resing error""<<endl;
        return -1;
    }

    const Tensor& resized_tensor = resized_tensors[0];
    std::cout << resized_tensor.DebugString()<<endl;

    vector<tensorflow::Tensor> outputs;
    tensorflow::string output_node1 = ""outputs_A:0"" , output_node2 = ""outputs_B:0"";
    tensorflow::Tensor phase_train(DT_BOOL, {1});
    tensorflow::Tensor switch_all(DT_BOOL, {1});
    tensorflow::Tensor keep_prob(DT_FLOAT,{1});
    auto phase_train_data = phase_train.tensor<bool, 1>(); phase_train_data(0) = false;
    auto switch_all_data = switch_all.tensor<bool, 1>(); switch_all_data(0) = false;
    auto keep_prob_data = keep_prob.tensor<float, 1>(); keep_prob_data(0) = 1.0;
    vector<pair<string, tensorflow::Tensor>> need_inputs = {{""inputs:0"", resized_tensor},
         {""switch:0"", switch_all}, {""keep_prob:0"", keep_prob}, {""phase_train:0"", phase_train}};

    outputs.clear();
    tensorflow::string  debug = ""FaceResNet/NetA_3/conv4/layer_0/Conv/p_re_lu/Relu:0"";
    Status status_run = session->Run(need_inputs, {debug}, {}, &outputs); // **I can get this results**
    status_run = session->Run(need_inputs, {output_node1}, {}, &outputs); // **this line will occur to segmentation fault**
    `
the error statcks are as following :
`Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>::operator()(float*, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer> const&, long, long, long, long) const 0x0000000105f7e75c
Eigen::TensorEvaluator<Eigen::TensorContractionOp<Eigen::array<Eigen::IndexPair<long>, 1ul> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>::Context<Eigen::internal::gemm_pack_lhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 48, 16, 0, false, false>, Eigen::internal::gemm_pack_rhs<float, long, Eigen::internal::TensorContractionSubMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, 4, 0, false, false>, Eigen::internal::gebp_kernel<float, float, long, Eigen::internal::blas_data_mapper<float, long, 0, 0>, 48, 4, false, false>, Eigen::internal::TensorContractionInputMapper<float, long, 1, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::TensorContractionInputMapper<float, long, 0, Eigen::TensorEvaluator<Eigen::TensorReshapingOp<Eigen::DSizes<long, 2> const, Eigen::TensorImagePatchOp<-1l, -1l, Eigen::TensorMap<Eigen::Tensor<float const, 4, 1, long>, 16, Eigen::MakePointer> const> const> const, Eigen::ThreadPoolDevice>, Eigen::array<long, 1ul>, Eigen::array<long, 1ul>, 16, true, false, 0, Eigen::MakePointer>, Eigen::internal::blas_data_mapper<float, long, 0, 0> >::pack_rhs(long, long) 0x0000000105f80aa8
Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) 0x000000010def1e7e
std::__1::__function::__func<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'(), std::__1::allocator<tensorflow::thread::EigenEnvironment::CreateThread(std::__1::function<void ()>)::'lambda'()>, void ()>::operator()() 0x000000010def17df
void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, std::__1::function<void ()> > >(void*) 0x000000010df15200
_pthread_start 0x00007fff6f560109
thread_start 0x00007fff6f55bb8b`

This model is work in python environment with tf 1.12. I am so puzzled.

`

def load_ckpt():

    sess = tf.Session()
    saver = tf.train.import_meta_graph(""./log/model_ldh.meta"", clear_devices = True)
    saver.restore(sess, ""./log/model_ldh"")
    phase_train_ = sess.graph.get_tensor_by_name('phase_train:0')
    keep_prob_ = sess.graph.get_tensor_by_name('keep_prob:0')
    inputs_ = sess.graph.get_tensor_by_name('inputs:0')
    switch_ = sess.graph.get_tensor_by_name('switch:0')
    outputs_B_ = sess.graph.get_tensor_by_name('outputs_B:0')
    outputs_A_ = sess.graph.get_tensor_by_name('outputs_A:0')
    images = np.random.randint(0, 255, size = (1, 112, 96, 3))
    switch = np.array([False])
    #debug = sess.graph.get_tensor_by_name(""FaceResNet/NetA_3/conv4/layer_0/Conv/p_re_lu/Relu:0"")

    feed_dict = {
        inputs_: images,
        switch_: switch, 
        phase_train_: np.array(False),
        keep_prob_: np.array(1.0)
    }

    ret = sess.run(outputs_A_, feed_dict) # This program could work
    print(ret.shape)
`"
43133,tensorflow.python.framework.errors_impl.UnknownError: ,"I tried  to test the face filter model which i had in my system. when i run the test script i am getting the following error. In cpu it is running without any issue but in gpu its causing the issue.

tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](conv2d_1/Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, conv2d_1/Conv2D/ReadVariableOp)]]
"
43132,Tensorflow 2.3: No gradients provided for any variable,"**System information**
- Used Functional API
- OS: Ubuntu 20.04 and Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from source
- TensorFlow version: 2.3
- Python version:3.6.7


Hi I have built a model using functional API but when I start to train the model it says ""**No gradients provided for any variable:**""
 A minimalistic [gist](https://gist.github.com/partha117/1c20da6252ef43f429c30fafe17cc93f)  is attached to recreate the error
"
43131,"Just Qustion: In Android JAVA, keras model is ok?","Hi, I'm developing android app

and my NN in python is keras(Tf.keras) with LSTM, conv2d, resnet, attention (very deep)

I want to use this NN in JAVA.android

1) can I use TF-lite for my keras of python?? 

I heard TF.lite only support some TF model so I'm worried

2) and can I convert keras to tensorflow? and just use tensorflow in JAVA?

3) and can I convert keras to tensorflow? and just use tensorflow lite or tensorflow.mobile in JAVA?

Thx.
"
43130,A dynamic link library (DLL) initialization routine failed. ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): libtensorflow_jni-cpu-windows-x86_64-1.14.0
- TensorFlow version: 1.14


**Describe the problem**
So I was trying to run `` using a guide for [windows 10](https://www.tensorflow.org/install/lang_java#windows) and I followed the guide exactly as it said to.
line 108 in `org/tensorflow/NativeLibrary.java`: System.load(extractResource(jniResource, jniLibName, tempDirectory));, the exception occurred:

Exception in thread ""main"" java.lang.UnsatisfiedLinkError: I:\Code\Android\test\SpeechRecognition_tf_lite\lib\tensorflow_jni.dll: A dynamic link library (DLL) initialization routine failed. 
	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(Unknown Source)
	at java.lang.ClassLoader.loadLibrary(Unknown Source)
	at java.lang.Runtime.load0(Unknown Source)
	at java.lang.System.load(Unknown Source)
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:108)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:67)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:82)
	at org.tensorflow.Graph.<clinit>(Graph.java:479)
	at TFInterface.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:61)
	at ASR.Chain.<init>(Chain.java:60)
	at ASR.ASR.main(ASR.java:6)
"
43129,Integer quantization error,"Traceback (most recent call last):
  File ""D:/hua_work/tf_lite/lite_test.py"", line 32, in <module>
    interpreter.allocate_tensors()
  File ""E:\Anaconda3\envs\tensorflow23\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 243, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: tensorflow/lite/kernels/pad.cc:118 op_context.input->type != op_context.constant_values->type (INT8 != FLOAT32)Node number 3 (PADV2) failed to prepare.

There is pad layer in the model, error after full quantization
"
43128,Models for TensorFlow Lite which are quantized according to the post-training quantization does not run on NPU of some devices.,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Y es
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS 10.13.6, Android 10.1.0
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MatePad Pro
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.3
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I am trying int8 quantization of my model on TensorFlow Lite.
Conversion itself worked using tensorflow 1.15.3 but the converted model ran extremely slowly on Kirin 990. logcat indicates that it runs on CPU, not NPU.
(Conversion using tensorflow 2.3.0 did not work.)

mobilenet_v1_1.0_224_quant.tflite in tensorflow/examples runs fast on Kirin 990. logcat indicates that it runs on NPU.

So I checked the differences between them.

1. My model is int8(tf.lite.OpsSet.TFLITE_BUILTINS_INT8) quantization but mobilenet_v1_1.0_224_quant.tflite seems uint8 quantization.

2. The ""filter"" property of Conv2D has a ""quantization"" attribute in mobilenet_v1_1.0_224_quant.tflite, but The ""filter"" property of Conv2D has no ""quantization"" attribute in my converted model.

How can I convert my model like mobilenet_v1_1.0_224_quant.tflite?
Please disclose the conversion script for mobilenet_v1_1.0_224_quant.tflite.

I asked [the question](https://stackoverflow.com/questions/63700590/how-can-i-convert-my-model-like-mobilenet-v1-1-0-224-quant-tflite-in-tensorflow) on stackoverflow, but have not gotten relevant answers, so I ask the question here again.

Thanks.

[Edit]
In my current understanding, some NNAPI drivers have not supported per-channel quantized model yet. But the post-training quantization in TensorFlow supports only er-channel quantization.
I will appreciate if you provide solutions to resolve this situation.

**Describe the expected behavior**

I will get a fast, quantized model.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43126,Int16 softmax is using std::vector,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 35d9474383c9befd99f457031ada977d681742c4
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F4

**Describe the problem**
This command:
```
make -f tensorflow/lite/micro/tools/make/Makefile TARGET=stm32f4 kernel_softmax_test
```

Fails with the following error message:
```
/home/advaitjain/tensorflow/github/tensorflow/tensorflow/lite/micro/tools/make/downloads/gcc_embedded/bin/../lib/gcc/arm-none-eabi/7.3.1/thumb/v7e-m/libgcc.a(unwind-arm.o): In function `get_eit_entry':
unwind-arm.c:(.text+0x138): undefined reference to `__exidx_end'
unwind-arm.c:(.text+0x13c): undefined reference to `__exidx_start'
collect2: error: ld returned 1 exit status
```

The underlying issue is that the reference implementation of int16 softmax uses std::vector which is incompatible with Micro.

https://github.com/tensorflow/tensorflow/blob/35d9474383c9befd99f457031ada977d681742c4/tensorflow/lite/kernels/internal/reference/softmax.h#L168

The near-term fix will be to revert PR #38873. Longer-term we would have to fix up the reference implementation to be friendly for embedded platforms.

This issue was not caught by continuous integration when the PR was merged because we do not build for TARGET=stm32f4 with only the reference kernels. We only do so with TAGS=cmsis-nn.

Two ways to safeguard against this in the future would be:
 1. build STM32F4 without any additional TAGS
 1. fix up bluepill and add more tests to that target -- this is likely preferable.
"
43125,tf.data.experimental.service throws error when using with TPUs,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, I put an example to reproduce the issue below

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
`
$ uname -a
Linux james-tpu 4.19.0-10-cloud-amd64 #1 SMP Debian 4.19.132-1 (2020-07-24) x86_64 GNU/Linux
`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version:
3.7.3
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
Errors out with `2020-09-10 22:59:13.294374: E tensorflow/core/common_runtime/eager/context.cc:678] Failed to register function remotely due to Failed to register dataset: failed to connect to all addresses` see full log below

**Describe the expected behavior**
I wouldn't expect an error for this code.
**Standalone code to reproduce the issue**
<details>
<summary>Code</summary>

```
import logging

import tensorflow as tf
import numpy as np

logging.getLogger('tensorflow').setLevel(logging.DEBUG)


def get_dataset(dispatcher, batch_size):
  fake_data = np.zeros((128, 256, 256), dtype=np.float32)
  ds = tf.data.Dataset.from_tensor_slices(fake_data)
  ds = ds.repeat()
  ds = ds.batch(batch_size)
  ds = ds.apply(tf.data.experimental.service.distribute('parallel_epochs', dispatcher.target, job_name='data_job'))
  return ds

def run(strategy, batch_size):
  data_dispatcher = tf.data.experimental.service.DispatchServer(port=0)
  dispatcher_address = data_dispatcher.target.split(""://"")[1]
  worker = tf.data.experimental.service.WorkerServer(
    port=0, dispatcher_address=dispatcher_address)

  per_replica_batch_size = batch_size // strategy.num_replicas_in_sync
  dataset = strategy.experimental_distribute_datasets_from_function(
    lambda _: get_dataset(
      data_dispatcher,
      per_replica_batch_size,
    )
  )
  ds_iterator = iter(dataset)

  @tf.function(input_signature=[tf.TensorSpec([256, 256], dtype=tf.float32)])
  def step_fn(inputs):
    return

  @tf.function
  def train_step(iterator):
    strategy.run(step_fn, args=(next(iterator),))

  for _ in range(10):
    train_step(ds_iterator)


def setup_strategy(tpu=False):
  resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='james-tpu')
  tf.config.experimental_connect_to_cluster(resolver)
  tf.tpu.experimental.initialize_tpu_system(resolver)
  return tf.distribute.TPUStrategy(resolver)


if __name__ == '__main__':
  strategy = setup_strategy()
  batch_size = 64
  run(strategy, batch_size)
```
</details>


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
<details>
<summary>Logs</summary>

```
2020-09-10 22:59:07.877313: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-10 22:59:07.882757: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2299995000 Hz
2020-09-10 22:59:07.883041: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3aa2460 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-10 22:59:07.883156: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-10 22:59:07.889580: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.184.89.74:8470}
2020-09-10 22:59:07.889626: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:31299}
2020-09-10 22:59:07.904477: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> 10.184.89.74:8470}
2020-09-10 22:59:07.904533: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job localhost -> {0 -> localhost:31299}
2020-09-10 22:59:07.905018: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://localhost:31299
INFO:tensorflow:Initializing the TPU system: james-tpu
INFO:tensorflow:Initializing the TPU system: james-tpu
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Clearing out eager caches
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)
2020-09-10 22:59:13.294374: E tensorflow/core/common_runtime/eager/context.cc:678] Failed to register function remotely due to Failed to register dataset: failed to connect to all addresses
This shouldn't happen, please file a bug to tensorflow team.
Traceback (most recent call last):
  File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/jamesbartlett/train/pixieml/pixieml/models/transformer/minimal_example.py"", line 54, in <module>
    run(strategy, batch_size)
  File ""/home/jamesbartlett/train/pixieml/pixieml/models/transformer/minimal_example.py"", line 30, in run
    ds_iterator = iter(dataset)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1199, in __iter__
    enable_legacy_iterators)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1752, in _create_iterators_per_worker
    worker_devices)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1609, in __init__
    devices)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1448, in __init__
    self._make_iterator()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1619, in _make_iterator
    self._dataset, self._devices, source_device=host_device)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 547, in __init__
    dataset.element_spec)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 54, in __init__
    init_func_concrete = _init_func.get_concrete_function()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 2939, in get_concrete_function
2020-09-10 22:59:13.297630: W tensorflow/core/distributed_runtime/eager/remote_tensor_handle_data.cc:76] Unable to destroy remote tensor handles. If you are running a tf.function, it usually indicates some op in the graph gets an error: Failed to register dataset: failed to connect to all addresses
    *args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 2906, in _get_concrete_function_garbage_collected
    graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3213, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3075, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 991, in func_graph_from_py_func
    expand_composites=True)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py"", line 635, in map_structure
    structure[0], [func(*x) for x in entries],
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py"", line 635, in <listcomp>
    structure[0], [func(*x) for x in entries],
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 942, in convert
    x = ops.convert_to_tensor_or_composite(x)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1622, in convert_to_tensor_or_composite
    value=value, dtype=dtype, name=name, as_ref=False)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1661, in internal_convert_to_tensor_or_composite
    accepted_result_types=(Tensor, composite_tensor.CompositeTensor))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1467, in convert_to_tensor
    return graph.capture(value, name=name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 624, in capture
    return self.capture_eager_tensor(tensor, name)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py"", line 721, in capture_eager_tensor
    graph_const = constant_op.constant(tensor.numpy(), dtype=tensor.dtype,
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1063, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.UnavailableError: Failed to register dataset: failed to connect to all addresses
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/tpu_strategy.py"", line 540, in async_wait
    context.async_wait()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py"", line 2319, in async_wait
    context().sync_executors()
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/context.py"", line 658, in sync_executors
    pywrap_tfe.TFE_ContextSyncExecutors(self._context_handle)
tensorflow.python.framework.errors_impl.UnavailableError: Failed to register dataset: failed to connect to all addresses
2020-09-10 22:59:13.375822: W ./tensorflow/core/distributed_runtime/eager/destroy_tensor_handle_node.h:57] Ignoring an error encountered when deleting remote tensors handles: Invalid argument: Unable to find the relevant tensor remote_handle: Op ID: 30, Output num: 0
Additional GRPC error information from remote target /job:worker/replica:0/task:0:
:{""created"":""@1599778753.375752373"",""description"":""Error received from peer ipv4:10.184.89.74:8470"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Unable to find the relevant tensor remote_handle: Op ID: 30, Output num: 0"",""grpc_status"":3}
```
</details>"
43124,Generating ICU normalization_data.c and icu_conversion_data.c on s390x architecture,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.2.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): bazel 2.0.0- (@non-git)
- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
ICU data provided [here ](https://github.com/tensorflow/tensorflow/tree/master/third_party/icu/data) appears to be generated on x86 platform.  I am trying to generate this data on s390x but running into some issues.

Firstly, filters.json as per [instructions ](https://github.com/tensorflow/tensorflow/blob/master/third_party/icu/data/BUILD.bazel) is not compatible with `icu` `release-64-2`. It seems that TF 2.2.0 expects `release-64-2`.  I can make it work with `release-65-1` but unsure of generating `icu_conversion_data.c.gz.*`   I could see that `conversion_data.c` is generated by concatenating `*.gz` unzipped contents.  Can someone please provide some pointers here on how to generate the `*.gz` files?

Secondly, [instructions ](https://github.com/tensorflow/text/blob/master/third_party/icu/data/BUILD) on generating `normalization_data.c`  are almost identical to the `icu_conversion_data.c`.  Are these two same files or am I missing something?

Thanks again for your time."
43122,"model_main_tf2.py ""module tensorflow.compat.v2 has no attribute app""","
### System informatio

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: model_main_tf2.py 
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04.5
-   **TensorFlow installed from (source or binary)**: 2.3.0
-   **TensorFlow version (use command below)**: v2.3.0-rc2-23-gb36436b087 2.3.0
-   **Python version**: Python 3.6.9
-   **GPU model and memory**: V100
-   **Exact command to reproduce**: python3 ../../models/research/object_detection/model_main_tf2.py --pipeline_config_path=model/pipe.config --model_dir=training --logtostderr

### Describe the problem
I'm just trying to run a basic training job through the provided script in the object_detection samples. I used to run the model_main.py with TF 1.x.

### Source code / logs
```
2020-09-10 20:33:44.263700: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Traceback (most recent call last):
  File ""../../models/research/object_detection/model_main_tf2.py"", line 112, in <module>
    tf.app.run()
AttributeError: module 'tensorflow.compat.v2' has no attribute 'app'
```"
43121,[TFLu] int8 ops slower than f32,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip install tf-nightly
- Tensorflow version (commit SHA if source): 2.4.0-dev20200908
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Cortex M4f

**Describe the problem**
I compared the time spent by `MicroInterpreter::Invoke()` to perform different ops on the same model with with int8 quantization and without. I also tried the CMSIS-NN kernels for some of the ops. The problem is that besides the fully connected op, every other op is the same or slower with the int8 ops. 

Here is a table showing the average time in ticks spent by each op's `Eval()`. The first column shows model with int8 quantization using cmsis-nn kernels for mul, add, fullyconnected. The second column uses the reference kernels and third is floating point.

  | q7cmsis | q7 ref | f32
-- | -- | -- | --
fullyconnected | 5990 | 8611 | 6639
tanh | 15114 | 15122 | 1101
add | 2686 | 2887 | 971
mul | 2202 | 1834 | 1261
sub | 3301 | 3299 | 1380
split_v | 898 | 915 | 836
split | 794 | 817 | 814
reshape | 441 | 443 | 918

The tanh kernel performs the worst with 13x slower than the floating point equivalent. Is this expected or known behavior?

**Please provide the exact sequence of commands/steps when you ran into the problem**
I have attached the models that I used for profiling. 
[profiling_models.zip](https://github.com/tensorflow/tensorflow/files/5204489/profiling_models.zip)


"
43120,"After several iterations the error:  ""input and filter must have the same depth"" is arose","I am trying to fit my model by using a generator working on my dataset (audio files). I use a gold cluster (SLURM) to run my program including generator for creating my input features and corresponding targets (by using yeilding method, and safe threading), and fit_generator for training my model.
Unfortunetly, after 388 iterations including batches with 128 size, the error:

 tensorflow.python.framework.errors_impl.InvalidArgumentError:  input and filter must have the same depth: 6 vs 18
	 [[node model_4/conv2d_80/BiasAdd (defined at /Project1/Training/Prog/preANDtrain_generator_model2.py:187) ]] [Op:__inference_train_function_9095]

Function call stack:
train_function

2020-09-10 17:09:40.560724: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.
	 [[{{node PyFunc}}]] 

was shown and my program was failed. 
The depth (#channels) of the input features is 6 and the first layer is a covolutional layer with 18 channels (filters). I became confused why the error was shown after 388 iterations, and why the depth of input features and the first layer must be same.
I will be so grateful if someone can help me to solve the problem
[New Text Document.txt](https://github.com/tensorflow/tensorflow/files/5204020/New.Text.Document.txt)
.

"
43119,Segfault on absurdly large shuffle buffer for `make_csv_dataset`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch linux kernel 5.8.7 (primary), Ubuntu 18.04 (container)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): (git) v2.3.0-rc2-23-gb36436b087 (tf version) 2.3.0
- Python version: 3.8.5
- CUDA/cuDNN version: n/a
- GPU model and memory: N/A -- run on a CPU

**Describe the current behavior**
Under certain (admittedly abnormal) shuffle buffer sizes, attempting to use the returned dataset fails with a segfault.

**Standalone code to reproduce the issue**

With a CSV with ~300 pieces of data per entry, the following snippet causes a segfault:

```py
# Given:
#   ABSURDLY_LARGE_NUMBER = <around 2×10^17>
#   YOUR_CSV_PATH = <somewhere you put your csv file>
#   LABEL_COLUMN = <label column from the csv>
#   INPUT_COLUMNS = <list of columns from the csv, len() around 300>

import tensorflow as tf

# Helper function to get a dataset
def get_dataset(file_path, **kwargs):
    return tf.data.experimental.make_csv_dataset(
        file_path,
        batch_size=1,
        num_epochs=1,
        label_name=LABEL_COLUMN,
        select_columns=[LABEL_COLUMN] + INPUT_COLUMNS, # Around 300 entries
        header=True,
        shuffle=True,
        shuffle_buffer_size=ABSURDLY_LARGE_NUMBER, # <- SOURCE OF ERROR
        **kwargs)

# Used later, with some model constructed with the keras `Sequential` API

model.evaluate(get_dataset(YOUR_CSV_PATH))
```

[Full standalone example](https://gist.github.com/sharnoff/5dc5000fca80a2ab0f78b2786b75c2eb)

There might be (justified) responses like:

> Well sure - a segfault should be expected if you're going to be doing silly things like this!

And that's perhaps fair, but bugs are often only dumb in hindsight, and this was a result of rapid modification of a script over time - pieces get left in that you might not expect.

## Possible cause

**[disclaimer]** This is purely speculation; I'm basing this on the bits of information I found after

Because the maximum value of a 64-bit unsigned integer is around 2×10^19, and the requested buffer size was around 2×10^17, the size of an individual piece of data (at around 300 inputs) would have been enough to trigger overflow in the allocation size where it might not have been caught initially.

The segfault may have also been a result of the size of the requested allocation for the shuffle buffer.

When running inside GDB, an *internal* error was generated, reading:
```
../../gdb/utils.c:684: internal-error: virtual memory exhausted: can't allocate 5506620787261440008 bytes.
A problem internal to GDB has been detected, further debugging may prove unreliable
```

While an internal bug in GDB is a completely separate topic, the indication of the requested allocation size may be helpful here.

## Possible source

(with the extensive help of a friend) I managed to track a possible source for a bad allocation down to [these](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/shuffle_dataset_op.cc#L146) two [lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/data/shuffle_dataset_op.cc#L351), where an invalid allocation might be attempted. I believe these could be sources of the segfault (though there may be more)"
43116,"I want to add a ""RecvTensor"" Op on ""timeline"" Tracing  for RDMA  , But  The code does not worked, can help me ? ","The ""rdma"" extension is now on this[ link ](https://github.com/tensorflow/networking/blob/e1c7ae09923293f9cce5eba0a159e75993b793be/tensorflow_networking/verbs/rdma_rendezvous_mgr.cc#L74)

I want to add ""RecvTensor"" Op on ""Chrome://tracing"" which is for rdma.

rdma.cc
`class RdmaTensorRequest {
...
 RdmaChannel* rdma_channel() {
    return channel_;
  }
....
}`



rdma_rendezvous_mgr.cc 

```void RdmaRemoteRendezvous::RecvFromRemoteAsync(  ...

 RdmaTensorRequest* request =
      rc->InsertTensorRequest(key, step_id_, dst_dev, recv_args, done);
  request->set_recv_req_start_micros(Env::Default()->NowMicros());  

auto* worker_cache = request->rdma_channel()->adapter_->
            worker_env_->session_mgr->LegacySession()->worker_cache.get();
  auto* timeline_logger = dynamic_cast<WorkerCachePartial*>(worker_cache)->GetLoagger();
  bool logging_active = timeline_logger->LoggingActive() || VLOG_IS_ON(2);
  if (logging_active) {
    if (timeline_logger->LoggingActive()) {
      // timeline_logger->RecordDataTransfer(
      //         step_id, send_start_usec, end_usec, key, request->src_device(),
      //         request->dst_device(), num_bytes, """", ""RecvBuf"");
      std::vector<string> key_parts = str_util::Split(key, ';');
      if (key_parts.size() != 5) {
        LOG(WARNING) << ""Bad key: "" << key;
      } else {

        int64 end_usec = Env::Default()->NowMicros();
        int64 send_start_usec = end_usec - 10;
        timeline_logger->RecordRecvTensor(step_id_, send_start_usec, end_usec,
                                  key_parts[3],  // tensor name
                                  key_parts[0],  // src_device
                                  key_parts[2],  // dst_device
                                  2);
      }
    }
  }
`


But this code  does not work, can help me?"
43115,tf.signal.ifft returns results with different dtypes for subsequent calls with the same parameter,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 10.4
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-40510-ga32c74ae8f 2.4.0-dev20200829
- Python version: Python 3.8.3
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

When calling `tf.signal.ifft` with an operand with a `complex128` dtype several times, the first call returns a `complex128` array as expected, but all subsequent calls return a `complex64` array.

**Describe the expected behavior**

`tf.signal.ifft` should always return an array with the same type as its input per the documentation; more importantly, it should be consistent across calls.

**Standalone code to reproduce the issue**

```python
import numpy as np
import tensorflow as tf

operand = np.ones((2, 2), dtype=np.complex128)
print(tf.signal.ifft(operand).dtype)
print(tf.signal.ifft(operand).dtype)
```
"
43114,Problem saving and loading Keras Model that was built with StellarGraph library,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
I am using StellarGraph package which uses Keras for building graph neural networks. 
 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Catalina
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NIL

- TensorFlow installed from (source or binary): binary 
- TensorFlow version (use command below): pip install tensorflow 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I have used the following code to save and load the Keras model:

```
json_file = open(os.path.join(""./outputs/model_new/"", 'model.json'), 'r')
model_json = json_file.read()
json_file.close()
model = model_from_json(model_json, custom_objects=sg.custom_keras_layers)
# load weights into new model
model.load_weights(os.path.join(""./outputs/model_new/"", ""model.h5""))   
model.compile(
            optimizer=optimizers.Adam(lr=0.002),
            loss=keras.losses.binary_crossentropy,
            metrics=METRICS,
        )
```

However, it returns the following error:

```
ValueError: Dimension 1 in both shapes must be equal, but are 2 and 1. Shapes are [?,2] and [?,1]. for '{{node mean_hin_aggregator_3/concat_5}} = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32](mean_hin_aggregator_3/Reshape_35, mean_hin_aggregator_3/truediv_5, mean_hin_aggregator_3/concat_5/axis)' with input shapes: [?,2,18], [?,1,18], [] and with computed input tensors: input[2] = <2>.
```

Please note that I am using StellarGraph to build the graph model. There are some custom layers involved such as Mean_hin_aggregator. I suspect this is the cause of the problem. A similar problem was mentioned in StellarGraph issues before: https://github.com/stellargraph/stellargraph/issues/201

**Describe the expected behavior**

It should return the trained model.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43112,Basic Queries regarding MobileBERT predictions using TFLite Model Maker,"Hello,

I trained the pre-trained MobileBERT using TFLite Model Maker for a binary text-classification task. The training and evaluation went fine.

I have two doubts:

1) How can the trained model be used to predict the label for a **sample sentence** which is passed as an argument to the model? When we tried the many possible methods to predict the label for a sample sentence, we got errors regarding ""tensors required"". Rather, how can we pass a **list of strings** to the model and **get the labels**? What is the appropriate method?

2) Also, what are the correct methods to **re-load the saved model** into a new Colab session to perform predictions? 

I would be grateful for any guidance received. Thanks.
"
43111,tf .function gives different output than standard function,"No custom code. TF 2.3

I am trying to make a ""backtest"" differentiable so i decide to use tf.function's auto differentiation capability.

Despite taking a very long time to compile the function, finally it gets converted into TF ops. However the ""loss"" i get when running a TF version of the function and the python version are completely different.

```
@tf.function
def get_loss(h_actions, nh_actions, prices):
    entry_point = 0.0
    exit_point = 0.0
    holding = False
    profit = 0.0
    index = 0
    fee = 0.001
    keep_perc = 1 - fee
    final_index = len(prices) - 1

    while index < final_index:
        curr_price = prices[index]

        if holding:
            act_value = h_actions[index]
        else:
            act_value = nh_actions[index]

        if act_value > 0.5:
            if holding is False:  # BUY
                holding = True
                entry_point = curr_price

            else:  # SELL
                holding = False
                exit_point = curr_price
                profit += (pow(keep_perc, 2) * exit_point / entry_point) - 1

        index += 1
    return profit
```

The inputs for this function are precomputed predictions and price history is supplied.

I don't believe the values for these specifically are very important. I was able to produce the same behavior with a dummy set

```
samples = np.random.normal(size=(1000, 10))
prices = [random.uniform(1, 1000) for _ in range(1000)]
samples[:, -1] = 1
holding_preds = model.predict(samples)
samples[:, -1] = -1
not_holding_preds = model.predict(samples)
```

The only thing i see is that a warning in thrown saying that there's a large unroll loop. Other than that why would i get getting a different output?
"
43108,Several visual studio  editions and error of  LLVM requires at least MSVC 2017.,"I want to build tf-2.2.0 with bazel in win10.After some preparations， I follow cmds like below:
**First:**
python ./configure.py

.... **everything ok**

**Then:**
bazel build --config=opt //tensorflow:tensorflow_cc.dll

I get **some errors:** 
 D:/softwares/tensorflow-2.2.0/tensorflow/core/framework/BUILD:582:1: C++ compilation of rule '//tensorflow/core/framework:bfloat16' failed (Exit 2)


注意: 包含文件:                C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\corecrt_wstring.h
注意: 包含文件:              C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\stdexcept
注意: 包含文件:               C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\exception
注意: 包含文件:                C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\type_traits
注意: 包含文件:                 C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\xstddef
注意: 包含文件:                  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\cstddef
注意: 包含文件:                  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\initializer_list
注意: 包含文件:                C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\malloc.h
注意: 包含文件:                C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\vcruntime_exception.h
注意: 包含文件:                 C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\eh.h
注意: 包含文件:                  C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE\corecrt_terminate.h

external/llvm-project/llvm/include\llvm/Support/Compiler.h(88): fatal error C1189: #error:  LLVM requires at least MSVC 2017.

**I know** it means that I need at least vs 2017 but I already have vs2019 (vs2019 is set up at D:\Softwares\vs2019).

It seems that bazel will serach vs in default path, e.g. C:\Program Files (x86)\Microsoft Visual Studio 14.0 instead of my costume path. 
So I want to know where to **change** the default visual stuio path that bazel uses with my costume path?

os : win10
vs: vs2013/vs2015/vs2019
Need help please."
43107,No gradient defined for operation RaggedTensorFromVariant / or no gradients at all,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.2
- GPU model and memory: RTX 2070 SUPER, 8Gb


**Describe the current behavior**
Using RaggedTensor I want to minimize KL loss for different segments of a weights matrix (assuming their coefficients drawn from multivariate normal distribution in each segment). RaggedTensors is used inside tf.map_fn. First, I met [issue](https://github.com/tensorflow/tensorflow/issues/42189) 
LookupError: gradient registry has no entry for: RaggedTensorFromVariant
I was fixed it with [provided code](https://github.com/tensorflow/tensorflow/issues/42189#issuecomment-673529072)
However, optimization is not working. Weights are not updated during training. For simplicity, I want to minimize only trace in covariance matrix, but nothing happens. I think that provided code doesn't allow gradients to pass through.
 
**Describe the expected behavior**
I expect trace will be minimized to zero

**Standalone code to reproduce the issue**


```
import tensorflow as tf
import numpy as np

class DenseCov(tf.keras.layers.Dense):
    def __init__(self, units, segments, **kwargs):
        
        self.segment_ids = tf.keras.backend.constant(segments, dtype=tf.int64, name='segment_ids')
        self.W_ragged = None
        self.embed_dim = None
        super().__init__(units=units, use_bias=False, kernel_initializer=tf.keras.initializers.Ones(), **kwargs)
    
    def call(self, inputs):
        logits = super().call(inputs)

        # Want to minimize Kullback–Leibler divergence between two multivariate normal distributions
        # But for simplicity minimize only trace

        W_ragged = tf.RaggedTensor.from_value_rowids(tf.transpose(self.kernel), self.segment_ids, name='init_ragged')
        means = tf.reduce_mean(W_ragged, axis=1, name='means')
        W_centred = W_ragged - tf.expand_dims(means, 1)
        cov_matrix = tf.map_fn(lambda x: tf.matmul(x, x, True) / (tf.cast(tf.shape(x)[0], tf.float32) - 1), W_centred, name='calc_covar')
        cov_matrix = cov_matrix.to_tensor(name='convert_dense')
        
        traces =  tf.map_fn(lambda x: tf.linalg.trace(x), cov_matrix, name='calc_trace')
        # traces = 0.

        # logdets = tf.map_fn(lambda x: -tf.linalg.logdet(x), cov_matrix, name='calc_logdet')  # disabled for simplicity
        logdets = 0.

        # loss = traces + logdets + tf.reduce_sum(centroid_means**2, axis=1)  # disabled for simplicity
        loss = traces

        loss = tf.reduce_mean(loss, name='total_loss')        
        self.add_loss(loss)
        return logits
    
    def build(self, input_shape):
        super().build(input_shape)

        # Distorted matrix a bit
        W = self.kernel
        W.assign_add(np.random.randn(N_dim, N_class).astype(np.float32))
        
    def get_config(self):
        config = {
            'segments': self.segment_ids.numpy(),
        }
        base_config = super().get_config()
        base_config.update(config)
        return base_config

N_class, N_domains, N_dim, N_samples = 50000, 1, 10, 100
segments = np.random.randint(0, N_domains, N_class)
segments = np.sort(segments)

data = np.random.randn(N_samples, N_dim).astype(np.float32)
y_true = np.random.randn(N_samples).astype(np.float32)

tf.keras.backend.clear_session()
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(N_dim,)))
model.add(DenseCov(N_class, segments))
model.add(tf.keras.layers.Dense(1))

def dummy_loss(y_true, y_pred):
    return 0*tf.reduce_sum(y_pred)

model.compile(optimizer=tf.keras.optimizers.Adam(0.1), loss=dummy_loss)

from tensorflow.raw_ops import RaggedTensorToVariant

@tf.RegisterGradient(""RaggedTensorFromVariant"")
def _RaggedTensorFromVariantGrad(*args):
    if len(args) == 2:
        op, grad = args
        res = [RaggedTensorToVariant(rt_nested_splits=[], rt_dense_values=grad,
                                      batched_input=False)]
    else:
        op, empty, grad = args
        res = [RaggedTensorToVariant(rt_nested_splits=[op.outputs[0]], rt_dense_values=grad,
                                    batched_input=True)]
    return res

initial_trace = np.trace(np.cov((model.layers[-2].weights[0].numpy())))

loss = []
for i in range(10):
    res = model.train_on_batch(data[:10], y_true[:10])
    print(f""Iter {i}, loss: {res}, delta: {initial_trace-res} "")
```
Returns
```
/home/data/venv/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""

Iter 0, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 1, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 2, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 3, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 4, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 5, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 6, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 7, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 8, loss: 9.99978256225586, delta: 1.1581866488086234e-07 
Iter 9, loss: 9.99978256225586, delta: 1.1581866488086234e-07 

```"
43104,"TF lite fails conversion with post train int quantization, converts fine without: RuntimeError: Mismatch between number of weight maxs and channels","Hey there, I've managed to convert StyleGan2 to a tf lite model with your recent advice, however now upon trying to quantize the model I'm getting an error. The model converts **with no issues** if quantization is not used. It also converts fine with dynamic range quantization, only failing if a **representative dataset** is used. I have replicated the issue on a small part of the full model here.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): tried on tf 2.3, tf nightly 

If possible, please share a link to Colab/Jupyter/any notebook.
**Link to colab notebook with all commands and downloads:**
https://colab.research.google.com/drive/1b-o3cPz_1er_EQC0Ezq-vsTmlH55kmAe?usp=sharing

**Link to SavedModel tar**
https://drive.google.com/file/d/1BdFA3CJ-uDIFrLWMTy9hAWPKTHWp9YCw/view?usp=sharing

**Link to successfully converted tf lite model without post-train quantization**
https://drive.google.com/file/d/157ujWxsnjAuR2tn5es5Oz3rRIYNwmiwO/view?usp=sharing

**Command used to run the converter or code if you’re using the Python API**
Takes in [1, 18, 512] tensor of random input:
```python
samples = []

for i in range(10):
    sample = np.random.randn(1, 18, 512)
    sample = sample.astype(np.float32)
    samples.append(sample)

def representative_data_gen():
    for sample in samples:
        yield [sample]
```

Converter code
```python
converter = tf.lite.TFLiteConverter.from_saved_model('/content/synth_const')
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model = converter.convert()

with tf.io.gfile.GFile('synth_const_opt.tflite', 'wb') as f:
    f.write(tflite_model)
```

**The output from the converter invocation**

```
2020-09-10 11:03:29.887482: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-10 11:03:30.014402: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-10 11:03:32.260056: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.260451: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.260987: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-09-10 11:03:32.261050: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-10 11:03:32.261693: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 1, reason: Invalid argument: Invalid device ordinal value (1). Valid range is [0, 0].
2020-09-10 11:03:32.319520: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.319815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-10 11:03:32.319865: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.320143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s
2020-09-10 11:03:32.320163: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-10 11:03:32.320177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-10 11:03:32.320188: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-10 11:03:32.320198: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-10 11:03:32.320208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-10 11:03:32.320218: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-10 11:03:32.320228: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-10 11:03:32.320263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.320548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.320844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.321124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.321394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
2020-09-10 11:03:32.321401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-10 11:03:32.321419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-10 11:03:32.321424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
2020-09-10 11:03:32.321428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N 
2020-09-10 11:03:32.321432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N 
2020-09-10 11:03:32.321489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.321776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.322041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9842 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-09-10 11:03:32.366684: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-09-10 11:03:32.366717: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 88 nodes (84), 89 edges (86), time = 31.342ms.
2020-09-10 11:03:32.366721: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.045ms.
2020-09-10 11:03:32.479552: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-09-10 11:03:32.479576: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2020-09-10 11:03:32.498762: I tensorflow/compiler/jit/xla_gpu_device.cc:161] Ignoring visible XLA_GPU_JIT device. Device number is 1, reason: Invalid argument: Invalid device ordinal value (1). Valid range is [0, 0].
2020-09-10 11:03:32.498945: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.499265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-10 11:03:32.499316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.499602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s
2020-09-10 11:03:32.499622: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-10 11:03:32.499636: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-10 11:03:32.499645: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-10 11:03:32.499654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-10 11:03:32.499663: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-10 11:03:32.499672: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-10 11:03:32.499681: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-10 11:03:32.499718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.500010: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.500314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.500599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.500966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
2020-09-10 11:03:32.500978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-10 11:03:32.501002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-10 11:03:32.501007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
2020-09-10 11:03:32.501011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N 
2020-09-10 11:03:32.501015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N 
2020-09-10 11:03:32.501084: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.501389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-10 11:03:32.501696: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9842 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/y4tsu/PycharmProjects/sg2_nhwc_reduced_cleanup/test_pieces.py"", line 187, in <module>
    main()
  File ""/home/y4tsu/PycharmProjects/sg2_nhwc_reduced_cleanup/test_pieces.py"", line 174, in main
    write_synth_const(g_params, convert)
  File ""/home/y4tsu/PycharmProjects/sg2_nhwc_reduced_cleanup/test_pieces.py"", line 81, in write_synth_const
    tflite_model = converter.convert()
  File ""/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1076, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 899, in convert
    return super(TFLiteFrozenGraphConverterV2,
  File ""/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 638, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 450, in _calibrate_quantize_model
    return calibrate_quantize.calibrate_and_quantize(
  File ""/home/y4tsu/anaconda3/envs/tf2_3/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 95, in calibrate_and_quantize
    return self._calibrator.QuantizeModel(
RuntimeError: Mismatch between number of weight maxs and channels: 1 vs 512
```

I think the Quantize op is expecting a shape that it's not receiving, but I've tried changing the dimensionality of tensors at various points and I haven't been able to identify the source of the issue. If you have any ideas please let me know!
"
43103,cuDNN error when passing all-masked sequences to RNN layers on GPU,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: CentOS 8
- TensorFlow installed from: binary (pip)
- TensorFlow version: 2.3.0 (v2.3.0-rc2-23-gb36436b087)
- Python version: 3.6.8
- CUDA/cuDNN version: 10.1 / 7
- GPU model and memory: RTX 6000 (24GB)

**Describe the current behavior**

When passing a boolean mask together with data to a RNN layer running on GPU, a cuDNN error is raised if any row of the mask is made entirely of False values, _i.e._ if one of the batched input sequences is made entirely of padding values.

The raised error is the following:
```
UnknownError: CUDNN_STATUS_BAD_PARAM
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1521): 'cudnnSetRNNDataDescriptor( data_desc.get(), data_type, layout, max_seq_length, batch_size, data_size, seq_lengths_array, (void*)&padding_fill)' [Op:CudnnRNNV3]
```

This may seem as an edge case, but can be encountered when reshaping a [batch, block, sequence, dimension] tensor with variable-size (therefore zero-padded) blocks of sequences representing e.g. a long text into a [..., sequence, dimension] tensor to be processed at once by a RNN and then reshaped back to [batch, block, rnn_dim].

I was able to implement a work-around using a custom wrapper class, but this does not feel like a righteous solution.

**Describe the expected behavior**

I would expect the RNN to output some default value (e.g. a vector or zeros) representing the all-padding sequence(s), as it does on CPU.

**Standalone code to reproduce the issue**

Minimal code to reproduce the error:

```python
import tensorflow as tf

gru = tf.keras.layers.GRU(128)
rng = tf.random.get_global_generator()
inp = rng.normal((8, 64, 256))
msk = tf.concat([tf.ones((4, 64), dtype=tf.bool), tf.zeros((4, 64), dtype=tf.bool)], axis=0)

# works
with tf.device('CPU:0'):
    gru(inp, mask=msk)
# works
with tf.device('GPU:0'):
    gru(inp, mask=tf.ones_like(msk))
# fails
with tf.device('GPU:0'):
    gru(inp, mask=msk)
```

Workaround I implemented, which ""unmasks"" all-padding sequences, thus triggering unrequired computations:
```python
import tensorflow as tf

class SafeRNN(tf.keras.layers.Wrapper):
    """"""Wrapper for keras RNN layers avoiding a mask-caused cuda error.""""""

    def call(self, inputs, mask=None, **kwargs):
        """"""Run inputs through the wrapped layer.""""""
        if mask is not None:
            valid = tf.reduce_any(mask, axis=1, keepdims=True)
            mask = tf.where(valid, mask, tf.ones_like(mask))
        return self.layer(inputs, mask=mask, **kwargs)

    def compute_mask(self, inputs, mask=None):
        """"""Return an output mask tensor.""""""
        if mask is None:
            return None
        return tf.reduce_any(mask, axis=1)

gru = SafeRNN(tf.keras.layers.GRU(128))
rng = tf.random.get_global_generator()
inp = rng.normal((8, 64, 256))
msk = tf.concat([tf.ones((4, 64), dtype=tf.bool), tf.zeros((4, 64), dtype=tf.bool)], axis=0)

# works
with tf.device('GPU:0'):
    gru(inp, mask=msk)
```

**Other info**

This issue is not consistent from a system to the other; it does not trigger on my Linux Mint 19.1 system, with the same Python, TensorFlow and CUDA/cuDNN versions (but distinct GPU: Quadro P1000)."
43102,Huge size difference of GPU delegate library between static and dynamic,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.6
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10.4
- GPU model and memory: GTX 1050
**Describe the current behavior**
So I tried to use bazel to build the gpu delegate library as written in https://www.tensorflow.org/lite/performance/gpu_advanced
`bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:delegate                           # for static library`
`bazel build -c opt --config android_arm64 tensorflow/lite/delegates/gpu:libtensorflowlite_gpu_delegate.so  # for dynamic library`

The resulting dynamic library .so is 106 MB and static library .a is 1.1 MB. Why their size are so different? I need to use the dynamic library, however, 106 MB RAM will be used only for loading this dynamic library. Is there any way to reduce the size of the dynamic library?

Thanks!

"
43101,Graph mode failure with mask-fed RNN layers within a train_step loop on GPU.,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Linux Mint 19.1 & CentOS 8
- TensorFlow installed from: binary (pip)
- TensorFlow version: 2.2.0 (v2.2.0-rc4-8-g2b96f3662b)
- Python version: 3.6.9
- CUDA/cuDNN version: 10.1 / 7
- GPU model and memory: Quadro P1000 (4GB) & RTX 6000 (24GB)

**Describe the current behavior**

Model training fails in graph mode when the custom `tf.keras.Model.train_step` involves both:
* a loop (except if iterating over an int and not a tensor)
* a `tf.keras.layers.RNN` inheriting layer
* the passing of a `mask` to said layer (whether implicitly based on previous layer's `compute_mask` output or manually passing a deterministic mask, even an whole-true one)

The error raised has the following format:
```
InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  InstantiateOptions.input_devices must have the same length as the number of arguments: input_devices length = 49 number of arguments = 50
	 [[{{node while/body/_1/StatefulPartitionedCall}}]]
	 [[while/exit/_59/_46]]
  (1) Invalid argument:  InstantiateOptions.input_devices must have the same length as the number of arguments: input_devices length = 49 number of arguments = 50
	 [[{{node while/body/_1/StatefulPartitionedCall}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_<some_id_number>]

Function call stack:
train_function -> train_function
```

It appears to be raised on the second call to `self._get_gradients`, _i.e._ when entering the loop within the custom `train_step` and passing inputs to the model for the second time.

Unresolved, automatically-closed issue [#39827](https://github.com/tensorflow/tensorflow/issues/39827) appears to be similar to mine.

I hereby provide a minimal example to reproduce this issue, which is arguably an edge-case but has caused me quite some trouble. In this example, I implement some gradient stacking as part of the custom training step, in a non-general way for the sake of simplicity. Note that in this example, since the number of substeps is fixed, the loop could be rewrote as a Python `for` loop over an `int`; then, the issue does not arise. In real life though, I am using a generalized gradient stacking implementation that requires iterating over tensors (hence my providing here a `tf.while_loop` based example).

**Describe the expected behavior**

I would expect all four tested cases to run without triggering an exception (as they do in 2.3 or on CPU), so that proper masking may be used with RNNs within gradient stacking training loops in graph mode, which is the most desirable setting in terms of both model correctness and code optimization.

**Standalone code to reproduce the issue**

The following script will raise the issue if run on GPU with TF2.2.
Using a GRU layer and/or a Bidirectional wrapper does not alter the issue.
Note that is will _not_ raise the issue if run on CPU or with TF2.3.

```python
# coding: utf-8

""""""Minimal example script for an RNN issue within custom train loops.""""""

import numpy as np
import tensorflow as tf


class StackedModel(tf.keras.Model):
    """"""Minimal gradient stacking example Model subclass.""""""

    def train_step(self, data):
        # NOTE: here we assume data is a single (x, y) tuple
        #       in order to provide with a minimal example
        inputs, y_true = data
        size = tf.shape(inputs)[0] // 4
        # Compute gradients on the batch's first quarter.
        gradients = self._get_gradients(inputs[:size], y_true[:size])
        # Define a process to compute and stack gradients.
        def process_quarter(idx, gradients):
            """"""Compute gradients on a data sub-batch and stack them.""""""
            grads_loc = self._get_gradients(
                inputs[idx * size:(idx + 1) * size],
                y_true[idx * size:(idx + 1) * size]
            )
            gradients = [
                self._add_gradients(a, b) for a, b in zip(gradients, grads_loc)
            ]
            return tf.add(idx, 1), gradients
        # Iteratively process the remaining data quarters using the former.
        _, gradients = tf.while_loop(
            cond=lambda idx, _: tf.math.less(idx, 4),
            body=process_quarter,
            loop_vars=[tf.constant(1), gradients],
            parallel_iterations=1
        )
        # Apply the aggregated gradients.
        grads_and_vars = zip(gradients, self.trainable_variables)
        self.optimizer.apply_gradients(grads_and_vars)
        # Return the current values of the loss and metrics.
        return {m.name: m.result() for m in self.metrics}

    def _get_gradients(self, inputs, y_true):
        """"""Compute gradients for given (x, y) data.""""""
        with tf.GradientTape() as tape:
            y_pred = self(inputs, training=True)
            loss = self.compiled_loss(y_true, y_pred)
        return tape.gradient(loss, self.trainable_variables)

    @staticmethod
    def _add_gradients(grad_a, grad_b):
        """"""Return the sum of two gradient objects (Tensor of IndexedSlices).""""""
        if not isinstance(grad_b, type(grad_a)):
            raise TypeError(""Trying to add objects of distinct types."")
        if isinstance(grad_a, tf.Tensor):
            return tf.add(grad_a, grad_b)
        if isinstance(grad_a, tf.IndexedSlices):
            values = tf.concat([grad_a.values, grad_b.values], axis=0)
            indices = tf.concat([grad_a.indices, grad_b.indices], axis=0)
            return tf.IndexedSlices(values, indices, grad_a.dense_shape)


def build_example_model(run_eagerly, avoid_mask):
    """"""Return a keras Model for binary classification of tokens sequences.

    This model expects an input batch of tokens, with zero values
    being treated as padding, and thus masked. An embedding layer
    encodes the tokens into vectors in R^{128}, then a LSTM layer
    produces sequence-wise vectors in R^{128}, which are finally
    transformed into binary probabilities by a dense layer.
    """"""
    inputs = tf.keras.Input((None,), dtype=tf.int32)
    emb = tf.keras.layers.Embedding(
        input_dim=200,
        output_dim=128,
        mask_zero=True
    )
    rnn = tf.keras.layers.LSTM(128)
    out = tf.keras.layers.Dense(2, 'softmax')
    embedding = emb(inputs)
    if avoid_mask:
        embedding = rnn(embedding, mask=None)
    else:
        embedding = rnn(embedding)  # mask is passed implicitly
    model = StackedModel(inputs, out(embedding))
    model.compile(loss='binary_crossentropy', run_eagerly=run_eagerly)
    return model


def build_example_dataset():
    """"""Return a tf.data.Dataset of batched right-padded tokens sequences.""""""
    # Define a random tokens sequences generator.
    def generator():
        """"""Yield sequences of 8 to 32 random ints in (1, 200(, plus a label.""""""
        sizes = 8 + np.random.choice(24, size=640, replace=True)
        for i in range(640):
            seq = 1 + np.random.choice(199, size=sizes[i], replace=True)
            lab = tf.one_hot(np.random.choice(2), depth=2)
            yield (seq, lab)
    # Set up and return a Dataset made of batches of 32 padded sequences.
    dst = tf.data.Dataset.from_generator(
        generator,
        output_shapes=((None,), (2,)),
        output_types=(tf.int32, tf.float32)
    )
    return dst.padded_batch(32, padded_shapes=((None,), (2,)))


def main():
    """"""Minimal demonstration script.""""""
    dst = build_example_dataset().repeat()
    print('Running eagerly without masking at LSTM.')
    model = build_example_model(run_eagerly=True, avoid_mask=True)
    model.fit(dst, steps_per_epoch=20, epochs=3)
    print('Running eagerly with masking at LSTM.')
    model = build_example_model(run_eagerly=True, avoid_mask=False)
    model.fit(dst, steps_per_epoch=20, epochs=3)
    print('Running in graph mode without masking at LSTM.')
    model = build_example_model(run_eagerly=False, avoid_mask=True)
    model.fit(dst, steps_per_epoch=20, epochs=3)
    print('Running in graph mode with masking at LSTM -- prepare for failure.')
    model = build_example_model(run_eagerly=False, avoid_mask=False)
    model.fit(dst, steps_per_epoch=20, epochs=3)


if __name__ == '__main__':
    main()
```

**Other info**

The issue I raise here appears to have been fixed in TensorFlow 2.3.0; since I realized that after having spent a few hours tracking down the bug, revising my code and eventually implementing this test case, I still thought that it would be worth reporting. I would like to have someone confirm whether this has properly been fixed in 2.3, and if possible I would be interested in some insight as to the issue's initial cause and solving. I am also wondering whether the fix would be worth backporting to TF 2.2 (e.g. as a version 2.2.1) as updating custom code from 2.2 to 2.3 can require a limited yet non-anecdotal effort."
43100,Have similar default values for min_delta in ReduceLROnPlateau and EarlyStopping callbacks,"The min_delta default values are 1e-4 for the ReduceLROnPlateau callback and 0 for the EarlyStopping callback. 

This leads to cases in which training is stopped before the learning rate is reduced even though the patience parameter is higher for EarlyStopping, when the min_delta value is not set explicitly. 

"
43099,Allow symmetric TFLite quantization (no zero point/scale only) ,"As far as I know, TFLite's quantization forces activations to have both scales and zero points. However, for some networks, symmetric quantization (no zero point) does not cause a significant loss in accuracy. It is therefore sufficient to use scale only. Please add support for symmetric quantization. "
43097,Understanding the Difference between Normal Training vs eager mode training in TFOD API v2,"Eager mode allowed you to see tensor values when TF 1 used sessions and graphs but since eager mode is the default in tf 2.0 so its a little confusing.

I want to know the differences between the normal training vs Eager mode training.  One clear difference I can see is that the code is a lot different for both of them. Even the final data format is not in tfrecords in eager mode.  And the code is bigger.

**I have the following questions:**

- So is the Eager Mode Method Faster (Since it can train a really basic detector in just 5 minutes and the normal method takes just a few minutes to initialize)?

- If the params of pipeline.config are not changed then will the final model have the same accuracy regardless of which method was used (normal or eager) and again will the training time be different? 

- What's the advantage of eager mode vs normal training, is it more flexibility in training?

- Going forward what method will you recommend new users to use? and when will you prefer one method over the other?

_Thanks, would really appreciate if these questions can be answered as I can't find any docs that address these questions._"
43096,dense_to_ragged_batch fails with map function implemented with py_function,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
v2.3.0-rc2-23-gb36436b087 2.3.0

**Describe the current behavior**

dense_to_ragged_batch fails when I use a map function implemented with py_function.

**Describe the expected behavior**

dense_to_ragged_batch should work no matter which map_function I use.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3

import numpy as np;
import tensorflow as tf

def map_function(x):

  image, bbox, label = tf.py_function(map_function_impl, inp = [x], Tout = [tf.float32, tf.float32, tf.int32]);
  return image, bbox, label;

def map_function_impl(x):

  image = np.random.normal(size = (416, 416, 3));
  num_target = np.random.randint(low = 0, high = x, size = ());
  bbox = np.random.normal(size = (num_target ,4));
  label = np.random.randint(low = 0, high = 10, size = (num_target,));
  return image, bbox, label;

def main():

  dataset = tf.data.Dataset.from_tensor_slices(np.random.randint(low = 3, high = 10, size = (6,)));
  dataset = dataset.map(map_function);
  print(dataset.element_spec[0].shape);
  print(dataset.element_spec[1].shape);
  print(dataset.element_spec[2].shape);
  dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(batch_size = 2));
  for batch in dataset:
    print(batch);

if __name__ == ""__main__"":

  main();
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43095,Input_c/Input_h shape in Cudnn LSTMP,"Hi, I'm using Cudnn LSTMP as implemented here: [https://github.com/tensorflow/tensorflow/blob/v1.15.0/tensorflow/contrib/cudnn_rnn/python/ops/cudnn_rnn_ops.py#L1848](url).

I have a question about the shapes of **input_h** and **input_c** in the `__call__` method. Why the shape is `[n_layers, batch_size, n_proj]` for both shapes. In LSTM cell, `h` is of shape `[n_layers, batch_size, n_proj]` while **c** is of shape `[n_layers, batch_size, n_units]`. I would really be appreciated if you could explain."
43094,batch training with model.fit not working for all batch_sizes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    Yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    Windows

- TensorFlow installed from (source or binary):
    binary via anaconda

- TensorFlow version (use command below): 
    2.1.0

- Python version: 3.7.9



**Describe the current behavior**
When the number of samples in the training set is not equal to a factor of the batch size, model.fit will throw an error. On 2.1.0, I get an error saying that the shapes of two operators are incompatible. Not sure if this is the expected behavior or not. 

```
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [32,1] vs. [4,1]
         [[node mul_5 (defined at .\classify.py:31) ]] [Op:__inference_distributed_function_435]

Function call stack:
distributed_function
```

It seems like when the function is creating the last batch in the epoch, it doesn't have enough samples remaining to create a full batch, as a result, it builds what it can then errors out. The size of my dataset is 1763, not exactly a neat number. The only other method I have to implement minibatch training is to split the dataset into batches myself and train manually, without model.fit. Like I said, if this is the expected behavior, ignore this probably, but it seems like a hassle. Especially if the size of someone's dataset was a prime number, in which case they would be restricted to training either with a batch size of 1 or the size of their full dataset, which seems inconvenient.


**Describe the expected behavior**
The expected behavior for me would be for model.fit to split the data into batches without running out of room on the last batch.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential, optimizers, losses
from tensorflow.keras.layers import Input, Dense, Flatten



def make_model(input_shape, output_shape, batch_size=32):
    model = Sequential()
    model.add(Input(shape=input_shape, batch_size=batch_size, name='input'))
    
    model.add(Flatten())
    model.add(Dense(output_shape))
    model.build((batch_size, *input_shape))
    return model
    

if __name__ == '__main__':
    batch_size = 32
    input_size = (5, 5)
    output_size = 1
    num_samples = 100
    model = make_model(input_size, output_size, batch_size=batch_size)
    model.compile(optimizer=optimizers.Adam(), loss=losses.SparseCategoricalCrossentropy(from_logits=True))       
    X = np.ones((num_samples, *input_size))
    y = np.zeros(num_samples)
    model.fit(X, y, batch_size=batch_size, epochs=10)

```
Here's a link to a Colab notebook as well. It seems like in more recent version of tensorflow, the error is still there, but the exact error that comes up is slightly different. 
https://colab.research.google.com/drive/1RSOIKMYFAlg2jfPaljkosBpQyhaFbQNy?usp=sharing
"
43093,RaggedTensor support for model output,"**System information**
- TensorFlow version: 2.3.0
- Are you willing to contribute it (Yes/No): No



**Current behavior/state.**
Loss function is implemented in a way that tries to convert `y_true` and `y_pred` to Tensor. That means the output of a model can't be a RaggedTensor, otherwise error shows up as RaggedTensor can't be converted to Tensor.

**Will this change the current api? How?**
Line 62 of `/tensorflow/python/ops/confusion_matrix.py`: `predictions = ops.convert_to_tensor(predictions)`.
Line 63 of `/tensorflow/python/ops/confusion_matrix.py`: `labels = ops.convert_to_tensor(labels)`.
There should be an option to work with `labels` and `predictions` as RaggedTensor instead of just converting it to Tensor.

**Who will benefit with this feature?**
Sequence-to-sequence autoencoders with RaggedTensor inputs and outputs or models with RaggedTensor outputs in general will benefit.
"
43092,"OP_REQUIRES failed at reshape_op.h:57 : Invalid argument: Size 1 must be non-negative, not -9","1.environment: windows10, cuda10.1, cudnn7.6.5 and tensorflow2.2.0

2.my problem:
i wanted to save my total model with tf.saved_model.save , and i used tf.squeeze and tf.reshape together. Then, when i used `model.signatures[""serving_default""](inputs)` some bugs happened.
**But, if i used tf.keras.models.load_model('./saved_model') to save model, everything is ok when predicting.**

3.part of my codes:
```python
# save model
raw_prediction = tf.squeeze(raw_prediction)
shape = tf.shape(raw_prediction)
height = shape[0]
width = shape[1]
shape_float = tf.cast(shape[0:2], dtype=tf.float32)
raw_prediction = keras.backend.reshape(raw_prediction, shape=(height, width, num_anchors, -1))

# predict
model = tf.saved_model.load('./saved_model')
model.signatures[""serving_default""](inputs)  # bugs happen when i predicted
```

4.bugs:
```
 OP_REQUIRES failed at reshape_op.h:57 : Invalid argument: Size 1 must be non-negative, not -9
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument:  Size 1 must be non-negative, not -9
	 [[{{node StatefulPartitionedCall/model_1/tf_op_layer_RealDiv_14/RealDiv_14-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]]
	 [[StatefulPartitionedCall/model_1/tf_op_layer_GatherV2_5/GatherV2_5/_18]]
  (1) Invalid argument:  Size 1 must be non-negative, not -9
	 [[{{node StatefulPartitionedCall/model_1/tf_op_layer_RealDiv_14/RealDiv_14-1-ReshapeNHWCToNCHW-LayoutOptimizer}}]]
0 successful operations.
0 derived errors ignored. [Op:__inference_signature_wrapper_10761]

Function call stack:
signature_wrapper -> signature_wrapper
```"
43090,LSTM Issue ,"Hello, I am trying to implement an LSTM with input (123,45,4) and output (123,45,1) with a sequence of 4 integers as the input and a single number as the output. I am using Mac OS, Google Colab, and TF version 2.3.0. 

Here is my model:

```
def define_models(n_input, n_output, n_units):
	# define training encoder
	encoder_inputs = Input(shape=(None, n_input))
	encoder = LSTM(n_units, return_state=True)
	encoder_outputs, state_h, state_c = encoder(encoder_inputs)
	encoder_states = [state_h, state_c]
	# define training decoder
	decoder_inputs = Input(shape=(None, n_output))
	decoder_lstm = LSTM(n_units, return_sequences=True, return_state=True)
	decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)
	decoder_dense = Dense(n_output, activation='softmax')
	decoder_outputs = decoder_dense(decoder_outputs)
	model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
	# define inference encoder
	encoder_model = Model(encoder_inputs, encoder_states)
	# define inference decoder
	decoder_state_input_h = Input(shape=(n_units,))
	decoder_state_input_c = Input(shape=(n_units,))
	decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
	decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)
	decoder_states = [state_h, state_c]
	decoder_outputs = decoder_dense(decoder_outputs)
	decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)
	# return all models
	return model, encoder_model, decoder_model
```


When I try running the code: model.fit(x_train, y_train, epochs = 50) I get the error:     **AssertionError: Could not compute output Tensor(""dense_2/truediv:0"", shape=(None, None, 1), dtype=float32).** Does anyone know how to fix this?

Here is the code to reproduce the issue:

Load Data:
```
with open(""training_data_input.txt"") as fopen:
  with open(""training_data_output.txt"") as fopen2:
    for line in fopen:
      myList = line.strip().split()
      myList[0] = myList[0].replace(""["","""")
      if myList[0] == """":
        myList = myList[1:]
      if ""]["" in myList[3]:
        j = 0
        print(myList[3])
        myList[3] = myList[3].replace(']][[',"""")
        if len(myList[3]) > 3:
          myList[3] = (myList[3][:3])
        myList = myList[:4]
      myList[len(myList)-1] = myList[len(myList)-1].replace(""]"","""")
      x = np.empty((154,45,4),dtype=np.float32)
      i = 0
      j = 0
      if j >=45:
        j = 0
      print(myList)
      x[i][j] = myList
      i+=1
      j+=1
    for line in fopen2:
      myList = line.strip().split()
      x_out = np.empty((154,45,1), dtype=np.float32)
      myList[0] = myList[0].replace(""["","""")
      if myList[0] == """":
        myList = myList[1:]
      if ""]["" in myList[0]:
        j = 0
        myList[0] = myList[0].replace(']][[',"""")
        if len(myList[0]) > 3:
          myList[0] = (myList[0][:2])
        myList = myList[:1]
      myList[len(myList)-1] = myList[len(myList)-1].replace(""]"","""")
      i = 0
      j = 0
      if j >=45:
        j = 0
      x_out[i][j] = myList
      i+=1
      
print(x.shape)
print(x_out.shape)
```

Train Model:

```

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, x_out, test_size = 0.2, random_state = 4)
print(x_train.shape)
print(y_train.shape)

model.fit(x_train, y_train, epochs = 50)
```

The input data:
[training_data_input.txt](https://github.com/tensorflow/tensorflow/files/5198490/training_data_input.txt)
[training_data_output.txt](https://github.com/tensorflow/tensorflow/files/5198491/training_data_output.txt)

"
43089,Link doesn't work.,"Link for _TensorFlow graph optimization with Grappler_ from url https://www.tensorflow.org/guide/tf_numpy#performance_comparisons does not exist.
The link points to page https://www.tensorflow.org/guide/guide/graph_optimization which give Page not found error"
43087,tflite_runtime Interpreter import error,"OS: Ubuntu 16.04

Hi, there. I'm trying to use tflite_runtime Interpreter from [this docs](https://www.tensorflow.org/lite/guide/python) , but unfortunately I have an import error, when trying to use this package after installation.
I have tested all ""supported"" versions of python from this doc, and installed tflite_runtime package for each versions of pythons from 3.5 to 3.8.
My code is:
```python
import tflite_runtime.interpreter as tflite
model = tflite.Interpreter(model_path=""/path/to/model/my_model.tflite"")
model.get_input_details()
```
It only works at python 3.5. But it's unacceptable for me, because I need python 3.7 version.
All other versions of python, except 3.5 give me the same error.
For example:
Python 3.7.4 error:
```bash
Python 3.7.4 (default, Aug 13 2019, 20:35:49) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tflite_runtime.interpreter as tflite
Traceback (most recent call last):
  File ""/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter_wrapper.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""/home/bocharick/anaconda3/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 1006, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 983, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 967, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 670, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 583, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 1043, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/_interpreter_wrapper.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter.py"", line 46, in <module>
    from tflite_runtime import interpreter_wrapper as _interpreter_wrapper
  File ""/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter_wrapper.py"", line 17, in <module>
    _interpreter_wrapper = swig_import_helper()
  File ""/home/bocharick/anaconda3/lib/python3.7/site-packages/tflite_runtime/interpreter_wrapper.py"", line 16, in swig_import_helper
    return importlib.import_module('_interpreter_wrapper')
  File ""/home/bocharick/anaconda3/lib/python3.7/importlib/__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_interpreter_wrapper'
>>> 
```

Python 3.6.12 error:
```python
Python 3.6.12 (default, Sep 10 2020, 03:15:05) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tflite_runtime.interpreter as tflite
Traceback (most recent call last):
  File ""/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter_wrapper.py"", line 14, in swig_import_helper
    return importlib.import_module(mname)
  File ""/home/bocharick/Downloads/Python-3.6.12/Lib/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""<frozen importlib._bootstrap>"", line 994, in _gcd_import
  File ""<frozen importlib._bootstrap>"", line 971, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 955, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 658, in _load_unlocked
  File ""<frozen importlib._bootstrap>"", line 571, in module_from_spec
  File ""<frozen importlib._bootstrap_external>"", line 922, in create_module
  File ""<frozen importlib._bootstrap>"", line 219, in _call_with_frames_removed
ImportError: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.27' not found (required by /home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/_interpreter_wrapper.so)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter.py"", line 46, in <module>
    from tflite_runtime import interpreter_wrapper as _interpreter_wrapper
  File ""/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter_wrapper.py"", line 17, in <module>
    _interpreter_wrapper = swig_import_helper()
  File ""/home/bocharick/.local/lib/python3.6/site-packages/tflite_runtime/interpreter_wrapper.py"", line 16, in swig_import_helper
    return importlib.import_module('_interpreter_wrapper')
  File ""/home/bocharick/Downloads/Python-3.6.12/Lib/importlib/__init__.py"", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
ModuleNotFoundError: No module named '_interpreter_wrapper'
>>>
```

So as I see, it's `GLIBC_2.27`  problem. And what I must to do, if I have Ubuntu 16.04 with `GLIBC_2.23`, but I need to use python3.6+???"
43086,Issue on Load Trax,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 18.04
- Docker container: tensorflow/tensorflow:latest-devel-gpu (sha256:81b1629c567b638dbc197aa24aa7aaa22b84ae88d1bb6c963fa7e3d970cbe77e)
- CUDA/cuDNN version: Cuda compilation tools, release 11.0, V11.0.221
Build cuda_11.0_bu.TC445_37.28845127_0
- GPU model and memory: nvidia gtx 1060 6gb

**Describe the current behavior**
I have installed jupyter in the tensorflow/tensorflow:latest-devel-gpu  docker image, I'm running the following files:

```
!pip install --upgrade jax
!pip install --upgrade jaxlib
!pip install --upgrade trax

!pip install --upgrade 'https://storage.googleapis.com/jax-releases/cuda110/jaxlib-0.1.51-cp36-none-manylinux2010_x86_64.whl'

!pip install --upgrade jax
```

```
from jax.lib import xla_bridge
print(xla_bridge.get_backend().platform)
```
But when I run this code, I'm getting the following error (The GPU is not loading):
`E external/org_tensorflow/tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)`

**Describe the expected behavior**
When I run the code with the docker image tensorflow/tensorflow:nightly-gpu-jupyter everything is working as expected.
"
43085, TF 2.3 ImportError: DLL load failed: The specified module could not be found. ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source?
- TensorFlow version: 2.3
- Python version: 3.6
- Installed using virtualenv? pip? conda? pip
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): na
- CUDA/cuDNN version: 10.1
- GPU model and memory: cpu

I run this code 

import tensorflow as tf
print(tf.__version__)

and this error log appears

Traceback (most recent call last):
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found. 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:/Christian/ML/main.py"", line 1, in <module>
    import tensorflow as tf
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\Christian\ML\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: Das angegebene Modul wurde nicht gefunden.

Failed to load the native TensorFlow runtime."
43084,"""Failed starting model allocation."" while trying to allocate tensors","**System information**
Hello everyone,
I am working on a project to implement a prediction algorithm, which is to be implemented for a microcontroller, based on an LSTM network. Therefore, I have not installed the complete library, instead I have downloaded the most recent repository and I have been selecting the files that are useful for my purpose(one by one). For the beginning I am building the c++ project on ""S32 Design Studio"" and running it on the laptop with Windows 10(haven't started to compile on microcontroller yet)

**the tf keras model which was as below: **
```
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(batch_input_shape=(1,6, 3), name='input'),
    tf.keras.layers.LSTM(n_neurons, time_major=False, return_sequences=False),
    tf.keras.layers.Dense(1, activation=tf.keras.activations.sigmoid, name='output')
])
model.compile(loss='mean_squared_error', optimizer='adam')
```
and I have converted it without any problem. This is the output of conversion :
```
run_model = tf.function(lambda x: model(x))
# This is important, let's fix the input size.
BATCH_SIZE = 1
STEPS = time_ev
INPUT_SIZE = 3
concrete_func = run_model.get_concrete_function(
    tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], model.inputs[0].dtype))

# model directory.
MODEL_DIR = ""keras_lstm""
model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)

converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)

```
```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/tracking.py:109: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
INFO:tensorflow:Assets written to: keras_lstm/assets
```
Then I have downloaded the model as a .cc file because I could not use the function to read .tflite file: 

```
# Define paths to model files
import os

MODELS_DIR = 'models/'
if not os.path.exists(MODELS_DIR):
    os.mkdir(MODELS_DIR)
MODEL_TF = MODELS_DIR + 'model.pb'
MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'
MODEL_TFLITE = MODELS_DIR + 'model.tflite'
MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'

# # Save the model to disk
open(MODEL_NO_QUANT_TFLITE, ""wb"").write(tflite_model)

# Install xxd if it is not available
!apt-get update && apt-get -qq install xxd
# Convert to a C source file
!xxd -i {MODEL_NO_QUANT_TFLITE} > {MODEL_TFLITE_MICRO}
# Update variable names
REPLACE_TEXT = MODEL_NO_QUANT_TFLITE.replace('/', '_').replace('.', '_')
!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}

files.download(""models/model.cc"") 
```

Finally I have this small script just to import the model and build the interpreter:

```
#include <string.h>

#include ""tensorflow/lite/micro/kernels/micro_ops.h""
#include ""tensorflow/lite/micro/micro_error_reporter.h""
#include ""tensorflow/lite/micro/micro_interpreter.h""
#include ""tensorflow/lite/micro/micro_mutable_op_resolver.h""
#include ""tensorflow/lite/micro/all_ops_resolver.h""
#include ""tensorflow/lite/version.h""
#include ""model.h""
#include ""tensorflow/lite/micro/micro_optional_debug_tools.h""

// Globals, used for compatibility with Arduino-style sketches.
namespace {
tflite::ErrorReporter* error_reporter;
//const tflite::Model* model ;
tflite::MicroInterpreter* interpreter = nullptr;

// Create an area of memory to use for input, output, and intermediate arrays.
// Minimum arena size, at the time of writing. After allocating tensors
// you can retrieve this value by invoking interpreter.arena_used_bytes().
constexpr size_t allocator_buffer_size = 2096 /* optimal arena size at the time of writting. */
			+ 16 /* alignment */ + 100 /* some headroom */;

uint8_t allocator_buffer[allocator_buffer_size];
} // namespace

int main() {

	const tflite::Model* model = ::tflite::GetModel(g_model);

	if (model->version() == TFLITE_SCHEMA_VERSION) {
		puts(""Model provided is supported.\n"");
		//fprintf(stderr, ""Model provided is supported.\n"");

	}

	tflite::MicroErrorReporter micro_error_reporter;
	error_reporter = &micro_error_reporter;

	static tflite::AllOpsResolver resolver;

         static tflite::MicroInterpreter static_interpreter(model, resolver, allocator_buffer, allocator_buffer_size, error_reporter);

         interpreter = &static_interpreter;
         interpreter->AllocateTensors();
         printf(""Interpreter has %d tensors and %d nodes\n"",
                 interpreter->tensors_size(), interpreter->operators_size());
	return 0;

}
```

When I run the code, I obtain this in the console : 

```
Model provided is supported.

Interpreter has 0 tensors and 4 nodes

Didn't find op for builtin opcode 'UNIDIRECTIONAL_SEQUENCE_LSTM' version '1'


Failed to get registration from op code UNIDIRECTIONAL_SEQUENCE_LSTM
 

Failed starting model allocation.
```
Can anybody help me ? Is this network not supported ? "
43083,"tf.data.experimental.make_csv_dataset, currently only supports one label; However, most datasets can have more than one label","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

test_dataset = tf.data.experimental.make_csv_dataset(
    test_fp,
    batch_size,
    column_names=column_names,
    label_name='species', M=<-- Currenly can only pass one label
    num_epochs=1,
    shuffle=False)



**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
tf.data.experimental.make_csv_dataset only accepts one label, to create the dataset. 

**Will this change the current api? How?**
API, needs to accept list of strings as label_name or a string containing delimiters for each column name as label_name

**Who will benefit with this feature?**
The community will benefit, by being able to generate more than one output label to the dataset, Otherwise, the API is useless for more than one output network
**Any Other info.**
See issue #43074 for sample test case.
"
43082,List of tensor names in graph in Tensorflow,"I got a snippet of code for [List of tensor names in graph in Tensorflow](https://stackoverflow.com/questions/35336648/list-of-tensor-names-in-graph-in-tensorflow)

```
import tensorflow as tf
a = tf.Variable(5)
b = tf.Variable(6)
c = tf.Variable(7)
d = (a + b) * c

for i in tf.get_default_graph().get_operations():
    print (i.name)
```

but under tf2.x this ends with error:

```
2020-09-09 18:24:25.626596: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
Traceback (most recent call last):
  File ""c:/DATA/Projects/AITEC/unet/ops.py"", line 7, in <module>
    for i in tf.get_default_graph().get_operations():
AttributeError: module 'tensorflow' has no attribute 'get_default_graph'

```

can you advise on this?"
43081,[BodyPix] Improve body segmentation(flickering edges) updates for a video stream ,"This is related to body segmentation using body-pix.
body segmentation has a flickering effect around the edges for a video stream.
Even setting edgeBlurAmount to maximum for the bokeh effect still had flickering effect around the edges since we are continuously trying to predict, segment and update frames from a video. 
Had similar flickering around the edges when i get image data from the segmentation and write it to a canvas. Is there any way to minimize/reduce the flickering around the edges?

**System information**
- TensorFlow version (you are using): tfjs@2.1.0, body-pix@2.0.5

- Are you willing to contribute it (Yes/No): 
Yes, I'm willing to help with whatever i can. But,  I'm not familiar with tensorflow models though.

**Describe the feature and the current behavior/state.**
This is related to body segmentation using body-pix.
body segmentation has a flickering effect around the edges for a video stream.
Even setting edgeBlurAmount to maximum for the bokeh effect still had flickering effect around the edges since we are continuously trying to predict, segment and update frames from a video. 
Had similar flickering around the edges when i get image data from the segmentation and write it to a canvas. Is there any way to minimize/reduce the flickering around the edges?

**Will this change the current api? How?**
Not sure. Might change current api if additional masks are added.

**Who will benefit with this feature?**
Everyone in the community who uses body-pix model.

**Any Other info.**
"
43080,Support INT16 quantisation for RESIZE_NEAREST_NEIGHBOR,"Per issue #43064 I'm requesting the RESIZE_NEAREST_NEIGHBOR of int16 implementation.

`RuntimeError: Quantization to 16x8-bit not yet supported for op: 'RESIZE_NEAREST_NEIGHBOR'.` 

Thanks."
43076,Reproduce internal CI errors via TF Lite Micro Makefile,"@tensorflow/micro

See [this comment](https://github.com/tensorflow/tensorflow/pull/42452#issuecomment-689199971) for an internal error, due to -Wswitch that we could not reproduce via the TFLM Makefile.

The underlying issue there was a missing -Wswitch in the makefile and an inability to build without -DTF_LITE_STATIC_MEMORY."
43075,install,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43074, tf.data.experimental.make_csv_dataset seems to fail on this dataset; https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Used sample code using api given in tensorflow 2.0 (as opposed to using a stock example script provided in TensorFlow):
- colab:
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow present in the colab, version 2.3.0:

**Standalone code to reproduce the issue**

The data set was downloaded from https://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring and the following was used to read the dataset, that resulted in an error:
Sample data:
subject#,age,sex,test_time,motor_UPDRS,total_UPDRS,Jitter(%),Jitter(Abs),Jitter:RAP,Jitter:PPQ5,Jitter:DDP,Shimmer,Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,Shimmer:APQ11,Shimmer:DDA,NHR,HNR,RPDE,DFA,PPE
1,72,0,5.6431,28.199,34.398,0.00662,3.38e-005,0.00401,0.00317,0.01204,0.02565,0.23,0.01438,0.01309,0.01662,0.04314,0.01429,21.64,0.41888,0.54842,0.16006
1,72,0,12.666,28.447,34.894,0.003,1.68e-005,0.00132,0.0015,0.00395,0.02024,0.179,0.00994,0.01072,0.01689,0.02982,0.011112,27.183,0.43493,0.56477,0.1081
1,72,0,19.681,28.695,35.389,0.00481,2.462e-005,0.00205,0.00208,0.00616,0.01675,0.181,0.00734,0.00844,0.01458,0.02202,0.02022,23.047,0.46222,0.54405,0.21014
1,72,0,25.647,28.905,35.81,0.00528,2.657e-005,0.00191,0.00264,0.00573,0.02309,0.327,0.01106,0.01265,0.01963,0.03317,0.027837,24.445,0.4873,0.57794,0.33277
1,72,0,33.642,29.187,36.375,0.00335,2.014e-005,0.00093,0.0013,0.00278,0.01703,0.176,0.00679,0.00929,0.01819,0.02036,0.011625,26.126,0.47188,0.56122,0.19361
1,72,0,40.652,29.435,36.87,0.00353,2.29e-005,0.00119,0.00159,0.00357,0.02227,0.214,0.01006,0.01337,0.02263,0.03019,0.009438,22.946,0.53949,0.57243,0.195
1,72,0,47.649,29.682,37.363,0.00422,2.404e-005,0.00212,0.00221,0.00637,0.04352,0.445,0.02376,0.02621,0.03488,0.07128,0.01326,22.506,0.4925,0.54779,0.17563
1,72,0,54.64,29.928,37.857,0.00476,2.471e-005,0.00226,0.00259,0.00678,0.02191,0.212,0.00979,0.01462,0.01911,0.02937,0.027969,22.929,0.47712,0.54234,0.23844
1,72,0,61.669,30.177,38.353,0.00432,2.854e-005,0.00156,0.00207,0.00468,0.04296,0.371,0.01774,0.02134,0.03451,0.05323,0.013381,22.078,0.51563,0.61864,0.20037

The date was sucessfuly read into data set using:
types=[tf.int32,tf.int32,tf.int32,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64,tf.float64]
adataset = tf.data.experimental.CsvDataset(data, types, header=True)

However, when attempted to read from the following, it generates error:
column_names=['subject#','age','sex','test_time','motor_UPDRS','total_UPDRS','Jitter(%)','Jitter(Abs)',
              'Jitter:RAP','Jitter:PPQ5','Jitter:DDP','Shimmer','Shimmer(dB)','Shimmer:APQ3','Shimmer:APQ5',
              'Shimmer:APQ11','Shimmer:DDA','NHR','HNR','RPDE','DFA','PPE']
labels=column_names.pop(4)
labels=[labels,column_names.pop(4)]
print(labels)
print(column_names)
batch_size = 32

train_dataset = tf.data.experimental.make_csv_dataset(
    data,
    batch_size,
    column_names=column_names,
    label_name=labels,
    num_epochs=1)

**Other info / logs** Include any logs or source code that would be helpful to

Generates error:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-12-a43b43d3b38b> in <module>()
      6     column_names=column_names,
      7     label_name=labels,
----> 8     num_epochs=1)

2 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/experimental/ops/readers.py in _next_csv_row(filenames, num_cols, field_delim, use_quote_delim, header, file_io_fn)
    123         if len(csv_row) != num_cols:
    124           raise ValueError(
--> 125               ""Problem inferring types: CSV row has different number of fields ""
    126               ""than expected."")
    127         yield csv_row

ValueError: Problem inferring types: CSV row has different number of fields than expected.
**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1JM1iQk5ZXWo9O9JwKpab58aQyYYERty0?usp=sharing

"
43073,interperter with GPU Delegate blocks Android from rendering on Main thread.,"
**System information**
- Have I written custom code: Yes.
- OS Platform and Distribution: Android 27, 28, 29, and more
- Mobile device Samsong Galaxy S10, A30, J6, Pixel 3, 4a, ....
- TensorFlow imported as a gradle dependancy. 
- TensorFlow version 2.2.0 / 2.3.0


**Describe the current behavior**
I've created an interperter, and ran it without the `GpuDelegate`, everyting works fine.
both creation and `interperter.run` are running **not** on the main thread, but on another thread.
this thread has it's own GL context, which is **not** the same as the main thread.

When i add the `GpuDelegate` to the interperter options, aside from the model running much faster,
it blocks me from rendering animations in the main thread. 
this happens with Lottie animations, View animations, and simple android views like `ProgressBar`.
all the animations freeze while the interperter is running.

the included image shows the main thread and the tread running the interperter.
the tiny stack in the left in the main therad, is a Lottie animation view render call, and the main thread is stuck in `syncAndDrawFrame` until the interperter is finished (which in this case is  ~200ms on a flagship device, which is super fast for this interperter, but an eternity for the main thread.)
![202009092340696141122502934](https://user-images.githubusercontent.com/37263856/92591818-4045d500-f2a7-11ea-8f52-53da69a74f5a.jpg)

I assume the `GpuDelegate` consumes all the GPU resources for my interperter.

**Describe the expected behavior**
the interperter invocation should not interfere with other threads, or at least other GL context, and should not block the main thread."
43072,"Duplicate code in `tf.python.keras.engine.training.Model.evaluate()`, `train_on_batch()` and `test_on_batch()`","There is some duplicate code in methods ``tf.python.keras.engine.training.Model.evaluate()``, ``train_on_batch()`` and ``test_on_batch()``. I am not sure to have time to make a clean pull request so I report the duplicate and a solution here and someone may be able to fix it sooner.

The duplicate code is at the end of each method:
```python
    logs = tf_utils.to_numpy_or_python_type(logs)
    if return_dict:
      return logs
    else:
      results = [logs.get(name, None) for name in self.metrics_names]
      if len(results) == 1:
        return results[0]
      return results
```

I propose to factorize this code into a new private method ``_format_logs_results(self, logs, return_dict)``. Very easy fix, it seems, but not useless. It would allow to operate on the logs just before the return statement all in one."
43071,TFLite model gives random outputs when running on Android,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S10E
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
On desktop, my saved_model works perfectly fine. I converted it to TFLite format, tested it using Python API, and it also worked perfectly fine. However, when I deployed it to an Android phone, it would give very bizarre and often very random outputs. To be more specific, the output is supposed to be a 1x256x256x1 tensor with each value normally falling in the range of 0.05 ~ 1. However, on android, the values often drop below 1e-27 or reach above 1e25, seemingly at random. Sometimes the first run would give the correct values, but the subsequent runs would output smaller and smaller values. 

**Describe the expected behavior**
The TFLite model running on android should return outputs with values similar to that when running on desktop.

**Standalone code to reproduce the issue**
Android code
```
    private var options = Interpreter.Options()
    private val modelFile = File(assetFilePath(context, segmentationModulePath))
    private var interpreter: Interpreter
    private var imageProcessor: ImageProcessor

    init {
        options.addDelegate(GpuDelegate())
        options.setNumThreads(4)
        interpreter = Interpreter(modelFile, options)
        imageProcessor = ImageProcessor.Builder()
            .add(ResizeOp(IMAGE_WIDTH, IMAGE_HEIGHT, ResizeOp.ResizeMethod.BILINEAR))
            .add(NormalizeOp(IMAGE_MEAN, IMAGE_STD))
            .build()
    }

    fun loadImage(bitmap: Bitmap): TensorImage {
        val image = TensorImage(DataType.FLOAT32)
        image.load(bitmap)
        return imageProcessor.process(image)
    }
// The main executing function
    fun infer(bmp: Bitmap): Bitmap {
        val input = loadImage(bmp)
        val output = TensorBuffer.createFixedSize(intArrayOf(1, 256, 256, 1), DataType.FLOAT32)
        val start = System.nanoTime()
        interpreter.run(input.buffer, output.buffer)
// Omitted 
    }
    companion object {
        private const val IMAGE_WIDTH = 256
        private const val IMAGE_HEIGHT = 256
        private const val IMAGE_MEAN = 127.5f
        private const val IMAGE_STD = 127.5f
    }
```
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
The model is attached.
[shm.zip](https://github.com/tensorflow/tensorflow/files/5194486/shm.zip)
"
43068,Model Predict problem when used one data at a time,"**System information**
- Have I written custom code **Yes**
- OS Platform and Distribution : **docker tensorflow/tensorflow:2.2.0-gpu**
- TensorFlow version (use command below): **2.2.0**
- Python version: **3.6.9**
- CUDA/cuDNN version: **V10.1.243**
- GPU model and memory: **TITAN X (Pascal) 12Go**

**Describe the current behavior**

I am currently trying to learn the sum of 2 digits thanks to Keras.
My input is an array of 2 ints and output is one value.

The problem is in the predict function :
When using << model.predict >> with a batch, it works perfectly !
However, when trying to predict one data at a time, << model.predict >> doesn't work as it should and return a warning :

> WARNING:tensorflow:Model was constructed with shape (None, 2) for input Tensor(""input_1:0"", shape=(None, 2), dtype=float32), but it was called on an input with incompatible shape (None, 1).

**Describe the expected behavior**

Using the exact same code in docker tensorflow/tensorflow:1.9.0-py3, both method (batch or one data at a time) gives the same results.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1kkXVQAKHlztpqjn1XaBPNdbwJz21JOWa?usp=sharing"
43065,No gradients provided for any variable,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the current behavior**

The problem is in the learn function, and more specifically the actor network's update with tape gradient.

I am trying to calculate the gradients with respect to the model's trainable variables. I use tf.reduce_mean() to calculate loss that i am going to use. I have tried to define a @tf.function to calculate the reduced mean, but that doesn't help either. 
There are trainable variables, the loss value is not zero. The buffer and network files are not included here, i know, but maybe there could be a fix found with just looking at the code.

What's interesting to me is that before the problem, i also calculate gradients for the other networks without a problem.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
"""""" Twin delayed deep deterministic policy gradient agent: """"""
import tensorflow as tf
import matplotlib.pyplot as plt
import os
import datetime
import numpy as np
from tensorflow.keras.optimizers import Adam
from Networks import ActorNetwork, CriticNetwork
from Buffer import Buffer


class Agent:
    def __init__(self,
                 lr_actor,
                 lr_critic,
                 num_actions,
                 num_states,
                 gamma,
                 tau,
                 batch_size,
                 b_size,
                 fc1_actor,
                 fc2_actor,
                 fc1_critic,
                 fc2_critic
                 ):
        # Saving variables:
        self.num_actions = num_actions
        self.num_states = num_states
        self.gamma = gamma
        self.tau = tau
        self.batch_size = batch_size
        self.buffer = Buffer(buffer_size=b_size, batch_size=batch_size, num_states=num_states, num_action=num_actions)

        # Networks:
        self.actor = ActorNetwork(n_actions=num_actions, fc1_dims=fc1_actor, fc2_dims=fc2_actor, name='Actor')
        self.target_actor = ActorNetwork(n_actions=num_actions, fc1_dims=fc1_actor, fc2_dims=fc2_actor,
                                         name='ActorTarget')
        self.critic1 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic, name=""Critic1"")
        self.critic2 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic, name=""Critic2"")
        self.target_critic1 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic,
                                            name=""CriticTarget1"")
        self.target_critic2 = CriticNetwork(n_actions=num_actions, fc1_dims=fc1_critic, fc2_dims=fc2_critic,
                                            name=""CriticTarget2"")

        # Compiling the networks:
        self.actor.compile(optimizer=Adam(learning_rate=lr_actor))
        self.target_actor.compile(optimizer=Adam(learning_rate=lr_actor))
        self.critic1.compile(optimizer=Adam(learning_rate=lr_critic))
        self.critic2.compile(optimizer=Adam(learning_rate=lr_critic))
        self.target_critic1.compile(optimizer=Adam(learning_rate=lr_critic))
        self.target_critic2.compile(optimizer=Adam(learning_rate=lr_critic))

        self.update_target_networks()

    def learn(self, timestep):
        # If there is not enough data, just return, don't learn from zeroes
        if self.buffer.counter < self.batch_size:
            return
        """""" 
        Learning comes after: 
        - choosing action
        - implementing that action
        - storing new variables in buffer
        """"""
        s_batch, a_batch, r_batch, ns_batch = self.buffer.batch_sample()
        s_batch = tf.convert_to_tensor(s_batch, dtype=tf.float64)
        a_batch = tf.convert_to_tensor(a_batch, dtype=tf.float64)
        r_batch = tf.convert_to_tensor(r_batch, dtype=tf.float32)
        ns_batch = tf.convert_to_tensor(ns_batch, dtype=tf.float64)

        next_action = self.actor(ns_batch)
        noise = tf.random.normal(shape=(1, self.num_actions), mean=0.0, stddev=0.2)
        noise = np.clip(noise, -0.5, 0.5)
        next_action += noise
        next_action = np.clip(next_action, -1, 1)

        # next_action = tf.convert_to_tensor(next_action)
        """""" Update of the critic networks with gradient """"""
        with tf.GradientTape(persistent=True) as tape:
            # Target Q values via target critic networks (next state, next action)
            q1_ = tf.squeeze(self.target_critic1(ns_batch, next_action))
            q2_ = tf.squeeze(self.target_critic2(ns_batch, next_action))

            # Choose minimum from these two values for the double Q update rule
            q_ = tf.math.minimum(q1_, q2_)
            # Calculate actual Q value:
            y = r_batch + (self.gamma * q_)

            # Current Q values via critic networks (state, action)
            q1 = self.critic1(s_batch, a_batch)
            q2 = self.critic2(s_batch, a_batch)

            # Loss is calculated as the sum of the MSE loss between target Q value and q1, q2
            q1_loss = tf.keras.losses.MSE(y, q1)
            q2_loss = tf.keras.losses.MSE(y, q2)
            q_loss = q1_loss + q2_loss

            # Optimize critic networks:
        critic_gradient = tape.gradient(q_loss, self.critic1.trainable_variables)
        self.critic1.optimizer.apply_gradients(
            zip(critic_gradient, self.critic1.trainable_variables))

        critic_gradient = tape.gradient(q_loss, self.critic2.trainable_variables)
        self.critic2.optimizer.apply_gradients(
            zip(critic_gradient, self.critic2.trainable_variables))
        del tape

        if timestep % 2:
            # Update actor with gradient
            with tf.GradientTape() as tape:
                actions = tf.convert_to_tensor(self.actor(s_batch))
                critic_value = tf.squeeze(-self.critic1(s_batch, actions))
                actor_loss = tf.math.reduce_mean(critic_value)

            actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)
            self.actor.optimizer.apply_gradients(
                zip(actor_grad, self.actor.trainable_variables))

            self.update_target_networks()

    def update_target_networks(self):

        # Update target critics: - soft update with tau

        new_weights_1 = []
        target_variables_1 = self.target_critic1.weights
        for i, variable in enumerate(self.critic1.weights):
            new_weights_1.append(variable * self.tau + target_variables_1[i] * (1 - self.tau))
        self.target_critic1.set_weights(new_weights_1)

        new_weights_2 = []
        target_variables_2 = self.target_critic2.weights
        for i, variable in enumerate(self.critic2.weights):
            new_weights_2.append(variable * self.tau + target_variables_2[i] * (1 - self.tau))
        self.target_critic2.set_weights(new_weights_2)

        # Update target actor:
        new_weights_3 = []
        target_variables_3 = self.target_actor.weights
        for i, variable in enumerate(self.actor.weights):
            new_weights_3.append(variable * self.tau + target_variables_3[i] * (1 - self.tau))
        self.target_actor.set_weights(new_weights_3)"
43064,16 bit integer quantization,"Is there a way to use the TensorFlow Lite Converter for 16-bit quantization? If so, how should it be done?"
43063,imx8 tflite opencl ,"platform:imx8
tensorflow:2.3
 
tflite support **Vivante** OpenCL device **gc7000l** ???
i can not found **Vivante** in code   tensorflow/tensorflow/lite/delegates/gpu/cl/cl_device.h
**enum class Vendor { QUALCOMM, MALI, POWERVR, NVIDIA, AMD, INTEL, UNKNOWN }**"
43062,TF_lite Convert using integer-only quantization,"import tensorflow as tf
saved_model_dir='D:/sfz/tf_sfz'
def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(**train_images**).batch(1).take(100):
      yield [input_value]

converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8
tflite_model_quant = converter.convert()
open(""D:/sfz/tf_sfz/model25.tflite"",""wb"").write(tflite_model_quant)

the **train_images** don't know what it is,I only know to select a part of the representative data, I don't know what to do next?The training data is JPG file and TXT file，They are in a folder, how do I make **train_images**?What type of **train_images** is it?Can you fix the code a little bit
"
43061,[tflite] Processing time is high in Windows 10 but better in Ubuntu 16.0,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): From binary
- TensorFlow version (use command below): 2.3
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am running [this](https://github.com/metalwhale/hand_tracking) project. When run on an Ubuntu 16.04 machine it takes delay of ~0.2 S. The processor is I3, and RAM 4GB. But when I ran this on a Windows 10 machine having I7 processor and RAM 4GB, the delay is increased. It varies in the range of 0.4-0.6 S. I tried the TFLite runtime also. But delay is increased. 
**Describe the expected behavior**
Running on a processor with high spec should be giving a better performance.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://github.com/metalwhale/hand_tracking


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached."
43060,Training of a simple model with KerasLayer hangs on macOSX with tf 2.3.0,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
custom code

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
macOS X 10.15.4 (19E287)

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
v2.3.0-rc2-23-gb36436b087 2.3.0


- Python version:
Python 3.7.3

**Describe the current behavior**
Simple model training hangs on macOS X with tf 2.3.0, but works in Colab and on macOS X with tf 2.2.0

**Describe the expected behavior**
should wourk

**Standalone code to reproduce the issue**

```

import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as tf_text


print(tf.__version__)
print(tf.keras.__version__)

EMBEDDING = ""https://tfhub.dev/google/universal-sentence-encoder-multilingual/3""

embed = hub.KerasLayer(EMBEDDING, dtype=tf.string, trainable=True)

s1 = tf.keras.Input(shape=[], dtype=tf.string)
s2 = tf.keras.Input(shape=[], dtype=tf.string)

v1 = embed(s1)
v2 = embed(s2)

cd = tf.reduce_sum(tf.multiply(v1, v2), axis=-1)

train_model = tf.keras.Model(inputs=[s1, s2], outputs=[cd])
optimizer = tf.optimizers.SGD(learning_rate=0.001)

i1 = tf.constant([""x"", ""y"", ""z""])
i2 = tf.constant([""a"", ""b"", ""c""])
c0 = tf.constant([1.0, 1.0, 1.0])

train_model.compile(optimizer=optimizer, loss=""mse"", metrics=[""mse""])
train_model.fit(x=[i1, i2], y=c0, batch_size=1, epochs=5, verbose=2)


```
"
43054,Tensorflow Bazel Build ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): 3.5.0
- CUDA/cuDNN version: 11/8.0.3
- GPU model and memory: GTX 1650

ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377, column 38, in _cuda_autoconf_impl
                _create_local_cuda_repository(repository_ctx)
        File ""C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1162, column 17, in _create_local_cuda_repository
                cc = find_cc(repository_ctx)
        File ""C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 216, column 34, in find_cc
                return _get_msvc_compiler(repository_ctx)
        File ""C:/users/soria/downloads/tensorflow/third_party/gpus/cuda_configure.bzl"", line 133, column 26, in _get_msvc_compiler
                return find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")
        File ""C:/users/soria/_bazel_soria/s7y7zhp3/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 404, column 27, in find_msvc_tool
                if _is_vs_2017_or_2019(vc_path) or _is_msbuildtools(vc_path):
        File ""C:/users/soria/_bazel_soria/s7y7zhp3/external/bazel_tools/tools/cpp/windows_cc_configure.bzl"", line 261, column 19, in _is_vs_2017_or_2019
                return vc_path.find(""2017"") != -1 or vc_path.find(""2019"") != -1
Error: 'NoneType' value has no field or method 'find'
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': 'NoneType' value has no field or method 'find'
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': 'NoneType' value has no field or method 'find'
INFO: Elapsed time: 7.514s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package"
43052,DLL load fail _pywrap_tensorflow_internal during CPU-only installation,"I am trying to install the CPU-only version of tf.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64bit (v.1912)
- TensorFlow installed from (source or binary): pip (v.2.3.0), conda (v.2.1.0)
- TensorFlow version: 2.3.0
- Python version: 3.7.0 and 3.8.3
- Installed using virtualenv? conda and without using an environment
- CUDA/cuDNN version: I didn't install as my GPU doesn't support tf
- GPU model and memory: Intel HD 520
- CPU Model: Intel® Core™ Prozessor i5-6200U (based on issue #39007 I checked for AVX support, the [datasheet](https://ark.intel.com/content/www/us/en/ark/products/88193/intel-core-i5-6200u-processor-3m-cache-up-to-2-80-ghz.html) says it supports Intel® AVX2)

I tried to install tensorflow to an existing environment, which already didn't work. Trying to the fix the issue, I uninstalled anaconda (incl. caches, pip, and conda) and installed miniconda. Running the following commands:
```
$ conda create --name tf_test
$ activate tf_test
$ pip install tensorflow
$ python
>>>import tensorflow
```
I am getting the same error as before, which stays the same also without activating the conda environment:
```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\sebas\miniconda3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\sebas\miniconda3\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\sebas\miniconda3\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\sebas\miniconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\sebas\miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\sebas\miniconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: Das angegebene Modul wurde nicht gefunden. 
(*The specified module could not be found.*) 


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

Thanks for your help!"
43049,How to use unified memory in TF2.1,"Hi, I am trying to test unified memory performance. But I cannot find any introduction about how to use unified memory in tf2.1. Could you please offer some help? Thanks a lot!
"
43048,ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none),"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

On Win10 using Python 3.7 64bit
trying to install tensorflow with 
pip3 install --user --upgrade tensorflow this gives
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)

tried just plain pip install tensorflow as well same exception is thrown as far as I can tell I am using latest version of pip and I have admin rights/"
43047,"Cardinality returns unknown for a dataset with (string, float), but also for dataset with float variable alone","Cardinality returns unknown from a dataset with (string, float) but also for dataset with float variable alone. 

I created data stored in a csv files without header representing text and a float number corresponding to this text (NALU like dataset). 

I susptect that it is not possible to find the size of the data since the first column is a string it gives me an UNKNOWN cardinality (-2) back.

Dataset creation: 
```py
import tensorflow_datasets as tfds
import tensorflow as tf

with open(""file.csv"", ""w"") as f:
    for text, number in [(""one"", 1.0), (""two"", 2.0)]:
        f.write(f""{text:s}, {number:.1f}\n"")

ds = tf.data.experimental.CsvDataset(
    filenames=""file.csv"", record_defaults=[tf.string, tf.float32], header=False)

tf.data.experimental.cardinality(ds)
```

One would expect that when creating a dataset with the float column only, or mapping the dataset to the target only, the cardinality would be returned properly, but the next mapping stil has UNKNOWN cardinality.

```py
# check first method:
with open(""file1.csv"", ""w"") as f:
    for number in [1.0, 2.0]:
        f.write(f""{number:.1f}\n"")

ds1 = tf.data.experimental.CsvDataset(
    filenames=""file1.csv"", record_defaults=[tf.float32], header=False)

tf.data.experimental.cardinality(ds1)
#returns -2

# check second method
def get_target_pyfn(text, target):
    target = tf.py_function(lambda text, target: target, inp=[text, target], Tout=tf.float32)
    target.set_shape([])
    return target

tf.data.experimental.cardinality( ds.map(get_target_pyfn))
#returns -2
```


**System information**
tensorflow version = 2.3.0
Python virtual environment is setup with pyproject.toml in https://github.com/lynochka/parser/blob/bf2a006bee210b62552618dceb181101655753ff/pyproject.toml
"
43046,TensorFlow send/recv operations scheduling,"Link to the documentation: 
https://www.usenix.org/system/files/conference/osdi16/osdi16-abadi.pdf
link to the OpKernels implementation:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sendrecv_ops.h

## Description of issue:
When a Tensorflow execute a graph, it partitions it into subgraphs s.t. each subgraph consists of nodes assigned to a single device. Value copy operations between those graphs are introduced in the form of Send and Recv nodes.

![image](https://user-images.githubusercontent.com/18283933/92479762-a3befc80-f1ec-11ea-9b1d-c37069ca10d5.png)

Send/Recv nodes are OpKernels that being executed on the CPU but trigger a memory copy operation (host to device or device to host). The framework support executing these operations in any order by using the Rendezvous object. Due to Recv1 OpKernel, we can be sure that OpKernel2 will be executed only when its input is ready. The question is about Send1 OpKernel. How does the framework schedule it? Since Send1 executed on CPU it should be executed immediately after calling to OpKernel1->compute in this case data dependency may be broken. Or maybe Send1 somehow belongs to the same subgraph as OpKernel1 even though they are being executed on different devices?

"
43045,TFX for Mobile: Additional dtypes,"**System information**
- MacOS
- tensorflow==2.3.0.rc
- tfx==0.22.0


**Provide the text output from tflite_convert**

```
def _get_serve_tf_examples_fn(model, tf_transform_output):
        """"""Returns a function applies TFT.""""""

        model.tft_layer = tf_transform_output.transform_features_layer()

        @tf.function
        def serve_tf_examples_fn(text):
            """"""Returns the output to be used in the serving signature.""""""
            reshaped_text = tf.reshape(text, [-1, 1])

            transformed_features = model.tft_layer({""text"": reshaped_text})

            outputs = model(transformed_features)
            return {'outputs': outputs}

        return serve_tf_examples_fn


    signatures = {
        'serving_default':
            _get_serve_tf_examples_fn(model,
                                        tft_output).get_concrete_function(
                                            tf.TensorSpec(
                                                shape=[None],
                                                dtype=tf.float32,
                                                name='examples')),
    }

    model.save(serving_dir, save_format='tf', signatures=signatures)


    converter = tf.lite.TFLiteConverter.from_saved_model(serving_dir)
    converter.experimental_new_converter=True
    converter.target_spec.supported_ops = [
            tf.lite.OpsSet.TFLITE_BUILTINS,
            tf.lite.OpsSet.SELECT_TF_OPS
        ]
    tflite_model = converter.convert()

ValueError: Input 1 of node StatefulPartitionedCall was passed bool from unknown:0 incompatible with expected resource.
```

TFX provides a powerful ecosystem for running production-grade ML pipelines, where one of its most important components is the Transform component. Currently, it seems that the tflite converter is incapable of receiving an input string and performing the tokenization in the tfgraph. Is this expected to be released in the near future? Same holds true for image decoding: https://github.com/tensorflow/tfx/tree/master/tfx/examples/mnist takes arrays as input rather than raw image strings. 
"
43044,"TF_TensorToPyArray is hard coded for equality, limits performance enhancements","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes, modified TF code with overriding existing Op 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 18.04):  4.15.0-115-generic
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:  x86_64 24 core processor
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.15
- Python version: Python 3.7.6
- Bazel version (if compiling from source): Build label: 0.26.1
- GCC/Compiler version (if compiling from source): gcc version 7.5.0
- CUDA/cuDNN version: Not using
- GPU model and memory: Not using

**Describe the current behavior**
I am running one CNN model(which is sequential and similar to VGGNET) which has convolution and FC layers. 
For getting optimal performance, I am using o/p buffers which are allocated with allocate_persistent() API and I am reusing these buffers multiple times as next layer i/p ans new o/p as Network is pretty sequential.
Basically i am having two buffer, using it like ping pong model (keep switching b/w those in alternate way). This way making sure that buffer will always be cached in memory. 
For achieving this i am pre-allocating big chunks with allocate_persistent() API and for every Op execution i use the same buffers by setting with different shape.

With convolutions it worked fine and i see some performance gains too . However once FC layer gets executed it throws below exception. Because of this exception, I am not able to run my network.

_INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes
I0907 13:07:13.556839 140574226294592 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes_

After looking into code, I found function TF_TensorToPyArray() gets called, _line 471 in tensorflow/python/lib/core/ndarray_tensor.cc_, which compare the ndarray bytes with TF_tensor bytes, which is fine, But why does it need to be exact match?
It should have been below comparison, instead of exact match. If i do below change it works fine and i get the right results too.
PyArray_NBYTES(py_array)) >
TF_TensorByteSize(tensor.get()

This way same buffers can be used for different Op execution(in sequential case), just we need to change the shape every time.

**My pseudo code:**
// below global variables will be allocated with allocate_persistent() API
//allocated with max buffer shape
TensorOut1
TensorOut2 

//Overrided compute() kernel for these Ops
convolutionOp1(input1, TensorOut1)   //with set_shape(which is public now) i am changing to required shape for this conv o/p     
convolutionOp2(TensorOut1, TensorOut2) // same as above comment
convolutionOp3(TensorOut2, TensorOut1) // same as above comment
convolutionOp4(TensorOut1, TensorOut2) //same as above comment
.
.
fullyConnected(TensorOut1, TensorOut2)  

Once n/w is fully executed, I get below error. 
Reason is Output of FC is 2D shape(changed from set_shape). But no. of bytes allocated is more, as we are working with pre allocated buffers. 

INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes
I0907 13:07:13.556839 140574226294592 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes


**Describe the expected behavior**
_I had to do two below changes in existing code_
1. Make set_shape(const TensorShape& shape) to public in tensorflow/core/framework/tensor.h
2. Change below line in tensorflow/python/lib/core/ndarray_tensor.cc ate line 471
existing:  } else if (static_cast<size_t>(PyArray_NBYTES(py_array)) !=
new:  } else if (static_cast<size_t>(PyArray_NBYTES(py_array)) >
 
With these i see expected gain in performance and correctness too. I am looking for some alternative to #1 
while #2 limits my use case, why are we comparing for equality? above suggested alternative will give this kind of flexibility to TF users

**Standalone code to reproduce the issue**

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43043,GradientTape.batch_jacobian returns the wrong type in case of constant functions,"**System information**
Google Colaboratory with TF 2.3.0, git version v2.3.0-0-gb36436b087

**Describe the current behavior**
tf.GradientTape.batch_jacobian of a constant function with return type float64 returns a zero matrix of type float32

**Describe the expected behavior**
return a zero matrix of type float64

**Standalone code to reproduce the issue**
```
@tf.function
def foo(val):
  return tf.constant([[3.0],[4.0]],dtype=tf.dtypes.float64)

c = tf.constant([[1.],[2.]],dtype=tf.dtypes.float64)
with tf.GradientTape() as g:
			g.watch(c)
			f = foo(c)

print(g.batch_jacobian(f,c)) 
```

**Other info / logs**

Workaround: setting the option unconnected_gradients to 'zero'
```
@tf.function
def foo(val):
  return tf.constant([[3.0],[4.0]],dtype=tf.dtypes.float64)

c = tf.constant([[1.],[2.]],dtype=tf.dtypes.float64)
with tf.GradientTape() as g:
			g.watch(c)
			f = foo(c)

print(g.batch_jacobian(f,c,unconnected_gradients='zero'))
```"
43042,tensorflow converter.convert() failure,"I built model in tf==2.3.0 and I saved my model in SavedModel format.

Here is my conversion operation:
```
converter = tf.lite.TFLiteConverter.from_saved_model('./binary_bert_model')
converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]
tflite_quant_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_quant_model)
```
Here is my model summary, which is a typical text classification model.
```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_3 (InputLayer)            [(None, 1)]          0
__________________________________________________________________________________________________
text_vectorization_7 (TextVecto (None, 128)          0           input_3[0][0]
__________________________________________________________________________________________________
token_and_position_embedding_2  (None, 128, 128)     1252608     text_vectorization_7[0][0]
__________________________________________________________________________________________________
transformer_block_2 (Transforme (None, 128, 128)     99584       token_and_position_embedding_2[0]
__________________________________________________________________________________________________
embedding_4 (Embedding)         (None, 128, 128)     1236224     text_vectorization_7[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_10 (Gl (None, 128)          0           transformer_block_2[0][0]
__________________________________________________________________________________________________
conv1d_8 (Conv1D)               (None, 125, 128)     65664       embedding_4[0][0]
__________________________________________________________________________________________________
conv1d_9 (Conv1D)               (None, 126, 128)     49280       embedding_4[0][0]
__________________________________________________________________________________________________
conv1d_10 (Conv1D)              (None, 127, 128)     32896       embedding_4[0][0]
__________________________________________________________________________________________________
conv1d_11 (Conv1D)              (None, 128, 128)     16512       embedding_4[0][0]
__________________________________________________________________________________________________
dropout_10 (Dropout)            (None, 128)          0           global_average_pooling1d_10[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_11 (Gl (None, 128)          0           conv1d_8[0][0]
__________________________________________________________________________________________________
global_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_12 (Gl (None, 128)          0           conv1d_9[0][0]
__________________________________________________________________________________________________
global_max_pooling1d_9 (GlobalM (None, 128)          0           conv1d_9[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_13 (Gl (None, 128)          0           conv1d_10[0][0]
__________________________________________________________________________________________________
global_max_pooling1d_10 (Global (None, 128)          0           conv1d_10[0][0]
__________________________________________________________________________________________________
global_average_pooling1d_14 (Gl (None, 128)          0           conv1d_11[0][0]
__________________________________________________________________________________________________
global_max_pooling1d_11 (Global (None, 128)          0           conv1d_11[0][0]
__________________________________________________________________________________________________
dense_20 (Dense)                (None, 64)           8256        dropout_10[0][0]
__________________________________________________________________________________________________
concatenate_4 (Concatenate)     (None, 1024)         0           global_average_pooling1d_11[0][0]
                                                                 global_max_pooling1d_8[0][0]
                                                                 global_average_pooling1d_12[0][0]
                                                                 global_max_pooling1d_9[0][0]
                                                                 global_average_pooling1d_13[0][0]
                                                                 global_max_pooling1d_10[0][0]
                                                                 global_average_pooling1d_14[0][0]
                                                                 global_max_pooling1d_11[0][0]
__________________________________________________________________________________________________
dropout_11 (Dropout)            (None, 64)           0           dense_20[0][0]
__________________________________________________________________________________________________
concatenate_5 (Concatenate)     (None, 1088)         0           concatenate_4[0][0]
                                                                 dropout_11[0][0]
__________________________________________________________________________________________________
mlp1 (Dense)                    (None, 512)          557568      concatenate_5[0][0]
__________________________________________________________________________________________________
mlp2 (Dense)                    (None, 1)            513         mlp1[0][0]
==================================================================================================
Total params: 2,082,881
Trainable params: 2,082,881
Non-trainable params: 0
__________________________________________________________________________________________________
```


Here is the failure errors:

> ---------------------------------------------------------------------------
> InvalidArgumentError                      Traceback (most recent call last)
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
>     496         results = c_api.TF_GraphImportGraphDefWithResults(
> --> 497             graph._c_graph, serialized, options)  # pylint: disable=protected-access
>     498         results = c_api_util.ScopedTFImportGraphDefResults(results)
> 
> InvalidArgumentError: Input 3 of node StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/transformer_encoder/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/input/_622:0 incompatible with expected resource.
> 
> During handling of the above exception, another exception occurred:
> 
> ValueError                                Traceback (most recent call last)
> <ipython-input-7-ec5379be901e> in <module>
> ----> 1 converter.convert()
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
>    1074         Invalid quantization parameters.
>    1075     """"""
> -> 1076     return super(TFLiteConverterV2, self).convert()
>    1077
>    1078
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
>     876     frozen_func, graph_def = (
>     877         _convert_to_constants.convert_variables_to_constants_v2_as_graph(
> --> 878             self._funcs[0], lower_control_flow=False))
>     879
>     880     input_tensors = [
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)
>    1107
>    1108   frozen_func = _construct_concrete_function(func, output_graph_def,
> -> 1109                                              converted_input_indices)
>    1110   return frozen_func, output_graph_def
>    1111
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _construct_concrete_function(func, output_graph_def, converted_input_indices)
>     999   new_func = wrap_function.function_from_graph_def(output_graph_def,
>    1000                                                    new_input_names,
> -> 1001                                                    new_output_names)
>    1002
>    1003   # Manually propagate shape for input tensors where the shape is not correctly
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in function_from_graph_def(graph_def, inputs, outputs)
>     648     importer.import_graph_def(graph_def, name="""")
>     649
> --> 650   wrapped_import = wrap_function(_imports_graph_def, [])
>     651   import_graph = wrapped_import.graph
>     652   return wrapped_import.prune(
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)
>     626           signature=signature,
>     627           add_control_dependencies=False,
> --> 628           collections={}),
>     629       variable_holder=holder,
>     630       signature=signature)
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
>     984         _, original_func = tf_decorator.unwrap(python_func)
>     985
> --> 986       func_outputs = python_func(*func_args, **func_kwargs)
>     987
>     988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)
>      85
>      86   def __call__(self, *args, **kwargs):
> ---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
>      88
>      89   def call_with_variable_creator_scope(self, fn):
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)
>      91     def wrapped(*args, **kwargs):
>      92       with variable_scope.variable_creator_scope(self.variable_creator_scope):
> ---> 93         return fn(*args, **kwargs)
>      94
>      95     return wrapped
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in _imports_graph_def()
>     646
>     647   def _imports_graph_def():
> --> 648     importer.import_graph_def(graph_def, name="""")
>     649
>     650   wrapped_import = wrap_function(_imports_graph_def, [])
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
>     505                 'in a future version' if date is None else ('after %s' % date),
>     506                 instructions)
> --> 507       return func(*args, **kwargs)
>     508
>     509     doc = _add_deprecated_arg_notice_to_docstring(
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)
>     403       return_elements=return_elements,
>     404       name=name,
> --> 405       producer_op_list=producer_op_list)
>     406
>     407
> 
> /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
>     499       except errors.InvalidArgumentError as e:
>     500         # Convert to ValueError for backwards compatibility.
> --> 501         raise ValueError(str(e))
>     502
>     503     # Create _DefinedFunctions for any imported functions.
> 
> ValueError: Input 3 of node StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/transformer_encoder/StatefulPartitionedCall was passed float from Func/StatefulPartitionedCall/functional_3/keras_layer_1/StatefulPartitionedCall/StatefulPartitionedCall/StatefulPartitionedCall/input/_622:0 incompatible with expected resource."
43041,TFLite C API problem inferencing,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I'm using TFLite C API to run inference repeatedly, and got the following error. This error does not happen right away, but only randomly after many different runs
```
2020-09-08 17:45:35.676 11696-25578/? E/tflite: Unsupported data type: 0
2020-09-08 17:45:35.676 11696-25578/? E/tflite: Node number 0 (POW) failed to invoke.
2020-09-08 17:45:38.469 25588-25588/? A/DEBUG:       #03 pc 0000000000011e50  /system/lib64/libtensorflowlite_c.so (TfLiteInterpreterDelete+36)
2020-09-08 17:51:45.920 26100-26312/? E/tflite: tensorflow/lite/kernels/add.cc:373 Type NOTYPE is unsupported by op Add.
2020-09-08 17:51:45.921 26100-26312/? E/tflite: Node number 9 (ADD) failed to invoke.
2020-09-08 17:51:48.680 26320-26320/? A/DEBUG:       #03 pc 0000000000011e50  /system/lib64/libtensorflowlite_c.so (TfLiteInterpreterDelete+36)
2020-09-08 17:52:02.371 26405-26405/? A/DEBUG:       #00 pc 0000000000011f04  /system/lib64/libtensorflowlite_c.so (TfLiteInterpreterGetInputTensor+16)
```

Is this related to failure in copying data to `TfLiteTensorCopyToBuffer`? How could we try catch this problem?

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
43039,MultiWorkerMirroredStrategy.experimental_distribute_dataset return an empty DistributedDataset,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  docker hub image: tensorflow/tensorflow:2.3.0-gpu
- TensorFlow version (use command below): 2.3.0-gpu
- Python version: 3
- CUDA/cuDNN version: 7.6.4.38-1
- GPU model and memory:GeForce GTX 1080 Ti , 12G memory

**Describe the current behavior**
I use `MultiWorkerMirroredStrategy` with three machines, one GPU on each. And create Dataset from tfrecord, then wrap it with `experimental_distribute_dataset`. I want to iterator the returned DistributedDataset, but encounter `tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence` error. It means the DistributedDataset is empty. But I succeed to print the tensors in original dataset.

**Describe the expected behavior**
Print the DistributedDataset elements properly.

**Standalone code to reproduce the issue**

tfrecord file [link:](https://drive.google.com/file/d/1bvqxBHB3jiZewDpKt85tT3pjau6AvEpQ/view?usp=sharing)

You should change the `dataset_path` in dict `cfg` to proper location after you download the tfrecord file.

```python
import os
import tensorflow as tf
from absl import app, flags, logging

import math
import tensorflow as tf
import numpy as np
from itertools import product as product


###############################################################################
#   Tensorflow / Numpy Priors                                                 #
###############################################################################
def prior_box(image_sizes, min_sizes, steps, clip=False):
    """"""prior box""""""
    feature_maps = [
        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]
        for step in steps]

    anchors = []
    for k, f in enumerate(feature_maps):
        for i, j in product(range(f[0]), range(f[1])):
            for min_size in min_sizes[k]:
                s_kx = min_size / image_sizes[1]
                s_ky = min_size / image_sizes[0]
                cx = (j + 0.5) * steps[k] / image_sizes[1]
                cy = (i + 0.5) * steps[k] / image_sizes[0]
                anchors += [cx, cy, s_kx, s_ky]

    output = np.asarray(anchors).reshape([-1, 4])

    if clip:
        output = np.clip(output, 0, 1)

    return output


def prior_box_tf(image_sizes, min_sizes, steps, clip=False):
    """"""prior box""""""
    image_sizes = tf.cast(tf.convert_to_tensor(image_sizes), tf.float32)
    feature_maps = tf.math.ceil(
        tf.reshape(image_sizes, [1, 2]) /
        tf.reshape(tf.cast(steps, tf.float32), [-1, 1]))

    anchors = []
    for k in range(len(min_sizes)):
        grid_x, grid_y = _meshgrid_tf(tf.range(feature_maps[k][1]),
                                      tf.range(feature_maps[k][0]))
        cx = (grid_x + 0.5) * steps[k] / image_sizes[1]
        cy = (grid_y + 0.5) * steps[k] / image_sizes[0]
        cxcy = tf.stack([cx, cy], axis=-1)
        cxcy = tf.reshape(cxcy, [-1, 2])
        cxcy = tf.repeat(cxcy, repeats=tf.shape(min_sizes[k])[0], axis=0)

        sx = min_sizes[k] / image_sizes[1]
        sy = min_sizes[k] / image_sizes[0]
        sxsy = tf.stack([sx, sy], 1)
        sxsy = tf.repeat(sxsy[tf.newaxis],
                         repeats=tf.shape(grid_x)[0] * tf.shape(grid_x)[1],
                         axis=0)
        sxsy = tf.reshape(sxsy, [-1, 2])

        anchors.append(tf.concat([cxcy, sxsy], 1))

    output = tf.concat(anchors, axis=0)

    if clip:
        output = tf.clip_by_value(output, 0, 1)

    return output


def _meshgrid_tf(x, y):
    """""" workaround solution of the tf.meshgrid() issue:
        https://github.com/tensorflow/tensorflow/issues/34470""""""
    grid_shape = [tf.shape(y)[0], tf.shape(x)[0]]
    grid_x = tf.broadcast_to(tf.reshape(x, [1, -1]), grid_shape)
    grid_y = tf.broadcast_to(tf.reshape(y, [-1, 1]), grid_shape)
    return grid_x, grid_y


###############################################################################
#   Tensorflow Encoding                                                       #
###############################################################################
def encode_tf(labels, priors, match_thresh, ignore_thresh,
              variances=[0.1, 0.2]):
    """"""tensorflow encoding""""""
    assert ignore_thresh <= match_thresh
    priors = tf.cast(priors, tf.float32)
    bbox = labels[:, :4]
    landm = labels[:, 4:-1]
    landm_valid = labels[:, -1]  # 1: with landm, 0: w/o landm.

    # jaccard index
    overlaps = _jaccard(bbox, _point_form(priors))

    # (Bipartite Matching)
    # [num_objects] best prior for each ground truth
    best_prior_overlap, best_prior_idx = tf.math.top_k(overlaps, k=1)
    best_prior_overlap = best_prior_overlap[:, 0]
    best_prior_idx = best_prior_idx[:, 0]

    # [num_priors] best ground truth for each prior
    overlaps_t = tf.transpose(overlaps)
    best_truth_overlap, best_truth_idx = tf.math.top_k(overlaps_t, k=1)
    best_truth_overlap = best_truth_overlap[:, 0]
    best_truth_idx = best_truth_idx[:, 0]

    # ensure best prior
    def _loop_body(i, bt_idx, bt_overlap):
        bp_mask = tf.one_hot(best_prior_idx[i], tf.shape(bt_idx)[0])
        bp_mask_int = tf.cast(bp_mask, tf.int32)
        new_bt_idx = bt_idx * (1 - bp_mask_int) + bp_mask_int * i
        bp_mask_float = tf.cast(bp_mask, tf.float32)
        new_bt_overlap = bt_overlap * (1 - bp_mask_float) + bp_mask_float * 2
        return tf.cond(best_prior_overlap[i] > match_thresh,
                       lambda: (i + 1, new_bt_idx, new_bt_overlap),
                       lambda: (i + 1, bt_idx, bt_overlap))
    _, best_truth_idx, best_truth_overlap = tf.while_loop(
        lambda i, bt_idx, bt_overlap: tf.less(i, tf.shape(best_prior_idx)[0]),
        _loop_body, [tf.constant(0), best_truth_idx, best_truth_overlap])

    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]
    matches_landm = tf.gather(landm, best_truth_idx)  # [num_priors, 10]
    matches_landm_v = tf.gather(landm_valid, best_truth_idx)  # [num_priors]

    loc_t = _encode_bbox(matches_bbox, priors, variances)
    landm_t = _encode_landm(matches_landm, priors, variances)
    landm_valid_t = tf.cast(matches_landm_v > 0, tf.float32)
    conf_t = tf.cast(best_truth_overlap > match_thresh, tf.float32)
    conf_t = tf.where(
        tf.logical_and(best_truth_overlap < match_thresh,
                       best_truth_overlap > ignore_thresh),
        tf.ones_like(conf_t) * -1, conf_t)    # 1: pos, 0: neg, -1: ignore

    return tf.concat([loc_t, landm_t, landm_valid_t[..., tf.newaxis],
                      conf_t[..., tf.newaxis]], axis=1)


def _encode_bbox(matched, priors, variances):
    """"""Encode the variances from the priorbox layers into the ground truth
    boxes we have matched (based on jaccard overlap) with the prior boxes.
    Args:
        matched: (tensor) Coords of ground truth for each prior in point-form
            Shape: [num_priors, 4].
        priors: (tensor) Prior boxes in center-offset form
            Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes
    Return:
        encoded boxes (tensor), Shape: [num_priors, 4]
    """"""

    # dist b/t match center and prior's center
    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]
    # encode variance
    g_cxcy /= (variances[0] * priors[:, 2:])
    # match wh / prior wh
    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]
    g_wh = tf.math.log(g_wh) / variances[1]
    # return target for smooth_l1_loss
    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]


def _encode_landm(matched, priors, variances):
    """"""Encode the variances from the priorbox layers into the ground truth
    boxes we have matched (based on jaccard overlap) with the prior boxes.
    Args:
        matched: (tensor) Coords of ground truth for each prior in point-form
            Shape: [num_priors, 10].
        priors: (tensor) Prior boxes in center-offset form
            Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes
    Return:
        encoded landm (tensor), Shape: [num_priors, 10]
    """"""

    # dist b/t match center and prior's center
    matched = tf.reshape(matched, [tf.shape(matched)[0], 5, 2])
    priors = tf.broadcast_to(
        tf.expand_dims(priors, 1), [tf.shape(matched)[0], 5, 4])
    g_cxcy = matched[:, :, :2] - priors[:, :, :2]
    # encode variance
    g_cxcy /= (variances[0] * priors[:, :, 2:])
    # g_cxcy /= priors[:, :, 2:]
    g_cxcy = tf.reshape(g_cxcy, [tf.shape(g_cxcy)[0], -1])
    # return target for smooth_l1_loss
    return g_cxcy


def _point_form(boxes):
    """""" Convert prior_boxes to (xmin, ymin, xmax, ymax)
    representation for comparison to point form ground truth data.
    Args:
        boxes: (tensor) center-size default boxes from priorbox layers.
    Return:
        boxes: (tensor) Converted xmin, ymin, xmax, ymax form of boxes.
    """"""
    return tf.concat((boxes[:, :2] - boxes[:, 2:] / 2,
                      boxes[:, :2] + boxes[:, 2:] / 2), axis=1)


def _intersect(box_a, box_b):
    """""" We resize both tensors to [A,B,2]:
    [A,2] -> [A,1,2] -> [A,B,2]
    [B,2] -> [1,B,2] -> [A,B,2]
    Then we compute the area of intersect between box_a and box_b.
    Args:
      box_a: (tensor) bounding boxes, Shape: [A,4].
      box_b: (tensor) bounding boxes, Shape: [B,4].
    Return:
      (tensor) intersection area, Shape: [A,B].
    """"""
    A = tf.shape(box_a)[0]
    B = tf.shape(box_b)[0]
    max_xy = tf.minimum(
        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),
        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))
    min_xy = tf.maximum(
        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),
        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))
    inter = tf.maximum((max_xy - min_xy), tf.zeros_like(max_xy - min_xy))
    return inter[:, :, 0] * inter[:, :, 1]


def _jaccard(box_a, box_b):
    """"""Compute the jaccard overlap of two sets of boxes.  The jaccard overlap
    is simply the intersection over union of two boxes.  Here we operate on
    ground truth boxes and default boxes.
    E.g.:
        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)
    Args:
        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]
        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]
    Return:
        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]
    """"""
    inter = _intersect(box_a, box_b)
    area_a = tf.broadcast_to(
        tf.expand_dims(
            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),
        tf.shape(inter))  # [A,B]
    area_b = tf.broadcast_to(
        tf.expand_dims(
            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),
        tf.shape(inter))  # [A,B]
    union = area_a + area_b - inter
    return inter / union  # [A,B]


###############################################################################
#   Tensorflow Decoding                                                       #
###############################################################################
def decode_tf(labels, priors, variances=[0.1, 0.2]):
    """"""tensorflow decoding""""""
    bbox = _decode_bbox(labels[:, :4], priors, variances)
    landm = _decode_landm(labels[:, 4:14], priors, variances)
    landm_valid = labels[:, 14][:, tf.newaxis]
    conf = labels[:, 15][:, tf.newaxis]

    return tf.concat([bbox, landm, landm_valid, conf], axis=1)


def _decode_bbox(pre, priors, variances=[0.1, 0.2]):
    """"""Decode locations from predictions using priors to undo
    the encoding we did for offset regression at train time.
    Args:
        pre (tensor): location predictions for loc layers,
            Shape: [num_priors,4]
        priors (tensor): Prior boxes in center-offset form.
            Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes
    Return:
        decoded bounding box predictions
    """"""
    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]
    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])

    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)


def _decode_landm(pre, priors, variances=[0.1, 0.2]):
    """"""Decode landm from predictions using priors to undo
    the encoding we did for offset regression at train time.
    Args:
        pre (tensor): landm predictions for loc layers,
            Shape: [num_priors,10]
        priors (tensor): Prior boxes in center-offset form.
            Shape: [num_priors,4].
        variances: (list[float]) Variances of priorboxes
    Return:
        decoded landm predictions
    """"""
    landms = tf.concat(
        [priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:],
         priors[:, :2] + pre[:, 2:4] * variances[0] * priors[:, 2:],
         priors[:, :2] + pre[:, 4:6] * variances[0] * priors[:, 2:],
         priors[:, :2] + pre[:, 6:8] * variances[0] * priors[:, 2:],
         priors[:, :2] + pre[:, 8:10] * variances[0] * priors[:, 2:]], axis=1)
    return landms

def _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,
                    using_encoding, priors, match_thresh, ignore_thresh,
                    variances):
    def parse_tfrecord(tfrecord):
        features = {
            'image/img_name': tf.io.FixedLenFeature([], tf.string),
            'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),
            'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),
            'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),
            'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark0/x': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark0/y': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark1/x': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark1/y': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark2/x': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark2/y': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark3/x': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark3/y': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark4/x': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark4/y': tf.io.VarLenFeature(tf.float32),
            'image/object/landmark/valid': tf.io.VarLenFeature(tf.float32)}
        if using_bin:
            features['image/encoded'] = tf.io.FixedLenFeature([], tf.string)
            x = tf.io.parse_single_example(tfrecord, features)
            img = tf.image.decode_jpeg(x['image/encoded'], channels=3)
        else:
            features['image/img_path'] = tf.io.FixedLenFeature([], tf.string)
            x = tf.io.parse_single_example(tfrecord, features)
            image_encoded = tf.io.read_file(x['image/img_path'])
            img = tf.image.decode_jpeg(image_encoded, channels=3)

        labels = tf.stack(
            [tf.sparse.to_dense(x['image/object/bbox/xmin']),
             tf.sparse.to_dense(x['image/object/bbox/ymin']),
             tf.sparse.to_dense(x['image/object/bbox/xmax']),
             tf.sparse.to_dense(x['image/object/bbox/ymax']),
             tf.sparse.to_dense(x['image/object/landmark0/x']),
             tf.sparse.to_dense(x['image/object/landmark0/y']),
             tf.sparse.to_dense(x['image/object/landmark1/x']),
             tf.sparse.to_dense(x['image/object/landmark1/y']),
             tf.sparse.to_dense(x['image/object/landmark2/x']),
             tf.sparse.to_dense(x['image/object/landmark2/y']),
             tf.sparse.to_dense(x['image/object/landmark3/x']),
             tf.sparse.to_dense(x['image/object/landmark3/y']),
             tf.sparse.to_dense(x['image/object/landmark4/x']),
             tf.sparse.to_dense(x['image/object/landmark4/y']),
             tf.sparse.to_dense(x['image/object/landmark/valid'])], axis=1)

        img, labels = _transform_data(
            img_dim, using_flip, using_distort, using_encoding, priors,
            match_thresh, ignore_thresh, variances)(img, labels)

        return img, labels
    return parse_tfrecord


def _transform_data(img_dim, using_flip, using_distort, using_encoding, priors,
                    match_thresh, ignore_thresh, variances):
    def transform_data(img, labels):
        img = tf.cast(img, tf.float32)

        # randomly crop
        img, labels = _crop(img, labels)

        # padding to square
        img = _pad_to_square(img)

        # resize
        img, labels = _resize(img, labels, img_dim)

        # randomly left-right flip
        if using_flip:
            img, labels = _flip(img, labels)

        # distort
        if using_distort:
            img = _distort(img)

        # encode labels to feature targets
        if using_encoding:
            labels = encode_tf(labels=labels, priors=priors,
                               match_thresh=match_thresh,
                               ignore_thresh=ignore_thresh,
                               variances=variances)

        return img, labels
    return transform_data


def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,
                          using_bin=True, using_flip=True, using_distort=True,
                          using_encoding=True, priors=None, match_thresh=0.45,
                          ignore_thresh=0.3, variances=[0.1, 0.2],
                          shuffle=True, buffer_size=10240):
    """"""load dataset from tfrecord""""""
    if not using_encoding:
        assert batch_size == 1  # dynamic data len when using_encoding
    else:
        assert priors is not None

    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)
    raw_dataset = raw_dataset.repeat()
    if shuffle:
        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)
    dataset = raw_dataset.map(
        _parse_tfrecord(img_dim, using_bin, using_flip, using_distort,
                        using_encoding, priors, match_thresh, ignore_thresh,
                        variances),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    dataset = dataset.batch(batch_size, drop_remainder=True)
    dataset = dataset.prefetch(
        buffer_size=tf.data.experimental.AUTOTUNE)

    return dataset


###############################################################################
#   Data Augmentation                                                         #
###############################################################################
def _flip(img, labels):
    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)

    def flip_func():
        flip_img = tf.image.flip_left_right(img)
        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],
                                1 - labels[:, 0],  labels[:, 3],
                                1 - labels[:, 6],  labels[:, 7],
                                1 - labels[:, 4],  labels[:, 5],
                                1 - labels[:, 8],  labels[:, 9],
                                1 - labels[:, 12], labels[:, 13],
                                1 - labels[:, 10], labels[:, 11],
                                labels[:, 14]], axis=1)

        return flip_img, flip_labels

    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],
                          default=lambda: (img, labels))

    return img, labels


def _crop(img, labels, max_loop=250):
    shape = tf.shape(img)

    def matrix_iof(a, b):
        """"""
        return iof of a and b, numpy version for data augenmentation
        """"""
        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])
        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])

        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \
            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)
        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)
        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)

    def crop_loop_body(i, img, labels):
        valid_crop = tf.constant(1, tf.int32)

        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)
        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]
        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)
        h = w = tf.cast(scale * short_side, tf.int32)
        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)
        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)
        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])
        roi = tf.cast(roi, tf.float32)

        value = matrix_iof(labels[:, :4], roi[tf.newaxis])
        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),
                             lambda: valid_crop, lambda: 0)

        centers = (labels[:, :2] + labels[:, 2:4]) / 2
        mask_a = tf.reduce_all(
            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),
            axis=1)
        labels_t = tf.boolean_mask(labels, mask_a)
        valid_crop = tf.cond(tf.reduce_any(mask_a),
                             lambda: valid_crop, lambda: 0)

        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]
        h_offset = tf.cast(h_offset, tf.float32)
        w_offset = tf.cast(w_offset, tf.float32)
        labels_t = tf.stack(
            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,
             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,
             labels_t[:, 4] - w_offset,  labels_t[:, 5] - h_offset,
             labels_t[:, 6] - w_offset,  labels_t[:, 7] - h_offset,
             labels_t[:, 8] - w_offset,  labels_t[:, 9] - h_offset,
             labels_t[:, 10] - w_offset, labels_t[:, 11] - h_offset,
             labels_t[:, 12] - w_offset, labels_t[:, 13] - h_offset,
             labels_t[:, 14]], axis=1)

        return tf.cond(valid_crop == 1,
                       lambda: (max_loop, img_t, labels_t),
                       lambda: (i + 1, img, labels))

    _, img, labels = tf.while_loop(
        lambda i, img, labels: tf.less(i, max_loop),
        crop_loop_body,
        [tf.constant(-1), img, labels],
        shape_invariants=[tf.TensorShape([]),
                          tf.TensorShape([None, None, 3]),
                          tf.TensorShape([None, 15])])

    return img, labels


def _pad_to_square(img):
    height = tf.shape(img)[0]
    width = tf.shape(img)[1]

    def pad_h():
        img_pad_h = tf.ones([width - height, width, 3]) * \
            tf.reduce_mean(img, axis=[0, 1], keepdims=True)
        return tf.concat([img, img_pad_h], axis=0)

    def pad_w():
        img_pad_w = tf.ones([height, height - width, 3]) * \
            tf.reduce_mean(img, axis=[0, 1], keepdims=True)
        return tf.concat([img, img_pad_w], axis=1)

    img = tf.case([(tf.greater(height, width), pad_w),
                   (tf.less(height, width), pad_h)], default=lambda: img)

    return img


def _resize(img, labels, img_dim):
    w_f = tf.cast(tf.shape(img)[1], tf.float32)
    h_f = tf.cast(tf.shape(img)[0], tf.float32)
    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,
                     labels[:, 2] / w_f,  labels[:, 3] / h_f,
                     labels[:, 4] / w_f,  labels[:, 5] / h_f,
                     labels[:, 6] / w_f,  labels[:, 7] / h_f,
                     labels[:, 8] / w_f,  labels[:, 9] / h_f,
                     labels[:, 10] / w_f, labels[:, 11] / h_f,
                     labels[:, 12] / w_f, labels[:, 13] / h_f], axis=1)
    locs = tf.clip_by_value(locs, 0, 1)
    labels = tf.concat([locs, labels[:, 14][:, tf.newaxis]], axis=1)

    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)

    def resize(method):
        def _resize():
            return tf.image.resize(
                img, [img_dim, img_dim], method=method, antialias=True)
        return _resize

    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),
                   (tf.equal(resize_case, 1), resize('area')),
                   (tf.equal(resize_case, 2), resize('nearest')),
                   (tf.equal(resize_case, 3), resize('lanczos3'))],
                  default=resize('bilinear'))

    return img, labels


def _distort(img):
    img = tf.image.random_brightness(img, 0.4)
    img = tf.image.random_contrast(img, 0.5, 1.5)
    img = tf.image.random_saturation(img, 0.5, 1.5)
    img = tf.image.random_hue(img, 0.1)

    return img

def load_dataset(cfg, priors, shuffle=True, buffer_size=10240):
    """"""load dataset""""""
    logging.info(""load dataset from {}"".format(cfg['dataset_path']))
    dataset = load_tfrecord_dataset(
        tfrecord_name=cfg['dataset_path'],
        batch_size=cfg['batch_size'],
        img_dim=cfg['input_size'],
        using_bin=cfg['using_bin'],
        using_flip=cfg['using_flip'],
        using_distort=cfg['using_distort'],
        using_encoding=True,
        priors=priors,
        match_thresh=cfg['match_thresh'],
        ignore_thresh=cfg['ignore_thresh'],
        variances=cfg['variances'],
        shuffle=shuffle,
        buffer_size=buffer_size)
    return dataset

def set_memory_growth():
    gpus = tf.config.experimental.list_physical_devices('GPU')
    if gpus:
        try:
            # Currently, memory growth needs to be the same across GPUs
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
                logical_gpus = tf.config.experimental.list_logical_devices(
                    'GPU')
                logging.info(
                    ""Detect {} Physical GPUs, {} Logical GPUs."".format(
                        len(gpus), len(logical_gpus)))
        except RuntimeError as e:
            # Memory growth must be set before GPUs have been initialized
            logging.info(e)

def prior_box(image_sizes, min_sizes, steps, clip=False):
    """"""prior box""""""
    feature_maps = [
        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]
        for step in steps]

    anchors = []
    for k, f in enumerate(feature_maps):
        for i, j in product(range(f[0]), range(f[1])):
            for min_size in min_sizes[k]:
                s_kx = min_size / image_sizes[1]
                s_ky = min_size / image_sizes[0]
                cx = (j + 0.5) * steps[k] / image_sizes[1]
                cy = (i + 0.5) * steps[k] / image_sizes[0]
                anchors += [cx, cy, s_kx, s_ky]

    output = np.asarray(anchors).reshape([-1, 4])

    if clip:
        output = np.clip(output, 0, 1)

    return output

cfg = {
    # general setting
    ""batch_size"": 8,
    ""input_size"": 640,  # (h,w)

    ""backbone_type"": 'ResNet50',  # 'ResNet50', 'MobileNetV2'
    ""sub_name"": 'retinaface_res50',

    # training dataset
    ""dataset_path"": '/mnt/retinaface-tf2/data/widerface_test_bin.tfrecord',  # 'dataset/trainval_mask.tfrecord'
    ""testing_dataset_path"": './data/widerface/val',  #
    ""dataset_len"": 12880,  # train 6115 , trainval 7954, number of training samples
    ""val_len"": 1839,
    ""using_crop"": True,
    ""using_bin"": True,
    ""using_flip"": True,
    ""using_distort"": True,
    ""using_normalizing"": True,
    ""labels_list"": ['background', 'mask', 'unmask'],  # xml annotation

    # anchor setting
    # ""min_sizes"": [[(9, 7), (24, 20), (39, 35)], [(54, 41), (65, 61), (81, 66)],
    #               [(94, 86), (113, 95), (131, 122)], [(137, 128), (172, 162), (176, 210)]],
    ""min_sizes"": [[16, 32], [64, 128], [256, 512]],
    ""steps"": [8, 16, 32],
    ""match_thresh"": 0.45,
    ""ignore_thresh"": 0.3,
    ""variances"": [0.1, 0.2],
    ""clip"": False,

    # network
    ""out_channel"": 256,

    # training setting
    ""resume"": False,  # if False,training from scratch
    ""epoch"": 100,
    ""init_lr"": 1e-2,
    ""lr_decay_epoch"": [50, 68],
    ""lr_rate"": 0.1,
    ""warmup_epoch"": 5,
    ""min_lr"": 1e-3,
    ""pretrain"": True,

    ""save_steps"": 2000,

    ""weights_decay"": 5e-4,
    ""momentum"": 0.9,
    ""save_freq"": 1, #frequency of save model weights

    # inference
    ""score_threshold"": 0.5,
    ""nms_threshold"": 0.4,
    ""max_number_keep"": 200
}

def main(_):

    multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    # init
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'

    logger = tf.get_logger()
    logger.disabled = True
    set_memory_growth()
    priors = prior_box((cfg['input_size'], cfg['input_size']),
                           cfg['min_sizes'],  cfg['steps'], cfg['clip'])
    dataset = load_dataset(cfg, priors, shuffle=True)
    train_iter = iter(dataset)
    print(next(train_iter))
    dist_dataset = multiworker_strategy.experimental_distribute_dataset(dataset)
    dist_train_iter = iter(dist_dataset)
    print(next(dist_train_iter))


if __name__ == '__main__':
    app.run(main)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```bash
2020-09-08 10:19:16.853430: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-08 10:19:18.227937: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-08 10:19:18.254671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-08 10:19:18.254730: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-08 10:19:18.257145: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-08 10:19:18.259279: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-08 10:19:18.259620: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-08 10:19:18.262046: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-08 10:19:18.263487: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-08 10:19:18.268749: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-08 10:19:18.271254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-08 10:19:18.271693: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-08 10:19:18.283629: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2199875000 Hz
2020-09-08 10:19:18.286517: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x597c600 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-08 10:19:18.286539: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-08 10:19:19.151667: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59e8480 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-08 10:19:19.151723: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-09-08 10:19:19.154118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-08 10:19:19.154177: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-08 10:19:19.154212: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-08 10:19:19.154246: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-08 10:19:19.154274: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-08 10:19:19.154302: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-08 10:19:19.154328: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-08 10:19:19.154352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-08 10:19:19.158471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-08 10:19:19.158536: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-08 10:19:19.793164: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-08 10:19:19.793223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-08 10:19:19.793233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-08 10:19:19.795000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)
2020-09-08 10:19:19.800484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:82:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.582GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-08 10:19:19.800568: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-08 10:19:19.800618: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-08 10:19:19.800650: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-08 10:19:19.800681: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-08 10:19:19.800711: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-08 10:19:19.800741: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-08 10:19:19.800768: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-08 10:19:19.803844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-08 10:19:19.803916: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-08 10:19:19.803935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-08 10:19:19.803953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-08 10:19:19.806763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 10265 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1)
2020-09-08 10:19:19.814394: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> dist-data-worker-0.t9k-sample.svc:2222, 1 -> dist-data-worker-1.t9k-sample.svc:2222, 2 -> dist-data-worker-2.t9k-sample.svc:2222}
2020-09-08 10:19:19.816567: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:405] Started server with target: grpc://dist-data-worker-0.t9k-sample.svc:2222
INFO:tensorflow:Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0', '/job:worker/replica:0/task:0/device:XLA_GPU:0', '/job:worker/replica:0/task:0/device:GPU:0']
I0908 10:19:19.817300 139928804230976 collective_all_reduce_strategy.py:329] Enabled multi-worker collective ops with available devices: ['/job:worker/replica:0/task:0/device:CPU:0', '/job:worker/replica:0/task:0/device:XLA_CPU:0', '/job:worker/replica:0/task:0/device:XLA_GPU:0', '/job:worker/replica:0/task:0/device:GPU:0']
INFO:tensorflow:Using MirroredStrategy with devices ('/job:worker/task:0/device:GPU:0',)
I0908 10:19:19.818905 139928804230976 mirrored_strategy.py:341] Using MirroredStrategy with devices ('/job:worker/task:0/device:GPU:0',)
INFO:tensorflow:MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['dist-data-worker-0.t9k-sample.svc:2222', 'dist-data-worker-1.t9k-sample.svc:2222', 'dist-data-worker-2.t9k-sample.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO
I0908 10:19:19.819303 139928804230976 collective_all_reduce_strategy.py:380] MultiWorkerMirroredStrategy with cluster_spec = {'worker': ['dist-data-worker-0.t9k-sample.svc:2222', 'dist-data-worker-1.t9k-sample.svc:2222', 'dist-data-worker-2.t9k-sample.svc:2222']}, task_type = 'worker', task_id = 0, num_workers = 3, local_devices = ('/job:worker/task:0/device:GPU:0',), communication = CollectiveCommunication.AUTO
I0908 10:19:19.819926 139928804230976 dist_data.py:600] Physical devices cannot be modified after being initialized
I0908 10:19:19.850645 139928804230976 dist_data.py:569] load dataset from /mnt/retinaface-tf2/data/widerface_test_bin.tfrecord
(<tf.Tensor: shape=(8, 640, 640, 3), dtype=float32, numpy=
array([[[[ 1.24728775e+02,  1.22055222e+02,  1.15999634e+02],
         [ 1.22892578e+02,  1.20219025e+02,  1.14163445e+02],
         [ 1.22099586e+02,  1.19426033e+02,  1.13370453e+02],
         ...,
         [ 8.73357391e+01,  8.56875610e+01,  7.93239059e+01],
         [ 8.93529053e+01,  8.77047119e+01,  8.13410645e+01],
         [ 9.24133301e+01,  9.07651443e+01,  8.44014969e+01]],

        [[ 1.09426826e+02,  1.06753273e+02,  1.00697693e+02],
         [ 1.08657150e+02,  1.05983597e+02,  9.99280167e+01],
         [ 1.08476242e+02,  1.05802689e+02,  9.97471085e+01],
         ...,
         [ 9.34566040e+01,  9.18084183e+01,  8.54447632e+01],
         [ 9.55525665e+01,  9.39043808e+01,  8.75407257e+01],
         [ 9.91462860e+01,  9.74981003e+01,  9.11344452e+01]],

        [[ 9.72546539e+01,  9.45811005e+01,  8.85255203e+01],
         [ 9.70970535e+01,  9.44235001e+01,  8.83679199e+01],
         [ 9.78483429e+01,  9.51747971e+01,  8.91192169e+01],
         ...,
         [ 9.82655640e+01,  9.66173859e+01,  9.02537308e+01],
         [ 9.94642181e+01,  9.78160324e+01,  9.14523773e+01],
         [ 1.02227783e+02,  1.00579597e+02,  9.42159424e+01]],

        ...,

        [[ 1.03908783e+02,  1.07768463e+02,  7.75826416e+01],
         [ 1.02116661e+02,  1.05932289e+02,  7.63713684e+01],
         [ 9.66357956e+01,  1.00444916e+02,  7.09763565e+01],
         ...,
         [ 9.68215866e+01,  9.82557907e+01,  7.19535065e+01],
         [ 1.00859833e+02,  1.02294022e+02,  7.59917374e+01],
         [ 9.64176331e+01,  9.78518372e+01,  7.15495605e+01]],

        [[ 1.06085648e+02,  1.09945328e+02,  7.97595062e+01],
         [ 1.04293526e+02,  1.08109146e+02,  7.85482330e+01],
         [ 9.88126526e+01,  1.02621773e+02,  7.31532135e+01],
         ...,
         [ 9.66639557e+01,  9.80981598e+01,  7.17958755e+01],
         [ 1.00690552e+02,  1.02124748e+02,  7.58224716e+01],
         [ 9.57151184e+01,  9.71493149e+01,  7.08470383e+01]],

        [[ 1.06697701e+02,  1.10557381e+02,  8.03715591e+01],
         [ 1.04905586e+02,  1.08721207e+02,  7.91602936e+01],
         [ 9.94247055e+01,  1.03233826e+02,  7.37652664e+01],
         ...,
         [ 9.55974274e+01,  9.70316315e+01,  7.07293549e+01],
         [ 9.99996796e+01,  1.01433876e+02,  7.51315994e+01],
         [ 9.44909668e+01,  9.59251709e+01,  6.96228943e+01]]],


       [[[ 1.79020119e+01,  2.19689369e+01,  2.99244499e+00],
         [ 1.58624716e+01,  2.06704369e+01,  0.00000000e+00],
         [ 1.51764307e+01,  1.90834465e+01,  0.00000000e+00],
         ...,
         [ 8.16278076e+01,  9.86631927e+01,  6.32296028e+01],
         [ 6.73102951e+01,  8.42869415e+01,  4.91317062e+01],
         [ 6.12751694e+01,  7.79278717e+01,  4.47397270e+01]],

        [[ 1.95028820e+01,  2.17408943e+01,  7.81370163e-01],
         [ 1.96296520e+01,  2.13537941e+01,  0.00000000e+00],
         [ 2.09664173e+01,  2.15310116e+01,  0.00000000e+00],
         ...,
         [ 8.06434631e+01,  9.79421234e+01,  6.09631004e+01],
         [ 5.99489441e+01,  7.71724243e+01,  4.05182495e+01],
         [ 5.25240326e+01,  6.93798370e+01,  3.49581909e+01]],

        [[ 2.45029068e+01,  2.24879875e+01,  0.00000000e+00],
         [ 2.51510925e+01,  2.26621895e+01,  0.00000000e+00],
         [ 2.70248432e+01,  2.31384068e+01,  0.00000000e+00],
         ...,
         [ 7.55968781e+01,  9.31968842e+01,  5.44395905e+01],
         [ 4.63827667e+01,  6.39156876e+01,  2.53827095e+01],
         [ 3.69936066e+01,  5.42851677e+01,  1.72176933e+01]],

        ...,

        [[-1.93025589e+00, -1.93025589e+00, -1.93025589e+00],
         [-2.22468567e+00, -2.22468567e+00, -2.22468567e+00],
         [-3.08513641e+00, -3.08513641e+00, -3.08513641e+00],
         ...,
         [ 7.91158600e+01,  9.09350739e+01,  3.32669144e+01],
         [ 7.68767090e+01,  8.71371689e+01,  2.85244370e+01],
         [ 7.47488556e+01,  8.26550827e+01,  2.33686600e+01]],

        [[-6.81667328e-01, -6.81667328e-01, -6.81667328e-01],
         [-1.40288925e+00, -1.40288925e+00, -1.40288925e+00],
         [-2.36134720e+00, -2.36134720e+00, -2.36134720e+00],
         ...,
         [ 8.02405701e+01,  9.04674072e+01,  3.20591087e+01],
         [ 7.76517258e+01,  8.66896591e+01,  2.68999023e+01],
         [ 7.68033295e+01,  8.36673737e+01,  2.28854980e+01]],

        [[-1.89395905e-01, -1.89395905e-01, -1.89395905e-01],
         [-7.97077179e-01, -7.97077179e-01, -7.97077179e-01],
         [-1.66807556e+00, -1.66807556e+00, -1.66807556e+00],
         ...,
         [ 8.23338089e+01,  9.02438660e+01,  3.09343338e+01],
         [ 7.98906250e+01,  8.67535095e+01,  2.59786530e+01],
         [ 7.89819565e+01,  8.43535233e+01,  2.31105156e+01]]],


       [[[ 9.29103622e+01,  8.38115387e+01,  9.26200790e+01],
         [ 9.21722794e+01,  8.30734787e+01,  9.18819885e+01],
         [ 9.14833374e+01,  8.23845596e+01,  9.11930695e+01],
         ...,
         [ 5.40838509e+01,  4.53541336e+01,  4.25934219e+00],
         [ 5.80340424e+01,  4.97181396e+01,  1.29117203e+00],
         [ 5.84229126e+01,  4.98948250e+01,  0.00000000e+00]],

        [[ 8.68602219e+01,  7.79978180e+01,  8.60694427e+01],
         [ 8.78025208e+01,  7.89401245e+01,  8.70117950e+01],
         [ 8.75972595e+01,  7.87348328e+01,  8.68065033e+01],
         ...,
         [ 5.68775215e+01,  4.34964104e+01,  3.66986465e+00],
         [ 5.98933830e+01,  4.68066788e+01,  0.00000000e+00],
         [ 6.04419479e+01,  4.84969749e+01,  0.00000000e+00]],

        [[ 9.39529495e+01,  8.52325211e+01,  9.65406189e+01],
         [ 9.61381149e+01,  8.74176636e+01,  9.87257614e+01],
         [ 9.64990234e+01,  8.77786179e+01,  9.90867004e+01],
         ...,
         [ 5.81260529e+01,  4.53354836e+01,  5.51031113e+00],
         [ 6.04816360e+01,  4.78775520e+01,  8.11939240e-01],
         [ 6.11883049e+01,  4.94758110e+01,  0.00000000e+00]],

        ...,

        [[ 1.24503052e+02,  1.62149078e+02,  8.95797729e-01],
         [ 1.24353889e+02,  1.59983643e+02,  0.00000000e+00],
         [ 1.25073418e+02,  1.57816925e+02,  0.00000000e+00],
         ...,
         [ 1.29795639e+02,  1.66753372e+02,  2.55889435e+01],
         [ 1.27766533e+02,  1.65338638e+02,  2.42820740e+01],
         [ 1.29873688e+02,  1.64272003e+02,  2.05228882e+01]],

        [[ 1.24627296e+02,  1.61203949e+02,  0.00000000e+00],
         [ 1.24903076e+02,  1.59984085e+02,  0.00000000e+00],
         [ 1.24675163e+02,  1.57494263e+02,  0.00000000e+00],
         ...,
         [ 1.32041168e+02,  1.68460938e+02,  2.15579376e+01],
         [ 1.33513550e+02,  1.69203796e+02,  2.11235657e+01],
         [ 1.32195358e+02,  1.66914062e+02,  1.70703583e+01]],

        [[ 1.28943268e+02,  1.65691986e+02,  0.00000000e+00],
         [ 1.28879425e+02,  1.64225494e+02,  1.00944519e+00],
         [ 1.29246002e+02,  1.62467010e+02,  3.57754517e+00],
         ...,
         [ 1.34914566e+02,  1.71236755e+02,  1.62087860e+01],
         [ 1.34592255e+02,  1.68047272e+02,  1.38260651e+01],
         [ 1.31932831e+02,  1.66052216e+02,  8.41160583e+00]]],


       ...,


       [[[ 7.57140045e+01,  1.32128372e+02,  1.51906891e+02],
         [ 7.55561981e+01,  1.31970551e+02,  1.51749100e+02],
         [ 7.58803101e+01,  1.32294662e+02,  1.52073212e+02],
         ...,
         [ 7.63204269e+01,  1.38002457e+02,  1.55694473e+02],
         [ 7.63204346e+01,  1.38002441e+02,  1.55694458e+02],
         [ 7.63204346e+01,  1.38002441e+02,  1.55694458e+02]],

        [[ 7.60969543e+01,  1.32511337e+02,  1.52289871e+02],
         [ 7.55828400e+01,  1.31997223e+02,  1.51775757e+02],
         [ 7.59586639e+01,  1.32373047e+02,  1.52151581e+02],
         ...,
         [ 7.68187943e+01,  1.38500839e+02,  1.56192856e+02],
         [ 7.68188095e+01,  1.38500854e+02,  1.56192841e+02],
         [ 7.68188095e+01,  1.38500854e+02,  1.56192841e+02]],

        [[ 7.60515213e+01,  1.32465881e+02,  1.52244431e+02],
         [ 7.59733200e+01,  1.32387695e+02,  1.52166214e+02],
         [ 7.63387985e+01,  1.32753159e+02,  1.52531708e+02],
         ...,
         [ 7.67181396e+01,  1.38400146e+02,  1.56092163e+02],
         [ 7.67181244e+01,  1.38400162e+02,  1.56092178e+02],
         [ 7.67181320e+01,  1.38400177e+02,  1.56092163e+02]],

        ...,

        [[ 9.98160553e+01,  1.20287552e+02,  3.95508881e+01],
         [ 1.03795059e+02,  1.25791283e+02,  4.36515121e+01],
         [ 1.05114639e+02,  1.28117142e+02,  4.40065994e+01],
         ...,
         [ 1.06390022e+02,  1.25751274e+02,  4.62075577e+01],
         [ 1.05510178e+02,  1.24841278e+02,  4.33839493e+01],
         [ 9.98391418e+01,  1.19300049e+02,  3.51180725e+01]],

        [[ 9.31870270e+01,  1.14661629e+02,  3.25725708e+01],
         [ 1.07921799e+02,  1.29681992e+02,  4.67424469e+01],
         [ 1.07605896e+02,  1.30248184e+02,  4.56777954e+01],
         ...,
         [ 1.00495163e+02,  1.19859558e+02,  4.13804169e+01],
         [ 9.99863281e+01,  1.19170776e+02,  3.83058929e+01],
         [ 9.57622681e+01,  1.15001633e+02,  3.31129532e+01]],

        [[ 1.01265877e+02,  1.22496277e+02,  4.03028183e+01],
         [ 1.12068802e+02,  1.33676376e+02,  5.00867157e+01],
         [ 1.08309929e+02,  1.31249863e+02,  4.57988663e+01],
         ...,
         [ 9.07576447e+01,  1.09978981e+02,  3.21097488e+01],
         [ 8.99604492e+01,  1.09427834e+02,  2.96014786e+01],
         [ 9.18549347e+01,  1.10977539e+02,  3.00187531e+01]]],


       [[[ 1.91680695e+02,  2.38228241e+02,  2.35225067e+02],
         [ 1.92582962e+02,  2.39130493e+02,  2.36127335e+02],
         [ 1.93529877e+02,  2.40061951e+02,  2.37057846e+02],
         ...,
         [ 1.72848755e+02,  1.60696655e+02,  1.06524742e+02],
         [ 1.72897552e+02,  1.60745483e+02,  1.06573517e+02],
         [ 1.72923737e+02,  1.60771683e+02,  1.06599754e+02]],

        [[ 1.90356064e+02,  2.37042603e+02,  2.34052338e+02],
         [ 1.91232483e+02,  2.37919006e+02,  2.34928741e+02],
         [ 1.92235733e+02,  2.38913666e+02,  2.35919846e+02],
         ...,
         [ 1.73280212e+02,  1.61128143e+02,  1.06956184e+02],
         [ 1.72987717e+02,  1.60835648e+02,  1.06663681e+02],
         [ 1.72798050e+02,  1.60645981e+02,  1.06474014e+02]],

        [[ 1.88877411e+02,  2.35250122e+02,  2.32223495e+02],
         [ 1.89737762e+02,  2.36111267e+02,  2.33084335e+02],
         [ 1.90798828e+02,  2.37171631e+02,  2.34137512e+02],
         ...,
         [ 1.73966415e+02,  1.61814331e+02,  1.07642403e+02],
         [ 1.73338882e+02,  1.61186813e+02,  1.07014870e+02],
         [ 1.72980103e+02,  1.60828033e+02,  1.06656113e+02]],

        ...,

        [[ 1.62996979e+01,  9.67771759e+01,  1.01026886e+02],
         [ 1.63509140e+01,  9.67883759e+01,  1.01029572e+02],
         [ 1.63950500e+01,  9.67980347e+01,  1.01031891e+02],
         ...,
         [ 1.03893127e+01,  1.00420944e+02,  1.02773575e+02],
         [ 1.03893280e+01,  1.00420883e+02,  1.02773560e+02],
         [ 1.03893280e+01,  1.00420883e+02,  1.02773560e+02]],

        [[ 1.56857376e+01,  9.51733170e+01,  1.01032059e+02],
         [ 1.56471558e+01,  9.51697311e+01,  1.01032059e+02],
         [ 1.56139221e+01,  9.51666565e+01,  1.01032059e+02],
         ...,
         [ 1.06467667e+01,  9.76320877e+01,  1.02251343e+02],
         [ 1.06467667e+01,  9.76320877e+01,  1.02251343e+02],
         [ 1.06467667e+01,  9.76320877e+01,  1.02251343e+02]],

        [[ 1.49750595e+01,  9.38345718e+01,  1.01032059e+02],
         [ 1.49750671e+01,  9.38345947e+01,  1.01032059e+02],
         [ 1.49750671e+01,  9.38345947e+01,  1.01032059e+02],
         ...,
         [ 1.08568802e+01,  9.53557205e+01,  1.01825073e+02],
         [ 1.08568802e+01,  9.53557205e+01,  1.01825073e+02],
         [ 1.08568802e+01,  9.53557205e+01,  1.01825073e+02]]],


       [[[ 7.83921051e+01,  1.24869156e+02,  7.64564514e+01],
         [ 7.83407288e+01,  1.22990463e+02,  7.69286194e+01],
         [ 9.65125351e+01,  1.37986038e+02,  9.52618561e+01],
         ...,
         [ 1.18475189e+02,  1.53988434e+02,  1.06111961e+02],
         [ 1.15520340e+02,  1.49667419e+02,  1.04680801e+02],
         [ 1.05429680e+02,  1.39591858e+02,  9.70172577e+01]],

        [[ 8.45359344e+01,  1.31023071e+02,  8.38709183e+01],
         [ 8.28325424e+01,  1.27507378e+02,  8.16269608e+01],
         [ 9.68063507e+01,  1.38501221e+02,  9.59045181e+01],
         ...,
         [ 1.09455978e+02,  1.43719650e+02,  9.77291107e+01],
         [ 1.13780060e+02,  1.46499756e+02,  1.03052628e+02],
         [ 1.12625420e+02,  1.45448212e+02,  1.03386864e+02]],

        [[ 9.08953476e+01,  1.36272064e+02,  9.05691833e+01],
         [ 8.86048050e+01,  1.32403290e+02,  8.71840515e+01],
         [ 1.01192871e+02,  1.42970306e+02,  1.00444214e+02],
         ...,
         [ 1.06194847e+02,  1.38833054e+02,  9.51865082e+01],
         [ 1.17479927e+02,  1.49704147e+02,  1.07861160e+02],
         [ 1.11618668e+02,  1.43836151e+02,  1.02608093e+02]],

        ...,

        [[ 4.97378540e+01,  6.34630661e+01,  3.84943619e+01],
         [ 4.56276321e+01,  5.83987885e+01,  3.39005966e+01],
         [ 4.70007172e+01,  5.92259178e+01,  3.53177795e+01],
         ...,
         [ 1.30571686e+02,  1.43362213e+02,  1.07660027e+02],
         [ 1.26607735e+02,  1.35462082e+02,  1.00648926e+02],
         [ 1.20941162e+02,  1.26461502e+02,  9.25070038e+01]],

        [[ 5.56412773e+01,  6.93217773e+01,  4.46427994e+01],
         [ 5.28044205e+01,  6.67666626e+01,  4.17459335e+01],
         [ 5.03333588e+01,  6.32095261e+01,  4.01224670e+01],
         ...,
         [ 1.30266159e+02,  1.45737137e+02,  1.06612762e+02],
         [ 1.25733429e+02,  1.37645782e+02,  1.00882072e+02],
         [ 1.19405090e+02,  1.28468903e+02,  9.46394958e+01]],

        [[ 6.66966553e+01,  8.03032684e+01,  5.69682922e+01],
         [ 6.34833031e+01,  7.71879883e+01,  5.36312714e+01],
         [ 5.69458885e+01,  6.96463470e+01,  4.78098984e+01],
         ...,
         [ 1.32660233e+02,  1.49680664e+02,  1.07838509e+02],
         [ 1.26706856e+02,  1.41510147e+02,  1.03348465e+02],
         [ 1.19581390e+02,  1.32401215e+02,  9.78807983e+01]]]],
      dtype=float32)>, <tf.Tensor: shape=(8, 16800, 16), dtype=float32, numpy=
array([[[  5.612871  ,  68.04673   ,  -5.210217  , ...,  -2.4999998 ,
           0.        ,   0.        ],
        [  2.8064356 ,  34.023365  ,  -8.675953  , ...,  -1.2499999 ,
           0.        ,   0.        ],
        [  0.6128713 ,  68.04673   ,  -5.210217  , ...,  -2.4999998 ,
           0.        ,   0.        ],
        ...,
        [-11.308971  ,  -9.982915  , -22.538898  , ..., -12.187499  ,
           0.        ,   0.        ],
        [-23.867945  , -19.96583   , -19.07316   , ..., -24.374998  ,
           0.        ,   0.        ],
        [-11.933972  ,  -9.982915  , -22.538898  , ..., -12.187499  ,
           0.        ,   0.        ]],

       [[ 26.56724   ,  30.905638  ,   8.800869  , ...,  55.820175  ,
           1.        ,   0.        ],
        [ 13.28362   ,  15.452819  ,   5.3351336 , ...,  27.910088  ,
           1.        ,   0.        ],
        [ 21.56724   ,  30.905638  ,   8.800869  , ...,  55.820175  ,
           1.        ,   0.        ],
        ...,
        [-10.654148  , -11.143574  ,  -8.52781   , ..., -10.364994  ,
           1.        ,   0.        ],
        [-22.558296  , -22.287148  ,  -5.062074  , ..., -20.729988  ,
           1.        ,   0.        ],
        [-11.279148  , -11.143574  ,  -8.52781   , ..., -10.364994  ,
           1.        ,   0.        ]],

       [[ 38.281437  ,  85.41209   ,   2.6807168 , ...,  -2.4999998 ,
           0.        ,   0.        ],
        [ 19.140718  ,  42.706043  ,  -0.78501904, ...,  -1.2499999 ,
           0.        ,   0.        ],
        [ 33.281437  ,  85.41209   ,   2.6807168 , ...,  -2.4999998 ,
           0.        ,   0.        ],
        ...,
        [-10.288079  ,  -9.440247  , -14.647963  , ..., -12.187499  ,
           0.        ,   0.        ],
        [-21.826159  , -18.880493  , -11.182227  , ..., -24.374998  ,
           0.        ,   0.        ],
        [-10.913079  ,  -9.440247  , -14.647963  , ..., -12.187499  ,
           0.        ,   0.        ]],

       ...,

       [[337.60413   , 152.18748   ,  -0.9116125 , ..., 155.5406    ,
           1.        ,   0.        ],
        [168.80206   ,  76.09374   ,  -4.3773484 , ...,  77.7703    ,
           1.        ,   0.        ],
        [332.60413   , 152.18748   ,  -0.9116125 , ..., 155.5406    ,
           1.        ,   0.        ],
        ...,
        [ -0.93424535,  -7.3535156 , -18.240292  , ...,  -7.2487307 ,
           1.        ,   0.        ],
        [ -3.118491  , -14.707031  , -14.774556  , ..., -14.497461  ,
           1.        ,   0.        ],
        [ -1.5592455 ,  -7.3535156 , -18.240292  , ...,  -7.2487307 ,
           1.        ,   0.        ]],

       [[298.7106    , 101.13196   ,  14.91776   , ..., 126.88885   ,
           1.        ,   0.        ],
        [149.3553    ,  50.56598   ,  11.4520235 , ...,  63.444424  ,
           1.        ,   0.        ],
        [293.7106    , 101.13196   ,  14.91776   , ..., 126.88885   ,
           1.        ,   0.        ],
        ...,
        [ -2.1496675 ,  -8.949     ,  -2.4109201 , ...,  -8.144098  ,
           1.        ,   0.        ],
        [ -5.549335  , -17.898     ,   1.0548158 , ..., -16.288197  ,
           1.        ,   0.        ],
        [ -2.7746675 ,  -8.949     ,  -2.4109201 , ...,  -8.144098  ,
           1.        ,   0.        ]],

       [[193.08823   , 121.7647    ,  10.969757  , ..., 152.64705   ,
           1.        ,   0.        ],
        [ 96.54411   ,  60.88235   ,   7.5040207 , ...,  76.323524  ,
           1.        ,   0.        ],
        [188.08821   , 121.7647    ,  10.969757  , ..., 152.64705   ,
           1.        ,   0.        ],
        ...,
        [ -5.4503675 ,  -8.304227  ,  -6.3589225 , ...,  -7.339154  ,
           1.        ,   0.        ],
        [-12.150735  , -16.608454  ,  -2.8931866 , ..., -14.678308  ,
           1.        ,   0.        ],
        [ -6.0753675 ,  -8.304227  ,  -6.3589225 , ...,  -7.339154  ,
           1.        ,   0.        ]]], dtype=float32)>)
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 2102, in execution_mode
    yield
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 758, in _next_internal
    output_shapes=self._flat_output_shapes)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2610, in iterator_get_next
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 6843, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 649, in __next__
    return self.get_next()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 721, in get_next
    strict=True,
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 1210, in cond
    result = false_fn()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 720, in <lambda>
    lambda: out_of_range_fn(i, device),
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 702, in out_of_range_fn
    data = self._iterators[worker_index].get_next(device)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1457, in get_next
    return self._iterator.get_next(device)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 576, in get_next
    return self._device_iterators[index].get_next()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 825, in get_next
    return self._next_internal()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 764, in _next_internal
    return structure.from_compatible_tensor_list(self._element_spec, ret)
  File ""/usr/lib/python3.6/contextlib.py"", line 99, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py"", line 2105, in execution_mode
    executor_new.wait()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py"", line 67, in wait
    pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/retinaface-tf2/dist_data.py"", line 701, in <module>
    app.run(main)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run
    _run_main(main, args)
  File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""/mnt/retinaface-tf2/dist_data.py"", line 697, in main
    print(next(dist_train_iter))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 651, in __next__
    raise StopIteration
StopIteration
2020-09-08 10:19:30.991376: W tensorflow/core/common_runtime/eager/context.cc:566] Unable to destroy server_ object, so releasing instead. Servers don't support clean shutdown.

```
"
43038,Pylint incorrectly identifies tensorflow public API functions in tensorflow 2.2+,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Apart from the example below, no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04/OSX 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: :x:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2+
- Python version: 3.7.4
- Bazel version (if compiling from source): :x:
- GCC/Compiler version (if compiling from source): :x:
- CUDA/cuDNN version: :x:
- GPU model and memory: :x:


**Describe the current behavior**
When using pylint, a lot of functions from the public API are misidentified.
For example, the public api function `tf.split`, which is publicly defined as `tensorflow/python/ops/array_ops.split` is misidentified (I think as `tensorflow/python/ops/gen_array_ops.split`).

Other examples include `tf.random.uniform`, `tf.concat` and the list goes on.

Let's take the code snipped below:

`example.py`:
```python
import tensorflow as tf

tensor = tf.random.uniform((2, 4), minval=0, maxval=256)
tf.split(tensor, num_or_size_splits=2, axis=-1)
```

This is perfectly fine code, it runs as expected.

However, when running pylint both functions are misidentified and lots of linting errors are raised.
Running pylint on the module (`pylint -E example.py`) gives:
```
example.py:3:9: E1123: Unexpected keyword argument 'minval' in function call (unexpected-keyword-arg)
example.py:3:9: E1123: Unexpected keyword argument 'maxval' in function call (unexpected-keyword-arg)
example.py:3:9: E1120: No value for argument 'dtype' in function call (no-value-for-parameter)
example.py:4:0: E1123: Unexpected keyword argument 'num_or_size_splits' in function call (unexpected-keyword-arg)
example.py:4:0: E1124: Argument 'axis' passed by position and keyword in function call (redundant-keyword-arg)
example.py:4:0: E1120: No value for argument 'value' in function call (no-value-for-parameter)
example.py:4:0: E1120: No value for argument 'num_split' in function call (no-value-for-parameter)
```

**Describe the expected behavior**
Running `pylint -E example.py` should not give any errors.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf

tensor = tf.random.uniform((2, 4), minval=0, maxval=256)
tf.split(tensor, num_or_size_splits=2, axis=-1)
```

**Other info / logs**
This occurs with `tensorflow>=2.2.0`.
Previous versions of tensorflow `2.X` (e.g. tensorflow `2.1.X` and tensorflow `2.0.X`) do not have these problems.

I have used `pylint==2.6.0` for the example, but previous versions have the same behaviour.

One of the things that might have caused this (just a guess) is the upgrade to a new version of `gast` that occurred in tensorflow `2.2`, where they went from `gast==0.2.2` to `gast==0.3.3`.

Now, I know that this issue is not a code breaking issue, but it is a workflow breaking issue when using tensorflow in a professional setting. For example, one of the requirements for passing all steps in the CI may be running pylint, which now fails. Pylint allow disabling errors for specific third-party packages, so really the only solution is to add a `pylint: disable=...` comment every time you use a tensorflow function which is misidentified or to disable pylint for the project all together. Both options aren't desirable.

This issue was also raised in the `pylint` repo (https://github.com/PyCQA/pylint/issues/3613) and probably also related to https://github.com/PyCQA/pylint/issues/3596. But I don't think these issues belong in the `pylint` repo` (or `astroid` for that matter), but here in the tensorflow repo as it's probably caused by the import structure of tensorflow.

One lead might be that a wildstar import overwrites functions. An examples might be the wildstar import in https://github.com/tensorflow/tensorflow/blob/v2.2.0/tensorflow/python/ops/array_ops.py#L40 which overwrites `tensorflow/python/ops/array_ops.split` with the starred import `tensorflow/python/ops/gen_array_ops.split`. I'm not sure, but not performing wildstar imports might solve this linter problem."
43037,"Error ""failed to connect to all addresses"" when training on TPU with Colab","Hello!

**Describe the current behavior**
When running the my Colab I get the following error:
```
UnavailableError: failed to connect to all addresses
Additional GRPC error information from remote target /job:localhost/replica:0/task:0/device:CPU:0:
:{""created"":""@1599555147.064735819"",""description"":""Failed to pick subchannel"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/client_channel.cc"",""file_line"":3948,""referenced_errors"":[{""created"":""@1599555147.064732799"",""description"":""failed to connect to all addresses"",""file"":""third_party/grpc/src/core/ext/filters/client_channel/lb_policy/pick_first/pick_first.cc"",""file_line"":394,""grpc_status"":14}]}
	 [[{{node MultiDeviceIteratorGetNextFromShard}}]]
	 [[RemoteCall]]
```
When I try to iterator over my dataset:
```
<ipython-input-5-9732b6b5faf1> in train(self)
     42 
     43             for epoch_iter in range(1, 5):
---> 44                 for step, batch in enumerate(train_ds):
     45                     self.global_step = iterations.numpy()
     46 
```

**Describe the expected behavior**
Being able to iterate over the dataset.

**Standalone code to reproduce the issue**
The full Colab can be found here https://colab.research.google.com/drive/1jZaFmrDmBGbGkg8oKEJ78cL2PgIB7UF5?usp=sharing
"
43036,I want to minimize 'cost' with variable 'h0',"M=np.array([[1,0],[0,0]])
M=np.kron(M,np.identity(32))
M=tf.constant(M,dtype=tf.float32)
M=tf.complex(M,tf.zeros([64,64],tf.float32))
h0=tf.Variable(tf.random.normal([64,64],0,1,tf.float32),dtype=tf.float32,shape=[64,64])
h0_T=tf.transpose(h0)
ih = tf.complex(h0 - h0_T, h0 + h0_T)
u = tf.linalg.expm(ih)
u_dagger=tf.linalg.expm(-ih)
ro=tf.constant(ro[0],dtype=tf.float32)
ro=tf.complex(ro,tf.zeros([64,64],tf.float32))
cost= lambda: tf.math.real(tf.linalg.trace(u*ro*u_dagger*M))
opt = tf.keras.optimizers.SGD(learning_rate=0.001)
train=opt.minimize(cost, var_list=[h0]).numpy()



This is my code, and i error occurs like below



---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-4-0f648ef2f92b> in <module>
     12 cost= lambda: tf.math.real(tf.linalg.trace(u*ro*u_dagger*M))
     13 opt = tf.keras.optimizers.SGD(learning_rate=0.001)
---> 14 train=opt.minimize(cost, var_list=[h0]).numpy()
     15 with tf.Session() as sess:
     16     sess.run(tf.global_variables_initializer())

~\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py in minimize(self, loss, var_list, grad_loss, name)
    375         loss, var_list=var_list, grad_loss=grad_loss)
    376 
--> 377     return self.apply_gradients(grads_and_vars, name=name)
    378 
    379   def _clip_gradients(self, grads):

~\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py in apply_gradients(self, grads_and_vars, name, experimental_aggregate_gradients)
    511       ValueError: If none of the variables have gradients.
    512     """"""
--> 513     grads_and_vars = _filter_grads(grads_and_vars)
    514     var_list = [v for (_, v) in grads_and_vars]
    515 

~\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py in _filter_grads(grads_and_vars)
   1268   filtered = tuple(filtered)
   1269   if not filtered:
-> 1270     raise ValueError(""No gradients provided for any variable: %s."" %
   1271                      ([v.name for _, v in grads_and_vars],))
   1272   if vars_with_empty_grads:

ValueError: No gradients provided for any variable: ['Variable:0'].


What is problem? Thank you very much"
43035,Generate sparse tensor in tflite,"I'm trying to understand the `SparsityParameters` in tflite schema. But, AFAIK, there is only one example that contains this parameters(`tensorflow/lite/testdata/sparse_tensor.json`).

So, I've got trouble generating a network which has tensors whose contents include `SparsityParameters`.

As `tensorflow/core/util/sparse/README.md` said, it sounds like, even if I try to generate a sparse tensor, it will generate just two dense tensors with a shape.
> Sparse Tensors are stored as two dense tensors and a shape

And actually, when I write a code like below and convert it to tflite file,

```python
import tensorflow as tf
tf.compat.v1.disable_eager_execution()
lhs_ = tf.compat.v1.placeholder(dtype=tf.float32, shape=(2), name=""Hole"")
rhs_ = tf.compat.v1.placeholder(dtype=tf.float32, shape=(2), name=""Hole"")

st1 = tf.compat.v1.sparse.SparseTensor(
    indices=[[0, 0], [1, 2]], values=lhs_, dense_shape=[3, 4])
st2 = tf.compat.v1.sparse.SparseTensor(
    indices=[[0, 0], [1, 2]], values=rhs_, dense_shape=[3, 4])

# op_ = tf.compat.v1.sparse_add(st1, st2)
# op_ = tf.compat.v1.matmul(st1, st2)
```
It doesn't generate a sparse tensor.

**How can I generate the network that contains sparse tensors?**

p.s. I can manually force it to be generated with json file. But it doesn't seem natural.

<details>

```json
{
  version: 3,
  operator_codes: [
    {
      builtin_code: ""FULLY_CONNECTED""
    }
  ],
  subgraphs: [
    {
      tensors: [
        {
          shape: [
            1,
            8
          ],
          buffer: 2,
          name: ""input"",
          quantization: {
          }
        },
        {
          shape: [
            1,
            4
          ],
          buffer: 3,
          name: ""output"",
          quantization: {
          }
        },
        {
          shape: [
            4,
            8
          ],
          buffer: 1,
          name: ""weight"",
          quantization: {
          },
          sparsity: {
            traversal_order: [
              0,
              1
            ],
            dim_metadata: [
              {
                dense_size: 4
              },
              {
                format: ""SPARSE_CSR"",
                array_segments_type: ""Uint16Vector"",
                array_segments: {
                  values: [
                    0,
                    1,
                    3,
                    6,
                    10
                  ]
                },
                array_indices_type: ""Uint16Vector"",
                array_indices: {
                  values: [
                    1,
                    1,
                    4,
                    0,
                    4,
                    5,
                    0,
                    2,
                    6,
                    7
                  ]
                }
              }
            ]
          }
        }
      ],
      inputs: [
        0
      ],
      outputs: [
        1
      ],
      operators: [
        {
          inputs: [
            0,
            2,
            -1
          ],
          outputs: [
            1
          ],
          builtin_options_type: ""FullyConnectedOptions"",
          builtin_options: {
          }
        }
      ]
    }
  ],
  description: ""TOCO Converted."",
  buffers: [
    {
    },
    {
      data: [
        0,
        0,
        128,
        63,
        0,
        0,
        128,
        63,
        0,
        0,
        0,
        64,
        0,
        0,
        128,
        63,
        0,
        0,
        0,
        64,
        0,
        0,
        64,
        64,
        0,
        0,
        128,
        63,
        0,
        0,
        0,
        64,
        0,
        0,
        0,
        64,
        0,
        0,
        128,
        63
      ]
    },
    {
    },
    {
    }
  ]
}
```

</details>

"
43033,ImportError: DLL load failed: The specified module could not be found.,"Name: tensorflow
Version: 2.3.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author: Google Inc.
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: c:\users\user\anaconda3\lib\site-packages
Requires: numpy, wrapt, gast, protobuf, google-pasta, keras-preprocessing, tensorflow-estimator, termcolor, scipy, opt-einsum, tensorboard, absl-py, astunparse, wheel, grpcio, six, h5py
Required-by: 


---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     63   try:
---> 64     from tensorflow.python._pywrap_tensorflow_internal import *
     65   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-3-ac9bda315bf2> in <module>
      1 # Importing the required libraries
----> 2 import tensorflow as tf
      3 
      4 
      5 # Import MNIST data

~\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     39 
---> 40 from tensorflow.python.eager import context
     41 
     42 # pylint: enable=wildcard-import

~\anaconda3\lib\site-packages\tensorflow\python\eager\context.py in <module>
     33 from tensorflow.core.protobuf import config_pb2
     34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
     36 from tensorflow.python import tf2
     37 from tensorflow.python.client import pywrap_tf_session

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py in <module>
     26 
     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
---> 28 from tensorflow.python import pywrap_tensorflow
     29 from tensorflow.python._pywrap_tfe import *

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     81 for some common reasons and solutions.  Include the entire stack trace
     82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83   raise ImportError(msg)
     84 
     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\user\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
43032, Neural machine translation with attention issue :Translate function give Keyerror with Tunisian dialect,"
i 'm working with attention model for Tunisian dialect _Standard arabic translation , with google colab ,

![image](https://user-images.githubusercontent.com/53334878/92415130-6b1d1580-f14f-11ea-8ffd-c56e417a2901.png)
"
43031,AttributeError: module 'tensorflow' has no attribute 'Dimension',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

My list of packages:

c:\Dev\Projects\TooGAN\stylegan>pip list
Package                  Version
------------------------ ------------
absl-py                  0.9.0
appdirs                  1.4.4
asgiref                  3.2.5
astor                    0.8.1
astroid                  2.3.3
attrs                    19.3.0
audioread                2.1.8
backcall                 0.1.0
bleach                   3.1.3
cachetools               4.0.0
certifi                  2019.11.28
cffi                     1.14.0
chardet                  3.0.4
colorama                 0.4.3
cv                       1.0.0
cycler                   0.10.0
decorator                4.4.2
defusedxml               0.6.0
distlib                  0.3.1
Django                   3.0.4
django-crispy-forms      1.9.0
dnspython                1.16.0
dtw                      1.4.0
entrypoints              0.3
fake-useragent           0.1.11
ffmpeg                   1.4
filelock                 3.0.12
gast                     0.2.2
google-auth              1.11.3
google-auth-oauthlib     0.4.1
google-images-download   2.8.0
google-pasta             0.2.0
grpcio                   1.27.2
h5py                     2.10.0
idna                     2.9
imageio                  2.8.0
importlib-metadata       1.5.0
imutils                  0.5.3
ipykernel                5.1.4
ipython                  7.13.0
ipython-genutils         0.2.0
ipywidgets               7.5.1
isort                    4.3.21
jedi                     0.16.0
Jinja2                   2.11.1
joblib                   0.14.1
jsonschema               3.2.0
jupyter                  1.0.0
jupyter-client           6.1.0
jupyter-console          6.1.0
jupyter-core             4.6.3
Keras-Applications       1.0.8
Keras-Preprocessing      1.1.0
kiwisolver               1.1.0
lazy-object-proxy        1.4.3
librosa                  0.7.2
llvmlite                 0.31.0
Markdown                 3.2.1
MarkupSafe               1.1.1
matplotlib               3.2.1
mccabe                   0.6.1
mido                     1.2.9
mistune                  0.8.4
mysql-connector-python   8.0.19
mysqlclient              1.4.6
nbconvert                5.6.1
nbformat                 5.0.4
notebook                 6.0.3
numba                    0.48.0
numpy                    1.18.2
oauthlib                 3.1.0
opencv-python            4.4.0.42
opt-einsum               3.2.0
pandocfilters            1.4.2
parso                    0.6.2
pickleshare              0.7.5
Pillow                   7.1.1
pip                      20.2.2
progressbar              2.5
prometheus-client        0.7.1
prompt-toolkit           3.0.4
protobuf                 3.11.3
pyasn1                   0.4.8
pyasn1-modules           0.2.8
pycparser                2.20
pydub                    0.23.1
Pygments                 2.6.1
pylint                   2.4.4
pyparsing                2.4.6
pyrsistent               0.15.7
python-dateutil          2.8.1
pytz                     2019.3
pywin32                  227
pywinpty                 0.5.7
PyYAML                   5.3.1
pyzmq                    19.0.0
qtconsole                4.7.1
QtPy                     1.9.0
requests                 2.23.0
requests-oauthlib        1.3.0
resampy                  0.2.2
rsa                      4.0
scikit-learn             0.22.2.post1
scipy                    1.4.1
selenium                 3.141.0
Send2Trash               1.5.0
setuptools               41.2.0
six                      1.14.0
SoundFile                0.10.3.post1
SpeechRecognition        3.8.1
sqlparse                 0.3.1
tensorboard              2.1.1
tensorflow-estimator     2.1.0
tensorflow-gpu           2.1.0
tensorflow-gpu-estimator 2.1.0
termcolor                1.1.0
terminado                0.8.3
testpath                 0.4.4
tornado                  6.0.4
traitlets                4.3.3
typed-ast                1.4.1
urllib3                  1.25.8
virtualenv               20.0.31
wcwidth                  0.1.8
webencodings             0.5.1
Werkzeug                 1.0.0
wheel                    0.34.2
widgetsnbextension       3.5.1
wrapt                    1.11.2
zipp                     3.1.0

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I am trying to run StyleGan library

https://github.com/NVlabs/stylegan

Running ""python pretrained_example.py""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
43030,Could not load dynamic library 'cupti64_110.dll'; dlerror: cupti64_110.dll not found,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Education
- TensorFlow version: tf-nightly-2.4.0.dev20200907
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: 11.0/8.0.2
- GPU model and memory: GeForce GTX 1050, 4GB

**Describe the problem**
cupti64_110.dll cannot be found, and the only CUPTI .dll file I can find in the CUDA 11.0 files is C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.0\extras\CUPTI\lib64\cupti64_2020.1.1.dll

Am I missing some files/part of the installation? 

Thanks
"
43029,TensorFlow2 tutorials Actor-Critic method,"## URL(s) with the issue:

https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb

## Description of issue (what needs changing):

### Clear description
At the second cell, we want to install additional packages for visualization.
Maybe should be added ""sudo apt-get update"" before the line ""sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1"".
Let the xvfb package can be installed normally.

### Usage example
Correct the second cell as below:
%%bash

sudo apt-get update
sudo apt-get install -y xvfb python-opengl > /dev/null 2>&1
pip install pyvirtualdisplay > /dev/null 2>&1
pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1
"
43028,Installation error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: I use PC
- TensorFlow installed from (source or binary):
- TensorFlow version: 
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: NVIDIA GeForce GTX 750 Ti



**Describe the problem**

**Provide the exact sequence of commands/steps that you executed before running into the problem**


**Any other info/logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



--MY QUESTION STARTS HERE--
Whenever I do  pip install tensor flow, it runs code then it shows this

Installing collected packages: tensorflow
ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\minad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\tensorflow\\include\\external\\com_github_grpc_grpc\\src\\core\\ext\\filters\\client_channel\\lb_policy\\grpclb\\client_load_reporting_filter.h'

please help"
43027,Please update the tflite_convert -h command line help ,"What I see when I run 

tflite_convert -h


```
usage: tflite_convert [-h] --output_file OUTPUT_FILE [--saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE]
                      [--enable_v1_converter] [--experimental_new_converter [EXPERIMENTAL_NEW_CONVERTER]]

Command line tool to run TensorFlow Lite Converter.

optional arguments:
  -h, --help            show this help message and exit
  --output_file OUTPUT_FILE
                        Full filepath of the output file.
  --saved_model_dir SAVED_MODEL_DIR
                        Full path of the directory containing the SavedModel.
  --keras_model_file KERAS_MODEL_FILE
                        Full filepath of HDF5 file containing tf.Keras model.
  --enable_v1_converter
                        Enables the TensorFlow V1 converter in 2.0
  --experimental_new_converter [EXPERIMENTAL_NEW_CONVERTER]
                        Experimental flag, subject to change. Enables MLIR-based conversion instead of TOCO conversion. (default
                        True)
```


and what I see from python is 

```

usage: tflite_convert.py [-h] --output_file OUTPUT_FILE
                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)
                          [--output_format {TFLITE,GRAPHVIZ_DOT}]
                          [--inference_type {FLOAT,QUANTIZED_UINT8}]
                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
                          [--input_arrays INPUT_ARRAYS]
                          [--input_shapes INPUT_SHAPES]
                          [--output_arrays OUTPUT_ARRAYS]
                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]
                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
                          [--std_dev_values STD_DEV_VALUES]
                          [--mean_values MEAN_VALUES]
                          [--default_ranges_min DEFAULT_RANGES_MIN]
                          [--default_ranges_max DEFAULT_RANGES_MAX]
                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]
                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]
                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]
                          [--allow_custom_ops ALLOW_CUSTOM_OPS]

```


If the above is available for tflite_convert using the command line could the help file reflect that ability.

"
43026,ImportError: DLL load failed: The specified procedure could not be found.,"
**System information**
OS Name	Microsoft Windows 10 Pro
Version	10.0.17134 Build 17134
System Type	x64-based PC

**Describe the problem**
I was following this [guide](https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606)
So I installed the dependencies in the terminal:

`pip3 install tensorflow==1.13.1 `
`pip3 install opencv-python`
`pip3 install keras==2.2.4`
`pip3 install numpy==1.16.1`
`pip3 install imageai --upgrade`

**Python Version:**
```
C:\Users\danieltsilva\Documents\Python\document_segmentaton36>python --version
Python 3.6.0
```
**Some more info:**
```
C:\Users\danieltsilva\Documents\Python\document_segmentaton36>python -m pip list
Package              Version
-------------------- ---------
absl-py              0.10.0
astor                0.8.1
certifi              2020.6.20
cycler               0.10.0
gast                 0.4.0
google-pasta         0.2.0
grpcio               1.31.0
h5py                 2.10.0
imageai              2.1.5
importlib-metadata   1.7.0
Keras                2.2.4
Keras-Applications   1.0.8
Keras-Preprocessing  1.1.2
kiwisolver           1.2.0
Markdown             3.2.2
matplotlib           3.3.1
mock                 4.0.2
numpy                1.16.1
opencv-python        4.4.0.42
Pillow               7.2.0
pip                  20.2.2
protobuf             3.13.0
pyparsing            2.4.7
python-dateutil      2.8.1
PyYAML               5.3.1
scipy                1.5.2
setuptools           50.3.0
six                  1.15.0
tensorboard          1.13.1
tensorflow           1.13.1
tensorflow-estimator 1.13.0
termcolor            1.1.0
Werkzeug             1.0.1
wheel                0.35.1
wrapt                1.12.1
zipp                 3.1.0
```
**The code Im running** (the one on the guide i mentioned):

```
from imageai.Detection import ObjectDetection
import os

execution_path = os.getcwd()

detector = ObjectDetection()
detector.setModelTypeAsRetinaNet()
detector.setModelPath( os.path.join(execution_path , ""resnet50_coco_best_v2.0.1.h5""))
detector.loadModel()
detections = detector.detectObjectsFromImage(input_image=os.path.join(execution_path , ""image.jpg""), output_image_path=os.path.join(execution_path , ""imagenew.jpg""))

for eachObject in detections:
    print(eachObject[""name""] , "" : "" , eachObject[""percentage_probability""] )
```

**The error I get:**
```

C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\Scripts\python.exe C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\document_extraction.py
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\document_extraction.py"", line 1, in <module>
    from imageai.Detection import ObjectDetection
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\imageai\Detection\__init__.py"", line 2, in <module>
    from imageai.Detection.keras_retinanet.models.resnet import resnet50_retinanet
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\imageai\Detection\keras_retinanet\models\resnet.py"", line 19, in <module>
    import keras
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\danieltsilva\Documents\Python\document_segmentaton36\venv\new_env\lib\site-packages\google\protobuf\descriptor.py"", line 48, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.

Process finished with exit code 1

```

I don´t really know what's failing. If there is a similar issue pls hit me up (i haven´t found it), or if you have any tip tho share with me. Thank you <3"
43025,Using Intermediate (non-Input) Layer as input to secondary network,"**System information**
- TensorFlow version: 2.3.0 
- Are you willing to contribute it: No



**Describe the feature and the current behavior/state.**
Currently, a user can define a network to only run partially through a network using something like below:
```
# Load Pre-Trained Model
model = ...
# Using I/O of previous model, build model with an additional (or replacement) output 
first_half = tf.keras.models.Model(model.inputs, model.get_layer(""some_intermediate_layer_n"").output)
```
However, if you were to build a final model to run through the remainder of the network, from `some_intermediate_layer_n` onward, it falls over. It would be nice to be able to make a similar call, to use the base models outputs, but starting from an intermediate input step as below. You could use the output from the network above as the input for the remainder of the network in this example. 
```
 second_half = tf.keras.models.Model(model.get_layer(""some_intermediate_layer_n_plus_1"").input, model.outputs)
```

 The exact warning is `WARNING:tensorflow:Functional inputs must come from tf.keras.Input (thus holding past layer metadata), they cannot be the output of a previous non-Input layer.` This is quickly followed by a `Graph Disconnected` error.



**Will this change the current api? How?**
Not to my knowledge. Should be able to call in a similar fashion above, where the new input is the intermediate layer you wish to start from.

**Who will benefit with this feature?**
Any network architecture where an intermediate representation is important. Architecture's reliant on pairs of networks, or encoding networks would benefit at the inference stage.  

**Any Other info.**
I'm assuming it's quite likely this has been requested before but I couldn't find it. If so, it would also be nice to have an explanation as to why the current limitation to inputs coming from `tf.keras.Input (thus holding past layer metadata)` is important. I understand this from a training perspective, but when inferring, I would've thought this could be overlooked.

I of course understand you could build two networks with the latter building on from the prior but this way you can wrap it all up into one when training, or if using a pre-trained network, you can split it out as and when needed. 

Thank you."
43022,"Building and testing CPU Ops, ","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 2.3
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.5
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
In Custom Ops, I'm trying to build the example files. When I run the build commands I just get an error saying:

""Executing genrule @local_config_tf//:tf_header_include failed (Exit 35584)""


**Provide the exact sequence of commands / steps that you executed before running into the problem**

$ ./configure.sh
Do you want to build ops again TensorFlow CPU pip package? Y or enter for CPU (tensorflow), N for GPU (tensorflow-gpu). [Y/n] y
Build with CPU pip package.
On windows, skipping toolchain flags..
Are you building against TensorFlow 2.1(including RCs) or newer?[Y/n] y
Build against TensorFlow 2.1 or newer.
Using installed tensorflow
2020-09-07 15:46:38.915193: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-07 15:46:38.915211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2020-09-07 15:46:40.468711: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-07 15:46:40.468729: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

$ bazel build tensorflow_zero_out:python/ops/_zero_out_ops.so
Loading:
Loading: 0 packages loaded
Analyzing: target //tensorflow_zero_out:python/ops/_zero_out_ops.so (0 packages loaded, 0 targets configured)
INFO: Analyzed target //tensorflow_zero_out:python/ops/_zero_out_ops.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
[0 / 5] [Prepa] BazelWorkspaceStatusAction stable-status.txt
ERROR: C:/users/patrik.veges/_bazel_patrik.veges/gdil2b47/external/local_config_tf/BUILD:17:8: Executing genrule @local_config_tf//:tf_header_include failed (Exit 35584)
      0 [] bash 287 cygwin_exception::open_stackdumpfile: Dumping stack trace to bash.exe.stackdump
Target //tensorflow_zero_out:python/ops/_zero_out_ops.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: C:/users/patrik.veges/_bazel_patrik.veges/gdil2b47/external/local_config_tf/BUILD:3:11 Executing genrule @local_config_tf//:tf_header_include failed (Exit 35584)
INFO: Elapsed time: 0.899s, Critical Path: 0.80s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

-
"
43021,"TF_TensorToPyArray is hard coded for equality, limits performance enhancements ","Hi,

I am running one CNN model which has convolution and FC layers. 
For getting optimal performance i am using buffers allocated with allocate_persistent() API and I am reusing these buffers multiple times as Network is pretty sequential. 
For achieving this i pre-allocate big chunks with allocate_persistent() API and for every Op execution i use the same buffers by setting with different shape. 

With convolutions it worked fine and i see some performance gains. However once FC layer gets executed it throws below exception. Because of this exception, I am not able to run my network. 

_INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes
I0907 13:07:13.556839 140574226294592 coordinator.py:224] Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InternalError'>, ndarray was 1921920 bytes but TF_Tensor was 12331253760 bytes_

After looking into code, I found function **TF_TensorToPyArray()** gets called, which compare the ndarray bytes with TF_tensor bytes, which is fine, But why does it need to be exact match?
It should have been below comparison for throwing error, instead of exact match.  
_PyArray_NBYTES(py_array)) >
             TF_TensorByteSize(tensor.get()_

This way same buffers can be used for Op execution, just we need to change the shape.

For experiment i changed 
set_shape(const TensorShape& shape)  to Public scope, looking for some alternate-native for this too. "
43020,Arduino tensorflow,"- Mac OS Catalina Version 10.15.6
- Arduino 1.8.13
- Tensorflow Library ""Arduino_TensorFlowLite"" version 2.1.0-ALPHA-precompiled
- Arduino Nano BLE 33

Following the Arduino instructions as listed on the [tensorflow github](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/person_detection#running-on-arduino
) the default person detection demonstration fails on compilation looking for `cmsis_compiler.h`

I was able to simply copy v5.01 of the `cmsis_compiler_h` to the `../Arduino_TensorFlowLite/src/tensorflow/lite/micro/tools/make/downloads/cmsis/CMSIS/DSP/Include` directory and the demonstration compiled successfully and worked.
"
43019,action_env ignored in some cc_library calls,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7.5
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0
- Python version: 3.7.4
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): 8.3

**Describe the problem**

I'm trying to build TensorFlow with TF_SYSTEM_LIBS and need to set `$CPATH` via `--action_env`. However for some instances of `cc_library` this env variable is not passed through to the compiler invocation.   
I observed the issue for tensorflow/python/tools/BUILD:226:10 C++ compilation of rule '//tensorflow/core/platform/default:mutex'. The definition for that is at https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/core/platform/default/BUILD#L206

Log output:

```
  (cd /tmp/easybuild-tmp/eb-tHWBp_/tmphugHLy-bazel-build/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=<redacted> \
    PATH=<redacted> \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/ppc-opt-exec-50AE0418/bin/tensorflow/core/platform/default/_objs/mutex/mutex.d '-frandom-seed=bazel-out/ppc-opt-exec-50AE0418/bin/tensorflow/core/platform/default/_objs/mutex/mutex.o' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/ppc-opt-exec-50AE0418/bin -iquote external/com_google_absl -iquote bazel-out/ppc-opt-exec-50AE0418/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/ppc-opt-exec-50AE0418/bin/external/nsync -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIE -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 -g0 '-std=c++14' -c tensorflow/core/platform/default/mutex.cc -o bazel-out/ppc-opt-exec-50AE0418/bin/tensorflow/core/platform/default/_objs/mutex/mutex.o)
Execution platform: @local_execution_config_platform//:platform
```

However it doesn't always occur:  E.g. for `SUBCOMMAND: # @com_google_absl//absl/strings:strings [action 'Compiling external/com_google_absl/absl/strings/internal/memutil.cc', configuration: cab848d308c51b34c791977a9ba0d73e541cabe37f3e6da7b163b34d8bf29b6b, execution platform: @local_execution_config_platform//:platform]` I see all my action_env variables being passed and that is also added with cc_library: https://github.com/abseil/abseil-cpp/blob/df3ea785d8c30a9503321a3d35ee7d35808f190d/absl/strings/BUILD.bazel#L30

Cross-posting this from https://github.com/bazelbuild/bazel/issues/12059 as I'm not sure this is a Bazel or TF issue as it seems to only happens sometimes I'd guess it is on the TF side.

Same issue (I guess) reported at https://github.com/tensorflow/tensorflow/issues/37861#issuecomment-686418236

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- `TF_SYSTEM_LIBS=icu,jsoncpp_git,lmdb,nasm,pcre,org_sqlite,swig,curl,double_conversion,flatbuffers,gif,hwloc,libjpeg_turbo,png,nsync,com_google_protobuf,pybind11,snappy,zlib,absl_py,astor_archive,astunparse_archive,cython,enum34_archive,functools32_archive,gast_archive,opt_einsum_archive,pasta,six_archive,termcolor_archive,wrapt"" ./configure`
- `bazel --output_base=<...> --install_base=/<...> --output_user_root=<...> build --compilation_mode=opt --config=opt --subcommands --verbose_failures --config=noaws --jobs=64 --copt=""-fPIC"" --action_env=CPATH='<...>' --action_env=LIBRARY_PATH='<...>' --action_env=PYTHONPATH --action_env=PYTHONNOUSERSITE=1 --distinct_host_configuration=false  //tensorflow/tools/pip_package:build_pip_package`"
43016,Dataset.from_generator breaks on last element,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from: Docker Image
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.6.9

Running the attached gist throws InvalidArgumentError: Argument 0 is out of range. [[{{node args_0}}]].

**Describe the expected behavior**
It should run through the dataset, and once completed exit gracefully.

**Standalone code to reproduce the issue**
https://gist.github.com/optiluca/f97a34237837d19f9643b54ef17f8a76

**Other info / logs**
This issue goes away if I do any one of the following:
- If I don't batch the data ( commenting line 41)
- If I don't run _transform_after_windowing (commenting line 38)

I noticed this issue when running a similar pipeline and feeding it to a tf.keras model.predict().  It broke with the aforementioned error."
43015,Wrong licenses / license breach in TensorFlow,"Hi TensorFlow authors,
My name is Petr Prokop, I serve in NXP as a Trusted Advisor – person in charge of SW licenses compliance.
NXP adapts TensorFlow for its i.MX processors. Such releases are subject of license-compliance check.

Unfortunately, our scanning tool (Black Duck Protex by Synopsis) has reported following license breach:
TensorFlow file https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache.cc is very similar to
LevelDB file https://github.com/google/leveldb/blob/master/util/cache.cc.

Likewise, files
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache.h
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/cache_test.cc
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/io/table.cc
are reported to be similar to 
https://github.com/google/leveldb/blob/master/include/leveldb/cache.h
https://github.com/google/leveldb/blob/master/util/cache_test.cc
https://github.com/google/leveldb/blob/master/table/table.cc.

As many SW tools, Black Duck Protex has only limited efficiency, so I’ve found some further files in core/lib/io directory during my investigation manually. 

My understanding is, that TensorFlow reuses LevelDB files, that is pretty fine according original LevelDB license (BSD-3-Clause). However, changing both copyright and license breaches claim#1 of BSD. I wouldn’t dare to advise you anything, but described state is an obstacle for using TensorFlow by NXP. Simultaneously, our internal policy prohibits to change 3rd party copyrights, even if they were wrong.

Can I ask you to allow update of *.h, *.cc and addition of Authors/License_LevelDB.txt files in your repository?
My modification are prepared in https://github.com/PetrProkopNXP/tensorflow, based on original TensorFlow master branch.

Feel free change exact wording of “Modification” clause and/or formatting.

Regards,
Petr"
43014,"tf.estimator.train_and_evaluate  ""loss"" meaning in ""saving dict for global step""","when i run tf.estimator.train_and_evaluate, log prints that:
[INFO:tensorflow:Saving dict for global step 1000: eval_loss = 0.0055662687, global_step = 1000, loss = 3.3700151]
INFO:tensorflow:global_steps = 1000, loss = 0.05528491 (1202.815 sec)

question:
what's meaning of the ""loss = 3.3700151""

=====================log=====================
```
2020-09-07 12:33:03.540827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-09-07 12:33:03.542687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-07 12:33:03.542709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-09-07 12:33:03.542718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-09-07 12:33:03.542826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22853 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt.
2020-09-07 12:33:26.224454: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
INFO:tensorflow:loss = 96.96749, step = 0
INFO:tensorflow:global_steps = 0, loss = 96.96749
INFO:tensorflow:global_step/sec: 0.803899
INFO:tensorflow:loss = 4.264659, step = 100 (124.394 sec)
INFO:tensorflow:global_step/sec: 0.682575
INFO:tensorflow:loss = 3.211001, step = 200 (146.504 sec)
INFO:tensorflow:global_step/sec: 0.442842
INFO:tensorflow:loss = 0.8785575, step = 300 (225.815 sec)
INFO:tensorflow:global_step/sec: 0.388806
INFO:tensorflow:loss = 1.20301, step = 400 (257.197 sec)
INFO:tensorflow:global_step/sec: 0.389973
INFO:tensorflow:loss = 0.50117, step = 500 (256.429 sec)
INFO:tensorflow:global_steps = 500, loss = 0.50117 (1010.338 sec)
INFO:tensorflow:global_step/sec: 0.431264
INFO:tensorflow:loss = 0.52161276, step = 600 (231.876 sec)
INFO:tensorflow:global_step/sec: 0.42097
INFO:tensorflow:loss = 0.32268906, step = 700 (237.547 sec)
INFO:tensorflow:global_step/sec: 0.409772
INFO:tensorflow:loss = 0.28711706, step = 800 (244.038 sec)
INFO:tensorflow:global_step/sec: 0.398877
INFO:tensorflow:loss = 0.32013908, step = 900 (250.704 sec)
INFO:tensorflow:Saving checkpoints for 1000 into /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt.
INFO:tensorflow:Calling model_fn.
I:NER Training:[ber:mod:384]:*** Features ***
I:NER Training:[ber:mod:386]:  name = input_ids, shape = (?, 202)
I:NER Training:[ber:mod:386]:  name = input_mask, shape = (?, 202)
I:NER Training:[ber:mod:386]:  name = label_ids, shape = (?, 202)
I:NER Training:[ber:mod:386]:  name = segment_ids, shape = (?, 202)
WARNING:tensorflow:From /home/panyinghao/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:363: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2020-09-07T05:10:03Z
INFO:tensorflow:Graph was finalized.
2020-09-07 13:10:03.661048: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-09-07 13:10:03.661119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-07 13:10:03.661130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-09-07 13:10:03.661137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-09-07 13:10:03.661247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22853 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/panyinghao/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1000
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [10/100]
INFO:tensorflow:Evaluation [20/100]
INFO:tensorflow:Finished evaluation at 2020-09-07-05:10:18
INFO:tensorflow:Saving dict for global step 1000: eval_loss = 0.0055662687, global_step = 1000, loss = 3.3700151
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1000: /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1000
INFO:tensorflow:global_step/sec: 0.419043
WARNING:tensorflow:From /home/panyinghao/anaconda3/envs/tf13/lib/python3.6/site-packages/tensorflow/python/summary/summary_iterator.py:68: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.
Instructions for updating:
Use eager execution and: 
`tf.data.TFRecordDataset(path)`
INFO:tensorflow:loss = 0.05528491, step = 1000 (238.650 sec)
INFO:tensorflow:global_steps = 1000, loss = 0.05528491 (1202.815 sec)
INFO:tensorflow:global_step/sec: 0.66271
INFO:tensorflow:loss = 0.20649494, step = 1100 (150.885 sec)
INFO:tensorflow:global_step/sec: 0.681622
INFO:tensorflow:loss = 0.21718065, step = 1200 (146.709 sec)
INFO:tensorflow:global_step/sec: 0.684914
INFO:tensorflow:loss = 0.12876259, step = 1300 (146.004 sec)
INFO:tensorflow:global_step/sec: 0.680007
INFO:tensorflow:loss = 0.100298606, step = 1400 (147.057 sec)
INFO:tensorflow:global_step/sec: 0.676314
INFO:tensorflow:loss = 0.12061991, step = 1500 (147.873 sec)
INFO:tensorflow:global_steps = 1500, loss = 0.12061991 (738.528 sec)
INFO:tensorflow:global_step/sec: 0.642209
INFO:tensorflow:loss = 0.0042267526, step = 1600 (155.699 sec)
INFO:tensorflow:global_step/sec: 0.637592
INFO:tensorflow:loss = 0.0035796848, step = 1700 (156.841 sec)
INFO:tensorflow:Saving checkpoints for 1793 into /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt.
INFO:tensorflow:Calling model_fn.
I:NER Training:[ber:mod:384]:*** Features ***
I:NER Training:[ber:mod:386]:  name = input_ids, shape = (?, 202)
I:NER Training:[ber:mod:386]:  name = input_mask, shape = (?, 202)
I:NER Training:[ber:mod:386]:  name = label_ids, shape = (?, 202)
I:NER Training:[ber:mod:386]:  name = segment_ids, shape = (?, 202)
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2020-09-07T05:30:20Z
INFO:tensorflow:Graph was finalized.
2020-09-07 13:30:21.334671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2020-09-07 13:30:21.334728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-07 13:30:21.334737: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2020-09-07 13:30:21.334743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2020-09-07 13:30:21.835343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 22853 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5)
INFO:tensorflow:Restoring parameters from /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1793
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Evaluation [10/100]
INFO:tensorflow:Evaluation [20/100]
INFO:tensorflow:Finished evaluation at 2020-09-07-05:30:38
INFO:tensorflow:Saving dict for global step 1793: eval_loss = 0.005485246, global_step = 1793, loss = 4.2882214
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1793: /home/panyinghao/model/checkpoints/test/test_10ep/model.ckpt-1793
INFO:tensorflow:Loss for final step: 0.12153285.
```"
43013,Why is the quantified TF_LITE slower than the unquantified TF_LITE test time on the PC side？,"
import tensorflow as tf
import numpy as np
import cv2
import os
import time
from PIL import Image


model = ""D:/sfz/tf_sfz/uint9.tflite""
interpreter = tf.lite.Interpreter(model_path=model)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
print(str(input_details), '\n', str(output_details))
img = Image.open(“D:/data/000001.jpg”)
        num = img.size[0] * 32 // img.size[1]
        if num > max_width:
            num = max_width
        if num < 5:
            num = 5
        out_mid = img.resize((num, 32))
        out = img.resize((max_width, 32))
        out_crop = out_mid.crop((num - 1, 0, out_mid.size[0], out_mid.size[1]))
        out.paste(out_mid, (0, 0))
        while num < max_width:
            out.paste(out_crop, (num, 0))
            num += 1
        img = cv2.cvtColor(np.asarray(out), cv2.COLOR_RGB2BGR)
        inputImg=(img/255).astype(np.float32)
        interpreter = tf.lite.Interpreter(model_path=model)
        interpreter.allocate_tensors()
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        interpreter.set_tensor(input_details[0]['index'], [inputImg])
        a = time.time()
        # print(a)
        interpreter.invoke()
        b = time.time()
        # print(b)
        print((b - a) * 1000)

"
43011,Same variable name for tf.feature_column.embedding_column embedding tensors in tf 2.3.0,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: 64 bit Linux
- TensorFlow installed from source
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0 
- Python version: 3.6.10
- Cuda compilation tools, release 10.0, V10.0.130

**Describe the current behavior**
In TF 2.1.0, when I use tf.feature_column.embedding_column, the variable names of the embedding variables are derived from the categorical column name.
Example: if I create an embedding column using a categorical column
feature_alpha = tf.feature_column.categorical_column_with_hash_bucket('feature_alpha', 100, dtype=tf.dtypes.string)
alpha_emb = tf.feature_column.embedding_column(feature_alpha, dimension=10)

feature_beta = tf.feature_column.categorical_column_with_hash_bucket('feature_beta', 200, dtype=tf.dtypes.string)
beta_emb = tf.feature_column.embedding_column(feature_beta, dimension=20)

the embedding variable for 'feature_alpha' is named 'dense_features/feature_alpha_embedding/embedding_weights:0'
the embedding variable for 'feature_beta' is named 'dense_features/feature_beta_embedding/embedding_weights:0'

the variable names contain the categorical column name, 'feature_alpha...' and 'feature_beta...'

In TF 2.3.0

the embedding variable for 'feature_alpha' is named 'dense_features/embedding_weights:0'.
the embedding variable for 'feature_beta' is named 'dense_features/embedding_weights:0'

This works fine if there is only a single feature column. but when there are multiple feature columns.
The embedding variables are assigned the same name, which creates problems while saving the model.
I get the error ""RuntimeError: Unable to create link (name already exists)"" while saving the model, since the embedding variables for 'feature_alpha' and 'feature_beta' have the same variable name.

**Describe the expected behavior**

the expected behaviour is that what we observe in TF 2.1.0.

**Standalone code to reproduce the issue**
This code works in TF 2.1.0 but fails in TF 2.3.0 with the error ""RuntimeError: Unable to create link (name already exists)"" due to the same variable names.
```
import tensorflow as tf
inputs = {'feature_alpha' : tf.keras.layers.Input(name='feature_alpha', 
                                                 shape=(None,), 
                                                 sparse=True, 
                                                 dtype=tf.dtypes.string),
         'feature_beta' : tf.keras.layers.Input(name='feature_beta', 
                                                 shape=(None,), 
                                                 sparse=True, 
                                                 dtype=tf.dtypes.string)}
def gen_model(inputs):
    feature_alpha = tf.feature_column.categorical_column_with_hash_bucket('feature_alpha', 100, dtype=tf.dtypes.string)
    feature_beta = tf.feature_column.categorical_column_with_hash_bucket('feature_beta', 200, dtype=tf.dtypes.string)    
    
    alpha_emb = tf.feature_column.embedding_column(feature_alpha, dimension=10)
    beta_emb = tf.feature_column.embedding_column(feature_beta, dimension=20)
    out = tf.keras.layers.DenseFeatures([alpha_emb, beta_emb])(inputs)

    out = tf.keras.layers.Dense(64, activation='relu')(out)

    model = tf.keras.Model(inputs, out)

    return model        

model = gen_model(inputs)

print(model.trainable_variables)
model.save('mdl.h5')
```"
43010,TF_lite inference error,"

RuntimeError: tensorflow/lite/kernels/pad.cc:205 op_context.output->params.zero_point != op_context.constant_values->params.zero_point (0 != 255)Node number 2 (PADV2) failed to invoke.

tensorflow 1.5.0gpu + windows10 +python3.5"
43006,Issues while installing for raspberry pi through pip command,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):

**Describe the problem**
When did pip install TensorFlow lite for raspberry pi it started installation but meanwhile showing error and couldn't install the whole package 
**Please provide the exact sequence of commands/steps when you ran into the problem**

"
43004,Error when build the pip package for Tensorflow 1.8,"
    Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow):No
    OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 18.04
    TensorFlow installed from (source or binary):Source
    TensorFlow version (use command below):1.8
    Python version:2.7.17/3.6.9
    Bazel version (if compiling from source):0.10.0
    GCC/Compiler version (if compiling from source):Tried 6.5, 5.5
    CUDA/cuDNN version: 9.0/7.6.5
    GPU model and memory:GTX 1080

ERROR: /home/yuyangleng/tensorflow/tensorflow/contrib/framework/BUILD:101:1: output 'tensorflow/contrib/framework/_objs/python/ops/_variable_ops_gpu/tensorflow/contrib/framework/kernels/zero_initializer_op_gpu.cu.pic.o' was not created

ERROR: /home/yuyangleng/tensorflow/tensorflow/contrib/framework/BUILD:101:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 79.510s, Critical Path: 9.19s
FAILED: Build did NOT complete successfully


"
43003,ImportError: DLL load failed: The specified module could not be found.,"### System information

-   **just written import tensorflow**
-   **Windows 10**
-   **TensorFlow installed from pip**
-   **TensorFlow version: Latest**
-   **Python version**: 3.7.3
-   **GPU model and memory**: I am using cpu version

### Describe the problem
I have been stuck on this problem forever: https://pastebin.com/n7y5cmEQ

I have downloaded tensorflow on an i7-5600u, 16 gb ram device.

If I am missing any info please tell me

Thanks
"
43002,Tf.Variable handling when tf.function is set appears unfixably broken,"-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS 10.15.6
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: No
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: v2.3.0-rc2-23-gb36436b087 2.3.0
-   **Python version**: Python 3.7.4
-   **Bazel version (if compiling from source)**: N/A
-   **GCC/Compiler version (if compiling from source)**: N/A
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**: run below script

### Describe the problem
Consider the following case:

(1) Running keras-style functional models
(2) Training requires that each evaluation step runs against several modified versions of the input data in which different columns are replaced at different times
(3) Running in eager mode is two orders of magnitude slower than running in graph mode, so running in eager mode is flat out.

### Source code / logs

(Approximate minimal case extracted from my source; the actual notebook is several thousand lines long and would merely confuse matters.)

import numpy as np
import tensorflow as tf

class BrokenLayer(tf.keras.layers.Layer):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
    def call(self, inputs):
        transposed_inputs = tf.transpose(inputs)
        
        retval = []
        for delta in [[0], [0, 1]]:
            v = tf.Variable(
                np.zeros(len(delta, 1)),
                shape=tf.TensorShape((len(delta), None)))
            
            retval += [
                tf.tensor_scatter_nd_update(
                    tf.transpost(inputs),
                    delta,
                    v)]

        return retval

... No run this inside tf.keras.model.fit

It blows up because of the classic 'ValueError: tf.function-decorated function tried to create variables on non-first call.'

This behavior is by design, according to https://github.com/tensorflow/tensorflow/issues/27120. Unfortunately, despite spending the best part of a day trying to create a by-row constant tensor with a undefined second axis, I have found no way to replace the call to tf.Variable.

Lest it seem like this is a minor issue -- 'just create a Variable at class initialization and reuse that' is the first, obvous, suggestion -- *THAT DOES NOT WORK*. You need to be able to use tensor_scatter_nd_update to get there, and that's not possible, multiplying inputs by zero gives a tensor with the wrong shape. Using tensor array doesn't work, since it requires the each assignment be of known shape, and that brings us back to creating variables.

This design is completely broken, in what is not all that surprising a situation."
43001,NXP FRDM K66F example for micro speech does not even build ,"System information

    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
    TensorFlow installed from (source or binary): source
    TensorFlow version: bfc8733
    GCC/Compiler version (if compiling from source): arm-none-eabi-g++ 8.2.1

The example I'm running can be found here: 

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-nxp-frdm-k66f

Describe the problem
When running

$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""nxp_k66f"" generate_micro_speech_mbed_project

it results in the following error:

/make/Makefile TARGET=mbed TAGS=""nxp_k66f"" generate_micro_speech_mbed_project
tensorflow/lite/micro/tools/make/Makefile:313: warning: overriding recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
tensorflow/lite/micro/tools/make/Makefile:313: warning: ignoring old recipe for target 'tensorflow/lite/micro/tools/make/downloads/person_model_int8'
make: *** No rule to make target 'tensorflow/lite/micro/tools/make/gen/mbed_cortex-m4/prj/micro_speech/mbed/third_party/gemmlowp/fixedpoint/fixedpoint.h', needed by 'generate_micro_speech_mbed_project'. Stop.
"
43000,Tensorflow problem in Python 3.6,"Hi, I have installed these packages, pip list,
but python 3.6 tells me it takes tensorflow 2.2 or higher, but here it is installed!
I don't know what's wrong...
In the Python 3.6 shell when I enter ""import keras"" or ""import tensorflow"" it gives me an error.
Thanks for your help.
ric

- C:\>pip list
Package                Version
---------------------- ---------
absl-py                0.10.0
astunparse             1.6.3
cachetools             4.1.1
certifi                2020.6.20
chardet                3.0.4
cycler                 0.10.0
gast                   0.3.3
google-auth            1.21.1
google-auth-oauthlib   0.4.1
google-pasta           0.2.0
grpcio                 1.31.0
h5py                   2.10.0
idna                   2.10
importlib-metadata     1.7.0
joblib                 0.16.0
Keras                  2.4.3
Keras-Preprocessing    1.1.2
kiwisolver             1.2.0
Markdown               3.2.2
matplotlib             3.3.1
numpy                  1.18.5
oauthlib               3.1.0
opt-einsum             3.3.0
pandas                 0.25.3
Pillow                 7.2.0
pip                    20.2.2
protobuf               3.13.0
pyasn1                 0.4.8
pyasn1-modules         0.2.8
pyparsing              2.4.7
python-dateutil        2.8.1
pytz                   2020.1
PyYAML                 5.3.1
requests               2.24.0
requests-oauthlib      1.3.0
rsa                    4.6
scikit-learn           0.23.2
scipy                  1.4.1
setuptools             50.3.0
six                    1.15.0
sklearn                0.0
tensorboard            2.3.0
tensorboard-plugin-wit 1.7.0
tensorflow             2.3.0
tensorflow-estimator   2.3.0
termcolor              1.1.0
threadpoolctl          2.1.0
urllib3                1.25.10
Werkzeug               1.0.1
wheel                  0.35.1
wrapt                  1.12.1
zipp                   3.1.0
"
42999,tflite(micro) pointer-arithmetic overflow left unhandled in TfLiteStatus SimpleMemoryAllocator::EnsureHeadSize,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (SHA-1): 44dbace063796ea99c5b3d27a3a5810048d5096c


**Describe the current behavior**

The current implementation the check for an already-sufficient Head area in `SimpleMemoryAllocator::EnsureHeadSize` will fail due to a pointer-arithmetic overflow if the arena is allocated close to highest address and a large head size is requested resulting.

```
  if (aligned_result + size < head_) {
    // Size is below the current head size, just return.
    return kTfLiteOk;
  }
```

The overflowed pointer aligned_result + size will be below head_ causing a false kTfliteOk return rather than failing.
The Bug showed compiling tflite(u) test  for RISCV 32-bit using gcc-9 with default settings using the [RISCV GNU toolchain](https://github.com/riscv/riscv-gnu-toolchain) and executing on a semi-hosting simulator (whisper).    The test fails as in target environment a small arena is allocated on the stack very high in memory with a size large enough to overflow aligned_result + size.

**Describe the expected behavior**

The overflow case has to be handled.   The fix in our local fork is

 ```
 ptrdiff_t aligned_head_head_diff = head_-aligned_result;
 if (aligned_head_head_diff >=0 && size < static_cast<size_t>(aligned_head_head_diff)) {
    // Size is below the current head size, just return.
    return kTfLiteOk;
  }
```


**Standalone code to reproduce the issue**

Add to recording_simple_memory_allocator_test

```
TF_LITE_MICRO_TEST(DemonstrateOverflow) {
  constexpr size_t arena_size = 1024;
  uint8_t *arena = reinterpret_cast<uint8_t *>(-arena_size);
  tflite::RecordingSimpleMemoryAllocator allocator(micro_test::reporter, arena,
                                                   arena_size);

  TF_LITE_MICRO_EXPECT_EQ(
      kTfLiteError, allocator.EnsureHeadSize(/*size=*/2048, /*alignment=*/1));
}
```



"
42998,Go implementation does not provide SummaryImage/SummaryAudio,"**System information**
- Master branch of tensorflow/tensorflow latest. bb49eafc080caf205d5ba4478d9c93552ce46d57
- Windows 10

**Describe the current behavior**

I can not decode bitmap to tensor on Go.

```go
func decodeBitmapGraph() (*tf.Graph, tf.Output, tf.Output, error) {
	s := op.NewScope()
	input := op.Placeholder(s, tf.String)
	output := op.ExpandDims(
		s,
		op.DecodeBmp(s, input, op.DecodeBmpChannels(3)),
		op.Const(s.SubScope(""make_batch""), int32(0)))
	graph, err := s.Finalize()
	return graph, input, output, err
}
```

**Describe the expected behavior**

This code did work correctly in Go 1.5 or later. But latest tensorflow does not. Probably, you provide SummaryImage to manipulate images for TF_STRING. But Go does not provide APIs for that.


*EDIT*

Current implementation return

>error making input tensor: Malformed TF_STRING tensor; element 0 out of range"
42997,ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable In TPU stragegy,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I use add_weight function to add variable to model In TPU strategy but it shows that /usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    256       except Exception as e:  # pylint:disable=broad-except
    257         if hasattr(e, 'ag_error_metadata'):
--> 258           raise e.ag_error_metadata.to_exception(e)
    259         else:
    260           raise

ValueError: in user code:

    <ipython-input-6-cf880f69ac70>:38 call  *
        target_feat = self.build_(parents)
    <ipython-input-6-cf880f69ac70>:26 build_  *
        parent_weights = [tf.nn.relu(tf.cast(self.add_weight(initializer = tf.ones_initializer(), name='block{}_fusion{}'.format(1, j)), dtype=dtype)) for j in range(len(parents))]
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:614 add_weight  **
        caching_device=caching_device)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py:750 _add_variable_with_custom_getter
        **kwargs_for_getter)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py:145 make_variable
        shape=variable_shape if variable_shape else None)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:260 __call__
        return cls._variable_v1_call(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:221 _variable_v1_call
        shape=shape)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:67 getter
        return captured_getter(captured_previous, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2024 creator_with_resource_vars
        created = self._create_variable(next_creator, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:870 _create_variable
        **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_utils.py:291 create_mirrored_variable
        value_list = real_mirrored_creator(**kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py:861 _real_mirrored_creator
        v = next_creator(**kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:199 <lambda>
        previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py:2597 default_variable_creator
        shape=shape)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py:264 __call__
        return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1518 __init__
        distribute_strategy=distribute_strategy)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1601 _init_from_args
        raise ValueError(""Tensor-typed variable initializers must either be ""

    ValueError: Tensor-typed variable initializers must either be wrapped in an init_scope or callable (e.g., `tf.Variable(lambda : tf.truncated_normal([10, 40]))`) when building functions. Please file a feature request if this restriction inconveniences you.
**Describe the expected behavior**
Can successful add new variable to model.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
The standalone code can be see here https://colab.research.google.com/drive/1GB_kujPR2Mu8iayBuglmbB8ylfGh_YYk?usp=sharing, this code can sucessful run in GPU, CPU, but get error in TPU.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42996,Multiple branches executing in switch_case,
42995,Micro speech example not working on Sparkfun Edge,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Windows 10 Home 64-bit
- Console: Git Bash
- GNU Make 4.3
- TensorFlow installed from (source or binary): (Doesn't apply)
- Tensorflow version: (Doesn't apply)
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Sparkfun Edge

**Describe the problem**
I'm trying to deploy the [micro speech example](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech) to my Sparkfun edge, and although it compiles, links and flashes correctly, the behavior is wrong. It does not recognize any commands and just flashes the ""unknown"" word LED and sometimes the ""no"" word LED, regardless of what you say. I've done exactly as the wiki explains. I've tried deploying to Arduino and I've had no problem.


**Please provide the exact sequence of commands/steps when you ran into the problem**
I've carefully followed the steps on [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/micro_speech#deploy-to-sparkfun-edge) but I'll try everything one more time as I write this issue:

Clone trensorflow repo
`$ git clone https://github.com/tensorflow/tensorflow.git`
`$ cd tensorflow`

Compile the binary
`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=sparkfun_edge TAGS=""cmsis-nn"" micro_speech_bin`

Sign the binary
```
$ cp tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/keys_info0.py \
tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/keys_info.py
```

```
$ python3 tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/create_cust_image_blob.py \
--bin tensorflow/lite/micro/tools/make/gen/sparkfun_edge_cortex-m4/bin/micro_speech.bin \
--load-address 0xC000 \
--magic-num 0xCB \
-o main_nonsecure_ota \
--version 0x0
```

```
$ python3 tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/create_cust_wireupdate_blob.py \
--load-address 0x20000 \
--bin main_nonsecure_ota.bin \
-i 6 \
-o main_nonsecure_wire \
--options 0x1
```
Flash the binary
`export DEVICENAME=COM11`
`export BAUD_RATE=921600`

Now, with the board on boot mode(holding button `14` and press reset, and still holding `14`)
```
python3 tensorflow/lite/micro/tools/make/downloads/AmbiqSuite-Rel2.2.0/tools/apollo3_scripts/uart_wired_update.py \
-b ${BAUD_RATE} ${DEVICENAME} \
-r 1 \
-f main_nonsecure_wire.bin \
-i 6
```
It flashes correctly(output from the console):
```
...
Sending Data Packet of length  1468
Sending Reset Command.
Done.
```
When I try the program, saying some commands, the board just keeps flashing the blue LED(""unknown word"") and sometimes the red LED(""no"" word), no matter what I say. The room is quiet and I've tried different tones and volumes, no luck. At first I thought I had faulty hardware but I actually bought two boards and both show the same behavior. The Arduino version works fine for me on the Nano 33 BLE board, so I guess it has to be related to the Sparkfun Edge implementation, but I could not find the issue. Here is some output from the serial port when I'm issuing the commands:

```
Apollo3 Burst Mode is Available

Apollo3 operating in Burst Mode (96MHz)

Heard no (231) @256ms
Heard unknown (203) @5568ms
Heard unknown (202) @9472ms
Heard unknown (201) @14208ms
Heard unknown (201) @16128ms
Heard unknown (224) @17664ms
Heard no (208) @23424ms
Heard unknown (201) @26688ms
Heard unknown (207) @44800ms
```

"
42994,Jolting training metrics after running Model.evaluate() during Model.fit(),"I'm copying some text over from [a Stack Overflow post of mine from last week](https://stackoverflow.com/questions/63711560/tf-keras-model-training-accuracy-jumps-after-running-model-evaluate-during-tra?noredirect=1#comment112664862_63711560) which is yet to be answered. I hope this is the right place to post for advice - I tried developers@tensorflow.org but my email was blocked.

Minimum working example demonstrating the problem on MNIST: https://colab.research.google.com/drive/1JLj23q5LjZu6cvcU7SQmfmY97qYwV5Gj?usp=sharing

I'm training a TensorFlow Keras `Model` using `Model.fit()`. I'm also using callbacks to log my training accuracy metrics after every batch using TensorFlow's `on_train_batch_end()` syntax. In addition, I'm using another callback to run `Model.evaluate()` every 500 batches to compute validation set accuracy and update the `logs` dict passed around the callbacks during `Model.fit()`.

Looking at the logged metrics vs. batch number shows very perplexing results. After the `Model.evaluate()` run, the training accuracy experiences a significant 'jolt', initially triggering a rapid increase in the logged training accuracy and subsequently triggering a significant drop training accuracy followed by a slower recovery. See the image below for an example.

![jolt-acc](https://user-images.githubusercontent.com/26459412/92324130-9bbd5c00-f036-11ea-9165-cb3e994dc46d.png)

My guess is that it's something to do with the Model.evaluate()'s call to `reset_metrics()`, which loops through and calls the reset_states() method on each metric. I can't work out what `reset_states()` is doing and if this is relevant to the behaviour I'm observing. Are the metrics shown during `Model.fit()` actually some form of moving averages rather than the batch-wise metric? In that case, the `reset_states()` method would be resetting the moving average, producing the jolting behaviour that flattens out over time.

I'm worried that calling `Model.evaluate()` within `Model.fit()` is silently training on my validation data, although I would expect better validation performance if that were the case."
42993,The inconsistency of CTC API. Please optimize it.,"**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state**:

The API of CTC_loss and CTC_decoder is not consistent.

1. `tf.nn.ctc_loss(labels, logits, label_length, logit_length, logits_time_major=True, unique=None, blank_index=None, name=None)`
Default blank label is 0 rather num_classes - 1, unless overridden by blank_index.

2. `tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1)`
and
`tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True)`
This decoder treats blank label as num_classes -1 by default. And there is no option for setting blank label index.

Therefore, it could be a bit inconvenient to train a model with CTC loss and decode with the beam search decoder.
Hope someone could optimize this API.

**Will this change the current api? How?**
Original one:
`tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1)`
`tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True)`

New one: 
`tf.nn.ctc_beam_search_decoder(inputs, sequence_length, beam_width=100, top_paths=1, blank_index=None)`
`tf.nn.ctc_greedy_decoder(inputs, sequence_length, merge_repeated=True, blank_index=None)`

**Who will benefit with this feature?**
Researchers who are dealing with sequence recognition problems such as scene text recognition, OCR and speech recognition."
42992,Cuda Library not found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Installed using PIP in a Conda environment
- TensorFlow version: 2.3.0
- Python version:- Installed using virtualenv? pip? conda?: Installed using PIP in a Conda environment
- Bazel version (if compiling from source): None
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**
I don't have a GPU in my Laptop. I installed tensorflow using Pip inside a Conda Environment.
The command that I used to install it was -
```bash
pip install tensorflow
```
And I opened a Python Shell inside the CMD by typing
```bash
python
```
and I typed
```python
>>> import tensorflow
```
and I got this error -
![image](https://user-images.githubusercontent.com/52103891/92318272-3d3cb180-f027-11ea-8f4b-d4b26079db98.png)

But when I tried to print out the version, using
```python
print(tensorflow.__version__)
```
the output was -
![image](https://user-images.githubusercontent.com/52103891/92318285-8987f180-f027-11ea-89c7-8477f295ca87.png)

Please tell me whether I should Ignore this,
I know that the error is telling to Ignore if you don't have a GPU in you PC.
But whenever I import it it is throwing this Error

This is the Exact sequence of command that I used
![image](https://user-images.githubusercontent.com/52103891/92318320-1f238100-f028-11ea-8034-7a12d307ce3d.png)



I also tried the same in Pycharm, By setting the same above used python interpreter. But it is also showing the same error
```
E:\Anaconda3\envs\tensorflow\pythonw.exe E:/Robotics/python/ML(techwithtim)/tensorflowEnv/test.py
2020-09-05 23:13:00.132025: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-09-05 23:13:00.132610: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

Process finished with exit code 0
```

![image](https://user-images.githubusercontent.com/52103891/92318380-4038a180-f029-11ea-8706-cf931586c50a.png)

And I followed this Tutorial for the Installation and Setup -
https://www.youtube.com/watch?v=ujTCoH21GlA"
42991,tf.convert_to_tensor can't convert a float32 tensor to a float64 one,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

The following code threw exception:  ValueError: Tensor conversion requested dtype float64 for Tensor with dtype float32: <tf.Tensor: shape=(6,), dtype=float32, numpy=array([-0.6  , -0.04 , -0.001,  0.3  ,  0.4  ,  1.   ], dtype=float32)>

```
    labels = [-0.6, -0.04, -0.001, .3, .4, 1.0]
    labels = tf.convert_to_tensor(labels, dtype=tf.float32)
    labels = tf.convert_to_tensor(labels, dtype=tf.float64)
```

**Describe the expected behavior**
It should convert it.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42990,tape.gradient slow with RNN GRU model,"I have a trained  keras model, I'm trying to find the gradient of `target_preds` with respect of `sample_inputs` in the trained model. I tried to do it with CNN model and it computed in couple of seconds. When I insert a model that is RNN, the function it take it seems that the same code executes for long time (couple of minutes), and when I interrupt the code- it (is running) at line   `grads = tape.gradient(target_preds, sample_inputs)`).

I'm using Tensorflow 2.3.0

The relevant code is below:


```
    with tf.GradientTape() as tape:
        tape.watch(sample_inputs)
        preds = model(sample_inputs)
        if (target_range is None):
            target_preds = preds[:, :]
        else:
            target_preds = preds[:, target_range]
    
    if(jacobian):
        grads = tape.jacobian(target_preds, sample_inputs)
    else:
        grads = tape.gradient(target_preds, sample_inputs)
    return grads
```

One more important note:
when I used tensorflow 1 I tried the apply the same operation to the same model using: 
```
...
gradients = model.optimizer.get_gradients(model.output[:, c], model.input)
...
get_gradients = keras.backend.function(inputs=input_tensors, outputs=gradients)
...
gradients = get_gradients(_input)
```
This way it was computed in seconds,  but Tensorflow 2 stopped supporting this. "
42989,Inference time almost 5x SLOWER than Pytorch Same Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9.12 (stretch)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.6
- CUDA/cuDNN version: 10.1.243/7.6.5
- GPU model and memory: 1 x NVIDIA Tesla P4
- Pytorch version: 1.5.0

**Describe the current behavior**
I am implementing this [paper](https://arxiv.org/pdf/2004.02147.pdf) and found an existing implementation in Pytorch. So I ported the exact model to tensorflow with the keras api. Nonetheless, the inference time is almost 5x slower when running inference on GPU for the tensorflow version. I tested with the tesla P4 but also on a NVIDIA Xavier AGX and got same results (5 times slower). What am I missing? :s

I got the following results:
pytorch: 82.5 FPS 12.11 ms
tensorflow: 20 FPS 49.9 ms

**Describe the expected behavior**
If you check the code, it refers to the same architecture and number of parameters, one would expect similar running times.

**Standalone code to reproduce the issue**
You can check both pytorch and tensorflow implementation in this repo https://github.com/charlielito/test, and test the speed with:
```
python bisenetv2_tensorflow.py
```
```
python bisenetv2_torch.py
```


"
42988,Conv1D (and probablyall other Conv layers) with dilation_rate != 1  does not reliably handle changes in input size,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Bug is present with  tensorflow 2.1 and tensorflow 2.2 from Anaconda 

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04 and  Debian 4.9.189-3+deb9u2 (2019-11-11)

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
not mobile

- TensorFlow installed from (source or binary):

binary

- TensorFlow version (use command below):

unknown 2.1.0 and unknown 2.2.0

- Python version:

3.7

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):

- CUDA/cuDNN version:
10.1.243/7.6.5

- GPU model and memory:

FGeForce GTX 1050 Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

This extremely simple sequnec of calls of a Conv1D layer fails on the third call

import numpy as np
import tensorflow as tf

cc = tf.keras.layers.Conv1D(1, (3,), padding=""same"", dilation_rate=3)

res1= cc(np.zeros((1,100,1), dtype=np.float32))
res2= cc(np.zeros((1,101,1), dtype=np.float32))
res3= cc(np.zeros((1,100,1), dtype=np.float32))

**Describe the expected behavior**

The third call should run similarly as the others 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

[test_code.zip](https://github.com/tensorflow/tensorflow/files/5178711/test_code.zip)


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Error message:
res1 done
res2 done
2020-09-06 00:05:58.405706: W tensorflow/core/framework/op_kernel.cc:1753] OP_REQUIRES failed at spacetobatch_op.cc:219 : Invalid argument: padded_shape[0]=107 is not divisible by block_shape[0]=3
Traceback (most recent call last):
  File ""./test_conv_dila.py"", line 12, in <module>
    res3= cc(np.zeros((1,100,1), dtype=np.float32))
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 968, in __call__
    outputs = self.call(cast_inputs, *args, **kwargs)
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 207, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1106, in __call__
    return self.conv_op(inp, filter)
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 638, in __call__
    return self.call(inp, filter)
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 621, in _with_space_to_batch_call
    input=inp, block_shape=dilation_rate, paddings=paddings)
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 9491, in space_to_batch_nd
    _ops.raise_from_not_ok_status(e, name)
  File ""/data/anasynth/anaconda3/envs/tf2.2/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6653, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: padded_shape[0]=107 is not divisible by block_shape[0]=3 [Op:SpaceToBatchND]


"
42987,toco,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42986,LSTM Input Shape Error,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04 (through WSL 2 on Windows 10, Version 2004, Build 20206.1000)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **No**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **2.3.0**
- Python version: **3.7.9**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **10.1, 7.6.5.32**
- GPU model and memory: **Titan V, 12 GiB HBM2**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Running LSTM example code produces error

```
ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 28, 28, 1]
```

**Describe the expected behavior**
Sample code works.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Source:**
======
```
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.compat.v1.train import Saver
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

BATCH_SIZE = 64

(ds_train, ds_test), ds_info = tfds.load(
    ""mnist"",
    split=[""train"", ""test""],
    shuffle_files=True,
    as_supervised=True,
    with_info=True,
)


def normalize_img(image, label):
    """"""Normalizes images: `uint8` -> `float32`.""""""
    return tf.cast(image, tf.float32) / 255.0, label


ds_train = ds_train.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_train = ds_train.cache()
ds_train = ds_train.shuffle(ds_info.splits[""train""].num_examples)
ds_train = ds_train.batch(128)
ds_train = ds_train.prefetch(tf.data.experimental.AUTOTUNE)

ds_test = ds_test.map(normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)
ds_test = ds_test.batch(128)
ds_test = ds_test.cache()
ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)

shape = (28, 28, 1)
print(shape)

model = Sequential(
    [
        LSTM(128, input_shape=shape, return_sequences=True),
        Dropout(0.2),
        LSTM(128),
        Dropout(0.2),
        Dense(32, activation=""relu""),
        Dropout(0.2),
        Dense(10, activation=""softmax""),
    ]
)

model.compile(
    loss=""sparse_categorical_crossentropy"",
    optimizer=Adam(lr=1e-3, decay=1e-5),
    metrics=[""accuracy""],
)

model.fit(
    ds_train,
    epochs=6,
    validation_data=ds_test,
)

model.save(""mnist_model.h5"")
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**Results:**
=======

```
2020-09-05 13:12:19.582270: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-05 13:12:20.625604: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-05 13:12:20.819050: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:20.819191: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s
2020-09-05 13:12:20.819233: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-05 13:12:20.820267: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-05 13:12:20.821208: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-05 13:12:20.821395: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-05 13:12:20.822386: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-05 13:12:20.822973: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-05 13:12:20.825025: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-05 13:12:20.825824: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:20.827090: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:20.827504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-05 13:12:20.827843: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-05 13:12:20.834505: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2304005000 Hz
2020-09-05 13:12:20.836670: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556ef1d85530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-05 13:12:20.836696: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-05 13:12:21.114332: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:21.114811: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556ef1df11a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-05 13:12:21.114841: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN V, Compute Capability 7.0
2020-09-05 13:12:21.115526: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:21.115846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:41:00.0 name: TITAN V computeCapability: 7.0
coreClock: 1.455GHz coreCount: 80 deviceMemorySize: 12.00GiB deviceMemoryBandwidth: 607.97GiB/s
2020-09-05 13:12:21.115883: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-05 13:12:21.115904: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-05 13:12:21.115913: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-05 13:12:21.115943: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-05 13:12:21.115971: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-05 13:12:21.115983: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-05 13:12:21.115992: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-05 13:12:21.116516: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:21.117082: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:21.117231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-05 13:12:21.117269: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-05 13:12:21.793471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-05 13:12:21.793510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-05 13:12:21.793518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-05 13:12:21.794276: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:21.794418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.
2020-09-05 13:12:21.794805: E tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:968] could not open file to read NUMA node: /sys/bus/pci/devices/0000:41:00.0/numa_node
Your kernel may have been built without NUMA support.
2020-09-05 13:12:21.794965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10218 MB memory) -> physical GPU (device: 0, name: TITAN V, pci bus id: 0000:41:00.0, compute capability: 7.0)
(28, 28, 1)
Traceback (most recent call last):
  File ""test.py"", line 46, in <module>
    Dense(10, activation=""softmax""),
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 142, in __init__
    self.add(layer)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py"", line 457, in _method_wrapper
    result = method(self, *args, **kwargs)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 206, in add
    layer(x)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 663, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 926, in __call__
    input_list)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1092, in _functional_construction_call
    input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/engine/input_spec.py"", line 180, in assert_input_compatibility
    str(x.shape.as_list()))
ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, 28, 28, 1]
```"
42985,Cannot Download MNIST on WSL2 - Temporary Failure in Name Resolution,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **No**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 18.04 LTS (through WSL 2; Windows 10 Version 2004 Build 20206.1000)**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below): **2.3.0**
- Python version: **3.7.9**
- Bazel version (if compiling from source): **N/A**
- GCC/Compiler version (if compiling from source): **N/A**
- CUDA/cuDNN version: **10.1, 7.6.5.32**
- GPU model and memory: **Titan V, 12 GB of HBM 2**

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Cannot download MNIST data set on Ubuntu 18.04 from WSL 2. Creating `/etc/wsl.conf` with

```
[network]
generateResolvConf = false
```

 and adding `nameserver 8.8.8.8` to `/etc/resolv.conf` does not resolve this issue.

**Describe the expected behavior**
MNIST example data downloads

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Source:**
=====
```
""""""test.py""""""
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

**Result:**
======
```
> python test.py
2020-09-05 12:02:27.043432: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
Traceback (most recent call last):
  File ""/usr/lib/python3.7/urllib/request.py"", line 1350, in do_open
    encode_chunked=req.has_header('Transfer-encoding'))
  File ""/usr/lib/python3.7/http/client.py"", line 1277, in request
    self._send_request(method, url, body, headers, encode_chunked)
  File ""/usr/lib/python3.7/http/client.py"", line 1323, in _send_request
    self.endheaders(body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.7/http/client.py"", line 1272, in endheaders
    self._send_output(message_body, encode_chunked=encode_chunked)
  File ""/usr/lib/python3.7/http/client.py"", line 1032, in _send_output
    self.send(msg)
  File ""/usr/lib/python3.7/http/client.py"", line 972, in send
    self.connect()
  File ""/usr/lib/python3.7/http/client.py"", line 1439, in connect
    super().connect()
  File ""/usr/lib/python3.7/http/client.py"", line 944, in connect
    (self.host,self.port), self.timeout, self.source_address)
  File ""/usr/lib/python3.7/socket.py"", line 707, in create_connection
    for res in getaddrinfo(host, port, 0, SOCK_STREAM):
  File ""/usr/lib/python3.7/socket.py"", line 752, in getaddrinfo
    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):
socket.gaierror: [Errno -3] Temporary failure in name resolution

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 278, in get_file
    urlretrieve(origin, fpath, dl_progress)
  File ""/usr/lib/python3.7/urllib/request.py"", line 247, in urlretrieve
    with contextlib.closing(urlopen(url, data)) as fp:
  File ""/usr/lib/python3.7/urllib/request.py"", line 222, in urlopen
    return opener.open(url, data, timeout)
  File ""/usr/lib/python3.7/urllib/request.py"", line 525, in open
    response = self._open(req, data)
  File ""/usr/lib/python3.7/urllib/request.py"", line 543, in _open
    '_open', req)
  File ""/usr/lib/python3.7/urllib/request.py"", line 503, in _call_chain
    result = func(*args)
  File ""/usr/lib/python3.7/urllib/request.py"", line 1393, in https_open
    context=self._context, check_hostname=self._check_hostname)
  File ""/usr/lib/python3.7/urllib/request.py"", line 1352, in do_open
    raise URLError(err)
urllib.error.URLError: <urlopen error [Errno -3] Temporary failure in name resolution>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 12, in <module>
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/datasets/mnist.py"", line 62, in load_data
    '731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1')
  File ""/home/hendra11/Code/Glucose_Challenge/.venv_py3_ubuntu/lib/python3.7/site-packages/tensorflow/python/keras/utils/data_utils.py"", line 282, in get_file
    raise Exception(error_msg.format(origin, e.errno, e.reason))
Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz: None -- [Errno -3] Temporary failure in name resolution
```"
42984,tf.GradientTape().gradient() returns None,"I am using a training loop with GradientTape, however, it returns a list of Nones : [None, None, None, None, None, None, None, None]

Colab Code : https://colab.research.google.com/gist/26345211/3fa3d06165d1ffa0ddf66b3847933e11/untitled1.ipynb"
42982,AttributeError: module 'tensorflow_addons.layers' has no attribute 'SpectralNormalization',
42981,"How to fix ""AttributeError: module 'tensorflow.keras.backend' has no attribute 'get_session'""","Hello i'm trying to make a deepfake without coding (because i dont know how lmao) and i have a issue , here my code 

fd = MTCNNFaceDetector(sess=K.get_session(), model_path=""./mtcnn_weights/"")
save_interval = 5 # perform face detection every {save_interval} frames
save_path = ""./faceA/""
preprocess_video(fn_source_video, fd, save_interval, save_path)
save_path = ""./faceB/""
preprocess_video(fn_target_video, fd, save_interval, save_path)

thank you and sorry if i seem dumb"
42980,Loading a TF1 Protocol buffer does not work in TF versions 2.2.0 and above,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux 4.19.112+** and **Windows 10**
- TensorFlow version (use command below): **v2.2.0-rc4-8-g2b96f3662b 2.2.0** and **v2.3.0-0-gb36436b087 2.3.0**
- Python version: **3.6.9**
- CUDA/cuDNN version: **CUDA disabled**
- GPU model and memory: **GPU disabled**

**Describe the current behavior**

Model trained with Tensorflow 1 and saved with the [tf.saved_model.simple_save](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model/simple_save) can't be loaded with Tensorflow 2.2.0 and above using [tf.saved_model.load](https://www.tensorflow.org/api_docs/python/tf/saved_model/load). The same code works perfectly well with Tensorflow 2.1.1. 

The error I get in TF 2.2.0 and above:

```
<ipython-input-3-fa86b40288e8> in <module>()
----> 1 model_loaded = tf.saved_model.load('tensorflow_model/')
      2 model_loaded = model_loaded.signatures['serving_default']

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in load(export_dir, tags, options)
    601     ValueError: If `tags` don't match a MetaGraph in the SavedModel.
    602   """"""
--> 603   return load_internal(export_dir, tags, options)
    604 
    605 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load.py in load_internal(export_dir, tags, options, loader_cls)
    647   else:
    648     with ops.init_scope():
--> 649       root = load_v1_in_v2.load(export_dir, tags)
    650       root.graph_debug_info = debug_info
    651   return root

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(export_dir, tags)
    261   """"""Load a v1-style SavedModel as an object.""""""
    262   loader = _EagerSavedModelLoader(export_dir)
--> 263   return loader.load(tags=tags)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load(self, tags)
    207     wrapped = wrap_function.wrap_function(
    208         functools.partial(self.load_graph, load_graph_returns, meta_graph_def),
--> 209         signature=[])
    210     saver, = load_graph_returns
    211     restore_from_saver = self._extract_saver_restore(wrapped, saver)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrap_function(fn, signature, name)
    626           signature=signature,
    627           add_control_dependencies=False,
--> 628           collections={}),
    629       variable_holder=holder,
    630       signature=signature)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in __call__(self, *args, **kwargs)
     85 
     86   def __call__(self, *args, **kwargs):
---> 87     return self.call_with_variable_creator_scope(self._fn)(*args, **kwargs)
     88 
     89   def call_with_variable_creator_scope(self, fn):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/wrap_function.py in wrapped(*args, **kwargs)
     91     def wrapped(*args, **kwargs):
     92       with variable_scope.variable_creator_scope(self.variable_creator_scope):
---> 93         return fn(*args, **kwargs)
     94 
     95     return wrapped

/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/load_v1_in_v2.py in load_graph(self, returns, meta_graph_def)
     88     # pylint: disable=protected-access
     89     saver, _ = tf_saver._import_meta_graph_with_return_elements(
---> 90         meta_graph_def)
     91     # pylint: enable=protected-access
     92     returns[0] = saver

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py in _import_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)
   1484           import_scope=import_scope,
   1485           return_elements=return_elements,
-> 1486           **kwargs))
   1487 
   1488   saver = _create_saver_from_imported_meta_graph(meta_graph_def, import_scope,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/meta_graph.py in import_scoped_meta_graph_with_return_elements(meta_graph_or_file, clear_devices, graph, import_scope, input_map, unbound_inputs_col_name, restore_collections_predicate, return_elements)
    797         input_map=input_map,
    798         producer_op_list=producer_op_list,
--> 799         return_elements=return_elements)
    800 
    801     # TensorFlow versions before 1.9 (not inclusive) exported SavedModels

/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)
    505                 'in a future version' if date is None else ('after %s' % date),
    506                 instructions)
--> 507       return func(*args, **kwargs)
    508 
    509     doc = _add_deprecated_arg_notice_to_docstring(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in import_graph_def(***failed resolving arguments***)
    403       return_elements=return_elements,
    404       name=name,
--> 405       producer_op_list=producer_op_list)
    406 
    407 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/importer.py in _import_graph_def_internal(graph_def, input_map, return_elements, validate_colocation_constraints, name, producer_op_list)
    499       except errors.InvalidArgumentError as e:
    500         # Convert to ValueError for backwards compatibility.
--> 501         raise ValueError(str(e))
    502 
    503     # Create _DefinedFunctions for any imported functions.

ValueError: Node 'loss/gradients/model/batch_normalization_3/FusedBatchNormV3_1_grad/FusedBatchNormGradV3' has an _output_shapes attribute inconsistent with the GraphDef for output #3: Dimension 0 in both shapes must be equal, but are 0 and 64. Shapes are [0] and [64].
```

**Describe the expected behavior**

Models should be loaded regardless of TF2 version. 

**Standalone code to reproduce the issue**

[Colab notebook to reproduce issue](https://colab.research.google.com/drive/18IWxx-eppX2Sjoo2h1hGe0TzCHNiykF1?usp=sharing)

**Other info / logs** 



Although this should be enough information to reproduce the issue there are even more details in the [Stackoverflow question](https://stackoverflow.com/questions/63656778/how-to-load-a-trained-tf1-protobuf-model-into-tf2) I created.


**Please provide a workaround in the latest Tensorflow version if possible as I have to use this model in an environment where I am constrained to the latest Tensorflow version.**


"
42979,bazel-bin does not contain full API,"I a trying to use C++ API of tensorflow 2.x after run ""bazel build //tensorflow:libtensorflow_cc.so"" command build was succeeded but some of the API are missing in bazel-bin\tensorflow like ""tensorflow\tensorflow\core\public"" is I am doing anything wrong?
**System information**
- OS Platform and Distribution (Window 10):
- TensorFlow installed from (source): yes
- TensorFlow version: 2.3.0 (latest)
- Python version: 3.7
- Bazel version (if compiling from source): 3.4.1
- CUDA/cuDNN version: No GPU support 




**exact sequence of commands**
I follow [this](https://www.tensorflow.org/install/source_windows) but change build option as bazel build //tensorflow:libtensorflow_cc.so

**logs**
this is the following output I am getting after running ""bazel build //tensorflow:libtensorflow_cc.so"" command
```
C:\Users\username\Documents\TF\tensorflow>bazel build //tensorflow:libtensorflow_cc.so
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\users\username\documents\tf\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/python/python.exe
INFO: Reading rc options for 'build' from c:\users\username\documents\tf\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\users\...\documents\tf\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/python/python.exe --action_env PYTHON_LIB_PATH=C:/python/lib/site-packages --python_path=C:/python/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\users\username\documents\tf\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\users\username\documents\tf\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\users\username\documents\tf\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:windows in file c:\users\username\documents\tf\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\users\username\documents\tf\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Analyzed target //tensorflow:libtensorflow_cc.so (202 packages loaded, 19125 targets configured).
INFO: Found 1 target...
Target //tensorflow:libtensorflow_cc.so up-to-date:
  bazel-bin/tensorflow/libtensorflow_cc.so
INFO: Elapsed time: 37794.951s, Critical Path: 1324.20s
INFO: 8556 processes: 8556 local.
INFO: Build completed successfully, 10656 total actions

```"
42978,Multiple version of tensorflow that exist in system site packages and venv site packages,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version: nightly (2.4.0.dev20200904)
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: virtualenv+pip
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**
Error when importing tensorflow nightly installed in venv while having another version of tensorflow installed system-wide.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Following steps in https://www.tensorflow.org/install/pip.
```
pip3 install --upgrade tensorflow-cpu
python3 -m venv --system-site-packages venv
source venv/bin/activate
(venv) pip install --upgrade pip
(venv) pip install --upgrade tf-nightly-cpu
(venv) python -c ""import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))""
```

**Any other info / logs**
Traceback:
```
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""somewhere/venv/lib/python3.8/site-packages/tensorflow/__init__.py"", line 433, in <module>
    _ll.load_library(_main_dir)
  File ""somewhere/venv/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library
    py_tf.TF_LoadLibrary(lib)
tensorflow.python.framework.errors_impl.NotFoundError: /home/user/.local/lib/python3.8/site-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow8OpKernel11TraceStringEPNS_15OpKernelContextEb
```

I have tensorflow 2.3.0 installed in system site packages and that causes https://github.com/tensorflow/tensorflow/blob/47aab0b49ef7432261f60430f5e29bb331cd5233/tensorflow/api_template.__init__.py#L138-L142 to load tensorflow kernel from system site packages.

Is there a way to have multiple version of tensorflow that exist in system site packages and venv site packages?"
42977,ERROR: No matching distribution found for tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): py -m pip install tensorflow
- Python version: Python 3.8.5
- Installed using virtualenv? pip? conda?: pip

Error Message:
ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)
ERROR: No matching distribution found for tensorflow

Im trying to install Tensorflow through pip but it doesn't seem to work. I have upgraded both pip and python to the newest versions

\- Regards
"
42976,"UnimplementedError: Fusion is not implemented: [BiasAdd,Add]","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):archlinux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):some source, some binary
- TensorFlow version (use command below):unknown 2.3.0
- Python version:3.8
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:11/8
- GPU model and memory: GTX1070

**Describe the current behavior**
Running the following code with TF 2.3:
```python
import tensorflow.compat.v1 as tf
tf.disable_eager_execution()

def model():
    a = tf.get_variable('x1', [1, 256, 200, 304])
    b = tf.get_variable('x2', [1, 256, 100, 152])

    def upsample2x(x):
        resize = tf.image.resize_images
        shp2d = tf.shape(x)[2:]
        x = tf.transpose(x, [0, 2, 3, 1])
        x = resize(x, shp2d * 2, 'nearest')
        x = tf.transpose(x, [0, 3, 1, 2])
        return x

    def conv(x, kernel):
        return tf.layers.Conv2D(256, kernel, padding='SAME',
                data_format='channels_first')(x)

    a, b = [conv(c, 1) for c in [a, b]]
    a = a + upsample2x(b)
    out = [conv(x, 3) for x in [a, b]]

    loss = tf.add_n([tf.reduce_mean(x) for x in out])
    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss,
            var_list=tf.trainable_variables())
    return train_op

with tf.device('/gpu:0'):
    train_op = model()
config = tf.ConfigProto()
sess = tf.Session(config=config)
with sess.as_default():
    sess.run(tf.global_variables_initializer())
    train_op.run()
```

throws the following error in __certain__ environments:
```
2020-09-04 18:42:52.932125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-04 18:42:53.726786: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-04 18:42:53.731417: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3600000000 Hz
2020-09-04 18:42:53.731651: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fabf1f410 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-04 18:42:53.731662: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-04 18:42:53.733086: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-04 18:42:53.796642: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-04 18:42:53.797070: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562fabfc86d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-04 18:42:53.797085: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1070, Compute Capability 6.1
2020-09-04 18:42:53.797204: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-04 18:42:53.797483: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1070 computeCapability: 6.1
coreClock: 1.683GHz coreCount: 15 deviceMemorySize: 7.93GiB deviceMemoryBandwidth: 238.66GiB/s
2020-09-04 18:42:53.797507: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-04 18:42:53.799077: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.11
2020-09-04 18:42:53.799724: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-04 18:42:53.799867: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-04 18:42:53.801546: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-04 18:42:53.801930: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.11
2020-09-04 18:42:53.802055: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.8
2020-09-04 18:42:53.802122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-04 18:42:53.802476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-04 18:42:53.802778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-04 18:42:53.802801: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.11.0
2020-09-04 18:42:54.070300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-04 18:42:54.070324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-09-04 18:42:54.070330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-09-04 18:42:54.070460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-04 18:42:54.070791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-04 18:42:54.071130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5626 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
2020-09-04 18:42:54.071508: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
2020-09-04 18:42:54.294130: W tensorflow/core/framework/op_kernel.cc:1744] OP_REQUIRES failed at conv_ops_fused_impl.h:700 : Unimplemented: Fusion is not implemented: [BiasAdd,Add]
Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1349, in _run_fn
    return self._call_tf_sessionrun(options, feed_dict, fetch_list,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1441, in _call_tf_sessionrun
    return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,
tensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,Add]
         [[{{node add}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""bug.py"", line 37, in <module>
    train_op.run()
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 2551, in run
    _run_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 5547, in _run_using_default_session
    session.run(operation, feed_dict)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 957, in run
    result = self._run(None, fetches, feed_dict, options_ptr,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1180, in _run
    results = self._do_run(handle, final_targets, final_fetches,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1358, in _do_run
    return self._do_call(_run_fn, feeds, fetches, targets, options,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/client/session.py"", line 1384, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnimplementedError: Fusion is not implemented: [BiasAdd,Add]
         [[node add (defined at bug.py:23) ]]

Errors may have originated from an input operation.
Input Source operations connected to node add:
 transpose_1 (defined at bug.py:15)
 conv2d/BiasAdd (defined at bug.py:19)

Original stack trace for 'add':
  File ""bug.py"", line 32, in <module>
    train_op = model()
  File ""bug.py"", line 23, in model
    a = a + upsample2x(b)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py"", line 1125, in binary_op_wrapper
    return func(x, y, name=name)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py"", line 1447, in _add_dispatch
    return gen_math_ops.add_v2(x, y, name=name)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 495, in add_v2
    _, _, _op, _outputs = _op_def_library._apply_op_helper(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"", line 742, in _apply_op_helper
    op = g._create_op_internal(op_type_name, inputs, dtypes=None,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 3477, in _create_op_internal
    ret = Operation(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 1949, in __init__
    self._traceback = tf_stack.extract_stack()
```

Environment that I have tried:
1. GTX1070, cuda11, cudnn8: failed
2. GTX1080Ti, cuda10.2, cudnn8: failed
3. GTX1080Ti, cuda10.2, cudnn7.6: failed
    * under the same environment, using TF1.15 instead of TF2.3: succeed
4. P100, cuda10.1, cudnn7.6: succeed
5. V100, cuda10.1, cudnn7.6: succeed
6. Colab with Tesla T4, cuda10.x, cudnn7: succeed

Maybe this is unique to GPUs that have compute capability 6.1"
42975, [ROCm] Cannot find rocm library hip_hcc,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.0
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): 10.2.0
- CUDA/cuDNN version: N/A
- ROCm version: 3.7.0
- GPU model and memory:



**Describe the problem**

When building I am getting the following error:
```
INFO: Repository local_config_rocm instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule rocm_configure defined at:
  /home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl:866:33: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_rocm':
   Traceback (most recent call last):
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 845
		_create_local_rocm_repository(<1 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 652, in _create_local_rocm_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 453, in _find_libs
		_select_rocm_lib_paths(<3 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 422, in _select_rocm_lib_paths
		auto_configure_fail(<1 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 162, in auto_configure_fail
		fail(<1 more arguments>)

ROCm Configuration Error: Cannot find rocm library hip_hcc
ERROR: Skipping '//tensorflow:libtensorflow.so': no such package '@local_config_rocm//rocm': Traceback (most recent call last):
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 845
		_create_local_rocm_repository(<1 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 652, in _create_local_rocm_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 453, in _find_libs
		_select_rocm_lib_paths(<3 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 422, in _select_rocm_lib_paths
		auto_configure_fail(<1 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 162, in auto_configure_fail
		fail(<1 more arguments>)

ROCm Configuration Error: Cannot find rocm library hip_hcc
ERROR: no such package '@local_config_rocm//rocm': Traceback (most recent call last):
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 845
		_create_local_rocm_repository(<1 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 652, in _create_local_rocm_repository
		_find_libs(repository_ctx, <2 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 453, in _find_libs
		_select_rocm_lib_paths(<3 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 422, in _select_rocm_lib_paths
		auto_configure_fail(<1 more arguments>)
	File ""/home/acxz/vcs/git/github/rocm-arch/tensorflow-rocm/src/tensorflow-2.3.0-rocm/third_party/gpus/rocm_configure.bzl"", line 162, in auto_configure_fail
		fail(<1 more arguments>)

ROCm Configuration Error: Cannot find rocm library hip_hcc
INFO: Elapsed time: 18.566s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow ... (2 packages)
    Fetching @com_google_protobuf; fetching
    Fetching ...otobuf; Extracting /home/acxz/.cache/bazel/_bazel_acxz/032a3a4c537da51b5f6f596867eba382/external/com_google_prot\
obuf/v3.9.2.zip
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. git clone
2. `export TF_NEED_ROCM=1`
3. `./configure`
4. 
```
  bazel \
        build --config=mkl --config=avx2_linux -c opt \
          //tensorflow:libtensorflow.so \
          //tensorflow:libtensorflow_cc.so \
          //tensorflow:install_headers \
          //tensorflow/tools/pip_package:build_pip_package
      bazel-bin/tensorflow/tools/pip_package/build_pip_package --gpu ""${srcdir}""/tmpoptrocm
```

To be exactly precise I am using the following build script (PKGBUILD):
https://aur.archlinux.org/cgit/aur.git/tree/PKGBUILD?h=tensorflow-rocm

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Downstream issue: https://github.com/rocm-arch/tensorflow-rocm/issues/5
"
42973,tf.nn.max_pool2d: No support for tf.Tensor input for ksize,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10.0.19041
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.6
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: 11.0/7.6.5
- GPU model and memory: RTX 2070 8GB

**Describe the current behavior**
The ksize arguement of tf.nn.max_pool2d supports only native Python int but not tf.Tensor. This is not a problem when using eager mode, but very disadvantageous when calculating the ksize as a Tensor in TF Graph mode.

`TypeError: Expected int for argument 'ksize' not <tf.Tensor 'random_uniform:0' shape=() dtype=int32>.`

**Describe the expected behavior**
Tensor support for tf.nn.max_pool2d function parameters, espacially ksize.

**Standalone code to reproduce the issue**
```
@tf.function
def issue():
    inp = tf.convert_to_tensor(np.arange(75).reshape((1, 5, 5, 3)))
    ksize = tf.random.uniform([], 1, 3, dtype=tf.int32)
    return tf.nn.max_pool2d(inp, [ksize], 1, ""SAME"")

if __name__ == ""__main__"":
    issue()
```

**Other info / logs**
[traceback.txt](https://github.com/tensorflow/tensorflow/files/5177269/traceback.txt)

Thank you in advance!"
42972,Did not find a parser for CONV_2D with latest repo download,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Running on Raspberry Raspian ( latest version )
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 2.3.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Raspian

**Describe the problem**
With latest sources, I get Did not find a parser for CONV_2D when I try to load a converted model that contains
  model = models.Sequential([
   layers.Conv2D(32, (3, 3), activation='relu', input_shape=(height, width,1)),
   layers.Conv2D(64, (3, 3), activation='relu',padding='same'),
   layers.MaxPooling2D(pool_size=(2, 2),padding='same'),
   layers.Conv2D(128, (3, 3), activation='relu',padding='same'),
   layers.MaxPooling2D(pool_size=(2, 2),padding='same'),
   layers.Conv2D(64, 3, padding='same', activation='relu'),
   layers.MaxPooling2D(pool_size=(2, 2),padding='same'),
   layers.Flatten(),
   layers.Dense(128, activation='linear'),
   layers.Dense(num_classes, activation='softmax')
 ])

I got rid of all layer operations that aren't supported by tf micro.  I got a RESHAPE version error, so I updated from the tensorflow source repo and rebuilt the tf micro library for rpi.  Now I get ""Did not find a parser for CONV_2D"" when I run

bool PositionML::setup()
  {
    // Not sure why they did it this way, but all examples do it this way
    static tflite::MicroErrorReporter micro_error_reporter;
    mErrorReporter = &micro_error_reporter;

    mModel = tflite::GetModel(model_kara_cnn_tflite);
    if (mModel->version() != TFLITE_SCHEMA_VERSION) {
      TF_LITE_REPORT_ERROR(mErrorReporter, ""Model provided is schema version %d not equal to supported version %d."", mModel->version(), TFLITE_SCHEMA_VERSION);
      return false;
    }

    constexpr int tensor_arena_size = 16 * 1024;
    uint8_t tensor_arena[tensor_arena_size];
    tflite::MicroMutableOpResolver<4> resolver;
    resolver.AddConv2D();
    resolver.AddFullyConnected();
    resolver.AddMaxPool2D();
    resolver.AddSoftmax();

    tflite::MicroInterpreter interpreter(mModel, resolver, tensor_arena, tensor_arena_size, mErrorReporter);
    TfLiteStatus allocate_status = interpreter.AllocateTensors();
    if (allocate_status != kTfLiteOk) {
      TF_LITE_REPORT_ERROR(mErrorReporter, ""AllocateTensors() failed"");
      return false;
    }

    std::cout << ""Setup complete"" << std::endl;
    return true;
  }


"
42968,Large number is multiplied with the embedding output,"## URL(s) with the issue:

The url with the issue is https://www.tensorflow.org/tutorials/text/transformer

## Description of issue (what needs changing):

### Clear description

While, I was checking the transformers code from this [tensorflow documentation](https://www.tensorflow.org/tutorials/text/transformer), in the **Encoder** subsection under **Encoder and decoder** section, I found that before adding positional encoding, square root of `d_model` is multiplied with the embedding output. I doubt what is the specific reason behind multiplying the embedding output with a large number. Attaching the screenshot of the specific code snippet from the link.

<img width=""551"" alt=""Screenshot 2020-09-05 at 1 01 55 AM"" src=""https://user-images.githubusercontent.com/44110256/92278708-69ffa480-ef13-11ea-8e02-7f20f939bb6a.png"">
"
42967,Extremely slow eigendecomposition compared to numpy/scipy.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I am using eigendecomposition in Tensorflow and find that it is extremely slow. This is on a mac, so these are CPU computations. But I've also done this on a linux box with a GPU and found the same thing. Here's the code to show Tensorflow's speed vs numpy and scipy:

```
import numpy as np
import scipy as sp
import tensorflow as tf
from time import time

A = np.random.randn(400, 400)
A_tf = tf.constant(A)

cur = time()
d, v = sp.linalg.eig(A)
print(f'sp: {time() - cur:4.2f} s')

cur = time()
d, v = np.linalg.eig(A)
print(f'np: {time() - cur:4.2f} s')

cur = time()
d, v = tf.linalg.eig(A_tf)
print(f'tf: {time() - cur:4.2f} s')
```
This gives the following output:
```
sp: 0.09 s
np: 0.08 s
tf: 5.04 s
```
Any ideas of what's up here?

"
42966,Misleading error message on keras.Model initialization,"### System information
---
 Ubuntu linux 20.04
From pip, tf version 2.3.0

### Describe the current behavior
---
Consider this simple code from [this tutorial](https://www.tensorflow.org/guide/keras/sequential_model#feature_extraction_with_a_sequential_model)

```python
import tensorflow as tf
from tensorflow import keras

assert tf.__version__ == '2.3.0'

initial_model = keras.Sequential(
    [
        keras.Input(shape=(250, 250, 3)),
        keras.layers.Conv2D(32, 5, strides=2, activation=""relu""),
        keras.layers.Conv2D(32, 3, activation=""relu""),
        keras.layers.Conv2D(32, 3, activation=""relu""),
    ]
)


feature_extractor = keras.Model(
    inputs=initial_model.inputs,
    #Note the error!
    blah=[layer.output for layer in initial_model.layers], 
)

```
And the error is:
> TypeError: ('Keyword argument not understood:', 'inputs')  

### Describe the expected behavior
---
I expect the error to contain a meaningful message, and at least to mention the incorrectly specified keyword `blah`. Just mentioning the one argument that is correctly written does not help. Maybe the error could be like 
> TypeError: ('Keyword argument not understood:', 'blah')  

or   

> TypeError: Invalid keyword arguments


"
42964,MicroInterpreter::tensors_size() always returns zero,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Fedora 32
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 238981a91a9b780ab4449829469a71c5d668a273
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): x86

**Describe the problem**

The tensors_size() method of the MicroInterpreter class always returns zero.  

**Please provide the exact sequence of commands/steps when you ran into the problem**

To demonstrate, insert the following line:

`TF_LITE_MICRO_EXPECT_GT(static_cast<size_t>(0), interpreter.tensors_size());`

here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/micro_interpreter_test.cc#L87

And run:

`$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=""x86"" test_micro_interpreter_test`

"
42963,Muliple conventions ,"## URL(s) with the issue:
https://www.tensorflow.org/tutorials/quickstart/beginner
https://www.tensorflow.org/tutorials/keras/classification?hl=en, https://keras.io/api/models/sequential/
https://keras.io/guides/sequential_model/

## Description of issue (what needs changing):
tf.keras.layers.Dense
keras.layers.Dense
layers.Dense

Muliple conventions are followed. Which is the preferred way?"
42961,Where is the source code for keras v2.4.3?,"Not found in both tensorflow and keras github repo?

It is released before tensorflow 2.3.0. Why it is not integrated into this?"
42960,Tensorflow build with AVX2 without warnings of ignoring it. But after the installation it is not using it anymore.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-Windows 10 Pro
-GTX 1060 3gb
-compute capabilities 6.1
-Python 3.6
-Tensorflow 2.3
-bazel 3.1.0
-TensorRT 6.0
-using AVX2



**Describe the problem**
I build tensorflow 2.3 with all the infos at the top. In the python ./configure.py part in the installation i choosed ""Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2"". While building there was no Error or Warning that it would be ignored. 
After the installation of the package i ran a deepspeech command to test it and it printed out this : ""I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2"". So my CPU supports AVX2 but my build wasnt compile to use that.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

C:\TenserFlow\tensorflow>python ./configure.py
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is C:\Python38-32\python.exe]:


Found possible Python library paths:
  C:\Python38-32\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Python38-32\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]: /arch:AVX2


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.


C:\TenserFlow\tensorflow>bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

C:\TenserFlow\tensorflow>bazel-bin\tensorflow\tools\pip_package\build_pip_package C:/tmp/tensorflow_pkg

C:/tmp/tensorflow_pkg>pip3 install C:/tmp/tensorflow_pkg/tensorflow-version-cp36-cp36m-win_amd64.whl

Then i ran DeepSpeech that used my installed Tensorflow Version.
deepspeech --model deepspeech-0.8.1-models.pbmm --scorer deepspeech-0.8.1-models.scorer --audio audio/2830-3980-0043.wav

The output works fine but it also gives the Info that AVX2 isnt used."
42958,Tensorflow and RTX 3000 (TF 1.X or 2.X required),"1. Will tensorflow support new cards?
2. Which version of TF is required?
3. Will I run the program on version tf.1.x with RTX3000?"
42957,GPU devices not detected in tf-nightly in latest releases (Google Colaboratory),"Hello, 

I installed `tf-nightly` in a Google Colaboratory notebook and noticed that the model training time is significantly longer than when I was training it at the beginning of August. Then I ran 

```
print(tf.test.gpu_device_name())
print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))
```

and no GPU devices were detected, even though the accelerator was set to 'GPU'. 

But after uninstalling the latest release and installing `tf-nightly-2.4.0-dev20200815` the GPU was successfully detected. 
Trying out different releases, seems like it doesn't work as of `2.4.0-dev20200819` release.
"
42956,benchmark  error  cannot run in phone,"use  benchmark

HWBKK-Q:/data/local/tmp $ ls
benchmark_model         libhexagon_nn_skel_v65.so     
libhexagon_interface.so libhexagon_nn_skel_v66.so     
libhexagon_nn_skel.so   mobilenet_quant_v1_224.tflite 
/benchmark_model --graph=./mobilenet_quant_v1_224.tflite --num_threads=4      <
/system/bin/sh: ./benchmark_model: not executable: 64-bit ELF file
1|HWBKK-Q:/data/local/tmp $ uname -a
Linux localhost 4.9.82+ #1 SMP PREEMPT Mon Jul 20 02:03:26 CST 2020 aarch64
HWBKK-Q:/data/local/tmp $ 
"
42955,tf.io.gfile.glob hangs if gcs directory contains a directory named '/'.,"**Describe the current behavior**
```
mkdir -p .//
gsutil -m rsync -r .//  gs://remote-folder//
```
```
import tensorflow as tf
tf.io.gfile.glob('gs://remote-folder//*')
```
program hangs forever. The remote folder in google cloud storage has to contain a folder named '/'.  
The directory was created using `gsutil rsync`.

This was on Tensorflow 1.15.3 enterprise, and had the same behavior on 1.15.2 from pip.

edit: just happened again. I am not sure what generates that directory. Tensorboard's 1.15 TPU profiler maybe?

![image](https://user-images.githubusercontent.com/112599/92290815-6267dd80-eeca-11ea-875c-ebf6eac7b804.png)

~~seems this line does the trick of creating that spurious directory:~~
~~tensorboard --logdir=gs://$BUCKET/models/$MODEL/$EXPERIMENT/ --reload_multifile=true --master_tpu_unsecure_channel=$TPU_NAME~~
~notice the trailing `/`~

This is the culprit:

```python
from tensorflow.contrib import summary
with summary.create_file_writer(os.path.join(self._log_dir, 'scalars')).as_default():
    offset = 0
     with summary.record_summaries_every_n_global_steps(
```
if self.log_dir ends with '/' summary writer will create that '/' directory. 
The problem though is most likely in the logic of `gfile.glob`"
42954,"savermodel quantization arrive TFlite model,on the windows+python Test times are slow","quantization: 
import tensorflow as tf
saved_model_dir=""D:/tf_ocr/tf_resnet10/""
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_quantized_model=converter.convert()
open(""D:/sfz/tf_sfz/model.tflite"",""wb"").write(tflite_quantized_model)

test:
 inputImg=(img/255).astype(np.float32)
        interpreter = tf.lite.Interpreter(model_path=model)
        interpreter.allocate_tensors()
        input_details = interpreter.get_input_details()
        output_details = interpreter.get_output_details()
        interpreter.set_tensor(input_details[0]['index'], [inputImg])
        a = time.time()
        # print(a)
        interpreter.invoke()
        b = time.time()
        # print(b)
        print((b - a) * 1000)

tensorflow 1.15.0 + python3.5 + windows10

The size of the model is reduced to its original size 1/4,but the savermodel test time about is 17.16ms,After the quantitative lite test time about is 55.62ms,
The speed was slowed down three times

"
42953,Can't loading model from checkpoint if use adam optimizer,"Tensorflow version:v2.3.0-rc2-23-gb36436b087 2.3.0
Ubuntu 18.04

===================
code:
```
model.compile(optimizer=""adam"",
                 loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                 metrics=[""accuracy""])

checkpoint_dir = 'training/'
checkpoint_path = os.path.join(checkpoint_dir,""cp-{epoch:04d}.ckpt"")
checkpoint_path = os.path.join(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=1000)

model.load_weights(tf.train.latest_checkpoint(os.path.dirname(checkpoint_dir))).expect_partial()
```
If the optimizer = ""adam"" , load_weights get a error:
```
Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-14.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-14.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-16.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-16.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-18.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-18.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-20.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-20.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-22.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-22.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-24.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-24.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-26.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-26.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-28.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-28.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-30.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-30.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-32.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-32.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-13.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-13.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-14.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-14.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-15.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-15.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-16.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-16.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-17.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-17.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-18.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-18.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-19.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-19.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-20.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-20.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-21.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-21.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-22.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-22.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-23.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-23.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-24.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-24.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-25.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-25.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-26.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-26.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-27.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-27.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-28.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-28.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-29.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-29.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-30.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-30.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-31.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-31.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-32.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-32.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-13.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-13.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-14.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-14.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-15.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-15.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-16.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-16.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-17.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-17.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-18.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-18.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-19.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-19.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-20.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-20.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-21.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-21.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-22.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-22.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-23.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-23.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-24.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-24.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-25.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-25.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-26.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-26.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-27.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-27.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-28.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-28.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-29.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-29.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-30.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-30.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-31.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-31.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-32.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-32.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
```

But if optimizer ='sgm' ,everything is OK.  "
42951,CMSIS-NN: Incorrect flags for non-DSP/MVE processors (convolution op only),"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 5a16264ba6f12883726d12d484d4cd61405ddab7
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arm internal testing using models for Cortex-M3

**Describe the problem**
CalculateOpData() in cmsis-nn/conv.cc is inside a '#if defined(__ARM_FEATURE_DSP) || defined(__ARM_FEATURE_MVE)' flag which is not set for a Cortex-M3 or similar that do not have DSP or MVE extensions. This results in uninitialized quantization parameters being passed on to the convolution API in the Eval() procedure. This could result in an assert in debug builds or atleast an incorrect output from convolution.

**Please provide the exact sequence of commands/steps when you ran into the problem** 
Just a visual code check.

"
42950,Regarding the operation of arbitrary scale access to the network,"
When using vgg for migration learning, the specific layer of the network will be used here. I hope that the network can accept pictures of any size as input. I found some information on the Internet. One way is to use lambda as the input layer to replace the original vgg The input layer, but I don’t know how to operate this, is there a better way."
42948,Using custom model with fewer that 1000 classes,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
Andoid studio 4.01 emulator will crash with error below if model has fewer classes than 1000


**Describe the problem**
I am trying to use a custom trained model that has fewer than 1000 classes - is there a way to change the number of classes? I see all the pretrained ones have 1000 classes


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
 java.lang.IllegalArgumentException: Label number 1005 mismatch the shape on axis 1 is the error I get because the output tensor of my model only includes 4 classes. Is this configurable? If not it would be a great feature to add.


Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42947,ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Surface Pro
- TensorFlow installed from (source or binary):
- TensorFlow version:2
- Python version:3.7.9
- Installed using virtualenv? pip? conda?:PIP

Can't import

>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\rache\AppData\Roaming\Python\Python37\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
42946,tf.nn.ctc_greedy_decoder: self define the output node's name,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.15
- Are you willing to contribute it (Yes/No): Maybe



**Describe the feature and the current behavior/state.**
There's no ""name"" argument in the current ""tf.nn.ctc_greedy_decoder"", in both tensorflow 1.15 and tensorflow 2. 
But it does has one in ""tensorflow.python.ops.gen_ctc_ops.ctc_greedy_decoder"", which the ""tf.nn.ctc_greedy_decoder"" inherits from.  
So why not add a ""name"" argument, so users can define the name of output node's name of  ""tf.nn.ctc_greedy_decoder"" by themselves, and locate it easily? 

**Will this change the current api? How?**
Only need to make a little change to ""tf.nn.ctc_greedy_decoder""

**Who will benefit with this feature?**
Everyone who uses this function. 

**Any Other info.**
Actually, I also wonder what's the default output node's name of tf.nn.ctc_greedy_decoder? "
42945,Is this framework suitable for IPv6 network environment?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux centOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Is this framework suitable for IPv6 network environment?


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42943,New tf.image.resize method: Maximum Value,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.2.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
The tf.image.resize method does not include a Maximum Value method. 

**Will this change the current api? How?**
No, I don't think so.

**Who will benefit with this feature?**
tf.image.resize may be a useful function for Region of Interest (RoI) MaxPooling if it allowed for a method that extracts the maximum value for each pooled areas.

In my situation, 3D voxelised data (i.e. XYZ regular point cloud) needs to be unstacked  from 3D into 2D, and then  each 2D stack resized into a new dimension.  Using tf.image.resize with maximum voxel value that falls within each resized element could be useful. 



**Any Other info.**
"
42941,TFLite TransposeConvV2 Operator Slow on x86 CPU Ubuntu,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly 2.4.0.dev20200902
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.5.0
- GCC/Compiler version (if compiling from source): gcc 7.5.0
- CUDA/cuDNN version: CUDA 10.1
- GPU model and memory: Using CPU (Intel(R) Core(TM) i7-8086K CPU @ 4.00GHz)

**Describe the current behavior**
The `TRANSPOSE_CONV` operator takes up >80% of total computation time when using the TFLite benchmarking tool.
```
============================ Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	          TRANSPOSE_CONV	          235.882	  487.948	  482.097	 26.927%	 26.927%	     0.000	        1	[device_0/g_ae/dec_0/conv2d_transpose1]:102
	          TRANSPOSE_CONV	          719.253	  163.323	  162.187	  9.059%	 35.986%	     0.000	        1	[device_0/g_ae/dec_1/conv2d_transpose1]:113
	          TRANSPOSE_CONV	         1634.919	  108.162	  111.988	  6.255%	 42.241%	     0.000	        1	[device_0/g_ae/dec_9/conv2d_transpose1]:201
	          TRANSPOSE_CONV	         1501.813	  116.089	  109.471	  6.114%	 48.355%	     0.000	        1	[device_0/g_ae/dec_8/conv2d_transpose1]:190
	          TRANSPOSE_CONV	          882.714	  111.952	  108.459	  6.058%	 54.413%	     0.000	        1	[device_0/g_ae/dec_2/conv2d_transpose1]:124
	          TRANSPOSE_CONV	          993.583	  103.796	   97.807	  5.463%	 59.876%	     0.000	        1	[device_0/g_ae/dec_3/conv2d_transpose1]:135
	          TRANSPOSE_CONV	         1287.885	   92.329	   95.829	  5.352%	 65.229%	     0.000	        1	[device_0/g_ae/dec_6/conv2d_transpose1]:168
	          TRANSPOSE_CONV	         1394.631	  109.527	   95.786	  5.350%	 70.579%	     0.000	        1	[device_0/g_ae/dec_7/conv2d_transpose1]:179
	          TRANSPOSE_CONV	         1093.908	   92.043	   93.959	  5.248%	 75.827%	     0.000	        1	[device_0/g_ae/dec_4/conv2d_transpose1]:146
	          TRANSPOSE_CONV	         1193.164	   88.003	   89.509	  4.999%	 80.826%	     0.000	        1	[device_0/g_ae/dec_5/conv2d_transpose1]:157

Number of nodes executed: 216
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	          TRANSPOSE_CONV	       11	  1466.204	    81.898%	    81.898%	     0.000	       11
	                 CONV_2D	       11	   159.086	     8.886%	    90.785%	     0.000	       11
	                     ABS	       21	   111.310	     6.217%	    97.002%	     0.000	       21
	                     ADD	       32	    11.551	     0.645%	    97.647%	     0.000	       32
	                     MUL	       42	    10.792	     0.603%	    98.250%	     0.000	       42
	                 RESHAPE	       44	     9.645	     0.539%	    98.789%	     0.000	       44
	                     SUB	       21	     9.366	     0.523%	    99.312%	     0.000	       21
	                    RELU	       21	     6.514	     0.364%	    99.676%	     0.000	       21
	           CONCATENATION	       11	     5.129	     0.286%	    99.962%	     0.000	       11
	      TfLiteFlexDelegate	        1	     0.430	     0.024%	    99.986%	     0.000	        1
	                    TANH	        1	     0.245	     0.014%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=1811076 curr=1775110 min=1738229 max=1895058 avg=1.79038e+06 std=32080
```


**Describe the expected behavior**
Faster execution of this operator. I expected my model to run inference faster once I converted to TFLite, but currently it is running more slowly than regular tensorflow on the same hardware.

**Standalone code to reproduce the issue**
```
bazel-bin/tensorflow/lite/tools/benchmark/benchmark_model_plus_flex \
  --graph=.../converted_model_float32.tflite --num_threads=4 --enable_op_profiling=true > .../float32_benchmark.txt
```
(The benchmarking tool was built from tf master source commit 86db5756535f70f1b1fab61c6f3f0483141510e8)

**Other info / logs**
[Full TFLite benchmark output](https://github.com/tensorflow/tensorflow/files/5171974/float32_benchmark.txt)

I'd appreciate any tips for how I could profile this operator. How can I find out why it is taking so much time in my network? Can I use C++ profiling tools to find the computation time sinks in the [transpose_conv.h](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/internal/optimized/optimized_ops.h#L5804)?"
42940,Converter spews out alphanumeric symbols,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.6
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 2.3.0
- Python 3.8.5


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
from transformers import DistilBertConfig, TFDistilBertForSequenceClassification

model_name = 'distilbert-base-uncased'
config = DistilBertConfig(num_labels=2)
model = TFDistilBertForSequenceClassification.from_pretrained(model_name, config=config)
model.load_weights('tf_model.h5')
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with tf.io.gfile.GFile('distilbert_sst2.tflite', 'wb') as f:
        f.write(tflite_model)
```

**The output from the converter invocation**
There are 3 sets of debug messages/errors. For the third, I'm not sure if they are error or debug messages but a tsunami of alphanumeric symbols is outputted to the terminal.

1.
```
loc(callsite(""tf_distil_bert_for_sequence_classification/distilbert/transformer/layer_._0/ffn/activation_10/Erf""(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py"":742:0) at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py"":3295:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/transformers/modeling_tf_distilbert.py"":70:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py"":427:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"":985:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/transformers/modeling_tf_distilbert.py"":297:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"":302:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"":985:0 at callsite(""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/transformers/modeling_tf_distilbert.py"":347:0 at ""/Users/taylor/anaconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"":302:0)))))))))): error: 'tf.Erf' op is neither a custom op nor a flex op
```
2.
```
error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.Erf {device = """"}
```
3.
```
FF04BD305693BC0F148BBCC1E52DBC089D96BDE60BE2BC4919B5BCE816F8BC0FB82FBD1A333BBDFFB50BBCB3349...
```

**Also, please include a link to the saved model or GraphDef**

https://drive.google.com/drive/folders/1D5cNxuya7uxFvF7DhmSs1pap-8ATbY08?usp=sharing

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Even after 5+ minutes of alphanumeric symbols showing up on the terminal, the `tflite` file is not generated.


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42939,Keras inconsistent behavior when using model.predict for accessing intermediate layers output with spektral GCN,"**Describe the current behavior**
I'm trying to access output of intermediate layers of Graph Convolutional Networks (GCN) and model.predict is throwing InvalidArgument Error for input value where as model.fit is working fine with the same input.

I have copied my code below and it using 'CORA' citation dataset from OGB provided by spektral library (https://graphneural.network/) that provide algorithms and examples for Graph Convolutional network. My code is based on one of the example from the same library, here (https://graphneural.network/getting-started/)

I am using Functional API and I have also verified that the same logic works if I apply it on cases other than Graph Convolutional network. 

**Describe the expected behavior**
The error message is related to mismatch of inner dimensions for multiplication. I tried to use the transponse for input like model_input = [X, At] to fix the issue but still face the same error.

Since, the model gets trailed and evaluated without problems on the input i.e. [X,A], it is expected that model.predict should also work consistently. 

**Standalone code to reproduce the issue**
`from spektral.datasets import citation
from spektral.layers import GraphConv
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense
import numpy as np

A, X, y, train_mask, val_mask, test_mask = citation.load_data('cora')

At = A.transpose()

N = A.shape[0]
F = X.shape[-1]
n_classes = y.shape[-1]

X_in = Input(shape=(F, ))
A_in = Input((N, ), sparse=True)
X_1 = GraphConv(16, 'relu', name=""layer1"")([X_in, A_in])
X_1 = Dropout(0.5, name=""layer2"")(X_1)
X_2 = GraphConv(n_classes, 'softmax', name=""output"")([X_1, A_in])
model = Model(inputs=[X_in, A_in], outputs=X_2)

A = GraphConv.preprocess(A).astype('f4')
At = GraphConv.preprocess(At).astype('f4')

model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              weighted_metrics=['acc'])
model.summary()

# Prepare data
X = X.toarray()
A = A.astype('f4')
At = At.astype('f4')
validation_data = ([X, A], y, val_mask)

# Train model
model.fit([X, A], 
          y,
          sample_weight=train_mask,
          validation_data=validation_data,
          epochs=1,
          batch_size=N,
          shuffle=False
)

# Access intemediate layers of model
layer_name = 'layer2'
intermediate_layer_model = Model(inputs=model.input,
                                 outputs=model.get_layer(layer_name).output)

model_input = [X,A]
intermediate_output = intermediate_layer_model.predict(model_input)
print(""\n\nIntermediate_output="",intermediate_output,""\n\n"")`

**Other info / logs** 
`Traceback (most recent call last):
  File ""PLGcn_example4_stackflow_debug.py"", line 53, in <module>
    intermediate_output = intermediate_layer_model.predict(model_input)
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 130, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1599, in predict
    tmp_batch_outputs = predict_function(iterator)
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/home/mansoor4/.local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  Cannot multiply A and B because inner dimension does not match: 2708 vs. 32.  Did you forget a transpose?  Dimensions of A: [32, 2708).  Dimensions of B: [32,16]
         [[node functional_3/layer1/SparseTensorDenseMatMul/SparseTensorDenseMatMul (defined at /home/mansoor4/.local/lib/python3.7/site-packages/spektral/layers/ops/matmul.py:33) ]] [Op:__inference_predict_function_22928]

Errors may have originated from an input operation.
Input Source operations connected to node functional_3/layer1/SparseTensorDenseMatMul/SparseTensorDenseMatMul:
 stack (defined at PLGcn_example4_stackflow_debug.py:53)
 functional_3/layer1/MatMul (defined at /home/mansoor4/.local/lib/python3.7/site-packages/spektral/layers/ops/matmul.py:45)

Function call stack:
predict_function`
== check pips ===================================================
numpy                  1.18.5
protobuf               3.13.0
tensorflow             2.3.0
tensorflow-estimator   2.3.0

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.version.VERSION = 2.3.0
tf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087
tf.version.COMPILER_VERSION = 7.3.1 20180303

== check python ===================================================
python version: 3.7.5
python branch:
python build version: ('default', 'Dec 16 2019 14:09:08')
python compiler version: GCC 6.3.0
python implementation: CPython


== check os platform ===============================================
os: Linux
os kernel version: #1 SMP Tue Mar 17 23:49:17 UTC 2020
os release version: 3.10.0-1062.18.1.el7.x86_64
os platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-centos-7.7.1908-Core
linux distribution: ('CentOS Linux', '7.7.1908', 'Core')
linux os distribution: ('centos', '7.7.1908', 'Core')
mac version: ('', ('', '', ''), '')
uname: uname_result(system='Linux', node='login1.cluster', release='3.10.0-1062.18.1.el7.x86_64', version='#1 SMP Tue Mar 17 23:49:17 UTC 2020', machine='x86_64', processor='x86_64')
architecture: ('64bit', 'ELF')
machine: x86_64


== are we in docker =============================================
No

== compiler =====================================================
c++ (GCC) 6.3.0
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== env ==========================================================
LD_LIBRARY_PATH /vol/apps/gcc/gcc-6.3.0/lib/:/vol/apps/gcc/gcc-6.3.0/lib64:/vol/apps/gcc/gcc-6.3.0/lib/gcc/x86_64-redhat-linux/6.3.0
DYLD_LIBRARY_PATH is unset

== tensorflow installed from info ==================
Name: tensorflow
Version: 2.3.0
Summary: TensorFlow is an open source machine learning framework for everyone.
Home-page: https://www.tensorflow.org/
Author-email: packages@tensorflow.org
License: Apache 2.0
Location: /home/mansoor4/.local/lib/python3.7/site-packages
Required-by: spektral

== python version  ==============================================
(major, minor, micro, releaselevel, serial)
(3, 7, 5, 'final', 0)


"
42937,"Tflite ops: BatchMatMul, TensorListReserve, TensorListSetItem, TensorListStack","**System information**
- Linux Ubuntu 18.04
- TensorFlow installed from source 
- TensorFlow version 2.1.0

**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, COS, DIV, EXPAND_DIMS, FULLY_CONNECTED, MAX_POOL_2D, MEAN, MUL, NEG, PACK, PAD, RESHAPE, SIN, SQRT, SQUEEZE, STRIDED_SLICE, SUB, SUM. Here is a list of operators for which you will need custom implementations: BatchMatMul, TensorListReserve, TensorListSetItem, TensorListStack.
```

I try to convert generator model from here: https://github.com/russoale/hmr2.0/blob/master/src/main/generator.py

Can you please give me an estimate when such operations as 
BatchMatMul, TensorListReserve, TensorListSetItem, TensorListStack will be transferred to tflite?

Looking forward to hearing from you!

"
42936,Tensorflow GPU Support Installation instructions do not work due to missing cuBLAS,"### System information

-   **OS**: Linux Ubuntu 16.04
-   **TensorFlow installed from (source or binary)**: PyPI binary
-   **TensorFlow version**: 2.3.0
-   **Python version**: 3.7.7
-   **CUDA/cuDNN version**: CUDA 10.1 _installed as suggested using_ `apt-get --no-install-recommends cuda-10-1`, cuDNN 7.6
-   **GPU model and memory**: GeForce GTX 1080 Ti 10.92GiB
-   **Exact command to reproduce**: `tf.config.experimental.list_physical_devices('GPU')`

### Describe the problem
Installing/upgrading CUDA using verbatim instructions for Ubuntu 16.04 from [TF Install GPU Support](https://www.tensorflow.org/install/gpu) results in CUDA _without_ cuBLAS, which does not work for TensorFlow.

It appears there is an issue with the `cuda-10-1` package on APT, because installing from the [Nvidia-provided CUDA 10.1 update 2 runfile](https://developer.nvidia.com/cuda-10.1-download-archive-update2) resulted in the expected cuBLAS files. My workaround was to sloppily extract the raw cuda-toolkit lib directory from the aforementioned runfile and copy the libcublas files into `/usr/local/cuda-10.1/targets/x86_64-linux/lib`. I didn't have to mess with any `include` headers or `/usr/lib` symlinks to get Tensorflow to work. But that's clearly not a good solution, and that shouldn't be the only way to get the ""tested"" configuration to work using the install instructions from TensorFlow's own website.

**To reproduce:**
On an Ubuntu 16.04 machine, uninstall all CUDA drivers and toolkits. Install CUDA using the exact commands listed under _Install CUDA with apt_ on the above page. Install tensorflow in a new virtualenv using pip: `python3 -m pip install tensorflow`. Any GPU code will produce the following error.

### Source code / logs
`Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory`
"
42934,How to save tf.data.Dataset object?,"**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): If possible

**Describe the feature and the current behavior/state.**

Hi, I am using `tf.data.Dataset` to prepare a streaming dataset which is used to train a tf.kears model. To use a python pipeline package like [Luigi](https://luigi.readthedocs.io/en/stable/index.html#) or [kedro](https://kedro.readthedocs.io/en/stable/) it is required to pickle the object. According to quantumblacklabs/kedro#91 the problem is that the `tf.data.Dataset` object ...

> can't be deep copied because somewhere deep inside it contains a `threading.RLock` object (which is quite unusual).

> The reference to pickle is somewhat misleading and comes up due to how deepcopy is implemented. See here for more detail: https://stackoverflow.com/questions/22388854/relationship-between-pickle-and-deepcopy/22618214
> 
> The best fix here would be to talk to the developers of keras about having adam optimizers [and tf.data.Dataset] implement the __getstate__ and __setstate__ functions so that they can be copied (and also pickled).

Would it be possible to add the feature which allows picklable `tf.data.Dataset` objects?

Issue #38483 is similar to this one but got closed because the idiomatic solution seems to be `tfrecords` and the implemented `tf.data.experimental.save` method. However, as far as I know, these approaches will save the complete dataset to disk which can be a huge waste of disk resources and the time it takes to save the complete dataset. 

Is there a way to save just the created `tf.data.Dataset` **object** to disk or are there any plans to implement this? And what would have to be done to achieve this?

Are there alternative suggestions to `tf.data.Dataset` when we want to use reproducible pipelines? 

- tfx
- tf.transform
- Apache beam pipelines (seem to be picklable)

I would welcome some more information about the differences between `tf.data.Dataset` and tfx releated packages (`tf.transform and apache beam). When should we use which?

**Will this change the current api? How?**

**Who will benefit with this feature?**

Everyone who wants to create reproducible pipelines with packages like `kedro` in combination with `tf.data.Dataset`."
42933,Add github handles for maintainer for micro/kernels/arc_mli to the README,"@tensorflow/micro

Tagging @JaccovG and @dzakhar since I'm requesting info from them."
42932,More details on the reduce_codesize tag used for arc,"@tensorflow/micro

This came up during the review for PR #42020 in [this comment](https://github.com/tensorflow/tensorflow/pull/42020#discussion_r483160984)

The TFLM team would like to understand better what this `reduce_codesize` tag is doing in `micro/examples/micro_speech/arc_emsdp/Makefile`

Topics of discussion:

 * In the current design, TAGS was mostly meant as a way to allow for multiple optimized kernel implementations. While not enforced, the expectation is that each tag has a corresponding directory in micro/kernels/

 * We are planning on making some changes in the interest of being able to register different kernel variants that might be useful for this instead of what appears to be a find and replace.
"
42931,"Multiprocessing TypeError: can't pickle _thread.RLock objects, using tf.keras.model","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Enterprise, cygwin64 2.905
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: V10.1.105
- GPU model and memory: NVIDIA GeForce RTX 2070 Super 4.0 GB

**Describe the current behavior**
When I run the standalone code (see below) that loads a tf.keras.model from a .h5 file, see attached and tries to use the model in a python multiprocessing pool I receive the following error
```Traceback (most recent call last):
  File ""simple.py"", line 18, in <module>
    pool.starmap(load_model,[[m,x],[m,x],[m,x]])
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py"", line 276, in starmap
    return self._map_async(func, iterable, starmapstar, chunksize).get()
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py"", line 657, in get
    raise self._value
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\pool.py"", line 431, in _handle_tasks
    put(task)
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\connection.py"", line 206, in send
    self._send_bytes(_ForkingPickler.dumps(obj))
  File ""C:\ProgramData\Anaconda3\lib\multiprocessing\reduction.py"", line 51, in dumps
    cls(buf, protocol).dump(obj)
TypeError: can't pickle _thread.RLock objects
```


**Describe the expected behavior**
I expect the code to provide the output of model.predict() using the loaded model using 2 CPUs

**Standalone code to reproduce the issue**


```
import tensorflow as tf
import multiprocessing
import numpy as np

def load_model(model,pt):
    return model.predict(pt)

if __name__=='__main__':
    x = np.zeros(6)
    m = tf.keras.models.load_model('test.h5')

    with multiprocessing.Pool(2) as pool:
        pool.starmap(load_model,[[m,x],[m,x],[m,x]])
```
[test.zip](https://github.com/tensorflow/tensorflow/files/5170124/test.zip)

Note: This happens if only the CPU is used as well by adding
```
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = '-1'
```

There are a number of github posts referencing this error, but I have not found any that came to a helpful conclusion.
"
42930,TF 1.15+ mutating sys.modules,"**System information**
- No custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 20.04.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.15.2-30-g4386a66 1.15.3
- Python version: 3.6.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the current behavior**
Can't iterate though sys.modules.values after importing tensorflow. Raises `RuntimeError` of `dictionary changed size during iteration`

**Describe the expected behavior**
Should not raise error

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```python
import sys
for v in sys.modules.values():
    print(v)
```
raises no error
```python
import sys
import tensorflow as tf
for v in sys.modules.values():
    print(v)
```
raises `RuntimeError: dictionary changed size during iteration`

Calling `list` on `sys.modules.values()` makes the problem go away, but sadly is not a viable solution. Lots of code currently depends on iterating through `sys.modules.values`, e.g. `unittest.TestCase.assertWarns`

Confirmed this behavior on tf1.15.3, tf2.0. 
Confirmed this behavior is not present in tf1.14 or 1.13

"
42929,libhexagon_nn_skel.so: error adding symbols,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: Samsung Galaxy S10
- TensorFlow version: 2.3
- Installed using: virtualenv, pip
- Bazel version: 3.1.0

**Describe the problem**

I'm trying to add the Hexagon Delegate to a Native (C++) Android app in Android Studio, but build fails with error:
`libhexagon_nn_skel.so: error adding symbols: File in wrong format`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

- Created `libtensorflowlite_hexagon_jni.so` and `libhexagon_interface.so` from Tensorflow Docker container using: `bazel build -c opt --config=android_arm64 [HEXAGON_TARGET_PATH]` (the hexagon repo header included [this link](https://storage.googleapis.com/mirror.tensorflow.org/storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_headers_v1.20.0.0.tgz) to `v1.20.0.0.tgz` so I used `v1.20` interface)
- Created `libhexagon_nn_skel.so`, `libhexagon_nn_skel_v65.so` and `libhexagon_nn_skel_v66.so` from the [v1.20](https://storage.cloud.google.com/download.tensorflow.org/tflite/hexagon_nn_skel_v1.20.0.0.run) link
- Put all shared libraries into the proper ABI folder (`arm64-v8a`) in a JNI folder in the Android Studio project
- Configured `CMakeLists.txt` to include all shared libraries like so:

```
add_library(tflite-hexagon-lib SHARED IMPORTED)
set_target_properties(tflite-hexagon-lib PROPERTIES IMPORTED_LOCATION
        ${JNI_DIR}/${ANDROID_ABI}/libtensorflowlite_hexagon_jni.so)
include_directories(
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/tensorflow/lite/delegates/hexagon)

# The same for libhexagon_interface.so

# The same for libhexagon_nn_skel.so

# The same for libhexagon_nn_skel_v65.so

# The same for libhexagon_nn_skel_v66.so

...

target_link_libraries(
        native-lib
        ...
        tflite-hexagon-lib
        tflite-hexagon-interface-lib
        tflite-hexagon-nn-skel-lib
        tflite-hexagon-nn-skel-v65-lib
        tflite-hexagon-nn-skel-v66-lib)
```

The build runs OK excluding the `nn_skel*` libraries -
Meaning the Tensorflow-Lite shared library itself, and the `libtensorflowlite_hexagon_jni.so` and `libhexagon_interface.so` build just fine.
When adding the `nn_skel*` libraries and trying to build, it fails with `libhexagon_nn_skel.so: error adding symbols: File in wrong format`.

I've checked that they are the same version as the `hexagon-interface` library (which included `--config=android_arm64` in the Bazel command), and they shouldn't be ABI dependent (current build is for `arm64-v8a` only).


**Any other info / logs**

```
[4/4] Linking CXX shared library /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so
FAILED: /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so 
: && /path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android29 --gcc-toolchain=/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -std=c++11 -frtti -fexceptions -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc_real.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -shared -Wl,-soname,libnative-lib.so -o /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflowlite.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflowlite_hexagon_jni.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_interface.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel_v65.so /path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel_v66.so /path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android/29/liblog.so -latomic -lm && :
/path/to/app/src/main/cpp/../jni/arm64-v8a/libhexagon_nn_skel.so: error adding symbols: File in wrong format
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
ninja: build stopped: subcommand failed.
```"
42927,tflite model compiled from h5 outputs just nan,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.3.1

Hey everybody,
i am still relatively inexperienced with tensorflow lite and would like to convert a model with custom training function into a tflite model in order to run it on a google coral accelerator if possible. I am training the model on a NVIDIA Tesla V100 and would like to convert the saved model/graph (.pb, h5) into a tflite.
My model is almost identical to the one in cycleGan Tutorial. 
After the conversion into the tflite format I get only ""NAN"" as model output.
I have also tried to convert the model from the tutorial into tflite and the same error occurs.
I have also tried different conversion types, Tensorflow versions etc. and nothing worked.
Are there possibly any functions in it which tflite does not support?

Thanks a lot in advance."
42926,I got error,"![1](https://user-images.githubusercontent.com/7116366/92135241-22e8b500-ee28-11ea-8039-191a6b92f3ce.PNG)
![2](https://user-images.githubusercontent.com/7116366/92135268-2bd98680-ee28-11ea-8c6f-de4bcf107110.PNG)
 I used the code and got the problem , so please suggest me the solution, please remember I am naive in tensorflow."
42925,"Sports betting data using AI to predict results.  I’m using the tensor flow module in python to aid in my project set by my professor to create a program to predict soccer matches, the only problem is that I’m confused about how the data should be presented.  Data types e.g. possession, av shots, av goals etc. Any suggestions on how to layout this data?",
42924,TensorFlow Lite Micro: Make it possible to specify optimization level when building with the Makefile,"@tensorflow/micro

It would be useful to be able to manually set the optimization levels for different BUILD_TYPE:s from the make-command.

My idea is that the default optimization flags should be as it is now, none for BUILD_TYPE=debug, -O3 for BUILD_TYPE=release. If the user want to use different optimization levels, that should be configurable.

My proposed fix for this is to change tensorflow/lite/micro/tools/make/Makefile as can be seen in [makefile.txt](https://github.com/tensorflow/tensorflow/files/5168744/makefile.txt) , if it looks OK I can submit a pull request.

"
42923,"Aborted (core dumped) - tf 2.3, tf lite model converts but crashes on invoke ","Hey there! 
I'm trying to convert StyleGan2 to TF lite. Currently, there seems to be an issue with invoking the synthesis blocks of the generator, so I've included a small model here which is just one synthesis block, to display the issue.

Converts fine in tf 2.3, with no issues. Then on attempting to invoke, I get the exit code
`Process finished with exit code 134 (interrupted by signal 6: SIGABRT)`
in Pycharm, and
`Aborted (core dumped)`
if running from the terminal. 

Judging from the graph in Netron, seems there are 2 Flex ops (Conv2D on 4-dim tensors in NCHW format) and everything else is built-in ops. 

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source - conda
- TensorFlow version (or github SHA if from source): converted with tf 2.3, tried to invoke on tf2.3 and tf-nightly 

**Command used to run the converter or code if you’re using the Python API**
Here's a colab notebook which downloads the tflite model and attempts to invoke: 
https://colab.research.google.com/drive/1g586G8gWdCrA2EQRzJ3V6PmtbnxpfQUT?usp=sharing

**Link to .tflite model**
https://drive.google.com/file/d/1yvcvTWZlgBbA6mihNJBfEiGCPVz9FqtD/view?usp=sharing

**Link to SavedModel**
https://drive.google.com/drive/folders/1JGtipSJ37p-JJSD-k4j5ZbbWgow0gHJo?usp=sharing

** Log from conversion**
[conversion_log.txt](https://github.com/tensorflow/tensorflow/files/5168414/conversion_log.txt)

```

# Here is the code used to convert the model:

converter = tf.lite.TFLiteConverter.from_keras_model(synth_const)
        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
        tflite_model = converter.convert()

        with tf.io.gfile.GFile('synth_const.tflite', 'wb') as f:
            f.write(tflite_model)

```

Attempted on both the SavedModel and the tf keras functional model. 

**The output from the converter invocation**
Without GPU:
```
2020-09-03 13:00:19.623097: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-09-03 13:00:20.461562: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-03 13:00:20.490338: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3199980000 Hz
2020-09-03 13:00:20.490824: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c893461c80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-03 13:00:20.490859: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-03 13:00:20.495591: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-03 13:00:20.508318: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2020-09-03 13:00:20.508384: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: y4tsu-pc
2020-09-03 13:00:20.508399: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: y4tsu-pc
2020-09-03 13:00:20.508513: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 450.66.0
2020-09-03 13:00:20.508567: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 450.66.0
2020-09-03 13:00:20.508580: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 450.66.0
INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 25 nodes with 2 partitions.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```
With GPU:
```
2020-09-03 13:03:30.866202: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-09-03 13:03:31.687743: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-09-03 13:03:31.710426: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3199980000 Hz
2020-09-03 13:03:31.711119: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564ecd17eb30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-03 13:03:31.711173: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-03 13:03:31.718118: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-09-03 13:03:31.873839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.886701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.887313: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564ecd21f560 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-03 13:03:31.887326: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2020-09-03 13:03:31.887331: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): GeForce GTX 1050 Ti, Compute Capability 6.1
2020-09-03 13:03:31.887558: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.887995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6325GHz coreCount: 28 deviceMemorySize: 10.91GiB deviceMemoryBandwidth: 451.17GiB/s
2020-09-03 13:03:31.888041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.888337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 1 with properties: 
pciBusID: 0000:05:00.0 name: GeForce GTX 1050 Ti computeCapability: 6.1
coreClock: 1.392GHz coreCount: 6 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 104.43GiB/s
2020-09-03 13:03:31.888357: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-03 13:03:31.889727: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-09-03 13:03:31.891078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-09-03 13:03:31.891344: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-09-03 13:03:31.892774: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-09-03 13:03:31.893593: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-09-03 13:03:31.896654: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-09-03 13:03:31.896761: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.897299: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.897737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.898261: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:31.898550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1843] Ignoring visible gpu device (device: 1, name: GeForce GTX 1050 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1) with core count: 6. The minimum required count is 8. You can adjust this requirement with the env var TF_MIN_GPU_MULTIPROCESSOR_COUNT.
2020-09-03 13:03:31.898559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-09-03 13:03:31.898586: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-09-03 13:03:32.301209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-03 13:03:32.301234: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 1 
2020-09-03 13:03:32.301241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N N 
2020-09-03 13:03:32.301244: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 1:   N N 
2020-09-03 13:03:32.301444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:32.302199: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-09-03 13:03:32.302626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9647 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 25 nodes with 2 partitions.

Process finished with exit code 134 (interrupted by signal 6: SIGABRT)
```
Any idea what's going on or a possible fix for this? Should I be changing some of the ops to fit tf lite better in some way? "
42922,Matrices do not consider sample weight ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): provided below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Docker
- TensorFlow version (use command below): Below
- Python version: 3.7

TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
v2.3.0-rc2-23-gb36436b087 2.3.0


**Describe the current behavior**
When  sample weight is provided metrics do not consider that
**Describe the expected behavior**
When  sample weight is provided metrics should consider that

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

data_size = 100
input_size=3
classes=2

x_train = np.random.rand(data_size ,input_size)
y_train= np.random.randint(0,classes,data_size )
x_val = np.random.rand(data_size ,input_size)
y_val= np.random.randint(0,classes,data_size )


inputs = tf.keras.layers.Input(shape=(input_size))
pred=tf.keras.layers.Dense(1, activation='sigmoid')(inputs)

model = tf.keras.models.Model(inputs=inputs, outputs=pred)

loss = tf.keras.losses.binary_crossentropy
metrics = tf.keras.metrics.BinaryCrossentropy()

model.compile(loss=loss , metrics=[metrics], optimizer='adam')


for layer in model.layers:
    layer.trainable = False


sample_weight_train = np.random.uniform(0,1,100)
sample_weight_val = np.random.uniform(0,1,100)
model.fit(x=x_train,y=y_train,sample_weight=sample_weight_train, validation_data=(x_val,y_val,sample_weight_val))
29ms/step - loss: 0.3799 - binary_crossentropy: 0.7369 - val_loss: 0.3454 - val_binary_crossentropy: 0.7502


pred1=model.predict(x_val)
log_loss(y_val,pred1,sample_weight=sample_weight_val),log_loss(y_val,pred1),log_loss(y_val,pred1,sample_weight=sample_weight_val)*np.sum(sample_weight_val)/len(sample_weight_val)
(0.7352043427636902, 0.7502077868580819, 0.34536829505647026)




```

Is it expected behaviour or am I missing something?"
42921,Does the framework support IPv6 networks?,Is this framework suitable for IPv6 network environment?
42920,tf.audio.decode_wav error in chinese wav file,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):tf_nightly-2.3.0.dev20200515-cp36-cp36m-manylinux2010_x86_64.whl
- TensorFlow version (use command below):v1.12.1-31980-g2b2e441205 2.3.0-dev20200515
- Python version:python 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA 10
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
when I use tensorflow to decode wav in an open dataset ""mobovihotwords""
![image](https://user-images.githubusercontent.com/56855140/92093777-4f8bd500-ee06-11ea-811f-56a57de61175.png)
Does tensorflow unable to decode chinese wav file? Or another error?
It's vert strange to me.
**Describe the expected behavior**
And the wav file is ok
![image](https://user-images.githubusercontent.com/56855140/92094121-af827b80-ee06-11ea-9033-bb66803233bf.png)
 besides I can success to decode the wav file in dataset speech commond in the same way?
![image](https://user-images.githubusercontent.com/56855140/92094243-caed8680-ee06-11ea-816d-280b61a3f56b.png)

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42919,请问该框架是否支持ipv6的网络？,"请问该框架是否适用ipv6网络环境？可以在ipv6环境下启动吗
"
42918,Non responsive model when building micro speech with cmsis-nn,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source 
- Tensorflow version (commit SHA if source): d3cdadd
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  Mbed OS (STM32F746)

**Describe the problem**
Trying to build the micro_speech application for STM32F746 (also seen with NXP FRDM K66F) with the cmsis-nn optimized kernels, results in an application which is non-responsive to input. Each iteration of the model results in equal average scores for each category (`= [64, 64, 64, 64]`), such that no predicted commands are ever displayed. The same network response is seen when supplying feature data from `yes_micro_features_data.h` and `no_micro_features_data.h` into the model.

This behavior is only seen when compiling for release mode, when compiling with debug mode the application is responsive to input data, and seems to be somewhat able to detect the spoken words yes and no. 

When generating the application without the cmsis-nn tag, the application runs fine, both for release and debug mode.

**Please provide the exact sequence of commands/steps when you ran into the problem**
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mbed TAGS=""cmsis-nn disco_f746ng"" generate_micro_speech_mbed_project`

In location of generated project:
`mbed config root .`
`mbed deploy` 
`mbed compile -m DISCO_F746NG -t GCC_ARM --profile release --flash`
"
42917,tf.string_split() cannot speedup with num_parallel_calls increased in Dataset.map(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   ``Ubuntu 18.04.4 LTS``
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): ``tensorflow 1.14, tensorflow 1.15, tensorflow 2.3``
- Python version:  ``3.6.9``
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
use the official docker image: ``tensorflow 1.14.0 cpu``, ``tensorflow 2.3.0 cpu``

testing code:
```
import time, sys
import tensorflow as tf
import numpy as np
def benchmark(dataset, num_epochs=1):
    for epoch_num in range(num_epochs):
        _iter = tf.compat.v1.data.make_initializable_iterator(dataset)
        _next = _iter.get_next()
        with tf.compat.v1.Session() as sess:
            sess.run(_iter.initializer)
            start_time = time.time()
            for i in range(1000):
                sess.run(_next)
            seconds = time.time() - start_time
    return seconds

def mapped_function(s):
    return tf.compat.v1.string_split([s], sep=',').values

num_calls = int(sys.argv[1])
csv_datas = [ ','.join(['1' for i in range(8000)]) ] * 1000
ds = tf.data.Dataset.from_tensor_slices(np.array(csv_datas))
ds = ds.map(mapped_function, num_parallel_calls=num_calls).prefetch(10)
print(benchmark(ds))
```

**Describe the current behavior**
run this code on a machine with 56 cores, cost 

*  tf1.14:  ``1.748 ``seconds, which means each ``tf.string_split()`` cost around ``1.7`` ms
*  tf2.3:   ``1.428 ``seconds, which means each ``tf.string_split()`` cost around ``1.4`` ms
*  tf2.3(eager mode):  
     * ``0.34`` seconds when num_calls=1
     * ``0.17`` seconds when num_calls=2 
     * ``0.17`` seconds when num_calls > 2 (cannot decreased any more)

the cost seconds not descrease, when ``num_calls`` increased. 

I also tried ``tf.config.threading.set_inter_op_parallelism_threads()``, not work


**Describe the expected behavior**
cost seconds should descreased when num_calls increased

benchmark for tf2 eager mode :
```
def benchmark2(dataset, num_epochs=1):
    start_time = time.time()
    for epoch in range(num_epochs):
        for ele in dataset:
            pass
    seconds = time.time() - start_time
    return seconds
```
"
42916,Failed to convert SparseTensor to Tensor,"I'm using tensorflow==2.3.0 and keras==2.4.3 on ubunto 20.04
the code work okay on  tensorflow==2.1.0 and keras==2.3.1

```
class Args():
    def __init__(self,dataset_path=""../datasets/train"" ,mymodel=""outputs/my_model.h5"", label=""outputs/le.pickle"", embeddings=""outputs/embeddings.pickle"", image_out=""../datasets/test/img_test.jpg"", image_in=""../datasets/test/001.jpg"", video_out=""../datasets/videos_output/test.mp4"", video_in=""../datasets/videos_input/Ok_Arya_Stark.mp4"", image_size='112,112', model='../models/arcface_r100_v1/model,0', ga_model='', detector='', gpu=0, det=0, flip=0, threshold=1.24):
        
        self.dataset=dataset_path
        self.mymodel=mymodel
        self.le=label
        self.embeddings=embeddings
        self.image_out=image_out
        self.image_in=image_in
        self.video_out=video_out
        self.video_in=video_in
        self.image_size=image_size
        self.model=model
        self.ga_model=ga_model
        self.detector=detector
        self.gpu=gpu
        self.det=det
        self.flip=flip
        self.threshold=threshold
        
    def init_parsearges(self):
        ap = argparse.ArgumentParser()
        
        # Argument of insightface
        ap.add_argument(""--dataset"", default=self.dataset,
                help=""Path to training dataset"")
        
        ap.add_argument(""--mymodel"", default=self.mymodel,
            help=""Path to recognizer model"")
        ap.add_argument(""--le"", default=self.le,
            help=""Path to label encoder"")
        ap.add_argument(""--embeddings"", default=self.embeddings,
            help='Path to embeddings')
        ap.add_argument(""--image-out"", default=self.image_out,
            help='Path to output image')
        ap.add_argument(""--image-in"", default=self.image_in,
            help='Path to output image')
        ap.add_argument(""--video-out"", default=self.video_out,
            help='Path to output video')
        ap.add_argument(""--video-in"", default=self.video_in)


        ap.add_argument('--image-size', default=self.image_size, help='')
        ap.add_argument('--model', default=self.model, help='path to load model.')
        ap.add_argument('--ga-model', default=self.ga_model, help='path to load model.')
        ap.add_argument('--detector', default=self.detector, type=str, help='face detector name')
        ap.add_argument('--gpu', default=self.gpu, type=int, help='gpu id')
        ap.add_argument('--det', default=self.det, type=int, help='mtcnn option, 1 means using R+O, 0 means detect from begining')
        ap.add_argument('--flip', default=self.flip, type=int, help='whether do lr flip aug')
        ap.add_argument('--threshold', default=self.threshold, type=float, help='ver dist threshold')

        args = ap.parse_args()
        
        return args



class SoftMax():
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes

    def build(self):
        from keras.losses import categorical_crossentropy
        from keras.models import Sequential
        from keras.optimizers import Adam
        from keras.layers import Dense, Dropout


        # create model
        model = Sequential()

        # add model layers
        model.add(Dense(1024, activation='relu', input_shape=self.input_shape))
        model.add(Dropout(0.5))
        model.add(Dense(1024, activation='relu'))
        model.add(Dropout(0.5))
        model.add(Dense(self.num_classes, activation='softmax'))

        # loss and optimizer
        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)
        model.compile(loss=categorical_crossentropy,
                      optimizer=optimizer,
                      metrics=['accuracy'])
        return model


def make_model(args, classifier=SoftMax):

    # Load the face embeddings
    data = pickle.loads(open(args.embeddings, ""rb"").read())

    num_classes = len(np.unique(data[""names""])) 
    ct = ColumnTransformer([('myٔName', OneHotEncoder(), [0])])
    labels = np.array(data[""names""]).reshape(-1, 1)
    labels = ct.fit_transform(labels)

    embeddings = np.array(data[""embeddings""])

    # Initialize Softmax training model arguments
    BATCH_SIZE = 32
    EPOCHS = 32
    input_shape = embeddings.shape[1]

    # Build classifier
    init_classifier = classifier(input_shape=(input_shape,), num_classes=num_classes)
    model = init_classifier.build()

    # Create KFold
    cv = KFold(n_splits = 5, random_state = None, shuffle=True)
    history = {'acc': [], 'val_acc': [], 'loss': [], 'val_loss': []}
    # Train
    for train_idx, valid_idx in cv.split(embeddings):
        X_train, X_val, y_train, y_val = embeddings[train_idx], embeddings[valid_idx], labels[train_idx], labels[valid_idx]
        his = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val, y_val))


    # write the face recognition model to output
    model.save(args.mymodel)
    f = open(args.le, ""wb"")
    f.write(pickle.dumps(LabelEncoder()))
    f.close()


```
I get the error:

```
---> 28         his = model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=1, validation_data=(X_val, y_val))

 TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""DeserializeSparse:0"", shape=(None, 2), dtype=int64), values=Tensor(""DeserializeSparse:1"", shape=(None,), dtype=float32), dense_shape=Tensor(""stack:0"", shape=(2,), dtype=int64)). Consider casting elements to a supported type.
```
I have used several issues solutions but none have worked."
42915,Failed to convert SparseTensor to Tensor,
42914,"centos8 gcc8.3 compile meet such question, anyone can help me ?","
ERROR: /home/docker_data/tensorflow_2.3/tensorflow/BUILD:754:1: Linking of rule '//tensorflow:libtensorflow_cc.so.2.3.0' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/docker_data/.bazel_cache/7a9fafc24832c7560e0a6e1a434658e0/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \
    CUDNN_INSTALL_PATH=/usr/local/cuda-10.2 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/targets/x86_64-linux/lib: \
    NCCL_INSTALL_PATH=/usr \
    PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/cuda-10.2/bin:/usr/local/cuda-10.2/targets/x86_64-linux/lib:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages \
    TENSORRT_INSTALL_PATH=/usr/local/cuda-10.2 \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_PATHS=/usr/local/cuda-10.2 \
    TF_CUDA_VERSION=10.2 \
    TF_CUDNN_VERSION=8 \
    TF_ENABLE_XLA=1 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_TENSORRT_VERSION=7 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc @bazel-out/k8-opt/bin/tensorflow/libtensorflow_cc.so.2.3.0-2.params)
Execution platform: @local_execution_config_platform//:platform
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x45c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#2}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x5ac): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#3}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x6fc): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#4}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x84c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#5}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text+0x99c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::kernel_factory::OpKernelRegistrar::OpKernelRegistrar(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, tensorflow::OpKernel* (*)(tensorflow::OpKernelConstruction*))':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow14kernel_factory17OpKernelRegistrarC2EPKNS_9KernelDefEN4absl14lts_2020_02_2511string_viewEPFPNS_8OpKernelEPNS_20OpKernelConstructionEE[_ZN10tensorflow14kernel_factory17OpKernelRegistrarC5EPKNS_9KernelDefEN4absl14lts_2020_02_2511string_viewEPFPNS_8OpKernelEPNS_20OpKernelConstructionEE]+0x55): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `std::_Function_handler<void (), tensorflow::DynamicPartitionOpGPU<std::complex<float> >::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZNSt17_Function_handlerIFvvEZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS1_15OpKernelContextESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data[_ZNSt17_Function_handlerIFvvEZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS1_15OpKernelContextESt8functionIS0_EEUlvE_E9_M_invokeERKSt9_Any_data]+0xa5): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<Eigen::half>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUIN5Eigen4halfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUIN5Eigen4halfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x31e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<float>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUIfE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUIfE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<double>::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUIdE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUIdE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o): In function `tensorflow::DynamicPartitionOpGPU<std::complex<float> >::ComputeAsync(tensorflow::OpKernelContext*, std::function<void ()>)':
tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUISt7complexIfEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): undefined reference to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libdynamic_partition_op_gpu.pic.lo(dynamic_partition_op_gpu.cu.pic.o):tmpxft_000095b8_00000000-5_dynamic_partition_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow21DynamicPartitionOpGPUISt7complexIdEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE[_ZN10tensorflow21DynamicPartitionOpGPUISt7complexIdEE12ComputeAsyncEPNS_15OpKernelContextESt8functionIFvvEE]+0x30e): more undefined references to `tensorflow::OpKernelContext::output_list(absl::lts_2020_02_25::string_view, tensorflow::OpOutputList*)' follow
bazel-out/k8-opt/bin/tensorflow/core/kernels/libgenerate_box_proposals_op_gpu.pic.lo(generate_box_proposals_op.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text+0x31a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
tmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text+0x3d8): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libgenerate_box_proposals_op_gpu.pic.lo(generate_box_proposals_op.cu.pic.o): In function `_GLOBAL__sub_I_tmpxft_000097d4_00000000_5_generate_box_proposals_op.cu.cudafe1.cpp':
tmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text.startup+0x3e0): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libgenerate_box_proposals_op_gpu.pic.lo(generate_box_proposals_op.cu.pic.o): In function `tensorflow::GenerateBoundingBoxProposals::Compute(tensorflow::OpKernelContext*)':
tmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text._ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE]+0x2b0): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
tmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text._ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE]+0x7a4): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
tmpxft_000097d4_00000000-5_generate_box_proposals_op.cu.cudafe1.cpp:(.text._ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE[_ZN10tensorflow28GenerateBoundingBoxProposals7ComputeEPNS_15OpKernelContextE]+0x88b): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libnon_max_suppression_op_gpu.pic.lo(non_max_suppression_op.cu.pic.o): In function `_GLOBAL__sub_I_tmpxft_0000b1d1_00000000_5_non_max_suppression_op.cu.cudafe1.cpp':
tmpxft_0000b1d1_00000000-5_non_max_suppression_op.cu.cudafe1.cpp:(.text.startup+0x4d5): undefined reference to `tensorflow::kernel_factory::OpKernelRegistrar::InitInternal(tensorflow::KernelDef const*, absl::lts_2020_02_25::string_view, std::unique_ptr<tensorflow::kernel_factory::OpKernelFactory, std::default_delete<tensorflow::kernel_factory::OpKernelFactory> >)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#3}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x7a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'
tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0xb2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x1da): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'
tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x212): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#2}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x33a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libsvd_op_gpu.pic.lo(svd_op_gpu.cu.pic.o):tmpxft_0000868d_00000000-5_svd_op_gpu.cu.cudafe1.cpp:(.text+0x372): more undefined references to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, bool*)' follow
bazel-out/k8-opt/bin/tensorflow/core/kernels/sparse/libkernels_gpu.pic.lo(kernels_gpu.cu.pic.o): In function `tensorflow::functor::CalculateNNZPerBatchMatrixFromIndices<Eigen::GpuDevice>::operator()(tensorflow::OpKernelContext*, Eigen::TensorMap<Eigen::Tensor<long long const, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 1, 1, long>, 16, Eigen::MakePointer>)':
tmpxft_0000d31a_00000000-5_kernels_gpu.cu.cudafe1.cpp:(.text+0x6ad): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
tmpxft_0000d31a_00000000-5_kernels_gpu.cu.cudafe1.cpp:(.text+0xa07): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o): In function `tensorflow::Status tensorflow::ZerosLikeTensor<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor*)':
tmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow15ZerosLikeTensorIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorEPS6_[_ZN10tensorflow15ZerosLikeTensorIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorEPS6_]+0x15c2): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o): In function `tensorflow::Status tensorflow::BinaryAddTensors<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::Tensor const&, tensorflow::Tensor const&, tensorflow::Tensor*)':
tmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow16BinaryAddTensorsIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorES8_PS6_[_ZN10tensorflow16BinaryAddTensorsIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_6TensorES8_PS6_]+0xa71): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o): In function `tensorflow::Status tensorflow::data::OptionalBinaryAdd<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant const&, tensorflow::data::OptionalVariant*)':
tmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_[_ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_]+0x13d): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/data/liboptional_ops_gpu.pic.lo(optional_ops.cu.pic.o):tmpxft_0000cdb1_00000000-5_optional_ops.cu.cudafe1.cpp:(.text._ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_[_ZN10tensorflow4data17OptionalBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS0_15OptionalVariantES9_PS7_]+0x1bf): more undefined references to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)' follow
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#3}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x272c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#67}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x281c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#43}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x290c): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#103}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x29fc): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#62}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x2aec): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o):tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x2bdc): more undefined references to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)' follow
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#25}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4a12): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#85}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4b3a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4b72): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#37}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4c9a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4cd2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#73}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4dfa): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4e32): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#97}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4f5a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x4f92): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#49}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x50ba): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x50f2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#1}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x521a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5252): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#61}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x537a): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x53b2): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#13}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x54da): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5512): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, int*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#101}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x564d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x566d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x56c1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#53}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x58fd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x591d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5971): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#30}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5bad): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5bcd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5c21): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#65}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5e5d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5e7d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x5ed1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#78}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x610d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x612d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6181): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#18}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x63bd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x63dd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6431): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#90}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x666d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x668d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x66e1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#54}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x691d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x693d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6991): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#6}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6bcd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6bed): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6c41): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#102}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6e7d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6e9d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x6ef1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#17}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x712d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x714d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x71a1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#41}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x73dd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x73fd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7451): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#42}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x768d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x76ad): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7701): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#29}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x793d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x795d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x79b1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#5}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7bed): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7c0d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7c61): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#66}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7e9d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7ebd): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x7f11): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#77}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x814d): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x816d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x81c1): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::{lambda(tensorflow::OpKernelConstruction*)#89}::_FUN(tensorflow::OpKernelConstruction*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x83fd): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::DataType*)'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x841d): undefined reference to `tensorflow::OpKernelConstruction::HasAttr(absl::lts_2020_02_25::string_view) const'
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text+0x8471): undefined reference to `tensorflow::GetNodeAttr(tensorflow::AttrSlice const&, absl::lts_2020_02_25::string_view, tensorflow::PartialTensorShape*)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/liblist_kernels_gpu.pic.lo(list_kernels.cu.pic.o): In function `tensorflow::Status tensorflow::TensorListBinaryAdd<Eigen::GpuDevice>(tensorflow::OpKernelContext*, tensorflow::TensorList const&, tensorflow::TensorList const&, tensorflow::TensorList*)':
tmpxft_00009f81_00000000-5_list_kernels.cu.cudafe1.cpp:(.text._ZN10tensorflow19TensorListBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_10TensorListES8_PS6_[_ZN10tensorflow19TensorListBinaryAddIN5Eigen9GpuDeviceEEENS_6StatusEPNS_15OpKernelContextERKNS_10TensorListES8_PS6_]+0x1c2): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libwhere_op_gpu.pic.lo(where_op_gpu_impl_1.cu.pic.o): In function `tensorflow::Status tensorflow::errors::Internal<char const*, char const*>(char const*, char const*)':
tmpxft_0000cbd8_00000000-5_where_op_gpu_impl_1.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcS3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcS3_EEENS_6StatusEDpT_]+0x68): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libwhere_op_gpu.pic.lo(where_op_gpu_impl_1.cu.pic.o): In function `tensorflow::Status tensorflow::errors::Internal<char const*, unsigned long, char const*, char const*>(char const*, unsigned long, char const*, char const*)':
tmpxft_0000cbd8_00000000-5_where_op_gpu_impl_1.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcmS3_S3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcmS3_S3_EEENS_6StatusEDpT_]+0xc5): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libbincount_op_gpu.pic.lo(bincount_op_gpu.cu.pic.o): In function `tensorflow::Status tensorflow::errors::Internal<char const*, char const*, char const*>(char const*, char const*, char const*)':
tmpxft_00008562_00000000-5_bincount_op_gpu.cu.cudafe1.cpp:(.text._ZN10tensorflow6errors8InternalIJPKcS3_S3_EEENS_6StatusEDpT_[_ZN10tensorflow6errors8InternalIJPKcS3_S3_EEENS_6StatusEDpT_]+0x94): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libtopk_op_gpu.pic.lo(topk_op_gpu_double.cu.pic.o): In function `tensorflow::functor::TopKFunctor<Eigen::GpuDevice, double>::Compute(tensorflow::OpKernelContext*, bool, int, Eigen::TensorMap<Eigen::Tensor<double const, 2, 1, long>, 16, Eigen::MakePointer> const&, long long, long long, Eigen::TensorMap<Eigen::Tensor<double, 2, 1, long>, 16, Eigen::MakePointer>, Eigen::TensorMap<Eigen::Tensor<int, 2, 1, long>, 16, Eigen::MakePointer>)':
tmpxft_0000d38b_00000000-5_topk_op_gpu_double.cu.cudafe1.cpp:(.text._ZN10tensorflow7functor11TopKFunctorIN5Eigen9GpuDeviceEdE7ComputeEPNS_15OpKernelContextEbiRKNS2_9TensorMapINS2_6TensorIKdLi2ELi1ElEELi16ENS2_11MakePointerEEExxNS7_INS8_IdLi2ELi1ElEELi16ESB_EENS7_INS8_IiLi2ELi1ElEELi16ESB_EE[_ZN10tensorflow7functor11TopKFunctorIN5Eigen9GpuDeviceEdE7ComputeEPNS_15OpKernelContextEbiRKNS2_9TensorMapINS2_6TensorIKdLi2ELi1ElEELi16ENS2_11MakePointerEEExxNS7_INS8_IdLi2ELi1ElEELi16ESB_EENS7_INS8_IiLi2ELi1ElEELi16ESB_EE]+0x1c9): undefined reference to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)'
bazel-out/k8-opt/bin/tensorflow/core/kernels/libtopk_op_gpu.pic.lo(topk_op_gpu_double.cu.pic.o):tmpxft_0000d38b_00000000-5_topk_op_gpu_double.cu.cudafe1.cpp:(.text._ZN10tensorflow4impl16LaunchSortKernelIdEENS_6StatusEPNS_15OpKernelContextEPKT_iiiNS_6TTypesIS5_Li2ElE6TensorEN5Eigen9TensorMapINSB_6TensorIiLi2ELi1ElEELi16ENSB_11MakePointerEEE[_ZN10tensorflow4impl16LaunchSortKernelIdEENS_6StatusEPNS_15OpKernelContextEPKT_iiiNS_6TTypesIS5_Li2ElE6TensorEN5Eigen9TensorMapINSB_6TensorIiLi2ELi1ElEELi16ENSB_11MakePointerEEE]+0x590): more undefined references to `tensorflow::Status::Status(tensorflow::error::Code, absl::lts_2020_02_25::string_view)' follow
collect2: error: ld returned 1 exit status
Target //tensorflow:libtensorflow_cc.so failed to build
INFO: Elapsed time: 2229.194s, Critical Path: 364.17s
INFO: 11324 processes: 11324 local.
FAILED: Build did NOT complete successfully
"
42912,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, FILL, FLOOR, GATHER, GATHER_ND, LEAKY_RELU, MAXIMUM, MEAN, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TRANSPOSE_CONV, UNPACK. Here is a list of operators for which you will need custom implementations: MaxPoolWithArgmax, Size.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CAST, CONCATENATION, CONV_2D, DIV, EXPAND_DIMS, FILL, FLOOR, GATHER, GATHER_ND, LEAKY_RELU, MAXIMUM, MEAN, MINIMUM, MUL, PACK, PAD, RANGE, RESHAPE, RESIZE_BILINEAR, RESIZE_NEAREST_NEIGHBOR, SHAPE, STRIDED_SLICE, SUB, TRANSPOSE_CONV, UNPACK. Here is a list of operators for which you will need custom implementations: MaxPoolWithArgmax, Size.


```
# Copy and paste here
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
42911,"bug when using ""num_parallel_calls"" when mapping dataset to tfa function","As mentioned over the issue [here](https://github.com/tensorflow/addons/issues/2139) and advised from other contributors, i'm creating this issue cause using ""num_parallel_calls=tf.data.experimental.AUTOTUNE"" inside the .map call from my dataset, appeared to generate a deadlock.

I've tested with tensorflow versions 2.2 and 2.3, and tensorflow addons 0.11.1 and 0.10.0
On Google Colab Pro gpu env, python3.8

Link to example TFRecord: https://drive.google.com/drive/folders/1dc6ehBGL_mwGTuSy71VhUYVp0eMdHADP?usp=sharing

### Code to Reproduce the issue:
```python
test_dataset = tf.data.TFRecordDataset(num_parallel_reads=tf.data.experimental.AUTOTUNE,filenames=DRIVE_DIR+'/tf_issue/test_0.tfrecord').map(parsing_fn,num_parallel_calls=tf.data.experimental.AUTOTUNE)
test_dataset = test_dataset.map(translate,num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE) 
test_dataset = test_dataset.unbatch()
iterator = tf.compat.v1.data.make_one_shot_iterator(test_dataset)
for i in range(5):
  image,label = iterator.get_next()
```
###  Auxiliary functions
```python

def parsing_fn(serialized):
    features = \
        {
            'image': tf.io.FixedLenFeature([], tf.string),
            'label': tf.io.FixedLenFeature([], tf.int64)            
        }
    parsed_example = tf.io.parse_single_example(serialized=serialized,
                                             features=features)
    image_raw = parsed_example['image']
    image = tf.io.decode_jpeg(image_raw)    
    image = tf.image.resize(image,size=[224,224])    
    label = parsed_example['label']    
    return image, label

translate = lambda image,label: tf.py_function(func=translate_pipeline,inp=[image,label],Tout=[tf.float32,tf.int64])
def translate_pipeline(original_image,label):
  print(1)
  height = tf.shape(original_image)[0].numpy()    
  width = tf.shape(original_image)[1].numpy()
  y_fraction  = tf.convert_to_tensor(height * 0.2, dtype=tf.float32)
  x_fraction = tf.convert_to_tensor(width * 0.2,dtype=tf.float32)
  print(2)
  batched_image = tf.tile(tf.expand_dims(original_image,axis=0),[4,1,1,1]) # Create 4 copied versions from the original image and add to a batch
  translated_images = tfa.image.translate_ops.translate(images=batched_image,translations=[[x_fraction,-y_fraction],[-x_fraction,y_fraction],[-x_fraction,-y_fraction],[x_fraction,y_fraction]])
  augmented_images = tf.concat([tf.expand_dims(original_image,axis=0),translated_images],axis=0)
  print(3)
  label = tf.reshape(label,[1,1])
  labels = tf.tile(label,[5,1])
  print(4)
  return augmented_images, labels

```
### Output
>1
>1
>2
>2

"
42909,Deprecated function setdiff1d still used in the tf source code,"**System information**
- I have written custom code
- Linux Ubuntu 20.04
- TensorFlow 2.3 installed using pip
- Python 3.8.2

**Current (unexpected) behavior**
Calculating the gradient of the `reduce_prod` function raises this warning.
> WARNING:tensorflow:From /home/prasanth/.local/pythonuserbase/lib/python3.8/site-packages/tensorflow/python/ops/math_grad.py:297: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.
Instructions for updating:
This op will be removed after the deprecation date. Please switch to tf.sets.difference().

**Standalone code to reproduce the issue**
(The warning will only be displayed once in a session)
```
import tensorflow as tf

x = tf.ones(5)
with tf.GradientTape() as g:
    g.watch(x)
    y = tf.math.reduce_prod(x)

grad = g.gradient(y, x)
```"
42906,Please release TFLite wheels for Beaglebone,"The [prebuilt wheels](https://www.tensorflow.org/lite/guide/python) of the Tensorflow Lite interpreter for armv7l do not run on the [Beaglebone](https://beagleboard.org/black/), triggering an ""invalid instruction"" error. They seem to have been built for the Raspberry Pi using the `-mfpu=neon-vfpv4` build option which is appropriate for that hardware, but not all armv7l devices.

Pull request #28724 added a `bbb` make target which produces a usable library on the Beaglebone, but when building the wheel, the code still assumes `rpi`:

https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package.sh#L42-L45
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/build_pip_package.sh#L85-L87
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/pip_package/setup.py#L54-L56

Since it is rather complicated to build, it would be great if official releases could be made. And in the interim, it would be helpful to note on the releases page what hardware features the builds require, to avoid confusion and frustration over incompatible wheels. "
42905,module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary (pip install tensor flow)
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
when try to import official.nlp.bert.bert_models it shows error (try to reimplement the code in  https://www.tensorflow.org/official_models/fine_tuning_bert)

module 'tensorflow.keras.layers' has no attribute 'MultiHeadAttention'

**Describe the expected behavior**


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

It's a little weird because when I check the current master branch of tensorflow, there is multiheadattention in keras.layers. However, there is no in version 2.3.0. I think I can proceed with the master branch. It seems that with `pip install tensorflow -U` I can only install 2.3.0. Is there any way to solve the problem? Maybe I should download the source code and compile by myself?
"
42900,experimental_steps_per_execution option discarded when loading a model from checkpoint,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Google Colab, TPU
- TensorFlow version: 2.3
- Python version: 3

**Describe the current behavior**
I have created and trained a model compiled with the `experimental_steps_per_execution` option. 
When I load the model through `tf.keras.models.load_model` to resume the training the option `experimental_steps_per_execution' is not taken in consideration and keras perform `on_step...` actions after every step and not after n steps as specified in `experimental_steps_per_execution` when the model was first compiled.

**Describe the expected behavior**
The model should load the `experimental_steps_per_execution' option or there should be an option to specify `experimental_steps_per_execution` when loading the model.

If necessary I can modify an MNIST example on Colab, but to reproduce this bug it is just needed to:

- create a model
- compile it with the `experimental_steps_per_execution` option enabled
- train it for a few epoch to verify the correct behaviour of `experimental_steps_per_execution`
- save the model
- load the model through `tf.keras.models.load_model`
- train the model for a few epoch to verify that `experimental_steps_per_execution` is being disregarded

**Other info / logs** 
One possible workaroud (which I am testing right now) is to set manually the steps per execution with `model._configure_steps_per_execution(experimental_steps_per_execution)` once it is loaded.

Thank you for your help!
"
42899,How preprocess image from tf.io.decode_raw ?,"I want to use tf.keras.preprocessing.image.random_rotation and other function to get more dataset , but these image from 
```
tf.io.decode_raw(features['image'], out_type=tf.uint8)
```
so it's tensor image , tf.keras.preprocessing.image.random_rotation must be numpy array ,anyone know how can I do?"
42898,How to reduce the size of the tflite model below 200mb while converting h5 to tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.2.0




If possible, please share a link to Colab/Jupyter/any notebook.

https://gist.github.com/SaiBalaji22/e3815c47970416c37328e9f69a43c958

`

**Failure details**

I have made an image classifier model to classifiy rock,paper,scissor images.But the problem is i was able to convert it to tflite model but the size of the model exceeds 200mb.I want to use it in android studio.When i add the model it give me error.How can i reduce the tflite model size while conversion.
![image](https://user-images.githubusercontent.com/51410810/92009720-8ca08a80-ed66-11ea-8f3b-7a2affb455ba.png)



"
42896,Documentation is becoming too undecipherable for a meaningful practical work,"The documentation is becoming too undecipherable for any practical applications using Tensorflow.

Look at this one, https://www.tensorflow.org/guide/data

How would anyone learn anything for a streaming data from Disk ? Not many how-tos for data that don't fit into memory. And that page.. just like a man-page for API. We end up wasting many man hours without good documentation."
42895,Does TensorFlow1.x support CUDA11? And how can i compile tensorflow 1.x with cuda11?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
42894,Keras load_model breaks for hdf5 files on Google Cloud Storage bucket,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 9 (TensorFlow:2.3 image on GCP) 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.0
- GPU model and memory: N/A

**Describe the current behavior**

Calling `tensorflow.keras.model.load_model` with hdf5 serialized models on GCS breaks
```
from tensorflow.keras.models import load_model

path = 'gs://mybucket/foo.hdf5'  # anonymized
load_model(path)
```
gives
```
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-7-33783c76cb4e> in <module>
      1 if config.model['build']['middle_ear'] is not None:
----> 2     config.model['build']['middle_ear'] = load_model(config.model['build']['middle_ear']['path'])

~/.venv/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    184     filepath = path_to_string(filepath)
    185     if isinstance(filepath, six.string_types):
--> 186       loader_impl.parse_saved_model(filepath)
    187       return saved_model_load.load(filepath, compile, options)
    188 

~/.venv/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py in parse_saved_model(export_dir)
    111                   (export_dir,
    112                    constants.SAVED_MODEL_FILENAME_PBTXT,
--> 113                    constants.SAVED_MODEL_FILENAME_PB, path_to_pbtxt, path_to_pb))
    114 
    115 

OSError: SavedModel file does not exist at: gs://mybucket/foo.hdf5/{saved_model.pbtxt|saved_model.pb}.
```
while loading the same file locally works fine
```
load_model('/tmp/081.hdf5')
<tensorflow.python.keras.engine.functional.Functional at 0x7f49f0276250>
```
I've tracked it down to the h5py check [here](https://github.com/tensorflow/tensorflow/blob/963e2693da908b0fa3d0174ac16c821d5bcc66fb/tensorflow/python/keras/saving/save.py#L182), which seems to always evaluate to false for `gs://` filepaths
```
>>> import h5py
>>> h5py.is_hdf5('gs://mybucket/foo.hdf5')
False
>>> h5py.is_hdf5('/tmp/foo.hdf5')
True
```
which is why it falsely ends up [here](https://github.com/tensorflow/tensorflow/blob/963e2693da908b0fa3d0174ac16c821d5bcc66fb/tensorflow/python/keras/saving/save.py#L188) in the case analysis.

**Describe the expected behavior**

hdf5 serialized models should be loadable from GCS buckets

"
42893,“No dashboards are active for the current dataset” despite the event file is created correctly,"**System information**
- OS Platform and Distribution: Win 10
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.1.0
- Python version: 3.7.9
- tensorboard 2.2.1
- jupyter notebook 6.1.6
- same problem with spyder 4.1.4
- GPU model and memory: None

**Describe the current behavior**
I trained a CNN and created log files for tensorboard successfully. Now I want to optimize the model with tensorboard. For that I called ""cmd"" in Windows 10, activated the correct environment and typed:
`tensorboard --logdir=C:\Users\bauerch\Documents\Py_DL_Env\DL_Sentdex\logs`

When I go to http://localhost:6006/ in my browser I get the following result:
![grafik](https://user-images.githubusercontent.com/67313472/91977423-f39c5000-ed22-11ea-8696-0f6053a719de.png)

**As you can see at the bottom of the picture there is written ""Data location: /tmp/MNIST_data"". But I created the event file in the directory named above.**

What I tried so far without success:
- https://github.com/tensorflow/tensorboard/blob/master/README.md#my-tensorboard-isnt-showing-any-data-whats-wrong.
- https://stackoverflow.com/questions/47113472/tensorboard-error-no-dashboards-are-active-for-current-data-set
- https://github.com/tensorflow/tensorflow/issues/7856

**Describe the expected behavior**
TensorBoard should be opened without a problem via http://localhost:6006/

**Standalone code to reproduce the issue**

```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D
import pickle
import numpy as np
from tensorflow.keras.callbacks import TensorBoard 
import time

# Tensorboard
NAME = ""Cats-vs-dog-cnn-64x2-{}"".format(int(time.time())) 
tensorboard = TensorBoard(log_dir=""logs\\{}"".format(NAME))

# Data import
X = pickle.load(open(""X.pickle"",""rb"")) 
y = pickle.load(open(""y.pickle"",""rb""))

# Model
model = Sequential()
### 1.Layer
model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:]))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
### 2.Layer
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu')) 
model.add(MaxPooling2D(pool_size=(2, 2)))
### 3.Layer
model.add(Flatten()) 
model.add(Dense(64))
model.add(Activation('relu')) # Activation function muss man stets hinzufügen, da man sonst
# einen sinnlosen linearen Output aus den Neuronen bekommt
### 4.Output Layer
model.add(Dense(1))
model.add(Activation('sigmoid'))

# Compile
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Training
model.fit(X, y, batch_size=32, epochs=2, validation_split=0.3, callbacks=[tensorboard])
```
"
42892,TFLite compiling error,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (or github SHA if from source): 2.2.0 (downgraded from 2.3.0)


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
model_name = ""new_model""
import_dir = "".\exported-models\\"" + model_name + ""\saved_model""

# Convert the model.
converter = tf.lite.TFLiteConverter.from_saved_model(import_dir)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile(""exported-models\\"" + model_name + "".tflite"", 'wb') as f:
  f.write(tflite_model)
```

**The output from the converter invocation**

```
Traceback (most recent call last):
  File ""custom_convert.py"", line 25, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py"", line 920, in convert
    return super(TFLiteConverterV2, self).convert()
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py"", line 751, in convert
    return super(TFLiteFrozenGraphConverterV2,
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py"", line 494, in convert
    result = _toco_convert_impl(
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 546, in toco_convert_impl
    data = toco_convert_protos(
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\convert.py"", line 256, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-09-02 12:55:48.008380: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:307] Ignored output_format.
2020-09-02 12:55:48.008562: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:310] Ignored drop_control_dependency.
loc(""Func/StatefulPartitionedCall/input/_1""): error: requires all operands and results to have compatible element types
Traceback (most recent call last):
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 93, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""c:\users\workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 50, in execute
    output_str = _pywrap_toco_api.TocoConvert(
Exception: <unknown>:0: error: loc(""Func/StatefulPartitionedCall/input/_1""): requires all operands and results to have compatible element types
<unknown>:0: note: loc(""Func/StatefulPartitionedCall/input/_1""): see current operation: %1 = ""tf.Identity""(%arg0) {device = """"} : (tensor<1x?x?x3x!tf.quint8>) -> tensor<1x?x?x3xui8>
```

**When I uninstaled tf-nightly and reinstalled tensorflow 2.2.0 i get this output after trying to convert**
```
Traceback (most recent call last):
  File ""custom_convert.py"", line 25, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\Workstation\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\lite\python\lite.py"", line 480, in convert
    raise ValueError(
ValueError: None is only supported in the 1st dimension. Tensor 'input_tensor' has invalid shape '[1, None, None, 3]'.
```

**Also, please include a link to the saved model or GraphDef**

```
https://cloud.zbe.si/s/ADNzzjizn82WtHe
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)

Error mid converting the file. I had to downgrade from 2.3.0 to 2.2.0 so I could load tflite model into my flutter app. Now i cant even convert the file without installing tf-nightly, which repeats the same error in flutter.


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42891,Tensorflow 2.3.0 Java API on Windows - UnsatisfiedLinkError: Cannot find TensorFlow native library for OS,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1 Pro (64bit)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: N/A (using Tensorflow Java API for Windows)
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A (CPU-only)

**Describe the current behavior**
Currently I've problems to run the `HelloTensorFlow.java` with Tensorflow Java 2.3.0 API on Windows. 

The start of `HelloTensorFlow.java` fails with the following error message:
```
org.tensorflow.NativeLibrary: jniResourceName: org/tensorflow/native/windows-x86_64/tensorflow_jni.dll
org.tensorflow.NativeLibrary: frameworkResourceName: org/tensorflow/native/windows-x86_64/tensorflow_framework.dll
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Cannot find TensorFlow native library for OS: windows, architecture: x86_64. See https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md for possible solutions (such as building the library from source). Additional information on attempts to find the native library can be obtained by adding org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM.
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:79)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:67)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:82)
	at org.tensorflow.Graph.<clinit>(Graph.java:479)
	at com.test.HelloTensorFlow.main(HelloTensorFlow.java:12)
```


I double-checked that _-Djava.library.path_ contains the correct path to _tensorflow_jni.dll_. _Visual C++2015 Redistributable_ is also installed on my Windows system.

I've tested the same `HelloTensorFlow.java` on macOS (with the corresponding macOS TF 2.3.0 native libs). Here everything works like a charm.

**Standalone code to reproduce the issue**
I've used 
- `tensorflow_jni.dll` downloaded at https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-windows-x86_64-2.3.0.zip
- `libtensorflow-2.3.0.jar` downloaded at https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-2.3.0.jar
- `HelloTensorFlow.java` (copied from https://www.tensorflow.org/install/lang_java)

I've also added the necessary VM argument: -Djava.library.path=<path-to-tensorflow_jni.dll>
"
42890,load_model not properly working in TF 2.3.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom Code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- TensorFlow installed from (source or binary): Colab TF 2.3.0 vs TF 2.2.0
- TensorFlow version (use command below): Colab TF 2.3.0 vs TF 2.2.0
- Python version: Colab Python
- GPU model and memory: Colab GPU

When I used TF 2.2.0 in colab I was able to fit, save and load a model. So I run my code, fit my model:
![grafik](https://user-images.githubusercontent.com/68148852/91966779-9bf5e880-ed12-11ea-8ec3-7e499a5024eb.png)

 and check again with model.evaluate:
![grafik](https://user-images.githubusercontent.com/68148852/91966830-ac0dc800-ed12-11ea-9ab8-68c2b84cb218.png)

Everything as expected. Then I save the model and load it back again and I check it again with model.evaluate and same accuracy is shown. Everything fine. However, the same code does not work in TF 2.3.0. See my code below. If this code is run by default TF 2.3.0 is used in colab and it does not work. The accuracy of the loaded model loadedmodel.evaluate(x_test_r,test_labels) is not the same:

![grafik](https://user-images.githubusercontent.com/68148852/91966880-c051c500-ed12-11ea-9b72-3c57fffc5646.png)

It is completely wrong (approx .098... vs supposed 0.99... when I run it). When I add !pip install tensorflow==2.2.0 to my code and run the same code in my colab again (note that the runtime has to be restarted) then with the 2.2.0 version it does work.

(When I save the model in TF 2.2.0 (where evaluate works and correct accuracy is shown) and then load it in TF 2.3.0, it does not work. model.evaluate shows a wrong accuracy.)

**Standalone code to reproduce the issue**
```
#!pip install tensorflow==2.2.0
import tensorflow as tf
import os
import PIL
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_datasets as tfds
import tensorflow_hub as hub

from tensorflow.keras.models import Sequential

from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator

import keras_preprocessing
from keras_preprocessing import image

from tensorflow.python.keras.utils.version_utils import training
from tensorflow.keras.optimizers import RMSprop

print(tf.__version__)

(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()

x_trainf = train_images.astype('float32') / 255.0
x_testf = test_images.astype('float32') / 255.0

x_train_r = x_trainf.reshape(x_trainf.shape[0], 28, 28, 1)
x_test_r = x_testf.reshape(x_testf.shape[0], 28, 28, 1)

model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(28,28,1))) 
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))
model.add(tf.keras.layers.MaxPooling2D(pool_size=2))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(10))

model.compile(optimizer='adam',
              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.fit(x_train_r, 
          train_labels, batch_size=32,  
          epochs=10,
          validation_data=(x_test_r,test_labels))

model.evaluate(x_test_r,test_labels)

model.save('/tmp/loaddatamnist.h5')

loadedmodel=tf.keras.models.load_model('/tmp/loaddatamnist.h5')
loadedmodel.evaluate(x_test_r,test_labels)
```


"
42889,nan gradient issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6.8 not using gpu

**Describe the current behavior**
training on an easy example, tf sometimes got `nan` for gradient
**Describe the expected behavior**

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np
import time
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
# on cpu nan at 1098 iter
# Iter    0: loss 9.99180138e-01,  runtime:     0.11
# Iter    1: loss 9.98347044e-01,  runtime:     0.11
# Iter    2: loss 9.97434497e-01,  runtime:     0.12
# on gpu nan at 2212 iter
# Iter    0: loss 9.99180079e-01,  runtime:     2.46
# Iter    1: loss 9.98346925e-01,  runtime:     2.47
# Iter    2: loss 9.97434497e-01,  runtime:     2.48

np.random.seed(1)
tf.keras.backend.set_floatx('float32')

func = lambda x: x
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-1)

data = np.array(
    [[0., 0.07179281], [0., 0.44064897], [0., 0.7666122], [0., -0.655319],
     [0., -0.28546047], [0., 0.8460491], [0., 0.14823522], [0., -0.14381762],
     [0., 0.7200559], [0., -0.92189044], [0., 0.37300184], [0., -0.525946],
     [0., 0.07766213], [0., 0.370439], [0., 0.17311008], [0., 0.88918954],
     [0., -0.5910955], [0., -0.947578], [0., -0.7192261], [0., 0.5109261],
     [0., 0.85887444], [0., -0.75145805], [0., 0.89897853], [0., 0.23428982],
     [0., 0.5785587], [0., 0.0541162], [0., 0.97772217], [0., 0.24339144],
     [0., -0.72505057], [0., -0.39533487], [0., 0.6692513], [0., -0.7257285],
     [0., 0.93652314], [0., 0.17861107], [0., 0.38464522], [0., 0.38880032],
     [0., -0.73994285], [0., -0.9602397], [0., 0.07763347], [0., 0.6147826],
     [0., 0.68406177], [0., 0.39951673], [0., -0.17188802], [0., -0.10017573],
     [0., 0.7917724], [0., 0.35767105], [0., 0.7892133], [0., -0.62747955],
     [0., 0.7562349], [0., -0.16161098], [0., -0.77050805], [0., 0.8068038],
     [0., -0.37315163], [0., -0.3467102], [0., -0.70654285], [0., -0.8679997],
     [0., 0.5002886], [0., -0.7214473], [0., 0.7718842], [0., -0.5767438],
     [0., 0.8550172], [0., 0.4230495], [0., -0.7064882], [0., 0.11737966],
     [0., 0.326883], [0., -0.439112], [0., -0.99425936], [0., -0.94338703],
     [0., -0.8153228], [0., 0.8651909], [0., -0.96342343], [0., 0.9296801],
     [0., -0.50757784], [0., 0.24734442], [0., 0.80675906], [0., 0.38375422],
     [0., -0.7953311], [0., -0.4127717], [0., 0.39363632], [0., -0.30887854],
     [0., -0.8299116], [0., -0.603797], [0., -0.9452248], [0., -0.80330634],
     [0., 0.34093502], [0., -0.793548], [0., 0.6014891], [0., 0.7527783],
     [0., 0.38179383], [0., -0.9000931], [0., 0.4963313], [0., 0.45199597],
     [0., -0.9612661], [0., -0.30446827], [0., 0.9946457], [0., 0.14735897],
     [0., 0.24672022], [0., -0.20646505], [0., -0.20464632], [0., -0.1837264],
     [0., 0.8170703], [0., -0.15778475], [0., 0.5018849], [0., -0.8932749],
     [0., 0.10564396], [0., 0.91577905], [0., -0.01685368], [0., -0.42444932],
     [0., -0.30220333], [0., -0.46014422], [0., -0.99977124], [0., 0.06633057],
     [0., 0.15677923], [0., -0.46890667], [0., -0.36896873], [0., -0.6692916],
     [0., -0.17164145], [0., 0.756285], [0., -0.16595599], [0., 0.817191],
     [0., 0.5016242], [0., 0.3275893], [0., 0.50775236], [0., 0.02977822],
     [0., -0.10421295], [0., -0.9683575], [0., -0.6603392], [0., -0.1653904]],
    dtype=np.float32)
data_x = data[:, 0:1]
data_y = data[:, 1:2]


def loss_func(x, y):
    return tf.reduce_mean(tf.norm(func(x) - y, axis=1) / tf.norm(y, axis=1))


class MyNN(tf.keras.Model):

    def __init__(self):
        super().__init__()

        self.input_dims = [1, 1]
        self.func = func
        self.optimizer = optimizer

        self.net1 = tf.keras.layers.Dense(
            **{""units"": 4, ""activation"": 'relu',
               ""kernel_initializer"": {
                   'class_name': 'glorot_uniform',
                   'config': {'seed': 1}}}
        )
        self.net2 = tf.keras.layers.Dense(
            **{""units"": 1, ""activation"": None,
               ""kernel_initializer"": {
                   'class_name': 'glorot_uniform',
                   'config': {'seed': 1}}}
        )

    def train_one_step(self, x, y):
        with tf.GradientTape() as tape:
            x_pred = self(x, y)
            loss = loss_func(x_pred, y)
        grads = tape.gradient(loss, self.trainable_variables)

        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))
        return loss

    def train(self, start_time=time.time(), max_iter=3000):
        for it in range(max_iter):
            loss = self.train_one_step(data_x, data_y)

            print(""Iter %4d: loss %14.8e,  runtime: %8.2f""
                  % (it, loss.numpy(), time.time() - start_time))

    def call(self, x, y):
        r = y - self.func(x)
        g = self.net2(self.net1(r)) * 2e-3
        return x + g


model = MyNN()
model.train()

```
"
42888,scaling issue by using multiworkerstrategy for CPU,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): conda 
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I have written code that need to scale to multiple host having 20 cpu cores each. I have used multiworkerstrategy. We are basically focusing on distributed training on CPU ""NOT ON GPU"" in tensorflow 2.3. I am able to successfully run the code on the cluster in all the node. 

Now the problem -

I see scaling issue. I have run it on single host and 5 different host. I doesn't see any performance gain on increasing the number of host. The time it take for each epochs and whole training is same in 1 machine vs 2 , 4 or 5 machine. There is no performance gain.

Than I have left my code and used the example in tensorflow docs with the same setup.
https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras  
I see the same behavior, not scaling on the increasing host.

Could someone please help me that 
1. Is multiworkerstrategy doesnot support scaling for distributed CPU in the different hosts?

**Describe the expected behavior**

Expected behaviour is that by using multiworkerstrategy it will scale by increasing the no of hosts.
[workers.log](https://github.com/tensorflow/tensorflow/files/5161305/workers.log)


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Attaching the logs of the worker. Each worker having the same logs as explained above time consumed for using 5 host or 1 host is same. So attaching 1 logs. 
"
42887,undefined reference to `aws_mutex_init',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Loongnix OS(based on Fedora) with kernel:Linux localhost.localdomain 3.10.84-22.fc21.loongson.7.mips64el
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:2.4.0
- Python version:Python：2.7.9 Python3：3.7.9
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):3.4.1
- GCC/Compiler version (if compiling from source):gcc version 7.3.1 20180303 (Red Hat 7.3.1-6) (GCC)
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
when I build the tensorflow 2.4.0 source code on loongson platform（arch：mips64）use bazel 3.4.1.， 
**use ld：ld.bfd,    not ld.gold**

**the build failed log is as below:**
ERROR: /home/loongson/tensorflow/tensorflow-206/tensorflow/tensorflow/compiler/tf2xla/cc/BUILD:30:21: Linking of rule '//tensorflow/compiler/tf2xla/cc:ops/xla_jit_ops_gen_cc' failed (Exit 1): gcc failed: error executing command
   (cd /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/execroot/org_tensorflow && \
   exec env - \
     LD_LIBRARY_PATH=/opt/protobuf-3.9.2/lib:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib \
     PATH=/opt/protobuf-3.9.2/bin:/opt/rh/devtoolset-7/root/usr/bin:/usr/lib64/qt-3.3/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/loongson/.local/bin:/home/loongson/bin \
     PWD=/proc/self/cwd \
   /opt/rh/devtoolset-7/root/usr/bin/gcc @bazel-out/mips64-opt-exec-50AE0418/bin/tensorflow/compiler/tf2xla/cc/ops/xla_jit_ops_gen_cc-2.params)
 Execution platform: @local_execution_config_platform//:platform
 bazel-out/mips64-opt-exec-50AE0418/bin/_solib_mips64/_U_S_Stensorflow_Scompiler_Stf2xla_Scc_Cops_Sxla_Ujit_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: undefined reference to `aws_mutex_init'
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Then, I set  .bazelrc file:     build:xla --define=with_xla_support=true to false**

**then the above failed log is disapear! !!**


**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42886,TFLite: C++/Java: experimental kernel ctc_beam_search_decoder returns always buffer length=length+1,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):  3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0 
- CUDA/cuDNN version: -
- GPU model and memory: -
- NDK: android-ndk-r20

**Describe the current behavior**
All returned Java `IntBuffer` (TFLite Android) from the concrete function ( `decoder.tflite`) have an extra added byte=0 in the end of the returned `dense_decoded`, e.g. [11,11,4,7,8,0].
=> This happens only in TFLite with the tflite experimental kernel [ctc_beam_search_decoder.cc](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/experimental/kernels/ctc_beam_search_decoder.cc) returned from Java.

**Describe the expected behavior**
The returned `dense_decoded` from the concrete function ( `decoder.tflite`) should be e.g. [11,11,4,7,8].
=> If I use the `concrete function` directly in python it works as expected
=> If I use the `concrete function` exported as `decoder.tflite` and loaded directly in python it works as expected.

**Standalone code to reproduce the issue**
```bash
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout -b master 6e9d916229b5aefbdcfd33cbc4b34c9f48b5e6e1
nano .tf_configure.bazelrc
```

Bazel config .tf_configure.bazelrc:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python""
build:xla --define with_xla_support=true
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --action_env ANDROID_NDK_HOME=""CHANGE_TO_YOUR_ANDROID_NDK_HOME""
build --action_env ANDROID_NDK_API_LEVEL=""21""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""28.0.0""
build --action_env ANDROID_SDK_API_LEVEL=""23""
build --action_env ANDROID_SDK_HOME=""CHANGE_TO_YOUR_ANDROID_SDK_HOME""
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu
test --build_tag_filters=-gpu
build --action_env TF_CONFIGURE_IOS=""0""
```
And compile with 
```bash
bazel build --cxxopt='--std=c++14' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a --config=monolithic \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  //tensorflow/lite/java:tensorflow-lite \
  //tensorflow/lite/java:tensorflow-lite-gpu \
  //tensorflow/lite/delegates/flex:delegate \
  //tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op \
  //tmp:tensorflow-lite-select-tf-ops
```

Concrete function exported to `decoder.tflite`:
```python
@tf.function
def decode(logits, top_paths=3, beam_width=3):
    batch_size_current, timesteps, _ = tf.shape(input=logits)
    seq_len = tf.fill([batch_size_current], timesteps)
    logits = tf.transpose(a=logits, perm=(1, 0, 2))

    decoded, log_probabilities = tf.nn.ctc_beam_search_decoder(inputs=logits, top_paths=top_paths, beam_width=beam_width, sequence_length=seq_len)

    dense_decoded = tf.sparse.to_dense(decoded[0], default_value=-1)
```"
42885,Unexpected behaviour of tf.keras.activations.relu,"**System information**
TensorFlow version: v2.3.0-0-gb36436b087 2.3.0

**Describe the current behavior**
When passing `np.nan` to `tf.keras.activations.relu`, it returns 0.0. 
This only happens when using GPU. The relu activation behaves as expected under CPU (it returns `nan`).

**Describe the expected behavior**
When passing `np.nan` to `tf.keras.activations.relu`, it should return `nan`.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1FhsgStZ_3tIIC3WJrCBl3TOjtgWyn2_2?usp=sharing

![image](https://user-images.githubusercontent.com/47107572/91958933-4583ac80-ed08-11ea-95d2-854e631c114e.png)
"
42884,"imdb.load_data() is not returning meaningful sentences as I have given print like x_train[0], x_train[1] after loading the data! ","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
keras imdb.load_data() method is not returning meaningful sentence as a row after loading in x_train, y_train, x_test, y_test.

**Describe the expected behavior**
It should have an actual meaningful review sentence from the dataset.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
`from keras.datasets import imdb
row = ''
(x_train, y_train), (x_test, y_test) = imdb.load_data()
word_to_ix = imdb.get_word_index()
#print(word_to_ix['hello'])
ix_to_word = dict( (value, key) for key, value in word_to_ix.items() )
#print(ix_to_word[4822])

for i in range(len(x_train[0])):
  row += ix_to_word[x_train[0][i]] + ' '
print(row)`
[https://colab.research.google.com/gist/tawsifsazid/ef2b113e487c233f5ebd8cbddc95ef75/bug_maybe.ipynb?authuser=1](url)
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42883,Uninitialized memory access of per-channel params,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): 5a16264ba6f12883726d12d484d4cd61405ddab7
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Host

**Describe the problem**
Function in question:  tflite::PopulateConvolutionQuantizationParams() in tensorflow/lite/kernels/kernel_util.cc (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/kernel_util.cc#L138)
Operators in question: conv and depthwise_conv

Pointer arguments 'per_channel_multiplier' and 'per_channel_shift' are accessed and written to in all cases. 
In the non-int<8,16> case, these arguments can be NULL pointers or uninitialized pointers. The reason it doesn't
crash now for reference kernels is because memory is allocated for per-channel quant parameters irrespective
of the quantization type. This ticket is for protecting accesses of per-channel params in PopulateConvolutionQuantizationParams().

Once that is done, memory usage for non per-channel cases can be reduced for TFLu(and TFL) as an improvement.


**Please provide the exact sequence of commands/steps when you ran into the problem**
_Simple Step:_
Simplest way is to run the unit test for conv or depthwise_conv and see that per-channel arguments are accessed and 
updated in the non-per channel case. 

./tensorflow/lite/micro/tools/make/gen/linux_x86_64/bin/kernel_depthwise_conv_test

_How it was discovered:_
Since it is now possible to dynamically allocate per-channel params in cmsis-nn/<op>.cc (Thanks to https://github.com/tensorflow/tensorflow/commit/59d177d9acabe8e70bc33e554a364d2620bc6999)
the conv.cc and depthwise_conv.cc in cmsis-nn folder was updated based on PR https://github.com/tensorflow/tensorflow/pull/42770 with some additional
correction to not allocate per-channel params for uint8 operators. This led to a crash.
"
42882,tensorflow lite div quantized problem,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):macOS 10.14.4
- TensorFlow installed from (source or binary):binary(pip install tensorflow==2.3.0)
- TensorFlow version (or github SHA if from source):2.3.0

I want to quantify div operations, but I couldn't get a model with quant div Op.

The test code is as follows:
#############################################
import tensorflow.compat.v1 as tf
import numpy as np


tf.disable_eager_execution()

data1 = np.random.rand(3).astype(np.float32)
beginInput1 = tf.placeholder(tf.float32,shape=[3])

data2 = np.random.rand(3).astype(np.float32)
beginInput2 = tf.placeholder(tf.float32,shape=[3])

y = tf.constant([4.0, 6.0, 2.0])

result = tf.div(beginInput1,y)
result = tf.quantization.fake_quant_with_min_max_args(result, 1, 2,name=""quant"")

init_op = tf.initialize_all_variables()

def representative_data_gen():
    yield [data1]

with tf.Session() as sess:
    sess.run(init_op)
    end = sess.run(result,feed_dict={beginInput1:data1,beginInput2:data2})

    output_graph_def = tf.graph_util.convert_variables_to_constants(sess,
                                                                    sess.graph_def,
                                                                    output_node_names=[""quant""])

    with tf.gfile.FastGFile(""../model/model.pb"", mode=""wb"") as f:
        f.write(output_graph_def.SerializeToString())

    converter = tf.lite.TFLiteConverter.from_frozen_graph(""../model/model.pb"",
                                                        input_arrays=[""Placeholder""],
                                                        output_arrays=[""quant""],
                                                        input_shapes={
                                                            ""Placeholder"": [3],
                                                        })

    converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
    input_arrays = converter.get_input_arrays()
    converter.quantized_input_stats = {input_arrays[0]: (0., 1.)}

    print(""converter begin."")
    tflite_model = converter.convert()
    open(""../model/model.tflite"", ""wb"").write(tflite_model)
#############################################

**Failure details**
![image](https://user-images.githubusercontent.com/16719562/91945888-b095a900-ed31-11ea-9bae-80046ab5422e.png)

You can see that tensorflow lite has supported quant div op.
https://github.com/tensorflow/tensorflow/blob/r2.3/tensorflow/lite/toco/graph_transformations/quantize.cc#L56

How to convert a model containing quant div op?


"
42881,[tf1.15] Low GPU usage with multi gpus and multi losses  ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): build from source according to official documents. branch is r1.15
- TensorFlow version (use command below): 1.15
- Python version: 2.7
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10.1 / 7.6.2
- GPU model and memory: v100, 16G

**Describe the current behavior**
I trained the model belowing with 2 v100 GPUs, each of which has 16GB memory, and get
Low GPU usage, average is 20 percent.

I tried batch size 384 on single GPU, the usage does not go any higher. I think it means that the low usage of GPU has nothing to do with batch size.

And I tried on colab, [results](https://colab.research.google.com/drive/13N7ukulsc06WWf8B079jQIcEpQ7zUVYt?usp=sharing). Although I can't get the GPU usage while training, but the time by each step is the same with my local experiments.

**Describe the expected behavior**
High GPU usage and faster training.

**Standalone code to reproduce the issue**
```
import numpy as np
import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import TensorBoard
from tensorflow.keras.layers import GRU, CuDNNGRU, Dense, Embedding, Reshape
from tensorflow.keras.optimizers import Adam

batch_size = 256
time_steps = 750

# datasets with almost no time delay
np_input = np.random.random((batch_size, time_steps, 512))
np_output = np.random.randint(0, 32, size=(batch_size, time_steps, 8))
def generate():
  while True:
    yield np_input, np_output
output_shapes = ((batch_size, time_steps, 512), (batch_size, time_steps, 8))


# split the output to 8 parts, calculate loss with 8 labels
def loss(y_true, y_pred):
  y_true = tf.split(y_true, num_or_size_splits=8, axis=-1)
  y_pred = tf.split(y_pred, num_or_size_splits=8, axis=-1)
  loss_func = lambda true, pred: tf.keras.losses.sparse_categorical_crossentropy(
      true, pred, from_logits=True)
  return tf.reduce_mean(
      tf.add_n([loss_func(y_true[i], y_pred[i]) for i in range(8)]))


# two v100 gpus, each of which has 16GB of memory
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  # get dataset
  dataset = tf.data.Dataset.from_generator(generate,
                                           output_types=(tf.float32, tf.int32),
                                           output_shapes=output_shapes)
  dataset = dataset.prefetch(2)

  # create model
  inputs = tf.keras.Input(shape=(None, 512))
  rnn_outputs, rnn_state = CuDNNGRU(384,
                                    return_sequences=True,
                                    return_state=True)(inputs)
  h_1, h_2 = tf.split(rnn_outputs, num_or_size_splits=2, axis=-1)
  logits_1 = Dense(128)(Dense(256, activation='relu')(h_1))
  logits_2 = Dense(128)(Dense(256, activation='relu')(h_2))
  outputs = K.concatenate([logits_1, logits_2], axis=-1)
  model = tf.keras.Model(inputs, outputs)

  tensorboard = TensorBoard(log_dir='/workspace/exp/tf_log',
                            update_freq=500,
                            profile_batch=0)
  optimizer = Adam(lr=1e-4, clipnorm=1.)
  model.compile(optimizer=optimizer, loss=loss)

# train
model.fit(dataset, epochs=4, steps_per_epoch=1000, callbacks=[tensorboard])

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

I split the outputs of CudnnGRU to 2 parts, and split the final logits to 8 parts to calculate loss. Is there any possible problems in my code ? If any, how can I improve the gpu usage ?

"
42880,weight decay (regularization) does not work with add_loss() function,"**System information**
- OS Platform: Ubuntu 16.04
- TensorFlow installed from: conda
- TensorFlow version: 2.2.0
- Python version: 3.7
- CUDA/cuDNN version: 10.1
- GPU model and memory:  nvidia 1080

**Describe the current behavior**
I tried to add regularization loss to a model with model.add_loss() function. Normally, if I set the regularization term large enough, the regularization should damage training and give a poor result. But regularization does not affect training at all, the following code shows a toy example
```
epochs = 18
reg = 10 # set a very large regularization and see if it works
(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()
x_train = np.expand_dims(x_train, axis=-1) / 255.0
x_test = np.expand_dims(x_test, axis=-1) / 255.0
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])
for w in model.trainable_weights:
    if 'bias' in w.name:
        continue
    model.add_loss(lambda: tf.nn.l2_loss(w) * reg )
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])
model.fit(x_train, y_train, epochs=epochs)
model.evaluate(x_test, y_test)
```

**Describe the expected behavior**
I replace the model with the following code, and it works properly:
```
model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
  tf.keras.layers.Dense(128, activation='relu',  kernel_regularizer=l2(reg)),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax', kernel_regularizer=l2(reg))
])
```
"
42879,NMS support in XLA,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): na
- CUDA/cuDNN version: na
- GPU model and memory: na

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I saw the [NMS implementation in tf2xla](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/tf2xla/kernels/image_ops.cc#L399), but the op has not been registered yet via REGISTER_XLA_OP. I am wondering what is the status of nms support in xla, since it is a data dependent dynamic op, would love to see how dynamic output is handled by XLA.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import numpy as np
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

# Non-Maximum Suppression
@tf.function(experimental_compile=True)
def test_nms():
    box_shape = (20, 4)
    score_shape = (20, )
    iou_threshold = 0.7
    score_threshold = 0.5
    out_size = 10
    dtype=""float32""

    boxes = np.random.uniform(0, 10, size=box_shape).astype(dtype)
    scores = np.random.uniform(size=score_shape).astype(dtype)
    max_output_size = np.int32(out_size)
    in_data_1 = tf.constant(boxes, dtype, boxes.shape, name=""in_data_1"")
    in_data_2 = tf.constant(scores, dtype, scores.shape, name=""in_data_2"")
    in_data_3 = tf.constant(max_output_size, ""int32"", name=""in_data_3"")
    res = tf.image.non_max_suppression(boxes=in_data_1, scores=in_data_2, max_output_size=in_data_3,
                                       iou_threshold=iou_threshold, score_threshold=score_threshold, name=""nms"")
    return res

test_nms()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

error message
```
nms/NonMaxSuppressionV3: unsupported op: No registered 'NonMaxSuppressionV3' OpKernel for XLA_CPU_JIT devices compatible with node {{node nms/NonMaxSuppressionV3}}
	Stacktrace:
		Node: __inference_test_nms_10, function: 
		Node: nms/NonMaxSuppressionV3, function: __inference_test_nms_10
 [Op:__inference_test_nms_10]
```"
42878,TensorArray issues in XLA,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3 (exists in 1.5 as well)
- Python version: 3.7.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
I know it was documented in xla official doc that xla doesn't support TensorArray with `dynamic_size=True`, however, I still ran into issues even w/ `dynamic_size=False`(it is default).

**Describe the expected behavior**
expect the tensorarray related stuffs are supported.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
import tensorflow as tf
tf.compat.v1.enable_eager_execution()

@tf.function(experimental_compile=True)
def test_tensor_array_scatter_gather():
    dtype = ""float32""
    t = tf.constant(np.array([[1.0], [2.0], [3.0]]).astype(dtype))
    scatter_indices = tf.constant([2, 1, 0])
    gather_indices = tf.constant([1, 2])
    ta1 = tf.TensorArray(dtype=dtype, size=3, infer_shape=True)
    ta2 = ta1.scatter(scatter_indices, t)
    t1 = ta2.gather(gather_indices)
    return t1

test_tensor_array_scatter_gather()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42877,tf.keras.Model.compile unexpectedly recovered old optimizer iterations,"
**System information**
- TensorFlow version (use command below):2.4 nightly
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11
- GPU model and memory: Titan RTX

After I set a new optimizer in tf.keras.Model.compile. The ""iterations"" become previous trained iterations"
42876,control flow support issue in TensorFlow XLA,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary (pip install tensorflow)
- TensorFlow version (use command below): 2.3 (issue exists in 1.5 as well)
- Python version: 3.7.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Seems control flow is supported in xla, but I ran into issues with some test cases

**Describe the expected behavior**
the graph should be supported by xla

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import numpy as np
import tensorflow as tf
from tensorflow.python.ops import control_flow_ops
tf.compat.v1.enable_eager_execution()
@tf.function(experimental_compile=True)
def test_switch():
    data_np = np.random.uniform(0, 5, size=(2, 4, 5, 1)).astype('float32')
    data = tf.constant(data_np)
    split = tf.split(data, 2, axis=0)
    flag = False
    output_false, output_true = control_flow_ops.switch(split[1], flag)
    return output_false

test_switch()
```



**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The following is the error message I got when tried to run with xla.
```
Switch: unsupported op: No registered 'Switch' OpKernel for XLA_CPU_JIT devices compatible with node {{node Switch}}
	Stacktrace:
		Node: __inference_test_switch_12, function: 
		Node: Switch, function: __inference_test_switch_12
 [Op:__inference_test_switch_12]
```"
42875,Tensorflow model dump in c++ during inference,"Tensorflow 1.15/2.x

For debugging process, we hit nan score issue during c++ inference (saved model). i would like to know which intermediate node having issue.  I try to search the official documentation and google, there is no way to dump all tensors in the graph in c++ side during inference.

**Will this change the current api? How?**
No, I imagine we can provide an option in the run options, to serialize all tensors after inference into string and put it into RunMetadata.

**Who will benefit with this feature?**
Everyone?  :)

**Any Other info.**
If we already have some ways to dump the all graph tensors in c++ side during inference, please let me know. Very appreciate it!
"
42873,TFLite conversion produces model with a different input shape despite providing input parameter,"**System information**
- OS Platform and Distribution: `Windows 10`
- TensorFlow installed from: `Binary`
- TensorFlow version: `2.4.0-dev20200901`


**Command used to run the converter or code if you’re using the Python API**

The original model is an SSD object detection model trained using ResNet50 as the base model.  TensorFlow core model operates as expected but lite model rejects the same input image.

**Python API Conversion:**
```python
import tensorflow as tf
saved_model_dir = r""C:\Users\alfarok\Desktop\tflite_graph\saved_model""
output_file_dir = r""C:\Users\alfarok\Desktop\output""
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir, signature_keys=['serving_default'])

converter.input_shape=(None,640,640,3)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]

tflite_model = converter.convert()
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
    f.write(tflite_model)
print(f""{output_file_dir} written"")
```

**The output from the converter invocation**

```
C:\Users\alfarok\Desktop\output written
```

**Also, please include a link to the saved model or GraphDef**

```
# Can be sent via email if required
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Input shape that was specified is not preserved
- `converter.input_shape=(None,640,640,3)` produces `'shape': array([1, 1, 1, 3]), 'shape_signature': array([ 1, -1, -1,  3])`
- Attempting to invoke the model `ValueError: Cannot set tensor: Dimension mismatch. Got 640 but expected 1 for dimension 1 of input 0.`

Details:
```python
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()
    
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

print(input_details)
```

```
[{'name': 'serving_default_input_tensor:0', 'index': 0, 'shape': array([1, 1, 1, 3]), 'shape_signature': array([ 1, -1, -1,  3]), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
```

```python
print(output_details)
```

```
[{'name': 'StatefulPartitionedCall:5', 'index': 740, 'shape': array([1]), 'shape_signature': array([1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:1', 'index': 836, 'shape': array([1, 1, 1]), 'shape_signature': array([ 1, -1, -1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:2', 'index': 814, 'shape': array([1, 1]), 'shape_signature': array([ 1, -1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:7', 'index': 452, 'shape': array([    1, 85250,    12]), 'shape_signature': array([    1, 85250,    12]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:4', 'index': 757, 'shape': array([1, 1]), 'shape_signature': array([ 1, -1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:6', 'index': 436, 'shape': array([    1, 85250,     4]), 'shape_signature': array([    1, 85250,     4]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:3', 'index': 797, 'shape': array([1, 1, 1]), 'shape_signature': array([ 1, -1, -1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}, {'name': 'StatefulPartitionedCall:0', 'index': 775, 'shape': array([1, 1]), 'shape_signature': array([ 1, -1]), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
```

```python
# Get expected width and height from models input dimensions
height = input_details[0]['shape'][1]
width = input_details[0]['shape'][2]
print(height, width)
```
```
1 1
```

**Any other info / logs**

I also tried to use the CLI such as but the output file is never produced.  It runs for a while and looks like it completes but I cannot find the `.tflite` model in the expected directory?
```
tflite_convert --saved_model_dir=inference_graph_ssd_resnet50\saved_model --output_file=inference_graph_ssd_resnet50.tflite --output_format=TFLITE --input_shapes=None,640,640,3 
```

```
Runs for a while eventually producing this (let me know if you need the entire log):
  %0 = ""tfl.cast""(%cst_228) : (tensor<1x85250xi32>) -> tensor<1x85250xf32>
  %1 = ""tfl.unpack""(%0) {axis = 0 : i32, num = 1 : i32} : (tensor<1x85250xf32>) -> tensor<85250xf32>
  %2 = ""tfl.slice""(%1, %cst_258, %cst_24) : (tensor<85250xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<85250xf32>
  %3 = ""tfl.cast""(%arg0) : (tensor<1x?x?x3xui8>) -> tensor<1x?x?x3xf32>
  %4 = ""tfl.sub""(%3, %cst_34) {fused_activation_function = ""NONE""} : (tensor<1x?x?x3xf32>, tensor<1x1x3xf32>) -> tensor<1x?x?x3xf32>
  %5 = ""tfl.unpack""(%4) {axis = 0 : i32, num = 1 : i32} : (tensor<1x?x?x3xf32>) -> tensor<?x?x3xf32>
  %6 = ""tfl.expand_dims""(%5, %cst_32) : (tensor<?x?x3xf32>, tensor<i32>) -> tensor<1x?x?x3xf32>
  %7 = ""tfl.resize_bilinear""(%6, %cst_33) {align_corners = false, half_pixel_centers = false} : (tensor<1x?x?x3xf32>, tensor<2xi32>) -> tensor<1x640x640x3xf32>
  %8 = ""tfl.reshape""(%7, %cst_229) : (tensor<1x640x640x3xf32>, tensor<3xi32>) -> tensor<640x640x3xf32>
  %9 = ""tfl.pack""(%8) {axis = 0 : i32, values_count = 1 : i32} : (tensor<640x640x3xf32>) -> tensor<1x640x640x3xf32>
  %10 = ""tfl.pad""(%9, %cst_37) : (tensor<1x640x640x3xf32>, tensor<4x2xi32>) -> tensor<1x646x646x3xf32>
  %11 = ""tfl.conv_2d""(%10, %cst_126, %cst_49) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x646x646x3xf32>, tensor<64x7x7x3xf32>, tensor<64xf32>) -> tensor<1x320x320x64xf32>
  %12 = ""tfl.max_pool_2d""(%11) {filter_height = 3 : i32, filter_width = 3 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x320x320x64xf32>) -> tensor<1x160x160x64xf32>
  %13 = ""tfl.conv_2d""(%12, %cst_127, %cst_230) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %14 = ""tfl.conv_2d""(%12, %cst_128, %cst_50) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x1x1x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %15 = ""tfl.conv_2d""(%14, %cst_129, %cst_51) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %16 = ""tfl.conv_2d""(%15, %cst_130, %cst_231) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %17 = ""tfl.add""(%13, %16) {fused_activation_function = ""RELU6""} : (tensor<1x160x160x256xf32>, tensor<1x160x160x256xf32>) -> tensor<1x160x160x256xf32>
  %18 = ""tfl.conv_2d""(%17, %cst_131, %cst_52) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x256xf32>, tensor<64x1x1x256xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %19 = ""tfl.conv_2d""(%18, %cst_132, %cst_53) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %20 = ""tfl.conv_2d""(%19, %cst_133, %cst_232) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %21 = ""tfl.add""(%17, %20) {fused_activation_function = ""RELU6""} : (tensor<1x160x160x256xf32>, tensor<1x160x160x256xf32>) -> tensor<1x160x160x256xf32>
  %22 = ""tfl.conv_2d""(%21, %cst_134, %cst_54) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x256xf32>, tensor<64x1x1x256xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %23 = ""tfl.conv_2d""(%22, %cst_135, %cst_55) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %24 = ""tfl.conv_2d""(%23, %cst_136, %cst_233) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %25 = ""tfl.add""(%21, %24) {fused_activation_function = ""RELU6""} : (tensor<1x160x160x256xf32>, tensor<1x160x160x256xf32>) -> tensor<1x160x160x256xf32>
  %26 = ""tfl.conv_2d""(%25, %cst_137, %cst_234) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x160x160x256xf32>, tensor<512x1x1x256xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %27 = ""tfl.conv_2d""(%25, %cst_138, %cst_56) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x160x160x256xf32>, tensor<128x1x1x256xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %28 = ""tfl.conv_2d""(%27, %cst_139, %cst_57) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %29 = ""tfl.conv_2d""(%28, %cst_140, %cst_235) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %30 = ""tfl.add""(%26, %29) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %31 = ""tfl.conv_2d""(%30, %cst_141, %cst_58) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<128x1x1x512xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %32 = ""tfl.conv_2d""(%31, %cst_142, %cst_59) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %33 = ""tfl.conv_2d""(%32, %cst_143, %cst_236) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %34 = ""tfl.add""(%30, %33) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %35 = ""tfl.conv_2d""(%34, %cst_144, %cst_60) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<128x1x1x512xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %36 = ""tfl.conv_2d""(%35, %cst_145, %cst_61) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %37 = ""tfl.conv_2d""(%36, %cst_146, %cst_237) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %38 = ""tfl.add""(%34, %37) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %39 = ""tfl.conv_2d""(%38, %cst_147, %cst_62) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<128x1x1x512xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %40 = ""tfl.conv_2d""(%39, %cst_148, %cst_63) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %41 = ""tfl.conv_2d""(%40, %cst_149, %cst_238) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %42 = ""tfl.add""(%38, %41) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %43 = ""tfl.conv_2d""(%42, %cst_150, %cst_239) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<256x1x1x512xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %44 = ""tfl.conv_2d""(%42, %cst_151, %cst_240) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x80x80x512xf32>, tensor<1024x1x1x512xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %45 = ""tfl.conv_2d""(%42, %cst_152, %cst_64) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x80x80x512xf32>, tensor<256x1x1x512xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %46 = ""tfl.conv_2d""(%45, %cst_153, %cst_65) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %47 = ""tfl.conv_2d""(%46, %cst_154, %cst_241) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %48 = ""tfl.add""(%44, %47) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %49 = ""tfl.conv_2d""(%48, %cst_155, %cst_66) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %50 = ""tfl.conv_2d""(%49, %cst_156, %cst_67) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %51 = ""tfl.conv_2d""(%50, %cst_157, %cst_242) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %52 = ""tfl.add""(%48, %51) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %53 = ""tfl.conv_2d""(%52, %cst_158, %cst_68) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %54 = ""tfl.conv_2d""(%53, %cst_159, %cst_69) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %55 = ""tfl.conv_2d""(%54, %cst_160, %cst_243) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %56 = ""tfl.add""(%52, %55) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %57 = ""tfl.conv_2d""(%56, %cst_161, %cst_70) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %58 = ""tfl.conv_2d""(%57, %cst_162, %cst_71) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %59 = ""tfl.conv_2d""(%58, %cst_163, %cst_244) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %60 = ""tfl.add""(%56, %59) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %61 = ""tfl.conv_2d""(%60, %cst_164, %cst_72) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %62 = ""tfl.conv_2d""(%61, %cst_165, %cst_73) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %63 = ""tfl.conv_2d""(%62, %cst_166, %cst_245) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %64 = ""tfl.add""(%60, %63) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %65 = ""tfl.conv_2d""(%64, %cst_167, %cst_74) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %66 = ""tfl.conv_2d""(%65, %cst_168, %cst_75) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %67 = ""tfl.conv_2d""(%66, %cst_169, %cst_246) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %68 = ""tfl.add""(%64, %67) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %69 = ""tfl.conv_2d""(%68, %cst_170, %cst_247) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %70 = ""tfl.conv_2d""(%68, %cst_171, %cst_248) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x40x40x1024xf32>, tensor<2048x1x1x1024xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %71 = ""tfl.conv_2d""(%68, %cst_172, %cst_76) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x40x40x1024xf32>, tensor<512x1x1x1024xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %72 = ""tfl.conv_2d""(%71, %cst_173, %cst_77) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %73 = ""tfl.conv_2d""(%72, %cst_174, %cst_249) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<2048x1x1x512xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %74 = ""tfl.add""(%70, %73) {fused_activation_function = ""RELU6""} : (tensor<1x20x20x2048xf32>, tensor<1x20x20x2048xf32>) -> tensor<1x20x20x2048xf32>
  %75 = ""tfl.conv_2d""(%74, %cst_175, %cst_78) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x2048xf32>, tensor<512x1x1x2048xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %76 = ""tfl.conv_2d""(%75, %cst_176, %cst_79) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %77 = ""tfl.conv_2d""(%76, %cst_177, %cst_250) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<2048x1x1x512xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %78 = ""tfl.add""(%74, %77) {fused_activation_function = ""RELU6""} : (tensor<1x20x20x2048xf32>, tensor<1x20x20x2048xf32>) -> tensor<1x20x20x2048xf32>
  %79 = ""tfl.conv_2d""(%78, %cst_178, %cst_80) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x2048xf32>, tensor<512x1x1x2048xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %80 = ""tfl.conv_2d""(%79, %cst_179, %cst_81) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %81 = ""tfl.conv_2d""(%80, %cst_180, %cst_251) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<2048x1x1x512xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %82 = ""tfl.add""(%78, %81) {fused_activation_function = ""RELU6""} : (tensor<1x20x20x2048xf32>, tensor<1x20x20x2048xf32>) -> tensor<1x20x20x2048xf32>
  %83 = ""tfl.conv_2d""(%82, %cst_181, %cst_252) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x2048xf32>, tensor<256x1x1x2048xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %84 = ""tfl.pack""(%83, %83) {axis = 3 : i32, values_count = 2 : i32} : (tensor<1x20x20x256xf32>, tensor<1x20x20x256xf32>) -> tensor<1x20x20x2x256xf32>
  %85 = ""tfl.pack""(%84, %84) {axis = 2 : i32, values_count = 2 : i32} : (tensor<1x20x20x2x256xf32>, tensor<1x20x20x2x256xf32>) -> tensor<1x20x2x20x2x256xf32>
  %86 = ""tfl.reshape""(%85, %cst_35) : (tensor<1x20x2x20x2x256xf32>, tensor<4xi32>) -> tensor<1x40x40x256xf32>
  %87 = ""tfl.add""(%86, %69) {fused_activation_function = ""NONE""} : (tensor<1x40x40x256xf32>, tensor<1x40x40x256xf32>) -> tensor<1x40x40x256xf32>
  %88 = ""tfl.conv_2d""(%87, %cst_182, %cst_82) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %89 = ""tfl.pack""(%88, %88) {axis = 3 : i32, values_count = 2 : i32} : (tensor<1x40x40x256xf32>, tensor<1x40x40x256xf32>) -> tensor<1x40x40x2x256xf32>
  %90 = ""tfl.pack""(%89, %89) {axis = 2 : i32, values_count = 2 : i32} : (tensor<1x40x40x2x256xf32>, tensor<1x40x40x2x256xf32>) -> tensor<1x40x2x40x2x256xf32>
  %91 = ""tfl.reshape""(%90, %cst_36) : (tensor<1x40x2x40x2x256xf32>, tensor<4xi32>) -> tensor<1x80x80x256xf32>
  %92 = ""tfl.add""(%91, %43) {fused_activation_function = ""NONE""} : (tensor<1x80x80x256xf32>, tensor<1x80x80x256xf32>) -> tensor<1x80x80x256xf32>
  %93 = ""tfl.conv_2d""(%92, %cst_183, %cst_83) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %94 = ""tfl.conv_2d""(%83, %cst_184, %cst_84) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %95 = ""tfl.conv_2d""(%94, %cst_185, %cst_85) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %96 = ""tfl.conv_2d""(%93, %cst_186, %cst_86) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %97 = ""tfl.conv_2d""(%88, %cst_187, %cst_87) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %98 = ""tfl.conv_2d""(%83, %cst_188, %cst_88) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %99 = ""tfl.conv_2d""(%94, %cst_189, %cst_89) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %100 = ""tfl.conv_2d""(%95, %cst_190, %cst_90) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %101 = ""tfl.conv_2d""(%96, %cst_191, %cst_91) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %102 = ""tfl.conv_2d""(%97, %cst_192, %cst_92) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %103 = ""tfl.conv_2d""(%98, %cst_193, %cst_93) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %104 = ""tfl.conv_2d""(%99, %cst_194, %cst_94) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %105 = ""tfl.conv_2d""(%100, %cst_195, %cst_95) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %106 = ""tfl.conv_2d""(%101, %cst_196, %cst_96) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %107 = ""tfl.conv_2d""(%102, %cst_197, %cst_97) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %108 = ""tfl.conv_2d""(%103, %cst_198, %cst_98) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %109 = ""tfl.conv_2d""(%104, %cst_199, %cst_99) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %110 = ""tfl.conv_2d""(%105, %cst_200, %cst_100) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %111 = ""tfl.conv_2d""(%106, %cst_201, %cst_101) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %112 = ""tfl.conv_2d""(%107, %cst_202, %cst_102) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %113 = ""tfl.conv_2d""(%108, %cst_203, %cst_103) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %114 = ""tfl.conv_2d""(%109, %cst_204, %cst_104) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %115 = ""tfl.conv_2d""(%110, %cst_205, %cst_105) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %116 = ""tfl.conv_2d""(%93, %cst_206, %cst_106) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %117 = ""tfl.conv_2d""(%88, %cst_207, %cst_107) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %118 = ""tfl.conv_2d""(%83, %cst_208, %cst_108) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %119 = ""tfl.conv_2d""(%94, %cst_209, %cst_109) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %120 = ""tfl.conv_2d""(%95, %cst_210, %cst_110) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %121 = ""tfl.conv_2d""(%116, %cst_211, %cst_111) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %122 = ""tfl.conv_2d""(%117, %cst_212, %cst_112) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %123 = ""tfl.conv_2d""(%118, %cst_213, %cst_113) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %124 = ""tfl.conv_2d""(%119, %cst_214, %cst_114) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %125 = ""tfl.conv_2d""(%120, %cst_215, %cst_115) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %126 = ""tfl.conv_2d""(%121, %cst_216, %cst_116) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %127 = ""tfl.conv_2d""(%122, %cst_217, %cst_117) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %128 = ""tfl.conv_2d""(%123, %cst_218, %cst_118) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %129 = ""tfl.conv_2d""(%124, %cst_219, %cst_119) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %130 = ""tfl.conv_2d""(%125, %cst_220, %cst_120) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %131 = ""tfl.conv_2d""(%126, %cst_221, %cst_121) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %132 = ""tfl.conv_2d""(%127, %cst_222, %cst_122) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %133 = ""tfl.conv_2d""(%128, %cst_223, %cst_123) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %134 = ""tfl.conv_2d""(%129, %cst_224, %cst_124) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %135 = ""tfl.conv_2d""(%130, %cst_225, %cst_125) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %136 = ""tfl.conv_2d""(%111, %cst_226, %cst_253) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<40x3x3x256xf32>, tensor<40xf32>) -> tensor<1x80x80x40xf32>
  %137 = ""tfl.reshape""(%136, %cst_38) : (tensor<1x80x80x40xf32>, tensor<3xi32>) -> tensor<1x64000x4xf32>
  %138 = ""tfl.conv_2d""(%112, %cst_226, %cst_253) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<40x3x3x256xf32>, tensor<40xf32>) -> tensor<1x40x40x40xf32>
  %139 = ""tfl.reshape""(%138, %cst_38) : (tensor<1x40x40x40xf32>, tensor<3xi32>) -> tensor<1x16000x4xf32>
  %140 = ""tfl.conv_2d""(%113, %cst_226, %cst_253) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<40x3x3x256xf32>, tensor<40xf32>) -> tensor<1x20x20x40xf32>
  %141 = ""tfl.reshape""(%140, %cst_38) : (tensor<1x20x20x40xf32>, tensor<3xi32>) -> tensor<1x4000x4xf32>
  %142 = ""tfl.conv_2d""(%114, %cst_226, %cst_253) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<40x3x3x256xf32>, tensor<40xf32>) -> tensor<1x10x10x40xf32>
  %143 = ""tfl.reshape""(%142, %cst_38) : (tensor<1x10x10x40xf32>, tensor<3xi32>) -> tensor<1x1000x4xf32>
  %144 = ""tfl.conv_2d""(%115, %cst_226, %cst_253) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<40x3x3x256xf32>, tensor<40xf32>) -> tensor<1x5x5x40xf32>
  %145 = ""tfl.reshape""(%144, %cst_38) : (tensor<1x5x5x40xf32>, tensor<3xi32>) -> tensor<1x250x4xf32>
  %146 = ""tfl.concatenation""(%137, %139, %141, %143, %145) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<1x64000x4xf32>, tensor<1x16000x4xf32>, tensor<1x4000x4xf32>, tensor<1x1000x4xf32>, tensor<1x250x4xf32>) -> tensor<1x85250x4xf32>
  %147 = ""tfl.reshape""(%146, %cst_29) : (tensor<1x85250x4xf32>, tensor<2xi32>) -> tensor<85250x4xf32>
  %148 = ""tfl.transpose""(%147, %cst_28) : (tensor<85250x4xf32>, tensor<2xi32>) -> tensor<4x85250xf32>
  %149:4 = ""tfl.unpack""(%148) {axis = 0 : i32, num = 4 : i32} : (tensor<4x85250xf32>) -> (tensor<85250xf32>, tensor<85250xf32>, tensor<85250xf32>, tensor<85250xf32>)
  %150 = ""tfl.mul""(%149#0, %cst_42) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<f32>) -> tensor<85250xf32>
  %151 = ""tfl.mul""(%150, %cst_45) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %152 = ""tfl.add""(%151, %cst_46) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %153 = ""tfl.mul""(%149#1, %cst_42) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<f32>) -> tensor<85250xf32>
  %154 = ""tfl.mul""(%153, %cst_47) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %155 = ""tfl.add""(%154, %cst_48) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %156 = ""tfl.mul""(%149#2, %cst_43) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<f32>) -> tensor<85250xf32>
  %157 = ""tfl.exp""(%156) : (tensor<85250xf32>) -> tensor<85250xf32>
  %158 = ""tfl.mul""(%157, %cst_45) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %159 = ""tfl.mul""(%158, %cst_44) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<f32>) -> tensor<85250xf32>
  %160 = ""tfl.sub""(%152, %159) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %161 = ""tfl.add""(%152, %159) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %162 = ""tfl.mul""(%149#3, %cst_43) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<f32>) -> tensor<85250xf32>
  %163 = ""tfl.exp""(%162) : (tensor<85250xf32>) -> tensor<85250xf32>
  %164 = ""tfl.mul""(%163, %cst_47) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %165 = ""tfl.mul""(%164, %cst_44) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<f32>) -> tensor<85250xf32>
  %166 = ""tfl.sub""(%155, %165) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %167 = ""tfl.add""(%155, %165) {fused_activation_function = ""NONE""} : (tensor<85250xf32>, tensor<85250xf32>) -> tensor<85250xf32>
  %168 = ""tfl.pack""(%160, %166, %161, %167) {axis = 0 : i32, values_count = 4 : i32} : (tensor<85250xf32>, tensor<85250xf32>, tensor<85250xf32>, tensor<85250xf32>) -> tensor<4x85250xf32>
  %169 = ""tfl.transpose""(%168, %cst_28) : (tensor<4x85250xf32>, tensor<2xi32>) -> tensor<85250x4xf32>
  %170 = ""tfl.reshape""(%169, %cst_255) : (tensor<85250x4xf32>, tensor<3xi32>) -> tensor<1x85250x4xf32>
  %171 = ""tfl.reshape""(%169, %cst_254) : (tensor<85250x4xf32>, tensor<4xi32>) -> tensor<1x85250x1x4xf32>
  %172 = ""tfl.unpack""(%171) {axis = 0 : i32, num = 1 : i32} : (tensor<1x85250x1x4xf32>) -> tensor<85250x1x4xf32>
  %173 = ""tfl.slice""(%172, %cst_20, %cst_22) : (tensor<85250x1x4xf32>, tensor<3xi32>, tensor<3xi32>) -> tensor<85250x1x4xf32>
  %174 = ""tfl.unpack""(%173) {axis = 1 : i32, num = 1 : i32} : (tensor<85250x1x4xf32>) -> tensor<85250x4xf32>
  %175 = ""tfl.conv_2d""(%131, %cst_227, %cst_256) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<120x3x3x256xf32>, tensor<120xf32>) -> tensor<1x80x80x120xf32>
  %176 = ""tfl.reshape""(%175, %cst_39) : (tensor<1x80x80x120xf32>, tensor<3xi32>) -> tensor<1x64000x12xf32>
  %177 = ""tfl.conv_2d""(%132, %cst_227, %cst_256) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<120x3x3x256xf32>, tensor<120xf32>) -> tensor<1x40x40x120xf32>
  %178 = ""tfl.reshape""(%177, %cst_39) : (tensor<1x40x40x120xf32>, tensor<3xi32>) -> tensor<1x16000x12xf32>
  %179 = ""tfl.conv_2d""(%133, %cst_227, %cst_256) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<120x3x3x256xf32>, tensor<120xf32>) -> tensor<1x20x20x120xf32>
  %180 = ""tfl.reshape""(%179, %cst_39) : (tensor<1x20x20x120xf32>, tensor<3xi32>) -> tensor<1x4000x12xf32>
  %181 = ""tfl.conv_2d""(%134, %cst_227, %cst_256) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<120x3x3x256xf32>, tensor<120xf32>) -> tensor<1x10x10x120xf32>
  %182 = ""tfl.reshape""(%181, %cst_39) : (tensor<1x10x10x120xf32>, tensor<3xi32>) -> tensor<1x1000x12xf32>
  %183 = ""tfl.conv_2d""(%135, %cst_227, %cst_256) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<120x3x3x256xf32>, tensor<120xf32>) -> tensor<1x5x5x120xf32>
  %184 = ""tfl.reshape""(%183, %cst_39) : (tensor<1x5x5x120xf32>, tensor<3xi32>) -> tensor<1x250x12xf32>
  %185 = ""tfl.concatenation""(%176, %178, %180, %182, %184) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<1x64000x12xf32>, tensor<1x16000x12xf32>, tensor<1x4000x12xf32>, tensor<1x1000x12xf32>, tensor<1x250x12xf32>) -> tensor<1x85250x12xf32>
  %186 = ""tfl.logistic""(%185) : (tensor<1x85250x12xf32>) -> tensor<1x85250x12xf32>
  %187 = ""tfl.unpack""(%186) {axis = 0 : i32, num = 1 : i32} : (tensor<1x85250x12xf32>) -> tensor<85250x12xf32>
  %188 = ""tfl.slice""(%187, %cst_21, %cst_23) : (tensor<85250x12xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x12xf32>
  %189 = ""tfl.slice""(%186, %cst_30, %cst_31) : (tensor<1x85250x12xf32>, tensor<3xi32>, tensor<3xi32>) -> tensor<1x85250x11xf32>
  %190 = ""tfl.unpack""(%189) {axis = 0 : i32, num = 1 : i32} : (tensor<1x85250x11xf32>) -> tensor<85250x11xf32>
  %191 = ""tfl.slice""(%190, %cst_21, %cst_23) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x11xf32>
  %192 = ""tfl.slice""(%191, %cst_21, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %193 = ""tfl.reshape""(%192, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices, %selected_scores, %valid_outputs = ""tfl.non_max_suppression_v5""(%174, %193, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %194 = ""tfl.shape""(%selected_indices) : (tensor<?xi32>) -> tensor<1xi32>
  %195 = ""tfl.strided_slice""(%194, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %196 = ""tfl.less""(%cst_14, %195) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %197 = ""tfl.sub""(%cst_19, %195) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %198 = ""tfl.reshape""(%197, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %199 = ""tfl.fill""(%198, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %200 = ""tfl.fill""(%198, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %201 = ""tfl.concatenation""(%selected_indices, %199) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %202 = ""tfl.gather""(%2, %201) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %203 = ""tfl.gather""(%188, %201) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %204 = ""tfl.gather""(%174, %201) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %205 = ""tfl.concatenation""(%selected_scores, %200) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %206 = ""tfl.select""(%196, %205, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %207 = ""tfl.slice""(%191, %cst, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %208 = ""tfl.reshape""(%207, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_261, %selected_scores_262, %valid_outputs_263 = ""tfl.non_max_suppression_v5""(%174, %208, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %209 = ""tfl.shape""(%selected_indices_261) : (tensor<?xi32>) -> tensor<1xi32>
  %210 = ""tfl.strided_slice""(%209, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %211 = ""tfl.less""(%cst_14, %210) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %212 = ""tfl.sub""(%cst_19, %210) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %213 = ""tfl.reshape""(%212, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %214 = ""tfl.fill""(%213, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %215 = ""tfl.fill""(%213, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %216 = ""tfl.concatenation""(%selected_indices_261, %214) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %217 = ""tfl.gather""(%2, %216) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %218 = ""tfl.gather""(%188, %216) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %219 = ""tfl.gather""(%174, %216) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %220 = ""tfl.concatenation""(%selected_scores_262, %215) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %221 = ""tfl.select""(%211, %220, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %222 = ""tfl.slice""(%191, %cst_0, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %223 = ""tfl.reshape""(%222, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_264, %selected_scores_265, %valid_outputs_266 = ""tfl.non_max_suppression_v5""(%174, %223, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %224 = ""tfl.shape""(%selected_indices_264) : (tensor<?xi32>) -> tensor<1xi32>
  %225 = ""tfl.strided_slice""(%224, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %226 = ""tfl.less""(%cst_14, %225) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %227 = ""tfl.sub""(%cst_19, %225) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %228 = ""tfl.reshape""(%227, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %229 = ""tfl.fill""(%228, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %230 = ""tfl.fill""(%228, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %231 = ""tfl.concatenation""(%selected_indices_264, %229) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %232 = ""tfl.gather""(%2, %231) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %233 = ""tfl.gather""(%188, %231) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %234 = ""tfl.gather""(%174, %231) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %235 = ""tfl.concatenation""(%selected_scores_265, %230) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %236 = ""tfl.select""(%226, %235, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %237 = ""tfl.slice""(%191, %cst_1, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %238 = ""tfl.reshape""(%237, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_267, %selected_scores_268, %valid_outputs_269 = ""tfl.non_max_suppression_v5""(%174, %238, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %239 = ""tfl.shape""(%selected_indices_267) : (tensor<?xi32>) -> tensor<1xi32>
  %240 = ""tfl.strided_slice""(%239, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %241 = ""tfl.less""(%cst_14, %240) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %242 = ""tfl.sub""(%cst_19, %240) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %243 = ""tfl.reshape""(%242, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %244 = ""tfl.fill""(%243, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %245 = ""tfl.fill""(%243, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %246 = ""tfl.concatenation""(%selected_indices_267, %244) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %247 = ""tfl.gather""(%2, %246) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %248 = ""tfl.gather""(%188, %246) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %249 = ""tfl.gather""(%174, %246) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %250 = ""tfl.concatenation""(%selected_scores_268, %245) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %251 = ""tfl.select""(%241, %250, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %252 = ""tfl.slice""(%191, %cst_2, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %253 = ""tfl.reshape""(%252, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_270, %selected_scores_271, %valid_outputs_272 = ""tfl.non_max_suppression_v5""(%174, %253, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %254 = ""tfl.shape""(%selected_indices_270) : (tensor<?xi32>) -> tensor<1xi32>
  %255 = ""tfl.strided_slice""(%254, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %256 = ""tfl.less""(%cst_14, %255) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %257 = ""tfl.sub""(%cst_19, %255) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %258 = ""tfl.reshape""(%257, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %259 = ""tfl.fill""(%258, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %260 = ""tfl.fill""(%258, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %261 = ""tfl.concatenation""(%selected_indices_270, %259) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %262 = ""tfl.gather""(%2, %261) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %263 = ""tfl.gather""(%188, %261) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %264 = ""tfl.gather""(%174, %261) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %265 = ""tfl.concatenation""(%selected_scores_271, %260) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %266 = ""tfl.select""(%256, %265, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %267 = ""tfl.slice""(%191, %cst_3, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %268 = ""tfl.reshape""(%267, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_273, %selected_scores_274, %valid_outputs_275 = ""tfl.non_max_suppression_v5""(%174, %268, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %269 = ""tfl.shape""(%selected_indices_273) : (tensor<?xi32>) -> tensor<1xi32>
  %270 = ""tfl.strided_slice""(%269, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %271 = ""tfl.less""(%cst_14, %270) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %272 = ""tfl.sub""(%cst_19, %270) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %273 = ""tfl.reshape""(%272, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %274 = ""tfl.fill""(%273, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %275 = ""tfl.fill""(%273, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %276 = ""tfl.concatenation""(%selected_indices_273, %274) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %277 = ""tfl.gather""(%2, %276) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %278 = ""tfl.gather""(%188, %276) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %279 = ""tfl.gather""(%174, %276) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %280 = ""tfl.concatenation""(%selected_scores_274, %275) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %281 = ""tfl.select""(%271, %280, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %282 = ""tfl.slice""(%191, %cst_4, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %283 = ""tfl.reshape""(%282, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_276, %selected_scores_277, %valid_outputs_278 = ""tfl.non_max_suppression_v5""(%174, %283, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %284 = ""tfl.shape""(%selected_indices_276) : (tensor<?xi32>) -> tensor<1xi32>
  %285 = ""tfl.strided_slice""(%284, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %286 = ""tfl.less""(%cst_14, %285) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %287 = ""tfl.sub""(%cst_19, %285) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %288 = ""tfl.reshape""(%287, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %289 = ""tfl.fill""(%288, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %290 = ""tfl.fill""(%288, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %291 = ""tfl.concatenation""(%selected_indices_276, %289) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %292 = ""tfl.gather""(%2, %291) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %293 = ""tfl.gather""(%188, %291) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %294 = ""tfl.gather""(%174, %291) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %295 = ""tfl.concatenation""(%selected_scores_277, %290) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %296 = ""tfl.select""(%286, %295, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %297 = ""tfl.slice""(%191, %cst_5, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %298 = ""tfl.reshape""(%297, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_279, %selected_scores_280, %valid_outputs_281 = ""tfl.non_max_suppression_v5""(%174, %298, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %299 = ""tfl.shape""(%selected_indices_279) : (tensor<?xi32>) -> tensor<1xi32>
  %300 = ""tfl.strided_slice""(%299, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %301 = ""tfl.less""(%cst_14, %300) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %302 = ""tfl.sub""(%cst_19, %300) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %303 = ""tfl.reshape""(%302, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %304 = ""tfl.fill""(%303, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %305 = ""tfl.fill""(%303, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %306 = ""tfl.concatenation""(%selected_indices_279, %304) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %307 = ""tfl.gather""(%2, %306) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %308 = ""tfl.gather""(%188, %306) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %309 = ""tfl.gather""(%174, %306) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %310 = ""tfl.concatenation""(%selected_scores_280, %305) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %311 = ""tfl.select""(%301, %310, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %312 = ""tfl.slice""(%191, %cst_6, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %313 = ""tfl.reshape""(%312, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_282, %selected_scores_283, %valid_outputs_284 = ""tfl.non_max_suppression_v5""(%174, %313, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %314 = ""tfl.shape""(%selected_indices_282) : (tensor<?xi32>) -> tensor<1xi32>
  %315 = ""tfl.strided_slice""(%314, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %316 = ""tfl.less""(%cst_14, %315) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %317 = ""tfl.sub""(%cst_19, %315) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %318 = ""tfl.reshape""(%317, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %319 = ""tfl.fill""(%318, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %320 = ""tfl.fill""(%318, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %321 = ""tfl.concatenation""(%selected_indices_282, %319) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %322 = ""tfl.gather""(%2, %321) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %323 = ""tfl.gather""(%188, %321) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %324 = ""tfl.gather""(%174, %321) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %325 = ""tfl.concatenation""(%selected_scores_283, %320) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %326 = ""tfl.select""(%316, %325, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %327 = ""tfl.slice""(%191, %cst_7, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %328 = ""tfl.reshape""(%327, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_285, %selected_scores_286, %valid_outputs_287 = ""tfl.non_max_suppression_v5""(%174, %328, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %329 = ""tfl.shape""(%selected_indices_285) : (tensor<?xi32>) -> tensor<1xi32>
  %330 = ""tfl.strided_slice""(%329, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %331 = ""tfl.less""(%cst_14, %330) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %332 = ""tfl.sub""(%cst_19, %330) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %333 = ""tfl.reshape""(%332, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %334 = ""tfl.fill""(%333, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %335 = ""tfl.fill""(%333, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %336 = ""tfl.concatenation""(%selected_indices_285, %334) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %337 = ""tfl.gather""(%2, %336) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %338 = ""tfl.gather""(%188, %336) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %339 = ""tfl.gather""(%174, %336) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %340 = ""tfl.concatenation""(%selected_scores_286, %335) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %341 = ""tfl.select""(%331, %340, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %342 = ""tfl.slice""(%191, %cst_8, %cst_13) : (tensor<85250x11xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<85250x1xf32>
  %343 = ""tfl.reshape""(%342, %cst_257) : (tensor<85250x1xf32>, tensor<1xi32>) -> tensor<85250xf32>
  %selected_indices_288, %selected_scores_289, %valid_outputs_290 = ""tfl.non_max_suppression_v5""(%174, %343, %cst_19, %cst_11, %cst_12, %cst_27) : (tensor<85250x4xf32>, tensor<85250xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %344 = ""tfl.shape""(%selected_indices_288) : (tensor<?xi32>) -> tensor<1xi32>
  %345 = ""tfl.strided_slice""(%344, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %346 = ""tfl.less""(%cst_14, %345) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %347 = ""tfl.sub""(%cst_19, %345) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %348 = ""tfl.reshape""(%347, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %349 = ""tfl.fill""(%348, %cst_32) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %350 = ""tfl.fill""(%348, %cst_27) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %351 = ""tfl.concatenation""(%selected_indices_288, %349) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %352 = ""tfl.gather""(%2, %351) {axis = 0 : i32} : (tensor<85250xf32>, tensor<?xi32>) -> tensor<?xf32>
  %353 = ""tfl.concatenation""(%202, %217, %247, %262, %277, %292, %307, %322, %337, %352, %232) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %354 = ""tfl.gather""(%188, %351) {axis = 0 : i32} : (tensor<85250x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %355 = ""tfl.concatenation""(%203, %218, %248, %263, %278, %293, %308, %323, %338, %354, %233) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>, tensor<?x12xf32>) -> tensor<?x12xf32>
  %356 = ""tfl.gather""(%174, %351) {axis = 0 : i32} : (tensor<85250x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %357 = ""tfl.concatenation""(%204, %219, %249, %264, %279, %294, %309, %324, %339, %356, %234) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>, tensor<?x4xf32>) -> tensor<?x4xf32>
  %358 = ""tfl.shape""(%357) : (tensor<?x4xf32>) -> tensor<2xi32>
  %359 = ""tfl.strided_slice""(%358, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %360 = ""tfl.equal""(%359, %cst_9) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %361 = ""tfl.concatenation""(%selected_scores_289, %350) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %362 = ""tfl.select""(%346, %361, %cst_15) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %363 = ""tfl.concatenation""(%206, %221, %251, %266, %281, %296, %311, %326, %341, %362, %236) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>, tensor<100xf32>) -> tensor<1100xf32>
  %values, %indices = ""tfl.topk_v2""(%363, %359) : (tensor<1100xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
  %364 = ""tfl.gather""(%363, %indices) {axis = 0 : i32} : (tensor<1100xf32>, tensor<?xi32>) -> tensor<?xf32>
  %365 = ""tfl.gather""(%353, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %366 = ""tfl.gather""(%355, %indices) {axis = 0 : i32} : (tensor<?x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %367 = ""tfl.gather""(%cst_16, %indices) {axis = 0 : i32} : (tensor<1100xf32>, tensor<?xi32>) -> tensor<?xf32>
  %368 = ""tfl.gather""(%357, %indices) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %369:4 = ""tfl.split""(%cst_41, %368) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %370 = ""tfl.minimum""(%369#0, %cst_40) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %371 = ""tfl.relu""(%370) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %372 = ""tfl.minimum""(%369#2, %cst_40) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %373 = ""tfl.relu""(%372) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %374 = ""tfl.minimum""(%369#1, %cst_40) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %375 = ""tfl.relu""(%374) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %376 = ""tfl.minimum""(%369#3, %cst_40) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %377 = ""tfl.relu""(%376) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %378 = ""tfl.concatenation""(%371, %375, %373, %377) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x4xf32>
  %379:4 = ""tfl.split""(%cst_41, %378) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %380 = ""tfl.sub""(%379#2, %379#0) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %381 = ""tfl.sub""(%379#3, %379#1) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %382 = ""tfl.mul""(%380, %381) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %383 = ""tfl.greater""(%382, %cst_27) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xi1>
  %384 = ""tfl.reshape""(%383, %cst_257) : (tensor<?x1xi1>, tensor<1xi32>) -> tensor<?xi1>
  %385 = ""tfl.where""(%384) : (tensor<?xi1>) -> tensor<?x1xi64>
  %386 = ""tfl.reshape""(%385, %cst_257) : (tensor<?x1xi64>, tensor<1xi32>) -> tensor<?xi64>
  %387 = ""tfl.cast""(%386) : (tensor<?xi64>) -> tensor<?xi32>
  %388 = ""tfl.gather""(%364, %387) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %389 = ""tfl.gather""(%365, %387) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %390 = ""tfl.gather""(%366, %387) {axis = 0 : i32} : (tensor<?x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %391 = ""tfl.gather""(%367, %387) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %392 = ""tfl.gather""(%378, %387) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %393:4 = ""tfl.split""(%cst_41, %392) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %394 = ""tfl.sub""(%393#2, %393#0) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %395 = ""tfl.sub""(%393#3, %393#1) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %396 = ""tfl.mul""(%394, %395) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %397 = ""tfl.reshape""(%396, %cst_257) : (tensor<?x1xf32>, tensor<1xi32>) -> tensor<?xf32>
  %398 = ""tfl.cast""(%397) : (tensor<?xf32>) -> tensor<?xi1>
  %399 = ""tfl.shape""(%392) : (tensor<?x4xf32>) -> tensor<2xi32>
  %400 = ""tfl.strided_slice""(%399, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %401 = ""tfl.reshape""(%400, %cst_257) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %402 = ""tfl.fill""(%401, %cst_40) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %403 = ""tfl.mul""(%402, %cst_10) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %404 = ""tfl.select""(%398, %388, %403) : (tensor<?xi1>, tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %405 = ""tfl.greater_equal""(%404, %cst_27) : (tensor<?xf32>, tensor<f32>) -> tensor<?xi1>
  %406 = ""tfl.cast""(%405) : (tensor<?xi1>) -> tensor<?xi32>
  %407 = ""tfl.sum""(%406, %cst_258) {keep_dims = false} : (tensor<?xi32>, tensor<1xi32>) -> tensor<i32>
  %408 = ""tf.Size""(%404) {device = """"} : (tensor<?xf32>) -> tensor<i32>
  %409 = ""tfl.equal""(%400, %408) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %values_291, %indices_292 = ""tfl.topk_v2""(%404, %400) : (tensor<?xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
  %410 = ""tfl.gather""(%404, %indices_292) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %411 = ""tfl.gather""(%389, %indices_292) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %412 = ""tfl.gather""(%390, %indices_292) {axis = 0 : i32} : (tensor<?x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %413 = ""tfl.gather""(%391, %indices_292) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %414 = ""tfl.gather""(%392, %indices_292) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %415 = ""tfl.sub""(%414, %cst_26) {fused_activation_function = ""NONE""} : (tensor<?x4xf32>, tensor<4xf32>) -> tensor<?x4xf32>
  %416:4 = ""tfl.split""(%cst_41, %415) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %417 = ""tfl.concatenation""(%416#0, %416#1, %416#2, %416#3) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x4xf32>
  %418 = ""tfl.shape""(%417) : (tensor<?x4xf32>) -> tensor<2xi32>
  %419 = ""tfl.strided_slice""(%418, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %420 = ""tfl.minimum""(%419, %cst_19) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %421 = ""tfl.greater""(%420, %407) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %422 = ""tfl.select""(%421, %407, %420) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %423 = ""tfl.range""(%cst_32, %422, %cst_41) : (tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<?xi32>
  %424 = ""tfl.pack""(%422) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %425 = ""tfl.cast""(%424) : (tensor<1xi32>) -> tensor<1xf32>
  %426 = ""tfl.range""(%cst_32, %420, %cst_41) : (tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<?xi32>
  %427 = ""tfl.gather""(%410, %426) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %428 = ""tfl.gather""(%427, %423) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %429 = ""tfl.shape""(%428) : (tensor<?xf32>) -> tensor<1xi32>
  %430 = ""tfl.strided_slice""(%429, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %431 = ""tfl.sub""(%430, %cst_19) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %432 = ""tfl.greater""(%431, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %433 = ""tfl.select""(%432, %cst_19, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %434 = ""tfl.pack""(%433) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %435 = ""tfl.slice""(%428, %cst_258, %434) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %436 = ""tfl.shape""(%435) : (tensor<?xf32>) -> tensor<1xi32>
  %437 = ""tfl.strided_slice""(%436, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %438 = ""tfl.sub""(%cst_19, %437) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %439 = ""tfl.pack""(%438) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %440 = ""tfl.pack""(%cst_258, %439) {axis = 1 : i32, values_count = 2 : i32} : (tensor<1xi32>, tensor<1xi32>) -> tensor<1x2xi32>
  %441 = ""tfl.pad""(%435, %440) : (tensor<?xf32>, tensor<1x2xi32>) -> tensor<?xf32>
  %442 = ""tfl.pack""(%441) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?xf32>) -> tensor<1x?xf32>
  %443 = ""tfl.gather""(%411, %426) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %444 = ""tfl.gather""(%443, %423) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %445 = ""tfl.shape""(%444) : (tensor<?xf32>) -> tensor<1xi32>
  %446 = ""tfl.strided_slice""(%445, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %447 = ""tfl.sub""(%446, %cst_19) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %448 = ""tfl.greater""(%447, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %449 = ""tfl.select""(%448, %cst_19, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %450 = ""tfl.pack""(%449) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %451 = ""tfl.slice""(%444, %cst_258, %450) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %452 = ""tfl.shape""(%451) : (tensor<?xf32>) -> tensor<1xi32>
  %453 = ""tfl.strided_slice""(%452, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %454 = ""tfl.sub""(%cst_19, %453) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %455 = ""tfl.pack""(%454) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %456 = ""tfl.pack""(%cst_258, %455) {axis = 1 : i32, values_count = 2 : i32} : (tensor<1xi32>, tensor<1xi32>) -> tensor<1x2xi32>
  %457 = ""tfl.pad""(%451, %456) : (tensor<?xf32>, tensor<1x2xi32>) -> tensor<?xf32>
  %458 = ""tfl.pack""(%457) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?xf32>) -> tensor<1x?xf32>
  %459 = ""tfl.cast""(%458) : (tensor<1x?xf32>) -> tensor<1x?xi32>
  %460 = ""tfl.cast""(%459) : (tensor<1x?xi32>) -> tensor<1x?xf32>
  %461 = ""tfl.gather""(%412, %426) {axis = 0 : i32} : (tensor<?x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %462 = ""tfl.gather""(%461, %423) {axis = 0 : i32} : (tensor<?x12xf32>, tensor<?xi32>) -> tensor<?x12xf32>
  %463 = ""tfl.shape""(%462) : (tensor<?x12xf32>) -> tensor<2xi32>
  %464 = ""tfl.strided_slice""(%463, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %465 = ""tfl.sub""(%464, %cst_19) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %466 = ""tfl.greater""(%465, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %467 = ""tfl.select""(%466, %cst_19, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %468 = ""tfl.strided_slice""(%463, %cst_260, %cst_259, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %469 = ""tfl.sub""(%468, %cst_17) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %470 = ""tfl.greater""(%469, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %471 = ""tfl.select""(%470, %cst_17, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %472 = ""tfl.pack""(%467, %471) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %473 = ""tfl.slice""(%462, %cst_21, %472) : (tensor<?x12xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>
  %474 = ""tfl.shape""(%473) : (tensor<?x?xf32>) -> tensor<2xi32>
  %475 = ""tfl.strided_slice""(%474, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %476 = ""tfl.sub""(%cst_19, %475) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %477 = ""tfl.strided_slice""(%474, %cst_260, %cst_259, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %478 = ""tfl.sub""(%cst_17, %477) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %479 = ""tfl.pack""(%476, %478) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %480 = ""tfl.pack""(%cst_21, %479) {axis = 1 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>
  %481 = ""tfl.pad""(%473, %480) : (tensor<?x?xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>
  %482 = ""tfl.pack""(%481) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?x?xf32>) -> tensor<1x?x?xf32>
  %483 = ""tfl.gather""(%413, %426) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %484 = ""tfl.gather""(%483, %423) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %485 = ""tfl.shape""(%484) : (tensor<?xf32>) -> tensor<1xi32>
  %486 = ""tfl.strided_slice""(%485, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %487 = ""tfl.sub""(%486, %cst_19) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %488 = ""tfl.greater""(%487, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %489 = ""tfl.select""(%488, %cst_19, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %490 = ""tfl.pack""(%489) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %491 = ""tfl.slice""(%484, %cst_258, %490) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %492 = ""tfl.shape""(%491) : (tensor<?xf32>) -> tensor<1xi32>
  %493 = ""tfl.strided_slice""(%492, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %494 = ""tfl.sub""(%cst_19, %493) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %495 = ""tfl.pack""(%494) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %496 = ""tfl.pack""(%cst_258, %495) {axis = 1 : i32, values_count = 2 : i32} : (tensor<1xi32>, tensor<1xi32>) -> tensor<1x2xi32>
  %497 = ""tfl.pad""(%491, %496) : (tensor<?xf32>, tensor<1x2xi32>) -> tensor<?xf32>
  %498 = ""tfl.pack""(%497) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?xf32>) -> tensor<1x?xf32>
  %499 = ""tfl.add""(%498, %cst_40) {fused_activation_function = ""NONE""} : (tensor<1x?xf32>, tensor<f32>) -> tensor<1x?xf32>
  %500 = ""tfl.gather""(%417, %426) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %501 = ""tfl.gather""(%500, %423) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %502 = ""tfl.shape""(%501) : (tensor<?x4xf32>) -> tensor<2xi32>
  %503 = ""tfl.strided_slice""(%502, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %504 = ""tfl.sub""(%503, %cst_19) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %505 = ""tfl.greater""(%504, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %506 = ""tfl.select""(%505, %cst_19, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %507 = ""tfl.strided_slice""(%502, %cst_260, %cst_259, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %508 = ""tfl.sub""(%507, %cst_18) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %509 = ""tfl.greater""(%508, %cst_32) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %510 = ""tfl.select""(%509, %cst_18, %cst_25) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %511 = ""tfl.pack""(%506, %510) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %512 = ""tfl.slice""(%501, %cst_21, %511) : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>
  %513 = ""tfl.shape""(%512) : (tensor<?x?xf32>) -> tensor<2xi32>
  %514 = ""tfl.strided_slice""(%513, %cst_258, %cst_260, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %515 = ""tfl.sub""(%cst_19, %514) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %516 = ""tfl.strided_slice""(%513, %cst_260, %cst_259, %cst_260) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %517 = ""tfl.sub""(%cst_18, %516) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %518 = ""tfl.pack""(%515, %517) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %519 = ""tfl.pack""(%cst_21, %518) {axis = 1 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>
  %520 = ""tfl.pad""(%512, %519) : (tensor<?x?xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>
  %521 = ""tfl.pack""(%520) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?x?xf32>) -> tensor<1x?x?xf32>
  ""std.return""(%425, %521, %499, %186, %442, %170, %482, %460) : (tensor<1xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>, tensor<1x85250x12xf32>, tensor<1x?xf32>, tensor<1x85250x4xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>) -> ()
}) {arg0 = {tf_saved_model.index_path = [""input_tensor""]}, result0 = {tf_saved_model.index_path = [""num_detections""]}, result1 = {tf_saved_model.index_path = [""detection_boxes""]}, result2 = {tf_saved_model.index_path = [""detection_classes""]}, result3 = {tf_saved_model.index_path = [""raw_detection_scores""]}, result4 = {tf_saved_model.index_path = [""detection_scores""]}, result5 = {tf_saved_model.index_path = [""raw_detection_boxes""]}, result6 = {tf_saved_model.index_path = [""detection_multiclass_scores""]}, result7 = {tf_saved_model.index_path = [""detection_anchor_indices""]}, sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_input_tensor:0"", outputs = ""StatefulPartitionedCall:5,StatefulPartitionedCall:1,StatefulPartitionedCall:2,StatefulPartitionedCall:7,StatefulPartitionedCall:4,StatefulPartitionedCall:6,StatefulPartitionedCall:3,StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor<1x?x?x3xui8>) -> (tensor<1xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>, tensor<1x85250x12xf32>, tensor<1x?xf32>, tensor<1x85250x4xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>)} : () -> ()


(tf24_nightly) C:\Users\alfarok\Desktop>
```
"
42872,ValueError: Unable to save the object (Saving a Keras.model),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): at least 2.2.0 and 2.3.0.
- Python version: 3.7.9

**Describe the current behavior**
Model saving crashes after `model.fit` is called:
```
ValueError: Unable to save the object {'loss': <function f at 0x7fc4e9f5cc20>, 'pred': None, 'acc': None} (a dictionary wrapper constructed automatically on attribute assignment). The wrapped dictionary was modified outside the wrapper (its final value was {'loss': <function f at 0x7fc4e9f5cc20>, 'pred': None, 'acc': None}, its value when a checkpoint dependency was added was None), which breaks restoration on object creation.
```

**Describe the expected behavior**
Model gets saved without crash (similar to calling `model.save` before first call of `model.fit`)

**Standalone code to reproduce the issue**
```[python]
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.callbacks import ModelCheckpoint

print(tf.__version__)

fashion_mnist = keras.datasets.fashion_mnist


# load data and create inputs
(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()


def group(img, gt):
    return {'img': img}, {'gt': gt}


train_data = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).batch(1).map(group)
test_data = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(1).map(group)

actual_inputs = {'img': keras.layers.Input((28, 28), dtype='int32', name='img')}
actual_targets = {'gt': keras.layers.Input(shape=(1,), dtype='int32', name='gt')}

# Give access to inputs and targets to the network by joining all available data
# this is required for setting up a loss that requires multiple inputs (e.g. ctc)
# here we just create a cross entropy loss
inputs = {**actual_inputs, **actual_targets}

# create the actual model
flatten_layer = keras.layers.Flatten()
dense_layer = keras.layers.Dense(10)
flattened = flatten_layer(K.expand_dims(K.cast(inputs['img'], dtype='float32')) / 255)
outputs = {'pred': dense_layer(flattened)}

# create losses and metric as 'outputs' of the network
losses = {'loss': tf.keras.layers.Lambda(lambda x: tf.keras.metrics.sparse_categorical_crossentropy(*x, from_logits=True),
                                       name='loss')((inputs['gt'], outputs['pred']))}
metrics = {'acc': tf.keras.layers.Lambda(lambda x: tf.keras.metrics.sparse_categorical_crossentropy(*x), name='acc')(
        (inputs['gt'], outputs['pred']))}
outputs = {**outputs, **losses, **metrics}

# create the model
model = keras.Model(inputs=inputs, outputs=outputs)


# compile the model but with a dummy loss that just returns the 'output' loss
# the same goes for the metric
def f(t, p):
    return p

# compile the model
# since we already comuted the loss as output, we can just forward the loss (function f)
model.compile(optimizer='Adam', loss={k: f for k, _ in losses.items()}, metrics={k: f for k, _ in metrics.items()})


# Saving the model before calling fit works
model.save('test')


# regroup adds a dummy output for every loss/metric so that they get mapped and called correctly
def regroup(inputs, outputs):
    return {**inputs, **outputs}, \
           {l: [0] for l, v in {**losses, **metrics}.items()}


# the model is training, but saving is not possible, e.g. in a ModelCheckpoint
model.fit(train_data.map(regroup), validation_data=test_data.map(regroup), steps_per_epoch=1, callbacks=[ModelCheckpoint('model')])
```

**Other info / logs** Include any logs or source code that would be helpful to
I digged a bit deeper into the code. The problem is, that `model.loss` gets changed during `model.fit` here:
(line 64: `struct = map_missing_dict_keys(outputs, struct)`)

Stacktrace
```
_conform_to_outputs, compile_utils.py:64
build, compile_utils.py:139
__call__, compile_utils.py:187
train_step, training.py:749
run_step, training.py:789
wrapper, api.py:275
_call_for_each_replica, distribute_lib.py:2945
call_for_each_replica, distribute_lib.py:2585
run, distribute_lib.py:1211
step_function, training.py:796
train_function, training.py:806
fit, training.py:1098
```

`struct` is the same dict as `model.loss`, `map_to_output_names` returns the identical object (no copy), `map_missing_dict_keys` changes `struct` and thus `model.loss`. If I manually create a copy of `struct` saving works flawlessly (possible fix)

"
42869,Tensorflow warmup dataset is not used by tensorflow serving,"We have 2 models in production in dockers with tensorflow serving. We have generated a warmup dataset for one of the models. Here are the docker logs:

```
tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:117] Starting to read warmup data for model at /models/model2/5/assets.extra/tf_serving_warmup_requests with model-warmup-options

2020-08-12 02:27:51.998915: I tensorflow_serving/servables/tensorflow/saved_model_warmup.cc:166] Finished reading warmup data for model at /models/model2/5/assets.extra/tf_serving_warmup_requests. Number of warmup records read: 1000. Elapsed time (microseconds): 1293140.

2020-08-12 02:27:52.003127: I tensorflow_serving/core/loader_harness.cc:87] Successfully loaded servable version {name: model2 version: 5}

2020-08-12 02:27:52.005692: I tensorflow_serving/model_servers/server_core.cc:462] Adding/updating models.

2020-08-12 02:27:52.005711: I tensorflow_serving/model_servers/server_core.cc:573] (Re-)adding model: model1

2020-08-12 02:27:52.005734: I tensorflow_serving/model_servers/server_core.cc:573] (Re-)adding model: model2

2020-08-12 02:27:52.008151: I tensorflow_serving/model_servers/server.cc:353] Running gRPC ModelServer at 0.0.0.0:8500 ...

[warn] getaddrinfo: address family for nodename not supported

2020-08-12 02:27:52.012942: I tensorflow_serving/model_servers/server.cc:373] Exporting HTTP/REST API at:localhost:8501 ...

[evhttp_server.cc : 238] NET_LOG: Entering the event loop ...
```

However, when I look in prometheus logs (http://localhost:8501/monitoring/prometheus/metrics) I see the following:

```
# TYPE :tensorflow:serving:model_warmup_latency histogram
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""10""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""18""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""32.4""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""58.32""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""104.976""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""188.957""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""340.122""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""612.22""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""1102""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""1983.59""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""3570.47""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""6426.84""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""11568.3""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""20823""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""37481.3""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""67466.4""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""121440""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""218591""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""393464""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""708235""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""1.27482e+06""} 0
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""2.29468e+06""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""4.13043e+06""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""7.43477e+06""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""1.33826e+07""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""2.40887e+07""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""4.33596e+07""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""7.80473e+07""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""1.40485e+08""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""2.52873e+08""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""4.55172e+08""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""8.19309e+08""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""1.47476e+09""} 1
:tensorflow:serving:model_warmup_latency_bucket{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested"",le=""+Inf""} 1
:tensorflow:serving:model_warmup_latency_sum{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested""} 1.29314e+06
:tensorflow:serving:model_warmup_latency_count{model_path=""/models/model2/5"",status=""Out of range: Read less bytes than requested""} 1
```

The question is: can I conclude that the warmup dataset was successfully consumed by tensorflow-serving? It does not seem so if I understand the Prometheus logs correctly.

Thank you!


**System information**
- Windows 10 (1909)
- Docker desktop: 2.3.0.4
- TensorFlow installed from binary
- TensorFlow version: v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Tensorflow serving: tensorflow/serving:2.2.0
- Python version: 3.7.7
- CUDA/cuDNN version: Cuda compilation tools, release 10.1, V10.1.105"
42867,Converting keras (`.h5`) models to `.tflite` fails with error `Windows fatal exception: access violation`,"**System information**
- OS Platform and Distribution: Windows 10, Version 1909
- TensorFlow installed from: binary
- TensorFlow version (or github SHA if from source): 2.2.0


**Command used to run the converter or code if you’re using the Python API**

Basically I just modified the example from https://www.tensorflow.org/lite/convert/python_api#converting_a_keras_model_.

```
import tensorflow as tf

# Load the model.
model = tf.keras.models.load_model(""model.h5"")

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile('model.tflite', 'wb') as f:
  f.write(tflite_model)
```

**The output from the converter invocation**

```
2020-09-01 17:24:41.539597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:24:43.716706: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-09-01 17:24:44.420857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0
coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-01 17:24:44.427385: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:24:44.433460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-01 17:24:44.444426: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-01 17:24:44.448774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-01 17:24:44.463120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-01 17:24:44.468504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-01 17:24:44.482707: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-01 17:24:44.486066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-01 17:24:44.489318: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-09-01 17:24:44.502237: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19f102e43f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-01 17:24:44.507055: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-01 17:24:44.509839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0
coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-01 17:24:44.514421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:24:44.517064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-01 17:24:44.519967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-01 17:24:44.524530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-01 17:24:44.528097: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-01 17:24:44.530805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-01 17:24:44.532843: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-01 17:24:44.535760: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-01 17:24:45.755590: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-01 17:24:45.758598: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-09-01 17:24:45.760523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-09-01 17:24:45.762401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1368 MB memory) -> physical GPU (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-09-01 17:24:45.771301: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x19f2c581570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-01 17:24:45.775896: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0
2020-09-01 17:24:46.935305: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-09-01 17:24:46.939007: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-01 17:24:46.942082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0
coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-01 17:24:46.945811: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:24:46.947792: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-01 17:24:46.950626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-01 17:24:46.953424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-01 17:24:46.955355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-01 17:24:46.958176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-01 17:24:46.961427: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-01 17:24:46.963952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-01 17:24:46.965588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-01 17:24:46.967750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-09-01 17:24:46.968919: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-09-01 17:24:46.970130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1368 MB memory) -> physical GPU (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-09-01 17:24:47.005582: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-09-01 17:24:47.009765: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0.003ms.
2020-09-01 17:24:47.013518: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-09-01 17:24:48.527683: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-09-01 17:24:48.531120: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-09-01 17:24:48.533535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0
coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-01 17:24:48.537018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:24:48.538710: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-01 17:24:48.540380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-01 17:24:48.543054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-01 17:24:48.546077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-01 17:24:48.549536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-01 17:24:48.552508: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-01 17:24:48.554275: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-01 17:24:48.556311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-01 17:24:48.559038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-09-01 17:24:48.561108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-09-01 17:24:48.562745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1368 MB memory) -> physical GPU (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-09-01 17:24:49.508141: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:797] Optimization results for grappler item: graph_to_optimize
2020-09-01 17:24:49.511726: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 135 nodes (-70), 166 edges (-71), time = 496.187ms.
2020-09-01 17:24:49.513808: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:799]   constant_folding: Graph size after: 135 nodes (0), 166 edges (0), time = 142.545ms.
Traceback (most recent call last):
  File ""test.py"", line 16, in <module>
    tflite_model = converter.convert()
  File ""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\lite\python\lite.py"", line 514, in convert
    result = _toco_convert_impl(
  File ""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\lite\python\convert.py"", line 491, in toco_convert_impl
    data = toco_convert_protos(
  File ""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\lite\python\convert.py"", line 227, in toco_convert_protos
    raise ConverterError(""See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: See console for info.
2020-09-01 17:25:45.917994: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:25:48.444793: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:144] Ignored output_format.
2020-09-01 17:25:48.444956: W tensorflow/compiler/mlir/lite/python/graphdef_to_tfl_flatbuffer.cc:147] Ignored drop_control_dependency.
2020-09-01 17:25:48.612894: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2020-09-01 17:25:48.620668: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24e9af13c60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-09-01 17:25:48.620946: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-09-01 17:25:48.621845: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library nvcuda.dll
2020-09-01 17:25:48.985335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 860M computeCapability: 5.0
coreClock: 1.0195GHz coreCount: 5 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 74.65GiB/s
2020-09-01 17:25:48.985688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
2020-09-01 17:25:48.988892: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_10.dll
2020-09-01 17:25:48.991552: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_10.dll
2020-09-01 17:25:48.992478: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_10.dll
2020-09-01 17:25:48.997442: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_10.dll
2020-09-01 17:25:48.999418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_10.dll
2020-09-01 17:25:49.005007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2020-09-01 17:25:49.005431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-09-01 17:25:50.289313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-09-01 17:25:50.289550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0
2020-09-01 17:25:50.289627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N
2020-09-01 17:25:50.289970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1354 MB memory) -> physical GPU (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)
2020-09-01 17:25:50.293330: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24ec910a3e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-09-01 17:25:50.293537: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0
loc(callsite(""sequential/batch_normalization_1/FusedBatchNormV3""(""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\python\eager\def_function.py"":865:0) at callsite(""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\python\eager\def_function.py"":959:0 at callsite(""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\lite\python\lite.py"":435:0 at ""test.py"":15:0)))): error: non-broadcastable operands
Windows fatal exception: access violation

Current thread 0x000036e4 (most recent call first):
  File ""c:\users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 50 in execute
  File ""c:\users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""c:\users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\absl\app.py"", line 300 in run
  File ""c:\users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\python\platform\app.py"", line 40 in run
  File ""c:\users\chrismit3s\projects\lemon-finder\nn\venv\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 93 in main
  File ""C:\Users\chrismit3s\projects\lemon-finder\nn\venv\Scripts\toco_from_protos.exe\__main__.py"", line 7 in <module>
  File ""C:\Python38\lib\runpy.py"", line 86 in _run_code
  File ""C:\Python38\lib\runpy.py"", line 193 in _run_module_as_main
```

**Also, please include a link to the saved model or GraphDef**

https://drive.google.com/file/d/17V3oYDLKiAB_N13kNJZIk4ukcwyP44Yp/view?usp=sharing

_Model definition:_

```
m = Sequential()

m.add(Input(shape=(100, 100, 3)))
m.add(GaussianNoise(stddev=0.06))

m.add(Conv2D(filters=64, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", activation=""swish""))
m.add(Conv2D(filters=64, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", activation=""swish""))
m.add(Conv2D(filters=64, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", activation=""swish""))
m.add(Conv2D(filters=64, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", strides=(2, 2)))
m.add(GaussianNoise(stddev=0.02))
m.add(BatchNormalization(axis=3))
m.add(Activation(activation=""swish""))

m.add(Conv2D(filters=256, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", activation=""swish""))
m.add(Conv2D(filters=256, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", activation=""swish""))
m.add(Conv2D(filters=256, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", activation=""swish""))
m.add(Conv2D(filters=256, kernel_size=(5, 5), padding=""valid"", data_format=""channels_last"", strides=(2, 2)))
m.add(GaussianNoise(stddev=0.02))
m.add(BatchNormalization(axis=3))
m.add(Activation(activation=""swish""))

m.add(Flatten())

m.add(Dense(units=256, activation=""swish""))
m.add(Dense(units=256, activation=""swish""))
m.add(Dense(units=256, activation=""swish""))
m.add(Dense(units=256))
m.add(GaussianNoise(stddev=0.02))
m.add(BatchNormalization(axis=3))
m.add(Activation(activation=""swish""))

m.add(Dense(units=64, activation=""swish""))
m.add(Dense(units=64, activation=""swish""))
m.add(Dense(units=64, activation=""swish""))
m.add(Dense(units=64, activation=""swish""))

m.add(Dense(units=4, activation=""softmax""))

m.compile(optimizer=""adam"", loss=""categorical_crossentropy"", metrics=[""acc""])
```

EDIT:

Running the same model through the `tflite_convert` script fails in a similar manner. With the `--experimental_new_converter=False` it still fails, just with a different error ([output](https://pastebin.com/SZ4N7zg4))."
42866,Converting keras (.h) models to `.tflite` fails with error `Windows fatal exception: access violation`,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# Copy and paste here the exact command
```

**The output from the converter invocation**

```
# Copy and paste the output here.
```

**Also, please include a link to the saved model or GraphDef**

```
# Put link here or attach to the issue.
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42865,Unable to convert h5model file to tflite file,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.2.0

# Put link here or attach to the issue.

https://gist.github.com/SaiBalaji22/e3815c47970416c37328e9f69a43c958

**Failure details**

I have made a rps classifier using tensor flow python.The problem is i was able to convert it to h5 file.But while converting to tflite file it takes long time.So if I interrupt and restart the kernel it still gives me  error.

**Any other info / logs**

`UnavailableError                          Traceback (most recent call last)
<ipython-input-6-217c00d782f4> in <module>
      1 model = tf.keras.models.load_model('RPSKeras22.h5')
      2 converter = tf.lite.TFLiteConverter.from_keras_model(model)
----> 3 tflite_models = converter.convert()
      4 open(""RPS22.tflite"", ""wb"").write(tflite_model)

~\.conda\envs\tensorflow_env\lib\site-packages\tensorflow_core\lite\python\lite.py in convert(self)
    421 
    422     frozen_func = _convert_to_constants.convert_variables_to_constants_v2(
--> 423         self._funcs[0], lower_control_flow=False)
    424     input_tensors = [
    425         tensor for tensor in frozen_func.inputs

~\.conda\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\framework\convert_to_constants.py in convert_variables_to_constants_v2(func, lower_control_flow)
    426   """"""
    427   # Inline the graph in order to remove functions when possible.
--> 428   graph_def = _run_inline_graph_optimization(func, lower_control_flow)
    429 
    430   # Gets list of all node defs include those in the library.

~\.conda\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\framework\convert_to_constants.py in _run_inline_graph_optimization(func, lower_control_flow)
    125   rewrite_options.min_graph_nodes = -1  # do not skip small graphs
    126   rewrite_options.optimizers.append(""function"")
--> 127   return tf_optimizer.OptimizeGraph(config, meta_graph)
    128 
    129 

~\.conda\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\grappler\tf_optimizer.py in OptimizeGraph(config_proto, metagraph, verbose, graph_id, cluster, strip_default_attributes)
     52                     type(config_proto))
     53   if cluster is None:
---> 54     cluster = gcluster.Cluster()
     55   ret_from_swig = tf_opt.TF_OptimizeGraph(cluster.tf_cluster,
     56                                           config_proto.SerializeToString(),

~\.conda\envs\tensorflow_env\lib\site-packages\tensorflow_core\python\grappler\cluster.py in __init__(self, allow_soft_placement, disable_detailed_stats, disable_timeline, devices)
     52     if devices is None:
     53       self._tf_cluster = tf_cluster.TF_NewCluster(allow_soft_placement,
---> 54                                                   disable_detailed_stats)
     55     else:
     56       devices_serialized = [device.SerializeToString() for device in devices]

UnavailableError: Can't provision more than one single cluster at a time`
"
42864,Inference only works if using same batch size as in training,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.6
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.2

**Describe the current behavior**
I have trained a model using the functional api and exported it to a saved_model.  I trained it using a batch_size of 32 but would like to do inference using only one sample (64ms of streaming audio).  For troubleshooting purposes I am evaluating a subset of the training data.  I read the serialised data into a tf.data.dataset for both training and inference, using the dataset.batch() method to set the batch size. Setting batch_size=1 gives the following result.

```
loss: 2.6619 - accuracy: 0.5155 - precision: 0.5155 - recall: 1.0000 - true_positives: 183.0000 - true_negatives: 0.0000e+00 - false_positives: 172.0000 - false_negatives: 0.0000e+00
```
This is a binary problem so the accuracy is only as good as random.

**Describe the expected behavior**
Setting the batch size to 32 (same as training) gives results consistent with the training metrics:
```
loss: 0.1208 - accuracy: 0.9549 - precision: 0.9665 - recall: 0.9454 - true_positives: 173.0000 - true_negatives: 166.0000 - false_positives: 6.0000 - false_negatives: 10.0000
```

I would expect the results to at least be similar to this when using a batch size of 1.  I have checked the input size at each layer of my model and the batch size dimension is unchanging.

**Standalone code to reproduce the issue**
I use the following code to generate the dataset 
```
def get_test_dataset(tfrecords,
                batch_size,
                input_size=1024,
                n_classes=1,
                shuffle=False,
                fake_input=False):
    """""" Read and preprocess tfrecords into a tf.data.Dataset """"""

    def parse_func(example_proto):
        """""" Parse tfrecords into tf.Feature, to be made into a dataset """"""

        feature_dict = {
            'signal/id': tf.io.FixedLenFeature([], tf.string),
            'segment/start': tf.io.FixedLenFeature([], tf.int64),
            'segment/end': tf.io.FixedLenFeature([], tf.int64),
            'subsegment/id': tf.io.FixedLenFeature([], tf.int64),
            'subsegment/length': tf.io.FixedLenFeature([], tf.int64),
            'subsegment/signal': tf.io.FixedLenFeature([input_size],
                                                       tf.float32),
            'subsegment/features': tf.io.FixedLenFeature(
                [FEAT_SIZE[0] * FEAT_SIZE[1]], tf.float32),
            'subsegment/label': tf.io.FixedLenFeature([], tf.int64)
        }

        parsed_feature = tf.io.parse_single_example(serialized=example_proto,
                                                    features=feature_dict)

        signal = parsed_feature['subsegment/signal']
        signal = tf.cast(signal, dtype=tf.float32)
        signal = tf.expand_dims(signal, 0)

        labels = parsed_feature['subsegment/label']
        labels = tf.expand_dims(labels, axis=-1)
        labels = tf.cast(labels, dtype=tf.float32)

        return signal, labels

    files = tf.data.Dataset.list_files(tfrecords)
    dataset = tf.data.TFRecordDataset(files)
    dataset = dataset.map(parse_func, num_parallel_calls=8)
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(buffer_size=256)

    return dataset

test_audio = get_test_dataset(tfrecords, batch_size)
```
and evaluate using:
```
print(model.evaluate(test_audio, batch_size))
```
Perhaps something is wrong in my model but since I have checked the dimensions at each stage I'm wondering whether it could be a bug.  Thanks!"
42863,Conv3d,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: CONCATENATION, CONV_2D, PACK, SPLIT, SUM. Here is a list of operators for which you will need custom implementations: Conv3D.  


**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

tensorflow.keras.layers.Conv3D(3, 3, padding='same')(x)

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
42862,Keras model.predict on large datasets throws OOM,"I am using ImageDataGenerator and a Keras model (tf.keras.applications.EfficientNetB5). While training I also use the ImageDataGenerator and everything works as expected. But if I use the predict function from the keras model on large data sets, it throws an OOM error, even if it worked in previous versions (e.g. 2.1).

The allocator also tries to allocate a weird shape, it seems like the GPU tries to allocate the memory for all the predictions in GPU memory and not into RAM. If I divide the data in chunks and process them in separate predict calls the error does not occur.

Plattform: Windows 10
GPU: RTX 2080 Ti (11GB)
Python Version: 3.7.7
CUDA Version: 11.0
Tensorflow version:  v2.3.0-rc2-23-gb36436b087 2.3.0
"
42861,How to start JupyterLab in a virtual environment,"Could you please tell me how to start JupyterLab in a virtual environment?

I use Windows 10 and have already installed Anaconda 64 bit.

I failed to install tensorflow in the base environment. Therefore, I created a virtual environment where tensorflow was  installed, by writing the following command on Anaconda prompt.
`conda create -n tf2 tensorflow python=3.7`

With this command, I created the virtual environment succesfully.

Then, I wrote the following command on the activated virtual environment:
`(tf2) conda install -c conda-forge jupyter lab`

Finally, I wrote this command:
`jupyter lab`

However, JupyterLab did not start.
Could you please teach me what I should do?"
42859,"imdb.load_data() is not returning meaningful sentences as I have given print like x_train[0], x_train[1] after loading the data!","**code example for review** 

(x_train, y_train), (x_test, y_test) = imdb.load_data()
word_to_ix = imdb.get_word_index()

ix_to_word = dict( (value, key) for key, value in word_to_ix.items() )

for i in range(len(x_train[0])):
  print(ix_to_word[x_train[0][i]])
"
42858,Linking error: Undefined reference to 'omp_get_max_threads'/'omp_in_parallel'/omp_get_thread_num'/'omp_get_num_threads',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: Python 3.7.0
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): bazel 3.4.0
- GCC/Compiler version (if compiling from source): GCC  9.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
I am building TensorFlow master with mkldnn_v1 (oneDNN) for Aarch64 (as per https://github.com/tensorflow/tensorflow/pull/41232#issuecomment-670049428), I want to incorporate changes into TensorFlow to get oneDNN built for Aarch64 automatically). I have made a temporary change to test this in the cc_library macro  in mkldnn_v1.BUILD to remove any x86 srcs:

```
cc_library(
    name = ""mkl_dnn"",
    srcs = glob([
        ""src/common/*.cpp"",
        ""src/common/*.hpp"",
        ""src/cpu/*.cpp"",
        ""src/cpu/*.hpp"",
        ""src/cpu/rnn/*.cpp"",
        ""src/cpu/rnn/*.hpp"",
        ""src/cpu/matmul/*.cpp"",
        ""src/cpu/matmul/*.hpp"",
        ""src/cpu/gemm/*.cpp"",
        ""src/cpu/gemm/*.hpp"",
        ""src/cpu/gemm/**/*.cpp"",
        ""src/cpu/gemm/**/*.hpp"",
    ]) + [
        "":dnnl_config_h"",
        "":dnnl_version_h"",
    ],
    hdrs = glob([""include/*""]),
    copts = [
        ""-fexceptions"",
        ""-DUSE_MKL"",
        ""-DUSE_CBLAS"",
    ] + if_mkl_open_source_only([
        ""-UUSE_MKL"",
        ""-UUSE_CBLAS"",
    ]) + if_mkl_v1([
        ""-UUSE_MKL"",
        ""-UUSE_CBLAS"",
    ]) + if_mkldnn_threadpool([
        ""-UUSE_MKL"",
        ""-UUSE_CBLAS"",
    ]) + select({
        ""@org_tensorflow//tensorflow:linux_x86_64"": [
            ""-fopenmp"",  # only works with gcc
        ],
        # TODO(ibiryukov): enable openmp with clang by including libomp as a
        # dependency.
        "":clang_linux_x86_64"": [],
        ""//conditions:default"": [],
    }),
    includes = [
        ""include"",
        ""src"",
        ""src/common"",
        ""src/cpu"",
        ""src/cpu/gemm"",
        ""src/cpu/xbyak"",
    ],
    visibility = [""//visibility:public""],
    deps = if_mkl_ml(
        [""@org_tensorflow//third_party/mkl:intel_binary_blob""],
        [],
    ),
)
```

 However, I am facing an issue at the linking stage as given below.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
export HOST_C_COMPILER=(which gcc)
export HOST_CXX_COMPILER=(which g++)
export PYTHON_BIN_PATH=(which python)
export USE_DEFAULT_PYTHON_LIB_PATH=1
export CC_OPT_FLAGS=""""
export TF_ENABLE_XLA=0
export TF_NEED_GCP=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=0
export TF_DOWNLOAD_CLANG=0
export TF_NEED_MPI=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_ROCM=0

 ./configure

bazel build $extra_args \
       --config=mkl_opensource_only \
       --copt=""-mcpu=native --copt=""-moutline-atomics"" --copt=""-O3"" \
       --cxxopt=""-mcpu=native --cxxopt=""-moutline-atomics"" --cxxopt=""-O3"" \
       --linkopt=""-lm"" --linkopt=""-fopenmp"" \
       --config=noaws --config=v2 --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" -s --verbose_failures \
       //tensorflow/tools/pip_package:build_pip_package`
```
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
SUBCOMMAND: # //tensorflow/cc:ops/data_flow_ops_gen_cc [action 'Linking tensorflow/cc/ops/data_flow_ops_gen_cc', configuration: c23edb309cdc7c1c180dab9acba8e07353df62a2190a5128337821049fc4c4fe, execution platform: @local_execution_config_platform//:platform]
(cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/3083553ad2d6c5e705de8a2de81b65f3/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/openblas/0.3.9/lib \
    PATH=/home/ubuntu/python3-venv/bin:/home/ubuntu//packages/bazel/output:/home/ubuntu/python3-venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc @bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/cc/ops/data_flow_ops_gen_cc-2.params)
ERROR: /home/ubuntu/packages/tensorflow/tensorflow/cc/BUILD:545:22: Linking of rule '//tensorflow/cc:ops/sparse_ops_gen_cc' failed (Exit 1): gcc failed: error executing command 
  (cd /home/ubuntu/.cache/bazel/_bazel_ubuntu/3083553ad2d6c5e705de8a2de81b65f3/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/openblas/0.3.9/lib\
    PATH=/home/ubuntu/python3-venv/bin:/home/ubuntu//packages/bazel/output:/home/ubuntu/python3-venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
  /usr/bin/gcc @bazel-out/aarch64-opt-exec-50AE0418/bin/tensorflow/cc/ops/sparse_ops_gen_cc-2.params)
Execution platform: @local_execution_config_platform//:platform
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Scc_Cops_Ssparse_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'omp_get_max_threads'
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Scc_Cops_Ssparse_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'omp_in_parallel'
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Scc_Cops_Ssparse_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'omp_get_thread_num'
bazel-out/aarch64-opt-exec-50AE0418/bin/_solib_aarch64/_U_S_Stensorflow_Scc_Cops_Ssparse_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so.2: error: undefined reference to 'omp_get_num_threads'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 259.203s, Critical Path: 182.59s
INFO: 3379 processes: 3379 local.
FAILED: Build did NOT complete successfully

```"
42857,TFLite conversion stops midway,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source):  tf-nightly


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
tflite_convert --saved_model_dir=.\exported-models\model2\saved_model\ --output_file=.\model.tflite

also tried with 

tflite_convert --saved_model_dir=.\exported-models\model2\saved_model\ --output_file=.\model.tflite --experimental_new_converter=true
```

**The output from the converter invocation**

```
%cst_241 = ""std.constant""() {value = dense<[0.00390369981, -0.0127573842, 0.0908339769, 0.0180643108, -0.00954434275, -0.0410878435, 0.0690650493, 0.0225697756, 0.00117305783, -0.0115958247, 0.26719141, -1.689380e-01, 0.0166990291, -9.513800e-03, 0.336577594, -0.351093948, -0.00157127564, -0.00931285507, -0.167294487, 0.267790347, -0.0237121694, -0.0350524895, -3.717270e-01, 0.36779502]> : tensor<24xf32>} : () -> tensor<24xf32>
  %cst_242 = ""std.constant""() {value = dense<[1, 51150, 1, 4]> : tensor<4xi32>} : () -> tensor<4xi32>
  %cst_243 = ""std.constant""() {value = dense<[1, 51150, 4]> : tensor<3xi32>} : () -> tensor<3xi32>
  %cst_244 = ""std.constant""() {value = dense<[-4.6671648, -4.234303, -4.66916609, -4.03752136, -4.66963768, -4.34722042, -4.6698885, -4.11983204, -4.67162561, -4.25614262, -4.67194557, -4.16072559]> : tensor<12xf32>} : () -> tensor<12xf32>
  %cst_245 = ""std.constant""() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_246 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_247 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_248 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %0 = ""tfl.cast""(%arg0) : (tensor<1x?x?x3xui8>) -> tensor<1x?x?x3xf32>
  %1 = ""tfl.sub""(%0, %cst_23) {fused_activation_function = ""NONE""} : (tensor<1x?x?x3xf32>, tensor<1x1x3xf32>) -> tensor<1x?x?x3xf32>
  %2 = ""tfl.unpack""(%1) {axis = 0 : i32, num = 1 : i32} : (tensor<1x?x?x3xf32>) -> tensor<?x?x3xf32>
  %3 = ""tfl.expand_dims""(%2, %cst_21) : (tensor<?x?x3xf32>, tensor<i32>) -> tensor<1x?x?x3xf32>
  %4 = ""tfl.resize_bilinear""(%3, %cst_22) {align_corners = false, half_pixel_centers = false} : (tensor<1x?x?x3xf32>, tensor<2xi32>) -> tensor<1x640x640x3xf32>
  %5 = ""tfl.reshape""(%4, %cst_217) : (tensor<1x640x640x3xf32>, tensor<3xi32>) -> tensor<640x640x3xf32>
  %6 = ""tfl.pack""(%5) {axis = 0 : i32, values_count = 1 : i32} : (tensor<640x640x3xf32>) -> tensor<1x640x640x3xf32>
  %7 = ""tfl.pad""(%6, %cst_26) : (tensor<1x640x640x3xf32>, tensor<4x2xi32>) -> tensor<1x646x646x3xf32>
  %8 = ""tfl.conv_2d""(%7, %cst_115, %cst_38) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""VALID"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x646x646x3xf32>, tensor<64x7x7x3xf32>, tensor<64xf32>) -> tensor<1x320x320x64xf32>
  %9 = ""tfl.max_pool_2d""(%8) {filter_height = 3 : i32, filter_width = 3 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x320x320x64xf32>) -> tensor<1x160x160x64xf32>
  %10 = ""tfl.conv_2d""(%9, %cst_116, %cst_218) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %11 = ""tfl.conv_2d""(%9, %cst_117, %cst_39) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x1x1x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %12 = ""tfl.conv_2d""(%11, %cst_118, %cst_40) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %13 = ""tfl.conv_2d""(%12, %cst_119, %cst_219) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %14 = ""tfl.add""(%10, %13) {fused_activation_function = ""RELU6""} : (tensor<1x160x160x256xf32>, tensor<1x160x160x256xf32>) -> tensor<1x160x160x256xf32>
  %15 = ""tfl.conv_2d""(%14, %cst_120, %cst_41) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x256xf32>, tensor<64x1x1x256xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %16 = ""tfl.conv_2d""(%15, %cst_121, %cst_42) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %17 = ""tfl.conv_2d""(%16, %cst_122, %cst_220) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %18 = ""tfl.add""(%14, %17) {fused_activation_function = ""RELU6""} : (tensor<1x160x160x256xf32>, tensor<1x160x160x256xf32>) -> tensor<1x160x160x256xf32>
  %19 = ""tfl.conv_2d""(%18, %cst_123, %cst_43) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x256xf32>, tensor<64x1x1x256xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %20 = ""tfl.conv_2d""(%19, %cst_124, %cst_44) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<64x3x3x64xf32>, tensor<64xf32>) -> tensor<1x160x160x64xf32>
  %21 = ""tfl.conv_2d""(%20, %cst_125, %cst_221) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x160x160x64xf32>, tensor<256x1x1x64xf32>, tensor<256xf32>) -> tensor<1x160x160x256xf32>
  %22 = ""tfl.add""(%18, %21) {fused_activation_function = ""RELU6""} : (tensor<1x160x160x256xf32>, tensor<1x160x160x256xf32>) -> tensor<1x160x160x256xf32>
  %23 = ""tfl.conv_2d""(%22, %cst_126, %cst_222) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x160x160x256xf32>, tensor<512x1x1x256xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %24 = ""tfl.conv_2d""(%22, %cst_127, %cst_45) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x160x160x256xf32>, tensor<128x1x1x256xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %25 = ""tfl.conv_2d""(%24, %cst_128, %cst_46) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %26 = ""tfl.conv_2d""(%25, %cst_129, %cst_223) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %27 = ""tfl.add""(%23, %26) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %28 = ""tfl.conv_2d""(%27, %cst_130, %cst_47) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<128x1x1x512xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %29 = ""tfl.conv_2d""(%28, %cst_131, %cst_48) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %30 = ""tfl.conv_2d""(%29, %cst_132, %cst_224) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %31 = ""tfl.add""(%27, %30) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %32 = ""tfl.conv_2d""(%31, %cst_133, %cst_49) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<128x1x1x512xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %33 = ""tfl.conv_2d""(%32, %cst_134, %cst_50) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %34 = ""tfl.conv_2d""(%33, %cst_135, %cst_225) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %35 = ""tfl.add""(%31, %34) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %36 = ""tfl.conv_2d""(%35, %cst_136, %cst_51) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<128x1x1x512xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %37 = ""tfl.conv_2d""(%36, %cst_137, %cst_52) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<128x3x3x128xf32>, tensor<128xf32>) -> tensor<1x80x80x128xf32>
  %38 = ""tfl.conv_2d""(%37, %cst_138, %cst_226) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x128xf32>, tensor<512x1x1x128xf32>, tensor<512xf32>) -> tensor<1x80x80x512xf32>
  %39 = ""tfl.add""(%35, %38) {fused_activation_function = ""RELU6""} : (tensor<1x80x80x512xf32>, tensor<1x80x80x512xf32>) -> tensor<1x80x80x512xf32>
  %40 = ""tfl.conv_2d""(%39, %cst_139, %cst_227) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x512xf32>, tensor<256x1x1x512xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %41 = ""tfl.conv_2d""(%39, %cst_140, %cst_228) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x80x80x512xf32>, tensor<1024x1x1x512xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %42 = ""tfl.conv_2d""(%39, %cst_141, %cst_53) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x80x80x512xf32>, tensor<256x1x1x512xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %43 = ""tfl.conv_2d""(%42, %cst_142, %cst_54) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %44 = ""tfl.conv_2d""(%43, %cst_143, %cst_229) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %45 = ""tfl.add""(%41, %44) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %46 = ""tfl.conv_2d""(%45, %cst_144, %cst_55) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %47 = ""tfl.conv_2d""(%46, %cst_145, %cst_56) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %48 = ""tfl.conv_2d""(%47, %cst_146, %cst_230) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %49 = ""tfl.add""(%45, %48) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %50 = ""tfl.conv_2d""(%49, %cst_147, %cst_57) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %51 = ""tfl.conv_2d""(%50, %cst_148, %cst_58) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %52 = ""tfl.conv_2d""(%51, %cst_149, %cst_231) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %53 = ""tfl.add""(%49, %52) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %54 = ""tfl.conv_2d""(%53, %cst_150, %cst_59) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %55 = ""tfl.conv_2d""(%54, %cst_151, %cst_60) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %56 = ""tfl.conv_2d""(%55, %cst_152, %cst_232) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %57 = ""tfl.add""(%53, %56) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %58 = ""tfl.conv_2d""(%57, %cst_153, %cst_61) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %59 = ""tfl.conv_2d""(%58, %cst_154, %cst_62) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %60 = ""tfl.conv_2d""(%59, %cst_155, %cst_233) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %61 = ""tfl.add""(%57, %60) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %62 = ""tfl.conv_2d""(%61, %cst_156, %cst_63) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %63 = ""tfl.conv_2d""(%62, %cst_157, %cst_64) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %64 = ""tfl.conv_2d""(%63, %cst_158, %cst_234) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<1024x1x1x256xf32>, tensor<1024xf32>) -> tensor<1x40x40x1024xf32>
  %65 = ""tfl.add""(%61, %64) {fused_activation_function = ""RELU6""} : (tensor<1x40x40x1024xf32>, tensor<1x40x40x1024xf32>) -> tensor<1x40x40x1024xf32>
  %66 = ""tfl.conv_2d""(%65, %cst_159, %cst_235) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x1024xf32>, tensor<256x1x1x1024xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %67 = ""tfl.conv_2d""(%65, %cst_160, %cst_236) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x40x40x1024xf32>, tensor<2048x1x1x1024xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %68 = ""tfl.conv_2d""(%65, %cst_161, %cst_65) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x40x40x1024xf32>, tensor<512x1x1x1024xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %69 = ""tfl.conv_2d""(%68, %cst_162, %cst_66) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %70 = ""tfl.conv_2d""(%69, %cst_163, %cst_237) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<2048x1x1x512xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %71 = ""tfl.add""(%67, %70) {fused_activation_function = ""RELU6""} : (tensor<1x20x20x2048xf32>, tensor<1x20x20x2048xf32>) -> tensor<1x20x20x2048xf32>
  %72 = ""tfl.conv_2d""(%71, %cst_164, %cst_67) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x2048xf32>, tensor<512x1x1x2048xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %73 = ""tfl.conv_2d""(%72, %cst_165, %cst_68) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %74 = ""tfl.conv_2d""(%73, %cst_166, %cst_238) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<2048x1x1x512xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %75 = ""tfl.add""(%71, %74) {fused_activation_function = ""RELU6""} : (tensor<1x20x20x2048xf32>, tensor<1x20x20x2048xf32>) -> tensor<1x20x20x2048xf32>
  %76 = ""tfl.conv_2d""(%75, %cst_167, %cst_69) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x2048xf32>, tensor<512x1x1x2048xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %77 = ""tfl.conv_2d""(%76, %cst_168, %cst_70) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<512x3x3x512xf32>, tensor<512xf32>) -> tensor<1x20x20x512xf32>
  %78 = ""tfl.conv_2d""(%77, %cst_169, %cst_239) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x512xf32>, tensor<2048x1x1x512xf32>, tensor<2048xf32>) -> tensor<1x20x20x2048xf32>
  %79 = ""tfl.add""(%75, %78) {fused_activation_function = ""RELU6""} : (tensor<1x20x20x2048xf32>, tensor<1x20x20x2048xf32>) -> tensor<1x20x20x2048xf32>
  %80 = ""tfl.conv_2d""(%79, %cst_170, %cst_240) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x2048xf32>, tensor<256x1x1x2048xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %81 = ""tfl.pack""(%80, %80) {axis = 3 : i32, values_count = 2 : i32} : (tensor<1x20x20x256xf32>, tensor<1x20x20x256xf32>) -> tensor<1x20x20x2x256xf32>
  %82 = ""tfl.pack""(%81, %81) {axis = 2 : i32, values_count = 2 : i32} : (tensor<1x20x20x2x256xf32>, tensor<1x20x20x2x256xf32>) -> tensor<1x20x2x20x2x256xf32>
  %83 = ""tfl.reshape""(%82, %cst_24) : (tensor<1x20x2x20x2x256xf32>, tensor<4xi32>) -> tensor<1x40x40x256xf32>
  %84 = ""tfl.add""(%83, %66) {fused_activation_function = ""NONE""} : (tensor<1x40x40x256xf32>, tensor<1x40x40x256xf32>) -> tensor<1x40x40x256xf32>
  %85 = ""tfl.conv_2d""(%84, %cst_171, %cst_71) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %86 = ""tfl.pack""(%85, %85) {axis = 3 : i32, values_count = 2 : i32} : (tensor<1x40x40x256xf32>, tensor<1x40x40x256xf32>) -> tensor<1x40x40x2x256xf32>
  %87 = ""tfl.pack""(%86, %86) {axis = 2 : i32, values_count = 2 : i32} : (tensor<1x40x40x2x256xf32>, tensor<1x40x40x2x256xf32>) -> tensor<1x40x2x40x2x256xf32>
  %88 = ""tfl.reshape""(%87, %cst_25) : (tensor<1x40x2x40x2x256xf32>, tensor<4xi32>) -> tensor<1x80x80x256xf32>
  %89 = ""tfl.add""(%88, %40) {fused_activation_function = ""NONE""} : (tensor<1x80x80x256xf32>, tensor<1x80x80x256xf32>) -> tensor<1x80x80x256xf32>
  %90 = ""tfl.conv_2d""(%89, %cst_172, %cst_72) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %91 = ""tfl.conv_2d""(%80, %cst_173, %cst_73) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %92 = ""tfl.conv_2d""(%91, %cst_174, %cst_74) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 2 : i32, stride_w = 2 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %93 = ""tfl.conv_2d""(%90, %cst_175, %cst_75) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %94 = ""tfl.conv_2d""(%85, %cst_176, %cst_76) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %95 = ""tfl.conv_2d""(%80, %cst_177, %cst_77) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %96 = ""tfl.conv_2d""(%91, %cst_178, %cst_78) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %97 = ""tfl.conv_2d""(%92, %cst_179, %cst_79) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %98 = ""tfl.conv_2d""(%93, %cst_180, %cst_80) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %99 = ""tfl.conv_2d""(%94, %cst_181, %cst_81) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %100 = ""tfl.conv_2d""(%95, %cst_182, %cst_82) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %101 = ""tfl.conv_2d""(%96, %cst_183, %cst_83) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %102 = ""tfl.conv_2d""(%97, %cst_184, %cst_84) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %103 = ""tfl.conv_2d""(%98, %cst_185, %cst_85) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %104 = ""tfl.conv_2d""(%99, %cst_186, %cst_86) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %105 = ""tfl.conv_2d""(%100, %cst_187, %cst_87) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %106 = ""tfl.conv_2d""(%101, %cst_188, %cst_88) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %107 = ""tfl.conv_2d""(%102, %cst_189, %cst_89) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %108 = ""tfl.conv_2d""(%103, %cst_190, %cst_90) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %109 = ""tfl.conv_2d""(%104, %cst_191, %cst_91) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %110 = ""tfl.conv_2d""(%105, %cst_192, %cst_92) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %111 = ""tfl.conv_2d""(%106, %cst_193, %cst_93) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %112 = ""tfl.conv_2d""(%107, %cst_194, %cst_94) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %113 = ""tfl.conv_2d""(%90, %cst_195, %cst_95) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %114 = ""tfl.conv_2d""(%85, %cst_196, %cst_96) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %115 = ""tfl.conv_2d""(%80, %cst_197, %cst_97) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %116 = ""tfl.conv_2d""(%91, %cst_198, %cst_98) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %117 = ""tfl.conv_2d""(%92, %cst_199, %cst_99) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %118 = ""tfl.conv_2d""(%113, %cst_200, %cst_100) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %119 = ""tfl.conv_2d""(%114, %cst_201, %cst_101) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %120 = ""tfl.conv_2d""(%115, %cst_202, %cst_102) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %121 = ""tfl.conv_2d""(%116, %cst_203, %cst_103) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %122 = ""tfl.conv_2d""(%117, %cst_204, %cst_104) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %123 = ""tfl.conv_2d""(%118, %cst_205, %cst_105) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %124 = ""tfl.conv_2d""(%119, %cst_206, %cst_106) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %125 = ""tfl.conv_2d""(%120, %cst_207, %cst_107) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %126 = ""tfl.conv_2d""(%121, %cst_208, %cst_108) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %127 = ""tfl.conv_2d""(%122, %cst_209, %cst_109) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %128 = ""tfl.conv_2d""(%123, %cst_210, %cst_110) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x80x80x256xf32>
  %129 = ""tfl.conv_2d""(%124, %cst_211, %cst_111) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x40x40x256xf32>
  %130 = ""tfl.conv_2d""(%125, %cst_212, %cst_112) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x20x20x256xf32>
  %131 = ""tfl.conv_2d""(%126, %cst_213, %cst_113) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x10x10x256xf32>
  %132 = ""tfl.conv_2d""(%127, %cst_214, %cst_114) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""RELU6"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<256x3x3x256xf32>, tensor<256xf32>) -> tensor<1x5x5x256xf32>
  %133 = ""tfl.conv_2d""(%108, %cst_215, %cst_241) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<24x3x3x256xf32>, tensor<24xf32>) -> tensor<1x80x80x24xf32>
  %134 = ""tfl.reshape""(%133, %cst_27) : (tensor<1x80x80x24xf32>, tensor<3xi32>) -> tensor<1x38400x4xf32>
  %135 = ""tfl.conv_2d""(%109, %cst_215, %cst_241) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<24x3x3x256xf32>, tensor<24xf32>) -> tensor<1x40x40x24xf32>
  %136 = ""tfl.reshape""(%135, %cst_27) : (tensor<1x40x40x24xf32>, tensor<3xi32>) -> tensor<1x9600x4xf32>
  %137 = ""tfl.conv_2d""(%110, %cst_215, %cst_241) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<24x3x3x256xf32>, tensor<24xf32>) -> tensor<1x20x20x24xf32>
  %138 = ""tfl.reshape""(%137, %cst_27) : (tensor<1x20x20x24xf32>, tensor<3xi32>) -> tensor<1x2400x4xf32>
  %139 = ""tfl.conv_2d""(%111, %cst_215, %cst_241) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<24x3x3x256xf32>, tensor<24xf32>) -> tensor<1x10x10x24xf32>
  %140 = ""tfl.reshape""(%139, %cst_27) : (tensor<1x10x10x24xf32>, tensor<3xi32>) -> tensor<1x600x4xf32>
  %141 = ""tfl.conv_2d""(%112, %cst_215, %cst_241) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<24x3x3x256xf32>, tensor<24xf32>) -> tensor<1x5x5x24xf32>
  %142 = ""tfl.reshape""(%141, %cst_27) : (tensor<1x5x5x24xf32>, tensor<3xi32>) -> tensor<1x150x4xf32>
  %143 = ""tfl.concatenation""(%134, %136, %138, %140, %142) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<1x38400x4xf32>, tensor<1x9600x4xf32>, tensor<1x2400x4xf32>, tensor<1x600x4xf32>, tensor<1x150x4xf32>) -> tensor<1x51150x4xf32>
  %144 = ""tfl.reshape""(%143, %cst_17) : (tensor<1x51150x4xf32>, tensor<2xi32>) -> tensor<51150x4xf32>
  %145 = ""tfl.transpose""(%144, %cst_15) : (tensor<51150x4xf32>, tensor<2xi32>) -> tensor<4x51150xf32>
  %146:4 = ""tfl.unpack""(%145) {axis = 0 : i32, num = 4 : i32} : (tensor<4x51150xf32>) -> (tensor<51150xf32>, tensor<51150xf32>, tensor<51150xf32>, tensor<51150xf32>)
  %147 = ""tfl.mul""(%146#0, %cst_31) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<f32>) -> tensor<51150xf32>
  %148 = ""tfl.mul""(%147, %cst_34) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %149 = ""tfl.add""(%148, %cst_35) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %150 = ""tfl.mul""(%146#1, %cst_31) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<f32>) -> tensor<51150xf32>
  %151 = ""tfl.mul""(%150, %cst_36) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %152 = ""tfl.add""(%151, %cst_37) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %153 = ""tfl.mul""(%146#2, %cst_32) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<f32>) -> tensor<51150xf32>
  %154 = ""tfl.exp""(%153) : (tensor<51150xf32>) -> tensor<51150xf32>
  %155 = ""tfl.mul""(%154, %cst_34) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %156 = ""tfl.mul""(%155, %cst_33) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<f32>) -> tensor<51150xf32>
  %157 = ""tfl.sub""(%149, %156) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %158 = ""tfl.add""(%149, %156) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %159 = ""tfl.mul""(%146#3, %cst_32) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<f32>) -> tensor<51150xf32>
  %160 = ""tfl.exp""(%159) : (tensor<51150xf32>) -> tensor<51150xf32>
  %161 = ""tfl.mul""(%160, %cst_36) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %162 = ""tfl.mul""(%161, %cst_33) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<f32>) -> tensor<51150xf32>
  %163 = ""tfl.sub""(%152, %162) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %164 = ""tfl.add""(%152, %162) {fused_activation_function = ""NONE""} : (tensor<51150xf32>, tensor<51150xf32>) -> tensor<51150xf32>
  %165 = ""tfl.pack""(%157, %163, %158, %164) {axis = 0 : i32, values_count = 4 : i32} : (tensor<51150xf32>, tensor<51150xf32>, tensor<51150xf32>, tensor<51150xf32>) -> tensor<4x51150xf32>
  %166 = ""tfl.transpose""(%165, %cst_15) : (tensor<4x51150xf32>, tensor<2xi32>) -> tensor<51150x4xf32>
  %167 = ""tfl.reshape""(%166, %cst_243) : (tensor<51150x4xf32>, tensor<3xi32>) -> tensor<1x51150x4xf32>
  %168 = ""tfl.reshape""(%166, %cst_242) : (tensor<51150x4xf32>, tensor<4xi32>) -> tensor<1x51150x1x4xf32>
  %169 = ""tfl.unpack""(%168) {axis = 0 : i32, num = 1 : i32} : (tensor<1x51150x1x4xf32>) -> tensor<51150x1x4xf32>
  %170 = ""tfl.slice""(%169, %cst_8, %cst_10) : (tensor<51150x1x4xf32>, tensor<3xi32>, tensor<3xi32>) -> tensor<51150x1x4xf32>
  %171 = ""tfl.unpack""(%170) {axis = 1 : i32, num = 1 : i32} : (tensor<51150x1x4xf32>) -> tensor<51150x4xf32>
  %172 = ""tfl.conv_2d""(%128, %cst_216, %cst_244) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x80x80x256xf32>, tensor<12x3x3x256xf32>, tensor<12xf32>) -> tensor<1x80x80x12xf32>
  %173 = ""tfl.reshape""(%172, %cst_28) : (tensor<1x80x80x12xf32>, tensor<3xi32>) -> tensor<1x38400x2xf32>
  %174 = ""tfl.conv_2d""(%129, %cst_216, %cst_244) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x40x40x256xf32>, tensor<12x3x3x256xf32>, tensor<12xf32>) -> tensor<1x40x40x12xf32>
  %175 = ""tfl.reshape""(%174, %cst_28) : (tensor<1x40x40x12xf32>, tensor<3xi32>) -> tensor<1x9600x2xf32>
  %176 = ""tfl.conv_2d""(%130, %cst_216, %cst_244) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x20x20x256xf32>, tensor<12x3x3x256xf32>, tensor<12xf32>) -> tensor<1x20x20x12xf32>
  %177 = ""tfl.reshape""(%176, %cst_28) : (tensor<1x20x20x12xf32>, tensor<3xi32>) -> tensor<1x2400x2xf32>
  %178 = ""tfl.conv_2d""(%131, %cst_216, %cst_244) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x10x10x256xf32>, tensor<12x3x3x256xf32>, tensor<12xf32>) -> tensor<1x10x10x12xf32>
  %179 = ""tfl.reshape""(%178, %cst_28) : (tensor<1x10x10x12xf32>, tensor<3xi32>) -> tensor<1x600x2xf32>
  %180 = ""tfl.conv_2d""(%132, %cst_216, %cst_244) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x5x5x256xf32>, tensor<12x3x3x256xf32>, tensor<12xf32>) -> tensor<1x5x5x12xf32>
  %181 = ""tfl.reshape""(%180, %cst_28) : (tensor<1x5x5x12xf32>, tensor<3xi32>) -> tensor<1x150x2xf32>
  %182 = ""tfl.concatenation""(%173, %175, %177, %179, %181) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<1x38400x2xf32>, tensor<1x9600x2xf32>, tensor<1x2400x2xf32>, tensor<1x600x2xf32>, tensor<1x150x2xf32>) -> tensor<1x51150x2xf32>
  %183 = ""tfl.logistic""(%182) : (tensor<1x51150x2xf32>) -> tensor<1x51150x2xf32>
  %184 = ""tfl.unpack""(%183) {axis = 0 : i32, num = 1 : i32} : (tensor<1x51150x2xf32>) -> tensor<51150x2xf32>
  %185 = ""tfl.slice""(%184, %cst_9, %cst_11) : (tensor<51150x2xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<51150x2xf32>
  %186 = ""tfl.slice""(%183, %cst_18, %cst_19) : (tensor<1x51150x2xf32>, tensor<3xi32>, tensor<3xi32>) -> tensor<1x51150x1xf32>
  %187 = ""tfl.unpack""(%186) {axis = 0 : i32, num = 1 : i32} : (tensor<1x51150x1xf32>) -> tensor<51150x1xf32>
  %188 = ""tfl.slice""(%187, %cst_9, %cst_11) : (tensor<51150x1xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<51150x1xf32>
  %189 = ""tfl.slice""(%188, %cst_9, %cst_2) : (tensor<51150x1xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<51150x1xf32>
  %190 = ""tfl.reshape""(%189, %cst_245) : (tensor<51150x1xf32>, tensor<1xi32>) -> tensor<51150xf32>
  %selected_indices, %selected_scores, %valid_outputs = ""tfl.non_max_suppression_v5""(%171, %190, %cst_7, %cst_0, %cst_1, %cst_14) : (tensor<51150x4xf32>, tensor<51150xf32>, tensor<i32>, tensor<f32>, tensor<f32>, tensor<f32>) -> (tensor<?xi32>, tensor<?xf32>, tensor<i32>)
  %191 = ""tfl.shape""(%selected_indices) : (tensor<?xi32>) -> tensor<1xi32>
  %192 = ""tfl.strided_slice""(%191, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %193 = ""tfl.less""(%cst_3, %192) : (tensor<100xi32>, tensor<i32>) -> tensor<100xi1>
  %194 = ""tfl.sub""(%cst_7, %192) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %195 = ""tfl.reshape""(%194, %cst_245) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %196 = ""tfl.fill""(%195, %cst_21) : (tensor<1xi32>, tensor<i32>) -> tensor<?xi32>
  %197 = ""tfl.fill""(%195, %cst_14) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %198 = ""tfl.concatenation""(%selected_indices, %196) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<?xi32>, tensor<?xi32>) -> tensor<?xi32>
  %199 = ""tfl.gather""(%cst_20, %198) {axis = 0 : i32} : (tensor<51150xf32>, tensor<?xi32>) -> tensor<?xf32>
  %200 = ""tfl.gather""(%185, %198) {axis = 0 : i32} : (tensor<51150x2xf32>, tensor<?xi32>) -> tensor<?x2xf32>
  %201 = ""tfl.gather""(%171, %198) {axis = 0 : i32} : (tensor<51150x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %202 = ""tfl.shape""(%201) : (tensor<?x4xf32>) -> tensor<2xi32>
  %203 = ""tfl.strided_slice""(%202, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %204 = ""tfl.equal""(%203, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %205 = ""tfl.concatenation""(%selected_scores, %197) {axis = -1 : i32, fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %206 = ""tfl.select""(%193, %205, %cst_4) : (tensor<100xi1>, tensor<?xf32>, tensor<100xf32>) -> tensor<100xf32>
  %values, %indices = ""tfl.topk_v2""(%206, %203) : (tensor<100xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
  %207 = ""tfl.gather""(%206, %indices) {axis = 0 : i32} : (tensor<100xf32>, tensor<?xi32>) -> tensor<?xf32>
  %208 = ""tfl.gather""(%199, %indices) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %209 = ""tfl.gather""(%200, %indices) {axis = 0 : i32} : (tensor<?x2xf32>, tensor<?xi32>) -> tensor<?x2xf32>
  %210 = ""tfl.gather""(%cst_5, %indices) {axis = 0 : i32} : (tensor<100xf32>, tensor<?xi32>) -> tensor<?xf32>
  %211 = ""tfl.gather""(%201, %indices) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %212:4 = ""tfl.split""(%cst_30, %211) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %213 = ""tfl.minimum""(%212#0, %cst_29) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %214 = ""tfl.relu""(%213) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %215 = ""tfl.minimum""(%212#2, %cst_29) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %216 = ""tfl.relu""(%215) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %217 = ""tfl.minimum""(%212#1, %cst_29) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %218 = ""tfl.relu""(%217) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %219 = ""tfl.minimum""(%212#3, %cst_29) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xf32>
  %220 = ""tfl.relu""(%219) : (tensor<?x1xf32>) -> tensor<?x1xf32>
  %221 = ""tfl.concatenation""(%214, %218, %216, %220) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x4xf32>
  %222:4 = ""tfl.split""(%cst_30, %221) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %223 = ""tfl.sub""(%222#2, %222#0) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %224 = ""tfl.sub""(%222#3, %222#1) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %225 = ""tfl.mul""(%223, %224) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %226 = ""tfl.greater""(%225, %cst_14) : (tensor<?x1xf32>, tensor<f32>) -> tensor<?x1xi1>
  %227 = ""tfl.reshape""(%226, %cst_245) : (tensor<?x1xi1>, tensor<1xi32>) -> tensor<?xi1>
  %228 = ""tfl.where""(%227) : (tensor<?xi1>) -> tensor<?x1xi64>
  %229 = ""tfl.reshape""(%228, %cst_245) : (tensor<?x1xi64>, tensor<1xi32>) -> tensor<?xi64>
  %230 = ""tfl.cast""(%229) : (tensor<?xi64>) -> tensor<?xi32>
  %231 = ""tfl.gather""(%207, %230) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %232 = ""tfl.gather""(%208, %230) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %233 = ""tfl.gather""(%209, %230) {axis = 0 : i32} : (tensor<?x2xf32>, tensor<?xi32>) -> tensor<?x2xf32>
  %234 = ""tfl.gather""(%210, %230) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %235 = ""tfl.gather""(%221, %230) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %236:4 = ""tfl.split""(%cst_30, %235) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %237 = ""tfl.sub""(%236#2, %236#0) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %238 = ""tfl.sub""(%236#3, %236#1) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %239 = ""tfl.mul""(%237, %238) {fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x1xf32>
  %240 = ""tfl.reshape""(%239, %cst_245) : (tensor<?x1xf32>, tensor<1xi32>) -> tensor<?xf32>
  %241 = ""tfl.cast""(%240) : (tensor<?xf32>) -> tensor<?xi1>
  %242 = ""tfl.shape""(%235) : (tensor<?x4xf32>) -> tensor<2xi32>
  %243 = ""tfl.strided_slice""(%242, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %244 = ""tfl.reshape""(%243, %cst_245) : (tensor<i32>, tensor<1xi32>) -> tensor<1xi32>
  %245 = ""tfl.fill""(%244, %cst_29) : (tensor<1xi32>, tensor<f32>) -> tensor<?xf32>
  %246 = ""tfl.mul""(%245, %cst) {fused_activation_function = ""NONE""} : (tensor<?xf32>, tensor<f32>) -> tensor<?xf32>
  %247 = ""tfl.select""(%241, %231, %246) : (tensor<?xi1>, tensor<?xf32>, tensor<?xf32>) -> tensor<?xf32>
  %248 = ""tfl.greater_equal""(%247, %cst_14) : (tensor<?xf32>, tensor<f32>) -> tensor<?xi1>
  %249 = ""tfl.cast""(%248) : (tensor<?xi1>) -> tensor<?xi32>
  %250 = ""tfl.sum""(%249, %cst_246) {keep_dims = false} : (tensor<?xi32>, tensor<1xi32>) -> tensor<i32>
  %251 = ""tf.Size""(%247) {device = """"} : (tensor<?xf32>) -> tensor<i32>
  %252 = ""tfl.equal""(%243, %251) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %values_249, %indices_250 = ""tfl.topk_v2""(%247, %243) : (tensor<?xf32>, tensor<i32>) -> (tensor<?xf32>, tensor<?xi32>)
  %253 = ""tfl.gather""(%247, %indices_250) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %254 = ""tfl.gather""(%232, %indices_250) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %255 = ""tfl.gather""(%233, %indices_250) {axis = 0 : i32} : (tensor<?x2xf32>, tensor<?xi32>) -> tensor<?x2xf32>
  %256 = ""tfl.gather""(%234, %indices_250) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %257 = ""tfl.gather""(%235, %indices_250) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %258 = ""tfl.sub""(%257, %cst_13) {fused_activation_function = ""NONE""} : (tensor<?x4xf32>, tensor<4xf32>) -> tensor<?x4xf32>
  %259:4 = ""tfl.split""(%cst_30, %258) {num_splits = 4 : i32} : (tensor<i32>, tensor<?x4xf32>) -> (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>)
  %260 = ""tfl.concatenation""(%259#0, %259#1, %259#2, %259#3) {axis = 1 : i32, fused_activation_function = ""NONE""} : (tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>, tensor<?x1xf32>) -> tensor<?x4xf32>
  %261 = ""tfl.shape""(%260) : (tensor<?x4xf32>) -> tensor<2xi32>
  %262 = ""tfl.strided_slice""(%261, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %263 = ""tfl.minimum""(%262, %cst_7) : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %264 = ""tfl.greater""(%263, %250) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %265 = ""tfl.select""(%264, %250, %263) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %266 = ""tfl.range""(%cst_21, %265, %cst_30) : (tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<?xi32>
  %267 = ""tfl.pack""(%265) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %268 = ""tfl.cast""(%267) : (tensor<1xi32>) -> tensor<1xf32>
  %269 = ""tfl.range""(%cst_21, %263, %cst_30) : (tensor<i32>, tensor<i32>, tensor<i32>) -> tensor<?xi32>
  %270 = ""tfl.gather""(%253, %269) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %271 = ""tfl.gather""(%270, %266) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %272 = ""tfl.shape""(%271) : (tensor<?xf32>) -> tensor<1xi32>
  %273 = ""tfl.strided_slice""(%272, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %274 = ""tfl.sub""(%273, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %275 = ""tfl.greater""(%274, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %276 = ""tfl.select""(%275, %cst_7, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %277 = ""tfl.pack""(%276) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %278 = ""tfl.slice""(%271, %cst_246, %277) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %279 = ""tfl.shape""(%278) : (tensor<?xf32>) -> tensor<1xi32>
  %280 = ""tfl.strided_slice""(%279, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %281 = ""tfl.sub""(%cst_7, %280) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %282 = ""tfl.pack""(%281) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %283 = ""tfl.pack""(%cst_246, %282) {axis = 1 : i32, values_count = 2 : i32} : (tensor<1xi32>, tensor<1xi32>) -> tensor<1x2xi32>
  %284 = ""tfl.pad""(%278, %283) : (tensor<?xf32>, tensor<1x2xi32>) -> tensor<?xf32>
  %285 = ""tfl.pack""(%284) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?xf32>) -> tensor<1x?xf32>
  %286 = ""tfl.gather""(%254, %269) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %287 = ""tfl.gather""(%286, %266) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %288 = ""tfl.shape""(%287) : (tensor<?xf32>) -> tensor<1xi32>
  %289 = ""tfl.strided_slice""(%288, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %290 = ""tfl.sub""(%289, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %291 = ""tfl.greater""(%290, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %292 = ""tfl.select""(%291, %cst_7, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %293 = ""tfl.pack""(%292) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %294 = ""tfl.slice""(%287, %cst_246, %293) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %295 = ""tfl.shape""(%294) : (tensor<?xf32>) -> tensor<1xi32>
  %296 = ""tfl.strided_slice""(%295, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %297 = ""tfl.sub""(%cst_7, %296) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %298 = ""tfl.pack""(%297) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %299 = ""tfl.pack""(%cst_246, %298) {axis = 1 : i32, values_count = 2 : i32} : (tensor<1xi32>, tensor<1xi32>) -> tensor<1x2xi32>
  %300 = ""tfl.pad""(%294, %299) : (tensor<?xf32>, tensor<1x2xi32>) -> tensor<?xf32>
  %301 = ""tfl.pack""(%300) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?xf32>) -> tensor<1x?xf32>
  %302 = ""tfl.cast""(%301) : (tensor<1x?xf32>) -> tensor<1x?xi32>
  %303 = ""tfl.cast""(%302) : (tensor<1x?xi32>) -> tensor<1x?xf32>
  %304 = ""tfl.gather""(%255, %269) {axis = 0 : i32} : (tensor<?x2xf32>, tensor<?xi32>) -> tensor<?x2xf32>
  %305 = ""tfl.gather""(%304, %266) {axis = 0 : i32} : (tensor<?x2xf32>, tensor<?xi32>) -> tensor<?x2xf32>
  %306 = ""tfl.shape""(%305) : (tensor<?x2xf32>) -> tensor<2xi32>
  %307 = ""tfl.strided_slice""(%306, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %308 = ""tfl.sub""(%307, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %309 = ""tfl.greater""(%308, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %310 = ""tfl.select""(%309, %cst_7, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %311 = ""tfl.strided_slice""(%306, %cst_248, %cst_247, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %312 = ""tfl.sub""(%311, %cst_16) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %313 = ""tfl.greater""(%312, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %314 = ""tfl.select""(%313, %cst_16, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %315 = ""tfl.pack""(%310, %314) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %316 = ""tfl.slice""(%305, %cst_9, %315) : (tensor<?x2xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>
  %317 = ""tfl.shape""(%316) : (tensor<?x?xf32>) -> tensor<2xi32>
  %318 = ""tfl.strided_slice""(%317, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %319 = ""tfl.sub""(%cst_7, %318) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %320 = ""tfl.strided_slice""(%317, %cst_248, %cst_247, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %321 = ""tfl.sub""(%cst_16, %320) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %322 = ""tfl.pack""(%319, %321) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %323 = ""tfl.pack""(%cst_9, %322) {axis = 1 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>
  %324 = ""tfl.pad""(%316, %323) : (tensor<?x?xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>
  %325 = ""tfl.pack""(%324) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?x?xf32>) -> tensor<1x?x?xf32>
  %326 = ""tfl.gather""(%256, %269) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %327 = ""tfl.gather""(%326, %266) {axis = 0 : i32} : (tensor<?xf32>, tensor<?xi32>) -> tensor<?xf32>
  %328 = ""tfl.shape""(%327) : (tensor<?xf32>) -> tensor<1xi32>
  %329 = ""tfl.strided_slice""(%328, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %330 = ""tfl.sub""(%329, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %331 = ""tfl.greater""(%330, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %332 = ""tfl.select""(%331, %cst_7, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %333 = ""tfl.pack""(%332) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %334 = ""tfl.slice""(%327, %cst_246, %333) : (tensor<?xf32>, tensor<1xi32>, tensor<1xi32>) -> tensor<?xf32>
  %335 = ""tfl.shape""(%334) : (tensor<?xf32>) -> tensor<1xi32>
  %336 = ""tfl.strided_slice""(%335, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<1xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %337 = ""tfl.sub""(%cst_7, %336) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %338 = ""tfl.pack""(%337) {axis = 0 : i32, values_count = 1 : i32} : (tensor<i32>) -> tensor<1xi32>
  %339 = ""tfl.pack""(%cst_246, %338) {axis = 1 : i32, values_count = 2 : i32} : (tensor<1xi32>, tensor<1xi32>) -> tensor<1x2xi32>
  %340 = ""tfl.pad""(%334, %339) : (tensor<?xf32>, tensor<1x2xi32>) -> tensor<?xf32>
  %341 = ""tfl.pack""(%340) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?xf32>) -> tensor<1x?xf32>
  %342 = ""tfl.add""(%341, %cst_29) {fused_activation_function = ""NONE""} : (tensor<1x?xf32>, tensor<f32>) -> tensor<1x?xf32>
  %343 = ""tfl.gather""(%260, %269) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %344 = ""tfl.gather""(%343, %266) {axis = 0 : i32} : (tensor<?x4xf32>, tensor<?xi32>) -> tensor<?x4xf32>
  %345 = ""tfl.shape""(%344) : (tensor<?x4xf32>) -> tensor<2xi32>
  %346 = ""tfl.strided_slice""(%345, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %347 = ""tfl.sub""(%346, %cst_7) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %348 = ""tfl.greater""(%347, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %349 = ""tfl.select""(%348, %cst_7, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %350 = ""tfl.strided_slice""(%345, %cst_248, %cst_247, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %351 = ""tfl.sub""(%350, %cst_6) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %352 = ""tfl.greater""(%351, %cst_21) : (tensor<i32>, tensor<i32>) -> tensor<i1>
  %353 = ""tfl.select""(%352, %cst_6, %cst_12) : (tensor<i1>, tensor<i32>, tensor<i32>) -> tensor<i32>
  %354 = ""tfl.pack""(%349, %353) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %355 = ""tfl.slice""(%344, %cst_9, %354) : (tensor<?x4xf32>, tensor<2xi32>, tensor<2xi32>) -> tensor<?x?xf32>
  %356 = ""tfl.shape""(%355) : (tensor<?x?xf32>) -> tensor<2xi32>
  %357 = ""tfl.strided_slice""(%356, %cst_246, %cst_248, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %358 = ""tfl.sub""(%cst_7, %357) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %359 = ""tfl.strided_slice""(%356, %cst_248, %cst_247, %cst_248) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<i32>
  %360 = ""tfl.sub""(%cst_6, %359) {fused_activation_function = ""NONE""} : (tensor<i32>, tensor<i32>) -> tensor<i32>
  %361 = ""tfl.pack""(%358, %360) {axis = 0 : i32, values_count = 2 : i32} : (tensor<i32>, tensor<i32>) -> tensor<2xi32>
  %362 = ""tfl.pack""(%cst_9, %361) {axis = 1 : i32, values_count = 2 : i32} : (tensor<2xi32>, tensor<2xi32>) -> tensor<2x2xi32>
  %363 = ""tfl.pad""(%355, %362) : (tensor<?x?xf32>, tensor<2x2xi32>) -> tensor<?x?xf32>
  %364 = ""tfl.pack""(%363) {axis = 0 : i32, values_count = 1 : i32} : (tensor<?x?xf32>) -> tensor<1x?x?xf32>
  ""std.return""(%268, %364, %342, %183, %285, %167, %325, %303) : (tensor<1xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>, tensor<1x51150x2xf32>, tensor<1x?xf32>, tensor<1x51150x4xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>) -> ()
}) {arg0 = {tf_saved_model.index_path = [""input_tensor""]}, result0 = {tf_saved_model.index_path = [""num_detections""]}, result1 = {tf_saved_model.index_path = [""detection_boxes""]}, result2 = {tf_saved_model.index_path = [""detection_classes""]}, result3 = {tf_saved_model.index_path = [""raw_detection_scores""]}, result4 = {tf_saved_model.index_path = [""detection_scores""]}, result5 = {tf_saved_model.index_path = [""raw_detection_boxes""]}, result6 = {tf_saved_model.index_path = [""detection_multiclass_scores""]}, result7 = {tf_saved_model.index_path = [""detection_anchor_indices""]}, sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_input_tensor:0"", outputs = ""StatefulPartitionedCall:5,StatefulPartitionedCall:1,StatefulPartitionedCall:2,StatefulPartitionedCall:7,StatefulPartitionedCall:4,StatefulPartitionedCall:6,StatefulPartitionedCall:3,StatefulPartitionedCall:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor<1x?x?x3xui8>) -> (tensor<1xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>, tensor<1x51150x2xf32>, tensor<1x?xf32>, tensor<1x51150x4xf32>, tensor<1x?x?xf32>, tensor<1x?xf32>)} : () -> ()
```

**Also, please include a link to the saved model or GraphDef**

```
https://cloud.zbe.si/s/ADNzzjizn82WtHe
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)

File isnt generated. The converter just stops after this output. There is more outputed data but it cuts me off in the console. This is everything readable that is outputted. This happens about 10-15mins into the conversion.


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42855,AttributeError: 'BatchDataset' object has no attribute 'ndim',"I have filelists to iterate over for 5-fold cross validation:
```
train_filelists = [ ""more_56_fold_1_train.txt"", ""more_56_fold_2_train.txt"", ""more_56_fold_3_train.txt"", ""more_56_fold_4_train.txt"", ""more_56_fold_5_train.txt"" ]
val_filelists = [""more_56_fold_1_val.txt"", ""more_56_fold_2_val.txt"", ""more_56_fold_3_val.txt"",""more_56_fold_4_val.txt"", ""more_56_fold_5_val.txt""]
```
I am reading the filelist as `train` and `val` sets into a `dataframe` , then defining the columns as `X_u_train_data`, `X_v_train_data` , `train_labels` and the same for `val` set. I have a double input model, therefore, `train_inputs` contains `u` va `v` column values having image paths for each input. I think the code snippet pretty much explains the rest.
```
for train_filelist, val_filelist in zip(train_filelists, val_filelists):
    cols = [""class_id"", ""List_No"", ""u"", ""v""]
    df_train_data = pd.read_csv(train_filelist, sep=""\t"", header=None, names=cols)
    df_train_data['class_id'] = df_train_data['class_id'].map({'more_Abnormal_05': ""abnormal"", 'more_Normal_05': ""normal""})
    df_val_data = pd.read_csv(val_filelist, sep=""\t"", header=None, names=cols)
    df_val_data['class_id'] = df_val_data['class_id'].map({'more_Abnormal_05': ""abnormal"", 'more_Normal_05': ""normal""})

    X_u_train_data = df_train_data.u
    X_v_train_data = df_train_data.v
    train_labels = df_train_data.class_id
                
    train_inputs = tf.data.Dataset.from_tensor_slices((X_u_train_data, X_v_train_data) ).\
           map(lambda X_u_train_data, X_v_train_data: (load(X_u_train_data), load(X_v_train_data)))
    tr_labels = tf.data.Dataset.from_tensor_slices(train_labels).map(lambda train_labels: label(train_labels))
    ds_train = tf.data.Dataset.zip((train_inputs, tr_labels)).batch(64)
    next(iter(ds_train)) # <BatchDataset shapes: (((None, 224, 224, 3), (None, 224, 224, 3)), (None,)), types: ((tf.float32, tf.float32), tf.int32)>

    X_u_val_data = df_val_data.u
    X_v_val_data = df_val_data.v
    validation_labels = df_val_data.class_id
                
    val_inputs = tf.data.Dataset.from_tensor_slices((X_u_val_data, X_v_val_data)).\
          map(lambda X_u_val_data, X_v_val_data: (load(X_u_val_data), load(X_v_val_data)))
    val_labels = tf.data.Dataset.from_tensor_slices(validation_labels).map(lambda validation_labels: label(validation_labels))
    ds_val = tf.data.Dataset.zip((val_inputs, val_labels)).batch(64)
    next(iter(ds_val))

    base_model = combined_net()
    hist = base_model.fit(ds_train,  epochs=EPOCHS,  verbose=1,  validation_data= ds_val, shuffle=True)
```

`load` and `label` functions are as follows:
```
def load(file_path):
    img = tf.io.read_file(file_path)
    img = tf.image.decode_png(img, channels=channels)
    img = tf.image.convert_image_dtype(img, tf.float32)
    img = tf.image.resize(img, size=(img_height, img_width))
    return img

def label(string):
    return tf.cast(tf.equal(string, 'abnormal'), tf.int32)
```

When run, I get the following error

```
AttributeError                            Traceback (most recent call last)
<ipython-input-19-47631c811be4> in <module>
     45 
     46                 base_model = combined_net()
---> 47                 hist = base_model.fit(ds_train,  epochs=EPOCHS,  verbose=1,  validation_data= ds_val, shuffle=True)
     48 
     49                 preds = base_model.predict(ds_val)

~\Anaconda3\lib\site-packages\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
   1152             sample_weight=sample_weight,
   1153             class_weight=class_weight,
-> 1154             batch_size=batch_size)
   1155 
   1156         # Prepare validation data.

~\Anaconda3\lib\site-packages\keras\engine\training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)
    577             feed_input_shapes,
    578             check_batch_axis=False,  # Don't enforce the batch size.
--> 579             exception_prefix='input')
    580 
    581         if y is not None:

~\Anaconda3\lib\site-packages\keras\engine\training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)
     97         data = data.values if data.__class__.__name__ == 'DataFrame' else data
     98         data = [data]
---> 99     data = [standardize_single_array(x) for x in data]
    100 
    101     if len(data) != len(names):

~\Anaconda3\lib\site-packages\keras\engine\training_utils.py in <listcomp>(.0)
     97         data = data.values if data.__class__.__name__ == 'DataFrame' else data
     98         data = [data]
---> 99     data = [standardize_single_array(x) for x in data]
    100 
    101     if len(data) != len(names):

~\Anaconda3\lib\site-packages\keras\engine\training_utils.py in standardize_single_array(x)
     32                 'Got tensor with shape: %s' % str(shape))
     33         return x
---> 34     elif x.ndim == 1:
     35         x = np.expand_dims(x, 1)
     36     return x

AttributeError: 'BatchDataset' object has no attribute 'ndim'
```
``
I have seen some similar issues both here and SO, but could not find a way to solve this issue.

I am trying to solve it on 2 environments: 

**keras=2.2.4-tf, tensorflow=2.1.0, python=3.7.4 on windows 10**

and

**keras=2.2.0, tensorflow-gpu=1.9.0, python=3.5.2 on Ubuntu 16.04**



I hope one could help me. Thank you. "
42854,Try running,"==> I try pip install tensorflow==1.5.0 
it was not working, and then I tried pip install tensorflow-gpu==1.5.0 
it was working. Thanks.
Try running
pip uninstall tensorflow
And then
pip install tensorflow==1.5

EDIT
just to give credit, solution is from here:
https://stackoverflow.com/questions/49094597/illegal-instruction-core-dumped-after-running-import-tensorflow

_Originally posted by @konnerthg in https://github.com/tensorflow/tensorflow/issues/17411#issuecomment-370393493_"
42853,Group Conv3D malloc abort trap 6 ,"I am experiencing the following error on MacOS 10.15 (on two different machines). The same code executes without a problem in Colab.
```
python(19819,0x111fa9dc0) malloc: Incorrect checksum for freed object 0x7fbfe429a5e0: probably modified after being freed.
Corrupt value: 0xbd67c765bd67c765
python(19819,0x111fa9dc0) malloc: *** set a breakpoint in malloc_error_break to debug
Abort trap: 6
```


**System information**
- Python 3.6.9
- Tensorflow 2.3.0 (tf.version.GIT_VERSION = v2.3.0-rc2-23-gb36436b087)
- MacOS 10.15.6

**Describe the current behavior**
Script causes seg fault or abort with malloc error whenever `group`. > 1 in the Conv3D args. The issue seems to arise at the first call of `tape.gradient`. Forward calls `model(x)` cause no issues.

**Describe the expected behavior**
Expect gradients computation. 

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np


input_shape = (4,12,12,12,4)
filters = 4
groups = 2

model = tf.keras.layers.Conv3D(filters, kernel_size=3, strides=1, use_bias=False, groups=groups, padding='same')


for i in range(3):
    with tf.GradientTape() as tape:
        x = np.random.rand(*input_shape).astype(np.float32)
        y = np.random.randint(2, size=(input_shape[0])).astype(np.int32)

        out = model(x)
        out = tf.reduce_mean(out, [1,2,3,4])

        loss = tf.keras.losses.binary_crossentropy(y, out, from_logits=True)

    grads = tape.gradient(loss, model.trainable_variables)
    # raises error here

```
"
42852,Save checkpoints and continue to training,"https://www.tensorflow.org/tutorials/keras/save_and_load

I read this doc to  save checkpionts by tf.keras.callbacks.ModelCheckpoint. 

I want to continue to training from last checkpiont but I don't know how to do . I use these code
```
checkpoint_dir = 'training'
checkpoint_path = os.path.join(checkpoint_dir,""cp-{epoch:04d}.ckpt"")
checkpoint_path = os.path.join(checkpoint_path)
cp_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    save_freq=500,
    myModel=tinydarknet)

model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
model.fit(
    train_ds,
    steps_per_epoch=TRAIN_STEPS_PER_EPOCH,
    epochs=epochs,
    validation_data=valid_ds,
    validation_steps=VAL_STEPS_PER_EPOCH,
    workers=NUM_WORKERS,
    callbacks = [tboard_callback,cp_callback]
)
```
There is a error:
```
Make sure the slot variables are created under the same strategy scope. This may happen if you're restoring from a checkpoint outside the scope
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-1.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-3.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-4.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-5.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-6.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-7.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-8.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-9.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-10.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-11.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-12.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-13.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-14.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-14.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-15.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-16.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-16.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-17.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-18.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-18.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-19.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-20.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-20.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-21.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-22.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-22.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-23.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-24.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-24.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-25.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-26.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-26.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-27.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-28.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-28.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-29.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-30.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-30.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.axis
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.moving_mean
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-31.moving_variance
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-32.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).layer_with_weights-32.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-3.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-4.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-5.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-6.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-7.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-8.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-9.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-10.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-11.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-12.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-13.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-13.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-14.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-14.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-15.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-15.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-16.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-16.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-17.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-17.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-18.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-18.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-19.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-19.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-20.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-20.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-21.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-21.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-22.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-22.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-23.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-23.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-24.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-24.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-25.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-25.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-26.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-26.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-27.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-27.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-28.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-28.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-29.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-29.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-30.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-30.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-31.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-31.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-32.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-32.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-3.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-4.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-5.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-6.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-7.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-8.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-9.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-10.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-11.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-12.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-13.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-13.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-14.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-14.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-15.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-15.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-16.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-16.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-17.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-17.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-18.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-18.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-19.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-19.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-20.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-20.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-21.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-21.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-22.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-22.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-23.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-23.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-24.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-24.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-25.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-25.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-26.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-26.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-27.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-27.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-28.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-28.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-29.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-29.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-30.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-30.bias
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-31.gamma
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-31.beta
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-32.kernel
WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-32.bias
WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.
```"
42851,Is it possible to adjust available GPU during training?,"Is it possible to adjust available GPU during training?

Think about this scenario, during traing, the user think one GPU card is too slow to training, and want to add another card to speed up the training. So Is it possible to adjust available GPU without breaking the training？

"
42850,"Android app keep crashing when executing `interpreter.run(input, output)`","**System information** `(for ML model creation and training)`
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 20.04`
- TensorFlow installed from (source or binary): `binary`
- TensorFlow version (use command below): `(tf.version.GIT_VERSION: v2.3.0-rc2-23-gb36436b087, tf.version.VERSION: 2.3.0)`
- Python version: `3.8.2`

**System information** `(for running android app)`
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux Ubuntu 16.04`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `Samsung: Galaxy J7`
- TensorFlow installed from (source or binary): `binary`
- TensorFlow version (use command below): `org.tensorflow:tensorflow-lite:0.0.0-nightly, org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly`
- Android SDK version: `minSdkVersion = 21`, `targetSdkVersion = 29`, `compileSdkVersion = 29`
- Java version: `1.8`
- Kotlin version: `1.3.72-release-Studio4.0-5`


**Describe the current behavior**
- Using Python API: tflite model is able to load and correctly give output
- Using Java API (When used in Android app): tflite model is able to load, gives correct i/p and o/p datatype and shape but it crashes the app when `interpreter.run(input, output)` gets executed

**Describe the expected behavior**
- Using Java API (When used in Android app): it should give the same response as in the case of python API instead of crashing the app


**Standalone code to reproduce the issue** `(tflite model creation and inference using python API)`
```
# tflite model creation:
import numpy as np
import tensorflow as tf

input_names = [""my_input""]
output_names = [""my_output""]
tf_model_path = ""../models/r2plus1d.pb""

ip_shape = tf.TensorShape([1, 3, 8, 112, 112])

converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(tf_model_path, input_names, output_names)
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()

r2plus1d_tflite_file = ""../models/r2plus1d.tflite""
with open(r2plus1d_tflite_file, ""wb"") as file:
    file.write(tflite_model)

---------------------------------------------------------------------------------------------
# inference using tflite model:
tflite_model_path = ""../models/r2plus1d.tflite""

interpreter = tf.lite.Interpreter(model_path=tflite_model_path)
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)

interpreter.invoke()

output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
```

**Code to reproduce the issue** `(Android app code snippet)` most of its logic has been copied from tensorflow/examples/android
```
package com.myproject.tensorflow

import android.content.res.AssetManager
import android.graphics.Bitmap
import com.google.gson.Gson
import com.myproject.extension.AppLogger
import org.tensorflow.lite.DataType
import org.tensorflow.lite.Interpreter
import java.io.FileInputStream
import java.io.IOException
import java.nio.ByteBuffer
import java.nio.ByteOrder
import java.nio.MappedByteBuffer
import java.nio.channels.FileChannel

class TensorFlowImageClassifier private constructor() : Classifier {
    private var counter = 0
    private var isRunning: Boolean = false
    private var interpreter: Interpreter? = null
    override fun recognizeImageForObjectDetection(bitmap: Bitmap): Boolean {
        return false
    }

    if(!isRunning) {
        AppLogger.printLog(""action isRunning : ""+counter++)
        isRunning = true
        AppLogger.printLog("" launch recognised"")
        // val byteBuffer = convertBitmapToByteBufferFinal(bitmap)
        val input = Array(1) {Array(3 ) {Array(8) {Array(112) { FloatArray(112) }}}}
        val result = Array(1) { FloatArray(11) }
        try {
            interpreter?.run(byteBuffer, result)
            // interpreter?.run(input, result)
        } catch (e: Exception) {
            e.printStackTrace()
        }
        AppLogger.printLog(""action output"")
        AppLogger.printLog(""action recognised output outer "" + Gson().toJson(result))
    }

    override fun close() {
        interpreter!!.close()
        interpreter = null
    }

    @Throws(IOException::class)
    private fun loadModelFile(
        assetManager: AssetManager,
        modelPath: String
    ): MappedByteBuffer {
        val fileDescriptor = assetManager.openFd(modelPath)
        val inputStream =  FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        val startOffset = fileDescriptor.startOffset
        val declaredLength = fileDescriptor.declaredLength
        return fileChannel.map(  FileChannel.MapMode.READ_ONLY, startOffset, declaredLength)
    }

    private fun convertBitmapToByteBufferFinal(bitmap: Bitmap): ByteBuffer {
        val byteBuffer: ByteBuffer = ByteBuffer.allocateDirect(4 * 1 * 3 * 8 * 112 * 112) // byte
        byteBuffer.order(ByteOrder.nativeOrder())
        for (m in 0 until 3) {
            for (i in 0 until 8) {
                for (j in 0..111) {
                    for (k in 0..111) {
                        byteBuffer.putFloat(0.5f)
                    }
                }
            }
        }
        return byteBuffer
    }

    companion object {
        private lateinit var mAassetManager: AssetManager
        private lateinit var mClassifier: TensorFlowImageClassifier
        private lateinit var mModelPath: String
        lateinit var imageShape: IntArray
        var probabilityDataType: DataType? = null
        var inputDataType: DataType? = null
        lateinit var outputShape: IntArray
        var inputBatchSize = 0
        var imageSizeY = 0
        var imageSizeX = 0
        var pixelValue = 0

        @Throws(IOException::class)
        fun create( assetManager: AssetManager, modelPath: String): Classifier {
            val classifier = TensorFlowImageClassifier()
            classifier.interpreter = getInterpreter(classifier, assetManager, modelPath)
            mAassetManager = assetManager
            mModelPath = modelPath
            mClassifier = classifier
            val probabilityTensorIndex = 0
            val imageTensorIndex = 0
            imageShape = classifier.interpreter!!.getInputTensor(imageTensorIndex).shape() // {1, 3, 8, 112, 112}
            pixelValue = imageShape[1]
            inputBatchSize = imageShape[2]
            imageSizeX = imageShape[3]
            imageSizeY = imageShape[4]
            probabilityDataType = classifier.interpreter!!.getOutputTensor(probabilityTensorIndex).dataType() //FLOAT32
            inputDataType = classifier.interpreter!!.getInputTensor(probabilityTensorIndex).dataType() //FLOAT32
            outputShape = classifier.interpreter!!.getOutputTensor(imageTensorIndex).shape() // {1, 11}
            return classifier
        }

        private fun getInterpreter(
            classifier: TensorFlowImageClassifier,
            assetManager: AssetManager,
            modelPath: String
        ): Interpreter {
            return Interpreter(
                classifier.loadModelFile(assetManager, modelPath),
                Interpreter.Options()
            )
        }
    }
}
```


**Other info / logs**
- [R(2+1)D architecture for action recognition in videos](https://arxiv.org/pdf/1905.00561.pdf)
- [tf model (r2plus1d.pb)](https://drive.google.com/file/d/1in71gffCtQf-eYrx7jz00ZC2IDGbBcVA/view?usp=sharing) - size is around 243MB, used it to convert to tflite
- [tflite model (r2plus1d.tflite)](https://drive.google.com/file/d/1-cUsecwb2HOD0YOFdrhX6BBcRgPHcjUF/view?usp=sharing) - size is around 242MB
- [visualize tflite model (r2plus1d.tflite.svg)](https://drive.google.com/file/d/1Rf8eXHbFH9hGdfwoSI6Jd4xy-KWq64oM/view?usp=sharing) - size is around 1MB
- i/p shape: (1, 3, 8, 112, 112) -> (batch_size, channels, num_of_frames, img_height, img_width)
- o/p shape: (1, 11) -> (num_of_classes)"
42848,Add Transformer Layers to tf.keras.layers,"Nowadays most of NLP researchers/practitioners uses Transformers instead of LSTMs for NLP, it would make sense to add some of the layers used in the transformer architecture to the Keras API to avoid third-party libs or a lot of rewriting of code, The API could be similar to this tutorial: https://medium.com/tensorflow/a-transformer-chatbot-tutorial-with-tensorflow-2-0-88bf59e66fe2

So the layers being EncoderLayer, DecoderLayer, Encoder, Decoder, MultiHeadAttention(which is already in beta), PositionalEncoding, LookAheadMask,  ScaledDotProduct, FeedForward and Transformer.

Thanks in advance."
42845,AttributeError: 'NoneType' object has no attribute 'python_grad_func' when using @tf.function(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b
- TensorFlow version (use command below):
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
- Python version: 3.6

**Describe the current behavior**

Getting AttributeError: 'NoneType' object has no attribute 'python_grad_func' when decorating functions with @tf.function()

**Describe the expected behavior**
No error

**Standalone code to reproduce the issue**

```python
import tensorflow as tf
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))

@tf.function()
def G(x, c):
    xx = tf.transpose(a=x[:, :, None], perm=[0, 2, 1])
    cc = tf.transpose(a=c[:, :, None], perm=[2, 0, 1])
    G = tf.reduce_sum(input_tensor=tf.square(xx - cc), axis=2)
    return G

@tf.function()
def ff(x, c , v):
    with tf.GradientTape(persistent=True) as t1:
        t1.watch(x)
        with tf.GradientTape(persistent=True) as t2:
            t2.watch(x)
            p = tf.reduce_sum(input_tensor=G(x,c) * v, axis=1, keepdims=True)
        f = t2.batch_jacobian(p, x, experimental_use_pfor=False)[:,:,0]
    f_w = t1.batch_jacobian(f, x, experimental_use_pfor=False)[:,:,0]
    return f_w

x = tf.constant([[1.],[2.]])
c = tf.ones((2,1))
v = tf.ones((1,2))

with tf.GradientTape() as tape:
    tape.watch(v)
    o = tf.reduce_sum(tf.square(ff(x, c, v)))
g = tape.gradient(o, v)
print(g)


```

**Other info / logs** 

```
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2486, in get_attr
    pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'while_1' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 331, in _MaybeCompile
    xla_compile = op.get_attr(""_XlaCompile"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2490, in get_attr
    raise ValueError(str(e))
ValueError: Operation 'while_1' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2486, in get_attr
    pywrap_tf_session.TF_OperationGetAttrValueProto(self._c_op, name, buf)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'gradient_tape/while/while_grad' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 331, in _MaybeCompile
    xla_compile = op.get_attr(""_XlaCompile"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2490, in get_attr
    raise ValueError(str(e))
ValueError: Operation 'gradient_tape/while/while_grad' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 607, in _GradientsHelper
    grad_fn = ops.get_gradient_function(op)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 2655, in get_gradient_function
    return _gradient_registry.lookup(op_type)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/registry.py"", line 97, in lookup
    ""%s registry has no entry for: %s"" % (self._name, name))
LookupError: gradient registry has no entry for: PartitionedCall

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/mnt/MultiPhaseModel_PINN/tests/issue_xla.py"", line 29, in <module>
    o = tf.reduce_sum(tf.square(ff(x, c, v)))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py"", line 846, in _call
    return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1929, in _call_flat
    forward_function, args_with_tangents = forward_backward.forward()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1433, in forward
    self._inference_args, self._input_tangents)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1189, in forward
    self._forward_and_backward_functions(inference_args, input_tangents))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 1341, in _forward_and_backward_functions
    outputs, inference_args, input_tangents)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py"", line 899, in _build_functions_for_outputs
    src_graph=self._func_graph)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 669, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 336, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 669, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 354, in _WhileGrad
    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 626, in _create_grad_func
    body_graph_inputs, body_graph_outputs))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 622, in <lambda>
    lambda *args: _grad_fn(ys, xs, args, body_graph),
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 682, in _grad_fn
    unconnected_gradients=""zero"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 669, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 336, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 669, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 354, in _WhileGrad
    util.unique_grad_fn_name(body_graph.name), op, maximum_iterations)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 626, in _create_grad_func
    body_graph_inputs, body_graph_outputs))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py"", line 986, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 622, in <lambda>
    lambda *args: _grad_fn(ys, xs, args, body_graph),
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/while_v2.py"", line 682, in _grad_fn
    unconnected_gradients=""zero"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gradients_util.py"", line 619, in _GradientsHelper
    grad_fn = func_call.python_grad_func
AttributeError: 'NoneType' object has no attribute 'python_grad_func'

Process finished with exit code 1

```


"
42844,Can's Training by using the data on external SSD,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: macOS Catalina
- Mobile device if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

### Bug Summary
I am training an image classifier on the local machine, it works ok when the data is on the local machine, however, when I tried the data which on my external SSD, error pops out. The data on external SSD can be read and decoded separately by using the following code:
```
import tensorflow as tf
IMG_HEIGHT = 90
IMG_WIDTH = 160
path = ""/Volumes/External-SSD/dataset/labeled/gaming/Fresh_68799179400006397.bmp""

def load_and_preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_bmp(img, channels=3)
    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])
    img /= 255.0  # normalize to [0,1] range
    return img

load_and_preprocess_image(path)
```

### My code

- Dataloader
```
class DataSetGenerator():
    def __init__(self, IMG_HEIGHT, IMG_WIDTH):
        self.IMG_HEIGHT = IMG_HEIGHT
        self.IMG_WIDTH = IMG_WIDTH

    def load_and_preprocess_image(self, path):
        img = tf.io.read_file(path)
        img = tf.image.decode_bmp(img, channels=3)
        img = tf.image.resize(img, [self.IMG_HEIGHT, self.IMG_WIDTH])
        img /= 255.0  # normalize to [0,1] range
        return img

    def prepare_for_training(self, ds, cache=True):
        if cache:
            if isinstance(cache, str):
                ds = ds.cache(cache)
        else:
          ds = ds.cache()

        ds = ds.shuffle(buffer_size=self.SHUFFLE_SIZE)

        ds = ds.batch(self.BATCH_SIZE)

        # `prefetch` lets the dataset fetch batches in the background while the model
        # is training.
        ds = ds.prefetch(buffer_size=self.AUTOTUNE)

        return ds

    def labeled_dataset(self, image_paths, labels):
        # a dataset that returns image paths
        path_ds = tf.data.Dataset.from_tensor_slices(image_paths)

        # a dataset that returns images (loaded off disk, decoded, and preprocessed)
        image_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=self.AUTOTUNE)

        # a dataset that returns labels
        label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64))

        # a dataset that returns images and labels
        image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))

        return image_label_ds

    def get_final_dataset(self, data_root, SHUFFLE_SIZE, BATCH_SIZE):
        self.SHUFFLE_SIZE = SHUFFLE_SIZE
        self.BATCH_SIZE = BATCH_SIZE
        self.AUTOTUNE = tf.data.experimental.AUTOTUNE

        # a dataset that returns image paths
        data_root = pathlib.Path(data_root)
        all_image_paths = list(data_root.glob('*/**/*'))
        all_image_paths = [str(path) for path in all_image_paths if str(path).lower().endswith(""bmp"")]
                                                
        random.shuffle(all_image_paths)

        self.label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())
        label_to_index = dict((name, index) for index, name in enumerate(self.label_names))
        all_labels = [label_to_index[path.split('/')[-2]]
                        for path in all_image_paths]

        # separate dataset
        train_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels)

        train_ds = self.labeled_dataset(train_paths, train_labels)
        train_ds = self.prepare_for_training(train_ds)

        test_ds = self.labeled_dataset(test_paths, test_labels)
        test_ds = self.prepare_for_training(test_ds)

        return train_ds, test_ds
```
- Model:
```
def GamingModel(IMG_HEIGHT, IMG_WIDTH, category_num):
    model = models.Sequential()
    model.add(layers.Conv2D(32, (3, 3), activation='relu', 
                            input_shape=(IMG_HEIGHT, IMG_WIDTH, 3)))
    model.add(layers.MaxPooling2D())
    model.add(layers.Conv2D(64, (3, 3), activation='relu'))
    model.add(layers.MaxPooling2D())
    model.add(layers.Flatten())
    model.add(layers.Dropout(rate=0.2))
    model.add(layers.Dense(category_num, activation='softmax'))
    model.compile(optimizer='adam',
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
    model.summary()

    return model
```
- Train:
```
from dataloader import DataSetGenerator
from models import GamingModel
from datetime import datetime

IMG_HEIGHT = 90
IMG_WIDTH = 160

data_root = ""/Volumes/External-SSD/dataset/labeled""
dsg = DataSetGenerator(IMG_HEIGHT, IMG_WIDTH)
train_ds, test_ds = dsg.get_final_dataset(data_root, SHUFFLE_SIZE=4000, BATCH_SIZE=32)

category_num = len(dsg.label_names)

model = GamingModel(IMG_HEIGHT, IMG_WIDTH, category_num)
es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, mode='auto')
history = model.fit(train_ds, validation_data=test_ds, epochs=10, callbacks=[es_callback])

now = datetime.now()
now = now.strftime(""%Y%m%d-%H%M"")
model.save(f""../endpoints/game_state_{now}.h5"")
```

### Log
```
2020-08-31 17:33:39.847255: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-31 17:33:39.860848: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd061d24690 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-31 17:33:39.860873: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Model: ""sequential""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d (Conv2D)              (None, 88, 158, 32)       896
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 44, 79, 32)        0
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 42, 77, 64)        18496
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 21, 38, 64)        0
_________________________________________________________________
flatten (Flatten)            (None, 51072)             0
_________________________________________________________________
dropout (Dropout)            (None, 51072)             0
_________________________________________________________________
dense (Dense)                (None, 12)                612876
=================================================================
Total params: 632,268
Trainable params: 632,268
Non-trainable params: 0
_________________________________________________________________
Epoch 1/10
Traceback (most recent call last):
  File ""train.py"", line 24, in <module>
    history = model.fit(train_ds, validation_data=test_ds, epochs=10, callbacks=[es_callback])
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1848, in _filtered_call
    cancellation_manager=cancellation_manager)
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 1924, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 550, in call
    ctx=ctx)
  File ""/Users/macuser/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  channels attribute 3 does not match bits per pixel from file 288
	 [[{{node DecodeBmp}}]]
	 [[IteratorGetNext]] [Op:__inference_train_function_755]

Function call stack:
train_function
```"
42841,org.tensorflow:tensorflow:jar:2.3.0 not found via Maven,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OSX 10.15.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.15.0 (current), 2.3.0 (future)
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
Unable to resolve `tensorflow` Java artifact via Maven at version 2.3.0, as per official [documentation](https://www.tensorflow.org/install/lang_java#tensorflow_with_apache_maven).

**Provide the exact sequence of commands / steps that you executed before running into the problem**
In my project's pom.xml file, modified `${tensorflow.version}` value in following snippet from 1.15.0 to 2.3.0 as per official [documentation](https://www.tensorflow.org/install/lang_java#tensorflow_with_apache_maven) and re-ran my project's build (which works with 1.15.0, as expected):

```
    <dependency>
      <groupId>org.tensorflow</groupId>
      <artifactId>tensorflow</artifactId>
      <version>${tensorflow.version}</version>
    </dependency>
```

**Any other info / logs**

```
Could not find artifact org.tensorflow:tensorflow:jar:2.3.0 in maven-central (https://repo1.maven.org/maven2)
```"
42838,Flags For activating AVX Bazel Build CPP Windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**

I successfully managed to build a TensorFlow library for CPP, Windows, but that was without AVX instructions. I want to enable the AVX instructions.  I want to take advantage of it and improve my inference speed.

**Provide the exact sequence of commands/steps that you executed before running into the problem**
I tried the following flags, but during the build, it doesn't recognize it. Looks like that is only valid for Python package.
- bazel build -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.2 

**Any other info / logs**: Error/Warning on the build
cl : Command line warning D9002 : ignoring unknown option '-mavx'
cl : Command line warning D9002 : ignoring unknown option '-mavx2'
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42835,tf.GradientTape().jacobian() triggers retracing ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  dockerhub container 'latest' Digest: 7bc36fe0ca1a051a808122e87f5438614b371263515df4794abef9a78440af8b
- TensorFlow version (use command below):
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
- Python version: 3.6

**Describe the current behavior**

The use of tf.GradientTape().jacobian() triggers retracing

**Describe the expected behavior**
No retracing

**Standalone code to reproduce the issue**

simple example:

```python
import tensorflow as tf
print('tf.version.GIT_VERSION={}'.format(tf.version.GIT_VERSION))
print('tf.version.VERSION={}'.format(tf.version.VERSION))

x = tf.ones((1,1))
for k in range(0, 10):
    with tf.GradientTape() as tape:
        tape.watch(x)
        y = 2*x
    g = tape.jacobian(y, x)
```

**Other info / logs** 

```
tf.version.GIT_VERSION=v2.3.0-rc2-23-gb36436b087
tf.version.VERSION=2.3.0
WARNING:tensorflow:5 out of the last 5 calls to <function pfor.<locals>.f at 0x7f89839c8e18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
```


"
42834,Nan Loss with Adam in MultiWorkerMirroredStrategy ,"Hello 
I have a custom training loop that uses MultiWorkerMirroredStrategy. I use tf.keras.losses.SparseCategoricalCrossentropy as the loss function and tf.keras.optimizers.Adam as the optimizer. I noticed that when I train the network with 4 nodes (each has 1 P100 16GB GPU) using gRPC communication layer, loss becomes NaN after some steps. With the same code, this time I use only 1 node with 4 V100 32GB GPUs and training works smoothly without any issue. For example:
```
### with 4 nodes the lost sharply drops with few steps
Step      10 ( 3.21%): loss = 3.98384 EER=0.38987 
Step      20 ( 6.41%): loss = 3.71272 EER=0.30024 
Step      30 ( 9.62%): loss = 3.50568 EER=0.25225 
Step      40 (12.82%): loss = 3.32774 EER=0.21305 
Step      50 (16.03%): loss = 3.19242 EER=0.19232 
Step      60 (19.23%): loss = 3.09014 EER=0.17402 
Step      80 (23.64%): loss = 0000nan EER=0.00000
```
```
with 1 node and 4 GPUs is working fine
Step       0 ( 0.00%): loss = 4.00920 EER=0.40084 
Step      10 ( 0.83%): loss = 3.92024 EER=0.38026 
Step      20 ( 1.67%): loss = 3.78321 EER=0.34209 
Step      30 ( 2.50%): loss = 3.77414 EER=0.33503 
Step      40 ( 3.33%): loss = 3.70165 EER=0.31749 
Step      50 ( 4.17%): loss = 3.65104 EER=0.30819 
Step      60 ( 5.00%): loss = 3.64083 EER=0.30599 
Step      70 ( 5.83%): loss = 3.58572 EER=0.29448 
Step      80 ( 6.67%): loss = 3.57196 EER=0.29532 
```

Could the problem be the GPUs? or the communication layer?
I also tested with low learning rate and did not help with Adam optimizer.

Thanks 

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: 'Red Hat Enterprise Linux Server', '7.7'
- TensorFlow installed from: binary
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version:  Python 3.7.7
- CUDA/cuDNN version: 10.1
- GPU model and memory: P100 16 GB and V100 32GB






"
42832,tflite(micro) resize_nearest_neightbour_test fails for BUILD_TYPE=debug,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (SHA-1 source): 44dbace063796ea99c5b3d27a3a5810048d5096c


**Describe the current behavior**
Test fails assertions in kernel implementation for BUILD_TYPE=debug build due to size tensor being provided as 2D non-constant tensor.

**Describe the expected behavior**

Test should pass.

**Standalone code to reproduce the issue**

make -f tensorflow/lite/micro/tools/make/Makefile BUILD_TYPE=debug test_kernel_resize_nearest_neighbor_test

Small unified diff patch to correctly setup the size input tensor as 1D constant to correct the issue is attached.

[resize_nearest_neighbor_test.patch.txt](https://github.com/tensorflow/tensorflow/files/5151701/resize_nearest_neighbor_test.patch.txt)
"
42831,Error while installing latest tensorflow version 2.3,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Nil
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.3.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): Nil
- GCC/Compiler version (if compiling from source): Anaconda prompt
- CUDA/cuDNN version: Nil
- GPU model and memory: 64 bit CPU



<b>Python 3.7.8 | packaged by conda-forge | (default, Jul 31 2020, 01:53:57) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\dell\Anaconda3\envs\ox\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>></b>

"
42830,Better Instruction/Insight as to how models.load_model() custom objects are loaded (doc/feature request/bug?),"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:
[https://www.tensorflow.org/api_docs/python/tf/keras/models/load_model?hl=no]()

It may be better to have a separate URL for new documentation.

## Description of issue (what needs changing):
This could be treated potentially as a feature request and/or bug as the issues encountered may or may not be a result of things out of my control. Better documentation is requested regardless.

The documentation on how to load models that contain custom objects is lacking as to prerequisites to make them loadable.

The only thing that is clear is that a dict of the objects of interest must be provided. It isn't clear what they keys are supposed to be. It is also unclear as to how the objects are loaded and what happens at load time.

When running this code

```
    def load_model(self, model_name, model_path):
        """"""Reload tensorflow session for saved model. Called by djinn.load,

        Args: 
            model_path (str, optional): Location of model if different than 
                       location set during training.
            model_name (str, optional): Name of model if different than 
                       name set during training.
        
        Returns: 
            Object: djinn regressor model.
            
        """"""
        # self.__sess = {}
        self.__models={}
        for p in range(0, self.__n_trees):
            tf.keras.backend.clear_session()
            # tf.reset_default_graph()
            # new_saver = \
            # tf.train.import_meta_graph('%s%s_tree%s.ckpt.meta'%(model_path,model_name,p))
            # self.__sess[p] = tf.Session()
            # new_saver.restore(self.__sess[p], '%s%s_tree%s.ckpt'%(model_path,model_name,p))
            self.__models[p] = tf.keras.models.load_model('%s%s_tree%s.ckpt'%(model_path,model_name,p),custom_objects={'WB_Init': WB_Init})
            # self.__models[p] = tf.keras.models.load_model('%s%s_tree%s.ckpt'%(model_path,model_name,p))
            print(""Model %s restored""%p)
```

where WB_Init is defined in another file as

```
class WB_Init(tf.keras.initializers.Initializer):
    def __init__(self,dat=None,name=None):
        self.dat = dat
        self.name = name
        # print(type(dat))
        # print(dat)
                
    def __call__(self,shape,dtype):
        
        if not isinstance(self.dat,tf.Tensor):
            a = tf.convert_to_tensor(self.dat,dtype=tf.float32,name=self.name)
        else:
            # a = self.dat.value()
            a = self.dat
                    
        return a
```

and the file doing the calling is indeed loading the object with
```
try:
    from djinn_fns import tree_to_nn_weights, tf_dropout_regression, \
                    get_hyperparams, tf_continue_training, WB_Init
except:
    from djinn.djinn_fns import tree_to_nn_weights, tf_dropout_regression, \
                    get_hyperparams, tf_continue_training, WB_Init
```

WB_Init is used to initialize weights and biases that have been predefined by other functions and need to be trainable. I have avoided the constant initializer because there is no immediate documentation to suggest the values produced by it are capable of being trained (another documentation clarification maybe).

When trying load in an already trained model I'm greeted with the error traceback:

```
Traceback (most recent call last):

  File ""C:\Users\Michael\Desktop\Code Projects\DJINN TF2\tests\djinn_regression_example.py"", line 81, in <module>
    m=model.predict(x_test) #returns the median prediction if more than one tree

  File ""..\djinn\djinn.py"", line 383, in predict
    return self.bayesian_predict(x_test, None, random_state)

  File ""..\djinn\djinn.py"", line 341, in bayesian_predict
    if(self.__models == None): self.load_model(self.modelname, self.modelpath)

  File ""..\djinn\djinn.py"", line 294, in load_model
    self.__models[p] = tf.keras.models.load_model('%s%s_tree%s.ckpt'%(model_path,model_name,p),custom_objects={'WB_Init': WB_Init})

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 182, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\saving\hdf5_format.py"", line 178, in load_model_from_hdf5
    custom_objects=custom_objects)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\saving\model_config.py"", line 55, in model_from_config
    return deserialize(config, custom_objects=custom_objects)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\layers\serialization.py"", line 175, in deserialize
    printable_module_name='layer')

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py"", line 358, in deserialize_keras_object
    list(custom_objects.items())))

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\functional.py"", line 617, in from_config
    config, custom_objects)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\functional.py"", line 1214, in reconstruct_from_config
    process_node(layer, node_data)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\functional.py"", line 1162, in process_node
    output_tensors = layer(input_tensors, **kwargs)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py"", line 757, in __call__
    self._maybe_build(inputs)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py"", line 2098, in _maybe_build
    self.build(input_shapes)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\layers\core.py"", line 1178, in build
    trainable=True)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer_v1.py"", line 448, in add_weight
    caching_device=caching_device)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\training\tracking\base.py"", line 750, in _add_variable_with_custom_getter
    **kwargs_for_getter)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\keras\engine\base_layer_utils.py"", line 145, in make_variable
    shape=variable_shape if variable_shape else None)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 260, in __call__
    return cls._variable_v1_call(*args, **kwargs)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 221, in _variable_v1_call
    shape=shape)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 199, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2597, in default_variable_creator
    shape=shape)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1518, in __init__
    distribute_strategy=distribute_strategy)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py"", line 1651, in _init_from_args
    initial_value() if init_from_fn else initial_value,

  File ""..\djinn\djinn_fns.py"", line 873, in __call__
    a = tf.convert_to_tensor(self.dat,dtype=tf.float32,name=self.name)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1382, in convert_to_tensor_v2
    as_ref=False)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1499, in convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 338, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 264, in constant
    allow_broadcast=True)

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py"", line 282, in _constant_impl
    allow_broadcast=allow_broadcast))

  File ""C:\Users\Michael\anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py"", line 444, in make_tensor_proto
    raise ValueError(""None values not supported."")

ValueError: None values not supported.
```
where it is clear as to the ""precise reason"" why load is failing. That reason is because the WB_Init.__init__ has a default `dat=None`
which when WB_Init.__call__ is used returns a None value which isn't a support return type for the initialization process. This is what happens when saved to HDF5 format. The tensorflow format breaks in a different manner at save time with the final error being `FailedPreconditionError: ./reg_djinn_test_tree0.ckpt is not a directory`. 

However, it clearly demonstrates that a fresh call of a WB_Init object is made at load time which doesn't make sense to me because the model has already been trained. It should have weights to load. The initializer shouldn't be run as far as I know. Setting `compile=False` doesn't fix this problem. If I just wanted to load the model for inference, I could potentially try to completely reconstruct the model from some other pickled custom object that hold the layer topology and then just use load_weights, but this is a horribly dirty solution. Using this workaround also doesn't let me save the optimizer state so that the model training can be picked-up at the most recent optimizer state.

There is another hypothetical fix where WB_Init is fed the saved weights at load time, but I would have no idea of how to make this happen.

There is also the highly non-preferred way of initializing the network at construction by filling it the models with random numbers and loading the pre-calculated weights after construction (and compilation?) of the model. This defeats the purpose of providing an initializer option at construction time though.

### Clear description

Please provide more a more clear description about how to make custom intializers (or any custom object ) and, more importantly, how to load custom objects (dict keys?) and the prerequisites for making the model and/or object loadable.

This problem and problems like this have been recurring. Some of them don't make sense to me, some are TF1 and don't apply, and some simply don't fix my issue. Some have simply been bugs.

[https://github.com/tensorflow/tensorflow/issues/39752]() ( this is the one that actually demonstrated how to use the 'custom_object' option )
[https://github.com/tensorflow/tensorflow/issues/42778]()
[https://github.com/qqwweee/keras-yolo3/issues/48]()  (referenced in the link above)
[https://github.com/tensorflow/tensorflow/issues/33646]()
[https://github.com/titu1994/keras-efficientnets/issues/3]()
[https://github.com/tensorflow/tensorflow/issues/26061]()

It clearly could use better documentation.

I will provide a full script upon request, and if it is possible, I'd prefer to supply it privately. I'm currently trying to convert a TF1/(TF2 with compatibility) model to a TF2 model that uses Keras. It will (almost certainly) break even if this problem is fixed because the conversion is in progress. The break should be different in nature though if this problem is fixed.


### Correct links

N/A

### Parameters defined

N/A

### Returns defined

N/A

### Raises listed and defined

N/A

### Usage example

See above, but it isn't usable without the rest of the files.


### Submit a pull request?
No
"
42827,tflite suppport for LSTM / RNN,Per [tflite roadmap](https://www.tensorflow.org/lite/guide/roadmap#usability) there is upcomming support forLSTM / RNN - when we can expect this for tflite?
42826,tflite delagte to nn accelerator,"I'm looking to create or contribute to NN delegator which is applicable to NPU, is there any open-source project for delegator which can be (re)used with tflite? I'm not so much interested in GPU, Hexagon DSP etc..."
42825,Custom loss using intermediate tensorflow probability layer with Eager execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, a colab [here](https://colab.research.google.com/drive/1nRvN-edSeP1q1Q2lCCwXm3RV7Yunxh--?usp=sharing).
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Locally, Ubuntu 20.04, otherwise current colab.
- TensorFlow installed from (source or binary): See colab.
- TensorFlow version (use command below): v2.3.0-0-gb36436b087
- Python version: Locally, 3.8.2

**Describe the current behavior**
When using a custom loss based on tensorflow probability `DistributionLambda`, or a distribution without `DistributionLambda`, I cannot move into Eager execution. In the first case, the gradients are not propagated properly, so variables say they have no gradients when training, and, in the second case, the layers are symbolic tensors when creating the network, so they cannot be used in the Eager execution loss.

**Describe the expected behavior**
I understand the problem. What I would like is a way to have the intermediate values within the network to be used in the loss. A regularization loss is not enough, as I need the outputs of the whole network, together with the intermediate layer values in order to compute the loss.

**Standalone code to reproduce the issue**
A [colab](https://colab.research.google.com/drive/1nRvN-edSeP1q1Q2lCCwXm3RV7Yunxh--?usp=sharing).

**Other info / logs**
Please see the attached colab. A useful piece of information is that I am using a class to create the model because this is part of a bigger project, and the network is much bigger, so while in the example I showed the problem in one of the last layers, this could happen for a layer that is in the middle of the network, and I would not like to add all intermediate layers as outputs of the model.
"
42824,tf.compat.v1.data.get_output_shapes(datasets) gives Errors,"## Iam getting different types of errors in different projects whenever I use tf.compat.v1.data.get_output_shapes(datasets)
I have tried all the Codes in Colab only.
</em>

**System information**
### COLAB

1. Tensorflow Version :  Latest on Colab : 2.3.0
2. Python 3
3. COLAB GPU


**Current behavior :**
While using **tf.compat.v1.data.get_output_shapes(datasets), I got 2 types of errors in 2 projects:**
1. ValueError: logits and labels must have the same shape ((None, 1) vs ())
2. ValueError: Input 0 of layer global_average_pooling1d is incompatible with the layer: expected ndim=3, found ndim=4. Full shape received: [None, None, None, 128]

**As a temporary solution, I used datasets.output_shape** & I got the desired output.

*But whenever I use datasets.output_shape , I get a warning as below:*
WARNING:tensorflow:From <ipython-input-3-3cfb74cd1ccb>:3: DatasetV1.output_shapes (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.data.get_output_shapes(dataset)`.


**Request to the Tensorflow Team:**
I kindly request the Tensorflow team to look into this issue and help me solve it.

**Thank you Tensorflow Team in Advance...!!!**

[Colab file Link](https://colab.research.google.com/drive/1m9yF3cXQGTc1xNLCgBVx0BdcHVnOctbz?usp=sharing)

"
42820,"Tensorflow only using a fraction of the GPU power available (NLP model, dual GPU, tf.data pipeline) ","Hi!

Somewhat recently I got a new training server which is really fast, but I'm currently having trouble utilizing it's GPU and CPU to it's full potential when training my model.

I'm training an NLP classification model with a string as input and a category as target. When I set the batch size to a reasonally small number, like 16 or 32, only around 10% of each of the 2 GPUs as well as of the CPU are used. Only when I size the batches up to 4096, CPU gets close to a 100% load but GPUs still only hit 7-8%. Training is really fast then but extremely inefficient because such batch sizes are b\*\*\*s\*\*\*, so the model converges only very slowly.

I found a sweet spot around bs=256, where only around 20% CPU and 10% GPUs load is achieved and gradient descent is still somewhat efficient, which means I get the best results in terms of wall time.

The data pipeline is implemented with [tf.data](https://tf.data), reading the data from several CSVs in parallel from an SSD. I couldn't find any bottlenecks so far.

This is somewhat frustrating because I can only make use of a fraction of the full potential of my new machine. Any ideas on how to improve this?

I'm grateful for any help.

&#x200B;

&#x200B;

My specs:

\- AMD Ryzen Threadripper 3960X 24-Core Processor

\- 64 GB RAM

\- two NVIDIA GeForce RTX 2070 SUPER with 8192MiB each

\- Win 10 (unfortunately, the ASrock Creator TRX40 motherboard we bought is currently incompatible with Linux, wtf...)

\- TF 2.1.0 installed from binary (anaconda)

\- Python 3.7.7

\- CUDA Version 10.2.89

&#x200B;

The relevant part of my code:

    class dataset_loader():
        
        def __init__(self, data_dir, csv_file, batch_size, cycle_length, tokenizer=None, n_threads=1, n_prefetch=1):
            
            self.batch_size = batch_size
            self.cycle_length = cycle_length
            self.n_threads = n_threads
            self.n_prefetch = n_prefetch
            
            self.output_size = 3943     ## !!!!!!!!!!! TODO nur für testzwecke bitte nicht hardcoden!!!!!!!!!!!
    
            if(tokenizer is None):
                self.tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)
                self.tokenizer.fit_on_texts(strat_search_words_with_beginnings)
            else:
                self.tokenizer = tokenizer
                
            char_dict = list(eval(self.tokenizer.get_config().get(""word_index"")).keys())[:-1]  # hier vorletztes weglassen, da ein Out-Of-Vocabulary slot bei StaticVocabularyTable zwingend angegeben werden muss
            char_index = list(eval(self.tokenizer.get_config().get(""word_index"")).values())[:-1]
            char_table_init = tf.lookup.KeyValueTensorInitializer(char_dict, char_index, value_dtype=tf.int64)
            self.char_table = tf.lookup.StaticVocabularyTable(char_table_init, 1)
            
            self.max_id = len(self.tokenizer.word_index)
            
            assert self.max_id == len(char_index)+1
    
            self._filepath_ = os.path.join(data_dir, csv_file)
    
        
        @tf.function
        def preprocessing(self, line):
            split_line = tf.strings.split(line, sep="";"")
            
            search_string = split_line[0]
            search_string = tf.strings.substr(search_string, 0, 50)      # zu lange strings abschneiden
            while tf.strings.length(search_string) < 50:                 # zu kurze strings padden
                search_string = search_string + tf.constant("" "")
    
            chars = tf.strings.bytes_split(search_string)
            indices = self.char_table.lookup(chars)
            one_hot_string = tf.one_hot(indices, depth=self.max_id+1)
            
            icd_idx = tf.strings.to_number(split_line[1], out_type=tf.dtypes.int32)
            one_hot_icd = tf.one_hot(icd_idx, self.output_size)
            
            return one_hot_string, one_hot_icd
        
        def interleaving(self, filepath):
            return tf.data.TextLineDataset(filepath).skip(1)
        
        def get_dataset(self):
            dataset = tf.data.Dataset.list_files(self._filepath_)
            dataset = dataset.interleave(self.interleaving, cycle_length=self.cycle_length, num_parallel_calls=self.n_threads)
            dataset = dataset.map(self.preprocessing, num_parallel_calls=self.n_threads)
            return dataset.batch(self.batch_size).prefetch(self.n_prefetch)
        
        def get_tokenizer(self):
            return self.tokenizer
           
    
    
    import pickle
    f = open(os.path.join(data_dir, ""tokenizer.pkl""), ""rb"")
    tokenizer = pickle.load(f)
    
    train_data_loader = dataset_loader(data_dir,
                                 ""train_*.csv"",
                                 batch_size=batch_size,
                                 tokenizer=tokenizer,
                                 cycle_length=106,             #Anzahl der CSV-Dateien
                                 n_threads=tf.data.experimental.AUTOTUNE,
                                 n_prefetch=40)
    
    valid_data_loader = dataset_loader(data_dir,
                                ""valid_*.csv"",
                                batch_size=batch_size,
                                tokenizer=tokenizer,
                                cycle_length=13,
                                n_threads=tf.data.experimental.AUTOTUNE,
                                n_prefetch=40)
    
    
    mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
    
    with mirrored_strategy.scope():
        model = keras.models.Sequential([
        #    keras.layers.GRU(128, return_sequences=True, batch_input_shape=[batch_size, None, max_id+1]),
            keras.layers.GRU(128, return_sequences=True, input_shape=[ None, train_data_loader.max_id+1], use_bias=False),
            keras.layers.GRU(128, return_sequences=True, use_bias=False),
            keras.layers.GRU(128, use_bias=False),
            keras.layers.Flatten(),
            keras.layers.Dense(train_data_loader.output_size, activation=""softmax"")
        ])
        model.compile(loss=[focal_loss_umbertogriffo.categorical_focal_loss(alpha=.25, gamma=2)], optimizer=""adam"", metrics=['accuracy'])
    
    callbacks = list()
    callbacks.append(keras.callbacks.EarlyStopping(patience=2))
    callbacks.append(keras.callbacks.ModelCheckpoint(filepath = os.path.join(data_dir, ""checkpoints""), save_best_only=True))
    
    history = model.fit(train_data_loader.get_dataset(), validation_data=valid_data_loader.get_dataset(), epochs=25, callbacks = callbacks)"
42819,TF r2.3 Bad Address build issue on windows,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-Windows 10 Pro
-GTX 1060 3gb
-compute capabilities 6.1
-Python 3.6
-Tensorflow 2.3
-bazel 3.1.0
-CUDA 10.1
-cuDNN SDK 7.6
-TensorRT 6.0
-using AVX2



**Describe the problem**
I know from testing that it is provided by using CUDA. Without it build without Problems. I already tried it with other Python versions like 3.8.

    This is the Error Message
ERROR: C:/tenserflow/tensorflow/tensorflow/core/framework/BUILD:1107:1: Executing genrule //tensorflow/core/framework:attr_value_proto_text_srcs failed (Exit 126): bash.exe failed: error executing command
  cd C:/users/benno/_bazel_benno/fdojh2ah/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0
    SET PYTHON_BIN_PATH=C:/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7
    SET TF_ENABLE_XLA=1
    SET TF_NEED_CUDA=1
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/x64_windows-opt/bin/tensorflow/core/framework tensorflow/core/framework/ tensorflow/core/framework/attr_value.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/types.proto tensorflow/tools/proto_text/placeholder.txt
Execution platform: @local_execution_config_platform//:platform
/usr/bin/bash: bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions: Bad address
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: C:/tenserflow/tensorflow/tensorflow/core/framework/BUILD:1107:1 Executing genrule //tensorflow/core/framework:attr_value_proto_text_srcs failed (Exit 126): bash.exe failed: error executing command
  cd C:/users/benno/_bazel_benno/fdojh2ah/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\WINDOWS;C:\WINDOWS\System32;C:\WINDOWS\System32\WindowsPowerShell\v1.0
    SET PYTHON_BIN_PATH=C:/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Python36/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=6.1
    SET TF_CUDA_PATHS=C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1
    SET TF_CUDA_VERSION=10.1
    SET TF_CUDNN_VERSION=7
    SET TF_ENABLE_XLA=1
    SET TF_NEED_CUDA=1
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt-exec-50AE0418/bin/tensorflow/tools/proto_text/gen_proto_text_functions bazel-out/x64_windows-opt/bin/tensorflow/core/framework tensorflow/core/framework/ tensorflow/core/framework/attr_value.proto tensorflow/core/framework/resource_handle.proto tensorflow/core/framework/tensor.proto tensorflow/core/framework/tensor_shape.proto tensorflow/core/framework/types.proto tensorflow/tools/proto_text/placeholder.txt
Execution platform: @local_execution_config_platform//:platform




**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed this Website https://www.tensorflow.org/install/source_windows#gpu

     This is my .tf_configure.bazelrc
build --action_env PYTHON_BIN_PATH=""C:/Python36/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Python36/lib/site-packages""
build --python_path=""C:/Python36/python.exe""
build --config=xla
build --action_env TF_CUDA_VERSION=""10.1""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_CUDA_PATHS=""C:/tools/cuda,C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1""
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --config=cuda
build:opt --copt=/arch:AVX2
build:opt --define with_default_optimizations=true
build --define=override_eigen_strong_inline=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_windows,-no_windows_gpu,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42818,TF2: Cannot convert EfficientDet-d0 to saved_model/TFlite ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux - Arch but also getting same issues on colab

- TensorFlow installed from (source or binary):
Binary - Conda

- TensorFlow version (use command below):
2.2/2.3

- Python version:
3.7

- CUDA/cuDNN version:
Running on CPU

**Describe the current behavior**
I trained an efficientdet-d0 from http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz on a custom dataset using the object detection API. I am able to use the trained model for inference without an issue. I am able to export this model in the format saved_model.pb using https://github.com/tensorflow/models/blob/master/research/object_detection/exporter_main_v2.py but  am getting the following error when I try to load the saved model:

```
FailedPreconditionError:  Error while reading resource variable EfficientDet-D0/bifpn/node_15/2_up_lvl_5/combine/bifpn_combine_weights_81607 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/EfficientDet-D0/bifpn/node_15/2_up_lvl_5/combine/bifpn_combine_weights_81607/N10tensorflow3VarE does not exist.
	 [[{{node StatefulPartitionedCall/EfficientDet-D0/bifpn/node_15/2_up_lvl_5/combine/Relu/ReadVariableOp}}]] [Op:__inference_signature_wrapper_35854]

Function call stack:
signature_wrapper
```
The error changes even if I run the same code again but it remains a `FailedPreconditionError`. 

I have also tried saving the model I use during inference using [the official documentation](https://www.tensorflow.org/guide/saved_model#saving_a_custom_model) but that is also not working and giving me this error:

```
KeyError: ""Failed to add concrete function b'__inference___call___31522' to object based saved model as it captures tensor tf.Tensor(<unprintable>, shape=(), dtype=resource) which is unsupported or not reachable from root. One reason could be that a stateful object or a variable that the function depends on is not assigned to an attribute of the serialized trackable object (see SaveTest.test_captures_unreachable_variable).""
```

Finally, I am also unable to convert this model to TFLite as outlined [here](https://www.tensorflow.org/lite/convert/python_api#converting_a_concrete_function_) as I am getting this error when I try to make an interpreter using:

```
interpreter = tf.lite.Interpreter(model_path=TFLITE_OUTPUT_PATH)
```

```
ValueError: Model provided has model identifier 'tent', should be 'TFL3'
```

I am also unable to use https://github.com/tensorflow/models/blob/master/research/object_detection/export_tflite_ssd_graph.py to export to tflite as it requires a .ckpt file whereas I only have ckpt.index and ckpt.data files. Any advice on how I can convert my model to tflite (with any sort of quantization/pruning) will be appreciated."
42817,which tensorflow version starts to support XLA compiler?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42816,Summary in TF2.3 wraps unexpected name_scope,"When `tf.summary` is in a condition scope, the tag in tensorboard will wrap unexpected 'cond/'.

For example, when executed the following code, the tag in tensorboard is `cond/foo`, not `foo`.

```python
import tensorflow as tf

writer = tf.summary.create_file_writer('log')
step = tf.Variable(0, dtype=tf.int64)


@tf.function
def test():
    if step % 5 == 0:
        with writer.as_default():
            tf.summary.scalar('foo', 0.5, step=step)
            writer.flush()

    step.assign_add(1)


for _ in range(100):
    test()
```

This bug only shows in TF2.3. Tags are correct in tf below 2.3."
42815,how to get a layer's variable name scope in tensorflow keras?,"I want to get a layer's variable name scope, but I found in keras document, it didn't show anything to do this. in document, it just told me how to get a layer's name.

"
42814,"Tensorflow 2 not compiling using Clang 10, cause of undefined references","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Nope
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 20 x86_64
- TensorFlow installed from (source or binary):
Cloned from github (master)
- TensorFlow version (use command below):
2
- Python version:
3.8.2
- Bazel version (if compiling from source):
3.1.0
- Compiler version (if compiling from source):
clang version 10.0.0-4ubuntu1 
Target: x86_64-pc-linux-gnu
Thread model: posix

Hi. Can't build Tensorflow, using Clang 10.

$bazel build tensorflow

ERROR: /media/ubuntu/4d5fa4ce-dc9b-4cb0-934c-72533ffc1586/tmp/_bazel_ubuntu/a25833c9d3161107f44691534d8684de/external/llvm-project/mlir/BUILD:3246:1: Linking of rule '@llvm-project//mlir:mlir-tblgen' failed (Exit 1)
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::opt<char [8], llvm::cl::desc, llvm::cl::cat, llvm::cl::MiscFlags>(char const (&) [8], llvm::cl::desc const&, llvm::cl::cat const&, llvm::cl::MiscFlags const&): error: undefined reference to 'operator delete(void*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::opt<char [8], llvm::cl::desc, llvm::cl::cat, llvm::cl::MiscFlags>(char const (&) [8], llvm::cl::desc const&, llvm::cl::cat const&, llvm::cl::MiscFlags const&): error: undefined reference to 'operator delete(void*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::~opt(): error: undefined reference to 'operator delete(void*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::~opt(): error: undefined reference to 'operator delete(void*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function __clang_call_terminate: error: undefined reference to '__cxa_begin_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function __clang_call_terminate: error: undefined reference to 'std::terminate()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:function llvm::filter_iterator_base<llvm::Record* const*, std::function<bool (llvm::Record const*)>, std::bidirectional_iterator_tag>::filter_iterator_base(llvm::Record* const*, llvm::Record* const*, std::function<bool (llvm::Record const*)>): error: undefined reference to 'std::__throw_bad_function_call()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for llvm::cl::OptionValueCopy<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >: error: undefined reference to 'vtable for __cxxabiv1::__si_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::{lambda(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)#1}: error: undefined reference to 'vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for llvm::detail::provider_format_adapter<llvm::StringRef&>: error: undefined reference to 'vtable for __cxxabiv1::__si_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for llvm::detail::provider_format_adapter<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&>: error: undefined reference to 'vtable for __cxxabiv1::__si_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for llvm::detail::provider_format_adapter<llvm::StringRef>: error: undefined reference to 'vtable for __cxxabiv1::__si_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for filterForDialect<mlir::tblgen::Attribute>(llvm::ArrayRef<llvm::Record*>, mlir::tblgen::Dialect&)::{lambda(llvm::Record const*)#1}: error: undefined reference to 'vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for filterForDialect<mlir::tblgen::Type>(llvm::ArrayRef<llvm::Record*>, mlir::tblgen::Dialect&)::{lambda(llvm::Record const*)#1}: error: undefined reference to 'vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o:DialectGen.cpp:typeinfo for $_1: error: undefined reference to 'vtable for __cxxabiv1::__class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/DialectGen.o(.eh_frame+0xc87b): error: undefined reference to '__gxx_personality_v0'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_0>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_0>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_0>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_0>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_0>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__throw_logic_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_0>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__throw_logic_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function makeIdentifier[abi:cxx11](llvm::StringRef): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function makeIdentifier[abi:cxx11](llvm::StringRef): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function emitEnumDef(llvm::Record const&, llvm::raw_ostream&): error: undefined reference to 'std::__throw_logic_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_append(char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/EnumsGen.o:EnumsGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*>(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__throw_length_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o:LLVMIRConversionGen.cpp:function emitBuilders(llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o:LLVMIRConversionGen.cpp:function (anonymous namespace)::LLVMEnumAttr::getAllCases() const: error: undefined reference to 'operator new(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o:LLVMIRConversionGen.cpp:function (anonymous namespace)::LLVMEnumAttr::getAllCases() const: error: undefined reference to 'std::__throw_length_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o:LLVMIRConversionGen.cpp:typeinfo for bool (llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'vtable for __cxxabiv1::__function_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRConversionGen.o:LLVMIRConversionGen.cpp:typeinfo for bool (*)(llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'vtable for __cxxabiv1::__pointer_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<llvm::StringRef*>(llvm::StringRef*, llvm::StringRef*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<llvm::StringRef*>(llvm::StringRef*, llvm::StringRef*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__throw_length_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<llvm::StringRef*>(llvm::StringRef*, llvm::StringRef*, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__throw_length_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function getOverloadableTypeIdxs(llvm::Record const&, char const*): error: undefined reference to 'operator new(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function void llvm::cl::initializer<char [12]>::apply<llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >(llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >&) const: error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function void llvm::cl::initializer<char [12]>::apply<llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >(llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >&) const: error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/LLVMIRIntrinsicGen.o:LLVMIRIntrinsicGen.cpp:function void llvm::cl::initializer<char [12]>::apply<llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >(llvm::cl::opt<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >&) const: error: undefined reference to 'std::__throw_logic_error(char const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function getAllDerivedDefinitions(llvm::RecordKeeper const&, llvm::StringRef): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base const*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function getAllDerivedDefinitions(llvm::RecordKeeper const&, llvm::StringRef): error: undefined reference to 'operator new(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function mlir::tblgen::FmtObject<std::tuple<llvm::detail::provider_format_adapter<llvm::StringRef> > >::FmtObject(llvm::StringRef, mlir::tblgen::FmtContext const*, std::tuple<llvm::detail::provider_format_adapter<llvm::StringRef> >&&): error: undefined reference to 'operator new(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function genAttributeVerifier(mlir::tblgen::Operator const&, char const*, llvm::Twine const&, bool, mlir::tblgen::FmtContext&, mlir::tblgen::OpMethodBody&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char const*, unsigned long, unsigned long) const'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function mlir::tblgen::Operator::Operator(mlir::tblgen::Operator const&): error: undefined reference to 'std::__throw_bad_alloc()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function (anonymous namespace)::OpEmitter::genBuilder(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function (anonymous namespace)::OpEmitter::genBuilder(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_replace(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>* std::__uninitialized_copy<false>::__uninit_copy<std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>(std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*): error: undefined reference to '__cxa_begin_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>* std::__uninitialized_copy<false>::__uninit_copy<std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>(std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*): error: undefined reference to '__cxa_rethrow'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>* std::__uninitialized_copy<false>::__uninit_copy<std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>(std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, std::move_iterator<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*): error: undefined reference to '__cxa_end_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>* std::__uninitialized_copy<false>::__uninit_copy<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>(llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*): error: undefined reference to '__cxa_begin_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>* std::__uninitialized_copy<false>::__uninit_copy<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>(llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*): error: undefined reference to '__cxa_rethrow'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>* std::__uninitialized_copy<false>::__uninit_copy<llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*>(llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u> const*, llvm::SmallVector<mlir::tblgen::Operator::ArgOrType, 2u>*): error: undefined reference to '__cxa_end_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function (anonymous namespace)::OpEmitter::genCodeForAddingArgAndRegionForBuilder(mlir::tblgen::OpMethodBody&, bool): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char const*, unsigned long, unsigned long) const'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function (anonymous namespace)::OpEmitter::genCodeForAddingArgAndRegionForBuilder(mlir::tblgen::OpMethodBody&, bool): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function (anonymous namespace)::OpEmitter::genCodeForAddingArgAndRegionForBuilder(mlir::tblgen::OpMethodBody&, bool): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char const*, unsigned long, unsigned long) const'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function (anonymous namespace)::OpEmitter::genCodeForAddingArgAndRegionForBuilder(mlir::tblgen::OpMethodBody&, bool): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function void std::__uninitialized_fill<false>::__uninit_fill<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__cxa_begin_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function void std::__uninitialized_fill<false>::__uninit_fill<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__cxa_rethrow'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDefinitionsGen.o:OpDefinitionsGen.cpp:function void std::__uninitialized_fill<false>::__uninit_fill<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to '__cxa_end_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Function_handler<bool (llvm::RecordKeeper const&, llvm::raw_ostream&), $_1>::_M_invoke(std::_Any_data const&, llvm::RecordKeeper const&, llvm::raw_ostream&): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >::pair(std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > const&): error: undefined reference to 'std::__throw_bad_alloc()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_emplace_hint_unique<std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>, std::tuple<> >(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>&&, std::tuple<>&&): error: undefined reference to 'std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_emplace_hint_unique<std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>, std::tuple<> >(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>&&, std::tuple<>&&): error: undefined reference to '__cxa_rethrow'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_emplace_hint_unique<std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>, std::tuple<> >(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>&&, std::tuple<>&&): error: undefined reference to '__cxa_end_catch'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, mlir::tblgen::Dialect const&): error: undefined reference to 'std::_Rb_tree_decrement(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, mlir::tblgen::Dialect const&): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, mlir::tblgen::Dialect const&): error: undefined reference to 'std::_Rb_tree_decrement(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, mlir::tblgen::Dialect const&): error: undefined reference to 'std::_Rb_tree_decrement(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function void std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > > >::_M_construct_node<std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>, std::tuple<> >(std::_Rb_tree_node<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Operator, std::allocator<mlir::tblgen::Operator> > > >*, std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>&&, std::tuple<>&&): error: undefined reference to 'std::__throw_bad_alloc()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > > std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > > >::_M_emplace_hint_unique<std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>, std::tuple<> >(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>&&, std::tuple<>&&): error: undefined reference to 'std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, mlir::tblgen::Dialect const&): error: undefined reference to 'std::_Rb_tree_decrement(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, mlir::tblgen::Dialect const&): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/OpDocGen.o:OpDocGen.cpp:function void std::_Rb_tree<mlir::tblgen::Dialect, std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > >, std::_Select1st<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >, std::less<mlir::tblgen::Dialect>, std::allocator<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > > >::_M_construct_node<std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>, std::tuple<> >(std::_Rb_tree_node<std::pair<mlir::tblgen::Dialect const, std::vector<mlir::tblgen::Type, std::allocator<mlir::tblgen::Type> > > >*, std::piecewise_construct_t const&, std::tuple<mlir::tblgen::Dialect const&>&&, std::tuple<>&&): error: undefined reference to 'std::__throw_bad_alloc()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/RewriterGen.o:RewriterGen.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > llvm::detail::join_impl<__gnu_cxx::__normal_iterator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > > >(__gnu_cxx::__normal_iterator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, __gnu_cxx::__normal_iterator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >*, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > >, llvm::StringRef, std::forward_iterator_tag): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/RewriterGen.o:RewriterGen.cpp:function (anonymous namespace)::PatternEmitter::handleOpCreation[abi:cxx11](mlir::tblgen::DagNode, int, int): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_assign(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/mlir-tblgen.o:mlir-tblgen.cpp:function MlirTableGenMain(llvm::raw_ostream&, llvm::RecordKeeper&): error: undefined reference to 'std::__throw_bad_function_call()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/mlir-tblgen.o:mlir-tblgen.cpp:function llvm::cl::opt<mlir::GenInfo const*, false, mlir::GenNameParser>::handleOccurrence(unsigned int, llvm::StringRef, llvm::StringRef): error: undefined reference to 'std::__throw_bad_function_call()'
bazel-out/host/bin/external/llvm-project/mlir/_objs/mlir-tblgen/mlir-tblgen.o:mlir-tblgen.cpp:typeinfo for llvm::cl::opt<mlir::GenInfo const*, false, mlir::GenNameParser>: error: undefined reference to 'vtable for __cxxabiv1::__vmi_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/mlir/_objs/TableGen/Predicate.o:Predicate.cpp:function buildPredicateTree(mlir::tblgen::Pred const&, llvm::SpecificBumpPtrAllocator<(anonymous namespace)::PredNode>&, llvm::ArrayRef<std::pair<llvm::StringRef, llvm::StringRef> >): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find(char const*, unsigned long, unsigned long) const'
bazel-out/host/bin/external/llvm-project/mlir/_objs/TableGen/Predicate.o:Predicate.cpp:function buildPredicateTree(mlir::tblgen::Pred const&, llvm::SpecificBumpPtrAllocator<(anonymous namespace)::PredNode>&, llvm::ArrayRef<std::pair<llvm::StringRef, llvm::StringRef> >): error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/llvm-project/mlir/_objs/TableGen/Operator.o:Operator.cpp:function std::pair<std::_Rb_tree_iterator<llvm::EquivalenceClasses<int>::ECValue>, bool> std::_Rb_tree<llvm::EquivalenceClasses<int>::ECValue, llvm::EquivalenceClasses<int>::ECValue, std::_Identity<llvm::EquivalenceClasses<int>::ECValue>, std::less<llvm::EquivalenceClasses<int>::ECValue>, std::allocator<llvm::EquivalenceClasses<int>::ECValue> >::_M_insert_unique<llvm::EquivalenceClasses<int>::ECValue>(llvm::EquivalenceClasses<int>::ECValue&&): error: undefined reference to 'std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_V2::system_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base const*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::TableGenMain(char const*, bool (*)(llvm::raw_ostream&, llvm::RecordKeeper&)): error: undefined reference to 'std::_V2::system_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:function std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(char const*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::reserve(unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:function llvm::cl::list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::handleOccurrence(unsigned int, llvm::StringRef, llvm::StringRef): error: undefined reference to 'std::__throw_bad_function_call()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Main.o:Main.cpp:typeinfo for llvm::cl::list<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, bool, llvm::cl::parser<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >: error: undefined reference to 'vtable for __cxxabiv1::__vmi_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/TGLexer.o:TGLexer.cpp:function llvm::TGLexer::LexString(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/TGLexer.o:TGLexer.cpp:function llvm::TGLexer::LexString(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/TGLexer.o:TGLexer.cpp:function llvm::TGLexer::LexString(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/TGLexer.o:TGLexer.cpp:function std::_Rb_tree_iterator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_M_insert_<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_Alloc_node>(std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, std::_Rb_tree<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::_Identity<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::less<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > >::_Alloc_node&): error: undefined reference to 'std::_Rb_tree_insert_and_rebalance(bool, std::_Rb_tree_node_base*, std::_Rb_tree_node_base*, std::_Rb_tree_node_base&)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::BitsRecTy::get(unsigned int): error: undefined reference to '__cxa_guard_acquire'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::BitsRecTy::get(unsigned int): error: undefined reference to '__cxa_guard_release'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::RecordRecTy::get(llvm::ArrayRef<llvm::Record*>): error: undefined reference to '__cxa_guard_acquire'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::RecordRecTy::get(llvm::ArrayRef<llvm::Record*>): error: undefined reference to '__cxa_guard_release'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::UnsetInit::get(): error: undefined reference to '__cxa_guard_acquire'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::UnsetInit::get(): error: undefined reference to '__cxa_guard_release'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::BitInit::get(bool): error: undefined reference to '__cxa_guard_acquire'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::BitInit::get(bool): error: undefined reference to '__cxa_guard_release'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::BitsInit::get(llvm::ArrayRef<llvm::Init*>): error: undefined reference to '__cxa_guard_abort'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::ListInit::get(llvm::ArrayRef<llvm::Init*>, llvm::RecTy*): error: undefined reference to '__cxa_guard_abort'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::UnOpInit::get(llvm::UnOpInit::UnaryOp, llvm::Init*, llvm::RecTy*): error: undefined reference to '__cxa_guard_abort'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::BinOpInit::get(llvm::BinOpInit::BinaryOp, llvm::Init*, llvm::Init*, llvm::RecTy*): error: undefined reference to '__cxa_guard_abort'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::TernOpInit::Fold(llvm::Record*) const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::operator<<(llvm::raw_ostream&, llvm::RecordKeeper const&): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base const*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function llvm::operator<<(llvm::raw_ostream&, llvm::RecordKeeper const&): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base const*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:function std::_Rb_tree<long, std::pair<long const, llvm::IntInit*>, std::_Select1st<std::pair<long const, llvm::IntInit*> >, std::less<long>, std::allocator<std::pair<long const, llvm::IntInit*> > >::_M_get_insert_hint_unique_pos(std::_Rb_tree_const_iterator<std::pair<long const, llvm::IntInit*> >, long const&): error: undefined reference to 'std::_Rb_tree_increment(std::_Rb_tree_node_base*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:typeinfo for llvm::TrailingObjects<llvm::RecordRecTy, llvm::Record*>: error: undefined reference to 'vtable for __cxxabiv1::__vmi_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/Record.o:Record.cpp:typeinfo for llvm::RecordRecTy: error: undefined reference to 'vtable for __cxxabiv1::__vmi_class_type_info'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/llvm/_objs/TableGen/TGParser.o:TGParser.cpp:function llvm::TGParser::ParseDeclaration(llvm::Record*, bool): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:function ExpandResponseFile(llvm::StringRef, llvm::StringSaver&, void (*)(llvm::StringRef, llvm::StringSaver&, llvm::SmallVectorImpl<char const*>&, bool), llvm::SmallVectorImpl<char const*>&, bool, bool, llvm::vfs::FileSystem&): error: undefined reference to 'std::_V2::generic_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:vtable for llvm::cl::Option: error: undefined reference to '__cxa_pure_virtual'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:vtable for llvm::cl::Option: error: undefined reference to '__cxa_pure_virtual'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:vtable for llvm::cl::Option: error: undefined reference to '__cxa_pure_virtual'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/CommandLine.o:CommandLine.cpp:vtable for llvm::cl::Option: error: undefined reference to '__cxa_pure_virtual'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ConvertUTFWrapper.o:ConvertUTFWrapper.cpp:function llvm::convertUTF16ToUTF8String(llvm::ArrayRef<char>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::resize(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ConvertUTFWrapper.o:ConvertUTFWrapper.cpp:function llvm::convertUTF16ToUTF8String(llvm::ArrayRef<char>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::resize(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ConvertUTFWrapper.o:ConvertUTFWrapper.cpp:function llvm::convertUTF16ToUTF8String(llvm::ArrayRef<char>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_mutate(unsigned long, unsigned long, char const*, unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ConvertUTFWrapper.o:ConvertUTFWrapper.cpp:function llvm::convertUTF16ToUTF8String(llvm::ArrayRef<char>, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_erase(unsigned long, unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:function llvm::errorToErrorCode(llvm::Error): error: undefined reference to 'std::_V2::system_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:function (anonymous namespace)::ErrorErrorCategory::~ErrorErrorCategory(): error: undefined reference to 'std::_V2::error_category::~error_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::~error_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::_M_message[abi:cxx11](int) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::default_error_condition(int) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::equivalent(int, std::error_condition const&) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:vtable for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'std::_V2::error_category::equivalent(std::error_code const&, int) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Error.o:Error.cpp:typeinfo for (anonymous namespace)::ErrorErrorCategory: error: undefined reference to 'typeinfo for std::_V2::error_category'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_fatal_error(llvm::Twine const&, bool): error: undefined reference to 'std::__throw_system_error(int)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to '__cxa_allocate_exception'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to 'vtable for std::bad_alloc'
/usr/bin/ld.gold: the vtable symbol may be undefined because the class is missing its key function
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to 'typeinfo for std::bad_alloc'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to 'std::bad_alloc::~bad_alloc()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to '__cxa_throw'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to 'std::__throw_system_error(int)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ErrorHandling.o:ErrorHandling.cpp:function llvm::report_bad_alloc_error(char const*, bool): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_ostream::SetBuffered(): error: undefined reference to 'operator new[](unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_ostream::SetBuffered(): error: undefined reference to 'operator delete[](void*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_ostream::SetBuffered(): error: undefined reference to 'operator delete[](void*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_ostream::SetBufferAndMode(char*, unsigned long, llvm::raw_ostream::BufferKind): error: undefined reference to 'operator delete[](void*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_ostream::operator<<(llvm::FormattedNumber const&): error: undefined reference to 'operator delete[](void*)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::raw_fd_ostream(llvm::StringRef, std::error_code&, llvm::sys::fs::CreationDisposition, llvm::sys::fs::FileAccess, llvm::sys::fs::OpenFlags): error: undefined reference to 'std::_V2::system_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::write_impl(char const*, unsigned long): error: undefined reference to 'std::_V2::generic_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::pwrite_impl(char const*, unsigned long, unsigned long): error: undefined reference to 'std::_V2::generic_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/raw_ostream.o:raw_ostream.cpp:function llvm::raw_fd_ostream::pwrite_impl(char const*, unsigned long, unsigned long): error: undefined reference to 'std::_V2::generic_category()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Host.o:Host.cpp:function updateTripleOSVersion(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::resize(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Host.o:Host.cpp:function updateTripleOSVersion(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::resize(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::ManagedStaticBase::RegisterManagedStatic(void* (*)(), void (*)(void*)) const: error: undefined reference to 'std::__once_callable'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::ManagedStaticBase::RegisterManagedStatic(void* (*)(), void (*)(void*)) const: error: undefined reference to 'std::__once_call'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::ManagedStaticBase::RegisterManagedStatic(void* (*)(), void (*)(void*)) const: error: undefined reference to '__once_proxy'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::ManagedStaticBase::RegisterManagedStatic(void* (*)(), void (*)(void*)) const: error: undefined reference to 'std::__throw_system_error(int)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::llvm_shutdown(): error: undefined reference to 'std::__once_callable'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::llvm_shutdown(): error: undefined reference to 'std::__once_call'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::llvm_shutdown(): error: undefined reference to '__once_proxy'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function llvm::llvm_shutdown(): error: undefined reference to 'std::__throw_system_error(int)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ManagedStatic.o:ManagedStatic.cpp:function std::call_once<void (&)()>(std::once_flag&, void (&)())::{lambda()#2}::__invoke(): error: undefined reference to 'std::__once_callable'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/MemoryBuffer.o:MemoryBuffer.cpp:function llvm::WritableMemoryBuffer::getNewUninitMemBuffer(unsigned long, llvm::Twine const&): error: undefined reference to 'std::nothrow'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/MemoryBuffer.o:MemoryBuffer.cpp:function llvm::WritableMemoryBuffer::getNewUninitMemBuffer(unsigned long, llvm::Twine const&): error: undefined reference to 'operator new(unsigned long, std::nothrow_t const&)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/VirtualFileSystem.o:VirtualFileSystem.cpp:function (anonymous namespace)::RealFile::~RealFile(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Process.o:Process.cpp:function llvm::sys::Process::GetTimeUsage(std::chrono::time_point<std::chrono::_V2::system_clock, std::chrono::duration<long, std::ratio<1l, 1000000000l> > >&, std::chrono::duration<long, std::ratio<1l, 1000000000l> >&, std::chrono::duration<long, std::ratio<1l, 1000000000l> >&): error: undefined reference to 'std::chrono::_V2::system_clock::now()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Process.o:Process.cpp:function llvm::sys::Process::GetRandomNumber(): error: undefined reference to 'std::chrono::_V2::system_clock::now()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/ToolOutputFile.o:ToolOutputFile.cpp:function llvm::ToolOutputFile::CleanupInstaller::~CleanupInstaller(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Signals.o:Signals.cpp:function llvm::sys::DontRemoveFileOnSignal(llvm::StringRef): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/SourceMgr.o:SourceMgr.cpp:function llvm::SMDiagnostic::print(char const*, llvm::raw_ostream&, bool, bool) const: error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/SourceMgr.o:SourceMgr.cpp:function llvm::SMDiagnostic::print(char const*, llvm::raw_ostream&, bool, bool) const: error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/SourceMgr.o:SourceMgr.cpp:function llvm::SMDiagnostic::print(char const*, llvm::raw_ostream&, bool, bool) const: error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::find_last_not_of(char, unsigned long) const'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/SourceMgr.o:SourceMgr.cpp:function llvm::SMDiagnostic::print(char const*, llvm::raw_ostream&, bool, bool) const: error: undefined reference to 'std::__throw_out_of_range_fmt(char const*, ...)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Statistic.o:Statistic.cpp:function llvm::PrintStatistics(llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Statistic.o:Statistic.cpp:function llvm::PrintStatistics(llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct(unsigned long, char)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Statistic.o:Statistic.cpp:function (anonymous namespace)::StatisticInfo::sort(): error: undefined reference to 'std::nothrow'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Statistic.o:Statistic.cpp:function (anonymous namespace)::StatisticInfo::sort(): error: undefined reference to 'operator new(unsigned long, std::nothrow_t const&)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/StringRef.o:StringRef.cpp:function unsigned int llvm::ComputeEditDistance<char>(llvm::ArrayRef<char>, llvm::ArrayRef<char>, bool, unsigned int): error: undefined reference to 'operator new[](unsigned long)'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Timer.o:Timer.cpp:function llvm::Timer::~Timer(): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string()'
bazel-out/host/bin/external/llvm-project/llvm/_objs/Support/Timer.o:Timer.cpp:function llvm::TimerGroup::PrintQueuedTimers(llvm::raw_ostream&): error: undefined reference to 'std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_construct(unsigned long, char)'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow:tensorflow failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2195.712s, Critical Path: 186.85s
INFO: 4174 processes: 4174 local.
FAILED: Build did NOT complete successfully

Ivan"
42811,"""cudart64_101.dll not found"" but the nvidia GPU computing toolkit has the file cudart64_102.dll file under the bin folder?","

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): using pip after creating environment from within the scripts folder of the environment after using ""activate"", so in short using binary
- TensorFlow version: tensorflow gpu 2.3.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip after creating an environment
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2
- GPU model and memory: Nvidia GeForce 940MX



I tried to install tensorflow gpu to get started with using CUDA with it. But after installing it in the created environment, when I open the python interactive mode and try to import tensorflow it throws a error/warning that cudart64_101.dll not found. But installed cuda 10.2 and then it's patch listed on the nvidia cuda website and also cudnn. I even included everything in the PATH. But why is my installation looking for cudart64_101.dll when I have an updated file named cudart64_102.dll in my nvidia gpu toolkit's binary folder. How can I fix this? I am trying this to learn about cuda and how to use them with tensorflow and pytorch both. The commands I used from start to finish and their output is pasted below:

**Provide the exact sequence of commands / steps that you executed before running into the problem**
C:\Users\nrifa>cd C:\Users\nrifa\AppData\Local\Programs\Python\Python36

C:\Users\nrifa\AppData\Local\Programs\Python\Python36>python -m venv C:\gputest

C:\Users\nrifa\AppData\Local\Programs\Python\Python36>cd C:\gputest

C:\gputest>cd Scripts

C:\gputest\Scripts>activate
(gputest) C:\gputest\Scripts>pip install tensorflow-gpu


Logs:

Collecting tensorflow-gpu
  Downloading https://files.pythonhosted.org/packages/9f/23/5d5a18870f4be65faee538aeb28276af809f6442a3a5dc6f7e58d7f70bd6/tensorflow_gpu-2.3.0-cp36-cp36m-win_amd64.whl (344.1MB)
    100% |████████████████████████████████| 344.1MB 3.2kB/s
Collecting keras-preprocessing<1.2,>=1.1.1 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)
    100% |████████████████████████████████| 51kB 631kB/s
Collecting h5py<2.11.0,>=2.10.0 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/0b/fa/bee65d2dbdbd3611702aafd128139c53c90a1285f169ba5467aab252e27a/h5py-2.10.0-cp36-cp36m-win_amd64.whl (2.4MB)
    100% |████████████████████████████████| 2.4MB 321kB/s
Collecting termcolor>=1.1.0 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz
Collecting tensorboard<3,>=2.3.0 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)
    100% |████████████████████████████████| 6.8MB 133kB/s
Collecting wheel>=0.26 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/a7/00/3df031b3ecd5444d572141321537080b40c1c25e1caa3d86cdd12e5e919c/wheel-0.35.1-py2.py3-none-any.whl
Collecting scipy==1.4.1 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/8d/2f/fcb6150813b89d628749784370132e431f687ebab5a1063eb298cc941f76/scipy-1.4.1-cp36-cp36m-win_amd64.whl (30.8MB)
    100% |████████████████████████████████| 30.8MB 16kB/s
Collecting gast==0.3.3 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl
Collecting absl-py>=0.7.0 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/b9/07/f69dd3367368ad69f174bfe426a973651412ec11d48ec05c000f19fe0561/absl_py-0.10.0-py3-none-any.whl (127kB)
    100% |████████████████████████████████| 133kB 141kB/s
Collecting protobuf>=3.9.2 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/f6/fe/9d8e70a86add02cb1ef35540ec03fd5b210d76323fe4645d7121b13ae33e/protobuf-3.13.0-cp36-cp36m-win_amd64.whl (1.1MB)
    100% |████████████████████████████████| 1.1MB 271kB/s
Collecting numpy<1.19.0,>=1.16.0 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/dc/18/e69ef84530360c2d39db51acb4cc0012990f27fb1fa7542dac45ad30e7ab/numpy-1.18.5-cp36-cp36m-win_amd64.whl (12.7MB)
    100% |████████████████████████████████| 12.7MB 34kB/s
Collecting tensorflow-gpu-estimator<2.4.0,>=2.3.0 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/9d/6f/87e922b1dbfa9aa2e79bf150bf05d567eaa4f83bfd329c04834b26b0725e/tensorflow_gpu_estimator-2.3.0-py2.py3-none-any.whl (474kB)
    100% |████████████████████████████████| 481kB 266kB/s
Collecting astunparse==1.6.3 (from tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl
Collecting wrapt>=1.11.1 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz
Collecting six>=1.12.0 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl
Collecting grpcio>=1.8.6 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/15/3f/f311f382bb658387fe78a30e1ed55193fe94c5e78b37abd134c34bd256eb/grpcio-1.31.0-cp36-cp36m-win_amd64.whl (2.6MB)
    100% |████████████████████████████████| 2.6MB 269kB/s
Collecting google-pasta>=0.1.8 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)
    100% |████████████████████████████████| 61kB 1.2MB/s
Collecting opt-einsum>=2.3.2 (from tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)
    100% |████████████████████████████████| 71kB 997kB/s
Collecting markdown>=2.6.8 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)
    100% |████████████████████████████████| 92kB 963kB/s
Collecting werkzeug>=0.11.15 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)
    100% |████████████████████████████████| 307kB 363kB/s
Collecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)
    100% |████████████████████████████████| 788kB 423kB/s
Collecting setuptools>=41.0.0 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/c3/a9/5dc32465951cf4812e9e93b4ad2d314893c2fa6d5f66ce5c057af6e76d85/setuptools-49.6.0-py3-none-any.whl (803kB)
    100% |████████████████████████████████| 808kB 248kB/s
Collecting google-auth<2,>=1.6.3 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/63/7f/ef6bcf2cc0f50c7163afb94382aab67a6b278e1e447c2e3981aa281b9747/google_auth-1.21.0-py2.py3-none-any.whl (92kB)
    100% |████████████████████████████████| 102kB 366kB/s
Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl
Collecting requests<3,>=2.21.0 (from tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)
    100% |████████████████████████████████| 71kB 432kB/s
Collecting importlib-metadata; python_version < ""3.8"" (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl
Collecting pyasn1-modules>=0.2.1 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)
    100% |████████████████████████████████| 163kB 475kB/s
Collecting cachetools<5.0,>=2.0.0 (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl
Collecting rsa<5,>=3.1.4; python_version >= ""3.5"" (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)
    100% |████████████████████████████████| 51kB 506kB/s
Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl
Collecting idna<3,>=2.5 (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)
    100% |████████████████████████████████| 61kB 501kB/s
Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)
    100% |████████████████████████████████| 163kB 312kB/s
Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)
    100% |████████████████████████████████| 133kB 412kB/s
Collecting chardet<4,>=3.0.2 (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)
    100% |████████████████████████████████| 143kB 506kB/s
Collecting zipp>=0.5 (from importlib-metadata; python_version < ""3.8""->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl
Collecting pyasn1<0.5.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)
    100% |████████████████████████████████| 81kB 101kB/s
Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu)
  Cache entry deserialization failed, entry ignored
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)
    100% |████████████████████████████████| 153kB 254kB/s
Installing collected packages: numpy, six, keras-preprocessing, h5py, termcolor, wheel, zipp, importlib-metadata, markdown, grpcio, werkzeug, setuptools, protobuf, tensorboard-plugin-wit, pyasn1, pyasn1-modules, cachetools, rsa, google-auth, idna, certifi, urllib3, chardet, requests, oauthlib, requests-oauthlib, google-auth-oauthlib, absl-py, tensorboard, scipy, gast, tensorflow-gpu-estimator, astunparse, wrapt, google-pasta, opt-einsum, tensorflow-gpu
  Running setup.py install for termcolor ... done
  Found existing installation: setuptools 28.8.0
    Uninstalling setuptools-28.8.0:
      Successfully uninstalled setuptools-28.8.0
  Running setup.py install for wrapt ... done
Successfully installed absl-py-0.10.0 astunparse-1.6.3 cachetools-4.1.1 certifi-2020.6.20 chardet-3.0.4 gast-0.3.3 google-auth-1.21.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.31.0 h5py-2.10.0 idna-2.10 importlib-metadata-1.7.0 keras-preprocessing-1.1.2 markdown-3.2.2 numpy-1.18.5 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.13.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.24.0 requests-oauthlib-1.3.0 rsa-4.6 scipy-1.4.1 setuptools-49.6.0 six-1.15.0 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-gpu-2.3.0 tensorflow-gpu-estimator-2.3.0 termcolor-1.1.0 urllib3-1.25.10 werkzeug-1.0.1 wheel-0.35.1 wrapt-1.12.1 zipp-3.1.0
You are using pip version 9.0.1, however version 20.2.2 is available.
You should consider upgrading via the 'python -m pip install --upgrade pip' command.

(gputest) C:\gputest\Scripts>python -m pip install --upgrade pip
Cache entry deserialization failed, entry ignored
Collecting pip
  Cache entry deserialization failed, entry ignored
  Downloading https://files.pythonhosted.org/packages/5a/4a/39400ff9b36e719bdf8f31c99fe1fa7842a42fa77432e584f707a5080063/pip-20.2.2-py2.py3-none-any.whl (1.5MB)
    100% |████████████████████████████████| 1.5MB 230kB/s
Installing collected packages: pip
  Found existing installation: pip 9.0.1
    Uninstalling pip-9.0.1:
      Successfully uninstalled pip-9.0.1
Successfully installed pip-20.2.2

(gputest) C:\gputest\Scripts>python
Python 3.6.4 (v3.6.4:d48eceb, Dec 19 2017, 06:54:40) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2020-08-30 21:51:17.099536: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudart64_101.dll'; dlerror: cudart64_101.dll not found
2020-08-30 21:51:17.100010: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.

"
42810,A problem about the install of tensorflow,"Even though I installed tensorflow, I failed to import it. It seems that the module tensorflow could not be found.
Here, I insert my coding.

Here, I am going to tell the history which has led to this problem.

I installed ""tensorflow (ver 2.0)"" to python with ""pip (ver 20.0.2)"" code.
As an environment, I am using ""jupyter lab"" in Anaconda 64 bit.

First, I executed the following code several times.
**pip install tensorflow** 

Then, I executed the following code mistakenly, which might have caused the problem.
**pip install tensorflow as tf**

At this time, the packages administered by pip have included **tensorflow and tf (ver 1.0.0)**.

As a result, now I have the trouble as I have told.

In addition, I tried to uninstall both tensorflow and tf, only to fail. Here is codes I executed.
**pip uninstall tensorflow**
**pip uninstall tf**

Could you please tell me how to solve this problem?
(Here, I attached the image of my coding.)
<img width=""601"" alt=""imag1"" src=""https://user-images.githubusercontent.com/70477660/91662272-41933700-eb1c-11ea-98a0-346fe6f67ce1.png"">
<img width=""430"" alt=""imag2"" src=""https://user-images.githubusercontent.com/70477660/91662275-4821ae80-eb1c-11ea-9a07-bde0c18801de.png"">


"
42809,Can't download Tenserflow,"Traceback (most recent call last):
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Oyuntuul\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime."
42808,Create data generator with tf.data.dataset for sequence models,"I have an image dataset including RGB images: img1.png, img2.png ... img250.png. I have extracted 100 small patches with size [64,64,3] from each image. So, I have now dataset like img1_1.png, img1_2.png ...img1_100.png, img2_1.png, img2_2.png, ... img2_100.png, img3_1, .....

I want to create a data generator with tf.data.dataset.from_tensor_slices to pass all patches of each image to an RNN model. So, I wanna the generator creates output like this : [batch_size, 100, 64, 64, 3]

How can I do that?"
42778,"Keras load model throwing no custom loss function while loading so used compile=False, but still the same error","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):google colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Google colab
- TensorFlow version (use command below): pip install tensorflow==1.15, pip insall keras==2.2.4
- Python version: Google colab
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Keras Load model is giving me error while loading.h5  model even when compile=False is applied. I installed tf-nightly but it is the same. I just want to load the model to see the model input and output names.



**Describe the expected behavior**
In keras laod_model function, when compile=False is given, it should not throw any error with custom loss functions
**Standalone code to reproduce the issue**
from keras import models
model = keras.models.load_model('/content/drive/My Drive/yolov3/New_Model_Weights/trained_weights_final.h5', compile=False)
print(model.summary())


Using TensorFlow backend.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:245: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

2020-08-29 20:32:46.545701: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-08-29 20:32:46.550727: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000129999 Hz
2020-08-29 20:32:46.550947: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28c8d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-29 20:32:46.550980: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-29 20:32:46.554322: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-29 20:32:46.704185: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-29 20:32:46.704869: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x28c8f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-29 20:32:46.704905: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5
2020-08-29 20:32:46.705090: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-29 20:32:46.705628: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59
pciBusID: 0000:00:04.0
2020-08-29 20:32:46.722565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-08-29 20:32:46.939361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2020-08-29 20:32:47.024295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2020-08-29 20:32:47.046622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2020-08-29 20:32:47.286071: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2020-08-29 20:32:47.423963: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2020-08-29 20:32:47.439325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-29 20:32:47.439470: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-29 20:32:47.440069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-29 20:32:47.440551: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2020-08-29 20:32:47.440611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2020-08-29 20:32:47.441756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-29 20:32:47.441787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2020-08-29 20:32:47.441799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2020-08-29 20:32:47.441974: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-29 20:32:47.442529: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-29 20:32:47.443018: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
2020-08-29 20:32:47.443055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.

Traceback (most recent call last):
  File ""/content/TrainYourOwnYOLO/2_Training/keras_inputoutput.py"", line 12, in <module>
    model2 = models.load_model('/content/drive/My Drive/yolov3/New_Model_Weights/trained_weights_final.h5', compile=False)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py"", line 419, in load_model
    model = _deserialize_model(f, custom_objects, compile)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py"", line 225, in _deserialize_model
    model = model_from_config(model_config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py"", line 458, in model_from_config
    return deserialize(config, custom_objects=custom_objects)
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/__init__.py"", line 55, in deserialize
    printable_module_name='layer')
  File ""/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py"", line 145, in deserialize_keras_object
    list(custom_objects.items())))
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/network.py"", line 1032, in from_config
    process_node(layer, node_data)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/network.py"", line 991, in process_node
    layer(unpack_singleton(input_tensors), **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py"", line 457, in __call__
    output = self.call(inputs, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/keras/layers/core.py"", line 687, in call
    return self.function(inputs, **arguments)
  File ""/content/TrainYourOwnYOLO/2_Training/src/keras_yolo3/yolo3/model.py"", line 440, in yolo_loss
    grid, raw_pred, pred_xy, pred_wh = yolo_head(
NameError: name 'yolo_head' is not defined


Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

from keras import models
model = keras.models.load_model('/content/drive/My Drive/yolov3/New_Model_Weights/trained_weights_final.h5', compile=False)
print(model.summary())

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42777,De-serialization bug from SavedModel format,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Getting a ValueError when deserializing a model built with functional API and no custom layers.
```
ValueError: You tried to call `count_params` on input_mnist, but the layer isn't built. You can build it manually via: `input_mnist.build(batch_input_shape)`.
```

Test code to reproduce:

```
import os
import tempfile

import tensorflow as tf
import numpy as np


def test_bug_tensorflow():

    nb_inputs = 784
    hidden_units = [32, 32]
    nb_classes = 10

    # backbone model
    input_1 = tf.keras.layers.Input(shape=(nb_inputs,), name=""input_backbone"", dtype=tf.float32)
    x = input_1
    for i, units in enumerate(hidden_units):
        x = tf.keras.layers.Dense(units, name=f""dense_{i}"", activation=""relu"")(x)
    backbone = tf.keras.models.Model(input_1, x, name=""backbone"")

    # classifier
    input_2 = tf.keras.layers.Input(shape=(nb_inputs,), name=""input_mnist"", dtype=tf.float32)
    x = backbone(input_2)
    x = tf.keras.layers.Dense(nb_classes, activation=""softmax"", name=""output"")(x)
    mnist_model = tf.keras.models.Model(input_2, x, name=""mnist"")
    mnist_model.predict(np.zeros((100, nb_inputs)))
    print(mnist_model.summary())

    tmp_dir = tempfile.TemporaryDirectory()
    tmp_path = str(tmp_dir.name)
    
    mnist_model.save(os.path.join(tmp_path, ""model.h5""), save_format=""h5"")
    new_model = tf.keras.models.load_model(os.path.join(tmp_path, ""model.h5""))
    mnist_model.predict(np.zeros((100, nb_inputs)))
    print(new_model.summary())

    mnist_model.save(tmp_path, save_format=""tf"")
    new_model = tf.keras.models.load_model(tmp_path)
    mnist_model.predict(np.zeros((100, nb_inputs)))
    print(new_model.summary())  # raises ValueError
```

**Describe the expected behavior**
Things work as expected with h5, but not save_format. If I remove the backbone model from the MNIST model, it works.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42776,errors while loading compiling tensorflow model,"**System information**
- Windows 10 64 bit:
- tensorflow  (v2.3) installed via pip installer
- Python v3.8.3 using Spyder4:
-CUDA: 10.1.:
- GPU: Nvidia GeForce GTX 1060 3 GB

Hello,

i am trying to compile a code from YT. In the video it worked. I noticed there are some warnings, which are probably regarded to the cudnn64_7.dll file and also some other missing parts.

Link to the Video and Code: https://pythonprogramming.net/crypto-rnn-model-deep-learning-python-tensorflow-keras/?completed=/balancing-rnn-data-deep-learning-python-tensorflow-keras/

I donwloaded Cuddn and extracted the files, as well as made a PATH systemvariable to this file.
This is the error I get:

**2020-08-29 18:45:48.778870: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-29 18:45:52.432459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-08-29 18:45:52.566076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-08-29 18:45:52.568033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-29 18:45:52.598770: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-29 18:45:52.635186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-29 18:45:52.651335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-29 18:45:52.693526: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-29 18:45:52.707974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-29 18:45:52.708778: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-08-29 18:45:52.709174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-08-29 18:45:52.711705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-29 18:45:52.735435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15544f7c200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-29 18:45:52.735933: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-29 18:45:52.737008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-29 18:45:52.737368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
2020-08-29 18:45:53.679101: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:45:53.680145: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
2020-08-29 18:45:53.682001: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found
2020-08-29 18:45:53.682796: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2020-08-29 18:45:53.683220: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-08-29 18:57:01.571888: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:57:01.572185: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
Epoch 1/10
   1/1218 [..............................] - ETA: 0s - loss: 0.7607 - accuracy: 0.6094WARNING:tensorflow:From C:\Users\matej\anaconda3\lib\site-packages\tensorflow\python\ops\summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.
Instructions for updating:
use `tf.profiler.experimental.stop` instead.

2020-08-29 18:45:48.778870: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-29 18:45:52.432459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-08-29 18:45:52.566076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-08-29 18:45:52.568033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-29 18:45:52.598770: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-29 18:45:52.635186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-29 18:45:52.651335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-29 18:45:52.693526: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-29 18:45:52.707974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-29 18:45:52.708778: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-08-29 18:45:52.709174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-08-29 18:45:52.711705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-29 18:45:52.735435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15544f7c200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-29 18:45:52.735933: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-29 18:45:52.737008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-29 18:45:52.737368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
2020-08-29 18:45:53.679101: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:45:53.680145: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
2020-08-29 18:45:53.682001: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found
2020-08-29 18:45:53.682796: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2020-08-29 18:45:53.683220: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-08-29 18:57:01.571888: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:57:01.572185: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-08-29 18:57:07.812968: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:57:07.813318: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
   2/1218 [..............................] - ETA: 4:13 - loss: 0.8007 - accuracy: 0.5312WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0698s vs `on_train_batch_end` time: 0.3459s). Check your callbacks.
  15/1218 [..............................] - ETA: 1:22 - loss: 0.8180 - accuracy: 0.4938
2020-08-29 18:45:48.778870: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-29 18:45:52.432459: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-08-29 18:45:52.566076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 3GB computeCapability: 6.1
coreClock: 1.7085GHz coreCount: 9 deviceMemorySize: 3.00GiB deviceMemoryBandwidth: 178.99GiB/s
2020-08-29 18:45:52.568033: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-29 18:45:52.598770: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-29 18:45:52.635186: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-29 18:45:52.651335: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-29 18:45:52.693526: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-29 18:45:52.707974: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-29 18:45:52.708778: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cudnn64_7.dll'; dlerror: cudnn64_7.dll not found
2020-08-29 18:45:52.709174: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1753] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
2020-08-29 18:45:52.711705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-29 18:45:52.735435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x15544f7c200 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-29 18:45:52.735933: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-29 18:45:52.737008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-29 18:45:52.737368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      
2020-08-29 18:45:53.679101: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:45:53.680145: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1391] Profiler found 1 GPUs
2020-08-29 18:45:53.682001: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti64_101.dll'; dlerror: cupti64_101.dll not found
2020-08-29 18:45:53.682796: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'cupti.dll'; dlerror: cupti.dll not found
2020-08-29 18:45:53.683220: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-08-29 18:57:01.571888: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:57:01.572185: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-08-29 18:57:07.812968: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session started.
2020-08-29 18:57:07.813318: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1441] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.
2020-08-29 18:57:07.943306: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:223]  GpuTracer has collected 0 callback api events and 0 activity events. 
2020-08-29 18:57:08.092651: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07
2020-08-29 18:57:08.155731: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for trace.json.gz to logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07\DESKTOP-KSAFH0A.trace.json.gz
2020-08-29 18:57:08.201015: I tensorflow/core/profiler/rpc/client/save_profile.cc:176] Creating directory: logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07
2020-08-29 18:57:08.206548: I tensorflow/core/profiler/rpc/client/save_profile.cc:182] Dumped gzipped tool data for memory_profile.json.gz to logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07\DESKTOP-KSAFH0A.memory_profile.json.gz
2020-08-29 18:57:08.220482: I tensorflow/python/profiler/internal/profiler_wrapper.cc:111] Creating directory: logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07Dumped tool data for xplane.pb to logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07\DESKTOP-KSAFH0A.xplane.pb
Dumped tool data for overview_page.pb to logs/.format(NAME)\train\plugins\profile\2020_08_29_16_57_07\DESKTOP-KSAFH0A.overview_page.pb**

Can somebody help me please??

Thanks in advance and BR!"
42773,You should use `dataset.take(k).cache().repeat()` instead.,"Tensorflow 2.3
Write some code , get a warning : You should use `dataset.take(k).cache().repeat()` instead. 

```
import pathlib
import tensorflow as tf
from tensorflow import keras
from datetime import datetime

IMG_HEIGHT = 224
IMG_WIDTH = 224
NUM_WORKERS = 4
batch_size = 128
epochs = 2000
AUTOTUNE = tf.data.experimental.AUTOTUNE
train_path = ""train_gen""
valid_path = ""valid_gen""

gpus = tf.config.experimental.list_physical_devices('GPU')
tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6000)])

def get_dataset(data_dir,subset):
    ds = tf.keras.preprocessing.image_dataset_from_directory(
        data_dir,
        validation_split=0.2,
        subset=subset,
        seed=123,
        image_size=(IMG_HEIGHT,IMG_WIDTH),
        batch_size=batch_size)
    return ds

train_ds = get_dataset(train_path,'training')
valid_ds = get_dataset(valid_path,'validation')

train_ds = train_ds.cache().shuffle(10).prefetch(buffer_size=AUTOTUNE).repeat()
valid_ds = valid_ds.cache().prefetch(buffer_size=AUTOTUNE).repeat()


tinydarknet = keras.Sequential([
    keras.layers.Conv2D(16, (3, 3), strides=[1, 1], padding=""same"", input_shape=(IMG_HEIGHT,IMG_WIDTH, 3)),
    keras.layers.BatchNormalization(),
 .........
   
])


logs = ""logs/"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")
tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,
                                                 histogram_freq = 1,
                                                 profile_batch = '500,520')

tinydarknet.compile(optimizer=""adam"",
             loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
             metrics=[""accuracy""])

history = tinydarknet.fit(
    train_ds,
    steps_per_epoch=15400 // batch_size,
    epochs=epochs,
    validation_data=valid_ds,
    validation_steps=3100 // batch_size,
    workers=NUM_WORKERS,
    callbacks = [tboard_callback]
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss=history.history['loss']
val_loss=history.history['val_loss']

epochs_range = range(epochs)



#save model
tinydarknet.save(""keras_model"")

converter = tf.lite.TFLiteConverter.from_keras_model(tinydarknet)
tflite_model = converter.convert()

with open(""tinydarknet.tflite"", ""w+b"") as fp:
    fp.write(tflite_model)
    fp.flush()
```
![image](https://user-images.githubusercontent.com/10364552/91641524-8acd8300-ea57-11ea-8220-af0778342757.png)

How to use the cache ?

"
42772,Quantum Generative Adverserial Networks ,"I am working with a short qGANs project where a shorter version of my code which doesn't include all the terms required to calculate the loss function compiles and runs. The code is here: 
```

import pennylane as qml
import numpy as np
import tensorflow as tf


dev1 = qml.device('default.qubit', wires=1) #real
dev2 = qml.device('default.qubit', wires=1) #generator


@qml.qnode(dev1, interface=""tf"")
def real_circuit(weights):
    
    qml.RX(weights[0], wires=0)
    qml.RY(weights[1], wires=0)
    qml.RZ(weights[2], wires=0)

    return qml.expval(qml.PauliX(0))

@qml.qnode(dev2, interface=""tf"")
def gen_circuit(weights):
    
    qml.RX(weights[0], wires=0)
    qml.RY(weights[1], wires=0)
    qml.RZ(weights[2], wires=0)

    return qml.expval(qml.PauliX(0)) 

def discriminator(disc_weights, real_exp, gen_exp):
    
    psi = disc_weights[0] * real_exp
    phi = disc_weights[1] * gen_exp
    
    return psi, phi 
    

def disc_loss(disc_weights):
    
    psi, phi = discriminator(disc_weights, real_exp, gen_exp)
    loss = psi - phi 
    
    return -loss 


def gen_loss(gen_weights):
    
    gen_exp = gen_circuit(gen_weights)
    
    psi, phi = discriminator(disc_weights, real_exp, gen_exp)
    loss = psi - phi 

    return loss 

real_weights = np.random.uniform(0, 2*np.pi, 3)
real_exp = real_circuit(real_weights)
real_sv = dev1.state

init_gen_weights = np.random.uniform(0, 2*np.pi, 3)
gen_weights = tf.Variable(init_gen_weights)
gen_exp = gen_circuit(gen_weights)
gen_sv = dev2.state

init_disc_weights = np.random.uniform(0, 2*np.pi, 2)
disc_weights = tf.Variable(init_disc_weights)

fidelity(real_sv, gen_sv)

print('initial fidelity is ' , fidelity(real_sv, gen_sv))
print('real weights ', real_weights)
print('initial gen weights are ', init_gen_weights)
print('initial disc weights are ', init_disc_weights)

opt = tf.keras.optimizers.SGD(0.4)

cost = lambda: disc_loss(disc_weights)

for step in range(50):
    opt.minimize(cost, disc_weights)
    if step % 5 == 0:
        cost_val = cost().numpy()
        print(""Step {}: cost = {}"".format(step, cost_val))

cost = lambda: gen_loss(gen_weights)

for step in range(50):
    opt.minimize(cost, gen_weights)
    if step % 5 == 0:
        cost_val = cost().numpy()
        print(""Step {}: cost = {}"".format(step, cost_val))



```

Now that I scale up the problem to output more values from the circuit and train more parameters and include more terms in the loss function, the code ceases to work. The extended code for which is here: 
```
import pennylane as qml
import numpy as np
import tensorflow as tf


dev1 = qml.device('default.qubit', wires=1) #real
dev2 = qml.device('default.qubit', wires=1) #generator

obs_list = [qml.PauliX(0) , qml.PauliY(0), qml.PauliZ(0) , qml.Identity(0)]


def real_circuit(weights, wires, **kwargs):
    
    qml.RX(weights[0], wires=0)
    qml.RY(weights[1], wires=0)
    qml.RZ(weights[2], wires=0)

qnodes_real = qml.map(real_circuit, obs_list, dev1, measure=""expval"",  interface=""tf"")


def gen_circuit(weights, wires, **kwargs):
    
    qml.RX(weights[0], wires=0)
    qml.RY(weights[1], wires=0)
    qml.RZ(weights[2], wires=0)

qnodes_gen = qml.map(gen_circuit, obs_list, dev2, measure=""expval"",  interface=""tf"")


def discriminator(disc_weights, real_exp, gen_exp):
    
    psi = disc_weights[0] * real_exp[0] + disc_weights[1] * real_exp[1] + disc_weights[2] * real_exp[2] + disc_weights[3] * real_exp[3] 
    phi = disc_weights[4] * gen_exp[0] + disc_weights[5] * gen_exp[1] + disc_weights[6] * gen_exp[2] + disc_weights[7] * gen_exp[3] 
    
    return psi, phi 
    

def disc_loss(disc_weights):
    
    psi, phi = discriminator(disc_weights, real_exp, gen_exp)
    loss = psi - phi 
    
    return -loss 


def gen_loss(gen_weights):

    gen_exp = qnodes_gen(gen_weights.numpy())

    psi, phi = discriminator(disc_weights, real_exp, gen_exp)
    loss = psi - phi 

    return loss 

real_weights = np.random.uniform(0, 2*np.pi, 3)
real_exp = qnodes_real(real_weights)#calculate the expectation values from the real circuit 

init_gen_weights = np.random.uniform(0, 2*np.pi, 3)
gen_weights = tf.Variable(init_gen_weights)
gen_exp = qnodes_gen(gen_weights.numpy()) #calculate the expectation values from the generator 

init_disc_weights = np.random.uniform(0, 2*np.pi, 8)
disc_weights = tf.Variable(init_disc_weights)

print('real weights ', real_weights)
print('initial gen weights are ', init_gen_weights)
print('initial disc weights are ', init_disc_weights)

opt = tf.keras.optimizers.SGD(0.4)

cost = lambda: disc_loss(disc_weights)

for step in range(50):
    opt.minimize(cost, disc_weights)
    if step % 5 == 0:
        cost_val = cost().numpy()
        print(""Step {}: cost = {}"".format(step, cost_val))
    

cost = lambda: gen_loss(gen_weights)

for step in range(50):
    opt.minimize(cost, gen_weights)
    if step % 5 == 0:
        cost_val = cost().numpy()
        print(""Step {}: cost = {}"".format(step, cost_val))

```

The error I am getting is `ValueError: No gradients provided for any variable: ['Variable:0'].` 

Any ideas? Thanks. 
"
42771,tfp.distributions works on TF vrsn 2.1 but not in 2.2 or 2.3......why?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42769,"bazel build error, //tensorflow:libtensorflow_cc.so is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import)","I have used a proto(lower version) in my project , so I want to build a dynamic link library that not rely proto
my BUILD：
![image](https://user-images.githubusercontent.com/20791462/91634620-3bba2a80-ea24-11ea-80ac-b3cff8384221.png)

**build error:**
in deps attribute of cc_binary rule //tensorflow/extern/crflstmmodel-api:libcrflstmmodel.so: cc_binary rule '**//tensorflow:libtensorflow_cc.so**' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '**//tensorflow:libtensorflow_cc.so**' does not have mandatory providers: 'CcSkylarkInfo'"
42768,Tensorflow 2.0 killed during training (opened graph),"Hello, I followed the Seq2seq Attention mechanisms to make Tacotron. I modified (https://www.tensorflow.org/tutorials/text/nmt_with_attention) this webpage but my code doesn't work.

Here is the difference things from upper pages.

- Model: same as Tacotron
- Dataset
Each of my dataset are bigger than text dataset because my decoder input is spectrogram (batch_size, 161, 400). 

So when I followed this:
```
@tf.function
def train_step(inp, targ, enc_hidden):
  loss = 0

  with tf.GradientTape() as tape:
    enc_output, enc_hidden = encoder(inp, enc_hidden)

    dec_hidden = enc_hidden

    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)

    # Teacher forcing - feeding the target as the next input
    for t in range(1, targ.shape[1]):
      # passing enc_output to the decoder
      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)

      loss += loss_function(targ[:, t], predictions)

      # using teacher forcing
      dec_input = tf.expand_dims(targ[:, t], 1)
```

as 

```
@tf.function(input_signature=train_step_signature)
def train_step(txt_inp, mel_inp, mag_inp, emo_inp):
    
    with tf.GradientTape() as tape:
        # text_inp = [Batch, 200]
        # mel_inp = [Batch, 160, 400]
        # mag_inp = [Batch, 160, 1025]
        # emo_inp = [Batch, 7]
        
        enc_outputs, _ = encoderNetwork(txt_inp)
        
        go_frames = tf.tile([[0.0]], [args.batch_size, args.n_mels * args.r_factor]) #[batch_size, 400]
        go_frames = tf.expand_dims(go_frames, axis=1)
        mel_input = tf.concat((go_frames, mel_inp), axis=1) #[batch_size, 161, 400]
        hidden1 = None
        hidden2 = None
        hidden3 = None
        prev_context = None
        target = mel_input[:, :, -args.n_mels:] #[batch_size, 161, 80]
        
        for i in range(mel_input.shape[1]): # 161
            
            t_mel_hats, t_alignments, hidden1, hidden2, hidden3, prev_context = decoderNetwork(tf.expand_dims(target[:, i, :], axis=1), emo_inp, enc_outputs, _, hidden1, hidden2, hidden3, prev_context, i)
            
            if i == 0:
                mel_hats = t_mel_hats
                alignments = t_alignments
            else:
                mel_hats = tf.concat((mel_hats, t_mel_hats), axis=1)
                alignments = tf.concat((alignments, t_alignments), axis=0)
            print('{}th finished, mel_hats {} alignments {} next input {}'.format(i, mel_hats.shape, alignments.shape, target.shape))
        
        print('vocoder starts, dec output', mel_hats.shape)
        mag_outputs = vocoderNetwork(mel_hats) 
        
        
        loss1 = loss_function(mel_inp, mel_hats[:, 1:, :])
        loss2 = loss_function(mag_inp, mag_outputs[:, 1*args.r_factor:, :])
        
        loss = loss1 + loss2

    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables + vocoderNetwork.trainable_variables
    gradients = tape.gradient(loss, variables)
    optimizer.apply_gradients(zip(gradients, variables))
    train_loss(loss)

    return mag_outputs, mel_outputs, alignments
```

Firstly, my code is work because when I change to not using iteration, I checked that it worked. And when construct graph, (before getting higher of GPU memory) it doesn't occured problem.

Actually it have to work after construct graphs. But after ```loss = loss1 + loss 2 ``` in my code, it doesn't work.  So I just waited, and 1 more graphs try to construct after 1 hour. Finally, the message come to me: ```Killed```.

I want to use (https://www.tensorflow.org/tutorials/text/nmt_with_attention) this tutorial because I want to use teacher forcing and decoder hidden state each outputs on iteration. In pytorch framework, I checked this seq2seq type model could work but I wonder why it doesn't work on tensorflow.


More information about my server
- GPUS: NVIDIA Quadro GV 100
- Mem: 503G
- Swp: 29.8G
- number of CPU processors: 32
- each CPU: Intel (R) Xeon(R) Silver 4108 CPU @ 1.80GHz




"
42766,C++ compilation of rule '//tensorflow/core/kernels/batching_util:batch_resource_base' failed,"**System information**
- OS Platform and Distribution: Ubuntu 18.04.5 LTS
- TensorFlow installed from: source
- TensorFlow version: master
- Installed using: Bazel
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: N/A
- Machine info: AWS EC2 t2.2xlarge - 8 vCPU & 32GiB

**Describe the problem**
When trying to Cross build TensorFlow Lite Standalone Pip with Bazel for aarch64 (Python3.5 and Python3.7)
using the next tutorial: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/pip_package
I'm getting the next error: ""C++ compilation of rule '//tensorflow/core/kernels/batching_util:batch_resource_base' failed""

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1) git clone https://github.com/tensorflow/tensorflow.git tf-github-master
2) cd /tf-github-master/tensorflow/tensorflow/lite/tools/pip_package/
3) tried both of the next commands and got the same issue:
3.1) sudo CI_DOCKER_EXTRA_PARAMS=""-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true"" \
../../../tools/ci_build/ci_build.sh PI-PYTHON3 \
tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64
3.2) sudo CI_DOCKER_EXTRA_PARAMS=""-e CUSTOM_BAZEL_FLAGS=--define=tflite_pip_with_flex=true"" \
../../../tools/ci_build/ci_build.sh PI-PYTHON37 \
tensorflow/lite/tools/pip_package/build_pip_package_with_bazel.sh aarch64


**Any other info / logs**
ERROR: /workspace/tensorflow/core/kernels/batching_util/BUILD:236:1: C++ compilation of rule '//tensorflow/core/kernels/batching_util:batch_resource_base' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command
  (cd /home/ubuntu/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH='' \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/local/bin/python3.7 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CONFIGURE_IOS=0 \
  /home/ubuntu/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/bin/aarch64-linux-gnu-gcc -fstack-protector -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -isystem /home/ubuntu/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include -isystem /home/ubuntu/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/lib/gcc/aarch64-linux-gnu/8.3.0/include-fixed -isystem /home/ubuntu/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/aarch64-linux-gnu/include/c++/8.3.0/ -isystem /home/ubuntu/tensorflow/bazel-ci_build-cache/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/aarch64_linux_toolchain/aarch64-linux-gnu/libc/usr/include/ -isystem /usr/include/python3.7 -isystem /usr/include/ -MD -MF bazel-out/aarch64-opt/bin/tensorflow/core/kernels/batching_util/_objs/batch_resource_base/batch_resource_base.pic.d '-frandom-seed=bazel-out/aarch64-opt/bin/tensorflow/core/kernels/batching_util/_objs/batch_resource_base/batch_resource_base.pic.o' -fPIC -DTF_USE_SNAPPY -DHAVE_SYS_UIO_H -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -iquote . -iquote bazel-out/aarch64-opt/bin -iquote external/com_google_absl -iquote bazel-out/aarch64-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/aarch64-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/aarch64-opt/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/aarch64-opt/bin/external/local_config_sycl -iquote external/gif -iquote bazel-out/aarch64-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/aarch64-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/aarch64-opt/bin/external/com_google_protobuf -iquote external/zlib -iquote bazel-out/aarch64-opt/bin/external/zlib -iquote external/com_googlesource_code_re2 -iquote bazel-out/aarch64-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/aarch64-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/aarch64-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/aarch64-opt/bin/external/highwayhash -iquote external/double_conversion -iquote bazel-out/aarch64-opt/bin/external/double_conversion -iquote external/snappy -iquote bazel-out/aarch64-opt/bin/external/snappy -isystem external/nsync/public -isystem bazel-out/aarch64-opt/bin/external/nsync/public -isystem external/eigen_archive -isystem bazel-out/aarch64-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/aarch64-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/aarch64-opt/bin/external/com_google_protobuf/src -isystem external/zlib -isystem bazel-out/aarch64-opt/bin/external/zlib -isystem external/farmhash_archive/src -isystem bazel-out/aarch64-opt/bin/external/farmhash_archive/src -isystem external/double_conversion -isystem bazel-out/aarch64-opt/bin/external/double_conversion -w -DAUTOLOAD_DYNAMIC_KERNELS -O3 '-std=c++14' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c tensorflow/core/kernels/batching_util/batch_resource_base.cc -o bazel-out/aarch64-opt/bin/tensorflow/core/kernels/batching_util/_objs/batch_resource_base/batch_resource_base.pic.o)
Execution platform: @local_execution_config_platform//:platform
In file included from ./tensorflow/core/lib/monitoring/percentile_sampler.h:27,
                 from tensorflow/core/kernels/batching_util/batch_resource_base.cc:22:
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h: In instantiation of 'static tensorflow::monitoring::PercentileSampler<NumLabels>* tensorflow::monitoring::PercentileSampler<NumLabels>::New(const tensorflow::monitoring::MetricDef<(tensorflow::monitoring::MetricKind)1, tensorflow::monitoring::Percentiles, NumLabels>&, std::vector<double>, size_t, tensorflow::monitoring::UnitOfMeasure) [with int NumLabels = 2; size_t = long unsigned int]':
tensorflow/core/kernels/batching_util/batch_resource_base.cc:31:69:   required from here
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h:63:10: error: no matching function for call to 'tensorflow::monitoring::PercentileSampler<2>::PercentileSampler()'
   return new PercentileSampler<NumLabels>();
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:29,
                 from ./tensorflow/core/platform/logging.h:27,
                 from ./tensorflow/core/framework/allocator.h:28,
                 from ./tensorflow/core/framework/op_kernel.h:24,
                 from ./tensorflow/core/kernels/batching_util/batch_resource_base.h:21,
                 from tensorflow/core/kernels/batching_util/batch_resource_base.cc:16:
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h:54:31: note: candidate: 'tensorflow::monitoring::PercentileSampler<NumLabels>::PercentileSampler(const tensorflow::monitoring::PercentileSampler<NumLabels>&) [with int NumLabels = 2]' <deleted>
   TF_DISALLOW_COPY_AND_ASSIGN(PercentileSampler);
                               ^~~~~~~~~~~~~~~~~
./tensorflow/core/platform/macros.h:112:3: note: in definition of macro 'TF_DISALLOW_COPY_AND_ASSIGN'
   TypeName(const TypeName&) = delete;         \
   ^~~~~~~~
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h:54:31: note:   candidate expects 1 argument, 0 provided
   TF_DISALLOW_COPY_AND_ASSIGN(PercentileSampler);
                               ^~~~~~~~~~~~~~~~~
./tensorflow/core/platform/macros.h:112:3: note: in definition of macro 'TF_DISALLOW_COPY_AND_ASSIGN'
   TypeName(const TypeName&) = delete;         \
   ^~~~~~~~
In file included from ./tensorflow/core/lib/monitoring/percentile_sampler.h:27,
                 from tensorflow/core/kernels/batching_util/batch_resource_base.cc:22:
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h: In instantiation of 'static tensorflow::monitoring::PercentileSampler<NumLabels>* tensorflow::monitoring::PercentileSampler<NumLabels>::New(const tensorflow::monitoring::MetricDef<(tensorflow::monitoring::MetricKind)1, tensorflow::monitoring::Percentiles, NumLabels>&, std::vector<double>, size_t, tensorflow::monitoring::UnitOfMeasure) [with int NumLabels = 1; size_t = long unsigned int]':
tensorflow/core/kernels/batching_util/batch_resource_base.cc:43:69:   required from here
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h:63:10: error: no matching function for call to 'tensorflow::monitoring::PercentileSampler<1>::PercentileSampler()'
   return new PercentileSampler<NumLabels>();
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from ./tensorflow/core/platform/default/logging.h:29,
                 from ./tensorflow/core/platform/logging.h:27,
                 from ./tensorflow/core/framework/allocator.h:28,
                 from ./tensorflow/core/framework/op_kernel.h:24,
                 from ./tensorflow/core/kernels/batching_util/batch_resource_base.h:21,
                 from tensorflow/core/kernels/batching_util/batch_resource_base.cc:16:
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h:54:31: note: candidate: 'tensorflow::monitoring::PercentileSampler<NumLabels>::PercentileSampler(const tensorflow::monitoring::PercentileSampler<NumLabels>&) [with int NumLabels = 1]' <deleted>
   TF_DISALLOW_COPY_AND_ASSIGN(PercentileSampler);
                               ^~~~~~~~~~~~~~~~~
./tensorflow/core/platform/macros.h:112:3: note: in definition of macro 'TF_DISALLOW_COPY_AND_ASSIGN'
   TypeName(const TypeName&) = delete;         \
   ^~~~~~~~
./tensorflow/core/lib/monitoring/mobile_percentile_sampler.h:54:31: note:   candidate expects 1 argument, 0 provided
   TF_DISALLOW_COPY_AND_ASSIGN(PercentileSampler);
                               ^~~~~~~~~~~~~~~~~
./tensorflow/core/platform/macros.h:112:3: note: in definition of macro 'TF_DISALLOW_COPY_AND_ASSIGN'
   TypeName(const TypeName&) = delete;         \
   ^~~~~~~~
Target //tensorflow/lite/python/interpreter_wrapper:_pywrap_tensorflow_interpreter_wrapper failed to build
INFO: Elapsed time: 1460.329s, Critical Path: 93.08s
INFO: 4912 processes: 4912 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
"
42765,Error when using 'make' on hello_world_test (from TinyML book page 84.),"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, Ubuntu on WSL - 20.04.1
- TensorFlow installed from (source or binary): -
- Tensorflow version (commit SHA if source): -
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Arduino Nano, Arduino Portenta H7

**Describe the problem**
I've been doing the exercises from the TinyML book and on page 84 it covers the step to use 'make' to run tests on the code. I'm using windows 10 and I did not have make or git installed on it so I went and installed Git 2.28.0 from their website and intalled make 4.3 through Chocolatey. 
After running the command from page 84 : ""make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test"" I get the following message: 

C:\Users\zalan\tensorflow>make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
process_begin: CreateProcess(NULL, uname -m, …) failed.
tensorflow/lite/micro/tools/make/Makefile:32: pipe: No error
-m was unexpected at this time.
FIND: Parameter format not correct
make: *** No rule to make target ‘test_hello_world_test’. Stop.

As I couldn't figure out what the problem was I went online and saw that somone experienced the same problem and downloaded Ubuntu on WSL on windows and managed to run make.
I've tried the same, downloaded the latest version and installed make and when I ran the code I got the following error:

zal_n@LAPTOP-74R8EDS8:/mnt/c/users/zalan/tensorflow$ make -f tensorflow/lite/micro/tools/make/Makefile test_hello_world_test
tensorflow/lite/micro/tools/make/Makefile:306: warning: overriding recipe for target ‘tensorflow/lite/micro/tools/make/downloads/ruy’
tensorflow/lite/micro/tools/make/Makefile:306: warning: ignoring old recipe for target ‘tensorflow/lite/micro/tools/make/downloads/ruy’
tensorflow/lite/micro/tools/make/Makefile:306: warning: overriding recipe for target ‘tensorflow/lite/micro/tools/make/downloads/person_model_grayscale’
tensorflow/lite/micro/tools/make/Makefile:306: warning: ignoring old recipe for target ‘tensorflow/lite/micro/tools/make/downloads/person_model_grayscale’
tensorflow/lite/micro/tools/make/Makefile:306: warning: overriding recipe for target ‘tensorflow/lite/micro/tools/make/downloads/person_model_int8’
tensorflow/lite/micro/tools/make/Makefile:306: warning: ignoring old recipe for target ‘tensorflow/lite/micro/tools/make/downloads/person_model_int8’
tensorflow/lite/micro/tools/make/download_and_extract.sh “https://github.com/google/gemmlowp/archive/719139ce755a0f31cbf1c37f7f98adcc7fc9f425.zip” “7e8191b24853d75de2af87622ad293ba” tensorflow/lite/micro/tools/make/downloads/gemmlowp
tensorflow/lite/micro/tools/make/download_and_extract.sh: line 16: '\r': command not found tensorflow/lite/micro/tools/make/download_and_extract.sh: line 24: ‘\r’: command not found
: invalid optionmicro/tools/make/download_and_extract.sh: line 25: set: -
set: usage: set [-abefhkmnptuvxBCHP] [-o option-name] [–] [arg …]
tensorflow/lite/micro/tools/make/download_and_extract.sh: line 26: '\r': command not found tensorflow/lite/micro/tools/make/download_and_extract.sh: line 28: syntax error near unexpected token `‘{\r’’
'ensorflow/lite/micro/tools/make/download_and_extract.sh: line 28: `patch_am_sdk() {
make: *** [tensorflow/lite/micro/tools/make/Makefile:306: tensorflow/lite/micro/tools/make/downloads/gemmlowp] Error 2

I'm not sure what to do at this point so any help would be appreciated. 


"
42764,Tensorflow uses only GPU0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: na
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.0.0
- Python version: 3.6.5
- CUDA/cuDNN version: 7.6.5
- GPU model and memory:
> name: ""/device:GPU:0""
   device_type: ""GPU""
   memory_limit: 32040203060
   physical_device_desc: ""device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:61:00.0, compute capability: 7.0""
> name: ""/device:GPU:1""
   device_type: ""GPU""
   memory_limit: 32040203060
  physical_device_desc: ""device: 1, name: Tesla V100-SXM2-32GB, pci bus id: 0000:62:00.0, compute capability: 7.0""

**Describe the current behavior**
I am trying to define scope of code for GPU1. However, the code is executed on GPU 0.
For example, output of the test code given below is

Input device : /GPU:1
2020-08-29 00:15:42.093921: I tensorflow/core/common_runtime/eager/execute.cc:574] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-29 00:15:42.094117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)


Both GPUS 0 and 1 are available

**Describe the expected behavior**
The code should be executed on GPU 1

**Standalone code to reproduce the issue**
Test code: 
iimport tensorflow as tf

tf.debugging.set_log_device_placement(True)
tf.config.set_soft_device_placement(False)

device = '/GPU:1'
with tf.device(device):
        print(""Input device : "" + device)
        a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
        b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

c = tf.matmul(a, b)
print(c)

"
42763,"NaN occurs when building with GaussianNoise, GaussianNoise and Dense","

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):tensorflow2-GPU
- Python version:3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I got the NaN as loss when I tried to train my model with GaussianNoise, GaussianNoise and Dense
**Describe the expected behavior**
The loss should be a number
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
batch_size = 110
epochs = 128
num_classes = 10
import os
save_dir = 'model'
model_name = 'test940_trained_model.h5'
import tensorflow.keras as keras
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
img_rows, img_cols = x_train.shape[1], x_train.shape[2]

x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.models.Sequential()
model.add(keras.layers.GaussianNoise(stddev=0.7498748441096037))

model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(num_classes, activation='relu'))
model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
print('Test accuracy:', score[1])
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Epoch 1/128

  110/60000 [..............................] - ETA: 43:48 - loss: 9.9702 - accuracy: 0.0636
  660/60000 [..............................] - ETA: 7:18 - loss: 9.6680 - accuracy: 0.0848 
 1100/60000 [..............................] - ETA: 4:24 - loss: 9.4182 - accuracy: 0.0991
 1540/60000 [..............................] - ETA: 3:10 - loss: nan - accuracy: 0.1019   
 1760/60000 [..............................] - ETA: 2:47 - loss: nan - accuracy: 0.1000
 2200/60000 [>.............................] - ETA: 2:14 - loss: nan - accuracy: 0.0995
 2530/60000 [>.............................] - ETA: 1:58 - loss: nan - accuracy: 0.0992
 3080/60000 [>.............................] - ETA: 1:37 - loss: nan - accuracy: 0.1042
 3630/60000 [>.............................] - ETA: 1:23 - loss: nan - accuracy: 0.1047
```"
42762,NaN occurs when building,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):tensorflow2-GPU
- Python version:3.6


**Describe the current behavior**
I got the NaN loss when I tried to train my model
**Describe the expected behavior**
The loss should be a number
**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
batch_size = 112
epochs = 119
num_classes = 10
import os
save_dir = 'model'
model_name = 'test971_trained_model.h5'
import tensorflow.keras as keras
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
img_rows, img_cols = x_train.shape[1], x_train.shape[2]
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.models.Sequential()
model.add(keras.layers.BatchNormalization(momentum = 0.4639004933194679,epsilon=0.6515653837017596))
model.add(keras.layers.PReLU(alpha_initializer='Zeros'))

model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(num_classes))
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
print('Test accuracy:', score[1])
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
Epoch 1/119

  112/50000 [..............................] - ETA: 37:05 - loss: nan - accuracy: 0.0804
  448/50000 [..............................] - ETA: 9:20 - loss: nan - accuracy: 0.0826 
  784/50000 [..............................] - ETA: 5:22 - loss: nan - accuracy: 0.0778
```"
42761,ModuleNotFoundError: No module named 'gast',"<em>Tensorflow==2.3 installed successfully, but, while importing, it throws ""ModuleNotFoundError: No module named 'gast'"".  where as gast==0.3.3 is installed. </em>

**System information**
- MacOS 10.15.6
- Environment:  Anaconda
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version:2.3
- Python version:Python 3.7.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:NO
- GPU model and memory:16 GB



**Describe the problem**

I am able to install tensorflow 2.3 successfully (actually upgraded, then uninstalled, and installed again). but, while importing it throws

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 46, in <module>
    from tensorflow.python import distribute
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/__init__.py"", line 23, in <module>
    from tensorflow.python.distribute import cross_device_ops
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_ops.py"", line 29, in <module>
    from tensorflow.python.distribute import cross_device_utils
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/cross_device_utils.py"", line 25, in <module>
    from tensorflow.python.distribute import values as value_lib
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/values.py"", line 23, in <module>
    from tensorflow.python.distribute import distribute_lib
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py"", line 200, in <module>
    from tensorflow.python.autograph.core import ag_ctx as autograph_ctx
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/__init__.py"", line 37, in <module>
    from tensorflow.python.autograph.core.converter import ConversionOptions
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/core/converter.py"", line 69, in <module>
    from tensorflow.python.autograph.pyct import anno
  File ""/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/anno.py"", line 28, in <module>
    import gast
ModuleNotFoundError: No module named 'gast'




"
42760,PReLU fails when alpha_initializer='Identity',"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):TF2-GPU
- Python version:3.6



**Describe the current behavior**
When I tried to define a PReLU layer and set alpha_initializer='Identity', the program crashed.
**Describe the expected behavior**
when I use MNIST it works. I expect that CIFAR-10 and CIFAR-100 should got the same performance as MNIST.


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
batch_size = 23
epochs = 55
num_classes = 10
import os
save_dir = 'model'
model_name = 'test98_trained_model.h5'
import tensorflow.keras as keras
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
img_rows, img_cols = x_train.shape[1], x_train.shape[2]
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.models.Sequential()
model.add(keras.layers.PReLU(alpha_initializer='Identity'))

model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(num_classes))
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
print('Test accuracy:', score[1])
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
ValueError: Identity matrix initializer can only be used for 2D matrices.

```
"
42759,libtensorflowlite XNNPACK support beyond f32,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information** march=armhf mfpu=neon
- TensorFlow version (you are using): v2.3.0
- Are you willing to contribute it (Yes/No):


**Describe the feature and the current behavior/state.**
In both `master` and `v2.3.0` where xnnpack config is available, it seems that only f32 ops are supported
```
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/delegates/xnnpack/BUILD#L35
cc_library(
    name = ""xnnpack_delegate"",
    ...
    deps = [
        ...
        ""@XNNPACK//:xnnpack_f32"",
    ],
```

Can this be expanded to support quantized models as well? For example, xnnpack already supports `QS8MobileNetV1` 

**Will this change the current api? How?**
No
**Who will benefit with this feature?**

**Any Other info.**
"
42758,Conv2DTranspose failed when kernel_initializer='Identity' with CIFAR-100,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):a GNU/Linux system with Linux kernel 4.15.0 on 1 6-core 3.60GHz Intel Core CPU i7-6850K with 64 GB RAM equipped with a NVIDIA Corporation GP102 GPUs
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):tensorflow2-GPU
- Python version:3.6
- Bazel version (if compiling from source):



**Describe the current behavior**
When I tried to define a Conv2DTranspose layer and set kernel_initializer='Identity', the program crashed.
**Describe the expected behavior**
when I use MNIST it works. I expect that CIFAR-10 and CIFAR-100 should got the same performance as MNIST.




**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
batch_size = 28
epochs = 77
num_classes = 100
import os
save_dir = 'model'
model_name = 'test60_trained_model.h5'
import tensorflow.keras as keras
(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')
img_rows, img_cols = x_train.shape[1], x_train.shape[2]
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
model = keras.models.Sequential()
model.add(keras.layers.Conv2DTranspose(filters=11,kernel_size=(19, 19), strides=(19, 19), padding='valid',activation='elu',kernel_initializer='Identity'))

model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(num_classes))
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adadelta(), metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test accuracy:', score[1])
model_path = os.path.join(save_dir, model_name)
model.save(model_path)
```


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
ValueError: Identity matrix initializer can only be used for 2D matrices.
```


"
42757,tensorflowlite v2.3 + XNNPACK run into error {ModifyGraphWithDelegate is disallowed},"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Raspbian buster 32-bit
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: RPI2
-   **TensorFlow installed from (source or binary)**: source
-   **TensorFlow version (use command below)**: v2.3.0
-   **Python version**: N/A
-   **Bazel version (if compiling from source)**: 3.1.0
-   **GCC/Compiler version (if compiling from source)**:  7.5
-   **CUDA/cuDNN version**: N/A
-   **GPU model and memory**: N/A
-   **Exact command to reproduce**:  N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I built tensorflow `v2.3.0` with XNNPack enabled `--define tflite_with_xnnpack=true` However, when I try to inference with `coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.tflite`, the starter model from [example page](https://www.tensorflow.org/lite/models/object_detection/overview), i see this error 
```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
ERROR: ModifyGraphWithDelegate is disallowed when graph is immutable.
```

The XNNPACK thus was not effective during inference.

Is this error expected? Is so if there a CPU model that can be used with XNNPACK?  (Right now it seems there is a dilemma that the object_dection optimization detailed [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md) is not yet available in tensorflow v2.3.0 but XNNPACK only works with v2.3.0, making `coco_ssd_mobilenet_v1_1.0_quant_2018_06_29.tflite` seeming the only CPU object detection model available at this point.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
42756,STM32F407G Build Error - AddBuiltin() mismatch in arguments,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution: Ubuntu 18.04.5 LTS
- TensorFlow installed from : source
- Tensorflow version : 2.3.0
- Target platform : STM32F407G - Discovery Board
- Tool chain: STM32 Cube IDE Tool chain

**Problem**

Mismatch in number of arguments between the function definition and the calling function in `AddBuiltin()` function.

Function Definition: `tensorflow/lite/micro/micro_mutable_op_resolver.h` (Line 473)

**Code Snippet:  (Number of arguments - 3)**
`TfLiteStatus AddBuiltin(tflite::BuiltinOperator op, const TfLiteRegistration& registration,                           MicroOpResolver::BuiltinParseFunction parser)`

Calling Function: main.cpp (generated using STM Toolchain)

Code Snippet: (Number of arguments - 2)
`tflite_status = micro_op_resolver.AddBuiltin( tflite::BuiltinOperator_FULLY_CONNECTED, tflite::ops::micro::Register_FULLY_CONNECTED());`

**Sequence of Steps Followed:**

We built all the examples for various compilers using this command.  

`$make -f tensorflow/lite/micro/tools/make/Makefile TAGS=""portable_optimized"" generate_non_kernel_projects`

hello_world project is created using STM32 Cube IDE.

The tensorflow and third party folders generated inside hello_world project workspace are copied. 

`main.cpp` is created for hello_world

The project is built in the STM32 Cube IDE. During the build process, we have the following error.
`
'g++' -std=c++11 -DTF_LITE_STATIC_MEMORY -DNDEBUG -O3 -DTF_LITE_DISABLE_X86_NEON -I. -I./third_party/gemmlowp -I./third_party/flatbuffers/include -I./third_party/ruy  -c tensorflow/lite/micro/examples/hello_world/main_functions.cc -o tensorflow/lite/micro/examples/hello_world/main_functions.o
tensorflow/lite/micro/examples/hello_world/main_functions.cc: In function ‘void setup()’:
tensorflow/lite/micro/examples/hello_world/main_functions.cc:81:53: error: no matching function for call to ‘tflite::MicroMutableOpResolver<1>::AddBuiltin(tflite::BuiltinOperator, TfLiteRegistration*)’
   	tflite::ops::micro::Register_FULLY_CONNECTED());
                                                 	^
In file included from ./tensorflow/lite/micro/all_ops_resolver.h:16:0,
             	from tensorflow/lite/micro/examples/hello_world/main_functions.cc:18:
./tensorflow/lite/micro/micro_mutable_op_resolver.h:473:16: note: candidate: TfLiteStatus tflite::MicroMutableOpResolver<tOpCount>::AddBuiltin(tflite::BuiltinOperator, const TfLiteRegistration&, tflite::MicroOpResolver::BuiltinParseFunction) [with unsigned int tOpCount = 1; TfLiteStatus = TfLiteStatus; TfLiteRegistration = TfLiteRegistration; tflite::MicroOpResolver::BuiltinParseFunction = TfLiteStatus (*)(const tflite::Operator*, tflite::BuiltinOperator, tflite::ErrorReporter*, tflite::BuiltinDataAllocator*, void**)]
   TfLiteStatus AddBuiltin(tflite::BuiltinOperator op,
            	^~~~~~~~~~
./tensorflow/lite/micro/micro_mutable_op_resolver.h:473:16: note:   candidate expects 3 arguments, 2 provided
Makefile:36: recipe for target 'tensorflow/lite/micro/examples/hello_world/main_functions.o' failed
`
 

Please suggest the code changes

The above steps are executed as suggested in [youtube](https://www.youtube.com/watch?v=gDFWCxrJruQ) 


"
42755,Unable to find the tf.raw_ops.mfcc python code in tensorflow,"Hello,

I would like to see the procedure of calculating the mfcc and filterbank values in tensorflow package using tf.raw_ops.mfcc, but I could not find it.
The closest code that I found was:
https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/ops/signal/mfcc_ops.py 

Is there anywhere I can find the exact  tf.raw_ops.mfcc function code?

"
42751,Failed to load the native TensorFlow runtime.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10 Home 1909**
- TensorFlow installed from (source or binary): **Tensorflow.org**
- TensorFlow version: **1.13.2 gpu**
- Python version: **3.75 (64-bit)**
- Installed using virtualenv? pip? conda?: **pip**
- CUDA/cuDNN version: **10.0**
- GPU model and memory: **ZOTAC GeForce GT 1030 2GB GDDR5**



**Describe the problem**

So I was trying to run ""CorentinJ's Real Time Voice Cloning project"" using a guide for windows 10 (https://poorlydocumented.com/2019/11/installing-corentinjs-real-time-voice-cloning-project-on-windows-10-from-scratch/) and I followed the guide exactly as it said to.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master> .\pip install tensorflow-gpu==1.13.2`

**I restarted powershell after installation finished**

`C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master> .\python.exe demo_toolbox.py`

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


```
Traceback (most recent call last):
  File ""demo_toolbox.py"", line 2, in <module>
    from toolbox import Toolbox
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master\toolbox\__init__.py"", line 3, in <module>
    from synthesizer.inference import Synthesizer
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master\synthesizer\inference.py"", line 1, in <module>
    from synthesizer.tacotron2 import Tacotron2
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master\synthesizer\tacotron2.py"", line 3, in <module>
    from synthesizer.models import create_model
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master\synthesizer\models\__init__.py"", line 1, in <module>
    from .tacotron import Tacotron
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\scripts\real-time-voice-cloning-master\synthesizer\models\tacotron.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Dolphin EMU\appdata\local\programs\python\python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: A dynamic link library (DLL) initialization routine failed.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

"
42750,Error on doing regression with weighted time steps using sample_weight_mode='temporal',"I am running into problems while doing regression when the output of the network contains multiple values which can be considered as output for a series of time-points. The loss of each output timestep needs to be weighted differently for each sample.

I have built a simple model with a dummy dataset. Here is the [Colab Notebook](https://colab.research.google.com/drive/1WbGHndnon8VbXcYg5Kmarva3l9tui1RB?usp=sharing)

I get an error in the calculation of the weighted losses. My understanding is that the sample weights should have the shape of (batch_size x time_steps)

```
import numpy as np
import tensorflow as tf

dummy_input = tf.random.normal((1024, 1000))
dummy_output = tf.random.normal((1024, 10))
dummy_output_weights = tf.random.uniform(minval=0, maxval=1, shape=(1024, 10))

print(dummy_input.shape)
print(dummy_output.shape)
print(dummy_output_weights.shape)

dummy_model = tf.keras.Sequential()
dummy_model.add(tf.keras.Input(shape=(1000,)))
dummy_model.add(tf.keras.layers.Dense(32, activation='relu'))
dummy_model.add(tf.keras.layers.Dense(10, activation='linear'))

dummy_model.compile(optimizer='sgd', loss='mse', sample_weight_mode='temporal')
dummy_model.summary()

dummy_model.fit(x=dummy_input, y=dummy_output, batch_size=32, epochs=10, sample_weight=dummy_output_weights)
```

Error message
```
Epoch 1/10
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-8-3d254f045172> in <module>()
----> 1 dummy_model.fit(x=dummy_input, y=dummy_output, batch_size=32, epochs=10, sample_weight=dummy_output_weights)

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in _method_wrapper(self, *args, **kwargs)
    106   def _method_wrapper(self, *args, **kwargs):
    107     if not self._in_multi_worker_mode():  # pylint: disable=protected-access
--> 108       return method(self, *args, **kwargs)
    109 
    110     # Running inside `run_distribute_coordinator` already.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1096                 batch_size=batch_size):
   1097               callbacks.on_train_batch_begin(step)
-> 1098               tmp_logs = train_function(iterator)
   1099               if data_handler.should_sync:
   1100                 context.async_wait()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    778       else:
    779         compiler = ""nonXla""
--> 780         result = self._call(*args, **kwds)
    781 
    782       new_tracing_count = self._get_tracing_count()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    821       # This is the first call of __call__, so we have to initialize.
    822       initializers = []
--> 823       self._initialize(args, kwds, add_initializers_to=initializers)
    824     finally:
    825       # At this point we know that the initialization is complete (or less

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    695     self._concrete_stateful_fn = (
    696         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 697             *args, **kwds))
    698 
    699     def invalid_creator_scope(*unused_args, **unused_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   2853       args, kwargs = None, None
   2854     with self._lock:
-> 2855       graph_function, _, _ = self._maybe_define_function(args, kwargs)
   2856     return graph_function
   2857 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3211 
   3212       self._function_cache.missed.add(call_context_key)
-> 3213       graph_function = self._create_graph_function(args, kwargs)
   3214       self._function_cache.primary[cache_key] = graph_function
   3215       return graph_function, args, kwargs

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3073             arg_names=arg_names,
   3074             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3075             capture_by_value=self._capture_by_value),
   3076         self._function_attributes,
   3077         function_spec=self.function_spec,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    984         _, original_func = tf_decorator.unwrap(python_func)
    985 
--> 986       func_outputs = python_func(*func_args, **func_kwargs)
    987 
    988       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    598         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    599         # the function a weak reference to itself to avoid a reference cycle.
--> 600         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    601     weak_wrapped_fn = weakref.ref(wrapped_fn)
    602 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    971           except Exception as e:  # pylint:disable=broad-except
    972             if hasattr(e, ""ag_error_metadata""):
--> 973               raise e.ag_error_metadata.to_exception(e)
    974             else:
    975               raise

ValueError: in user code:

    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:806 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:796 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1211 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2585 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2945 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:789 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:749 train_step
        y, y_pred, sample_weight, regularization_losses=self.losses)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/compile_utils.py:204 __call__
        loss_value = loss_obj(y_t, y_p, sample_weight=sw)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/losses.py:151 __call__
        losses, sample_weight, reduction=self._get_reduction())
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:112 compute_weighted_loss
        losses, sample_weight)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/util.py:143 scale_losses_by_sample_weight
        losses, None, sample_weight)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/util.py:95 squeeze_or_expand_dimensions
        sample_weight = array_ops.squeeze(sample_weight, [-1])
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:507 new_func
        return func(*args, **kwargs)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py:4259 squeeze
        return gen_array_ops.squeeze(input, axis, name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_array_ops.py:10044 squeeze
        ""Squeeze"", input=input, squeeze_dims=axis, name=name)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:593 _create_op_internal
        compute_device)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3485 _create_op_internal
        op_def=op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1975 __init__
        control_input_ops, op_def)
    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1815 _create_c_op
        raise ValueError(str(e))

    ValueError: Can not squeeze dim[1], expected a dimension of 1, got 10 for '{{node mean_squared_error/weighted_loss/Squeeze}} = Squeeze[T=DT_FLOAT, squeeze_dims=[-1]](IteratorGetNext:2)' with input shapes: [32,10].
```"
42748,CMSIS-NN Conv kernel writes outside of allocated memory when num_channels > 256,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): b36436b087bd8e8701ef51718179037cccdfc26e
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ST IoT Discovery Kit

**Describe the problem**

The CMSIS-NN convolutional and depthwise convolutional layers write in uninitialized memory in `CalculateOpData` when `num_channels` is higher than 256 ([kMaxChannels](https://github.com/tensorflow/tensorflow/blob/5d6860d3749109d62964d4e15e267f6b4465258c/tensorflow/lite/micro/kernels/cmsis-nn/conv.cc#L60)). There are no boundary checks for this function, and thus no error is thrown when this happens.  Non CMSIS-NN kernels have no problem. 

I can get around this by switching the structure to use dynamic memory:

```cpp
void* per_channel_output_multiplier;
context->AllocatePersistentBuffer(context, sizeof(int32_t) * num_channels, &per_channel_output_multiplier);
data->per_channel_output_multiplier = (int32_t*)per_channel_output_multiplier;
```

But this is a temporary object, so I assume I should use the scratch buffer instead. However, I have no idea how to clear the object again from the scratch buffer.

**Please provide the exact sequence of commands/steps when you ran into the problem**

The problem shows in the following tflite model. In one of the final layers it has 1280 channels. [ei-plants-vs-lamps-transfer-learning-tensorflow-lite-int8-quantized-model.lite.zip](https://github.com/tensorflow/tensorflow/files/5143770/ei-plants-vs-lamps-transfer-learning-tensorflow-lite-int8-quantized-model.lite.zip)

/cc @kwagyeman This could be the same issue as you're facing with CMSIS-NN. It shows on our image models."
42744,TFLite: TF2.3 error building tensorflow-lite-select-tf-ops,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 2
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):  3.1.0
- GCC/Compiler version (if compiling from source): 9.3.0 
- CUDA/cuDNN version: -
- GPU model and memory: -
- NDK: android-ndk-r20



**Describe the problem**
Getting this error if building tensorflow-lite with tensorflow-lite-select-tf-ops:
```bash
ERROR: /home/${User}/projects/downloads/tensorflow/tensorflow/lite/delegates/flex/BUILD:77:1: C++ compilation of rule '//tensorflow/lite/delegates/flex:delegate_only_runtime' failed (Exit 1)
tensorflow/lite/delegates/flex/delegate.cc:152:37: error: 'TF_AcquireFlexDelegate' has C-linkage specified, but returns user-defined type 'tflite::TfLiteDelegateUniquePtr' (aka 'unique_ptr<TfLiteDelegate, void (*)(TfLiteDelegate *)>') which is incompatible with C [-Werror,-Wreturn-type-c-linkage]
    tflite::TfLiteDelegateUniquePtr TF_AcquireFlexDelegate() {
                                    ^
1 error generated.
Target //tmp:tensorflow-lite-select-tf-ops failed to build
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
git checkout -b master 6e9d916229b5aefbdcfd33cbc4b34c9f48b5e6e1
nano .tf_configure.bazelrc
```

Bazel config .tf_configure.bazelrc:
```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python""
build:xla --define with_xla_support=true
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build --action_env ANDROID_NDK_HOME=""CHANGE_TO_YOUR_ANDROID_NDK_HOME""
build --action_env ANDROID_NDK_API_LEVEL=""21""
build --action_env ANDROID_BUILD_TOOLS_VERSION=""28.0.0""
build --action_env ANDROID_SDK_API_LEVEL=""23""
build --action_env ANDROID_SDK_HOME=""CHANGE_TO_YOUR_ANDROID_SDK_HOME""
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_tag_filters=-benchmark-test,-no_oss,-oss_serial
test --build_tag_filters=-benchmark-test,-no_oss
test --test_tag_filters=-gpu
test --build_tag_filters=-gpu
build --action_env TF_CONFIGURE_IOS=""0""
```
And compile with 
```bash
bazel build --cxxopt='--std=c++14' -c opt --fat_apk_cpu=arm64-v8a,armeabi-v7a --config=monolithic \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  //tensorflow/lite/java:tensorflow-lite \
  //tensorflow/lite/java:tensorflow-lite-gpu \
  //tensorflow/lite/delegates/flex:delegate \
  //tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op \
  //tmp:tensorflow-lite-select-tf-ops
```
or
```bash
EXPORT_DIR=/home/${User}/output/tflite
./tensorflow/lite/tools/build_aar.sh \
  --input_models=$EXPORT_DIR/detect.tflite,$EXPORT_DIR/Decode.tflite,$EXPORT_DIR/Encoder.tflite \
  --target_archs=arm64-v8a,armeabi-v7a \
  --tflite_custom_ops_deps=//tensorflow/lite/experimental/kernels:ctc_beam_search_decoder_op
```

**Fast fix**
Just comment the lines 152-154 in `tensorflow/lite/delegates/flex/delegate.cc` (https://github.com/tensorflow/tensorflow/commit/536f6be302d945513fedff57e65d89f6e0e026db#diff-5b63f45ccbd2b9d4aa2edaf64a92e14b), build and it worked (tested on Pixel 2 XL with Android 11)
```c++
  //  tflite::TfLiteDelegateUniquePtr TF_AcquireFlexDelegate() {
//  return tflite::FlexDelegate::Create();
//}
```"
42742,how to compile 32-bit tensorflow lib and DLL,"I try to compile tensorflow with bazel, but I don't know how to compile 32-bit tensorflow lib and DLL in windows , I only git the 64-bit .So I want to know how to compile DLL and lib of Win32 with bazel or CMake in windows. I use the tensorflow verion of 2.3.
"
42741,ModelCheckpoint behavior doesn't change when changing save_weights_only from True to False for custom tf.keras.Model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Python version: 3.6.9
- Tensorflow version: 2.3.0 ('v2.3.0-0-gb36436b087')

**Describe the current behavior**
I am migrating from TF1 to TF2 and I am trying to understand best practices when it comes to model saving/checkpointing during training.

I would like to checkpoint the model training so as to be able to resume training later.

Quoting from the documentation (https://www.tensorflow.org/guide/keras/save_and_serialize#saving_loading_only_the_models_weights_values)
```
You can choose to only save & load a model's weights. This can be useful if:
* You only need the model for inference: in this case you won't need to restart training, so you don't need the compilation information or optimizer state.
* You are doing transfer learning: in this case you will be training a new model reusing the state of a prior model, so you don't need the compilation information of the prior model.
```

So it seems to me given my use-case of resuming training that I would want to checkpoint the full model and not just the weights correct ? More specifically I would like the following to be saved at each epoch.

```
* The model's architecture/config
* The model's weight values (which were learned during training)
* The model's compilation information (if compile()) was called
* The optimizer and its state, if any (this enables you to restart training where you left)
```

The following code snippet shown in the documentation (https://www.tensorflow.org/tutorials/keras/save_and_load#checkpoint_callback_usage) only saves the weights:
```
checkpoint_path = ""training_1/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

# Train the model with the new callback
model.fit(train_images, 
          train_labels,  
          epochs=10,
          validation_data=(test_images,test_labels),
          callbacks=[cp_callback])  # Pass callback to training
```

So I made a single change to the above code changing `save_weights_only` from False to True
```
checkpoint_path = ""training_1/cp.ckpt""
checkpoint_dir = os.path.dirname(checkpoint_path)

# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=False,   # <- this is the change I made 
                                                 verbose=1)

# Train the model with the new callback
model.fit(train_images, 
          train_labels,  
          epochs=10,
          validation_data=(test_images,test_labels),
          callbacks=[cp_callback])  # Pass callback to training
```
and now I see a cp.ckpt file instead of the `.data` and `.index` file - all good so far.

However the example in the documentation is using a `tf.keras.Sequential` model - and I would like to subclass `tf.keras.Model` instead. So I replicate the example's model using a very simple tf.keras.Model

```
class SimpleModel(tf.keras.Model):
  def __init__(self, **kwargs):
    super().__init__(**kwargs)
    self.layer_1 = keras.layers.Dense(512, activation='relu', input_shape=(784,))
    self.layer_2 = keras.layers.Dropout(0.2)
    self.layer_3 = keras.layers.Dense(10)
  
  def call(self, inputs):
    x = self.layer_1(inputs)
    x = self.layer_2(x)
    return self.layer_3(x)
```
and now I no longer see cp.ckpt files being created by the callback but I see `.data` and `.index` files (indicating just the weights are being saved)

It would be great if someone can explain how to go about checkpointing the full model in a SavedModel (.tf) format for the purpose of resuming training specifically for models implemented by subclassing tf.keras.Model."
42740,AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TENSOR_LIKE_TYPES',"I installed the latest version of keras and tensorflow on anaconda.
The problem looks like the same as the following and it is still awaiting a response
https://github.com/tensorflow/tensorflow/issues/38589

```

from keras.datasets import mnist
from keras.utils import np_utils
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Dropout, Activation, Flatten
from keras.layers.advanced_activations import LeakyReLU
from keras.optimizers import Adam, RMSprop
import numpy as np
import matplotlib.pyplot as plt
import random
from tqdm import notebook

# Dataset of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.
(X_train, Y_train), (X_test, Y_test) = mnist.load_data()

# Preprocessing

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')/255
X_test = X_test.astype('float32')/255


# Set the dimensions of the noise
z_dim = 100

# Optimizer
adam = Adam(lr=0.0002, beta_1=0.5)

g = Sequential()
g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))
g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))
g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))
g.add(Dense(784, activation='sigmoid'))  # Values between 0 and 1
g.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

d = Sequential()
d.add(Dense(1024, input_dim=784, activation=LeakyReLU(alpha=0.2)))
d.add(Dropout(0.3))
d.add(Dense(512, activation=LeakyReLU(alpha=0.2)))
d.add(Dropout(0.3))
d.add(Dense(256, activation=LeakyReLU(alpha=0.2)))
d.add(Dropout(0.3))
d.add(Dense(1, activation='sigmoid'))  # Values between 0 and 1
d.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

d.trainable = False
inputs = Input(shape=(z_dim, ))
hidden = g(inputs)
output = d(hidden)
gan = Model(inputs, output)
gan.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])

```




AttributeError                            Traceback (most recent call last)
<ipython-input-4-fc09a4f0e640> in <module>
      3 
      4 g = Sequential()
----> 5 g.add(Dense(256, input_dim=z_dim, activation=LeakyReLU(alpha=0.2)))
      6 g.add(Dense(512, activation=LeakyReLU(alpha=0.2)))
      7 g.add(Dense(1024, activation=LeakyReLU(alpha=0.2)))

~\anaconda3\envs\tf\lib\site-packages\keras\engine\sequential.py in add(self, layer)
    164                     # and create the node connecting the current layer
    165                     # to the input layer we just created.
--> 166                     layer(x)
    167                     set_inputs = True
    168             else:

~\anaconda3\envs\tf\lib\site-packages\keras\backend\tensorflow_backend.py in symbolic_fn_wrapper(*args, **kwargs)
     73         if _SYMBOLIC_SCOPE.value:
     74             with get_graph().as_default():
---> 75                 return func(*args, **kwargs)
     76         else:
     77             return func(*args, **kwargs)

~\anaconda3\envs\tf\lib\site-packages\keras\engine\base_layer.py in __call__(self, inputs, **kwargs)
    444                 # Raise exceptions in case the input is not compatible
    445                 # with the input_spec specified in the layer constructor.
--> 446                 self.assert_input_compatibility(inputs)
    447 
    448                 # Collect input shapes to build layer.

~\anaconda3\envs\tf\lib\site-packages\keras\engine\base_layer.py in assert_input_compatibility(self, inputs)
    308         for x in inputs:
    309             try:
--> 310                 K.is_keras_tensor(x)
    311             except ValueError:
    312                 raise ValueError('Layer ' + self.name + ' was called with '

~\anaconda3\envs\tf\lib\site-packages\keras\backend\tensorflow_backend.py in is_keras_tensor(x)
    693     ```
    694     """"""
--> 695     if not is_tensor(x):
    696         raise ValueError('Unexpectedly found an instance of type `' +
    697                          str(type(x)) + '`. '

~\anaconda3\envs\tf\lib\site-packages\keras\backend\tensorflow_backend.py in is_tensor(x)
    701 
    702 def is_tensor(x):
--> 703     return isinstance(x, tf_ops._TENSOR_LIKE_TYPES) or tf_ops.is_dense_tensor_like(x)
    704 
    705 

AttributeError: module 'tensorflow.python.framework.ops' has no attribute '_TENSOR_LIKE_TYPES'"
42738,Successful NUMA node read from SysFS had negative value (-1),"Hello everyone,
I am running a tensorflow code on GPU and getting the following warnings and errors. 

```
coreClock: 1.176GHz coreCount: 5 deviceMemorySize: 3.95GiB deviceMemoryBandwidth: 74.65GiB/s
2020-08-28 16:02:27.033368: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-28 16:02:27.033380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 16:02:27.033390: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-28 16:02:27.033400: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-28 16:02:27.033411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-28 16:02:27.033421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-28 16:02:27.033431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 16:02:27.033482: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-28 16:02:27.033732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-28 16:02:27.033932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-28 16:02:27.033951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-28 16:02:27.033957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-08-28 16:02:27.033961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 

INFO:tensorflow:Running local_init_op.
I0828 16:08:52.357333 139711269271360 session_manager.py:505] Running local_init_op.
INFO:tensorflow:Done running local_init_op.
I0828 16:08:52.375836 139711269271360 session_manager.py:508] Done running local_init_op.
2020-08-28 16:08:52.660709: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-28 16:08:53.080643: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-28 16:08:53.190000: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2020-08-28 16:08:53.218688: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR


tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.
  (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node resnet18/conv2d/Conv2D}}]]
  (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node resnet18/conv2d/Conv2D}}]]
	 [[add_5/_453]]
0 successful operations.
0 derived errors ignored.
```


1.   successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2.  Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
3. Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.



Can anyone please help me in solving these issues? Thanks

-mz


"
42737,TPU - Probable memory leak in TF 2.3 on data augmentation using tf.keras.layers.experimental.preprocessing.*,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian Buster
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Install from pip
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: Cloud TPU v2-8

**Describe the current behavior**

The recent Random augmentations added in TF 2.3.0 generate a memory leak on TPU leading to a crash and GRPC Socket 14 error message.
TPU memory usage grows up to 315GB, then a crash occurs.
The problem does not appear when augmentations in the code below are disabled, or tf.image augmentations are applied.

**Standalone code to reproduce the issue**

Create a standard data loading pipeline, and insert a map call to a Sequential model containing tf.keras.layers.experimental.preprocessing transformations.

```
augmentations = tf.keras.Sequential([
    tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal',dtype=tf.float32),
    tf.keras.layers.experimental.preprocessing.RandomContrast(0.1,dtype=tf.float32),
    tf.keras.layers.experimental.preprocessing.RandomRotation(0.25/4,dtype=tf.float32), #0.25=pi/2
    tf.keras.layers.experimental.preprocessing.RandomTranslation(width_factor=0.1,height_factor=0.1,dtype=tf.float32),
    tf.keras.layers.experimental.preprocessing.RandomZoom(0.1,dtype=tf.float32)
])

def data_augment(image, label):
      image = augmentations(image, training=True)
    return image, label   

def get_training_dataset():
    dataset = load_dataset(TRAINING_FILENAMES)
    dataset = dataset.shuffle(8192)
    dataset = dataset.repeat()
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.map(data_augment, num_parallel_calls=AUTO) # Memory Leak here
    dataset = dataset.prefetch(AUTO)
    return dataset

```
The addition of dtype=tf.float32 in the augmentation code does not change anything.
The use of a single augmentation (instead of 5 in the example above) does not change the problem.

Also, if I replace the use of the augmentation layer by a simple lambda function such as the following, the problem disappears:

```
def data_augment_ok(image, label):
    max_angle = 30*math.pi/180
    rotation = tf.random.uniform(shape=[], minval=-max_angle, maxval=max_angle, dtype=tf.float32)
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_contrast(image, 0.7, 1)
    image = tfa.image.rotate(image,rotation)
    return image, label   

def get_training_dataset():
    dataset = load_dataset(TRAINING_FILENAMES)
    dataset = dataset.map(data_augment_ok, num_parallel_calls=AUTO) # Notice the call earlier in the pipeline
  
    dataset = dataset.shuffle(8192)
    dataset = dataset.repeat()
    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)
    return dataset
```

Thanks!

"
42736,Tensorflow flooding my Spyder Console,"Hello,

I've been searching for a way to suppress tensorflow information like this:
...
`
2020-08-28 14:53:56.959449: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_101.dll
`
...

This is very annoying to me. 
I tried various recommendations:
`
tf.autograph.set_verbosity(x) # x in [0,1,2,3,4,5 ...]
`

`
os.environ[""AUTOGRAPH_VERBOSITY""]=""y"" # y in [0,1,2,3]
`

`
import logging
logging.getLogger(""tensorflow"").setLevel(logging.ERROR)
logging.getLogger(""tensorflow"").addHandler(logging.NullHandler(logging.INFO)) 
`
and basically every permutation of those.
These annoying information seem to appear randomly.

My setting is:
- Spyder with python 3.7
- tensorflow-gpu==2.1.0

Is there anything I am doing wrong?
Sorry in advance, if this is not a proper issue. 
But I'm kinda tilted.

Greetings,
Ilkay"
42735,Android - Drawback of Fixing Error (Regular TensorFlow ops) is increasing the app size to 195MB! ,"### Object detection Android TF LITE EXAMPLE
I am running the example of [object detection Android version](https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android), which need about 14MB storage on the physical phone.


### My trained Object detection Android TF LITE
However, when I used my object detection trained model, (which is [converted to TF LITE using this link](https://github.com/tensorflow/tensorflow/issues/42114#issuecomment-671593386)), I got the following exception when the app started: 


```
org.tensorflow.lite.examples.detection E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 5086
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: Regular TensorFlow ops are not supported by this interpreter. Make sure you apply/link the Flex delegate before inference.
    Node number 223 (FlexSize) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:158)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:347)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:196)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)
        at android.os.Handler.handleCallback(Handler.java:938)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:223)
        at android.os.HandlerThread.run(HandlerThread.java:67)
```

* My tf-Lite model size: 5MB
* The app size on the mobile is about 15 MB


### To Solve This Error:
- I include the following dependency on the **build.gradle (Modle app)**:

```
    // This dependency adds the necessary TF op support.
    implementation 'org.tensorflow:tensorflow-lite-select-tf-ops:0.0.0-nightly'
```

<img width=""1019"" alt=""Screen Shot 2020-08-28 at 3 23 41 PM"" src=""https://user-images.githubusercontent.com/68266028/91560386-764b9680-e942-11ea-827e-28e2680d866c.png"">


That solves the problem and my object detection app is working very well. 

### However, The app is now 195MB size (storage) on the physical mobile! Does this (-tf-ops) dependency has this large size! If so how can I fix the previous exception without including this dependency in the android app?


<img width=""305"" alt=""Screen Shot 2020-08-28 at 3 21 26 PM"" src=""https://user-images.githubusercontent.com/68266028/91560234-266ccf80-e942-11ea-8b08-9ecb6aa2bb5d.png"">




"
42734,TPU - Memory leak in TF 2.3 on data augmentation using tf.keras.layers.experimental.preprocessing/,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42733,Problem with creating a tf.data.Dataset iterator from numpy array,"I am trying to create a tensorflow data iterator from a numpy array, placed on a certain device. I expected it to be of type tf.data.Dataset..., but what I got were something else. I wonder how to create an iterator based on this target_data. Or is there a better way to convert numpy to tf.data? 
```
target_data = tf.data.Dataset.from_tensor_slices(target_batched)
# <class 'tensorflow.python.data.ops.dataset_ops.TensorSliceDataset'>
target_data= target_data.apply(tf.data.experimental.prefetch_to_device(""/cpu:0"", 12))
# <class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>
```
(here, target_data.make_initializable_iterator() doesn't work) 
"
42731,Build issue on libtensorflowlite.a for cross compile,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
I used docker image downloaded few days ago from docker hub -->  tensorflow/tensorflow:devel 
- TensorFlow installed from (source or binary): 
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
cross compiler version : (result of $ arm-cortex_a9-linux-gnueabi-gcc --version)
arm-cortex_a9-linux-gnueabi-gcc (crosstool-NG linaro-1.13.1-4.8-2013.11 - nexell) 4.7.4 20131111 (prerelease)
Copyright (C) 2012 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

- CUDA/cuDNN version:
- GPU model and memory:


**Describe the problem**

I want to build a libtensorflowlite.a for my board, but it is not compatible with existing general cross compilers.
I followed the guide from the site (https://www.tensorflow.org/lite/guide/build_arm64?hl=ko), and I got libraries for 32-bit and 64-bit, but those has built with the toolchain described in the webpage (https://github.com/tensorflow/tensorflow/tree/master/third_party/toolchains/embedded/arm-linux).

I use arm-cortex_a9-linux-gnueabi- compilers for my board(nxp4330), and there are errors when I try to build the library using script file.

I'm not sure what is the problem, and I attached logs from the execution. Thanks in advance.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. I copied my toolchain into docker
2. I copied build_aarch64_lib.sh as build_armv7l_lib.sh (temporary file) and I modified toolchain path. (but it didn't work well)
3. I execute the script and I specified the tools with commands as follow:
root@c41d54d7a748:/tensorflow_src# ./tensorflow/lite/tools/make/build_armv7l_lib.sh CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ AR=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ar LD=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ld


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

root@c41d54d7a748:/tensorflow_src# ./tensorflow/lite/tools/make/build_armv7l_lib.sh CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ AR=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ar LD=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ld
+ set -e
+++ dirname ./tensorflow/lite/tools/make/build_armv7l_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/tensorflow_src/tensorflow/lite/tools/make
+ TENSORFLOW_DIR=/tensorflow_src/tensorflow/lite/tools/make/../../../..
++ free -m
++ awk '/^Mem/ {print $2}'
+ FREE_MEM=128532
+ [[ FREE_MEM -gt 2000 ]]
+ NO_JOB=4
+ TOOL_CHAIN_PREFIX=/root/4.7.4/arm=cortex_a9-linux-gnueabi/
+ CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc
+ CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++
+ make -j 4 TARGET=r9 ARCH=armv7-a -C /tensorflow_src/tensorflow/lite/tools/make/../../../.. -f tensorflow/lite/tools/make/Makefile CC=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc CXX=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ AR=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ar LD=/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-ld
make: Entering directory '/tensorflow_src'
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/allocation.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/arena_planner.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/c/c_api.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/c/c_api.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_experimental.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/c/c_api_experimental.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-gcc -O3 -DNDEBUG -fPIC -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/c/common.c -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/c/common.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/error_reporter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/error_reporter.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/flatbuffer_conversions.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/op_resolver.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/op_resolver.o
tensorflow/lite/c/c_api.cc:40:7: note: the mangling of ‘va_list’ has changed in GCC 4.4
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/api/tensor_utils.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/api/tensor_utils.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/core/subgraph.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/core/subgraph.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/resource_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/experimental/resource/resource_variable.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/experimental/resource/static_hashtable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/experimental/resource/static_hashtable.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/external_cpu_backend_context.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/external_cpu_backend_context.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/graph_info.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/graph_info.o
tensorflow/lite/core/subgraph.cc:1058:6: note: the mangling of ‘va_list’ has changed in GCC 4.4
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/interpreter.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/interpreter.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/interpreter_builder.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/interpreter_builder.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/activations.o
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,
from ./tensorflow/lite/kernels/cpu_backend_context.h:21,
from tensorflow/lite/kernels/activations.cc:25:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function ‘void gemmlowp::BlockingCounter::Wait()’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: ‘sleep_for’ is not a member of ‘std::this_thread’
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/add.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add.o
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from tensorflow/lite/kernels/activations.cc:29:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only ‘= 0’ is allowed) before ‘;’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: ‘kMaxMulParamsAlignment’ is not a type
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of ‘alignas’ with no type [-fpermissive]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected ‘;’ at end of member declaration
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from tensorflow/lite/kernels/activations.cc:29:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function ‘void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: ‘is_trivially_copyable’ is not a member of ‘std’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before ‘>’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: ‘::value’ has not been declared
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,
from ./tensorflow/lite/kernels/cpu_backend_context.h:21,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:44,
from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,
from tensorflow/lite/kernels/add.cc:15:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function ‘void gemmlowp::BlockingCounter::Wait()’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: ‘sleep_for’ is not a member of ‘std::this_thread’
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,
from tensorflow/lite/kernels/add.cc:15:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only ‘= 0’ is allowed) before ‘;’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: ‘kMaxMulParamsAlignment’ is not a type
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of ‘alignas’ with no type [-fpermissive]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected ‘;’ at end of member declaration
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from ./tensorflow/lite/kernels/internal/optimized/integer_ops/add.h:26,
from tensorflow/lite/kernels/add.cc:15:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function ‘void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: ‘is_trivially_copyable’ is not a member of ‘std’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before ‘>’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: ‘::value’ has not been declared
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/add_n.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add_n.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/arg_min_max.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/arg_min_max.o
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/assign_variable.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/assign_variable.o
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,
from ./tensorflow/lite/kernels/cpu_backend_context.h:21,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:44,
from tensorflow/lite/kernels/arg_min_max.cc:23:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function ‘void gemmlowp::BlockingCounter::Wait()’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: ‘sleep_for’ is not a member of ‘std::this_thread’
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from tensorflow/lite/kernels/arg_min_max.cc:23:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only ‘= 0’ is allowed) before ‘;’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: ‘kMaxMulParamsAlignment’ is not a type
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of ‘alignas’ with no type [-fpermissive]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected ‘;’ at end of member declaration
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from tensorflow/lite/kernels/arg_min_max.cc:23:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function ‘void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: ‘is_trivially_copyable’ is not a member of ‘std’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before ‘>’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: ‘::value’ has not been declared
/root/4.7.4/bin/arm-cortex_a9-linux-gnueabi-g++ -O3 -DNDEBUG -fPIC --std=c++11 -DTFLITE_WITHOUT_XNNPACK -I. -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/../../../../../../ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ -I/tensorflow_src/tensorflow/lite/tools/make/downloads/eigen -I/tensorflow_src/tensorflow/lite/tools/make/downloads/absl -I/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp -I/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy -I/tensorflow_src/tensorflow/lite/tools/make/downloads/neon_2_sse -I/tensorflow_src/tensorflow/lite/tools/make/downloads/farmhash/src -I/tensorflow_src/tensorflow/lite/tools/make/downloads/flatbuffers/include -I/tensorflow_src/tensorflow/lite/tools/make/downloads/fp16/include -I -I/usr/local/include -c tensorflow/lite/kernels/audio_spectrogram.cc -o /tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/audio_spectrogram.o
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
tensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/activations.o' failed
make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/activations.o] Error 1
make: *** Waiting for unfinished jobs....
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/dispatch_gemm_shape.h:23:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/gemmlowp.h:19,
from ./tensorflow/lite/kernels/cpu_backend_context.h:21,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:44,
from tensorflow/lite/kernels/audio_spectrogram.cc:24:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h: In member function ‘void gemmlowp::BlockingCounter::Wait()’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/gemmlowp/public/../internal/multi_thread_gemm.h:203:9: error: ‘sleep_for’ is not a member of ‘std::this_thread’
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:33:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from tensorflow/lite/kernels/audio_spectrogram.cc:24:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h: At global scope:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:76:36: error: invalid pure specifier (only ‘= 0’ is allowed) before ‘;’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:11: error: ‘kMaxMulParamsAlignment’ is not a type
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: ISO C++ forbids declaration of ‘alignas’ with no type [-fpermissive]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/trmul_params.h:87:33: error: expected ‘;’ at end of member declaration
In file included from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:30:0,
from /tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:23,
from ./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:21,
from ./tensorflow/lite/kernels/cpu_backend_gemm.h:25,
from ./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:45,
from tensorflow/lite/kernels/audio_spectrogram.cc:24:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In function ‘void ruy::detail::FinalizeMulParams(const ruy::MulParams<AccumScalar, DstScalar>&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:17: error: ‘is_trivially_copyable’ is not a member of ‘std’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:57: error: expected primary-expression before ‘>’ token
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:388:58: error: ‘::value’ has not been declared
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
tensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add.o' failed
make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/add.o] Error 1
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
tensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/arg_min_max.o' failed
make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/arg_min_max.o] Error 1
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = float; RhsScalar = float; AccumScalar = float; DstScalar = float; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)0]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:402:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:465:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:118:5: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = short int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:526:45: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = signed char; RhsScalar = signed char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:1544:79: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h: In instantiation of ‘void ruy::detail::PopulateTrMulParams(ruy::TrMulParams*) [with ruy::Path ThePath = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’:
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:197:7: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, InCompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; bool InCompiledPaths = true; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 0; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: required from ‘static void ruy::detail::PathSearchCountdown<CompiledPaths, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 1; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:212:5: required from ‘static void ruy::detail::PathSearchOnlyCompiledPaths<CompiledPaths, false, BitNumber, LhsScalar, RhsScalar, AccumScalar, DstScalar>::Search(ruy::Path, ruy::TrMulParams*) [with ruy::Path CompiledPaths = (ruy::Path)1u; int BitNumber = 2; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:222:5: [ skipping 15 instantiation contexts ]
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/frontend.h:89:3: required from ‘void ruy::MulFrontEnd(const ruy::Mat<LhsScalar>&, const ruy::Mat<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Ctx*, ruy::Mat<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:43:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<RhsScalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with ruy::Path CompiledPaths = (ruy::Path)1u; LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/ruy.h:98:3: required from ‘void ruy::Mul(const ruy::Matrix<Scalar>&, const ruy::Matrix<Scalar>&, const ruy::MulParams<AccumScalar, DstScalar>&, ruy::Context*, ruy::Matrix<DstScalar>*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int]’
./tensorflow/lite/kernels/cpu_backend_gemm_ruy.h:141:5: required from ‘static void tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<LhsScalar, RhsScalar, AccumScalar, DstScalar, quantization_flavor>::Run(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<DstScalar>&, DstScalar*, const tflite::cpu_backend_gemm::GemmParams<AccumScalar, DstScalar, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; AccumScalar = int; DstScalar = int; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1]’
./tensorflow/lite/kernels/cpu_backend_gemm.h:157:3: required from ‘void tflite::cpu_backend_gemm::Gemm(const tflite::cpu_backend_gemm::MatrixParams<LhsScalar>&, const LhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<RhsScalar>&, const RhsScalar*, const tflite::cpu_backend_gemm::MatrixParams<int>&, int32_t*, const tflite::cpu_backend_gemm::GemmParams<int, int, quantization_flavor>&, tflite::CpuBackendContext*) [with LhsScalar = unsigned char; RhsScalar = unsigned char; tflite::cpu_backend_gemm::QuantizationFlavor quantization_flavor = (tflite::cpu_backend_gemm::QuantizationFlavor)1; int32_t = int]’
./tensorflow/lite/kernels/internal/optimized/optimized_ops.h:6092:73: required from here
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:126:53: error: no type named ‘LhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
/tensorflow_src/tensorflow/lite/tools/make/downloads/ruy/ruy/create_trmul_params.h:127:53: error: no type named ‘RhsLayout’ in ‘using Kernel = struct ruy::Kernel<ThePath, typename ruy::PackedTypeImpl<ThePath, Scalar>::Type, typename ruy::PackedTypeImpl<ThePath, RhsScalar>::Type, AccumScalar, DstScalar>’
tensorflow/lite/tools/make/Makefile:311: recipe for target '/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/audio_spectrogram.o' failed
make: *** [/tensorflow_src/tensorflow/lite/tools/make/gen/r9_x86_64/obj/tensorflow/lite/kernels/audio_spectrogram.o] Error 1
make: Leaving directory '/tensorflow_src'"
42730,Error building tensorflow.dll (Windows 10),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 2.3.0
- Bazel version (if compiling from source): 3.1.0

**Describe the problem**
Error building tensorflow.dll
```
ERROR: C:/users/lotte/downloads/tensorflow-2.3.0/tensorflow/BUILD:663:1: Linking of rule '//tensorflow:tensorflow_framework.dll' failed (Exit 1127): link.exe failed: error executing command
  cd C:/users/lotte/_bazel_lotte/6a53jthk/execroot/org_tensorflow
  SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.27.29110\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/lotte/AppData/Local/Programs/Python/Python38/python.exe
    SET PYTHON_LIB_PATH=C:/Users/lotte/AppData/Local/Programs/Python/Python38/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\lotte\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_ENABLE_XLA=1
    SET TMP=C:\Users\lotte\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.27.29110/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_framework.dll-2.params
Execution platform: @local_execution_config_platform//:platform
external\mkl_windows\lib\libiomp5md.lib : fatal error LNK1127: library is corrupt
Target //tensorflow/tools/lib_package:libtensorflow failed to build
INFO: Elapsed time: 9542.848s, Critical Path: 619.14s
INFO: 8184 processes: 8184 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
python configure.py
    empty
    empty
    n
    n
    empty
    y
    n

bazel build -c opt --config=mkl //tensorflow/tools/lib_package:libtensorflow
```

**Edit: Possibly related to #42703 (Error building tensorflow_cc.dll)**"
42729,Autograph unable to process lambda statements @TF 2.3.0,"- `TF 2.0: python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` : `v2.3.0-rc2-23-gb36436b087 2.3.0`

## Problem description  
```python
import tensorflow as tf
assert tf.__version__ == '2.3.0'
dataset = tf.data.Dataset.from_tensor_slices(
   [1,2,3.]
).map(lambda x:x)
```
Throws unexpected warning:
```
WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f4ac022e710> and will run it as-is.
Cause: could not parse the source code:

).map(lambda x:x)

This error may be avoided by creating the lambda in a standalone statement.

To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
WARNING: AutoGraph could not transform <function <lambda> at 0x7f4ac022e710> and will run it as-is.
Cause: could not parse the source code:

).map(lambda x:x)

This error may be avoided by creating the lambda in a standalone statement.

To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
```

## Expected behaviour  
The autograph converting the lambda statement as usual.

## Additional info  
This bug does not seem to occur in earlier version '2.2.0'. 
as per:
```python
import tensorflow as tf
assert tf.__version__ == '2.2.0'
dt = tf.data.Dataset.from_tensor_slices(
    tf.ones((10,))
).map(lambda x:x)
```
running without any erroneous output.
* all installations were made using pip package manager

"
42728,OP_REQUIRES failed at conv_ops.cc:1115,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no, used code from CNN tutorial
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux, kernel 5.8.3-arch1-1
- TensorFlow installed from (source or binary): python-tensorflow-opt-cuda package
- TensorFlow version (use command below): unknown 2.3.0
- Python version: Python 3.8.5
- CUDA/cuDNN version: CUDA 11.0.2-1,  cuDNN 8.0.2.39-2
- GPU model and memory: Nvidia GeForce RTX 2070 8gb

**Describe the current behavior**

Any model fitting using Conv2D layers fails with `OP_REQUIRES failed at conv_ops.cc:1115 : Not found: No algorithm worked!` errors. Full trace:
```
2020-08-28 12:42:25.012311: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at conv_ops.cc:1115 : Not found: No algorithm worked!
Traceback (most recent call last):
  File ""test.py"", line 26, in <module>
    history = model.fit(train_images, train_labels, epochs=10, batch_size=64,
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 108, in _method_wrapper
    return method(self, *args, **kwargs)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1098, in fit
    tmp_logs = train_function(iterator)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 780, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 840, in _call
    return self._stateless_fn(*args, **kwds)
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 2829, in __call__
    return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1843, in _filtered_call
    return self._call_flat(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 1923, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 545, in call
    outputs = execute.execute(
  File ""/usr/lib/python3.8/site-packages/tensorflow/python/eager/execute.py"", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!
	 [[node sequential/conv2d/Conv2D (defined at test.py:26) ]] [Op:__inference_train_function_853]

Function call stack:
train_function
```

**Describe the expected behavior**

Model fitting works correctly.

**Standalone code to reproduce the issue**

I used [Convolutional Neural Network (CNN) Tutorial](https://www.tensorflow.org/tutorials/images/cnn), full code below:

```python
import tensorflow as tf

from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()

# Normalize pixel values to be between 0 and 1
train_images, test_images = train_images / 255.0, test_images / 255.0

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))

model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10))

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

history = model.fit(train_images, train_labels, epochs=10,
                    validation_data=(test_images, test_labels))
```

I'm attaching full log file (with `TF_CPP_MIN_LOG_LEVEL=0`): [log.txt](https://github.com/tensorflow/tensorflow/files/5141360/log.txt)

How can I help debugging this problem?"
42726,tensorflow serialize models,"I use multi model ,example bert1,bert2,bert3 for predict,
How to serialize to achieve modify model 
use below code ,it's not work
```
    if init_checkpoint:
        (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars,init_checkpoint)
        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)
```

```
2020-08-28 16:59:17.748977: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Data loss: Checksum does not match: stored 347393357 vs. calculated on the restored bytes 1825803478
2020-08-28 16:59:17.750680: W tensorflow/core/framework/op_kernel.cc:1275] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Data loss: Checksum does not match: stored 4097036962 vs. calculated on the restored bytes 3596090645
20
```
DataLossError: Checksum does not match: stored 1057886036 vs. calculated on the restored bytes 1146985826
"
42725,DLL load failed,"I'm relatively new to programming so please forgive me if some details mentioned before are not detailed enough. 
I downloaded tensorflow today and after looking at the documentation, downloaded/updated CUDA and cuDNN as well. But I'm getting this error:
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-d6579f534729> in <module>()
----> 1 import tensorflow

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\__init__.py in <module>()
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\__init__.py in <module>()
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     39 
---> 40 from tensorflow.python.eager import context
     41 
     42 # pylint: enable=wildcard-import

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\eager\context.py in <module>()
     33 from tensorflow.core.protobuf import config_pb2
     34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
     36 from tensorflow.python import tf2
     37 from tensorflow.python.client import pywrap_tf_session

~\AppData\Roaming\Python\Python35\site-packages\tensorflow\python\pywrap_tfe.py in <module>()
     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
     28 from tensorflow.python import pywrap_tensorflow
---> 29 from tensorflow.python._pywrap_tfe import *

ImportError: DLL load failed: The specified module could not be found.


I tried the solutions mentioned in older posts having the same error, but none worked.
I'm using:
Windows 10 64-bit
Mobile Device: N/A
TensorFlow downloaded through Anaconda prompt -pip install tensorflow
Version: 2.3.0
Python version 3.5.5
Bazel version (if compiling from source) - N/A
GCC/Compiler version (if compiling from source) - N/A
CUDA/cuDNN version - 10.2
GPU model and memory - NVIDIA GeForce 940MX; 4GB dedicated memory
Exact command to reproduce - import tensorflow
Tensorflow version - 2.3.0"
42724,ImportError: DLL load failed: The specified module could not be found.,"OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
TensorFlow installed from (source or binary):
TensorFlow version: 2.3.0
Python version: 3.8.3
Installed using virtualenv? pip? conda?: conda
Bazel version (if compiling from source): N/A
GCC/Compiler version (if compiling from source): N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Provide the exact sequence of commands / steps that you executed before running into the problem : Just ran a file with import tensorflow on python

C:\Users\User\.conda\envs\tensor\pythonw.exe F:/Tech_With_Tim/tensorEnv/test.py
Traceback (most recent call last):
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:/Tech_With_Tim/tensorEnv/test.py"", line 1, in <module>
    import tensorflow
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\User\.conda\envs\tensor\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

Process finished with exit code 1"
42723,"TensorFlow Lite Android nightly method ""runForMultipleInputsOutputs"" that loaded model using stft  not work with converted by tf.lite.OpsSet.SELECT_TF_OPS ","TensorFlow Lite Android nightly method ""runForMultipleInputsOutputs"" that loaded model using stft  not work with converted by tf.lite.OpsSet.SELECT_TF_OPS .
What is wrong ?

written code.
> import numpy as np
> import tensorflow as tf
> from tensorflow.keras.layers import Input, Dense, Dropout
> from tensorflow.keras.layers import Conv2D, MaxPool2D,Flatten
> from tensorflow.keras.layers import BatchNormalization,Activation,Add,GlobalAveragePooling2D,Reshape
> from tensorflow.keras.models import Model
> 
> _FFT_SIZE=512
> _HOP_SIZE=256
> _N_MEL_BINS=128
> 
> class LogMelSpectrogram(tf.keras.layers.Layer):
>     """"""Compute log-magnitude mel-scaled spectrograms.""""""
> 
>     def __init__(self, sample_rate, fft_size, hop_size, n_mels,
>                  f_min=0.0, f_max=None, **kwargs):
>         super(LogMelSpectrogram, self).__init__(**kwargs)
>         self.sample_rate = sample_rate
>         self.fft_size = fft_size
>         self.hop_size = hop_size
>         self.n_mels = n_mels
>         self.f_min = f_min
>         self.f_max = f_max if f_max else sample_rate / 2
>         self.mel_filterbank = tf.signal.linear_to_mel_weight_matrix(
>             num_mel_bins=self.n_mels,
>             num_spectrogram_bins=fft_size // 2 + 1,
>             sample_rate=self.sample_rate,
>             lower_edge_hertz=self.f_min,
>             upper_edge_hertz=self.f_max)
> 
>     def build(self, input_shape):
>         self.non_trainable_weights.append(self.mel_filterbank)
>         super(LogMelSpectrogram, self).build(input_shape)
> 
>     def call(self, waveforms):
>         """"""Forward pass.
>         Parameters
>         ----------
>         waveforms : tf.Tensor, shape = (None, n_samples)
>             A Batch of mono waveforms.
>         Returns
>         -------
>         log_mel_spectrograms : (tf.Tensor), shape = (None, time, freq, ch)
>             The corresponding batch of log-mel-spectrograms
>         """"""
>         def _tf_log10(x):
>             numerator = tf.math.log(x)
>             denominator = tf.math.log(tf.constant(10, dtype=numerator.dtype))
>             return numerator / denominator
> 
>         def power_to_db(magnitude, amin=1e-16, top_db=80.0):
>             """"""
>             https://librosa.github.io/librosa/generated/librosa.core.power_to_db.html
>             """"""
>             ref_value = tf.reduce_max(magnitude)
>             log_spec = 10.0 * _tf_log10(tf.maximum(amin, magnitude))
>             log_spec -= 10.0 * _tf_log10(tf.maximum(amin, ref_value))
>             log_spec = tf.maximum(log_spec, tf.reduce_max(log_spec) - top_db)
> 
>             return log_spec
> 
>         spectrograms = tf.signal.stft(waveforms,
>                                       frame_length=self.fft_size,
>                                       frame_step=self.hop_size,
>                                       pad_end=False)
> 
>         magnitude_spectrograms = tf.abs(spectrograms)
> 
>         mel_spectrograms = tf.matmul(tf.square(magnitude_spectrograms),
>                                      self.mel_filterbank)
> 
>         log_mel_spectrograms = power_to_db(mel_spectrograms)
> 
>         # add channel dimension
>         log_mel_spectrograms = tf.expand_dims(log_mel_spectrograms, 3)
> 
>         return log_mel_spectrograms
> 
>     def get_config(self):
>         config = {
>             'fft_size': self.fft_size,
>             'hop_size': self.hop_size,
>             'n_mels': self.n_mels,
>             'sample_rate': self.sample_rate,
>             'f_min': self.f_min,
>             'f_max': self.f_max,
>         }
>         config.update(super(LogMelSpectrogram, self).get_config())
> 
>         return config
> 
> 
> 
> def GetModel(n_classes, sample_rate=16000, duration=4,
>               fft_size=_FFT_SIZE, hop_size=_HOP_SIZE, n_mels=_N_MEL_BINS):
>     n_samples = sample_rate * duration
>     input_data = Input(shape=(n_samples,), name='input', dtype='float32')
>     freq = Input(name='freq',shape=[1],dtype='int32')
>     x = input_data
>     test_output = LogMelSpectrogram(sample_rate, fft_size, hop_size, n_mels)(x)
>     x = Dense(n_classes,activation='softmax')(x)
> 
>     return Model(inputs=[input_data,freq], outputs=x)
> 

convert model to tflite with tf.lite.OpsSet.SELECT_TF_OPS
> N_classes = 50
> Duration = 5
> 
> model = GetModel(N_classes,duration=Duration)
> opt = tf.keras.optimizers.Adam(lr=0.001, decay=1e-6, amsgrad=True)
> model.compile(loss='categorical_crossentropy',
>           optimizer=opt,
>           metrics=['accuracy'])
> model.summary()
> 
> 
> converter = tf.lite.TFLiteConverter.from_keras_model(model)
> 
> #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]
> #converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
> converter.target_spec.supported_ops = [tf.lite.OpsSet.SELECT_TF_OPS]
> 
> tflite_model = converter.convert()
> with open(""model.tflite"", ""wb"") as f:
>     f.write(tflite_model)

on Centos , tflite invoke from python is work.
>   input_data = np.zeros(input_shape, dtype=np.float32)
>   pprint(""input_data"")
>   pprint(input_data)
>   input_data[0][0:] = x2
>   npa = np.array([[0]], dtype=np.int32) 
>   npa[0] = 16000
> 
>   print(f""input_details[0]:{input_details[0]}"")
>   print(f""input_details[1]:{input_details[1]}"")
> 
>   interpreter.set_tensor(input_details[0]['index'], input_data)
>   interpreter.set_tensor(input_details[1]['index'], npa)
> 
>   interpreter.invoke()
>   output_data = interpreter.get_tensor(output_details[0]['index'])
>   print(""output_data:"")
>   pprint(output_data)
> 

Results:
> array([[0.01259949, 0.01274813, 0.02341373, 0.02667766, 0.01372223,
>         0.02063654, 0.01939304, 0.01231323, 0.02360482, 0.01985866,
>         0.01616379, 0.02790864, 0.01614609, 0.03300844, 0.02038008,
>         0.02689514, 0.01205852, 0.02373471, 0.03396689, 0.0159423 ,
>         0.01418541, 0.01825977, 0.01506583, 0.01811898, 0.0167273 ,
>         0.01219947, 0.03590814, 0.01607339, 0.0213427 , 0.02323929,
>         0.01976353, 0.01719474, 0.02329947, 0.02231019, 0.02448872,
>         0.01822161, 0.01806694, 0.02725129, 0.0257786 , 0.02799631,
>         0.02095988, 0.01663861, 0.01729359, 0.0102047 , 0.01404527,
>         0.01819064, 0.0125629 , 0.01387509, 0.01704353, 0.032522  ]],
>       dtype=float32)
>

on Android , Kotlin
>tfLite!!.runForMultipleInputsOutputs(inputArray, outputs) 

example 1
>Internal error: Failed to run on the given Interpreter: Matrix size-incompatible: In[0]: [80000,1], In[1]: [80000,50]
>    	 (while executing 'MatMul' via Eager)

example 2
>Cannot copy from a TensorFlowLite tensor (Identity) with shape [80000, 50] to a Java object with shape [1, 50].

"
42722,non_max_suppression outputs each ROI's assignment to non-max supressed bbox,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.1.0
- Are you willing to contribute it (Yes/No): 



**Describe the feature and the current behavior/state.**
`tf.image.non_max_suppresion` returns outputs containing only indices of non-max suppressed bboxes, the requested feature is to also return the assignment indices of deleted ROIs to its' NMS bbox, the usage would be like:
```
selected_indices, assign_nms_indices = tf.image.non_max_suppression(boxes, scores, max_output_size)
assert tf.shape(assign_nms_indices)[0] == tf.shape(boxes)[0]
```
**Will this change the current api? How?**

**Who will benefit with this feature?**
Whos want to manipulate the scores within ROIs deleted by NMS and the NMSed bbox.

**Any Other info.**
Apologize for my broken English"
42721,fatal error: tensorflow/core/protobuf/error_codes.pb.h: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Drax 5.4.0-42-generic #46~18.04.1-Ubuntu SMP Fri Jul 10 07:21:24 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
- TensorFlow source: v2.3.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0




compile any program that includes tensorflow/core/public/session.h


gcc -I/home/amascaro/Programming/src/tensorflow -I/home/amascaro/.local/lib/python3.6/site-packages/tensorflow_core/include/ loader.cc 
In file included from /home/amascaro/Programming/src/tensorflow/tensorflow/core/platform/errors.h:22:0,
                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/lib/core/errors.h:19,
                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/framework/tensor_shape.h:23,
                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/framework/tensor.h:24,
                 from /home/amascaro/Programming/src/tensorflow/tensorflow/core/public/session.h:24,
                 from loader.cc:1:
/home/amascaro/Programming/src/tensorflow/tensorflow/core/platform/status.h:28:10: fatal error: tensorflow/core/protobuf/error_codes.pb.h: No such file or directory
 #include ""tensorflow/core/protobuf/error_codes.pb.h""
          ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
compilation terminated.

The missing file is under another path: /home/amascaro/.local/lib/python3.6/site-packages/tensorflow_core/include/tensorflow/core/lib/core/error_codes.pb.h"
42720,I have no idea how to initialize the tf.feature_column.embedding_column from txt or numpy? ,I have no idea how to initialize the tf.feature_column.embedding_column from txt or numpy? thanks
42719,How to incrementally training the embedding with tf.feature_column.embedding_column?,"I will train a embedding model incrementally through the Estimator and feature_column, but I have no idea how to expand the embedding column when some new id appears, or load the embedding from a numpy or file.
Any help would be appreciated."
42716,labels for each object prior,"I'm building ssd(object detection) with pytorch-> tensorflow<br/>

let say I have 

    object_for_each_prior
    <tf.Tensor: shape=(8,), dtype=int64, numpy=array([2, 2, 0, 1, 0, 0, 1, 1])>
    
    labels
    <tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 20,  10, 5], dtype=int32)>

and want to represent the 

    label_for_each_prior = labels[object_for_each_prior]  # (8732)

expected output should be 

    tensor([ 5,  5,  0, 10,  0,  0, 10, 10], device='cuda:0')

"
42713,issue with iterating over batched mapped tf.data.Dataset object,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): conda installation
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.8.5
- CUDA/cuDNN version: 7.6.5
- GPU model and memory: Nvidia RTX2070 8GB



**Describe the current behavior**
When processing pipeline for ```tf.data.Dataset``` which contains the usage of ```tf.numpy_function``` the ```UnknownError``` is thrown sometimes and the code runs the other times  without changing anything. This can be seen in the last cell of the colab notebook

**Describe the expected behavior**
the ```tf.numpy_function``` should work all the time in the same way

**Standalone code to reproduce the issue**
Link to the [colab](https://colab.research.google.com/drive/1iqsYWN6HaM09oA_to7MRcKw0wfzFsrkj?usp=sharing) notebook


**Other info / logs** Include any logs or source code that would be helpful to
```
/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py:789: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.
  UserWarning,
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2101       ctx.executor = executor_new
-> 2102       yield
   2103     finally:

10 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    757             output_types=self._flat_output_types,
--> 758             output_shapes=self._flat_output_shapes)
    759 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py in iterator_get_next(iterator, output_types, output_shapes, name)
   2609     except _core._NotOkStatusException as e:
-> 2610       _ops.raise_from_not_ok_status(e, name)
   2611     except _core._FallbackException:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6842   # pylint: disable=protected-access
-> 6843   six.raise_from(core._status_to_exception(e.code, message), None)
   6844   # pylint: enable=protected-access

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnknownError: error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'

Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 244, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py"", line 302, in wrapper
    return func(*args, **kwargs)

  File ""<ipython-input-24-d518d3df69c5>"", line 3, in aug_fn
    aug_data = transforms(**data)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/core/composition.py"", line 176, in __call__
    data = t(force_apply=force_apply, **data)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py"", line 87, in __call__
    return self.apply_with_params(params, **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py"", line 100, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/transforms.py"", line 1630, in apply
    return F.image_compression(image, quality, image_type)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py"", line 54, in wrapped_function
    result = func(img, *args, **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py"", line 796, in image_compression
    _, encoded_img = cv2.imencode(image_type, img, (int(quality_flag), quality))

cv2.error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'



	 [[{{node PyFunc}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

UnknownError                              Traceback (most recent call last)
<ipython-input-46-ef3ac52e7aa5> in <module>()
      3                   num_parallel_calls=AUTOTUNE,deterministic=False).prefetch(AUTOTUNE)
      4 it = iter(ds_alb)
----> 5 batch = next(it)
      6 print(type(batch))
      7 images,labels = batch

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in __next__(self)
    734 
    735   def __next__(self):  # For Python 3 compatibility
--> 736     return self.next()
    737 
    738   def _next_internal(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in next(self)
    770   def next(self):
    771     try:
--> 772       return self._next_internal()
    773     except errors.OutOfRangeError:
    774       raise StopIteration

/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py in _next_internal(self)
    762         return self._element_spec._from_compatible_tensor_list(ret)  # pylint: disable=protected-access
    763       except AttributeError:
--> 764         return structure.from_compatible_tensor_list(self._element_spec, ret)
    765 
    766   @property

/usr/lib/python3.6/contextlib.py in __exit__(self, type, value, traceback)
     97                 value = type()
     98             try:
---> 99                 self.gen.throw(type, value, traceback)
    100             except StopIteration as exc:
    101                 # Suppress StopIteration *unless* it's the same exception that

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2103     finally:
   2104       ctx.executor = executor_old
-> 2105       executor_new.wait()
   2106 
   2107 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

UnknownError: error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'

Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 244, in __call__
    ret = func(*args)

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/autograph/impl/api.py"", line 302, in wrapper
    return func(*args, **kwargs)

  File ""<ipython-input-24-d518d3df69c5>"", line 3, in aug_fn
    aug_data = transforms(**data)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/core/composition.py"", line 176, in __call__
    data = t(force_apply=force_apply, **data)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py"", line 87, in __call__
    return self.apply_with_params(params, **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/core/transforms_interface.py"", line 100, in apply_with_params
    res[key] = target_function(arg, **dict(params, **target_dependencies))

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/transforms.py"", line 1630, in apply
    return F.image_compression(image, quality, image_type)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py"", line 54, in wrapped_function
    result = func(img, *args, **kwargs)

  File ""/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py"", line 796, in image_compression
    _, encoded_img = cv2.imencode(image_type, img, (int(quality_flag), quality))

cv2.error: OpenCV(4.1.2) /io/opencv/modules/imgcodecs/src/grfmt_base.cpp:145: error: (-10:Unknown error code -10) Raw image encoder error: Maximum supported image dimension is 65500 pixels in function 'throwOnEror'



	 [[{{node PyFunc}}]]
```"
42711,TPU Pods: Batches not scaling according to the TPU pod size,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

I am using a custom training loop to train a model on TPUs. To iterate over the dataset I'm enumerating over it. I'm not using any steps here.

for One TPU device: TPU v3-8 the total batches found to be 800 per epoch

so for a TPU pod: TPU v3-32 the total batches should be **200** per epoch but still, the epoch completes after 800 batches I don't quite understand why.

attached a snippet of the code 


**Standalone code to reproduce the issue**
```
#Connecting to TPU
tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='node-2')
print('Running on TPU ', tpu.master())

tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.TPUStrategy(tpu)



---------------------------------------------------
BATCH_SIZE = 128 * strategy.num_replicas_in_sync
per_replica_batch_size = BATCH_SIZE // strategy.num_replicas_in_sync

train_dataset = strategy.experimental_distribute_datasets_from_function(
	lambda _: get_training_dataset(per_replica_batch_size))

#the loss_plot array will be reset many times
loss_plot = []

@tf.function
def train_step(iterator):
	def step_fn(inputs):

		images, labels = inputs
		with tf.GradientTape() as tape:
			logits = model(images, training=True)
			loss = tf.keras.losses.sparse_categorical_crossentropy(
			labels, logits, from_logits=True)
			loss = tf.nn.compute_average_loss(loss, global_batch_size=batch_size)
		grads = tape.gradient(loss, model.trainable_variables)
		optimizer.apply_gradients(list(zip(grads, model.trainable_variables)))
		training_loss.update_state(loss * strategy.num_replicas_in_sync)
		training_accuracy.update_state(labels, logits)
		return training_loss, training_accuracy

	per_replica_losses, l_loss = strategy.run(step_fn, args=(iterator,))
	return strategy.reduce(tf.distribute.ReduceOp.MEAN, per_replica_losses,axis=None),strategy.reduce(tf.distribute.ReduceOp.MEAN, l_loss,axis=None)

for epoch in range(start_epoch, EPOCHS):
	start = time.time()
	total_loss = 0

	for (batch, (img_tensor, target)) in enumerate(train_dataset):
		print(target)
		training_loss, training_accuracy = train_step((img_tensor, target))
		total_loss += t_loss

		if batch % 50 == 0:
			print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy() / BATCH_SIZE), flush=True)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42709,Use of GPU delegate increases inference runtime in Android's Native environment (C++) but not using Java API,"**System information**

- Have I written custom code: I did
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: Samsung Galaxy S10
- TensorFlow installed from: binary
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.6.9

**Describe the current behavior**

I'm trying to run a simple UNet model that runs on Android's Native environment and uses the GPU delegate.
I'm testing the same model on `Float32` / `Uint8` fully-quantized (same model, separate `.tflite` files) and also with / without GPU delegate on both cases (so 4 different types of inference in total).

- When testing with / without the GPU delegate - inference runtimes are the same.
- `ModifyGraphWithDelegate()` returns `kTfLiteOk`, but GPU delegate is not applied, and returns an error:
`Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`
- Model (see below) shouldn't have any dynamic sized tensors.
- When checked the model input Dims `(B, H, W, C)`, I got an unexpected result.

Tensorflow model was built with `(1, 128, 128, 12)`, but with Android and Tensorflow-Lite I get `(4, 1, 128, 128)`.

**Describe the expected behavior**

Tensorflow model built with `(1, 128, 128, 12)`, should result in Tensorflow-Lite's `(1, 128, 128, 12)` model, and GPU delegate should be applied and reduce inference time on `Float32` type inference.

**Standalone code to reproduce the issue**

Sample C++ code which runs in Android Studio's NDK (bare code, excluding status and sanity checks):
```
std::unique_ptr<tflite::FlatBufferModel> model;
std::unique_ptr<tflite::Interpreter> interpreter;
model = tflite::FlatBufferModel::BuildFromFile(MODEL_FILENAME_HERE.c_str());
model -> error_reporter();
tflite::ops::builtin::BuiltinOpResolver resolver;
tflite::InterpreterBuilder(*model, resolver)(&interpreter);
TfLiteGpuDelegateOptionsV2 gpu_opts = {
        .is_precision_loss_allowed = -1,
        .inference_preference =
                TFLITE_GPU_INFERENCE_PREFERENCE_FAST_SINGLE_ANSWER,
        .inference_priority1 = TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION,
        .inference_priority2 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,
        .inference_priority3 = TFLITE_GPU_INFERENCE_PRIORITY_AUTO,
        .experimental_flags = TFLITE_GPU_EXPERIMENTAL_FLAGS_NONE,
        .max_delegated_partitions = 1,
};
gpu_opts.inference_preference =
        TFLITE_GPU_INFERENCE_PREFERENCE_SUSTAINED_SPEED;
gpu_opts.inference_priority1 =
        conf.allow_fp16 ? TFLITE_GPU_INFERENCE_PRIORITY_MIN_LATENCY
                : TFLITE_GPU_INFERENCE_PRIORITY_MAX_PRECISION;
auto * gpu_delegte = TfLiteGpuDelegateV2Create(&gpu_opts);
if (USE_GPU_DELEGATE_FLAG) {
        interpreter->ModifyGraphWithDelegate(gpu_delegte);
}
interpreter->AllocateTensors();
int input_tensor_index = interpreter -> inputs()[0];
TfLiteTensor * input_tensor = interpreter -> tensor(input_tensor_index);

// checkpoint - after here there's more code to set input, invoke interpreter, get inference output, etc..

...
```

At `checkpoint`, get input dims by logging them or any other form for example:
```
int d1 = input_tensor -> dims[0];
int d2 = input_tensor -> dims[1];
int d3 = input_tensor -> dims[2];
int d4 = input_tensor -> dims[3];
```

**Other info / logs** Include any logs or source code that would be helpful to

Simple UNet `.tflite` model files [can be found here](https://gofile.io/d/PVJiBY).

When logging with:
```
""Input dims are %dx%dx%dx%d"",
                input_tensor -> dims[0],
                input_tensor -> dims[1],
                input_tensor -> dims[2],
                input_tensor -> dims[3]
```
Log shows `Input dims are 4x1x128x128`.

Running inference without GPU delegate shows nothing special in log, but running with GPU shows:
`Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`
"
42708,tf.debugging.set_log_device_placement(True) does not seem to be working correctly.,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow version (use command below): ('v2.3.0-0-gb36436b087', '2.3.0')
- Python version: 3.6.9 (default, Jul 17 2020, 12:50:27) 
- GPU model and memory: Tesla K80, 11441 MiB

**Describe the current behavior**
1. I open the documentation at https://www.tensorflow.org/guide/gpu in a colab
2. I change the runtime to GPU
3. I run all the cells
4. Looking at the Manual device placement section - I see the following output:
<img width=""1094"" alt=""Screen Shot 2020-08-27 at 11 16 29 AM"" src=""https://user-images.githubusercontent.com/6924410/91461401-f1d01980-e856-11ea-986a-2a0539def458.png"">


**Describe the expected behavior**
I would have expected the computation to be done on the CPU instead - am I missing something here that is colab specific?

**Standalone code to reproduce the issue**
The colab link is the one generated from visiting `https://www.tensorflow.org/guide/gpu`

"
42707,Post Quantization with TensorFlow in Python,"I am totally beginner in Python and I have a Convolutional Neural Network which consists of some conv1D maxpooling1D and dense layers

I need to Quantize this model to integer for running on FPGA

I follow post-Quantization trough this link: https://www.tensorflow.org/lite/performance/post_training_integer_quant

But I got a strange error which is:

RuntimeError: tensorflow/lite/kernels/conv.cc:316 input->dims->data[3] != filter->dims->data[3] (4 != 3)Node number 1 (CONV_2D) failed to prepare

But I don’t have any conv2D at all.

I would be very grateful if anyone can help me."
42706,Memory leak when using tensorflow c++ api,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.15.0
- Python version:
- Bazel version (if compiling from source): 0.24.1
- GCC/Compiler version (if compiling from source): 5.5.0/c++11
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Our project using bazel and tensorflow c++ api for inference. 

In our workspace, we using ""https://github.com/tensorflow/tensorflow/archive/v1.15.0.tar.gz"".

When inference on features repeatedly, memory keeps rising.

Our model is an old style pb file, not saved model. It is generated from onnx to tensorflow project.  Our graph has two inputs and one output. Here is some key code.

```
class TfModel {
 public:
  /**
   * @brief Construct.
   */
  TfModel() : pb_path_(xxx), input_names_{""input1:0"", ""input2:0""}, output_names_{""output:0""}) {
    sess_options_.config.set_use_per_session_threads(true);
    sess_options_.config.set_intra_op_parallelism_threads(0);
    sess_options_.config.set_inter_op_parallelism_threads(0);

    sess_.reset(tensorflow::NewSession(sess_options_));
    tensorflow::GraphDef graph_def;
    auto default_env = tensorflow::Env::Default();
    auto status = tensorflow::ReadBinaryProto(default_env, pb_path, &graph_def);
    if (!status.ok()) {
      throw std::invalid_argument(""Error!"");
    }
    sess_->Create(graph_def);
  }

  /**
   * @brief Compute.
   */
  void Compute(const tensorflow::Tensor &tensor1,
               const tensorflow::Tensor &tensor2) const {
    std::vector<std::pair<std::string, tensorflow::Tensor>> tf_input;
    tf_input.emplace_back(input_names_[0], tensor1);
    tf_input.emplace_back(input_names_[1], tensor2);
    std::vector<tensorflow::Tensor> output;
    auto status = sess_->Run(tf_input, output_names_, {}, &output);
  }

 private:
  const std::string pb_path_;
  const std::vector<std::string> input_names_;
  const std::vector<std::string> output_names_;
  std::unique_ptr<tensorflow::Session> sess_;
  tensorflow::SessionOptions sess_options_;
};

int main() {
  model = TfModel();
  while (true) {
    model.Compute(tensor1, tensor2);
    PrintRssMemory();
  }
}
```

I have searched some solutions and try tcmalloc. The memory leak seems negligible, but is ~25% slower than before, which is not acceptable.

The memory leak problem seems very severe if I set inter_thead and intra_thead to 0. If set to 1, the problem seems disappear. But we must use larger inter_thead and intra_thead for multi-thead.

I don't attach model since it's a little large. Hope anyone can help, Thanks.

**Describe the expected behavior**

No memory leak.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42704,Clarify that tf.image.adjust_brightness accepts negative values,"## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/image/adjust_brightness

## Description of issue (what needs changing):

The documentation states that delta should be in the range [0, 1). However, the delta is added to the image so negative values in the range (-1, 0] are also valid, and useful if the image should be made darker.

### Clear description

It is useful to be able to darken images, and the documentation should make it clear how to do so."
42703,Error building tensorflow_cc.dll (Windows 10),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 2.3.0
- Bazel version (if compiling from source): 3.1.0

**Describe the problem**
Error building tensorflow_cc.dll
```
ERROR: D:/tensorflow-2.3.0/tensorflow/BUILD:754:1: Linking of rule '//tensorflow:tensorflow_cc.dll' failed (Exit 1127): link.exe failed: error executing command
  cd C:/users/lotte/_bazel_lotte/2enhdaow/execroot/org_tensorflow
  SET LIB=c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\ATLMFC\lib\x64;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.18362.0\um\x64;
    SET PATH=c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.25.28610\bin\HostX64\x64;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\HTML Help Workshop;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\Windows\system32;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;c:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Program Files/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Program Files/Python36/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\lotte\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CONFIGURE_IOS=0
    SET TF_ENABLE_XLA=1
    SET TMP=C:\Users\lotte\AppData\Local\Temp
  c:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.25.28610/bin/HostX64/x64/link.exe @bazel-out/x64_windows-opt/bin/tensorflow/tensorflow_cc.dll-2.params
Execution platform: @local_execution_config_platform//:platform
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
external\mkl_windows\lib\libiomp5md.lib : fatal error LNK1127: library is corrupt
Target //tensorflow:tensorflow_cc failed to build
INFO: Elapsed time: 571521.839s, Critical Path: 351212.43s
INFO: 8818 processes: 8818 local.
FAILED: Build did NOT complete successfully
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
python configure.py
    empty
    empty
    n
    n
    empty
    n
    n

bazel build -c opt --config=mkl //tensorflow:tensorflow_cc
```

**Edit: Possibly related to #42730 (Error building tensorflow.dll)**"
42701,Impossible to use non mirrored variables in distributed training,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: /
- GPU model and memory: /

I would like to use a non trainable variable placed on cpu in a multi-gpu training. It is useful for using a memory bank of negative examples for instance in self-supervised learning.

```python
import tensorflow as tf
from tensorflow import keras

layers = tf.keras.layers

strategy = tf.distribute.MirroredStrategy()

class Test(layers.Layer):

    def __init__(self, memory, **kargs):
        super(Test, self).__init__(**kargs)
        
        self.memory = memory
        self.dense = layers.Dense(128)

    def call(self, inputs):
        
        res = tf.matmul(inputs, self.memory, transpose_b=True)
        res = self.dense(res)

        return res
    
def create_model(memory):
    
    input_tensor = layers.Input(
        shape=[128], name=""input_tensor""
    )
 
    x = layers.Dense(128)(input_tensor)
    
    output = Test(memory)(x)

    return keras.Model(inputs=input_tensor, outputs=output)
    
with tf.device('/cpu'):
    
    memory = tf.Variable(tf.random.uniform([100, 128]), trainable=False)
    
    
with strategy.scope():
    
    model = create_model(memory)
    
model.compile()

model.summary()

print(memory)
```

However creating a variable outside strategy.scope() results in an error:

```
Traceback (most recent call last):
  File ""code_minimal_bug_memory.py"", line 44, in <module>
    model.compile()
  File ""/opt/conda/envs/tensorflow2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 538, in compile
    self._validate_compile(optimizer, metrics, **kwargs)
  File ""/opt/conda/envs/tensorflow2.3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 2512, in _validate_compile
    '  model.compile(...)' % (v, strategy))
ValueError: Variable (<tf.Variable 'Variable:0' shape=(100, 128) dtype=float32, numpy=
array([[3.0940974e-01, 9.5359027e-01, 9.3334043e-01, ..., 2.3660135e-01,
        4.1169703e-01, 7.6202512e-02],
       [1.0460985e-01, 2.4617815e-01, 6.7584288e-01, ..., 8.3745670e-01,
        3.0766165e-01, 6.8396461e-01],
       [7.1668625e-04, 6.1034310e-01, 8.1730366e-02, ..., 3.1000912e-01,
        8.4088564e-01, 3.5998344e-02],
       ...,
       [8.0560517e-01, 6.6493630e-02, 5.6640983e-02, ..., 2.0186901e-03,
        4.2683721e-01, 6.6080117e-01],
       [6.1158764e-01, 8.3118451e-01, 2.3782039e-01, ..., 9.0197289e-01,
        3.4783411e-01, 5.6294477e-01],
       [2.9402018e-02, 2.2738779e-01, 5.7802427e-01, ..., 7.3360741e-01,
        8.2319343e-01, 1.2766552e-01]], dtype=float32)>) was not created in the distribution strategy scope of (<tensorflow.python.distribute.mirrored_strategy.MirroredStrategy object at 0x7fef7d028710>). It is most likely due to not all layers or the model or optimizer being created outside the distribution strategy scope. Try to make sure your code looks similar to the following.
with strategy.scope():
  model=_create_model()
  model.compile(...)
```

Creating the variable memory inside the strategy.scope() works but creates a MirroredVariable replicated on every gpu which is not the desired behavior:

```python

import tensorflow as tf
from tensorflow import keras

layers = tf.keras.layers

strategy = tf.distribute.MirroredStrategy()

class Test(layers.Layer):

    def __init__(self, memory, **kargs):
        super(Test, self).__init__(**kargs)
        
        self.memory = memory
        self.dense = layers.Dense(128)

    def call(self, inputs):
        
        res = tf.matmul(inputs, self.memory, transpose_b=True)
        res = self.dense(res)

        return res
    
def create_model(memory):
    
    input_tensor = layers.Input(
        shape=[128], name=""input_tensor""
    )
 
    x = layers.Dense(128)(input_tensor)
    
    output = Test(memory)(x)

    return keras.Model(inputs=input_tensor, outputs=output)
    

with strategy.scope():
    
    with tf.device('/cpu'):
    
        memory = tf.Variable(tf.random.uniform([100, 128]), trainable=False)
    
    model = create_model(memory)
    
model.compile()

model.summary()

print(memory)

```

The output is:

```
MirroredVariable:{
  0: <tf.Variable 'Variable:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)>,
  1: <tf.Variable 'Variable/replica_1:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)>,
  2: <tf.Variable 'Variable/replica_2:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)>,
  3: <tf.Variable 'Variable/replica_3:0' shape=(100, 128) dtype=float32, numpy=
array([[0.9455886 , 0.35135317, 0.03729475, ..., 0.7572125 , 0.16159093,
        0.48011708],
       [0.25856972, 0.1125381 , 0.85940254, ..., 0.21053994, 0.97001517,
        0.52998424],
       [0.23188198, 0.7152246 , 0.6255108 , ..., 0.02806866, 0.82893085,
        0.7379434 ],
       ...,
       [0.4963615 , 0.13400674, 0.4220184 , ..., 0.9512589 , 0.44520867,
        0.5342829 ],
       [0.7703849 , 0.23092055, 0.46073604, ..., 0.81235087, 0.7762462 ,
        0.38599086],
       [0.93219006, 0.04940701, 0.19656885, ..., 0.71836615, 0.56049407,
        0.71457684]], dtype=float32)>
}

```
This is a MirroredVariable replicated on each gpu and not a Variable placed on CPU as I would like."
42700,error: undefined reference to '__umoddi3',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Loongnix OS(based on Fedora) with kernel:Linux localhost.localdomain 3.10.84-22.fc21.loongson.7.mips64el
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: 2.4.0
- Python version: Python：2.7.9      Python3：3.4.1
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):3.4.1
- GCC/Compiler version (if compiling from source):gcc version 7.3.1 20180303 (Red Hat 7.3.1-6) (GCC)
- CUDA/cuDNN version:no cuda
- GPU model and memory: no gpu mode 



**Describe the problem**
when I build the tensorflow 2.4.0 source code on loongson platform（arch：mips64）use bazel 3.4.1.
[loongson@localhost tensorflow-206]$ ./configure 
You have bazel 3.4.1- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python3]: 
Found possible Python library paths:
  /usr/lib64/python3.4/site-packages
  /usr/lib/python3.4/site-packages
Please input the desired Python library path to use.  Default is [/usr/lib64/python3.4/site-packages]
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.
Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.
Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=numa        	# Build with NUMA support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
	--config=v2          	# Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished


and then build:    bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
build  failed! log as below:
Repository rule git_repository defined at:
  /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (161 packages loaded, 27000 targets configured).
INFO: Found 1 target...
ERROR: /home/loongson/.cache/bazel/_bazel_loongson/f0c20e6eab3cd4f95dcf1e27683765aa/external/com_google_protobuf/BUILD:412:10: Linking of rule '@com_google_protobuf//:protoc' failed (Exit 1)
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&): error: undefined reference to '__umoddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::_Hashtable<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_rehash(unsigned long, unsigned long const&): error: undefined reference to '__umoddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::__detail::_Map_base<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&): error: undefined reference to '__umoddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/command_line_interface.o:command_line_interface.cc:function std::__detail::_Map_base<std::string, std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*>, std::allocator<std::pair<std::string const, google::protobuf::compiler::CommandLineInterface::GeneratorContextImpl*> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&): error: undefined reference to '__umoddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protoc_lib/cpp_padding_optimizer.o:cpp_padding_optimizer.cc:function __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > std::_V2::__rotate<__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > > >(__gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, __gnu_cxx::__normal_iterator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup*, std::vector<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup, std::allocator<google::protobuf::compiler::cpp::(anonymous namespace)::FieldGroup> > >, std::random_access_iterator_tag) [clone .isra.94]: error: undefined reference to '__moddi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/tokenizer.o:tokenizer.cc:function google::protobuf::io::Tokenizer::ParseInteger(std::string const&, unsigned long, unsigned long*): error: undefined reference to '__udivdi3'
bazel-out/host/bin/external/com_google_protobuf/_objs/protobuf/tokenizer.o:tokenizer.cc:function google::protobuf::io::Tokenizer::ParseInteger(std::string const&, unsigned long, unsigned long*): error: undefined reference to '__udivdi3'
collect2: error: ld returned 1 exit status
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 452.104s, Critical Path: 44.81s
INFO: 307 processes: 307 local.
FAILED: Build did NOT complete successfully



**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42698,Tensorflow & Keras update causes significantly worse model performance. ,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

While doing research, I make sure to architect it to be reproducible and have quality assurance checks. When using Google Colab which has the most recent version of Tensorflow and Keras, I noticed that the loss was dramatically worse for the exact same code, as referenced here, https://github.com/hammad93/hurricane-net/issues/7

The solution was to downgrade Keras to 2.2 although I think 2.3 would work too. I'm not sure if it's the learning rate because even after increasing the epochs, I wasn't able to achieve the same accuracy. I won't be able to switch over to Tensorflow or the newest version of Keras. Just for reference, the model loss as reported by mean squared error is supposed to be 0.31 but the lowest I have been able to get with the exact same code is 0.40 and doing a gaussian hyper parameter search does not fix it. 

Here's the notebook for reference, https://github.com/hammad93/hurricane-net/blob/master/hurricane_net.ipynb"
42696,DataHandler intermittent InvalidArgumentError for large input dimension,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc2-77-gaad398b5e9 2.2.0-rc3
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: V100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Based on tracing an error for a call to the `fit` method of a model, I determined that an `InvalidArgumentError` was contingent on both the number of inputs N in a dataset batch (in this case the data is of the form BxNxC) as well as whether `class_weights` were being set.

**Describe the expected behavior**
Setting `class_weights` as shown in the online [docs](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data) would not break calls to `fit` depending on the number of input data. I determined that the DataHandler was the object that threw the exception during the `model.fit` call, so the min example below is based on a DataHandler object.

This is related to this [issue](https://github.com/tensorflow/tensorflow/issues/23698), but I don't see how the issue is resolved for arbitrary models. In particular, it has nothing to do with NLP, as is mentioned in a [comment](https://github.com/tensorflow/tensorflow/issues/23698#issuecomment-673667914)

**Standalone code to reproduce the issue**
Colab [link](https://colab.research.google.com/drive/1jAPPKv5yKMNvwzJV4dvJZFBFzfrn6h8W#scrollTo=FinvXcZG3K67)

Same code posted here:
``` python
import tensorflow as tf
import numpy as np
from tensorflow.python.keras.engine import data_adapter
def test_weights(model, n_samples, use_weights=True):
    def test_data():
        def test_data_gen():
            n_classes = 5
            x = np.random.randn(n_samples,3)
            y = np.random.randint(0,n_classes,n_samples)
            yield (x.astype(np.float32),
                   y.astype(np.int32))
        gen_func = test_data_gen
        gen_types = (tf.float32, tf.int32)
        gen_shapes = ([None, 3], [None])
        return gen_func, gen_types, gen_shapes
    gen_fn, gen_tp, gen_sh = test_data()
    ds_tst = tf.data.Dataset.from_generator(gen_fn, gen_tp, gen_sh)
    ds_tst = ds_tst.batch(2)
    ds_tst = ds_tst.prefetch(2)
    cw = {0 : 0.0, 1 : 1.5, 2 : 0.5,
          3 : 0.5, 4 : 0.5}
    data_handler = data_adapter.DataHandler(
        x=ds_tst,
        y=None,
        sample_weight=None,
        batch_size=None,
        steps_per_epoch=10,
        initial_epoch=0,
        epochs=1,
        shuffle=True,
        class_weight=(cw if use_weights else None),
        max_queue_size=10,
        workers=1,
        use_multiprocessing=False,
        model=model)
    print ('NEXT',next(iter(data_handler._dataset)))

model = tf.keras.Model()
test_weights(model, 5) # Always succeeds
test_weights(model, 50000) # Sometimes fails
test_weights(model, 50000, use_weights=False) # Always succeeds
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
``` bash
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/context.py in execution_mode(mode)
   2101       ctx.executor = executor_new
-> 2102       yield
   2103     finally:

11 frames
InvalidArgumentError: indices[0] = 5 is not in [0, 5)
	 [[{{node GatherV2}}]] [Op:IteratorGetNext]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/executor.py in wait(self)
     65   def wait(self):
     66     """"""Waits for ops dispatched in this executor to finish.""""""
---> 67     pywrap_tfe.TFE_ExecutorWaitForAllPendingNodes(self._handle)
     68 
     69   def clear_error(self):

InvalidArgumentError: indices[0] = 5 is not in [0, 5)
	 [[{{node GatherV2}}]]
```"
42694,Translataion bug in tf.math.reduce_variance and tf.keras.backend.var,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra-10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Not checked
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
Model built using tensorflow and later converter to tflite gives different results for same input. See code provided to reproduce the error. Issue [#42346](https://github.com/tensorflow/tensorflow/issues/42346), I created earlier, might have the same error.

**Command used to run the converter or code if you’re using the Python API**
Code to reproduce error with output
```
import tensorflow as tf
import tflite
import numpy as np
import imutils

tf.compat.v1.enable_eager_execution()

class MyFunc(tf.Module):
    @tf.function(input_signature=[tf.TensorSpec(shape=(None, None, 3), dtype=tf.float32, name='imdata')])
    def detect(self, imdata):
        # both tf.keras.backend.var and tf.math.reduce_variance operations create the same error scores
        #tensor_var = tf.keras.backend.var(imdata)
        tensor_var = tf.math.reduce_variance(imdata)
        return tensor_var
tf_func = MyFunc()

conc_func = tf_func.detect.get_concrete_function()
converter = tf.lite.TFLiteConverter.from_concrete_functions([conc_func])
fpath = 'model.tflite'
with tf.io.gfile.GFile(fpath, 'wb') as f:
    f.write(converter.convert())
loaded_model = tf.lite.Interpreter(fpath)

def tflite_inference(tmp_model, tmp_img_data):
    input_details = tmp_model.get_input_details()
    output_details = tmp_model.get_output_details()
    input_data = np.array(tmp_img_data).astype(input_details[0]['dtype'])
    w_img, h_img, ch_img = tmp_img_data.shape
    tmp_model.resize_tensor_input(input_details[0][""index""], [w_img, h_img, 3])
    tmp_model.allocate_tensors()
    tmp_model.set_tensor(input_details[0]['index'], tmp_img_data)
    tmp_model.invoke()
    result = tmp_model.get_tensor(output_details[0]['index'])
    return result


images = ['https://images.unsplash.com/photo-1470020337050-543c4e581988',
          'https://images.unsplash.com/photo-1516811108838-030371f93644',
          'https://zipbooks.com/wp-content/uploads/2017/05/royalty-free-images-free-of-charge.jpeg',
       ]

print(""tensorflow vs tflite inference scores on images"")
for img_url in images:
    tmp_imdata = imutils.url_to_image(img_url).astype(np.float32)
    print(f'\t tensorflow: {tf_func.detect(tmp_imdata).numpy():.4f}', end='\t')
    print(f'tflite: {tflite_inference(loaded_model, tmp_imdata):.4f}')

    
print(""\n\n tensorflow vs tflite inference scores on random arrays"")
shape = (2000, 2000, 3)
num_range = 255
arrays = [np.random.randint(num_range, size=shape).astype(np.float32), 
          np.random.randint(num_range, size=shape).astype(np.float32),
          np.random.randint(num_range, size=shape).astype(np.float32)
         ]
print('\t tensorflow \t tflite \t   Equal?')
for arr in arrays:
    tf_score = tf_func.detect(arr).numpy()
    tflite_score = tflite_inference(loaded_model, arr)
    eq = tf_score == tflite_score
    print(f'\t {tf_score:.4f} \t {tflite_score:.4f} \t   {eq}')
```

**The output from the converter invocation** : 
```
tensorflow vs tflite inference scores on images
	 tensorflow: 4722.1460	tflite: 4706.3296
	 tensorflow: 5759.6831	tflite: 4486.9917
	 tensorflow: 1643.2089	tflite: 1642.7393


 tensorflow vs tflite inference scores on random arrays
	 tensorflow 	 tflite 	   Equal?
	 5417.6357 	 5351.8447 	   False
	 5418.3472 	 5352.1660 	   False
	 5419.2515 	 5353.3892 	   False
```

**Also, please include a link to the saved model or GraphDef** : Unzip file [model.tflite.zip](https://github.com/tensorflow/tensorflow/files/5133550/model.tflite.zip) .


**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy: Wrong results
- Producing correct results, but the model is slower than expected (model generated from old converter): No


**Any other info / logs**
See above for source code and logs.
"
42693,proximal policy gradient tensorflow pendulum issue,"import gym  
import numpy as np  
import tensorflow as tf


class Memory(object):  
    
    def __init__(self):
        self.ep_obs, self.ep_act, self.ep_rwd, self.ep_neglogp = [], [], [], []

    def store_transition(self, obs0, act, rwd, neglogp):
        self.ep_obs.append(obs0)
        self.ep_act.append(act)
        self.ep_rwd.append(rwd)
        self.ep_neglogp.append(neglogp)

    def covert_to_array(self):
        array_obs = np.vstack(self.ep_obs)
        array_act = np.vstack(self.ep_act)
        array_rwd = np.array(self.ep_rwd)
        array_neglogp = np.array(self.ep_neglogp).squeeze(axis=1)
        return array_obs, array_act, array_rwd, array_neglogp

    def reset(self):
        self.ep_obs, self.ep_act, self.ep_rwd, self.ep_neglogp = [], [], [], []


class ActorNetwork(object):
    
    def __init__(self, act_dim, name):
        self.act_dim = act_dim
        self.name = name

    def step(self, obs, reuse):
        with tf.variable_scope(self.name, reuse=reuse):
            h1 = tf.layers.dense(obs, 100, activation=tf.nn.relu)
            mu = 2 * tf.layers.dense(h1, self.act_dim, activation=tf.nn.tanh)
            sigma = tf.layers.dense(h1, self.act_dim, activation=tf.nn.softplus)
            pd = tf.distributions.Normal(loc=mu, scale=sigma)
        return pd

    def choose_action(self, obs, reuse=False):
        pd = self.step(obs, reuse)
        action = tf.squeeze(pd.sample(1), axis=0)
        action = tf.clip_by_value(action, -2, 2)
        return action

    def get_neglogp(self, obs, act, reuse=False):
        pd = self.step(obs, reuse)
        logp = pd.log_prob(act)
        return logp


class ValueNetwork(object):
    
    def __init__(self, name):
        self.name = name

    def step(self, obs, reuse):
        with tf.variable_scope(self.name, reuse=reuse):
            h1 = tf.layers.dense(inputs=obs, units=100, activation=tf.nn.relu)
            value = tf.layers.dense(inputs=h1, units=1)
            return value

    def get_value(self, obs, reuse=False):
        value = self.step(obs, reuse)
        return value


class PPO(object):
    
    def __init__(self, act_dim, obs_dim, lr_actor, lr_value, gamma, clip_range):
        self.act_dim = act_dim
        self.obs_dim = obs_dim
        self.lr_actor = lr_actor
        self.lr_value = lr_value
        self.gamma = gamma
        self.clip_range = clip_range

        self.OBS = tf.placeholder(tf.float32, [None, self.obs_dim], name=""observation"")
        self.ACT = tf.placeholder(tf.float32, [None, self.act_dim], name=""action"")
        self.Q_VAL = tf.placeholder(tf.float32, [None, 1], name=""q_value"")
        self.ADV = tf.placeholder(tf.float32, [None, 1], name=""advantage"")
        self.NEGLOGP = tf.placeholder(tf.float32, [None, 1], name=""old_neglogp"")

        actor = ActorNetwork(self.act_dim, 'actor')
        value = ValueNetwork('critic')
        self.memory = Memory()

        self.action = actor.choose_action(self.OBS)
        self.neglogp = actor.get_neglogp(self.OBS, self.ACT, reuse=True)
        ratio = tf.math.exp(self.neglogp - self.NEGLOGP)
        clip_ratio = tf.clip_by_value(ratio, 1. - self.clip_range, 1. + self.clip_range)
        actor_loss = -tf.reduce_mean((tf.minimum(ratio* self.ADV, clip_ratio* self.ADV)) )
        self.actor_train_op = tf.train.AdamOptimizer(self.lr_actor).minimize(actor_loss)

        self.value = value.get_value(self.OBS)
        self.advantage = self.Q_VAL - self.value
        value_loss = tf.reduce_mean(tf.square(self.advantage))
        self.value_train_op = tf.train.AdamOptimizer(self.lr_value).minimize(value_loss)

        self.sess = tf.Session()
        self.sess.run(tf.global_variables_initializer())

    def step(self, obs):
        if obs.ndim < 2: obs = obs[np.newaxis, :]
        action = self.sess.run(self.action, feed_dict={self.OBS: obs})
        action = np.squeeze(action, 1).clip(-2, 2)

        neglogp = self.sess.run(self.neglogp, feed_dict={self.OBS: obs, self.ACT: action[np.newaxis, :]})

        value = self.sess.run(self.value, feed_dict={self.OBS: obs})
        value = np.squeeze(value, 1).squeeze(0)
        return action, neglogp, value

    def learn(self, last_value, done):
        obs, act, rwd, neglogp = self.memory.covert_to_array()
        rwd = (rwd - rwd.mean()) / (rwd.std() + 1e-5)
        q_value = self.compute_q_value(last_value, obs, rwd)

        adv = self.sess.run(self.advantage, {self.OBS: obs, self.Q_VAL: q_value})

        [self.sess.run(self.value_train_op,
                       {self.OBS: obs, self.Q_VAL: q_value}) for _ in range(10)]
        [self.sess.run(self.actor_train_op,
                          {self.OBS: obs, self.ACT: act, self.ADV: adv, self.NEGLOGP: neglogp}) for _ in range(10)]

        self.memory.reset()

    def compute_q_value(self, last_value, obs, rwd):
        q_value = np.zeros_like(rwd)
        value = self.sess.run(self.value, feed_dict={self.OBS: obs})
        for t in reversed(range(0, len(rwd)-1)):
            q_value[t] = value[t+1] * self.gamma + rwd[t]
        return q_value[:, np.newaxis]


env = gym.make('Pendulum-v0')
env.seed(1)
env = env.unwrapped

agent = PPO(act_dim=env.action_space.shape[0], obs_dim=env.observation_space.shape[0],
            lr_actor=0.0004, lr_value=0.0003, gamma=0.9, clip_range=0.2)

nepisode = 1000
nstep = 200

for i_episode in range(nepisode):
    obs0 = env.reset()
    ep_rwd = 0

    for t in range(nstep):
        act, neglogp, _ = agent.step(obs0)
        obs1, rwd, done, _ = env.step(act)

        agent.memory.store_transition(obs0, act, rwd, neglogp)

        obs0 = obs1
        ep_rwd += rwd

        if (t + 1) % 32 == 0 or t == nstep - 1:
            _, _, last_value = agent.step(obs1)
            agent.learn(last_value, done)

    print('Ep: %i' % i_episode, ""|Ep_r: %i"" % ep_rwd)

I've implemented proximal policy optimization on tensorflow, enviroment pendulum v0

    if there is something wrong above tell me, why my code its not working, it's days that im trying to fix this, but my pendulum doesnt converge, I dont see what can be wrong ? can someone help? I've implemented proximal policy optimization on tensorflow, enviroment pendulum v0
    Im implementing a reinforcement learning algorithm, Im trying so hard to make this code to work no success;
"
42691,Cannot build network with a dict input_spec,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 (in docker container)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.2.0-rc4-8-g2b96f3662b 2.2.0
- Python version: 3.6.9
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.2, N/A
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Cannot build a model with a `dict` input_spec (`model.build`)

**Describe the expected behavior**

The model should be able to build the graph with a `dict` input_spec:

1. We can call a model with dictionary inputs, so specifying that spec as dictionary seems natural
2. I believe this is what the developers intended too, given [this line](https://github.com/tensorflow/tensorflow/blob/2b96f3662bd776e277f86997659e61046b56c315/tensorflow/python/keras/engine/network.py#L650) in the current master branch. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
In [33]: class TestModel(keras.Model):
    ...:     def __init__(self, *args, **kwargs):
    ...:         super(TestModel, self).__init__(*args, **kwargs)
    ...:         self.layer = keras.layers.Dense(10)
    ...:     def call(self, inputs):
    ...:         return {'b': self.layer(inputs['a'])}
    ...: 

In [34]: model = TestModel()

In [35]: model.build({ 'a': (None, 5) })
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-35-f5e425698cd1> in <module>
----> 1 model.build({ 'a': (None, 5) })

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py in build(self, input_shape)
    633                        'Please specify a batch input shape of type tuple or '
    634                        'list of input shapes. User provided '
--> 635                        'input type: {}'.format(type(input_shape)))
    636
    637     if input_shape and not self.inputs:

ValueError: Specified input shape is not one of the valid types. Please specify a batch input shape of type tuple or list of input shapes. User provided input type: <class 'dict'>

In [36]: model({'a': np.random.random((3,5)).astype(np.float32)})
Out[36]: 
{'b': <tf.Tensor: shape=(3, 10), dtype=float32, numpy=
 array([[ 0.57159793, -0.54181933, -0.05867613, -0.13133162,  0.33426586,
          0.88725936,  0.6102787 ,  0.25660542, -0.32508045,  0.18882477],
        [ 0.5146243 , -0.07790124, -0.13295858,  0.33439606,  0.6062204 ,
          0.67318   ,  0.7965131 , -0.09740189, -0.49425927, -0.3331495 ],
        [ 0.828179  , -0.3307852 , -0.49201646, -0.16802055,  0.44089833,
          0.8766354 ,  0.92791915,  0.28562796, -0.8893677 , -0.428018  ]],
       dtype=float32)>}
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42690,warning: 404 not found,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : SMP Debian 4.19.118-2 (2020-04-29)
- TensorFlow installed from : source
- TensorFlow version: 2.3
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 3.3
- GCC/Compiler version (if compiling from source):8.3
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A



**Describe the problem**
while building I get the following warning what does it mean?
WARNING: Download from https://storage.googleapis.com/mirror.tensorflow.org/github.com/llvm/llvm-project/archive/642cb7865f35ad7dbac78d716fcddaff561e8639.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=mkl  //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42688,individual neuron pruning (similar to tfkerassurgeon),"Just wanted to bring up that existing pruning packages (tfkerassurgeon and keras-surgeon https://github.com/Raukk/tf-keras-surgeon) are no longer supported or compatible with Tensorflow, so it'd be great if Tensorflow could implement their own simple individual neuron pruning capability. I'm aware of the existing Tensorflow APIs that focus on automated low-magnitude pruning(https://www.tensorflow.org/model_optimization/guide/pruning/comprehensive_guide), but being able to simply delete specific neurons/channels from layers or delete specific layers would be useful, and aid in the study of neural networks. For example, something like:
```
# delete layer_1 from a model
model = delete_layer(model, layer_1)
# delete channels 0, 4 and 67 from layer_2 in model
model = delete_channels(model, layer_2, [0,4,67])
```
This allows researchers and scientists much more flexibility in deciding how exactly they want to prune their model, and foster development of new pruning techniques"
42687,"[docs] `nbfmt` removes output cells but does not clear ""execution count""","Using `python -m tensorflow_docs.tools.nbfmt` on an executed notebook clears the output cells but leaves the ""execution count"" filled in, which can result in unnecessary diff noise and only really makes sense in the context of a filled-out notebook. For reference, the ""all output -> clear"" jupyter menu option sets all the execution counts to `null`."
42686,Unrecognized command line -msse3 while cross-compiling tensorflow to aarch64 targets,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
   -> Linux Ubuntu 18.04 (docker image)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
   -> Build setup is x86_64 machine
- TensorFlow installed from (source or binary):
   -> Source (compiling from source)
- TensorFlow version:
   -> v2.3.0
- Python version:
   -> python3.6
- Installed using virtualenv? pip? conda?:
   -> installed using pip3
- Bazel version (if compiling from source):
   -> bazel 3.1.0 (recommended for v2.3.0 inside documents)
- GCC/Compiler version (if compiling from source):
   -> Cross-compiler: aarch64-linux-gnu-gcc-7 (installed by nvidia sdk manager)
- CUDA/cuDNN version:
   -> Cuda: 10.2, cuDNN 7
- GPU model and memory:
   -> NA

**Describe the problem**
aarch64-linux-gnu-gcc-7: error: unrecognized command line option '-msse3'

**Provide the exact sequence of commands / steps that you executed before running into the problem**

This is the build command that I am using:
bazel build --cpu=aarch64 --config=opt --config=noaws //tensorflow:libtensorflow_cc.so --sandbox_debug 

Performed ./configure before running the above build command.
This is the output of .tf_configure_bazel.rc file:
build --action_env PYTHON_BIN_PATH=""/usr/bin/python3.6""
build --action_env PYTHON_LIB_PATH=""/usr/lib/python3/dist-packages""
build --python_path=""/usr/bin/python3.6""
build --config=xla
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda-10.2""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda-10.2/lib64:/usr/lib/x86_64-linux-gnu:/usr/lib/i386-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/aarch64-linux-gnu-gcc-7""
build --config=cuda
build:opt --copt=-march=armv8-a
build:opt --define with_default_optimizations=true
test --flaky_test_attempts=3
test --test_size_filters=small,medium
test --test_env=LD_LIBRARY_PATH
test:v1 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial
test:v1 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu
test:v2 --test_tag_filters=-benchmark-test,-no_oss,-no_gpu,-oss_serial,-v1only
test:v2 --build_tag_filters=-benchmark-test,-no_oss,-no_gpu,-v1only
build --action_env TF_CONFIGURE_IOS=""0""

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42684,Timing Discrepency betweeen Tflite Benchmarking Tool and Android Studio Inference Time,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.15
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
While Benchmarking the model using the benchmarking tool the timing reported did not match that as viewed while running inference on the model on an APK. In almost all instances, the timing reported was almost twice as that seen on the Benchmarking tool. The GPU delegate is used to run inference on this model. 
To measure timing on the APK we employed both the methods. 
i) Using the getLastNativeInferenceDurationNanoseconds() function provided by Tflite
ii) Using the SystemClock.uptimeMillis() function provided in the os.SystemClock library. 
   The code for the same is:
            long str_time = SystemClock.uptimeMillis();
            tflite_intep.run(inp,out);
            System.out.println(""Inference Time :""+(SystemClock.uptimeMillis()-str_time)); 

In this instance the output from both the getLastNativeInferenceDurationNanoseconds() as well as the Benchmarking tool is 35.421 ms while that shown in the logcat from the SystemClock function is 70.33 ms. 


**Describe the expected behavior**

The timing shown in all 3 scenarios need to match or at least be within a reasonable amount from each other. 


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


Please do let me know if any furthur information is needed. "
42682,MobilenetV3 Top 5 Accuracy issue,"I use pre-trained model from this site: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet 
I download v3-large_224_1.0_float.pb model link is (Large dm=1 (float) ). After I run it, I often get this result,
===== TENSORFLOW RESULTS =======
cello, violoncello (score = 1.00000)
hip, rose hip, rosehip (score = 0.00000)
banjo (score = 0.00000)
steel drum (score = 0.00000)
marimba, xylophone (score = 0.00000)
Then, I change input image, but the result is the same. I do know how to solve this problem.Thank you.
My code is as below:
[mobilenet_v3.txt](https://github.com/tensorflow/tensorflow/files/5131057/mobilenet_v3.txt)





"
42681,ImageDataGenerator should have a verbosity argument,"**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
When calling [`tensorflow.keras.preprocessing.image.ImageDataGenerator.flow_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory), currently a message like `Found 1437 images belonging to 2 classes.` is printed.

I suggest adding an optional keyword argument `verbose` with default setting `verbose=1` to this function, such that `verbose=0` suppresses this printed message. See e.g. the `verbose` keywords of [`tf.keras.Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit), or of the [`EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) or [`ModelCheckpoint`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callbacks.

**Will this change the current api? How?**
This will add an optional keyword argument.

**Who will benefit with this feature?**

* Everyone who needs to call this function repeatedly in order to loop over different `batch_size` values during hyperparameter tuning. In this case, the repeated message is uninteresting and distracts from the interesting output and should therefore be omittable.
* Everyone who upgraded from standalone Keras (which did not show this message, e.g. version 2.2.4), who wants to get the same output as before.
"
42680,adapt() method of a CategoryEncoding layer returns the wrong shape when fitted to Integer list not starting at 0,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.0-dev20200825
- Python version: 3.7.8

**Describe the current behavior**
When calling ""adapt()"" method on a CategoryEncoding layer with either:
- a dataset feature with integers not starting at 0 (eg. month identifiers from 1 to 12)
- a np.array with a list of possible integers not starting at 0
the resulting shape of the CategoryEncoding layer seems to be determined by the max integer, and not the cardinality of integers in the list / data.
eg, we get a shape of 13 if we adapt the layer to an integer list ranging from 1 to 12, or even from 3 to 12 for example

**Describe the expected behavior**
Expect the adapt method of a CategoryEncoding layer to output an object with a shape equal to the number of unique values in the integer-coded feature


**Standalone code to reproduce the issue**
```

import numpy as np
import tensorflow as tf

month_input_layer = tf.keras.layers.Input(shape=(1,), name=""month"", dtype=""int64"")
month_encoded_layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(output_mode=""binary"")
dense_layer = tf.keras.layers.Dense(1,activation=""relu"")

month_encoded_layer.adapt(np.arange(1,13))
output = dense_layer(month_encoded_layer(month_input_layer))

model = tf.keras.models.Model(month_input_layer, output)

model.summary()
```

**Other info / logs**:
```

2020-10-05 10:12:27.032788: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-05 10:12:27.103511: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1dbad9023f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-05 10:12:27.112275: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
(.venv) PS C:\Users\SamuelCheptou\CSN Energy\Marché - Paris Restricted - General\5# Ressources & Données\CDC\Analyses CDC forecasting\scripts python SC>  cd 'c:\Users\SamuelCheptou\CSN Energy\Marché - Paris Restricted - General\5# Ressources & Données\CDC\Analyses CDC forecasting\scripts python SC'; & 'c:\Users\SamuelCheptou\local_CDC\.venv\Scripts\python.exe' 'c:\Users\SamuelCheptou\.vscode\extensions\ms-python.python-2020.9.112786\pythonFiles\lib\python\debugpy\launcher' '57146' '--' 'c:\Users\SamuelCheptou\CSN Energy\Marché - Paris Restricted 
- General\5# Ressources & Données\CDC\Analyses CDC forecasting\scripts python SC\Github_bug.py' 
2020-10-05 10:31:37.812387: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-10-05 10:31:37.833441: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x241a1d351f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-10-05 10:31:37.861026: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Model: ""functional_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
month (InputLayer)           [(None, 1)]               0
_________________________________________________________________
category_encoding (CategoryE (None, 13)                1
_________________________________________________________________
dense (Dense)                (None, 1)                 14
=================================================================
Total params: 15
Trainable params: 14
Non-trainable params: 1
_________________________________________________________________

```"
42679,Bug using dictionary in tensorflow2.3,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: /
- GPU model and memory: /

The code below gives an incorrect summary.

```python
import tensorflow as tf
from tensorflow import keras

layers = tf.keras.layers


class LayerTest(layers.Layer):
    def __init__(self):
        super(LayerTest, self).__init__()

    def call(self, inputs) -> tf.Tensor:
        predictions = inputs
        
        for k in predictions.keys():
            predictions[k] = tf.math.l2_normalize(predictions[k], axis=-1)

        for step_name in predictions.keys():
            loss = tf.reduce_mean(predictions[k])
            
        return loss
    
    
def Model(target_dim: int = 64):
   
    input_tensor = layers.Input(
        shape=[target_dim], name=""input_tensor""
    )
 
    predictions = {'step_0': layers.Lambda(lambda _x: _x)(input_tensor), 'step_1': layers.Lambda(lambda _x: _x)(input_tensor)}
    
    logits = LayerTest()(predictions)

    return keras.Model(inputs=input_tensor, outputs=logits)

               
model = Model()
        
model.compile()
                       
model.summary()
```

The output using tensorflow2.3 is the following:

```
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_tensor (InputLayer)       [(None, 64)]         0                                            
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 64)           0           input_tensor[0][0]               
__________________________________________________________________________________________________
tf_op_layer_layer_test/l2_norma [(None, 64)]         0           lambda_1[0][0]                   
__________________________________________________________________________________________________
tf_op_layer_layer_test/l2_norma [(None, 1)]          0           tf_op_layer_layer_test/l2_normali
__________________________________________________________________________________________________
tf_op_layer_layer_test/l2_norma [(None, 1)]          0           tf_op_layer_layer_test/l2_normali
__________________________________________________________________________________________________
tf_op_layer_layer_test/l2_norma [(None, 1)]          0           tf_op_layer_layer_test/l2_normali
__________________________________________________________________________________________________
tf_op_layer_layer_test/l2_norma [(None, 64)]         0           lambda_1[0][0]                   
                                                                 tf_op_layer_layer_test/l2_normali
__________________________________________________________________________________________________
```

The correct output using tensorflow2.2 is the following:

```__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_tensor (InputLayer)       [(None, 64)]         0                                            
__________________________________________________________________________________________________
lambda (Lambda)                 (None, 64)           0           input_tensor[0][0]               
__________________________________________________________________________________________________
lambda_1 (Lambda)               (None, 64)           0           input_tensor[0][0]               
__________________________________________________________________________________________________
layer_test (LayerTest)          ()                   0           lambda[0][0]                     
                                                                 lambda_1[0][0]                   
=================================================================================================
```

Creating a new dictionary to make the updates gives the correct results:

```python
class LayerTest(layers.Layer):
    def __init__(self):
        super(LayerTest, self).__init__()

    def call(self, inputs) -> tf.Tensor:
        predictions = inputs

        predictions2 = {}
        
        for k in predictions.keys():
            predictions2[k] = tf.math.l2_normalize(predictions[k], axis=-1)

        for step_name in predictions2.keys():
            loss = tf.reduce_mean(predictions2[k])
            
        return loss
```"
42677,ValueError in tf.data.Dataset.from_tensor_slices when array contains a mixture of integers and strings,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes (variation of Load a pandas.DataFrame tutorial)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.3 LTS (Bionic Beaver)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9


**Describe the current behavior**

When a DataFrame ```df``` contains a mixture of integers and strings, calling ```tf.data.Dataset.from_tensor_slices(df.values)``` results in the error ```ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type int).```.

**Describe the expected behavior**

I don't think this should crash. I think it should produce a TensorSliceDataset object.

**Standalone code to reproduce the issue**

Reproduced the issue here: https://colab.research.google.com/drive/15qben_czJvRS80pVZyiltDO4FDzmHryg?usp=sharing

The code is loosely based on the ""Load a pandas.DataFrame"" tutorial, except I don't one-hot encode the ""thal"" column, as I have a similar use-case where I have a combination of sentences and numbers as features."
42675,"“default Round and Robin”, “tf.fixed_ size_ Partitioner” and “tf.min_max_variable_partitioner” ","Suppose there are three PS servers and three tf.Variable (shape=[100000,256])。
Round and Robin policy: each PS server puts a tensor;
tf.fixed_ size_ Partitioner (3,0): assign 1 / 3 of each tensor to each PS server.

**Is the computing efficiency of the two strategies the same?**

**If the shape of tensor is quite different, is the latter better?**
**Would it be better to have some small tensors on the same PS?**
**tf.min_max_variable_partitioner：min_ slice_ size can't be used up. Will it be recycled?**

Thanks!
"
42673,Cannot convert a Tensor of dtype resource to a NumPy array,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if you’re using the Python API**
Colab is here: https://colab.research.google.com/drive/1X0MhpbPbguscxwY-UDyp9d0OsL5hANR2?usp=sharing
```
tfliteModel = tf.lite.TFLiteConverter.from_keras_model(model).convert()
```

**The output from the converter invocation**
```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-11-ef48ab45d342> in <module>()
----> 1 tfliteModel = tf.lite.TFLiteConverter.from_keras_model(model).convert()
      2 
      3 with tf.io.gfile.GFile('my_model.tflite', 'wb') as f:
      4     f.write(tfliteModel)

6 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/lite.py in convert(self)
    807     frozen_func, graph_def = (
    808         _convert_to_constants.convert_variables_to_constants_v2_as_graph(
--> 809             self._funcs[0], lower_control_flow=False))
    810 
    811     input_tensors = [

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)
   1101       func=func,
   1102       lower_control_flow=lower_control_flow,
-> 1103       aggressive_inlining=aggressive_inlining)
   1104 
   1105   output_graph_def, converted_input_indices = _replace_variables_by_constants(

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in __init__(self, func, lower_control_flow, aggressive_inlining, variable_names_whitelist, variable_names_blacklist)
    802         variable_names_whitelist=variable_names_whitelist,
    803         variable_names_blacklist=variable_names_blacklist)
--> 804     self._build_tensor_data()
    805 
    806   def _build_tensor_data(self):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/convert_to_constants.py in _build_tensor_data(self)
    821         data = map_index_to_variable[idx].numpy()
    822       else:
--> 823         data = val_tensor.numpy()
    824       self._tensor_data[tensor_name] = _TensorData(
    825           numpy=data,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in numpy(self)
   1061     """"""
   1062     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1063     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1064     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1065 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1029       return self._numpy_internal()
   1030     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1031       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1032 
   1033   @property

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.
```

**Also, please include a link to the saved model or GraphDef**
```
https://drive.google.com/file/d/1iXmNGjMTPoqzBRpeNmTjarGGEIBK8Jdr/view?usp=sharing
```

**Any other info / logs**
I can't convert a model which contains tf.keras.layers.experimental.preprocessing.TextVectorization to tf.lite.
I've got this error:
```InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.```

- I saw this issue ( https://github.com/tensorflow/tensorflow/issues/32693 ) and tried to use `converter.experimental_new_converter = True` -- it did not help
- I saw this answer ( https://stackoverflow.com/questions/59962509/valueerror-cannot-convert-a-tensor-of-dtype-resource-to-a-numpy-array ) and tried to use a custom layer -- it did not help

Please see this colab:
https://colab.research.google.com/drive/1X0MhpbPbguscxwY-UDyp9d0OsL5hANR2?usp=sharing

Can you please help me"
42672,"tf.contrib.crf.crf_decode ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.","When I use the tf.contrib.crf.crf_decode function to find the most probably path in CTC. **The Code is:**

`import numpy as np
import tensorflow as tf

xs=tf.constant(np.random.rand(3,10,5),dtype=tf.float32)
xlen=tf.constant([8,10,9],dtype=tf.int32)
ys=tf.constant([[4,1,0,0],[2,4,3,3],[3,1,2,0]],dtype=tf.int32)
ylen=tf.constant([2,4,3],dtype=tf.int32)

> def viterbi_search(yt,xlen,ys,ylen):
>     
>     pad=tf.zeros_like(ys)
>     yb=tf.reshape(tf.stack([ys,pad],axis=-1),[tf.shape(ys)[0],-1])
>     yb=tf.concat([tf.zeros([tf.shape(ys)[0],1],dtype=tf.int32),yb],axis=-1) #插入blank之后的标签
> 
>     def search_best_path(yt,xlen,ys,ylen,yb):
>         # 构建概率发射矩阵 potentials
>         yt=tf.gather(yt,yb,axis=-1)
>         blen=tf.shape(yb)[0]
>         m=tf.reshape(tf.concat([yt[0,:2],tf.zeros(blen-2)],axis=0),(1,-1))
>         n=tf.reshape(tf.concat([tf.zeros(blen-2),yt[-1,-2:]],axis=0),(1,-1))
>         yt=tf.concat([m,yt[1:-1,:],n],axis=0)
> 
>         # 构建状态转移矩阵 transition_params
>         const=tf.constant([[2,3],[2,2]],dtype=tf.int32)
>         k=tf.cast(tf.math.equal(ys[:-1]-ys[1:],0),tf.int32)
>         tlen=tf.reshape(tf.gather(const,k),(-1,))
>         tlen=tf.concat([tlen,[2,2,1]],axis=0)+tf.range(blen)
> 
>         trans=tf.map_fn(lambda k:tf.concat([tf.ones(k,dtype=tf.int32),tf.zeros(blen-k,dtype=tf.int32)],axis=0),tlen)
>         trans=tf.math.log(tf.linalg.band_part(tf.cast(trans,dtype=tf.float32),0,-1))
> 
>         yt=tf.math.log(tf.expand_dims(yt,axis=0))
>         xlen=tf.reshape(xlen,shape=[-1])
>         yl,_=tf.contrib.crf.crf_decode(potentials=yt,transition_params=trans,sequence_length=xlen)
>         yf=tf.gather(yb,yl[0]) #每一帧的实际对齐结果 CTC sequence
>         ref=tf.concat([[0],yf[:-1]],axis=0)
>         yr=tf.where(tf.equal(yf,ref),tf.subtract(yf,ref),yf) #去重以后的帧级别对齐结果 trigger sequence
>         return tf.squeeze(tf.where(yr>0))
>     pad_len=tf.reduce_max(ylen)-ylen
>     def loop_align(i,zs):
>         k=search_best_path(yt[i][:xlen[i]],xlen[i],ys[i][:ylen[i]],ylen[i],yb[i][:ylen[i]*2+1])
>         zi=tf.concat([k,tf.zeros(pad_len[i],dtype=tf.int64)],axis=0)
>         zs=zs.write(i,zi) 
>         return i+1,zs
>     zs=tf.TensorArray(dtype=tf.int64,size=0,dynamic_size=True)
>     _,zs=tf.while_loop(lambda i,zs:i<tf.shape(yt)[0],loop_align,[0,zs])
>     zs=tf.cast(zs.stack(),dtype=tf.int32) #triggered sequence
>     return zs

zs=viterbi_search(xs,xlen,ys,ylen)`

**It gives the following error:**
  File ""/media/huaxin/tcl1/asr/yujiangling/Stream_Transformer/model.py"", line 135, in search_best_path
    yl,_=tf.contrib.crf.crf_decode(potentials=yt,transition_params=trans,sequence_length=xlen)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py"", line 589, in crf_decode
    false_fn=_multi_seq_fn)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/layers/utils.py"", line 202, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py"", line 59, in smart_cond
    name=name)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2097, in cond
    orig_res_f, res_f = context_f.BuildCondBranch(false_fn)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1930, in BuildCondBranch
    original_result = fn()
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/contrib/crf/python/ops/crf.py"", line 560, in _multi_seq_fn
    dtype=dtypes.int32)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 664, in dynamic_rnn
    dtype=dtype)
  File ""/media/huaxin/tcl1/asr/yujiangling/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 738, in _dynamic_rnn_loop
    ""Input size (depth of inputs) must be accessible via shape inference,""
ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.

It seems that when the tensor was send to the TensorFlow loop (tf.while_loop), the dimension of the tensor become None? Then the function tf.contrib.crf.crf_decode can not work? So, how can I solve it?"
42670,Could not find TensorFlow Java 2.3.0 release in maven central,"
**System information**
Not relevant


**Describe the problem**
```xml
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>2.3.0</version>
</dependency>
```
This dependency cannot resolve.
"
42669,test,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
42664,Slow training speed of  tensorflow.keras.layers.SeparableConv2D and tensorflow.keras.layers.DepthwiseConv2D,"I am trying to train a Fully Convolutional Model with fast inference
So instead of **tensorflow.keras.layers.Conv2d**, I tried using **tensorflow.keras.layers.SeparableConv2D**.

The SeparableConv2D model, when benchmarked using [TensorFlow.js Model Benchmark](https://tensorflow.github.io/tfjs/e2e/benchmarks/local-benchmark/index.html), performs better than Conv2D model.

But while training on GPU, the SeparableConv2D is extremely slow when compared to the Conv2D model.

**System information**
- Google Colab
- TensorFlow version 2.3.0 :
- Python version: 3.6.9

I have created a simple experiment to illustrate the issue. [Colab Notebook](https://colab.research.google.com/drive/1gZTZcj6VJfMEJS_5e2Lc3kGJKdbgnER5?usp=sharing)

The difference in the training time is very large. I also tried DepthwiseConv2D followed by pointwise Conv2d (kernel size 1)

It has come to my knowledge that slow speed of DepthwiseConv2D is a known issue. I was wondering if there is any fix or work around."
42662,Problems with custom learning rate schedules and it's not clear why,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.3.0
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem

I referred to [this example](https://www.tensorflow.org/tutorials/text/transformer) and especially the callback presented here. So, I decided to write out a callback inspired by [this one](https://github.com/facebookresearch/swav/blob/master/main_swav.py#L175). Basically it combines warm-ups and cosine decays. 

Here's how I coded it up - 

```python
class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):
    def __init__(self, base_lr=0.1, end_lr=0.001, warmup_steps=390*5):
        super(CustomSchedule, self).__init__()

        self.base_lr = base_lr
        self.end_lr = end_lr
        self.warmup_steps = warmup_steps
    
    def __call__(self, step=390*35):
        warmup_lr_schedule = tf.linspace(0., self.base_lr, self.warmup_steps)
        iters = tf.range(step, dtype=tf.float32) 
        cosine_lr_schedule = tf.convert_to_tensor([self.end_lr + 0.5 * (self.base_lr - self.end_lr) * (1 + \
                        tf.math.cos(tf.constant(math.pi) * t / (step))) for t in iters])
        lr_schedule = tf.concat([warmup_lr_schedule, cosine_lr_schedule], axis=0)
        
        return lr_schedule
```

I verified if this is the one I wanted and indeed it is - 

![image](https://user-images.githubusercontent.com/22957388/91200950-761e8180-e71d-11ea-93aa-abee10df7f2b.png)

But when I pass this callback inside an optimizer I run into weird stuff - 

```
OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.
```

What am I missing out on?

### Source code / logs

[Colab Notebook](https://colab.research.google.com/gist/sayakpaul/0f75d0177cf6824f80fcc62cd49dc78f/scratchpad.ipynb)
"
42661,Out of memory situation with tf keras Model.fit using tcmalloc library,"<em>
This is a bug related to memory management
</em>

**System information**
- I have created my own code for tf.Keras model building and compiling and run model.fit train and eval:
- Red Hat 7.8 (Maipo):
- Not a mobile device:
- From binary:
- v2.0.0-rc2-26-g64c3d38 2.0.0:
- 3.6.9:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- Cuda 10.0/cuDNN 7.X:
- Tesla P-100, 16 GB:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**
Currently the model.fit training and eval aborts due to out of memory error.  I found from stackoverflow that this error is due to the inability of malloc to efficiently collect garbage.  As suggested [here](https://github.com/tensorflow/tensorflow/issues/2942), I used tcmalloc through LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 before running the tensorflow training script.  The problem is alleviated but not eliminated completely.  Compared to just using gcc malloc, the tcmalloc allows for training a few more epochs, but ultimately gets into an out of memory situation.  The CPU memory that I have available is ~377 GB.  As the training proceeds over epochs, the memory usage increases until the limit and the whole program aborts.  Also, the speed of training decreases with every epoch
**

**
I expect the out of memory issue to never occur.  Also, I expect the speed of training to be consistent across the epochs.
**

**
`
if __name__=='__main__':
    tf.keras.backend.clear_session()
    tf.config.optimizer.set_jit(True)
    from tf_tools import read_h5
    from glob import glob
    import os
    from Feature_compression import build_model
    train_data_folder = os.path.join(os.environ['FAST_DRIVE'],'DATA','DSIFT','train','Visible','imagery')
    test_data_folder = os.path.join(os.environ['FAST_DRIVE'],'DATA','DSIFT','test','Visible','imagery')
    vis_model_folder = os.path.join(os.environ['FAST_DRIVE'],'MODEL','Visible2')
    with h5py.File(os.path.join(train_data_folder,'All_train_visible_data.h5'),'r') as f:
        num_samples = len(f['DSIFT_image'])*(len(f['DSIFT_image'])-1)//2
    strategy = tf.distribute.MirroredStrategy()
    BATCH_SIZE_PER_REPLICA =512
    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
    EPOCHS =15 
    STEPS_PER_EPOCH = 5000 #int(num_samples/EPOCHS) 
    model_folder = os.path.join(os.environ['FAST_DRIVE'],'MODEL','Visible_2_Visible')
    os.makedirs(model_folder,exist_ok=True)
    csv_logger = CSVLogger(os.path.join(model_folder,'DSIFT_training.log'),append=True)
    checkpoint_dir = os.path.join(model_folder,'training_checkpoints')
    checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt_{epoch}"")
    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=3,min_delta=1E-5,baseline=0.999)
    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True),csv_logger]#,early_stop]
#====================== Preparing the dataset optimizaion options =============================================================================#
    dataset_options = tf.data.Options()
    dataset_options.experimental_threading.private_threadpool_size = 60#mp.cpu_count()
    dataset_options.experimental_threading.max_intra_op_parallelism = 1 #mp.cpu_count()
    dataset_options.experimental_optimization.apply_default_optimizations = True
    dataset_options.experimental_optimization.map_vectorization.enabled = True
#====================== Preparing the training dataset =============================================================================#
    train = read_h5(h5py.File(os.path.join(train_data_folder,'All_train_visible_data.h5'),'r'),dataset=['DSIFT_image','labels'],label_str='labels')

    train_input_pds=tf.data.Dataset.from_generator(train.gen_func_pairwise(cls='positive'),output_types=(tf.uint8,tf.float32),
                     output_shapes=(tf.TensorShape((2,24,24,64)),tf.TensorShape((2,))))
    
    train_input_nds=tf.data.Dataset.from_generator(train.gen_func_pairwise(cls='negative'),output_types=(tf.uint8,tf.float32),
                     output_shapes=(tf.TensorShape((2,24,24,64)),tf.TensorShape((2,))))

    train_balanced_ds = tf.data.experimental.sample_from_datasets([train_input_pds.repeat(),train_input_nds.repeat()], [0.5,0.5]).batch(BATCH_SIZE)
    train_dataset = train_balanced_ds.map(lambda features,labels: (tf.concat(tf.unstack(features,axis=1),axis=0)/255,
                                        tf.cast(tf.equal(*tf.unstack(labels,axis=1)), dtype=tf.float32)),
                                        num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)
    train_dataset.with_options(dataset_options)
#==================================================================================================================================#
#====================== Preparing the testing dataset =============================================================================#
    test = read_h5(h5py.File(os.path.join(test_data_folder,'All_test_visible_data.h5'),'r'),dataset=['DSIFT_image','labels'],label_str='labels')

    test_input_pds=tf.data.Dataset.from_generator(test.gen_func_pairwise(cls='positive'),output_types=(tf.uint8,tf.float32),
                     output_shapes=(tf.TensorShape((2,24,24,64)),tf.TensorShape((2,))))
    
    test_input_nds=tf.data.Dataset.from_generator(test.gen_func_pairwise(cls='negative'),output_types=(tf.uint8,tf.float32),
                     output_shapes=(tf.TensorShape((2,24,24,64)),tf.TensorShape((2,))))

    test_balanced_ds = tf.data.experimental.sample_from_datasets([test_input_pds.repeat(),test_input_nds.repeat()], [0.5,0.5]).batch(BATCH_SIZE) 
    test_dataset = test_balanced_ds.map(lambda features,labels: (tf.concat(tf.unstack(features,axis=1),axis=0)/255,tf.cast(tf.equal(*tf.unstack(labels,axis=1)), dtype=tf.float32)),num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)

    test_dataset.with_options(dataset_options)
#==================================================================================================================================#
    with strategy.scope():
        CNN_model = build_model(num_classes=473)
        df = pd.read_csv(os.path.join(vis_model_folder,'DSIFT_Training_History.csv'),delimiter=',')
        CNN_model.load_weights(os.path.join(vis_model_folder,'training_checkpoints',f'ckpt_{1+df[""val_accuracy""].argmax()}'))
        CNN_base_model = tf.keras.Sequential(CNN_model.layers[:-1])
        dnn_model = build_vis2vis_model(layer_shape=[2,2])
        CNN_base_model.trainable = False
        inputs = tf.keras.Input(shape=CNN_base_model.input_shape[1:])
        x = CNN_base_model(inputs,training=False)
        x = tf.abs(tf.keras.layers.Subtract()(tf.split(x,2,axis=0)))
        outputs = dnn_model(x)
        model = tf.keras.Model(inputs,outputs)
        model.compile(loss=tf.keras.losses.hinge,
                      optimizer=tf.keras.optimizers.Adam(), metrics =['accuracy'])
    history = model.fit(x=train_dataset, epochs=EPOCHS,validation_data=test_dataset,
                        steps_per_epoch=STEPS_PER_EPOCH,validation_steps=STEPS_PER_EPOCH,
                        callbacks=callbacks)
    pd.DataFrame(history.history).to_csv(os.path.join(model_folder,""DSIFT_Training_History.csv""),index=False)
`
**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**
Train for 5000 steps, validate for 5000 steps
Epoch 1/15
5000/5000 [==============================] - 5142s 1s/step - loss: 3.5599e-04 - accuracy: 0.9999 - val_loss: 0.1393 - val_accuracy: 0.9757
Epoch 2/15
5000/5000 [==============================] - 5206s 1s/step - loss: 0.0346 - accuracy: 0.9971 - val_loss: 8.6923e-06 - val_accuracy: 1.0000
Epoch 3/15
5000/5000 [==============================] - 5217s 1s/step - loss: 3.7841e-04 - accuracy: 1.0000 - val_loss: 3.2933e-07 - val_accuracy: 1.0000
Epoch 4/15
5000/5000 [==============================] - 5264s 1s/step - loss: 7.6284e-04 - accuracy: 0.9999 - val_loss: 3.1577e-06 - val_accuracy: 1.0000
Epoch 5/15
5000/5000 [==============================] - 5335s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.0950e-06 - val_accuracy: 1.0000
Epoch 6/15
5000/5000 [==============================] - 5223s 1s/step - loss: 5.6489e-04 - accuracy: 0.9999 - val_loss: 1.1722e-05 - val_accuracy: 1.0000
Epoch 7/15
5000/5000 [==============================] - 5353s 1s/step - loss: 1.2962e-05 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000
Epoch 8/15
5000/5000 [==============================] - 5390s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000
Epoch 9/15
5000/5000 [==============================] - 5487s 1s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000
Epoch 10/15
5000/5000 [==============================] - 5761s 1s/step - loss: 3.3989e-04 - accuracy: 0.9999 - val_loss: 1.2558e-08 - val_accuracy: 1.0000
Epoch 11/15
4999/5000 [============================>.] - ETA: 0s - loss: 1.1383e-08 - accuracy: 1.0000/var/spool/pbs/mom_priv/jobs/6135009.pbsserver.SC: line 26:  5302 Killed                  singularity run --nv -B ${FAST_DRIVE}:/p/work3/srini ./tensorflow_2_0_0-gpu_tcmalloc.img python /p/work3/srini/python_scripts/Vis_to_Vis_mapping.py
Start Epilogue v2.5.3 Tue Aug 25 14:41:53 UTC 2020 
Memory usage reported in GB
                    % of     user     user     user    total    total
Node               limit      max    limit  current  current     phys
gaffney-g05       100.00   365.00   365.00     0.02     5.57   377.33
Memory summary:
		min  365.00 GB
		max  365.00 GB
		ave  365.00 GB
Out of Memory condition reached, job memory usage should be reevaluated.
** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42660,from_dlpack unable to process arrays with column-major strides,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Ubuntu 18.04
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.3.0
- Python version:
3.6.9/3.7
- CUDA/cuDNN version:
10.1/7
- GPU model and memory:
K80/V100-32GB


**Describe the current behavior**
Trying to pass dlpack capsules to TensorFlow that come from arrays with column-major strides, like those leveraged by CuDF, raises an `InvalidArgumentError`. It's possible to work around this by transposing the array first, passing it to TensorFlow via dlpack, then transposing back, but obviously this is less than ideal.

**Describe the expected behavior**
These arrays should be read properly without needing to transpose first.

**Standalone code to reproduce the issue**
```
import tensorflow as tf, cupy as cp

# initialize tf
x = tf.random.uniform((1,))

tf.experimental.dlpack.from_dlpack(cp.asfortranarray(cp.ones((5, 2))).toDlpack())
```

Output should be
```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-4-cb770ae77ab3> in <module>()
      1 import tensorflow as tf, cupy as cp
      2 x = tf.random.uniform((5,))
----> 3 tf.experimental.dlpack.from_dlpack(cp.asfortranarray(cp.ones((5, 2))).toDlpack())

/usr/local/lib/python3.6/dist-packages/tensorflow/python/dlpack/dlpack.py in from_dlpack(dlcapsule)
     64     A Tensorflow eager tensor
     65   """"""
---> 66   return pywrap_tfe.TFE_FromDlpackCapsule(dlcapsule, context.context()._handle)

InvalidArgumentError: Invalid strides array from DLPack
```
See notebook [here](https://colab.research.google.com/drive/1ALUBTckq2ycXwQDIBNUzifYWJl9UJLN1?usp=sharing)

---------------------------
@miguelusque for viz
"
42659,How to print Quantization Aware Training Model Int8 weights without TFLite,"Hi, I I've been trying to [Quantization aware training in Keras example](https://www.tensorflow.org/model_optimization/guide/quantization/training_example)

I thought QAT Model weight data type is FP32, but it's real value represents fixed-point number.
I want to know how to print fixed-point number of weights without converting TFLite.

Thank you."
42658,Data shape mismatch in custom training with 'train_step'  and `__get_item__`,"- TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""` : `unknown 2.2.0`

## Problem description 
I am developing a custom model training by overriding `train_step` inherited from `tf.keras.Model` and `__getitem__` of my data generator inherited from `tf.keras.utils.Sequence`. The `__get_item__` method returns the following data:

`return [train_data, observed_data]`
where `train_data` and `observed_data` are numpy arrays with shape (20, 92)

However, when I receive them in `train_step` function, the shapes of these variables are not correct: 
In `train_step` method, I extract the data as follows: 
`train_data, observed_data = data[0]` 

and here is what I get: 

- `train_data Tensor(""IteratorGetNext:0"", shape=(None, None), dtype=float32)`
- `observed_data Tensor(""IteratorGetNext:1"", shape=(None, None), dtype=float32)`

## The expected behavior
I am not sure if this is a bug or not, but the expected shape of these tensors should be as good as what is specified in `__get_item__` method."
42657,Tensorflow go installation fail (cannot find import),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version: 3.8
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 3.1
- GCC/Compiler version (if compiling from source):9.3
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
I have installed C , following the instructions from https://www.tensorflow.org/install/lang_c and when i ran:                       go get github.com/tensorflow/tensorflow/tensorflow/go
I get the following error:
../github.com/tensorflow/tensorflow/tensorflow/go/saved_model.go:25:2: cannot find package ""github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto"" in any of:
                     /usr/local/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOROOT)
	            /home/nyein/go/src/github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf/for_core_protos_go_proto (from $GOPATH)

I tried following this fix here at https://github.com/tensorflow/tensorflow/issues/14546#issuecomment-347433966 and I checkout from r1.11 and I am getting this error now:
/usr/bin/ld: $WORK/b032/_x007.o: in function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListCount':
/tmp/go-build/cgo-gcc-prolog:115: undefined reference to `TF_DeviceListCount'
/usr/bin/ld: $WORK/b032/_x007.o: in function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListMemoryBytes':
/tmp/go-build/cgo-gcc-prolog:136: undefined reference to `TF_DeviceListMemoryBytes'
/usr/bin/ld: $WORK/b032/_x007.o: in function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListName':
/tmp/go-build/cgo-gcc-prolog:157: undefined reference to `TF_DeviceListName'
/usr/bin/ld: $WORK/b032/_x007.o: in function `_cgo_d969e426e2e3_Cfunc_TF_DeviceListType':
/tmp/go-build/cgo-gcc-prolog:178: undefined reference to `TF_DeviceListType'
/usr/bin/ld: $WORK/b032/_x007.o: in function `_cgo_d969e426e2e3_Cfunc_TF_SessionListDevices':
/tmp/go-build/cgo-gcc-prolog:234: undefined reference to `TF_SessionListDevices'
/usr/bin/ld: $WORK/b032/_x007.o: in function `_cgo_d969e426e2e3_Cfunc_TF_DeleteDeviceList':
/tmp/go-build/cgo-gcc-prolog:62: undefined reference to `TF_DeleteDeviceList'
collect2: error: ld returned 1 exit status

Can someone point me to right right direction on fixing this problem?
"
42655,Cannot see per operator profiling in android studio with tensorflow lite models,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:

https://www.tensorflow.org/lite/performance/measurement#adding_trace_events_in_java_code

https://www.tensorflow.org/lite/performance/images/as_traces.png

## Description of issue (what needs changing):

I've added the trace sections to the code around the ``` interpreter.run() ``` command. When I start to profile the app I cannot see 
the per operator profiling as we can see in the second link.

### Clear description

This will help us in seeing how much time is being taken by each layer in the tflite model and we can change our models
according to that.

### My Code

``` 
Trace.beginSection(""runInference"");
 tflite_sr.run(ybuffer,sr_y_channel);
 Trace.endSection(); 
```"
42654,TFLite C++ API docs seems broken,"## URL(s) with the issue:

https://www.tensorflow.org/lite/api_docs/cc

## Description of issue (what needs changing):

The C++ API references for TFLite are missing many important components on tensorflow.org. For example, the docs for `tflite::FlatBufferModel` and `tflite::InterpreterBuilder` are completely gone, which used to be there the last time I checked, about two months ago.
"
42650,Add optimized SVDF kernel from CMSIS-NN,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA
- TensorFlow installed from (source or binary): NA
- Tensorflow version (commit SHA if source): NA
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): ARM Cortex-M

An optimized version of SVDF will soon be present in CMSIS-NN, and the ""glue"" code for it should be added to TensorFlow Lite Micro."
42649,[tflite] Conv2DTranspose output difference between mobile CPU and mobile GPU ,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution: Windows 10, Android
- Mobile device: Galaxy Fold, Pixel 3
- TensorFlow installed from binary
- TensorFlow version: v2.3.0-rc2-23-gb36436b087 2.3.0
- TensorFlow Lite version  ['org.tensorflow:tensorflow-lite:0.0.0-nightly'](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-39e7d8c00954e920b98e7636f0ac30b2R46) and 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: - 
- GPU model and memory: Adreno 640

**Describe the current behavior**
In a specific situation, `tf.keras.layers.Conv2DTranspose` [executed on mobile CPU and on mobile GPU](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-31e60fe4136762a27be59770ef684d01R71) returns different outputs -the mean difference between the output from CPU and output from GPU is  ~0.1 - 0.7 but the difference should be ~1e-07.
([my metric](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-31e60fe4136762a27be59770ef684d01R117) - mean difference => abs(cpu_output - gpu_output) => and it shoould be close to zero)

Specific situation:
* tf.keras.layers.Conv2DTranspose
* Input shape has odd width (NWHC) e.x. (1, 9, 6, 45)
* padding='same', kernel_size=(5, 3), strides=(2, 1),

[model](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-98d852ec33b2563b33b6c75585566a06R5)

[incorrect model - model_01](https://github.com/koodzi/tflite-comparing-cpu-gpu-out/commit/e9b92114cd412c3d9fbee0f4a5adf8db3e405ca6#diff-98d852ec33b2563b33b6c75585566a06R52)

**Describe the expected behavior**
The difference between the two outputs should be close to zero.

**Standalone code to reproduce the issue**
[https://github.com/koodzi/tflite-comparing-cpu-gpu-out](https://github.com/koodzi/tflite-comparing-cpu-gpu-out)

**Other info/logs** Include any logs or source code that would be helpful to
![diff](https://user-images.githubusercontent.com/18346395/91163679-42683b00-e6ce-11ea-8d6f-bb3128fe3a00.png)
"
42648,Calculation of effective scale differs from TFLite implementation in quantize kernel,"@tensorflow/micro

**Describe the problem**

In the quantize kernel the calculation of the effective scale differs slightly between TFLite and TFLu. We found that in some cases this results in single bit differences in output. Is this difference in implementation intentional?

https://github.com/tensorflow/tensorflow/blob/de1a269b21cbad035faa095d8ce88fc2d680cf4a/tensorflow/lite/kernels/quantize.cc#L129-L133

https://github.com/tensorflow/tensorflow/blob/de1a269b21cbad035faa095d8ce88fc2d680cf4a/tensorflow/lite/micro/kernels/quantize.cc#L75-L79
"
42647,whether is it better to add class_weight to validation dataset or not?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.x
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

## set `class_weight` to validation dataset

It is common for us to set `class_weight` to something like inverse of class supports  to `Model.fit()` when dealing with imbalance classification problems. And as the code implemented in `tensorflow.python.keras.engine.data_adapter._make_class_weight_map_fn`, `class_weight` will eventually be converted to `sample_weight`.

However, the current implementation of `Model.fit()` only add `class_weight` to the training dataset but not to the validation dataset

```python
# for training dataset
data_handler = data_adapter.DataHandler(
  x=x,
  y=y,
  sample_weight=sample_weight,
  batch_size=batch_size,
  steps_per_epoch=steps_per_epoch,
  initial_epoch=initial_epoch,
  epochs=epochs,
  shuffle=shuffle,
  class_weight=class_weight,
  max_queue_size=max_queue_size,
  workers=workers,
  use_multiprocessing=use_multiprocessing,
  model=self,
  steps_per_execution=self._steps_per_execution)

# for validation dataset
self._eval_data_handler = data_adapter.DataHandler(
    x=val_x,
    y=val_y,
    sample_weight=val_sample_weight,
    batch_size=validation_batch_size or batch_size,
    steps_per_epoch=validation_steps,
    initial_epoch=0,
    epochs=1,
    max_queue_size=max_queue_size,
    workers=workers,
    use_multiprocessing=use_multiprocessing,
    model=self,
    steps_per_execution=self._steps_per_execution)
```


which will output training loss and validation loss with (maybe) large gap, such as:

```
Epoch 2/10000
128/128 [==============================] - 63s 489ms/step - loss: 379.6439 - acc: 0.0073 - acc_f1: 0.0077 - cm: 10485.7598 - val_loss: 4.9299 - val_acc: 0.0076 - val_acc_f1: 0.0081 - val_cm: 2621.4399
128/128 [==============================] - 63s 492ms/step - loss: 374.9651 - acc: 0.0078 - acc_f1: 0.0099 - cm: 10485.7598 - val_loss: 4.8990 - val_acc: 0.0078 - val_acc_f1: 0.0120 - val_cm: 2621.4399
```

**I am wondering whether it is a good practice to add the same `class_weight` parameter to the training dataset and the validation dataset, in order to keep training loss and validation loss with the same scale**.


## normalized loss by the sum of `sample_weight`

And by the way, while reducing the loss by the size of a batch, the current loss is normalized by the size of `weighted_loss` (ref. to `tensorflow.python.keras.utils.losses_utils.reduce_weighted_loss`),  **whether is it better to normalize the loss by the sum of `sample_weight`**?



**Will this change the current api? How?**
Almost no changes.

For adding `class_weight` to validation dataset, only add `class_weight=class_weight` when calling `data_adapter.DataHandler`

```python
self._eval_data_handler = data_adapter.DataHandler(
    x=val_x,
    y=val_y,
    sample_weight=val_sample_weight,
    batch_size=validation_batch_size or batch_size,
    steps_per_epoch=validation_steps,
    initial_epoch=0,
    epochs=1,
    shuffle=shuffle,
    class_weight=class_weight,
    max_queue_size=max_queue_size,
    workers=workers,
    use_multiprocessing=use_multiprocessing,
    model=self,
    steps_per_execution=self._steps_per_execution)
```


For normalizing loss with the sum of `sample_weight`,  only changes the code snippet of `python.keras.utils.losses_utils.compute_weighted_loss`

from 

```python
    ....
    # Apply reduction function to the individual weighted losses.
    loss = reduce_weighted_loss(weighted_losses, reduction)
    # Convert the result back to the input type.
    loss = math_ops.cast(loss, input_dtype)
    return loss
```
to 

```python
   
    if sample_weight is None:
      sample_weight = array_ops.ones_like(losses, dtype=losses.dtype)
   ...
    # Apply reduction function to the individual weighted losses
    if reduction == ReductionV2.NONE:
      loss = weighted_losses
    else:
      loss = math_ops.reduce_sum(weighted_losses)
      if reduction == ReductionV2.SUM_OVER_BATCH_SIZE:
        loss = _safe_mean(loss, math_ops.reduce_sum(sample_weight))
    
    # Convert the result back to the input type.
    loss = math_ops.cast(loss, input_dtype)
    return loss
```


**Who will benefit with this feature?**
everyone if it is a good practice.

**Any Other info.**
"
42645,ImportError: Traceback (most recent call last) while importing the tensorflow,"**import tensorflow as**
I am getting following error error. Importing at jupyter notebook. Python version is 3.8.3.

---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     63   try:
---> 64     from tensorflow.python._pywrap_tensorflow_internal import *
     65   # This try catch logic is because there is no bazel equivalent for py_extension.

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\anaconda3\lib\site-packages\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>
     38 # pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top
     39 
---> 40 from tensorflow.python.eager import context
     41 
     42 # pylint: enable=wildcard-import

~\anaconda3\lib\site-packages\tensorflow\python\eager\context.py in <module>
     33 from tensorflow.core.protobuf import config_pb2
     34 from tensorflow.core.protobuf import rewriter_config_pb2
---> 35 from tensorflow.python import pywrap_tfe
     36 from tensorflow.python import tf2
     37 from tensorflow.python.client import pywrap_tf_session

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py in <module>
     26 
     27 # pylint: disable=invalid-import-order,g-bad-import-order, wildcard-import, unused-import
---> 28 from tensorflow.python import pywrap_tensorflow
     29 from tensorflow.python._pywrap_tfe import *

~\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     81 for some common reasons and solutions.  Include the entire stack trace
     82 above this error message when asking for help."""""" % traceback.format_exc()
---> 83   raise ImportError(msg)
     84 
     85 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\ssahu\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
42644,what  updates have been made in TF2.0 vs TF1.0 for c++ layer  such as  communication?,"Recently  I  have been doing my model training  based on TF2.2(**without  eager**) and  TF1.10。The result shows the TF2.2 have better performance than  TF1.10,  such as the training speed。
so，I wanna know  what updates have been made in TF2.2(without eager) to make  my model train faster，such  as  communication in c++ layer  or  GPU driver?  
Hope to hear from you soon! Thanks!"
42643,TFLite Int8 Quantization Conversion - There are unresolved custom ops: [],"**Error**
```
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: []Encountered unresolved custom op: RandomStandardNormal.Node number 0 (RandomStandardNormal) failed to prepare.
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 2.3.0


**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
import os, glob
os.environ[""CUDA_VISIBLE_DEVICES""]=""-1""
import tensorflow.compat.v1 as tf

from tensorflow.python.client import device_lib
import numpy as np

import faulthandler
faulthandler.enable()

import pdb

path = os.path.dirname(os.path.abspath(__file__))
pb_model_name = ""model.pb""
pb_model_path = os.path.join(path, pb_model_name)

model_name = 'model_int8.tflite'

if __name__ == '__main__':
  with tf.device(""/cpu:0""):
    configuration = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)
    with tf.Session(config=configuration) as sess:
      converter = tf.lite.TFLiteConverter.from_frozen_graph(
        graph_def_file=pb_model_path,
        # input_arrays=[""device_0/wav_and_noisy:1""],
        input_arrays=[""device_0/wav_and_noisy:1""],
        output_arrays=[""device_0/g_ae_1/Tanh""],
        input_shapes={""device_0/wav_and_noisy:1"": [100, 16384]}
      )
      converter.allow_custom_ops = True
      # converter.experimental_new_converter = True
      converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                                             tf.lite.OpsSet.SELECT_TF_OPS]
      # converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
      #                                        tf.lite.OpsSet.TFLITE_BUILTINS,
      #                                        tf.lite.OpsSet.SELECT_TF_OPS]
      converter.optimizations = [tf.lite.Optimize.DEFAULT]
      # converter.target_spec.supported_types = [tf.int8]
      converter.inference_input_type = tf.int8  # or tf.uint8
      converter.inference_output_type = tf.int8  # or tf.uint8

      def test():
        pdb.set_trace()
        zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
        dataset = tf.data.Dataset.from_tensor_slices(zeros).batch(1)
        yield [zeros]
      converter.representative_dataset = test

      # pdb.set_trace()
      tflite_model = converter.convert()

      tflite_model_size = open(model_name, 'wb').write(tflite_model)
      print('TFLite Model is %d bytes' % tflite_model_size)
```

I've tried also including the `TFLITE_BUILTINS` in addition to `TFLITE_BUILTINS_INT8`. In other issues on github, I've seen that adding `SELECT_TF_OPS` solves errors similar to mine, but it did not fix it for me. 

**The output from the converter invocation**

```
2020-08-25 02:22:18.873117: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-08-25 02:22:18.873149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From convert_model_to_tflite_int8.py:24: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.run_functions_eagerly` instead of the experimental version.
2020-08-25 02:22:19.959195: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-08-25 02:22:19.959229: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-08-25 02:22:19.959253: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-5-134): /proc/driver/nvidia/version does not exist
2020-08-25 02:22:19.959493: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-25 02:22:19.984669: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2999995000 Hz
2020-08-25 02:22:19.984899: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d64580 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-25 02:22:19.984922: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-25 02:22:19.987088: I tensorflow/core/common_runtime/direct_session.cc:360] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device

2020-08-25 02:22:22.412284: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-08-25 02:22:22.412445: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-08-25 02:22:23.196542: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-08-25 02:22:23.196595: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.005ms.
2020-08-25 02:22:23.196614: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-08-25 02:22:24.510075: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-08-25 02:22:24.510118: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
Traceback (most recent call last):
  File ""convert_model_to_tflite_int8.py"", line 102, in <module>
    tflite_model = converter.convert()
  File ""../python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1970, in convert
    return super(TFLiteConverter, self).convert()
  File ""../python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1339, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File ""../python3.6/site-packages/tensorflow/lite/python/lite.py"", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File ""../python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 91, in calibrate_and_quantize
    self._calibrator.Prepare([list(s.shape) for s in sample])
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: []Encountered unresolved custom op: RandomStandardNormal.Node number 0 (RandomStandardNormal) failed to prepare.
```

**Also, please include a link to the saved model or GraphDef**
Unfortunately cannot provide publicly at this time. I can share privately if needed.

**Failure details**
The conversion hits a RuntimeError and stops. From PDB (below) I know it is hitting my `representative_dataset` function twice before crashing.

**Any other info / logs**

With a break point inside my `representative_dataset` function:
```
2020-08-25 02:29:39.675277: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory
2020-08-25 02:29:39.675311: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
WARNING:tensorflow:From /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py:18: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.config.run_functions_eagerly` instead of the experimental version.
2020-08-25 02:29:40.563260: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2020-08-25 02:29:40.563287: W tensorflow/stream_executor/cuda/cuda_driver.cc:312] failed call to cuInit: UNKNOWN ERROR (303)
2020-08-25 02:29:40.563307: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-5-134): /proc/driver/nvidia/version does not exist
2020-08-25 02:29:40.563535: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-25 02:29:40.588667: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2999995000 Hz
2020-08-25 02:29:40.588892: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c1a8b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-25 02:29:40.588920: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-25 02:29:40.591168: I tensorflow/core/common_runtime/direct_session.cc:360] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device

2020-08-25 02:29:42.998639: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0
2020-08-25 02:29:42.998794: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-08-25 02:29:43.773981: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-08-25 02:29:43.774020: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.006ms.
2020-08-25 02:29:43.774031: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-08-25 02:29:45.082195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-08-25 02:29:45.082241: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
> /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(93)test()
-> zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
(Pdb) n
> /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(94)test()
-> dataset = tf.data.Dataset.from_tensor_slices(zeros).batch(1)
(Pdb)
> /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(97)test()
-> yield [zeros]
(Pdb)
GeneratorExit
> /home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py(97)test()
-> yield [zeros]
(Pdb)
Traceback (most recent call last):
  File ""/usr/lib/python3.6/pdb.py"", line 1667, in main
    pdb._runscript(mainpyfile)
  File ""/usr/lib/python3.6/pdb.py"", line 1548, in _runscript
    self.run(statement)
  File ""/usr/lib/python3.6/bdb.py"", line 434, in run
    exec(cmd, globals, locals)
  File ""<string>"", line 1, in <module>
  File ""/home/ubuntu/denoise-gst/denoise_deploy/inference_module/convert_model_to_tflite_int8.py"", line 18, in <module>
    '''
  File "".../python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1970, in convert
    return super(TFLiteConverter, self).convert()
  File "".../python3.6/site-packages/tensorflow/lite/python/lite.py"", line 1339, in convert
    result = self._calibrate_quantize_model(result, **flags)
  File "".../python3.6/site-packages/tensorflow/lite/python/lite.py"", line 452, in _calibrate_quantize_model
    inference_output_type, allow_float, activations_type)
  File "".../python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py"", line 91, in calibrate_and_quantize
    self._calibrator.Prepare([list(s.shape) for s in sample])
RuntimeError: Failed to initialize op resolver for calibration:
There are unresolved custom ops: []Encountered unresolved custom op: RandomStandardNormal.Node number 0 (RandomStandardNormal) failed to prepare.

Uncaught exception. Entering post mortem debugging
Running 'cont' or 'step' will restart the program
> .../python3.6/site-packages/tensorflow/lite/python/optimize/calibrator.py(91)calibrate_and_quantize()
-> self._calibrator.Prepare([list(s.shape) for s in sample])
```
"
42641,TypeError: can't pickle _thread.RLock objects: SKlearn/Tensorflow/Third Party content interference?,"For Neural-Network regression prediction task cross_val_predict from SKlearn throws the error (full error further below, below the model used)

```
TypeError: can't pickle _thread.RLock objects
```

when utilizing it with input data of form 

```
X_train.shape=1200,18,15 
y_train.shape=1200,18,1 
```

and the following in NN

```
def twds_model(layer1=32, layer2=32, layer3=16, dropout_rate=0.5, optimizer='Adam'
                    , learning_rate=0.001, activation='relu', loss='mse'):#, n_jobs=1):layer3=80, 
    
    model = Sequential()
    model.add(Bidirectional(GRU(layer1, return_sequences=True),input_shape=(X_train.shape[1],X_train.shape[2])))
    model.add(AveragePooling1D(2))
    model.add(Conv1D(layer2, 3, activation=activation, padding='same', 
               name='extractor'))
    model.add(Flatten())
    model.add(Dense(layer3,activation=activation))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1))
    model.compile(optimizer=optimizer,loss=loss)
    return model

twds_model=twds_model()
print(twds_model.summary())
```

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bidirectional_4 (Bidirection (None, 18, 64)            9216      
_________________________________________________________________
average_pooling1d_1 (Average (None, 9, 64)             0         
_________________________________________________________________
extractor (Conv1D)           (None, 9, 32)             6176      
_________________________________________________________________
flatten_1 (Flatten)          (None, 288)               0         
_________________________________________________________________
dense_3 (Dense)              (None, 16)                4624      
_________________________________________________________________
dropout_4 (Dropout)          (None, 16)                0         
_________________________________________________________________
dense_4 (Dense)              (None, 1)                 17        
=================================================================
Total params: 20,033
Trainable params: 20,033
Non-trainable params: 0
_________________________________________________________________
None
```

and 

```
model_twds=KerasRegressor(build_fn=twds_model, batch_size=144,epochs=6)#12
```

The error was mentioned at [SKlearn](https://github.com/scikit-learn/scikit-learn/issues/18171), but it was contemplated that the error is likely to be on the Tensorflow site or that it might be caused by third party content - however, I have no idea where this content might be involved.

The complete error: 


```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-603-37b55dfd53fd> in <module>
----> 1 GridLSTM.fit(X_train, y_train)

~\Anaconda3\envs\Tensorflow\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     70                           FutureWarning)
     71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---> 72         return f(**kwargs)
     73     return inner_f
     74 

~\Anaconda3\envs\Tensorflow\lib\site-packages\sklearn\model_selection\_search.py in fit(self, X, y, groups, **fit_params)
    679         n_splits = cv.get_n_splits(X, y, groups)
    680 
--> 681         base_estimator = clone(self.estimator)
    682 
    683         parallel = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,

~\Anaconda3\envs\Tensorflow\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     70                           FutureWarning)
     71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---> 72         return f(**kwargs)
     73     return inner_f
     74 

~\Anaconda3\envs\Tensorflow\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     85     new_object_params = estimator.get_params(deep=False)
     86     for name, param in new_object_params.items():
---> 87         new_object_params[name] = clone(param, safe=False)
     88     new_object = klass(**new_object_params)
     89     params_set = new_object.get_params(deep=False)

~\Anaconda3\envs\Tensorflow\lib\site-packages\sklearn\utils\validation.py in inner_f(*args, **kwargs)
     70                           FutureWarning)
     71         kwargs.update({k: arg for k, arg in zip(sig.parameters, args)})
---> 72         return f(**kwargs)
     73     return inner_f
     74 

~\Anaconda3\envs\Tensorflow\lib\site-packages\sklearn\base.py in clone(estimator, safe)
     69     elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):
     70         if not safe:
---> 71             return copy.deepcopy(estimator)
     72         else:
     73             if isinstance(estimator, type):

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\Anaconda3\envs\Tensorflow\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    278     if state is not None:
    279         if deep:
--> 280             state = deepcopy(state, memo)
    281         if hasattr(y, '__setstate__'):
    282             y.__setstate__(state)

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--> 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_list(x, memo, deepcopy)
    213     append = y.append
    214     for a in x:
--> 215         append(deepcopy(a, memo))
    216     return y
    217 d[list] = _deepcopy_list

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\Anaconda3\envs\Tensorflow\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    278     if state is not None:
    279         if deep:
--> 280             state = deepcopy(state, memo)
    281         if hasattr(y, '__setstate__'):
    282             y.__setstate__(state)

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--> 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_list(x, memo, deepcopy)
    213     append = y.append
    214     for a in x:
--> 215         append(deepcopy(a, memo))
    216     return y
    217 d[list] = _deepcopy_list

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\Anaconda3\envs\Tensorflow\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    278     if state is not None:
    279         if deep:
--> 280             state = deepcopy(state, memo)
    281         if hasattr(y, '__setstate__'):
    282             y.__setstate__(state)

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--> 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\Anaconda3\envs\Tensorflow\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    278     if state is not None:
    279         if deep:
--> 280             state = deepcopy(state, memo)
    281         if hasattr(y, '__setstate__'):
    282             y.__setstate__(state)

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--> 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\Anaconda3\envs\Tensorflow\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    278     if state is not None:
    279         if deep:
--> 280             state = deepcopy(state, memo)
    281         if hasattr(y, '__setstate__'):
    282             y.__setstate__(state)

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--> 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    178                     y = x
    179                 else:
--> 180                     y = _reconstruct(x, memo, *rv)
    181 
    182     # If is its own copy, don't memoize.

~\Anaconda3\envs\Tensorflow\lib\copy.py in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)
    278     if state is not None:
    279         if deep:
--> 280             state = deepcopy(state, memo)
    281         if hasattr(y, '__setstate__'):
    282             y.__setstate__(state)

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    148     copier = _deepcopy_dispatch.get(cls)
    149     if copier:
--> 150         y = copier(x, memo)
    151     else:
    152         try:

~\Anaconda3\envs\Tensorflow\lib\copy.py in _deepcopy_dict(x, memo, deepcopy)
    238     memo[id(x)] = y
    239     for key, value in x.items():
--> 240         y[deepcopy(key, memo)] = deepcopy(value, memo)
    241     return y
    242 d[dict] = _deepcopy_dict

~\Anaconda3\envs\Tensorflow\lib\copy.py in deepcopy(x, memo, _nil)
    167                     reductor = getattr(x, ""__reduce_ex__"", None)
    168                     if reductor:
--> 169                         rv = reductor(4)
    170                     else:
    171                         reductor = getattr(x, ""__reduce__"", None)

TypeError: can't pickle _thread.RLock objects


```
The versions used:

```
Package                  Version
------------------------ ---------------
-                        nsorflow-gpu
-ensorflow-gpu           2.3.0
-rotobuf                 3.11.3
absl-py                  0.9.0
antlr4-python3-runtime   4.8
asn1crypto               1.3.0
astor                    0.7.1
astropy                  3.2.1
astunparse               1.6.3
attrs                    19.3.0
audioread                2.1.8
autopep8                 1.5.3
backcall                 0.1.0
beautifulsoup4           4.9.0
bezier                   0.8.0
bkcharts                 0.2
bleach                   3.1.4
blis                     0.2.4
bokeh                    1.1.0
boto3                    1.9.253
botocore                 1.12.253
Bottleneck               1.3.2
cachetools               4.1.0
certifi                  2020.4.5.1
cffi                     1.14.0
chardet                  3.0.4
click                    6.7
cloudpickle              0.5.3
cmdstanpy                0.4.0
color                    0.1
colorama                 0.4.3
colorcet                 0.9.1
convertdate              2.2.1
copulas                  0.2.5
cryptography             2.8
ctgan                    0.2.1
cycler                   0.10.0
cymem                    2.0.2
Cython                   0.29.17
dash                     0.26.0
dash-core-components     0.27.2
dash-html-components     0.11.0
dash-renderer            0.13.2
dask                     0.18.1
dataclasses              0.6
datashader               0.7.0
datashape                0.5.2
datawig                  0.1.10
deap                     1.3.0
decorator                4.4.2
defusedxml               0.6.0
deltapy                  0.1.1
dill                     0.2.9
distributed              1.22.1
docutils                 0.14
entrypoints              0.3
ephem                    3.7.7.1
et-xmlfile               1.0.1
exrex                    0.10.5
Faker                    4.0.3
fastai                   1.0.60
fastprogress             0.2.2
fbprophet                0.6
fire                     0.3.1
Flask                    1.0.2
Flask-Compress           1.4.0
future                   0.17.1
gast                     0.3.3
geojson                  2.4.1
geomet                   0.2.0.post2
google-auth              1.14.0
google-auth-oauthlib     0.4.1
google-pasta             0.2.0
gplearn                  0.4.1
graphviz                 0.13.2
grpcio                   1.29.0
h5py                     2.10.0
HeapDict                 1.0.0
holidays                 0.10.2
holoviews                1.12.1
html2text                2018.1.9
hyperas                  0.4.1
hyperopt                 0.1.2
idna                     2.6
imageio                  2.5.0
imbalanced-learn         0.3.3
imblearn                 0.0
importlib-metadata       1.5.0
impyute                  0.0.8
ipykernel                5.1.4
ipython                  7.13.0
ipython-genutils         0.2.0
ipywidgets               7.5.1
itsdangerous             0.24
jdcal                    1.4
jedi                     0.16.0
Jinja2                   2.11.1
jmespath                 0.9.5
joblib                   0.13.2
jsonschema               3.2.0
jupyter                  1.0.0
jupyter-client           6.1.2
jupyter-console          6.0.0
jupyter-core             4.6.3
Keras                    2.4.3
Keras-Applications       1.0.8
Keras-Preprocessing      1.1.2
keras-rectified-adam     0.17.0
kiwisolver               1.2.0
korean-lunar-calendar    0.2.1
librosa                  0.7.2
llvmlite                 0.32.1
lml                      0.0.1
locket                   0.2.0
LunarCalendar            0.0.9
Markdown                 2.6.11
MarkupSafe               1.1.1
matplotlib               3.2.1
missingpy                0.2.0
mistune                  0.8.4
mkl-fft                  1.0.15
mkl-random               1.1.0
mkl-service              2.3.0
mock                     4.0.2
msgpack                  0.5.6
multipledispatch         0.6.0
murmurhash               1.0.2
mxnet                    1.4.1
nb-conda                 2.2.1
nb-conda-kernels         2.2.3
nbconvert                5.6.1
nbformat                 5.0.4
nbstripout               0.3.7
networkx                 2.1
notebook                 6.0.3
numba                    0.49.1
numexpr                  2.7.1
numpy                    1.18.5
oauthlib                 3.1.0
olefile                  0.46
opencv-python            4.2.0.34
openpyxl                 2.5.5
opt-einsum               3.2.1
packaging                20.3
pandas                   1.0.3
pandasvault              0.0.3
pandocfilters            1.4.2
param                    1.9.0
parso                    0.6.2
partd                    0.3.8
patsy                    0.5.1
pbr                      5.1.3
pickleshare              0.7.5
Pillow                   7.0.0
pip                      20.2.2
plac                     0.9.6
plotly                   4.7.1
plotly-express           0.4.1
preshed                  2.0.1
prometheus-client        0.7.1
prompt-toolkit           3.0.4
protobuf                 3.11.3
psutil                   5.4.7
py                       1.8.0
pyasn1                   0.4.8
pyasn1-modules           0.2.8
pycodestyle              2.6.0
pycparser                2.20
pyct                     0.4.5
pyensae                  1.3.839
pyexcel                  0.5.8
pyexcel-io               0.5.7
Pygments                 2.6.1
pykalman                 0.9.5
PyMeeus                  0.3.7
pymongo                  3.8.0
pyOpenSSL                19.1.0
pyparsing                2.4.7
pypi                     2.1
pyquickhelper            1.9.3418
pyrsistent               0.16.0
PySocks                  1.7.1
pystan                   2.19.1.1
python-dateutil          2.8.1
pytz                     2019.3
pyviz-comms              0.7.2
PyWavelets               0.5.2
pywin32                  227
pywinpty                 0.5.7
PyYAML                   5.3.1
pyzmq                    18.1.1
qtconsole                4.4.4
rdt                      0.2.1
RegscorePy               1.1
requests                 2.23.0
requests-oauthlib        1.3.0
resampy                  0.2.2
retrying                 1.3.3
rsa                      4.0
s3transfer               0.2.1
scikit-image             0.15.0
scikit-learn             0.23.2
scipy                    1.4.1
sdv                      0.3.2
seaborn                  0.9.0
seasonal                 0.3.1
Send2Trash               1.5.0
sentinelsat              0.12.2
setuptools               46.3.0
setuptools-git           1.2
six                      1.14.0
sklearn                  0.0
sortedcontainers         2.0.4
SoundFile                0.10.3.post1
soupsieve                2.0
spacy                    2.1.8
srsly                    0.1.0
statsmodels              0.9.0
stopit                   1.1.2
sugartensor              1.0.0.2
ta                       0.5.25
tb-nightly               1.14.0a20190603
tblib                    1.3.2
tensorboard              2.3.0
tensorboard-plugin-wit   1.7.0
tensorflow-gpu           2.3.0
tensorflow-gpu-estimator 2.3.0
termcolor                1.1.0
terminado                0.8.3
testpath                 0.4.4
text-unidecode           1.3
texttable                1.4.0
Theano                   1.0.4
thinc                    7.0.8
threadpoolctl            2.1.0
toml                     0.10.1
toolz                    0.10.0
torch                    1.4.0
torchvision              0.5.0
tornado                  6.0.4
TPOT                     0.10.2
tqdm                     4.45.0
traitlets                4.3.3
transforms3d             0.3.1
tsaug                    0.2.1
typeguard                2.7.1
typing                   3.6.6
update-checker           0.16
urllib3                  1.22
utm                      0.4.2
wasabi                   0.2.2
wcwidth                  0.1.9
webencodings             0.5.1
Werkzeug                 1.0.1
wheel                    0.34.2
widgetsnbextension       3.5.1
win-inet-pton            1.1.0
wincertstore             0.2
wrapt                    1.11.2
xarray                   0.10.8
xlrd                     1.1.0
yahoo-historical         0.3.2
zict                     0.1.3
zipp                     2.2.0
```"
42640,Tensorflow lite arm64 cross/native build issue,"Hi,

I am trying to build tensorflow-lite with edgetpu on Coral development board and following the instructions from the link below
https://www.tensorflow.org/lite/guide/build_arm64#compile_natively_on_arm64

I am checked out to d855adfc5a0195788bf5f92c3c7352e638aa1109(https://github.com/google-coral/edgetpu/blob/c48c88871fd3d2e10d298126cd6a08b88d22496c/WORKSPACE#L5 as per the instructions on step 3 of the link - https://coral.ai/docs/edgetpu/tflite-cpp/#build-your-project

I am running into build issues with native as well as cross compilation with this SHA(master builds fine). Below is the log snippet of build failure

```
In file included from /home/mg/workspace/nxpdemo/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/base/internal/periodic_sampler.h:22:0,
                 from tensorflow/lite/tools/make/downloads/absl/absl/base/internal/periodic_sampler.cc:15:
/home/mg/workspace/nxpdemo/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/base/internal/exponential_biased.h:24:1: error: ‘ABSL_NAMESPACE_BEGIN’ does not name a type
 ABSL_NAMESPACE_BEGIN
 ^~~~~~~~~~~~~~~~~~~~
tensorflow/lite/tools/make/downloads/absl/absl/base/internal/exponential_biased.cc:28:1: error: ‘ABSL_NAMESPACE_BEGIN’ does not name a type
 ABSL_NAMESPACE_BEGIN
 ^~~~~~~~~~~~~~~~~~~~
/home/mg/workspace/nxpdemo/tensorflow/tensorflow/lite/tools/make/downloads/absl/absl/base/internal/exponential_biased.h:127:1: error: ‘ABSL_NAMESPACE_END’ does not name a type; did you mean ‘ABSL_BASE_PORT_H_’?
 ABSL_NAMESPACE_END
 ^~~~~~~~~~~~~~~~~~

```
**Steps to build tensorflow-lite**

1. git clone https://github.com/tensorflow/tensorflow and checkout to  d855adfc5a0195788bf5f92c3c7352e638aa1109
2. cd tensorflow 
3. ./tensorflow/lite/tools/make/download_dependencies.sh
4. ./tensorflow/lite/tools/make/build_aarch64_lib.sh

I also tried to use master of tensorflow-lite with released version of libedgetup.so.1.0 with a sample test and ran into problems which I suspect are related to mismatch of versions between tensorflow-lite and libedgetpu

> mendel@neat-jet:~/coral-demo/tflite/cpp/examples/classification$ ./minimal 
>  model: /edgetpu/test_data/mobilenet_v1_1.0_224_quant_edgetpu.tflite
>  data: /edgetpu/test_data/resized_cat.bmp
> ERROR: Internal: Unsupported data type in custom op handler: -591183536
> ERROR: Node number 0 (edgetpu-custom-op) failed to prep

Thanks,

"
42639,How do I use a local LLVM installation?,"Hi,

I've been trying to build TensorFlow with my local LLVM installation. I figured I have to choose `N` when I run `./configure`:
```
Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
```
I also tried setting the environment variable manually for the build:
```
TF_DOWNLOAD_CLANG=""0"" bazel build ...
```
However the build is still pulling llvm-project as an external dependency. Is using a local LLVM installation not supported? What do I need to do? Could you please give me the steps I need to take as I am new to Bazel or refer me to a resource?"
42637,Running the Smart Replies Model in TFLite Python,"I'm experimenting with the [Smart Reply][1] Lite model in Python. I used Bazel to compile TFLite with the custom ops  needed to run this model (normalize.cc, predict.cc, extract_features.cc) with the help of this [fantastic tutorial][2] and now I'm trying to run inference. 

Here's the code I use:
```python
    import tensorflow as tf
    import numpy as np
    tflite_interpreter = tf.lite.Interpreter(model_path='smartreply.tflite')
    
    tflite_interpreter.allocate_tensors()
    input_details = tflite_interpreter.get_input_details()
    output_details = tflite_interpreter.get_output_details()
    
    # print(input_details)
    # print(output_details)
    
    tflite_interpreter.set_tensor(input_details[0]['index'], 'Where are you?')
    # Run inference
    tflite_interpreter.invoke()
    # Get prediction results
    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0])
    print(""Prediction results shape:"", tflite_model_predictions)
```

In doing so I got the following error:
```bash
Traceback (most recent call last):
  File ""run.py"", line 12, in <module>
    tflite_interpreter.set_tensor(input_details[0]['index'], 'Where are you?')
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 175, in set_tensor
    self._interpreter.SetTensor(tensor_index, value)
  File ""/home/ubuntu/.local/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 136, in SetTensor
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_SetTensor(self, i, value)
ValueError: numpy array had 56 bytes but expected 0 bytes.
```
I tried resizing the tensor with this line of code before calling `tflite_interpreter.allocate_tensors()`:
```python
tflite_interpreter.resize_tensor_input(0, [56])
```
But that raised `ValueError: Cannot set tensor: Dimension mismatch`. Similarly, if I try to change tensor shape to [1,56] it still fails with the same error. 

My understanding is, the string is converted to a numpy array(based on the model description, the input type is int32 - 4 bytes per character).
What changes do I need to make to this input method to run this model?


  [1]: https://www.tensorflow.org/lite/models/smart_reply/overview
  [2]: https://medium.com/@bsramasubramanian/running-a-tensorflow-lite-model-in-python-with-custom-ops-9b2b46efd355"
42636,Cannot convert predict function of LinearRegressor,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- TensorFlow installed from (source or binary):
binary / pip
- TensorFlow version (or github SHA if from source):
2.4.0-dev20200824

**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
# I have a tf.estimator.LinearRegressor and save it first with function export_saved_model from LinearRegressor.
# Then I load it and save only the predict funtion
imported = tf.saved_model.load('./modelbefore')
tf.saved_model.save(imported, 'model', imported.signatures[""predict""])
# Saved model attached below
# Wenn I then try to load as following I get the error below
converter = tf.lite.TFLiteConverter.from_saved_model('model')
tflite_model = converter.convert()
```

**The output from the converter invocation**

```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
~\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    195     try:
--> 196       model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
    197                                                  toco_flags_str, input_data_str,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\lite\python\wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     31   """"""Wraps TocoConvert with lazy loader.""""""
---> 32   return _pywrap_toco_api.TocoConvert(
     33       model_flags_str,

Exception: :0: error: loc(callsite(callsite(""ParseExample/ParseExampleV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.ParseExampleV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""ParseExample/ParseExampleV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %sparse_indices:5, %sparse_values:5, %sparse_shapes:5, %dense_values:5 = ""tf.ParseExampleV2""(%arg0, %cst_6, %cst_7, %cst_5, %cst_6, %cst_4, %cst_4, %cst_4, %cst_4, %cst_4) {dense_shapes = [#tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>], device = """", num_sparse = 5 : i64, result_segment_sizes = dense<[5, 5, 5, 5, 0, 0]> : vector<6xi32>} : (tensor, tensor<0x!tf.string>, tensor<5x!tf.string>, tensor<5x!tf.string>, tensor<0x!tf.string>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>) -> (tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor, tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/category_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/category_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %4 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_8d6f1b8e-423d-4fff-8a54-69f4ddbecf04_load_0_197"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %5 = ""tf.LookupTableFindV2""(%4, %sparse_values#0, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices, %output_shape = ""tf.SparseReshape""(%sparse_indices#0, %sparse_shapes#0, %8) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_22, %output_shape_23 = ""tf.SparseReshape""(%output_indices, %output_shape, %17) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_24, %output_values, %empty_row_indicator, %reverse_index_map = ""tf.SparseFillEmptyRows""(%18, %14, %output_shape_23, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/description_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/description_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %22 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = !tf.string, shared_name = ""hash_table_fc7c2e70-8a89-4115-84d4-2f713273e69c_load_0_198"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %23 = ""tf.LookupTableFindV2""(%22, %sparse_values#1, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_25, %output_shape_26 = ""tf.SparseReshape""(%sparse_indices#1, %sparse_shapes#1, %26) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_27, %output_shape_28 = ""tf.SparseReshape""(%output_indices_25, %output_shape_26, %35) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_29, %output_values_30, %empty_row_indicator_31, %reverse_index_map_32 = ""tf.SparseFillEmptyRows""(%36, %32, %output_shape_28, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/host_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/host_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %40 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_b60d3bcd-14f8-4085-a3b2-85948ec09373_load_0_199"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %41 = ""tf.LookupTableFindV2""(%40, %sparse_values#3, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_35, %output_shape_36 = ""tf.SparseReshape""(%sparse_indices#3, %sparse_shapes#3, %44) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_37, %output_shape_38 = ""tf.SparseReshape""(%output_indices_35, %output_shape_36, %53) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_39, %output_values_40, %empty_row_indicator_41, %reverse_index_map_42 = ""tf.SparseFillEmptyRows""(%54, %50, %output_shape_38, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/size_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/size_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %58 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_cb0918fe-8c8e-41f5-9aad-3750ec00bdad_load_0_200"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %59 = ""tf.LookupTableFindV2""(%58, %sparse_values#4, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_45, %output_shape_46 = ""tf.SparseReshape""(%sparse_indices#4, %sparse_shapes#4, %62) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_47, %output_shape_48 = ""tf.SparseReshape""(%output_indices_45, %output_shape_46, %71) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_49, %output_values_50, %empty_row_indicator_51, %reverse_index_map_52 = ""tf.SparseFillEmptyRows""(%72, %68, %output_shape_48, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%76, %idx, %21) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%88, %idx_34, %39) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%100, %idx_44, %57) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%113, %idx_54, %75) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.ParseExampleV2 {dense_shapes = [#tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>], device = """", num_sparse = 5 : i64, result_segment_sizes = dense<[5, 5, 5, 5, 0, 0]> : vector<6xi32>}
	tf.SparseFillEmptyRows {device = """"}
	tf.SparseReshape {device = """"}
	tf.SparseSegmentSum {T = f32, Tidx = i32, Tsegmentids = i64, device = """"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.HashTableV2 {container = """", device = """", key_dtype = !tf.string, shared_name = ""hash_table_fc7c2e70-8a89-4115-84d4-2f713273e69c_load_0_198"", use_node_name_sharing = true, value_dtype = i64}
	tf.HashTableV2 {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_8d6f1b8e-423d-4fff-8a54-69f4ddbecf04_load_0_197"", use_node_name_sharing = true, value_dtype = i64}
	tf.HashTableV2 {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_b60d3bcd-14f8-4085-a3b2-85948ec09373_load_0_199"", use_node_name_sharing = true, value_dtype = i64}
	tf.HashTableV2 {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_cb0918fe-8c8e-41f5-9aad-3750ec00bdad_load_0_200"", use_node_name_sharing = true, value_dtype = i64}
	tf.LookupTableFindV2 {device = """"}
:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor):  // no predecessors
  %cst = ""std.constant""() {value = dense<[[0.117987722], [-0.0684242323], [0.100408614], [0.0145546673], [0.0430826135], [-0.103112921], [0.0680344701], [-0.0248609539], [0.0398180261], [0.122247897], [0.0273514148], [0.0187784135], [0.102349631], [-0.0905613824], [-0.0723603144], [-0.0438856669], [0.0021427928], [0.0984751954], [0.0817138106], [-0.109699354], [-0.191155598], [-0.0545536913], [-9.727810e-02], [0.0141912363], [7.680510e-02], [-0.0899474472], [0.0498611145], [-0.0884774774], [-0.114087969], [0.0725763887], [-0.141074464], [-0.176522136], [0.0143758887], [0.0524854325], [-0.155160338], [-0.0285528414], [-0.264534861], [0.106433257], [0.135232121], [0.225332677], [0.129775301], [-0.191358164], [-0.0178745817], [0.0918667614], [0.107648872], [-0.0921946167], [0.064818345], [0.0105348462], [-0.132097453], [-0.110714845], [0.0700208098], [0.034297362], [0.0263220761], [-0.059998773], [-0.0116290115], [0.101751082], [0.0713425949], [-0.0987613201], [-0.209998265], [0.0471415743], [0.10908471], [7.703180e-03], [0.0123223783], [0.103961319], [0.00920343306], [-0.110373154], [-0.113558963], [-0.0215992182], [-0.21590668], [-0.103494935], [-0.21094574], [-0.132196262], [0.18838799], [0.659609914], [-0.209931418], [-0.195380583], [-0.115891144], [-0.130379677], [-0.236354247], [0.111823596]]> : tensor<80x1xf32>} : () -> tensor<80x1xf32>
  %cst_0 = ""std.constant""() {value = dense<[[0.0778336599], [0.0839953199]]> : tensor<2x1xf32>} : () -> tensor<2x1xf32>
  %cst_1 = ""std.constant""() {value = dense<""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tensor<6203x1xf32>} : () -> tensor<6203x1xf32>
  %cst_2 = ""std.constant""() {value = dense<[[0.137156427], [0.0727723241], [0.0427678488], [-2.75064172E-4], [-0.0233619846], [0.0394954272], [-0.0791109725]]> : tensor<7x1xf32>} : () -> tensor<7x1xf32>
  %cst_3 = ""std.constant""() {value = dense<0.131277829> : tensor<1xf32>} : () -> tensor<1xf32>
  %cst_4 = ""std.constant""() {value = dense<> : tensor<0xf32>} : () -> tensor<0xf32>
  %cst_5 = ""std.constant""() {value = dense<[""lat"", ""long"", ""month"", ""price"", ""year""]> : tensor<5x!tf.string>} : () -> tensor<5x!tf.string>
  %cst_6 = ""std.constant""() {value = dense<> : tensor<0x!tf.string>} : () -> tensor<0x!tf.string>
  %cst_7 = ""std.constant""() {value = dense<[""category_id"", ""description"", ""gender"", ""host_id"", ""size_id""]> : tensor<5x!tf.string>} : () -> tensor<5x!tf.string>
  %cst_8 = ""std.constant""() {value = dense<-1> : tensor} : () -> tensor
  %cst_9 = ""std.constant""() {value = dense<-1> : tensor} : () -> tensor
  %cst_10 = ""std.constant""() {value = dense<[-1, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_11 = ""std.constant""() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_12 = ""std.constant""() {value = dense<1> : tensor} : () -> tensor
  %cst_13 = ""std.constant""() {value = dense<0> : tensor} : () -> tensor
  %cst_14 = ""std.constant""() {value = dense<-0.0035018248> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
  %cst_15 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_16 = ""std.constant""() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_17 = ""std.constant""() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_18 = ""std.constant""() {value = dense<1> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_19 = ""std.constant""() {value} : () -> none
  %cst_20 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_21 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %sparse_indices:5, %sparse_values:5, %sparse_shapes:5, %dense_values:5 = ""tf.ParseExampleV2""(%arg0, %cst_6, %cst_7, %cst_5, %cst_6, %cst_4, %cst_4, %cst_4, %cst_4, %cst_4) {dense_shapes = [#tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>], device = """", num_sparse = 5 : i64, result_segment_sizes = dense<[5, 5, 5, 5, 0, 0]> : vector<6xi32>} : (tensor, tensor<0x!tf.string>, tensor<5x!tf.string>, tensor<5x!tf.string>, tensor<0x!tf.string>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>) -> (tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor, tensor, tensor, tensor, tensor)
  %0 = ""tfl.cast""(%sparse_shapes#0) : (tensor<2xi64>) -> tensor<2xi32>
  %1 = ""tfl.cast""(%sparse_shapes#1) : (tensor<2xi64>) -> tensor<2xi32>
  %2 = ""tfl.cast""(%sparse_shapes#3) : (tensor<2xi64>) -> tensor<2xi32>
  %3 = ""tfl.cast""(%sparse_shapes#4) : (tensor<2xi64>) -> tensor<2xi32>
  %4 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_8d6f1b8e-423d-4fff-8a54-69f4ddbecf04_load_0_197"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %5 = ""tf.LookupTableFindV2""(%4, %sparse_values#0, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %6 = ""tfl.strided_slice""(%0, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %7 = ""tfl.pack""(%6, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %8 = ""tfl.cast""(%7) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices, %output_shape = ""tf.SparseReshape""(%sparse_indices#0, %sparse_shapes#0, %8) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %9 = ""tfl.cast""(%output_shape) : (tensor<2xi64>) -> tensor<2xi32>
  %10 = ""tfl.gather""(%output_shape, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %11 = ""tfl.greater_equal""(%5, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %12 = ""tfl.where""(%11) : (tensor<*xi1>) -> tensor
  %13 = ""tfl.reshape""(%12, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %14 = ""tfl.gather""(%5, %13) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %15 = ""tfl.slice""(%output_shape, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %16 = ""tfl.reduce_prod""(%15, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %17 = ""tfl.pack""(%16, %10) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_22, %output_shape_23 = ""tf.SparseReshape""(%output_indices, %output_shape, %17) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %18 = ""tfl.gather""(%output_indices_22, %13) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %19 = ""tfl.slice""(%9, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_24, %output_values, %empty_row_indicator, %reverse_index_map = ""tf.SparseFillEmptyRows""(%18, %14, %output_shape_23, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %20 = ""tfl.reshape""(%empty_row_indicator, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output, %idx = ""tfl.unique""(%output_values) : (tensor) -> (tensor, tensor)
  %21 = ""tfl.strided_slice""(%output_indices_24, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %22 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = !tf.string, shared_name = ""hash_table_fc7c2e70-8a89-4115-84d4-2f713273e69c_load_0_198"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %23 = ""tf.LookupTableFindV2""(%22, %sparse_values#1, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %24 = ""tfl.strided_slice""(%1, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %25 = ""tfl.pack""(%24, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %26 = ""tfl.cast""(%25) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices_25, %output_shape_26 = ""tf.SparseReshape""(%sparse_indices#1, %sparse_shapes#1, %26) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %27 = ""tfl.cast""(%output_shape_26) : (tensor<2xi64>) -> tensor<2xi32>
  %28 = ""tfl.gather""(%output_shape_26, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %29 = ""tfl.greater_equal""(%23, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %30 = ""tfl.where""(%29) : (tensor<*xi1>) -> tensor
  %31 = ""tfl.reshape""(%30, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %32 = ""tfl.gather""(%23, %31) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %33 = ""tfl.slice""(%output_shape_26, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %34 = ""tfl.reduce_prod""(%33, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %35 = ""tfl.pack""(%34, %28) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_27, %output_shape_28 = ""tf.SparseReshape""(%output_indices_25, %output_shape_26, %35) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %36 = ""tfl.gather""(%output_indices_27, %31) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %37 = ""tfl.slice""(%27, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_29, %output_values_30, %empty_row_indicator_31, %reverse_index_map_32 = ""tf.SparseFillEmptyRows""(%36, %32, %output_shape_28, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %38 = ""tfl.reshape""(%empty_row_indicator_31, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output_33, %idx_34 = ""tfl.unique""(%output_values_30) : (tensor) -> (tensor, tensor)
  %39 = ""tfl.strided_slice""(%output_indices_29, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %40 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_b60d3bcd-14f8-4085-a3b2-85948ec09373_load_0_199"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %41 = ""tf.LookupTableFindV2""(%40, %sparse_values#3, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %42 = ""tfl.strided_slice""(%2, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %43 = ""tfl.pack""(%42, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %44 = ""tfl.cast""(%43) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices_35, %output_shape_36 = ""tf.SparseReshape""(%sparse_indices#3, %sparse_shapes#3, %44) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %45 = ""tfl.cast""(%output_shape_36) : (tensor<2xi64>) -> tensor<2xi32>
  %46 = ""tfl.gather""(%output_shape_36, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %47 = ""tfl.greater_equal""(%41, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %48 = ""tfl.where""(%47) : (tensor<*xi1>) -> tensor
  %49 = ""tfl.reshape""(%48, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %50 = ""tfl.gather""(%41, %49) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %51 = ""tfl.slice""(%output_shape_36, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %52 = ""tfl.reduce_prod""(%51, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %53 = ""tfl.pack""(%52, %46) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_37, %output_shape_38 = ""tf.SparseReshape""(%output_indices_35, %output_shape_36, %53) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %54 = ""tfl.gather""(%output_indices_37, %49) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %55 = ""tfl.slice""(%45, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_39, %output_values_40, %empty_row_indicator_41, %reverse_index_map_42 = ""tf.SparseFillEmptyRows""(%54, %50, %output_shape_38, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %56 = ""tfl.reshape""(%empty_row_indicator_41, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output_43, %idx_44 = ""tfl.unique""(%output_values_40) : (tensor) -> (tensor, tensor)
  %57 = ""tfl.strided_slice""(%output_indices_39, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %58 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_cb0918fe-8c8e-41f5-9aad-3750ec00bdad_load_0_200"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %59 = ""tf.LookupTableFindV2""(%58, %sparse_values#4, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %60 = ""tfl.strided_slice""(%3, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %61 = ""tfl.pack""(%60, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %62 = ""tfl.cast""(%61) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices_45, %output_shape_46 = ""tf.SparseReshape""(%sparse_indices#4, %sparse_shapes#4, %62) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %63 = ""tfl.cast""(%output_shape_46) : (tensor<2xi64>) -> tensor<2xi32>
  %64 = ""tfl.gather""(%output_shape_46, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %65 = ""tfl.greater_equal""(%59, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %66 = ""tfl.where""(%65) : (tensor<*xi1>) -> tensor
  %67 = ""tfl.reshape""(%66, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %68 = ""tfl.gather""(%59, %67) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %69 = ""tfl.slice""(%output_shape_46, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %70 = ""tfl.reduce_prod""(%69, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %71 = ""tfl.pack""(%70, %64) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_47, %output_shape_48 = ""tf.SparseReshape""(%output_indices_45, %output_shape_46, %71) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %72 = ""tfl.gather""(%output_indices_47, %67) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %73 = ""tfl.slice""(%63, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_49, %output_values_50, %empty_row_indicator_51, %reverse_index_map_52 = ""tf.SparseFillEmptyRows""(%72, %68, %output_shape_48, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %74 = ""tfl.reshape""(%empty_row_indicator_51, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output_53, %idx_54 = ""tfl.unique""(%output_values_50) : (tensor) -> (tensor, tensor)
  %75 = ""tfl.strided_slice""(%output_indices_49, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %76 = ""tfl.gather""(%cst_2, %output) {axis = 0 : i32} : (tensor<7x1xf32>, tensor) -> tensor
  %77 = ""tfl.custom_tf""(%76, %idx, %21) ( {
    %127 = ""tf.SparseSegmentSum""(%76, %idx, %21) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %78 = ""tfl.shape""(%77) : (tensor) -> tensor<2xi32>
  %79 = ""tfl.strided_slice""(%78, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %80 = ""tfl.pack""(%cst_12, %79) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %81 = ""tfl.tile""(%20, %80) : (tensor, tensor<2xi32>) -> tensor
  %82 = ""tfl.zeros_like""(%77) : (tensor) -> tensor
  %83 = ""tfl.select""(%81, %82, %77) : (tensor, tensor, tensor) -> tensor
  %84 = ""tfl.shape""(%83) : (tensor) -> tensor<2xi32>
  %85 = ""tfl.slice""(%84, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %86 = ""tfl.concatenation""(%19, %85) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %87 = ""tfl.reshape""(%83, %86) : (tensor, tensor<2xi32>) -> tensor
  %88 = ""tfl.gather""(%cst_1, %output_33) {axis = 0 : i32} : (tensor<6203x1xf32>, tensor) -> tensor
  %89 = ""tfl.custom_tf""(%88, %idx_34, %39) ( {
    %127 = ""tf.SparseSegmentSum""(%88, %idx_34, %39) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %90 = ""tfl.shape""(%89) : (tensor) -> tensor<2xi32>
  %91 = ""tfl.strided_slice""(%90, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %92 = ""tfl.pack""(%cst_12, %91) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %93 = ""tfl.tile""(%38, %92) : (tensor, tensor<2xi32>) -> tensor
  %94 = ""tfl.zeros_like""(%89) : (tensor) -> tensor
  %95 = ""tfl.select""(%93, %94, %89) : (tensor, tensor, tensor) -> tensor
  %96 = ""tfl.shape""(%95) : (tensor) -> tensor<2xi32>
  %97 = ""tfl.slice""(%96, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %98 = ""tfl.concatenation""(%37, %97) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %99 = ""tfl.reshape""(%95, %98) : (tensor, tensor<2xi32>) -> tensor
  %100 = ""tfl.gather""(%cst_0, %output_43) {axis = 0 : i32} : (tensor<2x1xf32>, tensor) -> tensor
  %101 = ""tfl.custom_tf""(%100, %idx_44, %57) ( {
    %127 = ""tf.SparseSegmentSum""(%100, %idx_44, %57) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %102 = ""tfl.shape""(%101) : (tensor) -> tensor<2xi32>
  %103 = ""tfl.strided_slice""(%102, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %104 = ""tfl.pack""(%cst_12, %103) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %105 = ""tfl.tile""(%56, %104) : (tensor, tensor<2xi32>) -> tensor
  %106 = ""tfl.zeros_like""(%101) : (tensor) -> tensor
  %107 = ""tfl.select""(%105, %106, %101) : (tensor, tensor, tensor) -> tensor
  %108 = ""tfl.shape""(%107) : (tensor) -> tensor<2xi32>
  %109 = ""tfl.slice""(%108, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %110 = ""tfl.concatenation""(%55, %109) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %111 = ""tfl.reshape""(%107, %110) : (tensor, tensor<2xi32>) -> tensor
  %112 = ""tfl.fully_connected""(%dense_values#3, %cst_14, %cst_19) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor, tensor<1x1xf32>, none) -> tensor
  %113 = ""tfl.gather""(%cst, %output_53) {axis = 0 : i32} : (tensor<80x1xf32>, tensor) -> tensor
  %114 = ""tfl.custom_tf""(%113, %idx_54, %75) ( {
    %127 = ""tf.SparseSegmentSum""(%113, %idx_54, %75) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %115 = ""tfl.shape""(%114) : (tensor) -> tensor<2xi32>
  %116 = ""tfl.strided_slice""(%115, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %117 = ""tfl.pack""(%cst_12, %116) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %118 = ""tfl.tile""(%74, %117) : (tensor, tensor<2xi32>) -> tensor
  %119 = ""tfl.zeros_like""(%114) : (tensor) -> tensor
  %120 = ""tfl.select""(%118, %119, %114) : (tensor, tensor, tensor) -> tensor
  %121 = ""tfl.shape""(%120) : (tensor) -> tensor<2xi32>
  %122 = ""tfl.slice""(%121, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %123 = ""tfl.concatenation""(%73, %122) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %124 = ""tfl.reshape""(%120, %123) : (tensor, tensor<2xi32>) -> tensor
  %125 = ""tfl.add_n""(%87, %99, %111, %112, %124) : (tensor, tensor, tensor, tensor, tensor) -> tensor
  %126 = ""tfl.add""(%125, %cst_3) {fused_activation_function = ""NONE""} : (tensor, tensor<1xf32>) -> tensor
  ""std.return""(%126) : (tensor) -> ()
}) {arg0 = {tf_saved_model.index_path = [""examples""]}, result0 = {tf_saved_model.index_path = [""predictions""]}, sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_examples:0"", outputs = ""StatefulPartitionedCall_1:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor) -> tensor} : () -> ()


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
 in 
      1 converter = tf.lite.TFLiteConverter.from_saved_model('test2')
----> 2 tflite_model = converter.convert()

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\lite\python\lite.py in convert(self)
    722         for key in signature_def.outputs
    723     ]
--> 724     return super(TFLiteSavedModelConverterV2,
    725                  self).convert(meta_graph.graph_def, input_tensors,
    726                                output_tensors)

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\lite\python\lite.py in convert(self, graph_def, input_tensors, output_tensors)
    637 
    638     # Converts model.
--> 639     result = _toco_convert_impl(
    640         input_data=graph_def,
    641         input_tensors=input_tensors,

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_impl(input_data, input_tensors, output_tensors, enable_mlir_converter, *args, **kwargs)
    567       input_tensors, output_tensors, *args, **kwargs)
    568   debug_info_str = debug_info.SerializeToString() if debug_info else None
--> 569   data = toco_convert_protos(
    570       model_flags.SerializeToString(),
    571       toco_flags.SerializeToString(),

~\AppData\Local\Programs\Python\Python38\lib\site-packages\tensorflow\lite\python\convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    200       return model_str
    201     except Exception as e:
--> 202       raise ConverterError(str(e))
    203 
    204   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: :0: error: loc(callsite(callsite(""ParseExample/ParseExampleV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.ParseExampleV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""ParseExample/ParseExampleV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %sparse_indices:5, %sparse_values:5, %sparse_shapes:5, %dense_values:5 = ""tf.ParseExampleV2""(%arg0, %cst_6, %cst_7, %cst_5, %cst_6, %cst_4, %cst_4, %cst_4, %cst_4, %cst_4) {dense_shapes = [#tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>], device = """", num_sparse = 5 : i64, result_segment_sizes = dense<[5, 5, 5, 5, 0, 0]> : vector<6xi32>} : (tensor, tensor<0x!tf.string>, tensor<5x!tf.string>, tensor<5x!tf.string>, tensor<0x!tf.string>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>) -> (tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor, tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/category_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/category_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %4 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_8d6f1b8e-423d-4fff-8a54-69f4ddbecf04_load_0_197"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %5 = ""tf.LookupTableFindV2""(%4, %sparse_values#0, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices, %output_shape = ""tf.SparseReshape""(%sparse_indices#0, %sparse_shapes#0, %8) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_22, %output_shape_23 = ""tf.SparseReshape""(%output_indices, %output_shape, %17) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_24, %output_values, %empty_row_indicator, %reverse_index_map = ""tf.SparseFillEmptyRows""(%18, %14, %output_shape_23, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/description_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/description_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %22 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = !tf.string, shared_name = ""hash_table_fc7c2e70-8a89-4115-84d4-2f713273e69c_load_0_198"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %23 = ""tf.LookupTableFindV2""(%22, %sparse_values#1, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_25, %output_shape_26 = ""tf.SparseReshape""(%sparse_indices#1, %sparse_shapes#1, %26) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_27, %output_shape_28 = ""tf.SparseReshape""(%output_indices_25, %output_shape_26, %35) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_29, %output_values_30, %empty_row_indicator_31, %reverse_index_map_32 = ""tf.SparseFillEmptyRows""(%36, %32, %output_shape_28, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/host_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/host_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %40 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_b60d3bcd-14f8-4085-a3b2-85948ec09373_load_0_199"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %41 = ""tf.LookupTableFindV2""(%40, %sparse_values#3, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_35, %output_shape_36 = ""tf.SparseReshape""(%sparse_indices#3, %sparse_shapes#3, %44) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_37, %output_shape_38 = ""tf.SparseReshape""(%output_indices_35, %output_shape_36, %53) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_39, %output_values_40, %empty_row_indicator_41, %reverse_index_map_42 = ""tf.SparseFillEmptyRows""(%54, %50, %output_shape_38, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/size_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.HashTableV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/size_id_lookup/hash_table/hash_table@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %58 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_cb0918fe-8c8e-41f5-9aad-3750ec00bdad_load_0_200"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.LookupTableFindV2' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/hash_table_Lookup/LookupTableFindV2@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %59 = ""tf.LookupTableFindV2""(%58, %sparse_values#4, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_45, %output_shape_46 = ""tf.SparseReshape""(%sparse_indices#4, %sparse_shapes#4, %62) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseReshape' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseReshape@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_47, %output_shape_48 = ""tf.SparseReshape""(%output_indices_45, %output_shape_46, %71) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseFillEmptyRows' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/SparseFillEmptyRows/SparseFillEmptyRows@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %output_indices_49, %output_values_50, %empty_row_indicator_51, %reverse_index_map_52 = ""tf.SparseFillEmptyRows""(%72, %68, %output_shape_48, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/category_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%76, %idx, %21) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/description/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%88, %idx_34, %39) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/host_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%100, %idx_44, %57) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): 'tf.SparseSegmentSum' op is neither a custom op nor a flex op
:0: note: loc(""StatefulPartitionedCall_1""): called from
:0: note: loc(callsite(callsite(""linear/linear_model/linear/linear_model/linear/linear_model/size_id/weighted_sum/embedding_lookup_sparse@__inference_pruned_1133"" at ""StatefulPartitionedCall@__inference_signature_wrapper_1850"") at ""StatefulPartitionedCall_1"")): see current operation: %127 = ""tf.SparseSegmentSum""(%113, %idx_54, %75) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
:0: error: failed while converting: 'main': Ops that can be supported by the flex runtime (enabled via setting the -emit-select-tf-ops flag):
	tf.ParseExampleV2 {dense_shapes = [#tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>], device = """", num_sparse = 5 : i64, result_segment_sizes = dense<[5, 5, 5, 5, 0, 0]> : vector<6xi32>}
	tf.SparseFillEmptyRows {device = """"}
	tf.SparseReshape {device = """"}
	tf.SparseSegmentSum {T = f32, Tidx = i32, Tsegmentids = i64, device = """"}Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.HashTableV2 {container = """", device = """", key_dtype = !tf.string, shared_name = ""hash_table_fc7c2e70-8a89-4115-84d4-2f713273e69c_load_0_198"", use_node_name_sharing = true, value_dtype = i64}
	tf.HashTableV2 {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_8d6f1b8e-423d-4fff-8a54-69f4ddbecf04_load_0_197"", use_node_name_sharing = true, value_dtype = i64}
	tf.HashTableV2 {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_b60d3bcd-14f8-4085-a3b2-85948ec09373_load_0_199"", use_node_name_sharing = true, value_dtype = i64}
	tf.HashTableV2 {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_cb0918fe-8c8e-41f5-9aad-3750ec00bdad_load_0_200"", use_node_name_sharing = true, value_dtype = i64}
	tf.LookupTableFindV2 {device = """"}
:0: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor):  // no predecessors
  %cst = ""std.constant""() {value = dense<[[0.117987722], [-0.0684242323], [0.100408614], [0.0145546673], [0.0430826135], [-0.103112921], [0.0680344701], [-0.0248609539], [0.0398180261], [0.122247897], [0.0273514148], [0.0187784135], [0.102349631], [-0.0905613824], [-0.0723603144], [-0.0438856669], [0.0021427928], [0.0984751954], [0.0817138106], [-0.109699354], [-0.191155598], [-0.0545536913], [-9.727810e-02], [0.0141912363], [7.680510e-02], [-0.0899474472], [0.0498611145], [-0.0884774774], [-0.114087969], [0.0725763887], [-0.141074464], [-0.176522136], [0.0143758887], [0.0524854325], [-0.155160338], [-0.0285528414], [-0.264534861], [0.106433257], [0.135232121], [0.225332677], [0.129775301], [-0.191358164], [-0.0178745817], [0.0918667614], [0.107648872], [-0.0921946167], [0.064818345], [0.0105348462], [-0.132097453], [-0.110714845], [0.0700208098], [0.034297362], [0.0263220761], [-0.059998773], [-0.0116290115], [0.101751082], [0.0713425949], [-0.0987613201], [-0.209998265], [0.0471415743], [0.10908471], [7.703180e-03], [0.0123223783], [0.103961319], [0.00920343306], [-0.110373154], [-0.113558963], [-0.0215992182], [-0.21590668], [-0.103494935], [-0.21094574], [-0.132196262], [0.18838799], [0.659609914], [-0.209931418], [-0.195380583], [-0.115891144], [-0.130379677], [-0.236354247], [0.111823596]]> : tensor<80x1xf32>} : () -> tensor<80x1xf32>
  %cst_0 = ""std.constant""() {value = dense<[[0.0778336599], [0.0839953199]]> : tensor<2x1xf32>} : () -> tensor<2x1xf32>
  %cst_1 = ""std.constant""() {value = dense<""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tensor<6203x1xf32>} : () -> tensor<6203x1xf32>
  %cst_2 = ""std.constant""() {value = dense<[[0.137156427], [0.0727723241], [0.0427678488], [-2.75064172E-4], [-0.0233619846], [0.0394954272], [-0.0791109725]]> : tensor<7x1xf32>} : () -> tensor<7x1xf32>
  %cst_3 = ""std.constant""() {value = dense<0.131277829> : tensor<1xf32>} : () -> tensor<1xf32>
  %cst_4 = ""std.constant""() {value = dense<> : tensor<0xf32>} : () -> tensor<0xf32>
  %cst_5 = ""std.constant""() {value = dense<[""lat"", ""long"", ""month"", ""price"", ""year""]> : tensor<5x!tf.string>} : () -> tensor<5x!tf.string>
  %cst_6 = ""std.constant""() {value = dense<> : tensor<0x!tf.string>} : () -> tensor<0x!tf.string>
  %cst_7 = ""std.constant""() {value = dense<[""category_id"", ""description"", ""gender"", ""host_id"", ""size_id""]> : tensor<5x!tf.string>} : () -> tensor<5x!tf.string>
  %cst_8 = ""std.constant""() {value = dense<-1> : tensor} : () -> tensor
  %cst_9 = ""std.constant""() {value = dense<-1> : tensor} : () -> tensor
  %cst_10 = ""std.constant""() {value = dense<[-1, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_11 = ""std.constant""() {value = dense<-1> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_12 = ""std.constant""() {value = dense<1> : tensor} : () -> tensor
  %cst_13 = ""std.constant""() {value = dense<0> : tensor} : () -> tensor
  %cst_14 = ""std.constant""() {value = dense<-0.0035018248> : tensor<1x1xf32>} : () -> tensor<1x1xf32>
  %cst_15 = ""std.constant""() {value = dense<0> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_16 = ""std.constant""() {value = dense<0> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_17 = ""std.constant""() {value = dense<[0, 1]> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_18 = ""std.constant""() {value = dense<1> : tensor<2xi32>} : () -> tensor<2xi32>
  %cst_19 = ""std.constant""() {value} : () -> none
  %cst_20 = ""std.constant""() {value = dense<2> : tensor<1xi32>} : () -> tensor<1xi32>
  %cst_21 = ""std.constant""() {value = dense<1> : tensor<1xi32>} : () -> tensor<1xi32>
  %sparse_indices:5, %sparse_values:5, %sparse_shapes:5, %dense_values:5 = ""tf.ParseExampleV2""(%arg0, %cst_6, %cst_7, %cst_5, %cst_6, %cst_4, %cst_4, %cst_4, %cst_4, %cst_4) {dense_shapes = [#tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>, #tf.shape<1>], device = """", num_sparse = 5 : i64, result_segment_sizes = dense<[5, 5, 5, 5, 0, 0]> : vector<6xi32>} : (tensor, tensor<0x!tf.string>, tensor<5x!tf.string>, tensor<5x!tf.string>, tensor<0x!tf.string>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>, tensor<0xf32>) -> (tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor<2xi64>, tensor, tensor, tensor, tensor, tensor)
  %0 = ""tfl.cast""(%sparse_shapes#0) : (tensor<2xi64>) -> tensor<2xi32>
  %1 = ""tfl.cast""(%sparse_shapes#1) : (tensor<2xi64>) -> tensor<2xi32>
  %2 = ""tfl.cast""(%sparse_shapes#3) : (tensor<2xi64>) -> tensor<2xi32>
  %3 = ""tfl.cast""(%sparse_shapes#4) : (tensor<2xi64>) -> tensor<2xi32>
  %4 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_8d6f1b8e-423d-4fff-8a54-69f4ddbecf04_load_0_197"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %5 = ""tf.LookupTableFindV2""(%4, %sparse_values#0, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %6 = ""tfl.strided_slice""(%0, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %7 = ""tfl.pack""(%6, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %8 = ""tfl.cast""(%7) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices, %output_shape = ""tf.SparseReshape""(%sparse_indices#0, %sparse_shapes#0, %8) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %9 = ""tfl.cast""(%output_shape) : (tensor<2xi64>) -> tensor<2xi32>
  %10 = ""tfl.gather""(%output_shape, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %11 = ""tfl.greater_equal""(%5, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %12 = ""tfl.where""(%11) : (tensor<*xi1>) -> tensor
  %13 = ""tfl.reshape""(%12, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %14 = ""tfl.gather""(%5, %13) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %15 = ""tfl.slice""(%output_shape, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %16 = ""tfl.reduce_prod""(%15, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %17 = ""tfl.pack""(%16, %10) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_22, %output_shape_23 = ""tf.SparseReshape""(%output_indices, %output_shape, %17) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %18 = ""tfl.gather""(%output_indices_22, %13) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %19 = ""tfl.slice""(%9, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_24, %output_values, %empty_row_indicator, %reverse_index_map = ""tf.SparseFillEmptyRows""(%18, %14, %output_shape_23, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %20 = ""tfl.reshape""(%empty_row_indicator, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output, %idx = ""tfl.unique""(%output_values) : (tensor) -> (tensor, tensor)
  %21 = ""tfl.strided_slice""(%output_indices_24, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %22 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = !tf.string, shared_name = ""hash_table_fc7c2e70-8a89-4115-84d4-2f713273e69c_load_0_198"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %23 = ""tf.LookupTableFindV2""(%22, %sparse_values#1, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %24 = ""tfl.strided_slice""(%1, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %25 = ""tfl.pack""(%24, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %26 = ""tfl.cast""(%25) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices_25, %output_shape_26 = ""tf.SparseReshape""(%sparse_indices#1, %sparse_shapes#1, %26) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %27 = ""tfl.cast""(%output_shape_26) : (tensor<2xi64>) -> tensor<2xi32>
  %28 = ""tfl.gather""(%output_shape_26, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %29 = ""tfl.greater_equal""(%23, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %30 = ""tfl.where""(%29) : (tensor<*xi1>) -> tensor
  %31 = ""tfl.reshape""(%30, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %32 = ""tfl.gather""(%23, %31) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %33 = ""tfl.slice""(%output_shape_26, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %34 = ""tfl.reduce_prod""(%33, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %35 = ""tfl.pack""(%34, %28) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_27, %output_shape_28 = ""tf.SparseReshape""(%output_indices_25, %output_shape_26, %35) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %36 = ""tfl.gather""(%output_indices_27, %31) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %37 = ""tfl.slice""(%27, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_29, %output_values_30, %empty_row_indicator_31, %reverse_index_map_32 = ""tf.SparseFillEmptyRows""(%36, %32, %output_shape_28, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %38 = ""tfl.reshape""(%empty_row_indicator_31, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output_33, %idx_34 = ""tfl.unique""(%output_values_30) : (tensor) -> (tensor, tensor)
  %39 = ""tfl.strided_slice""(%output_indices_29, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %40 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_b60d3bcd-14f8-4085-a3b2-85948ec09373_load_0_199"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %41 = ""tf.LookupTableFindV2""(%40, %sparse_values#3, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %42 = ""tfl.strided_slice""(%2, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %43 = ""tfl.pack""(%42, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %44 = ""tfl.cast""(%43) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices_35, %output_shape_36 = ""tf.SparseReshape""(%sparse_indices#3, %sparse_shapes#3, %44) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %45 = ""tfl.cast""(%output_shape_36) : (tensor<2xi64>) -> tensor<2xi32>
  %46 = ""tfl.gather""(%output_shape_36, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %47 = ""tfl.greater_equal""(%41, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %48 = ""tfl.where""(%47) : (tensor<*xi1>) -> tensor
  %49 = ""tfl.reshape""(%48, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %50 = ""tfl.gather""(%41, %49) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %51 = ""tfl.slice""(%output_shape_36, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %52 = ""tfl.reduce_prod""(%51, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %53 = ""tfl.pack""(%52, %46) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_37, %output_shape_38 = ""tf.SparseReshape""(%output_indices_35, %output_shape_36, %53) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %54 = ""tfl.gather""(%output_indices_37, %49) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %55 = ""tfl.slice""(%45, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_39, %output_values_40, %empty_row_indicator_41, %reverse_index_map_42 = ""tf.SparseFillEmptyRows""(%54, %50, %output_shape_38, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %56 = ""tfl.reshape""(%empty_row_indicator_41, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output_43, %idx_44 = ""tfl.unique""(%output_values_40) : (tensor) -> (tensor, tensor)
  %57 = ""tfl.strided_slice""(%output_indices_39, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %58 = ""tf.HashTableV2""() {container = """", device = """", key_dtype = i64, shared_name = ""hash_table_cb0918fe-8c8e-41f5-9aad-3750ec00bdad_load_0_200"", use_node_name_sharing = true, value_dtype = i64} : () -> tensor
  %59 = ""tf.LookupTableFindV2""(%58, %sparse_values#4, %cst_9) {device = """"} : (tensor, tensor, tensor) -> tensor<*xi64>
  %60 = ""tfl.strided_slice""(%3, %cst_15, %cst_21, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %61 = ""tfl.pack""(%60, %cst_8) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %62 = ""tfl.cast""(%61) : (tensor<2xi32>) -> tensor<2xi64>
  %output_indices_45, %output_shape_46 = ""tf.SparseReshape""(%sparse_indices#4, %sparse_shapes#4, %62) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %63 = ""tfl.cast""(%output_shape_46) : (tensor<2xi64>) -> tensor<2xi32>
  %64 = ""tfl.gather""(%output_shape_46, %cst_12) {axis = 0 : i32} : (tensor<2xi64>, tensor) -> tensor
  %65 = ""tfl.greater_equal""(%59, %cst_13) : (tensor<*xi64>, tensor) -> tensor<*xi1>
  %66 = ""tfl.where""(%65) : (tensor<*xi1>) -> tensor
  %67 = ""tfl.reshape""(%66, %cst_11) : (tensor, tensor<1xi32>) -> tensor
  %68 = ""tfl.gather""(%59, %67) {axis = 0 : i32} : (tensor<*xi64>, tensor) -> tensor<*xi64>
  %69 = ""tfl.slice""(%output_shape_46, %cst_15, %cst_21) : (tensor<2xi64>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi64>
  %70 = ""tfl.reduce_prod""(%69, %cst_15) {keep_dims = false} : (tensor<1xi64>, tensor<1xi32>) -> tensor
  %71 = ""tfl.pack""(%70, %64) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi64>
  %output_indices_47, %output_shape_48 = ""tf.SparseReshape""(%output_indices_45, %output_shape_46, %71) {device = """"} : (tensor, tensor<2xi64>, tensor<2xi64>) -> (tensor, tensor<2xi64>)
  %72 = ""tfl.gather""(%output_indices_47, %67) {axis = 0 : i32} : (tensor, tensor) -> tensor
  %73 = ""tfl.slice""(%63, %cst_15, %cst_21) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %output_indices_49, %output_values_50, %empty_row_indicator_51, %reverse_index_map_52 = ""tf.SparseFillEmptyRows""(%72, %68, %output_shape_48, %cst_13) {device = """"} : (tensor, tensor<*xi64>, tensor<2xi64>, tensor) -> (tensor, tensor, tensor, tensor)
  %74 = ""tfl.reshape""(%empty_row_indicator_51, %cst_10) : (tensor, tensor<2xi32>) -> tensor
  %output_53, %idx_54 = ""tfl.unique""(%output_values_50) : (tensor) -> (tensor, tensor)
  %75 = ""tfl.strided_slice""(%output_indices_49, %cst_16, %cst_17, %cst_18) {begin_mask = 1 : i32, ellipsis_mask = 0 : i32, end_mask = 1 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 2 : i32} : (tensor, tensor<2xi32>, tensor<2xi32>, tensor<2xi32>) -> tensor
  %76 = ""tfl.gather""(%cst_2, %output) {axis = 0 : i32} : (tensor<7x1xf32>, tensor) -> tensor
  %77 = ""tfl.custom_tf""(%76, %idx, %21) ( {
    %127 = ""tf.SparseSegmentSum""(%76, %idx, %21) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %78 = ""tfl.shape""(%77) : (tensor) -> tensor<2xi32>
  %79 = ""tfl.strided_slice""(%78, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %80 = ""tfl.pack""(%cst_12, %79) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %81 = ""tfl.tile""(%20, %80) : (tensor, tensor<2xi32>) -> tensor
  %82 = ""tfl.zeros_like""(%77) : (tensor) -> tensor
  %83 = ""tfl.select""(%81, %82, %77) : (tensor, tensor, tensor) -> tensor
  %84 = ""tfl.shape""(%83) : (tensor) -> tensor<2xi32>
  %85 = ""tfl.slice""(%84, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %86 = ""tfl.concatenation""(%19, %85) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %87 = ""tfl.reshape""(%83, %86) : (tensor, tensor<2xi32>) -> tensor
  %88 = ""tfl.gather""(%cst_1, %output_33) {axis = 0 : i32} : (tensor<6203x1xf32>, tensor) -> tensor
  %89 = ""tfl.custom_tf""(%88, %idx_34, %39) ( {
    %127 = ""tf.SparseSegmentSum""(%88, %idx_34, %39) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %90 = ""tfl.shape""(%89) : (tensor) -> tensor<2xi32>
  %91 = ""tfl.strided_slice""(%90, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %92 = ""tfl.pack""(%cst_12, %91) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %93 = ""tfl.tile""(%38, %92) : (tensor, tensor<2xi32>) -> tensor
  %94 = ""tfl.zeros_like""(%89) : (tensor) -> tensor
  %95 = ""tfl.select""(%93, %94, %89) : (tensor, tensor, tensor) -> tensor
  %96 = ""tfl.shape""(%95) : (tensor) -> tensor<2xi32>
  %97 = ""tfl.slice""(%96, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %98 = ""tfl.concatenation""(%37, %97) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %99 = ""tfl.reshape""(%95, %98) : (tensor, tensor<2xi32>) -> tensor
  %100 = ""tfl.gather""(%cst_0, %output_43) {axis = 0 : i32} : (tensor<2x1xf32>, tensor) -> tensor
  %101 = ""tfl.custom_tf""(%100, %idx_44, %57) ( {
    %127 = ""tf.SparseSegmentSum""(%100, %idx_44, %57) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %102 = ""tfl.shape""(%101) : (tensor) -> tensor<2xi32>
  %103 = ""tfl.strided_slice""(%102, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %104 = ""tfl.pack""(%cst_12, %103) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %105 = ""tfl.tile""(%56, %104) : (tensor, tensor<2xi32>) -> tensor
  %106 = ""tfl.zeros_like""(%101) : (tensor) -> tensor
  %107 = ""tfl.select""(%105, %106, %101) : (tensor, tensor, tensor) -> tensor
  %108 = ""tfl.shape""(%107) : (tensor) -> tensor<2xi32>
  %109 = ""tfl.slice""(%108, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %110 = ""tfl.concatenation""(%55, %109) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %111 = ""tfl.reshape""(%107, %110) : (tensor, tensor<2xi32>) -> tensor
  %112 = ""tfl.fully_connected""(%dense_values#3, %cst_14, %cst_19) {fused_activation_function = ""NONE"", keep_num_dims = false, weights_format = ""DEFAULT""} : (tensor, tensor<1x1xf32>, none) -> tensor
  %113 = ""tfl.gather""(%cst, %output_53) {axis = 0 : i32} : (tensor<80x1xf32>, tensor) -> tensor
  %114 = ""tfl.custom_tf""(%113, %idx_54, %75) ( {
    %127 = ""tf.SparseSegmentSum""(%113, %idx_54, %75) {T = f32, Tidx = i32, Tsegmentids = i64, device = """"} : (tensor, tensor, tensor) -> tensor
    ""tfl.yield""(%127) : (tensor) -> ()
  }) : (tensor, tensor, tensor) -> tensor
  %115 = ""tfl.shape""(%114) : (tensor) -> tensor<2xi32>
  %116 = ""tfl.strided_slice""(%115, %cst_21, %cst_20, %cst_21) {begin_mask = 0 : i32, ellipsis_mask = 0 : i32, end_mask = 0 : i32, new_axis_mask = 0 : i32, shrink_axis_mask = 1 : i32} : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor
  %117 = ""tfl.pack""(%cst_12, %116) {axis = 0 : i32, values_count = 2 : i32} : (tensor, tensor) -> tensor<2xi32>
  %118 = ""tfl.tile""(%74, %117) : (tensor, tensor<2xi32>) -> tensor
  %119 = ""tfl.zeros_like""(%114) : (tensor) -> tensor
  %120 = ""tfl.select""(%118, %119, %114) : (tensor, tensor, tensor) -> tensor
  %121 = ""tfl.shape""(%120) : (tensor) -> tensor<2xi32>
  %122 = ""tfl.slice""(%121, %cst_21, %cst_11) : (tensor<2xi32>, tensor<1xi32>, tensor<1xi32>) -> tensor<1xi32>
  %123 = ""tfl.concatenation""(%73, %122) {axis = 0 : i32, fused_activation_function = ""NONE""} : (tensor<1xi32>, tensor<1xi32>) -> tensor<2xi32>
  %124 = ""tfl.reshape""(%120, %123) : (tensor, tensor<2xi32>) -> tensor
  %125 = ""tfl.add_n""(%87, %99, %111, %112, %124) : (tensor, tensor, tensor, tensor, tensor) -> tensor
  %126 = ""tfl.add""(%125, %cst_3) {fused_activation_function = ""NONE""} : (tensor, tensor<1xf32>) -> tensor
  ""std.return""(%126) : (tensor) -> ()
}) {arg0 = {tf_saved_model.index_path = [""examples""]}, result0 = {tf_saved_model.index_path = [""predictions""]}, sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""serving_default_examples:0"", outputs = ""StatefulPartitionedCall_1:0""}, tf_saved_model.exported_names = [""serving_default""], type = (tensor) -> tensor} : () -> ()
```

**Also, please include a link to the saved model or GraphDef**

```
[model.zip](https://github.com/tensorflow/tensorflow/files/5120025/model.zip)
```

**Failure details**
If the conversion is successful, but the generated model is wrong,
state what is wrong:
- Producing wrong results and/or decrease in accuracy
- Producing correct results, but the model is slower than expected (model generated from old converter)


**RNN conversion support**
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42632,error when installing for go from source.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Debian 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
source
- TensorFlow version:
commit 2d0592a000989c28e94dbc6efabe6c3be54762e1
- Python version:
Python 3.7.3
- Installed using virtualenv? pip? conda?:
comiled from source, following [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/go/README.md)
- Bazel version (if compiling from source):
bazel 3.1.0
- GCC/Compiler version (if compiling from source):
gcc (Debian 8.3.0-6) 8.3.0
- CUDA/cuDNN version:
no idea
- GPU model and memory:



**Describe the problem**
on step 4 of the instructions (`go generate github.com/tensorflow/tensorflow/tensorflow/go/op`) I got:
```
google/protobuf/any.proto: File not found.
google/protobuf/duration.proto: File not found.
tensorflow/core/protobuf/autotuning.proto:10:1: Import ""google/protobuf/any.proto"" was not found or had errors.
tensorflow/core/protobuf/autotuning.proto:11:1: Import ""google/protobuf/duration.proto"" was not found or had errors.
tensorflow/core/protobuf/autotuning.proto:61:3: ""google.protobuf.Duration"" is not defined.
tensorflow/core/protobuf/autotuning.proto:74:3: ""google.protobuf.Any"" is not defined.
../genop/generate.go:19: running ""bash"": exit status 1
op/generate.go:17: running ""go"": exit status 1
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
go get -d github.com/tensorflow/tensorflow/tensorflow/go

cd ${GOPATH}/src/github.com/tensorflow/tensorflow
./configure
bazel build -c opt //tensorflow:libtensorflow.so

sudo cp ${GOPATH}/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow.so /usr/local/lib
sudo cp ${GOPATH}/src/github.com/tensorflow/tensorflow/bazel-bin/tensorflow/libtensorflow_framework.so.2 /usr/local/lib/libtensorflow_framework.so

go generate github.com/tensorflow/tensorflow/tensorflow/go/op
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42631,Embedding layer doesn't follow mixed_precision policy by default,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro 1909 OS build 18363.1016
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.6
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Embedding layer's output is float32 despite mixed_bfloat16 policy

**Describe the expected behavior**
Embedding layer's output should tie to mixed_bfloat16 policy

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.mixed_precision import experimental as mixed_precision
policy = mixed_precision.Policy('mixed_bfloat16')
mixed_precision.set_policy(policy)
inputs = tf.keras.Input(shape=(100,), name='digits')
emb = tf.keras.layers.Embedding(100, 100)
dense = tf.keras.layers.Dense(100)

outputs = emb(inputs) + dense(inputs) # <= the error is here (adding two different types)

model = tf.keras.Model(inputs=inputs, outputs=outputs)
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])
input = tf.random.uniform(shape=[1,25], maxval=100, dtype=tf.int32)
hist = model.fit(input, input, epochs=1, steps_per_epoch=1,verbose=0)

```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

> ValueError: Tensor conversion requested dtype float32 for Tensor with dtype bfloat16: <tf.Tensor 'dense/Identity:0' shape=(None, 100) dtype=bfloat16>

"
42629,Size 1 must be non-negative error when using boolean mask tensorflow,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Maybe?
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.4 LTS
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.1.0
- Python version: 3.6.8
- CUDA/cuDNN version: N/A using IPU, poplar version 1.2.100
- GPU model and memory: MK1 IPU

**Describe the current behavior**

When working with too big arrays, boolean crashes probably because of using int32 internally for indexing.
If I change the first dimension in the example below to 20000, processing is not an issue.

**Describe the expected behavior**

The input arrays for the boolean mask function have the same size as the expected output of the boolean mask.
Hence, I would expect that either the inputs objects get not created with an error that their size exceeds the limits or that it just works. Also, it would be better to have an internal size check to provide more meaningful output.

**Standalone code to reproduce the issue**

```
import tensorflow as tf
t_vector = tf.ones([8,35583,100000], dtype=tf.float32)
acceptance_vector = tf.cast(tf.ones([35583, 100000]), tf.bool)
eval_vector = tf.boolean_mask(t_vector, acceptance_vector, axis=1)
```

**Other info / logs**

[boolean_mask_error.log](https://github.com/tensorflow/tensorflow/files/5119308/boolean_mask_error.log)



"
42627,tf.keras.activations.deserialize document refers `x` as a parameter,"
## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/activations/deserialize

## Description of issue (what needs changing):

### Clear description

The document of `tf.keras.activations.deserialize` refers `x` as a parameter in the **Argument** section. But it is not in the signature and not accepted by the function.


![image](https://user-images.githubusercontent.com/24580222/91073972-e6d47980-e609-11ea-835d-4a1c17ead171.png)



### Usage example

~~~python
import tensorflow as tf
tf.keras.activations.deserialize(x='softmax')
~~~

Running the code above gives exception:

~~~
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: deserialize() got an unexpected keyword argument 'x'
~~~


## System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Mojave 10.14
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 2.0.0
- **Python version**: 3.6.9"
42623,Retrain a TensorFlow model from a .pb file,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: MacOS Catalina
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.15.0 
- Python version: 3.6

**I was trying to retrain a TensorFlow model from a .pb file**. I'm using the following function to retrieve it and load the graph in Python:

```
# Load protobuf as graph, given filepath
def load_pb(path_to_pb):
    with tf.gfile.GFile(path_to_pb, 'rb') as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())
    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def, name='')
        return graph
```
From here, I try to list its trainable variables and operations:
```
with tf.Session(graph=tf_graph) as sess:
    print(""Trainable variables: {}"".format(tf.trainable_variables())) 
    variables = [op for op in tf_graph.get_operations()]
    for var in variables:
        print(""{}"".format(var.name), end = ' ,')
```
This is the output from the above code:
[Code output][1]


As shown above, it says that there are no variables that can be trained, and when I try the following code to train the graph:

```
with tf.Session() as sess:
    random_input  = tf.convert_to_tensor(np.random.rand(1, 3, 2848, 4256)) # Input dimensions
    random_output = sess.run(random_input) 

    random_y0 = tf.convert_to_tensor(np.random.rand(1, 3, 2848, 4256))

    loss = tf.reduce_sum(tf.square(random_y0 - random_output))
    train = tf.train.GradientDescentOptimizer(1e-4).minimize(loss)

    sess.run(tf.global_variables_initializer())

    print(""Training"")

    for step in range(1):
        sess.run(train)

```

It gives me the error: 

```
train = tf.train.GradientDescentOptimizer(1e-4).minimize(loss)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_core/python/training/optimizer.py"", line 410, in minimize
    ([str(v) for _, v in grads_and_vars], loss))
ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables
```

I believe there is an issue with the conversion such that it is failing to find the training variables, could someone please clarify this? I can also provide the exact .pb file if needed. Thanks a lot!

  [1]: https://i.stack.imgur.com/SNkBk.png"
42622,Tensorflow lite: Failed to run on the given Interpreter,"I am using tflite for semantic segmentation. I have a model trained to segment objects from background, this model is trained on deeplab.
I have converted this model(frozen inference graph) into tflite format using the below code:
```
tflite_convert \
  --output_file=test.lite \
  --graph_def_file=frozen_inference_graph.pb \
  --input_arrays=ImageTensor \
  --output_arrays=SemanticPredictions \
  --input_shapes=1,513,513,3 \
  --inference_input_type=QUANTIZED_UINT8 \
  --inference_type=FLOAT \
  --mean_values=128 \
  --std_dev_values=128 
```
The model loads on android, but when I try to run inference it gives me this error:
```
Caused by:java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: tensorflow/lite/kernels/unpack.cc:71 output->type != input->type (FLOAT32 != INT32)
Node number 22 (UNPACK) failed to prepare.
```
How do I resove this error? Is there anyone who can solve my problem？
here is my model
[deeplabv3_513_mv2_gpu.zip](https://github.com/tensorflow/tensorflow/files/5118434/deeplabv3_257_mv_gpu.zip)


"
42621,CoordConv,"**System information**
- TensorFlow version 2.3.0 and everything latest and greatest:
- Are you willing to contribute it (Yes/No): Depends, there are already a few written by developers that might be a solution for this request.


**Describe the feature and the current behavior/state.**
Basically a gradient matrices before the convolutional operation (Conv1D, Conv2D, Conv3D) that gives more information about convolutional operation place in the image.

**Will this change the current api? How?**
I don't think so. CoordConv can be added as a separate layer before the convolutional operation.

**Who will benefit with this feature?**
In general, all the segmentation or detection task in which objects transition in the image is important

**Any Other info.**
Article: [https://arxiv.org/abs/1807.03247](https://arxiv.org/abs/1807.03247) and original implementation by authors [https://github.com/uber-research/CoordConv](https://github.com/uber-research/CoordConv)
Video explanation: [https://www.youtube.com/watch?v=8yFQc6elePA](https://www.youtube.com/watch?v=8yFQc6elePA)
Possible implementation: [https://github.com/mvoelk/keras_layers](https://github.com/mvoelk/keras_layers)
Similar feature issue/mention : #32222
"
42620,CUDA 11.0 Build with tensorflow master on Win 10 : Link error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10 Pro 2004 Edition
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3 (master)
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: Directly installed to C:\Python3\
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): MSVC VS 2019 v 16.7.2
- CUDA/cuDNN version: 11.0/cuDNN 8.0.2.39
- GPU model and memory: GTX 2080 (Cuda Compute : 7.5) 8 GB RAM on Graphics card, 32 GB on System



**Describe the problem**
At outset, I admit that the version 11.0 is **NOT** yet supported. However, since I managed to compile the C++ library with identical parameters successfully (issue #42340) , I attempted to compile for a Python tensorflow implementation. These ""issues"" being reported are just to help the team as the compilation results may be of some value to the build management team.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
The build process ran slowly but surely. Observations
 1. Selecting any of the noaws,nogcp,nohdf5 etc resulted into a build error as some variable or the other was not found to be duplicated.
However, not opting to reject any of these additional features continued the compilation smoothly.
2. The compilation went smooth, but the linking step resulted into fairly large count of warnings, and thereafter, failed since one extern variable not found. The output is in the attached file.

[py_build_results.txt](https://github.com/tensorflow/tensorflow/files/5117877/py_build_results.txt)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
42618,Pipenv EagerExecution,"I've written an object detection framework with Tensorflow 2 Keras and now I'm implementing some new functionalities. I noticed that eager_execution is not enabled if I run my code via `pipenv run python3 .../folder/main.py` ( tf tf.executing_eagerly() returns False) where as in `pipenv run python3 .... import tensorflow as tf tf.executing_eagerly()` returns True also in jupyter notebooks  tf tf.executing_eagerly() returns true.
Why does this behavior happen? I can't wrap my head around this."
42617,"`OOM when allocating tensor with shape[320000,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc`","`OOM when allocating tensor with shape[320000,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc`
I got this message while training CNN
Specifying batch_size 16, 32, 64, 128 did not help

_Originally posted by @Ritaprava95 in https://github.com/tensorflow/tensorflow/issues/16768#issuecomment-448324351_"
42616,Memory leak when using MultiWorkerMirroredStrategy for distributed training,"**System information**

- Have I written custom code: YES
- OS Platform and Distribution: CentOS 7.3
- TensorFlow installed from: pip
- TensorFlow version: 2.3.0
- Python version:3.7.7
- CPU ONLY

**Describe the current behavior**

When I use `MultiWorkerMirroredStrategy` for distributed training, as the number of training epochs increases, memory usage of tensorflow is also increasing, until beyond the memory limitation.

But the memory usage of stand-alone(not distributed) training is always stable.

Because I use cpu only for distributed training, I can't get any memory infomation from tensorboard using profiler.

**Standalone code to reproduce the issue**

Note that I don't know how to use `MultiWorkerMirroredStrategy` in `colab`, so I just give the reproduce steps here, and it's very easy.

1. Training Code (worker.py)

```python
import os
import json
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from absl import app, flags
import numpy as np

FLAGS = flags.FLAGS
flags.DEFINE_string(""logs"", ""logs"", ""logs dir"")
flags.DEFINE_integer(""index"", 0, ""worker index"")

class ThreeLayerMLP(keras.Model):
    def __init__(self, name=None):
        super().__init__(name=name)
        self.dense_1 = layers.Dense(32, activation='relu', name='dense_1')
        self.dense_2 = layers.Dense(16, activation='relu', name='dense_2')
        self.pred_layer = layers.Dense(
            1,
            activation='sigmoid',
            name='predictions',
        )

    def call(self, inputs, training=None):
        print(inputs.shape)
        x = self.dense_1(inputs)
        x = self.dense_2(x)
        return self.pred_layer(x)


def prepare_data():
    np.random.seed(0)
    x_train, y_train = (
        np.random.random((6000000, 31)),
        np.random.randint(2, size=(6000000, 1)),
    )

    x_val, y_val = (
        np.random.random((10000, 31)),
        np.random.randint(2, size=(10000, 1)),
    )

    return ((x_train, y_train), (x_val, y_val))


def main(argv):
    del argv  # Unused args
    tf_config = {
        ""cluster"": {
            ""worker"": [""ip1:12345"", ""ip2:12345""],
        },
        ""task"": {
            ""index"": FLAGS.index,
            ""type"": ""worker""
        }
    }
    os.environ[""TF_CONFIG""] = json.dumps(tf_config)
    print(json.loads(os.environ[""TF_CONFIG""]))
    # distributed strategy
    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()
    BATCH_SIZE_PER_REPLICA = 128
    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync
    print('Number of devices: %d' % strategy.num_replicas_in_sync)

    with strategy.scope():
        model = ThreeLayerMLP(name='3_layer_mlp')
        model.compile(
            loss=tf.keras.losses.BinaryCrossentropy(),
            optimizer=keras.optimizers.RMSprop(),
            metrics=[""AUC""],
        )

    tensorboard_callback = tf.keras.callbacks.TensorBoard(
        log_dir=FLAGS.logs,
        histogram_freq=10,
        update_freq=100,
    )

    ((x_train, y_train), (x_val, y_val)) = prepare_data()

    model.fit(
        x_train,
        y_train,
        epochs=100,
        batch_size=BATCH_SIZE,
        validation_data=(x_val, y_val),
        callbacks=[tensorboard_callback],
    )


if __name__ == '__main__':
    app.run(main)
```

2. Distributed training: change the `ip1` and `ip2` to your machine's ip in the codes above, and execute the command below seperately:

```shell
python worker.py --index=0
python worker.py --index=1
```

3. The memory change curve of distributed training in my machine is shown as below：
![image](https://user-images.githubusercontent.com/15494997/91021608-94ee1c80-e626-11ea-9adc-c775b3ff575a.png)

4. The memory usage of stand-alone training is only 3-4G."
42614,Request for Prebuilt C++ Library libtensorflow_cc.so (probably from CI artifacts),"**System information**
- TensorFlow v2.3.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
It has been lot of demand and lot of time consumption in building the TensorFlow C++ Library (monolithic), which could be saved if TensorFlow provides official `libtensorflow_cc` (C++ lib) as well along with `libtensorflow.so` (C lib) as there are lots of functionality which one would want from C++ lib and due to the fact that it is not available anywhere, one has to built it himself for this, which I believe Tensorflow official CI is already building (cache) so if we have CI package available for download and use for all various OS, this could reduce the deployment efforts for anyone to great extend. 

I have built the lib manually for various systems by following steps provided below as I wanted to have single lib + headers to deploy with other projects.

```bash
bazel build -c opt --config=monolithic //tensorflow:libtensorflow_cc.so //tensorflow:install_headers
```

In addition to that, if TensorFlow can provide complete package which includes `libtensorflow_cc.so`, `libtensorflow_lite.so`, `libtensorflow.so`, headers and `tools` along with Bazel BUILDs (i.e. package) then one could speed up his own CI compilation time and lot more as all he has to do is to fetch package and link the binaries against it. 

Package for each OS (i.e. `linux, windows, macOS`)

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Almost everyone who wants to use C++ APIs for deployment

**Any Other info.**
none"
42613,TF 2.3 Saved model | Experimental features not working properly,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.3.0
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**:
While serving a TF 2.3 trained saved model created with tf.keras.layers.experimental.preprocessing features, TMS(Tensorflow model server ) gives error on prediction. However predict method on the model runs absolutely fine without any error, seems to be something specific to TMS. Does TMS currently not support the new experimental pre-processing features that were added in TF 2.3.

I also noticed that saved_model_cli also throws error that Op type not registered 'DenseBincount' in binary but that could be related to saved_model_cli not yet updated in Colab with TF 2.3 support.

Bug has also been opened on TMS but it seems that the bug is not in serving but in TF runtime: https://github.com/tensorflow/serving/issues/1720#issuecomment-678842670 ( FYI: @christisg  )

**Describe the expected behavior**:
Saved model should work properly in Tensorflow serving and also show up properly in saved_model_cli

**Standalone code to reproduce the issue**
Run GIST - https://gist.github.com/rafiqhasan/6f00aecf1feafd83ba9dfefef8907ee8#file-dl-e2e-taxi-dataset-tf2-keras-ipynb

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
import json
import requests

#Create payload
data_py = {""instances"":[{'dropoff_latitude': [41.920452],
                         'dropoff_longitude': [-87.679955],
                         'pickup_latitude': [41.952823],
                         'pickup_longitude': [-87.653244],
                         'trip_start_day': [""1""],
                         'trip_start_hour': [""5""],
                         'trip_start_month': [""6""]}]}

data = json.dumps(data_py)
print(""payload: "", data)

#Run request on TMS
headers = {""content-type"": ""application/json""}
json_response = requests.post('http://localhost:8507/v1/models/model:predict', data=data, headers=headers)
json_response.text
```

Error message:

```
{
    ""error"": ""Missing 0-th output from {{node functional_3/category_encoding_13/bincount/DenseBincount}}""
}
```

But model prediction standalone code works:
```
# ##Prediction on model
# ## BUT HERE ALL FEATURES HAVE TO BE PASSED, EVEN THE Calculated ones
data = tf.data.Dataset.from_tensor_slices({'dropoff_latitude': [[41.920452]],
                         'dropoff_longitude': [[-87.679955]],
                         'pickup_latitude': [[41.952823]],
                         'pickup_longitude': [[-87.653244]],
                         'trip_start_day': [[""1""]],
                         'trip_start_hour': [[""5""]],
                         'trip_start_month': [[""6""]],
                         'distance':[[0.02]]})

m_.predict(data)
```"
42612,Mismatch in number of weights when loading quantized model (activation layer),"**Describe the bug**

Saving and subsequently loading a quantized model results in the following error:
```
Traceback (most recent call last):
  File ""test.py"", line 37, in <module>
    model = tf.keras.models.load_model('MinimalExample.h5')
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py"", line 182, in load_model
    return hdf5_format.load_model_from_hdf5(filepath, custom_objects, compile)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 181, in load_model_from_hdf5
    load_weights_from_hdf5_group(f['model_weights'], model.layers)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 706, in load_weights_from_hdf5_group
    str(len(weight_values)) + ' elements.')
ValueError: Layer #121 (named ""quant_y_a_relu"" in the current model) was found to correspond to layer quant_y_a_relu in the save file. However the new layer quant_y_a_relu expects 3 weights, but the saved weights have 1 elements.
```

The error can be reproduced using this code (test.py):
**Please note that there is no error when setting `quantize_model = False`**
```
import tensorflow as tf
import tensorflow_model_optimization as tfmot
from tensorflow_model_optimization.python.core.quantization.keras.default_8bit import default_8bit_quantize_configs

quantize_model = True

# Build model
base_model = tf.keras.applications.MobileNetV2(input_shape=(480,640,3),classes=2,weights='imagenet',include_top=False)
x = base_model.get_layer('block_12_add').output

y_a = tf.keras.layers.Conv2D(256,1,padding='same',dilation_rate=1,use_bias=False,kernel_initializer='he_normal',name='y_a_conv2d')(x)
y_a = tf.keras.layers.BatchNormalization(name='y_a_bn')(y_a)
y_a = tf.keras.layers.Activation('relu',name='y_a_relu')(y_a)

y_b = tf.keras.layers.Conv2D(256,3,padding='same',dilation_rate=6,use_bias=False,kernel_initializer='he_normal',name='y_b_conv2d')(x)
y_b = tf.keras.layers.BatchNormalization(name='y_b_bn')(y_b)
y_b = tf.keras.layers.Activation('relu',name='y_b_relu')(y_b)

output_tensor = tf.keras.layers.Concatenate(name='aspp_concat')([y_a,y_b])

model = tf.keras.models.Model(inputs=base_model.input,outputs=output_tensor,name='MinimalExample')

# Save model
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metrics = ['accuracy']
if quantize_model:
    with tf.keras.utils.custom_object_scope({'NoOpQuantizeConfig':default_8bit_quantize_configs.NoOpQuantizeConfig}):
        model = tfmot.quantization.keras.quantize_model(model)
model.compile(optimizer=optimizer,loss=loss,metrics=metrics)
model.save('MinimalExample.h5')
del model

# Load model
if quantize_model:
    with tfmot.quantization.keras.quantize_scope():
        model = tf.keras.models.load_model('MinimalExample.h5')
else:
    model = tf.keras.models.load_model('MinimalExample.h5')

# Convert model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Export model
open('MinimalExample.tflite','wb').write(tflite_model)
```

**System information**

TensorFlow version (installed from source or binary): 2.3.0 (Docker image)

TensorFlow Model Optimization version (installed from source or binary): 0.4.1 (Docker image)

Python version: 3.6.9 (Docker image)

**Describe the expected behavior**
Code should not crash

**Describe the current behavior**
Code crashes"
42611,how to change the specific value of tensor?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


I want to change the 
`D.object_for_each_prior-> tf.Tensor([0 0 0 ... 0 0 0], shape=(8732,), dtype=int64)
`want to change the position 8700 and 8701 to the 1 value 
since the assign value is no longer available how should I assign spicific value to the specific location? "
42610,"I've got a build error for ""Build a handwritten digit classifier app with TensorFlow Lite""","**System information**
- OS Platform: WIndows10
- Mobile device: AVD (Android 10.0(Google Play) API:29)
- TensorFlow installed from (source or binary): Google Colabo
- TensorFlow version: 2.3.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: Preinstalled in Google Colabo
- Bazel version (if compiling from source): 
> Android Studio 4.0
> Build #AI-193.6911.18.40.6514223, built on May 21, 2020
> Runtime version: 1.8.0_242-release-1644-b01 amd64
> VM: OpenJDK 64-Bit Server VM by JetBrains s.r.o
> Windows 10 10.0
> GC: ParNew, ConcurrentMarkSweep
> Memory: 1237M
> Cores: 8
> Registry: ide.new.welcome.screen.force=true
> Non-Bundled Plugins: org.jetbrains.kotlin

**Describe the problem**
I followed TF lite trial site and at step 5(https://codelabs.developers.google.com/codelabs/digit-classifier-tflite/#5), 
after pressing a ""Run 'app'"" button on Android Studio, I got the following error in Android Studio.

Only safe (?.) or non-null asserted (!!.) calls are allowed on a nullable receiver of type interpreter?
Following **interpreter** the right side of val inputShape = is highlighted.

private fun initializeInterpreter() {

    // TODO: Read the model input shape from model file.
    // Read input shape from model file
    val inputShape = interpreter.getInputTensor(0).shape()
    inputImageWidth = inputShape[1]
    inputImageHeight = inputShape[2]

I'm a newbie of Android Studio, thus I have not been able to find any wrong configuration in it.
Could you give me any clue?
Thank you.

**Any other info / logs**
At the very first try, when I press the ""Run 'app'"" button, Android Studio attempted to install something version 28.0.3(maybe SDK?). However, it was not able to fetch the version.
I have never seen the same message, even if I delete all ""example_master"" folder and change the location and try again.

One more info.
Although I don't know the difference b/w 'start' project and 'finish' project, I tried 'finish' project w/ same ""mnist.tflite"" too.
Then, Android studio spitted **""Compatible side by side NDK version was not found. Default is 21.0.6113669.""** error.


My build.gradle is below by the way.
> apply plugin: 'com.android.application'
> apply plugin: 'kotlin-android'
> apply plugin: 'kotlin-android-extensions'
> 
> android {
>     compileSdkVersion 29
>     defaultConfig {
>         applicationId ""org.tensorflow.lite.codelabs.digitclassifier""
>         minSdkVersion 21
>         targetSdkVersion 29
>         versionCode 1
>         versionName ""1.0""
>         testInstrumentationRunner ""androidx.test.runner.AndroidJUnitRunner""
>     }
> 
>     // TODO: Add an option to avoid compressing TF Lite model file
>     aaptOptions {
>         noCompress ""tflite""
>     }
> 
>     buildTypes {
>         release {
>             minifyEnabled false
>             proguardFiles getDefaultProguardFile('proguard-android-optimize.txt'), 'proguard-rules.pro'
>         }
>     }
> }
> 
> // Sanity check if you have trained and downloaded TF Lite model.
> preBuild.doFirst {
>     assert file(""./src/main/assets/mnist.tflite"").exists() :
>             ""mnist.tflite not found. Make sure you have trained and "" +
>                     ""downloaded your TensorFlow Lite model to assets/ folder""
> }
> 
> dependencies {
>     implementation fileTree(dir: 'libs', include: ['*.jar'])
>     implementation ""org.jetbrains.kotlin:kotlin-stdlib-jdk7:$kotlin_version""
> 
>     // Support Libraries
>     implementation 'androidx.appcompat:appcompat:1.1.0'
>     implementation 'androidx.core:core-ktx:1.1.0'
>     implementation 'androidx.constraintlayout:constraintlayout:1.1.3'
> 
>     // AndroidDraw Library
>     implementation 'com.github.divyanshub024:AndroidDraw:v0.1'
> 
>     // Task API
>     implementation ""com.google.android.gms:play-services-tasks:17.0.0""
> 
>     //TODO: Add TF Lite
>     implementation 'org.tensorflow:tensorflow-lite:2.0.0'
> 
>     testImplementation 'junit:junit:4.12'
>     androidTestImplementation 'androidx.test:runner:1.2.0'
>     androidTestImplementation 'androidx.test.espresso:espresso-core:3.2.0'
> }
I changed TF version, but each of them ended up w/ same error.
    implementation 'org.tensorflow:tensorflow-lite:2.3.0'
    implementation 'org.tensorflow:tensorflow-lite:2.2.0'

"
42609,tensorflow is not installing,"ImportError                               Traceback (most recent call last)
~\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59 

~\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

C:\ProgramData\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    241         else:
--> 242             return load_dynamic(name, filename, file)
    243     elif type_ == PKG_DIRECTORY:

C:\ProgramData\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    341             name=name, loader=loader, origin=path)
--> 342         return _load(spec)
    343 

ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>
----> 1 import tensorflow as tf

~\tensorflow\__init__.py in <module>
     39 import sys as _sys
     40 
---> 41 from tensorflow.python.tools import module_util as _module_util
     42 from tensorflow.python.util.lazy_loader import LazyLoader as _LazyLoader
     43 

~\tensorflow\python\__init__.py in <module>
     48 import numpy as np
     49 
---> 50 from tensorflow.python import pywrap_tensorflow
     51 
     52 # Protocol buffers

~\tensorflow\python\pywrap_tensorflow.py in <module>
     67 for some common reasons and solutions.  Include the entire stack trace
     68 above this error message when asking for help."""""" % traceback.format_exc()
---> 69   raise ImportError(msg)
     70 
     71 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\lenovo\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\lenovo\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\lenovo\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.


using windows 8.1(64-bit)
python 3.8 Anaconda Navigator
trying to install tensorflow




"
42608,CUDA_ERROR_LAUNCH_FAILED - possible memory issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  I have modified an example keras project (https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) by just removing the dataset size loaded.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Windows 10 Pro

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  N/A

- TensorFlow installed from (source or binary):
  tensorflow-gpu installed through pip

- TensorFlow version (use command below):
   Git Version: v2.3.0-rc2-23-gb36436b087
   Version: 2.3.0

- Python version:
   3.8.5 64-bit

- Bazel version (if compiling from source):
   N/A

- GCC/Compiler version (if compiling from source):
   N/A

- CUDA/cuDNN version:
   CUDA: 10.1
   cuDNN: 7.6

- GPU model and memory:
   GPU: GTX 1080 Ti
   Memory: 11GB (9.5 GB effective from log)

-System Memory:
  32GB

**Describe the current behavior**
Using the script here (https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py) to start learning a bit about LSTM's and Sequence2Sequence learning. The result is when training starts I have ~16 GB allocated by CPU side over the epoch it gradually raises itself until it nearly maxes out the system memory. Sometimes I can complete a few epochs but eventually I get the following error:

""CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1892): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-08-23 21:11:43.812986: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure""

My assumption is this has to do something with the memory allocation going a bit crazy and eventually triggering an OOM under the hood, but I'm not 100% sure how to get anymore data to help determine what is going on.

I have tested it at 100k items and everything works as expected, but not 124k. My assumption is something is moving everything over to the GPU and thus crazy allocations are happening once the entire dataset won't fit into GPU memory.

 If there is something dumb I didn't do to prevent the memory from scaling crazy I apologize, but hadn't got an answer on stackoverflow so figured asking here might be better.

**Describe the expected behavior**
I would expect the memory usage to scale only on batch_size, not total dataset size. I would expect if I can complete an epoch once I should be be able to complete the remaining ones without an error. I have built a generator to see if maybe the issue was all the memory being allocated, but that also didn't fix the issue. I could see adding more memory but seems like the issue is more that memory use scales by total dataset size. When doing image training I haven't ever seen anything like that, only with this LSTM network.

**Standalone code to reproduce the issue**
https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py 
Run this script but set the total size to 124000.

Output
(.venv) PS C:\Dev\CompressedMemory>  cd 'c:\Dev\CompressedMemory'; & 'c:\Dev\CompressedMemory\.venv\Scripts\python.exe' 'c:\Users\ChaseRLewis\.vscode\extensions\ms-python.python-2020.8.103604\pythonFiles\lib\python\debugpy\launcher' '50370' '--' 'c:\Dev\CompressedMemory\train.py' 
2020-08-23 20:59:49.133632: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
v2.3.0-rc2-23-gb36436b087
2.3.0
Number of samples: 124325
Number of unique input tokens: 91
Number of unique output tokens: 114
2020-08-23 21:08:54.280683: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library nvcuda.dll
2020-08-23 21:08:54.314777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2020-08-23 21:08:54.322233: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-23 21:08:54.355314: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-23 21:08:54.386173: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-23 21:08:54.398183: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-23 21:08:54.419257: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-23 21:08:54.439330: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-23 21:08:54.550070: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-08-23 21:08:54.554182: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-23 21:08:54.567696: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-23 21:08:54.662347: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b911c0e2b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-23 21:08:54.667058: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-23 21:08:54.671211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:0a:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 11.00GiB deviceMemoryBandwidth: 451.17GiB/s
2020-08-23 21:08:54.686125: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudart64_101.dll
2020-08-23 21:08:54.699127: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-23 21:08:54.702580: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cufft64_10.dll
2020-08-23 21:08:54.716295: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library curand64_10.dll
2020-08-23 21:08:54.719900: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusolver64_10.dll
2020-08-23 21:08:54.733723: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cusparse64_10.dll
2020-08-23 21:08:54.737621: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
2020-08-23 21:08:54.749712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-23 21:08:55.792500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-23 21:08:55.796084: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0
2020-08-23 21:08:55.798310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N
2020-08-23 21:08:55.801223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9525 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1)
2020-08-23 21:08:55.821954: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2b938950590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-23 21:08:55.832406: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
Epoch 1/300
2020-08-23 21:09:07.991819: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cublas64_10.dll
2020-08-23 21:09:08.422643: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library cudnn64_7.dll
3108/3108 [==============================] - ETA: 0s - loss: 0.21842020-08-23 21:11:43.812127: E tensorflow/stream_executor/dnn.cc:616] CUDNN_STATUS_INTERNAL_ERROR
in tensorflow/stream_executor/cuda/cuda_dnn.cc(1892): 'cudnnRNNForwardTraining( cudnn.handle(), rnn_desc.handle(), model_dims.max_seq_length, input_desc.handles(), input_data.opaque(), input_h_desc.handle(), input_h_data.opaque(), input_c_desc.handle(), input_c_data.opaque(), rnn_desc.params_handle(), params.opaque(), output_desc.handles(), output_data->opaque(), output_h_desc.handle(), output_h_data->opaque(), output_c_desc.handle(), output_c_data->opaque(), workspace.opaque(), workspace.size(), reserve_space.opaque(), reserve_space.size())'
2020-08-23 21:11:43.812986: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2020-08-23 21:11:43.839269: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at cudnn_rnn_ops.cc:1517 : Internal: Failed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 114, 256, 1, 280, 32, 256]
2020-08-23 21:11:43.853074: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:220] Unexpected Event status: 1"
42607,Failed precondition: Could not find required function definition __inference_tf_data_experimental_parallel_interleave_<class 'abc.ABCMeta'>,"Hey, I am trying to convert the code from tensorflow=1.9 to tensorflow=1.14 and I am getting the following error: 

Failed precondition: Could not find required function definition __inference_tf_data_experimental_parallel_interleave_<class 'abc.ABCMeta'>_40
	 [[{{node OptimizeDataset/create_dataset/ExperimentalParallelInterleaveDataset}}]]
2020-08-23 15:34:01.711575: W tensorflow/core/framework/op_kernel.cc:1502] OP_REQUIRES failed at iterator_ops.cc:973 : Failed precondition: Could not find required function definition __inference_tf_data_experimental_parallel_interleave_<class 'abc.ABCMeta'>_40
	 [[{{node OptimizeDataset/create_dataset/ExperimentalParallelInterleaveDataset}}]]
Traceback (most recent call last):
  File ""/home/jay/Documents/Reconstraction/DeepRecon/dl-inavJayV2/TV2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1356, in _do_call
    return fn(*args)
  File ""/home/jay/Documents/Reconstraction/DeepRecon/dl-inavJayV2/TV2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1341, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/jay/Documents/Reconstraction/DeepRecon/dl-inavJayV2/TV2/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1429, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.FailedPreconditionError: 2 root error(s) found.
  (0) Failed precondition: Could not find required function definition __inference_tf_data_experimental_parallel_interleave_<class 'abc.ABCMeta'>_40
	 [[{{node OptimizeDataset/create_dataset/ExperimentalParallelInterleaveDataset}}]]
	 [[OneShotIterator]]
	 [[IteratorGetNext/_711]]
  (1) Failed precondition: Could not find required function definition __inference_tf_data_experimental_parallel_interleave_<class 'abc.ABCMeta'>_40
	 [[{{node OptimizeDataset/create_dataset/ExperimentalParallelInterleaveDataset}}]]
	 [[OneShotIterator]]


May you please me with this issue? "
42606,[Bug] apply_gradients core dump when variable shape is [0] on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.7.5
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: GeForce RTX 2080

**Describe the current behavior**
Variable with empty shape **core dumps** on GPU but works fine on CPU.

**Describe the expected behavior**
Work on both CPU / GPU or fail gracefully.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
a = tf.Variable(tf.random.normal([3]))
b = tf.Variable(tf.random.normal([0]))
Adam = tf.optimizers.Adam
optimizer = Adam()
with tf.GradientTape() as t:
    c = tf.concat([a, b], axis=0)
grad = t.gradient(c, b)
optimizer.apply_gradients(zip([grad], [b]))
```
core dumped with: `F ./tensorflow/core/util/gpu_launch_config.h:129] Check failed: work_element_count > 0 (0 vs. 0)`
"
42605,Difference in batchnorm outputs when converting from TF model to Pytorch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): 2.2.2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I described the issue here. https://discuss.pytorch.org/t/difference-in-batchnorm-outputs-when-converting-from-tf-model-to-pytorch/93811"
42604,Wrong link for JNI download on website,"https://www.tensorflow.org/install/lang_java


The JNI download link on the Java page is still points to 1.14.0, it should be updated to 2.3.0.

page source:
```
<td>Linux CPU only</td>
  | <td class=""devsite-click-to-copy""><a href=""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-1.14.0.tar.gz"">https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-cpu-linux-x86_64-2.3.0.tar.gz</a></td>
  | </tr>
  | <tr>
  | <td>Linux GPU support</td>
  | <td class=""devsite-click-to-copy""><a href=""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-linux-x86_64-1.14.0.tar.gz"">https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-gpu-linux-x86_64-2.3.0.tar.gz</a></td>
  | </tr>
 ```"
42603,"NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.","Hi, I pretty new at this. The following is  my code and the error. Thank you for your help. Posted on stackoverflow too.

`
#WITH TPU

latent_dim = 2 #number of latent variables to learn
hidden_size = 32
input_dim = x_train.shape[1]
latent_dim = 3 # d, dimensionality of the latent code t.
intermediate_dim = 256 # Size of the hidden layer.


def create_model():
  x = Input(shape=(input_dim,))
  t = BatchNormalization()(x)
  t = Dense(intermediate_dim, activation='relu' , name='encoder_hidden')(t)
  t = BatchNormalization()(t)

  z_mean = Dense(latent_dim, name='z_mean')(t)
  z_log_var = Dense(latent_dim, name='z_log_var')(t)

  def sampling(args):
      z_mean, z_log_var = args
      epsilon = tf.random.normal(shape=tf.shape(z_mean), mean=0., stddev=1.,name=""epsilon"")
      return z_mean + tf.exp(z_log_var / 2) * epsilon

  z = Lambda(sampling, name='z_sampled')([z_mean, z_log_var])

  t = Dense(intermediate_dim, activation='relu', name='decoder_hidden')(z)

  decoded_mean = Dense(input_dim, activation=None, name='decoded_mean')(t)

  def kl_loss(y_true, y_pred):
    return - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)

  def rec_loss(y_true, y_pred):
    return tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)   

  def vae_loss(x, decoded_mean):
    rec_loss = tf.reduce_sum(tf.square(x - decoded_mean), axis=-1)
    kl_loss = - 0.5 * tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)
    return K.mean((rec_loss + kl_loss) / 2)

  vae = Model(x, decoded_mean)

  return vae
  

with strategy.scope():
  vae = create_model()
  #vae.compile(optimizer=tf.keras.optimizers.Nadam(), loss=negloglik)
  vae.compile(optimizer=Adam(lr=1e-2), loss=vae_loss, metrics=[rec_loss, kl_loss])
  vae.summary()
  n_epochs = 30
  batch_size = 128

  early_stopping = EarlyStopping(monitor='loss', patience=10, min_delta=1e-5) #stop training if loss does not decrease with at least 0.00001
  reduce_lr = ReduceLROnPlateau(monitor='loss', patience=5, min_delta=1e-5, factor=0.2) #reduce learning rate (divide it by 5 = multiply it by 0.2) if loss does not decrease with at least 0.00001

  callbacks = [early_stopping, reduce_lr]

  tf.config.experimental_run_functions_eagerly(True)

  tf_train = tf.data.Dataset.from_tensor_slices((x_train, x_train)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))
  tf_val = tf.data.Dataset.from_tensor_slices((x_val, x_val)).batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE).shuffle(int(10e4))

  hist = vae.fit(tf_train,
                 validation_data=tf_val,
                 shuffle=True,
                 verbose=0,
                 #batch_size=batch_size, 
                 epochs=n_epochs,
                 callbacks=callbacks)
  
  plot_loss(hist)

`
`
Model: ""functional_39""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_21 (InputLayer)           [(None, 30)]         0                                            
__________________________________________________________________________________________________
batch_normalization_40 (BatchNo (None, 30)           120         input_21[0][0]                   
__________________________________________________________________________________________________
encoder_hidden (Dense)          (None, 256)          7936        batch_normalization_40[0][0]     
__________________________________________________________________________________________________
batch_normalization_41 (BatchNo (None, 256)          1024        encoder_hidden[0][0]             
__________________________________________________________________________________________________
z_mean (Dense)                  (None, 3)            771         batch_normalization_41[0][0]     
__________________________________________________________________________________________________
z_log_var (Dense)               (None, 3)            771         batch_normalization_41[0][0]     
__________________________________________________________________________________________________
z_sampled (Lambda)              (None, 3)            0           z_mean[0][0]                     
                                                                 z_log_var[0][0]                  
__________________________________________________________________________________________________
decoder_hidden (Dense)          (None, 256)          1024        z_sampled[0][0]                  
__________________________________________________________________________________________________
decoded_mean (Dense)            (None, 30)           7710        decoder_hidden[0][0]             
==================================================================================================
Total params: 19,356
Trainable params: 18,784
Non-trainable params: 572
__________________________________________________________________________________________________
/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/dataset_ops.py:3350: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.
  ""Even though the tf.config.experimental_run_functions_eagerly ""
---------------------------------------------------------------------------
NotImplementedError                       Traceback (most recent call last)
<ipython-input-34-57c2ae5c49b9> in <module>()
     68                  #batch_size=batch_size,
     69                  epochs=n_epochs,
---> 70                  callbacks=callbacks)
     71 
     72   plot_loss(hist)

5 frames
/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/tpu_strategy.py in validate_run_function(fn)
     95       and not (callable(fn) and isinstance(fn.__call__, def_function.Function)):
     96     raise NotImplementedError(
---> 97         ""TPUStrategy.run(fn, ...) does not support pure eager ""
     98         ""execution. please make sure the function passed into ""
     99         ""`strategy.run` is a `tf.function` or ""

NotImplementedError: TPUStrategy.run(fn, ...) does not support pure eager execution. please make sure the function passed into `strategy.run` is a `tf.function` or `strategy.run` is called inside a `tf.function` if eager behavior is enabled.
`"
42602,[Bug] tf.sqrt Inconsistent behaviour,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: 3.5
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: V10.1.243
- GPU model and memory: RTX2070 MaxQ, 8GB

**Describe the current behavior**

When I run this code, I get the result and gradients

```py
x = tf.Variable([[1.0, 0.0]])

with tf.GradientTape() as tape:
    x = tf.square(x)
    x = tf.sqrt(x)
    z = tf.reduce_sum(x, axis=-1)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss,x))
'''
Output:
[1] 0.5
[[1 1]]
'''
```

However, when I refactor the three lines inside a function, the gradient becomes NaN

```py
@tf.function # Happens in both eager and graph
def fun(v):
    v = tf.square(v)
    v = tf.sqrt(v)
    return tf.reduce_sum(v, axis=-1)

x = tf.Variable([[1.0, 0.0]])
with tf.GradientTape() as tape:
    z = fun(x)
    loss = tf.nn.l2_loss(z)

tf.print(z, loss)
tf.print(tape.gradient(loss, x))

'''
Output:
[1] 0.5
[[1 nan]]
'''
```

Changing `x = tf.sqrt(x)` to `x = tf.sqrt(x + 1e-7)` makes the gradient non NaN.

**Describe the expected behavior**

Either result is acceptable as long as the behaviour is consistent.
"
42601,The command line is too long,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10 education
- TensorFlow installed from (source or binary):  source
- TensorFlow version:  r1.14
- Python version:  3.7.4
- Installed using virtualenv? pip? conda?:  conda
- Bazel version (if compiling from source):  0.25.2
- GCC/Compiler version (if compiling from source): vs2017
- CUDA/cuDNN version:  10.0/7.4
- GPU model and memory:  NVIDIA GeForce RTX2070 with Max-Q Design 16GB



**Describe the problem**
Error happend when running:
D:/ProgramData/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/bin/tensorflow/tf_python_api_gen_v1.genrule_script.sh
The error is: The command line is too long.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Open Developer Command Prompt for VS 2017.
2. Run cmd lines:
     cmd.exe ""/K"" D:\ProgramData\Anaconda3\Scripts\activate.bat
     conda activate tf_gpu114
     bazel --output_user_root=d:/bazel_out build  --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
(tf_gpu114) D:\newTf114\tensorflow>bazel --output_user_root=d:/bazel_out build  --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: D:/newtf114/tensorflow/tensorflow/python/BUILD:3469:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: D:/newtf114/tensorflow/tensorflow/python/BUILD:102:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/python:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: D:/newtf114/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: D:/newtf114/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: D:/newtf114/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: D:/newtf114/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: D:/newtf114/tensorflow/tensorflow/contrib/BUILD:12:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: D:/newtf114/tensorflow/tensorflow/BUILD:745:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command
  cd D:/bazel_out/73kxojlr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=D:/Program Files/nvidia gpu computing toolkit/cuda/v10.0
    SET PATH=D:\ProgramData\msys64\usr\bin;D:\ProgramData\msys64\bin;D:\ProgramData\Anaconda3;D:\ProgramData\Anaconda3\Library\mingw-w64\bin;D:\ProgramData\Anaconda3\Library\usr\bin;D:\ProgramData\Anaconda3\Library\bin;D:\ProgramData\Anaconda3\Scripts;D:\ProgramData\Anaconda3\bin;D:\ProgramData\Anaconda3\condabin;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX86\x86;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft SDKs\TypeScript\3.1;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;D:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x86;C:\Program Files (x86)\Windows Kits\10\bin\x86;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework\v4.0.30319;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.0\bin;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.0\libnvvp;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.1\bin;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.1\libnvvp;d:\ProgramData\Anaconda3;d:\ProgramData\Anaconda3\Library\mingw-w64\bin;d:\ProgramData\Anaconda3\Library\usr\bin;d:\ProgramData\Anaconda3\Library\bin;d:\ProgramData\Anaconda3\Scripts;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\bolix\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm;C:\Program Files\Microsoft SQL Server\120\Tools\Binn;C:\Program Files\CMake\bin;D:\Program Files\MATLAB\R2018b\runtime\win64;D:\Program Files\MATLAB\R2018b\bin;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\dotnet;C:\Program Files\Microsoft SQL Server\130\Tools\Binn;D:\Program Files\Git\cmd;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;D:\ProgramData\msys64\usr\bin;D:\ProgramData\msys64\usr\bin\bash.exe;D:\ProgramData\msys64;C:\Users\bolix\AppData\Local\Microsoft\WindowsApps;D:\Program Files\JetBrains\PyCharm Community Edition 2019.2.4\bin;D:\texlive\2019\bin\win32;C:\Users\bolix\.dotnet\tools;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PYTHON_BIN_PATH=d:/ProgramData/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=d:/ProgramData/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TF_NEED_TENSORRT=0
  D:/ProgramData/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/bin/tensorflow/tf_python_api_gen_v1.genrule_script.sh
Execution platform: @bazel_tools//platforms:host_platform
The command line is too long.（）
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 21.007s, Critical Path: 12.47s
INFO: 0 processes.
FAILED: Build did NOT complete successfully
"
42600,ImportError: DLL load failed: The specified module could not be found.,"from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""g:/Gaze_Dl/fast1.py"", line 6, in <module>
    from tensorflow.keras.models import load_model
  File ""C:\Users\Acer\anaconda3\envs\mytfenv\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\Acer\anaconda3\envs\mytfenv\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
    from tensorflow.python.eager import context
  File ""C:\Users\Acer\anaconda3\envs\mytfenv\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
    from tensorflow.python import pywrap_tfe
  File ""C:\Users\Acer\anaconda3\envs\mytfenv\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Acer\anaconda3\envs\mytfenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Acer\anaconda3\envs\mytfenv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
    from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
42597,"[Bug]The file copied by TF from HDFS to local may be wrong, when HDFS file is being overwritten","This is a issue from TaiJi AI platform in Tencent.

**System information**
- OS Platform and Distribution : Linux version 4.14.105-1-tlinux3-0010
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1（we use）and the latest version also has this problem
- Python version: 3.6
- C++ version: 11

**Describe the current behavior**
Our training sample data is generated by the spark program and stored on HDFS, an example of a training sample file: `hdfs://xxxx/example/20200822/part-r-0000036.tfr.gz`, and the file data is compressed by gzip.
The trigger condition of the training program is that the `_SUCCESS` file appears under `hdfs://xxxx/example/20200822/`. The training program first downloads the training samples on HDFS to the local, and then reads the local data for training. When the training program and the spark program are running at the same time, the downloaded HDFS file may be overwritten by the spark program, causing the gzip file downloaded to the local to be damaged. Once the gzip file is wrong, our tensorflow training program will always stay unzipped, and the CPU utilization rate is high. 
The wrong local gzip file is composed of part of the data of the HDFS file before and after overwriting.

code:
```
 auto env = tensorflow::Env::Default();
 auto st = env->CopyFile(src_file, des_file);
```
process pstack info：
![image](https://user-images.githubusercontent.com/70072713/90976846-30be5080-e573-11ea-9f02-dace76b15584.png)
top info:
![image](https://user-images.githubusercontent.com/70072713/90976850-3fa50300-e573-11ea-90bf-56ccf06172c7.png)

**Describe the expected behavior**
The local gzip file is consistent with the data of the HDFS file before overwriting, or the data of the HDFS file after overwriting, instead of containing the data of the HDFS file before and after overwriting

**issues analysis**
In order to solve the [issue:5438](https://github.com/tensorflow/tensorflow/issues/5438) that the tensorboard needs to get the latest data written, the HDFS file is reopened in [the HDFSRandomAccessFile Read](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226): when n>0 r=0, call hdfsOpenFile to reopen the HDFS file. Please refer to this [commit](https://github.com/tensorflow/tensorflow/commit/e6e8d8715552d8890c0dd10f49ec3dff931a9926) for details.
Before calling hdfsOpenFile, if the HDFS file is overwritten, a new HDFS file is generated.
After calling hdfsOpenFile, it will point to the new HDFS file. If the size of the new HDFS file is larger than the size of the old HDFS file, the HDFS file copied to the local file system by FileSystemCopyFile contains part of the data of the new and old HDFS files, causing the local gzip file to be wrong

**temp solution**: [patch-1](https://github.com/tensorflow/tensorflow/pull/42598)
[FileSystemCopyFile](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.cc#L466) avoids triggering the hdfsOpenFile operation of the HDFSRandomAccessFile Read. The size of the file copy is based on the file size, not based on kCopyFileBufferSize. The implementation principle of the temporary solution is the same as that of [ReadFileToString](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/env.cc#L423), but it is still possible that the file copied to the local file is wrong. Because `GetFileSize` and `READ` cannot form an atomic operation. For example, when the file size is obtained through GetFileSize, the HDFS file is overwritten, and the data of the new file is read based on the size of the old file. However, the possibility that the local file of the temporary solution is wrong is far less than the original solution. Generally speaking, reading the file data to the end of the file is a time-consuming operation, and the time-consuming operation of obtaining the file size is negligible

**may be a better solution**: [patch-2](https://github.com/tensorflow/tensorflow/pull/42599)
I'm not sure if this solution is a better solution. In some scenarios I don't know, it may require further discussion. RandomAccessFile READ is an abstraction of the operations supported by each file system, and the specific implementation is transparent to users. Adding the `hdfsOpenFile` to the [HDFSRandomAccessFile READ](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system.cc#L226) to read the latest data is a hidden and dangerous behavior. Because hdfsOpenFile may point to new files, data inconsistencies may occur. More fatally, there are a large number of methods that depend on the READ, which may cause some behaviors that are not what we expect, which is the root of all errors. I think it is better for users to use READ and REOPEN to obtain the latest data in the program.

**accepted solution**: [patch-3](https://github.com/tensorflow/tensorflow/pull/42860)
Quoting mihaimaruseac's comment:
```
Patch-1 has the issue of breaking separation of concern design principles (what happens if there is a new scheme for hdfs? We would have a bug in there until someone remembers the additional if). 
Patch-2 has the issue of removing a test that was added for creating a bug.
```
In order to overcome the shortcomings of patch-1 and patch-2, a switch is added to [patch-3](https://github.com/tensorflow/tensorflow/pull/42860),  the default HDFS_DISABLE_READ_EOF_RETRIED is false. [patch-3](https://github.com/tensorflow/tensorflow/pull/42860) will not remove the [WriteWhileReading](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/hadoop/hadoop_file_system_test.cc#L202) test case, and it can also solve the problem we encountered. If you need to turn off the HDFS_READ_EOF_RETRIED, set the environment variable:
 ```
source HDFS_DISABLE_READ_EOF_RETRIED=1
```
For more details, please refer to the comments below.
"
42596,Protobuf MergeFrom errors when tensorflow and python protobuf share a libprotobuf copy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master
- Python version: 3.8.2
- Bazel version (if compiling from source): 3.4.1
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

When compiling tensorflow from source against a system installed protobuf with a dynamically linked python protobuf (i.e., tensorflow and python protobuf sharing a single libprotobuf), strange errors occur.

Here is an example

```
$ python3 -c ""from tensorflow.keras.applications.resnet50 import ResNet50; model = ResNet50()""
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
2020-08-23 02:12:12.645582: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-23 02:12:12.651545: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3700065000 Hz
2020-08-23 02:12:12.653098: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2478a30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-23 02:12:12.653110: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/applications/resnet.py"", line 474, in ResNet50
    return ResNet(stack_fn, False, True, 'resnet50', include_top, weights,
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/applications/resnet.py"", line 171, in ResNet
    x = layers.Conv2D(64, 7, strides=2, use_bias=use_bias, name='conv1_conv')(x)
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 921, in __call__
    self._maybe_build(inputs)
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2474, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 179, in build
    self.kernel = self.add_weight(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 579, in add_weight
    variable = self._add_variable_with_custom_getter(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py"", line 738, in _add_variable_with_custom_getter
    new_variable = getter(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 134, in make_variable
    return tf_variables.VariableV1(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 260, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 206, in _variable_v1_call
    return previous_getter(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 199, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py"", line 2583, in default_variable_creator
    return resource_variable_ops.ResourceVariable(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1425, in __init__
    self._init_from_args(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1579, in _init_from_args
    handle = eager_safe_variable_handle(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 242, in eager_safe_variable_handle
    return _variable_handle_from_shape_and_dtype(
  File ""/home/sclarkson/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 180, in _variable_handle_from_shape_and_dtype
    cpp_shape_inference_pb2.CppShapeInferenceResult.HandleShapeAndType(
TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.
```

After then pip-installing protobuf (i.e., switching to a statically linked python protobuf and introducing a new libprotobuf instance), the error goes away.

```
$ pip3 install --force --no-deps protobuf
Collecting protobuf
  Using cached protobuf-3.13.0-cp38-cp38-manylinux1_x86_64.whl (1.3 MB)
Installing collected packages: protobuf
Successfully installed protobuf-3.13.0
$ python3 -c ""from tensorflow.keras.applications.resnet50 import ResNet50; model = ResNet50()""
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.
WARNING:root:Limited tf.summary API due to missing TensorBoard installation.
2020-08-23 02:25:07.778252: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-23 02:25:07.784258: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 3700065000 Hz
2020-08-23 02:25:07.785881: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1dc2cc0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-23 02:25:07.785894: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
```

**Describe the expected behavior**

Tensorflow and python protobuf should be able to share the same libprotobuf instance.

**Standalone code to reproduce the issue**

See example above.

**Other info / logs**

I was able to bisect the bug back to 5498a3c. Re-adding the line below to `tensorflow/python/__init__.py` appears to resolve the issue, though I have not tested extensively.
```
from tensorflow.python import pywrap_tensorflow
```

Note that you will need #42591 to compile master against a system protobuf install.

"
42595,Support for batch to single element conversion,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
The current implementation of batch data requires a batch output, however, I made a model that converts batch data into a single element. So the first dimensions need not be the same. 

**Will this change the current api? How?**
The current implementation gives me an error ""the first dimensions do not match"". That might change

**Who will benefit with this feature?**
Anyone who is making STGCNNs

**Any Other info.**
Though convoluted, here's the model summary for reference:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param      Connected to                     

input_1 (InputLayer)            [(150, 640, 3)]      0                                            
__________________________________________________________________________________________________
tf_op_layer_ExpandDims (TensorF [(1, 150, 640, 3)]   0           input_1[0][0]                    
__________________________________________________________________________________________________
tf_op_layer_Transpose (TensorFl [(1, 640, 150, 3)]   0           tf_op_layer_ExpandDims[0][0]     
__________________________________________________________________________________________________
glu (GLU)                       (1, 640, 148, 2)     40          tf_op_layer_Transpose[0][0]      
__________________________________________________________________________________________________
tf_op_layer_Transpose_1 (Tensor [(1, 148, 640, 2)]   0           glu[0][0]                        
__________________________________________________________________________________________________
input_3 (InputLayer)            [(150, 640, 640)]    0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(150, 640, 640, 1)] 0                                            
__________________________________________________________________________________________________
tf_op_layer_Squeeze (TensorFlow [(148, 640, 2)]      0           tf_op_layer_Transpose_1[0][0]    
__________________________________________________________________________________________________
tf_op_layer_strided_slice_1 (Te [(148, 640, 640)]    0           input_3[0][0]                    
__________________________________________________________________________________________________
tf_op_layer_strided_slice (Tens [(148, 640, 640, 1)] 0           input_2[0][0]                    
__________________________________________________________________________________________________
graph_conv (GraphConv)          (148, 640, 3)        9           tf_op_layer_Squeeze[0][0]        
                                                                 tf_op_layer_strided_slice_1[0][0]
                                                                 tf_op_layer_strided_slice[0][0]  
__________________________________________________________________________________________________
tf_op_layer_Transpose_2 (Tensor [(640, 148, 3)]      0           graph_conv[0][0]                 
__________________________________________________________________________________________________
tf_op_layer_ExpandDims_1 (Tenso [(1, 640, 148, 3)]   0           tf_op_layer_Transpose_2[0][0]    
__________________________________________________________________________________________________
glu_1 (GLU)                     (1, 640, 146, 2)     40          tf_op_layer_ExpandDims_1[0][0]   
__________________________________________________________________________________________________
glu_2 (GLU)                     (1, 640, 144, 2)     28          glu_1[0][0]                      
__________________________________________________________________________________________________
tf_op_layer_Transpose_3 (Tensor [(1, 144, 640, 2)]   0           glu_2[0][0]                      
__________________________________________________________________________________________________
tf_op_layer_Squeeze_1 (TensorFl [(144, 640, 2)]      0           tf_op_layer_Transpose_3[0][0]    
__________________________________________________________________________________________________
tf_op_layer_strided_slice_3 (Te [(144, 640, 640)]    0           input_3[0][0]                    
__________________________________________________________________________________________________
tf_op_layer_strided_slice_2 (Te [(144, 640, 640, 1)] 0           input_2[0][0]                    
__________________________________________________________________________________________________
graph_conv_1 (GraphConv)        (144, 640, 3)        9           tf_op_layer_Squeeze_1[0][0]      
                                                                 tf_op_layer_strided_slice_3[0][0]
                                                                 tf_op_layer_strided_slice_2[0][0]
__________________________________________________________________________________________________
tf_op_layer_Transpose_4 (Tensor [(640, 144, 3)]      0           graph_conv_1[0][0]               
__________________________________________________________________________________________________
tf_op_layer_ExpandDims_2 (Tenso [(1, 640, 144, 3)]   0           tf_op_layer_Transpose_4[0][0]    
__________________________________________________________________________________________________
glu_3 (GLU)                     (1, 640, 142, 2)     40          tf_op_layer_ExpandDims_2[0][0]   
__________________________________________________________________________________________________
glu_4 (GLU)                     (1, 640, 1, 3)       1710        glu_3[0][0]                      
__________________________________________________________________________________________________
tf_op_layer_Transpose_5 (Tensor [(1, 1, 640, 3)]     0           glu_4[0][0]                      
__________________________________________________________________________________________________
tf_op_layer_Squeeze_2 (TensorFl [(640, 3)]           0           tf_op_layer_Transpose_5[0][0]    
__________________________________________________________________________________________________
dense (Dense)                   (640, 3)             12          tf_op_layer_Squeeze_2[0][0]      

Total params: 1,888
Trainable params: 1,888
Non-trainable params: 0
__________________________________________________________________________________________________"
42594,"Unexpected behavior , none persistent errors and, Loop execution was cancelled.","hello ,

I having an issue with code piece of code I wrote to perform a customer operation in Tensor flow.  The code generally works fine but sometimes throws unexpected errors and exceptions even when the same input is used. 


System information
-Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
-OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
-Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
-TensorFlow installed from (source or binary): Binary
-TensorFlow version (use command below): '2.3.0'
-Python version: 3.7
-Bazel version (if compiling from source): NA
-GCC/Compiler version (if compiling from source): NA
-CUDA/cuDNN version: 10.1
-GPU model and memory: RTX 2080 TI
You can collect some of this information using our environment capture

**Describe the current behavior**
I have written some tensorflow code to perform multiplication on  RaggedTensors.  My issue is that code sometimes works fine and other times throws unexpected errors even when the same input is used

**Describe the expected behavior**
I would expect tensor flow to be more persistent/consistent in errors. especially when the same input is used, for example the same input sometimes generates the correct output , other times I get an `[_Derived_]Loop execution was cancelled` and some times I also get `Expected size[0] in [0, 0], but got 3`

**Standalone code to reproduce the issue**
I have  tired to reduce the code to a smaller example unfortunately I could not since the error is being thrown at runtime and I'm not sure which part of my code is the source of the issue. , below is my code with comments : 
```
import tensorflow.compat.v1 as tf
tf.compat.v1.disable_eager_execution()
tf.disable_v2_behavior()


myTensor_values = tf.placeholder(dtype=tf.float32)
myTensor_l2_splits = tf.placeholder(dtype=tf.int32)
myTensor_l1_splits = tf.placeholder(dtype=tf.int32)


def innerloop_processing(begin_index , end_index , input1) : 
    innerloop_counter = begin_index
    ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )
    def innerloop_body(counter , begin_index , end_index , input1 , ta) : 
        inner_being_index = input1[1][counter]
        inner_end_index = input1[1][counter+1]
        row = tf.slice(input1[0] , [inner_being_index] ,  [inner_end_index-inner_being_index])
        ta = ta.write(counter-begin_index , row)
        counter = counter + 1 
        return counter , begin_index , end_index , input1 , ta
    
    
    def innerloop_cond(counter , begin_index , end_index , input1 , ta ) : 
        return input1[1][counter] < input1[1][end_index] -1  #stop at the next pointer of the l2_splits 
 
    results = tf.while_loop(innerloop_cond , innerloop_body , [innerloop_counter , begin_index , end_index , input1 , ta] )
    print_resutls = tf.print(""this is the component result  :"" , results[4].stack())
    return results[4].stack()


def generateL1Tensor_writeback(start_offest,step,num):
    counter=tf.constant(0,tf.int32)
    values = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )
    def cond(values , start_offest , num ,counter) : 
        return counter*step <= num*step
    def body(values , start_offest , num ,counter) : 
        values = values.write(counter,[(counter*step)+start_offest])
        counter = counter+1
        return  values , start_offest , num ,counter
    
    final_values , _ , _ , _  = tf.while_loop(cond,body,[values , start_offest , num , counter])
    final = final_values.concat()
    #print_line = tf.print("" xxxxx This is the is the split : "" ,  final)
    return final

def multiply2n_ragged(tensor1 , tensor2) : 
    #this  function multiplies two ragged tesnsors of rank 2 . the most outer ranks of the two tensros must be equal .
    #setting variables and constats 
    outerloop_counter = tf.constant(0 , dtype=tf.int32)
    carry_on = tf.constant(0 , dtype=tf.int32)
    taValues = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )
    taL2Splits = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )
    taL1Splits = tf.TensorArray(tf.int32, size=0, dynamic_size=True, clear_after_read=False , infer_shape=False )
    taL1Splits = taL1Splits.write(0,[0]) ## required intialization for L1 split only
    innerloop_processing_graphed = tf.function(innerloop_processing)
    generateL1Tensor_writeback_graphed = tf.function(generateL1Tensor_writeback)
    def outerloop_cond(counter,input1,input2 ,taValues  ,taL2Splits , taL1Splits , carry_on ) :
        value = tf.shape(input1[2])[0]-1
        return counter < value ## this is the length of the outermost dimision , stop of this 
    def outloop_body(counter,input1,input2, taValues  ,taL2Splits , taL1Splits , carry_on) : 
        l1_comp_begin = input1[2][counter]                  ## this is begin position of the current row in the outer split  ( ie. the ith value in the outer row split tensor ) 
        l1_comp_end = input1[2][counter+1]                  ## this is end position of the current row in the outer split   (ie. the ith + 1 value in the outer row split tensor)
        l1_comp2_begin = input2[2][counter]                 ## we do the same for the second components 
        l1_comp2_end = input2[2][counter+1]                 ## we do the same for the second components
        comp  = innerloop_processing_graphed(l1_comp_begin ,l1_comp_end ,input1  ) ## now retrive the data to be procesed for the selected rows from vector1
        comp2  =innerloop_processing_graphed(l1_comp2_begin ,l1_comp2_end ,input2  ) ## do the same for vector 2 
        
        comp2 = tf.transpose(comp2) ### desired operation
        multiply =tf.matmul(comp , comp2) #### This is the desired operation  

        
        myshape= tf.shape(multiply) ## calculate the shape of the result in order to prepare to write the result in a ragged tensor format. 
        offset = tf.cond( taValues.size() >0  ,lambda: tf.shape(taValues.concat())[0] , lambda : [0]) ### this is a hack, TensorArray.concat returns an error if the array is empty. Thus we check before calling this. 
        l2v = generateL1Tensor_writeback_graphed(offset,myshape[1],myshape[0])  # generate the inner row split of the result for the current element
        taL2Splits=taL2Splits.write(counter,l2v) # write back the inner rowlplit to a TensorArray 
        taValues=taValues.write(counter,tf.reshape(multiply , [-1])) # wirte back the actual ragged tensor elemnts in a another TensorArray
        carry_on=carry_on+myshape[0] ## required to calculate the outer row splite
        taL1Splits=taL1Splits.write(counter+1,[carry_on]) ## This is the outmost row split. 
        counter = counter+1
        return counter , input1,input2, taValues  ,taL2Splits , taL1Splits , carry_on
    
    outerloop_finalcounter , _ , _ , ta1,ta2,ta3,_ = tf.while_loop(outerloop_cond,outloop_body,[outerloop_counter , tensor1 , tensor2 ,taValues  ,taL2Splits , taL1Splits,carry_on])
    uinquie_ta2 , _ = tf.unique(ta2.concat())  # this is required since some values might be duplicate in the row split itself 
    final_values = ta1.concat() , uinquie_ta2   ,ta3.concat()
    return final_values




t = myTensor_values , myTensor_l2_splits , myTensor_l1_splits

oo   =multiply2n_ragged(t,t)
new_oo = multiply2n_ragged(oo,oo)


sess = tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True)))
sess.run(tf.global_variables_initializer())
vals =np.array([1.0, 2.2  , 1.1 , 4.0, 5.0 , 1.1 , 6.0, 7.0 , 1.1 , 8.0, 9.0 , 1.1 ,10.0, 11.0 , 1.1 ])
l2_splits = np.array([0,3,6,9,12,15])
l1_splits = np.array([0, 2, 5  ]) 
re       = sess.run([new_oo  ] , feed_dict={myTensor_values:vals ,myTensor_l1_splits:l1_splits ,myTensor_l2_splits:l2_splits  } )
print(re)

```


 As I said the code works fine many times , however it some times generates the below errors for the same inputs . stack traces of the two different errors that I get : 
```

this is the component result  : [[1 2.2 1.1]
 [4 5 1.1]]
this is the component result  : [[6 7 1.1]
 [8 9 1.1]
 [10 11 1.1]]
this is the component result  : [[6 7 1.1]
 [8 9 1.1]
 [10 11 1.1]]
this is the component result  : [[1 2.2 1.1]
 [4 5 1.1]]
---------------------------------------------------------------------------
CancelledError                            Traceback (most recent call last)
C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1364     try:
-> 1365       return fn(*args)
   1366     except errors.OpError as e:

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,
-> 1350                                       target_list, run_metadata)
   1351 

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1442                                             fetch_list, target_list,
-> 1443                                             run_metadata)
   1444 

CancelledError: {{function_node __inference_innerloop_processing_11240}} {{function_node __inference_innerloop_processing_11240}} [_Derived_]Loop execution was cancelled.
	 [[{{node while/LoopCond/_20}}]]
	 [[while_27/StatefulPartitionedCall_1]]

During handling of the above exception, another exception occurred:

CancelledError                            Traceback (most recent call last)
<ipython-input-15-238a2ce9a03a> in <module>
     94 l2_splits = np.array([0,3,6,9,12,15])
     95 l1_splits = np.array([0, 2, 5  ])
---> 96 re       = sess.run([new_oo  ] , feed_dict={myTensor_values:vals ,myTensor_l1_splits:l1_splits ,myTensor_l2_splits:l2_splits  } )
     97 print(re)

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    956     try:
    957       result = self._run(None, fetches, feed_dict, options_ptr,
--> 958                          run_metadata_ptr)
    959       if run_metadata:
    960         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1179     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1180       results = self._do_run(handle, final_targets, final_fetches,
-> 1181                              feed_dict_tensor, options, run_metadata)
   1182     else:
   1183       results = []

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1357     if handle is None:
   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1359                            run_metadata)
   1360     else:
   1361       return self._do_call(_prun_fn, handle, feeds, fetches)

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-> 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

CancelledError:   [_Derived_]Loop execution was cancelled.
	 [[{{node while/LoopCond/_20}}]]
	 [[while_27/StatefulPartitionedCall_1]]

```


Another Error is : 
```

this is the component result  : [[1 2.2 1.1]
 [4 5 1.1]]
this is the component result  : [[1 2.2 1.1]
 [4 5 1.1]]
this is the component result  : [[6 7 1.1]
 [8 9 1.1]
 [10 11 1.1]]
this is the component result  : [[6 7 1.1]
 [8 9 1.1]
 [10 11 1.1]]
this is the component result  : [[7.05 16.21]
 [16.21 42.21]]
this is the component result  : [[7.05 16.21]
 [16.21 42.21]]
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1364     try:
-> 1365       return fn(*args)
   1366     except errors.OpError as e:

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1349       return self._call_tf_sessionrun(options, feed_dict, fetch_list,
-> 1350                                       target_list, run_metadata)
   1351 

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1442                                             fetch_list, target_list,
-> 1443                                             run_metadata)
   1444 

InvalidArgumentError: {{function_node __inference_innerloop_processing_13658}} {{function_node __inference_innerloop_processing_13658}} Expected size[0] in [0, 0], but got 3
	 [[{{node while/body/_1/while/Slice}}]]
	 [[while_33/StatefulPartitionedCall_1]]

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-18-238a2ce9a03a> in <module>
     94 l2_splits = np.array([0,3,6,9,12,15])
     95 l1_splits = np.array([0, 2, 5  ])
---> 96 re       = sess.run([new_oo  ] , feed_dict={myTensor_values:vals ,myTensor_l1_splits:l1_splits ,myTensor_l2_splits:l2_splits  } )
     97 print(re)

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in run(self, fetches, feed_dict, options, run_metadata)
    956     try:
    957       result = self._run(None, fetches, feed_dict, options_ptr,
--> 958                          run_metadata_ptr)
    959       if run_metadata:
    960         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1179     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1180       results = self._do_run(handle, final_targets, final_fetches,
-> 1181                              feed_dict_tensor, options, run_metadata)
   1182     else:
   1183       results = []

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1357     if handle is None:
   1358       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1359                            run_metadata)
   1360     else:
   1361       return self._do_call(_prun_fn, handle, feeds, fetches)

C:\ProgramData\Anaconda3\envs\AutoEncoder\lib\site-packages\tensorflow\python\client\session.py in _do_call(self, fn, *args)
   1382                     '\nsession_config.graph_options.rewrite_options.'
   1383                     'disable_meta_optimizer = True')
-> 1384       raise type(e)(node_def, op, message)
   1385 
   1386   def _extend_graph(self):

InvalidArgumentError:   Expected size[0] in [0, 0], but got 3
	 [[{{node while/body/_1/while/Slice}}]]
	 [[while_33/StatefulPartitionedCall_1]]

```



I would really  appreciate your help , Thanks in advance"
42593,Support converting from joblib or pickle models,"Hello,

I have a trained model that uses **sklearn** **EllipticEnvelope** (unfortunately couldn't find the equivalent model in tf) and I can get only `joblib` or `pickle` format models from that. Is there a way to convert such models to tflite? Thank you. "
42592,FusedBatchNorm vs FusedBatchNormV3,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>



**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

I described the issue here: https://stackoverflow.com/questions/63542404/fusedbatchnorm-vs-fusedbatchnormv3
"
42590,Cannot assign a device for operation when using multiple embedding columns,"

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux, Amazon Deep Learning Image, Amazon Linux 2
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
No
-   **TensorFlow installed from (source or binary)**:
Binary
-   **TensorFlow version (use command below)**:
2.3
-   **Python version**:
3.7.7
-   **Bazel version (if compiling from source)**:
N/A
-   **GCC/Compiler version (if compiling from source)**:
N/A
-   **CUDA/cuDNN version**:
CUDA Version: 11.0
-   **GPU model and memory**:
NVIDIA Tesla M60 - 16 GB (Amazon EC2 g3s.xlarge - single GPU)
-   **Exact command to reproduce**:
python repro.py

### Describe the problem
When using the functional API with Keras with more than one embedding input, I get device placement issues. The actual error is:
> tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation functional_1/first_embed_layer/test_embedding/test_embedding_weights/embedding_lookup_sparse/embedding_lookup: Could not satisfy explicit device specification '' because the node {{colocation_node functional_1/first_embed_layer/test_embedding/test_embedding_weights/embedding_lookup_sparse/embedding_lookup}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:XLA_CPU:0, /job:localhost/replica:0/task:0/device:XLA_GPU:0, /job:localhost/replica:0/task:0/device:GPU:0]

I've found several workarounds:
- Instead of using the functional API, use keras.model.Sequential() with a single DenseFeatures layer containing both of my embedding columns (I was trying to avoid this because I can't find an easy way to pull out the weights of only one of the columns when doing it this way, without searching by dimension size).
- Run it without a GPU. Also not ideal because of the slow down.

I also tried doing a single DenseFeatures layer with both of my embedding columns using the functional API, but that results in the same placement errors. For some reason not using the functional API fixes it. 

A minimal example from my code that repros the problem reliably:
```
import tensorflow as tf

def create_model():
  test_input = tf.keras.Input(shape=(None,), dtype='string', name='test')
  test2_input = tf.keras.Input(shape=(None,), dtype='string', name='test2')
  feature_layer_inputs = {}
  feature_layer_inputs['test'] = test_input
  feature_layer_inputs['test2'] = test2_input

  vocab_list = ['This', 'That', 'Thing']
  feature_col = tf.feature_column.categorical_column_with_vocabulary_list(
      key='test', vocabulary_list=vocab_list,
      num_oov_buckets=0)
  embed_col = tf.feature_column.embedding_column(
      categorical_column=feature_col, dimension=4, combiner='mean')
  first_embed_layer = tf.keras.layers.DenseFeatures(
      feature_columns=[embed_col], name=""first_embed_layer"")

  second_vocab_list = ['a', 'b', 'c']
  feature_col_two = tf.feature_column.categorical_column_with_vocabulary_list(
      key='test2', vocabulary_list=second_vocab_list,
      num_oov_buckets=0)
  embed_col_two = tf.feature_column.embedding_column(
      categorical_column=feature_col_two, dimension=4, combiner='mean')
  second_embed_layer = tf.keras.layers.DenseFeatures(
      feature_columns=[embed_col_two], name=""second_embed_layer"")
  
  x = first_embed_layer(feature_layer_inputs)
  y = second_embed_layer(feature_layer_inputs)
  x = tf.keras.layers.concatenate([x, y])
  
  hidden_layer = tf.keras.layers.Dense(units=35, use_bias=False,
      name=""user-embeddings-layer"")(x)

  model = tf.keras.Model(
    inputs=[v for v in feature_layer_inputs.values()],
    outputs=[hidden_layer]
  )

  model.compile(optimizer=tf.keras.optimizers.Adagrad(lr=.01),
                # loss=loss_func,
                loss=""sparse_categorical_crossentropy"",
                metrics=['accuracy'])
  return model

in_tensor = tf.constant(['This', 'That'])
other_tensor = tf.constant(['a', 'b'])

features = {
  'test': in_tensor,
  'test2': other_tensor,
}
y = tf.constant([1, 2])

model = create_model()
history = model.fit(x=features, y=y,
                    epochs=10, shuffle=False, 
                    batch_size=1,
                    verbose=1,
                    callbacks=[])
```

"
42589,what exactly does the argument convert_to_tensor_fn do?,"In [tfp.layers.IndependentBernoulli](https://www.tensorflow.org/probability/api_docs/python/tfp/layers/IndependentBernoulli), there's this argument `convert_to_tensor_fn` that has four value options: `tfd.Distribution.sample`, `tfd.Distribution.mean`, `tfd.Distribution.mode`, `tfd.Bernoulli.logits`. I followed [this post](https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE) when trying to implement VAE for MNIST data reconstruction, in which the output layer has a `IndependentBernoulli` distribution with `convert_to_tensor_fn` set to `tfd.Bernoulli.logits`, instead of the default `tfd.Distribution.sample`. However, I experimented with all the other 3 options and found that the results are basically identical, in terms of both the loss after each epoch and the quality of reconstructed images.

I'm thus wondering what does this `convert_to_tensor_fn` argument do, and what are the differences among all 4 options?"
42588,convert tesnorflow::ops::cast to tensorflow::tensor,"System information

- Have I written custom code :yes 
- OS Platform and Distribution :Linux ubtuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.15
- Python version: 3.6
- Bazel version (if compiling from source): 0.25.0
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: 10.2.89/7.6.5.32
- GPU model and memory: NVidia GTX 1060
- OpenCv 4.4.2


I have written a custom code and trying to read an image with opencv and convert it to a tensorflow::Tensor.
I did it successfully and can do inference on my model but my model gives me a tensor of int64  and now i wanna cast it to a tensor of int32. 

    ` auto root = tensorflow::Scope::NewRootScope();
      Tensor inputImg(tensorflow::DT_UINT8, tensorflow::TensorShape({1, input_height_, input_width_, 3}));
      uint8_t *image1_p_ = inputImg.flat<uint8_t>().data();
      Mat cv_image1_ = cv::Mat(input_height_, input_width_, CV_8UC3, image1_p_);
      Mat image = imread(""test_img.jpg"");
      cv::Mat resized_img;
      resize(image,resized_img,cv::Size(input_width_,input_height_));
      cvtColor(resized_img,resized_img,COLOR_BGR2RGB);
      resized_img.convertTo(cv_image1_, CV_8UC3);
      std::vector<std::pair<std::string, tensorflow::Tensor>> inputs = {{ ""ImageTensor:0"", inputImg} };
      std::vector<tensorflow::Tensor> outputs;
      status = session->Run(inputs, {""SemanticPredictions:0""},{}, &outputs);
      if (!status.ok()) {
        std::cout << status.ToString() << ""\n"";
        return 1;
        }
      auto int64_caster =  Cast(root.WithOpName(""int64_caster""), outputs[0], tensorflow::DT_INT32);` 

I searched and found tensorflow **Cast** function. it gives me an tensorflow::opes::cast output  with the name of **int64_caster**
but i don't know how to convert it to tensorflow::tensor object."
42587,Access Denied when fetching local_config_cuda while building TF from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.3.+
- Python version:  3.7.4
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.1/7.6.5
- GPU model and memory: Nvidia GeForce 940 MX 4Gb



**Describe the problem**
When i try to build the tensorflow from the source i got from the repo, the build fails when trying to build package for TF 2.x
**Provide the exact sequence of commands / steps that you executed before running into the problem**
All steps were followed as mentioned in the official documentation [here](https://www.tensorflow.org/install/source_windows#setup_for_windows)
Step 0: Got all the mentioned pre-requisites and included them all in the path variables
Step 1: Downloaded and checkout the master branch of tensorflow repo using git
Step 2: `cd tensorflow`
`python ./configure.py`

The output of the configure file is as follows: 
```
F:\tfrep\tensorflow>python configure.py
You have bazel 3.1.0 installed.
Please specify the location of python. [Default is F:\python\python.exe]:


Found possible Python library paths:
  F:\python\lib\site-packages
Please input the desired Python library path to use.  Default is [F:\python\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Found CUDA 10.1 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include
Found cuDNN 7 in:
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/lib/x64
    C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include


Please specify a list of comma-separated CUDA compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus. Each capability can be specified as ""x.y"" or ""compute_xy"" to include both virtual and binary GPU code, or as ""sm_xy"" to only include the binary code.
Please note that each additional compute capability significantly increases your build time and binary size, and that TensorFlow only supports compute capabilities >= 3.5 [Default is: 3.5,7.0]: 5.0


Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=nonccl         # Disable NVIDIA NCCL support.
```

Even the cuDNN version and the path as shown in the output is not valid since i have cuDNN version 7.6.5 installed in `C:/cuda`
Step 4: `bazel build //tensorflow/tools/pip_package:build_pip_package`
This is where the problem originates:
Output:-
```
F:\tfrep\tensorflow>bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from f:\tfrep\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=F:/python/python.exe
INFO: Reading rc options for 'build' from f:\tfrep\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from f:\tfrep\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=F:/python/python.exe --action_env PYTHON_LIB_PATH=F:/python/lib/site-packages --python_path=F:/python/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file f:\tfrep\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file f:\tfrep\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file f:\tfrep\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file f:\tfrep\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file f:\tfrep\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file f:\tfrep\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file f:\tfrep\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl:1407:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1221, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1222, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\msys64\usr\bin"" -c ""realpath \""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\""""): Access is denied.
 (error: 5)
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1221, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1222, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\msys64\usr\bin"" -c ""realpath \""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\""""): Access is denied.
 (error: 5)
WARNING: Target pattern parsing failed.
ERROR: no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1377
                _create_local_cuda_repository(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1221, in _create_local_cuda_repository
                to_list_of_strings(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1222, in to_list_of_strings
                _cuda_include_path(<2 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in _cuda_include_path
                inc_entries.append(<1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/gpus/cuda_configure.bzl"", line 364, in inc_entries.append
                realpath(repository_ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/remote_config/common.bzl"", line 268, in realpath
                execute(repository_ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/remote_config/common.bzl"", line 208, in execute
                fail(<1 more arguments>)
Repository command failed
java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\msys64\usr\bin"" -c ""realpath \""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.1/include\""""): Access is denied.
 (error: 5)
INFO: Elapsed time: 1.523s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Even if using no flag during `configure.py` for CUDA, the **Access enied** error originates again, although with a different error.

```
F:\tfrep\tensorflow>bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=211
INFO: Reading rc options for 'build' from f:\tfrep\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=F:/python/python.exe
INFO: Reading rc options for 'build' from f:\tfrep\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from f:\tfrep\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=F:/python/python.exe --action_env PYTHON_LIB_PATH=F:/python/lib/site-packages --python_path=F:/python/python.exe --config=xla --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file f:\tfrep\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file f:\tfrep\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file f:\tfrep\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:windows in file f:\tfrep\tensorflow\.bazelrc: --copt=/w --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file f:\tfrep\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository flatbuffers instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule third_party_http_archive defined at:
  F:/tfrep/tensorflow/third_party/repo.bzl:216:28: in <toplevel>
INFO: Repository 'flatbuffers' used the following cache hits instead of downloading the corresponding file.
 * Hash '62f2223fb9181d1d6338451375628975775f7522185266cd5296571ac152bc45' for https://storage.googleapis.com/mirror.tensorflow.org/github.com/google/flatbuffers/archive/v1.12.0.tar.gz
If the definition of 'flatbuffers' was updated, verify that the hashes were also updated.
ERROR: An error occurred during the fetch of repository 'flatbuffers':
   Traceback (most recent call last):
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 193
                _apply_delete(ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 74, in _apply_delete
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(256) when executing 'C:\msys64\usr\bin -l -c ""rm"" ""-rf"" ""C:/users/nikhil/appdata/roaming/spb_data/_bazel_nikhil/5i2b27md/external/flatbuffers/build_defs.bzl""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\msys64\usr\bin"" -l -c ""\""rm\"" \""-rf\"" \""C:/users/nikhil/appdata/roaming/spb_data/_bazel_nikhil/5i2b27md/external/flatbuffers/build_defs.bzl\""""): Access is denied.
 (error: 5)
ERROR: F:/tfrep/tensorflow/tensorflow/tools/pip_package/BUILD:284:1: //tensorflow/tools/pip_package:build_pip_package depends on //tensorflow/lite/python:tflite_convert in repository @ which failed to fetch. no such package '@flatbuffers//': Traceback (most recent call last):
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 193
                _apply_delete(ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 74, in _apply_delete
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(256) when executing 'C:\msys64\usr\bin -l -c ""rm"" ""-rf"" ""C:/users/nikhil/appdata/roaming/spb_data/_bazel_nikhil/5i2b27md/external/flatbuffers/build_defs.bzl""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\msys64\usr\bin"" -l -c ""\""rm\"" \""-rf\"" \""C:/users/nikhil/appdata/roaming/spb_data/_bazel_nikhil/5i2b27md/external/flatbuffers/build_defs.bzl\""""): Access is denied.
 (error: 5)
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@flatbuffers//': Traceback (most recent call last):
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 193
                _apply_delete(ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 74, in _apply_delete
                _execute_and_check_ret_code(ctx, <1 more arguments>)
        File ""F:/tfrep/tensorflow/third_party/repo.bzl"", line 52, in _execute_and_check_ret_code
                fail(<1 more arguments>)
Non-zero return code(256) when executing 'C:\msys64\usr\bin -l -c ""rm"" ""-rf"" ""C:/users/nikhil/appdata/roaming/spb_data/_bazel_nikhil/5i2b27md/external/flatbuffers/build_defs.bzl""':
Stdout:
Stderr: java.io.IOException: ERROR: src/main/native/windows/process.cc(199): CreateProcessW(""C:\msys64\usr\bin"" -l -c ""\""rm\"" \""-rf\"" \""C:/users/nikhil/appdata/roaming/spb_data/_bazel_nikhil/5i2b27md/external/flatbuffers/build_defs.bzl\""""): Access is denied.
 (error: 5)
INFO: Elapsed time: 2.584s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (49 packages loaded, 14 targets configured)
    currently loading: tensorflow/lite/python
```

I have been working on TFDS for quite some time now and would like to work on TF as well. Therefore, I need to build the source ASAP. Thanks in advance!"
42586,Resource exhausted: OOM while using shuffle(),"Hi, 
I am using a dataset of ~14k images for training a Conditional GAN. 
I understand that the buffer_size in shuffle takes the images equal to buffer_size into the memory and then randomly choose the images from those. 
While shuffling, the resources are exhausted which means that not all the images were able to be fitted in the memory. 
But my issue is that it works fine for the initial 10-12 epochs. It shuffles the entire dataset with the same settings but after 10-12 epochs, it runs into the OOM error. So, why is it that it works fine for the initial epochs and then runs into the error?
Is any of the data pilling up in the memory while using shuffle or is there any memory leakage problem? This happens during the shuffling stage."
42585,Executing genrule //tensorflow/python:pywrap_tensorflow_filtered_def_file failed (Exit 1): bash.exe failed,"
### System information

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  windows 10 education
-   **TensorFlow installed from (source or binary)**:  source
-   **TensorFlow version (use command below)**:  r1.14
-   **Python version**:  3.7.4
-   **Bazel version (if compiling from source)**:  0.25.2
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:  10.0/7.4
-   **GPU model and memory**:  NVIDIA GeForce RTX2070 with Max-Q Design 16GB
-   **Exact command to reproduce**:


### Describe the problem
Build tensorflow 1.14 from source with bazel0.25.2 + vs2017(15.9.26) + python3.7.4 + cuda10.0 + cudnn7.4.2.24 + msys2, failed by ERROR: D:/newtf114/tensorflow/tensorflow/python/BUILD:4684:1: Executing genrule //tensorflow/python:pywrap_tensorflow_filtered_def_file failed (Exit 1): bash.exe failed: error executing command....

### Source code / logs

ERROR: D:/newtf114/tensorflow/tensorflow/python/BUILD:4684:1: Executing genrule //tensorflow/python:pywrap_tensorflow_filtered_def_file failed (Exit 1): bash.exe failed: error executing command
  cd D:/bazel_out/73kxojlr/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=D:/Program Files/nvidia gpu computing toolkit/cuda/v10.0
    SET PATH=D:\ProgramData\msys64\usr\bin;D:\ProgramData\msys64\bin;D:\ProgramData\Anaconda3;D:\ProgramData\Anaconda3\Library\mingw-w64\bin;D:\ProgramData\Anaconda3\Library\usr\bin;D:\ProgramData\Anaconda3\Library\bin;D:\ProgramData\Anaconda3\Scripts;D:\ProgramData\Anaconda3\bin;D:\ProgramData\Anaconda3\condabin;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\HostX86\x86;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft SDKs\TypeScript\3.1;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin\Roslyn;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Team Tools\Performance Tools;D:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x86;C:\Program Files (x86)\Windows Kits\10\bin\x86;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\bin;C:\Windows\Microsoft.NET\Framework\v4.0.30319;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\Tools;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.0\bin;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.0\libnvvp;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.1\bin;D:\Program Files\nvidia gpu computing toolkit\cuda\v10.1\libnvvp;d:\ProgramData\Anaconda3;d:\ProgramData\Anaconda3\Library\mingw-w64\bin;d:\ProgramData\Anaconda3\Library\usr\bin;d:\ProgramData\Anaconda3\Library\bin;d:\ProgramData\Anaconda3\Scripts;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0;C:\Windows\System32\OpenSSH;C:\Users\bolix\.dnx\bin;C:\Program Files\Microsoft DNX\Dnvm;C:\Program Files\Microsoft SQL Server\120\Tools\Binn;C:\Program Files\CMake\bin;D:\Program Files\MATLAB\R2018b\runtime\win64;D:\Program Files\MATLAB\R2018b\bin;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Program Files\dotnet;C:\Program Files\Microsoft SQL Server\130\Tools\Binn;D:\Program Files\Git\cmd;C:\Program Files\NVIDIA Corporation\NVIDIA NvDLISR;D:\ProgramData\msys64\usr\bin;D:\ProgramData\msys64\usr\bin\bash.exe;D:\ProgramData\msys64;C:\Users\bolix\AppData\Local\Microsoft\WindowsApps;D:\Program Files\JetBrains\PyCharm Community Edition 2019.2.4\bin;D:\texlive\2019\bin\win32;C:\Users\bolix\.dotnet\tools;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;d:\Program Files (x86)\Microsoft Visual Studio\2017\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PYTHON_BIN_PATH=D:/ProgramData/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=D:/ProgramData/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TF_CONFIGURE_IOS=0
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TF_NEED_TENSORRT=0
  D:/ProgramData/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh;
              bazel-out/x64_windows-opt/bin/external/local_config_def_file_filter/def_file_filter.exe \
              --input bazel-out/x64_windows-opt/bin/tensorflow/tf_custom_op_library_additional_deps.dll.gen.def \
              --output bazel-out/x64_windows-opt/bin/tensorflow/python/pywrap_tensorflow_filtered_def_file.def \
              --target _pywrap_tensorflow_internal.pyd

Execution platform: @bazel_tools//platforms:host_platform
Traceback (most recent call last):
  File ""\\?\d:\AppData\Local\Tmp\Bazel.runfiles_2zky2bii\runfiles\local_config_def_file_filter\def_file_filter.py"", line 172, in <module>
    sys.exit(main())
  File ""\\?\d:\AppData\Local\Tmp\Bazel.runfiles_2zky2bii\runfiles\local_config_def_file_filter\def_file_filter.py"", line 132, in main
    proc = subprocess.Popen([UNDNAME, tmpfile.name], stdout=subprocess.PIPE)
  File ""D:\ProgramData\Anaconda3\lib\subprocess.py"", line 775, in __init__
    restore_signals, start_new_session)
  File ""D:\ProgramData\Anaconda3\lib\subprocess.py"", line 1178, in _execute_child
    startupinfo)
FileNotFoundError: [WinError 2] 系统找不到指定的文件(this Chinese sentence means:system cannot find the file)。
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 4041.976s, Critical Path: 918.02s
INFO: 4328 processes: 4328 local.
"
42584,Logging Memory Usage For each TensorFlow Operation,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: N/A
-   **TensorFlow installed from (source or binary)**: Source
-   **TensorFlow version (use command below)**: r1.14
-   **Python version**: 2.7
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**: 5.4
-   **CUDA/cuDNN version**: 10.1/7.1
-   **GPU model and memory**: NVIDIA Corporation GV100GL [Tesla V100 SXM2 16GB] (rev a1)
-   **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I am running a Tramsformer Model in which If I add some control dependencies (which are not required for its design, but performance can be improved if I can add those dependencies), I get OOM error. I wanted to log the memory Usage for each TF operation (including CUDA context memory and Memory Allocator Reservations in addition to the raw bytes needed to input and output tensors.)

I looked at profiling using Timeline as suggested in this [StackOverFlow post](https://stackoverflow.com/questions/36123740/is-there-a-way-of-determining-how-much-gpu-memory-is-in-use-by-tensorflow), but this is just logging raw bytes needed by the tensors, not the additional memory reserved or is marked free but is not garbage collected yet.

In addition, I also came across [tf.Profiler](https://github.com/tensorflow/tensorflow/tree/r1.14/tensorflow/core/profiler), but it has this [comment](https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/core/profiler/tfprof_log.proto#L128) saying that numbers are not reliable, which otherwise might have sufficed the requirement. Is there a better way to log all the memory used by each TF operation?

"
42582,SPLIT_V in TFLM,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source):
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): CEVA

**Describe the problem**
This issue is related to PR #42452 (https://github.com/tensorflow/tensorflow/pull/42452)

Currently no SPLIT_V op implementation in micro.
When using GRU layers in Keras, SPLIT_V op is used in the model converted to TFLite.
See this issue: https://github.com/tensorflow/tensorflow/issues/41690 the code samples there demonstrate this.


@advaitjain - Adding some details as requested.
Could you file a github issue with more context on why you need this new reference kernel?
The more detail the better, for example:

- **what your application is**
Our application is an end-t-end speech recognition engine focused on wake word detection and speech commands. It is targeted for low power, resource constrained platforms such as CEVA Cores.

**- what model architecture looks like**
The models is RNN based, more specifically GRU.

**any performance criteria that you have**
Our performance criteria are common to the speech recognition world, namely: False Reject Rate, False Accepts Per Hour for Wake Word detection and Confusion Matrix for Speech Commands.

**what embedded targets are you planning to run your models on?**
Our models are targeted at CEVA Cores.

**what your plans are for optimized implementations for the specific platforms that you care about?**
We are planning for extensive optimization of TFLM operators.

"
42581,"TensorScatterUpdate unsupported dtype complex64/128, tf2.3.0, GPU","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): 2.3.0
- Python version: 3.8
- CUDA/cuDNN version: 10.1/10.2
- GPU model and memory: TITAN V, RTX2080TI

**Describe the current behavior**

The `tf.tensor_scatter_nd_update` function raises:

`InvalidArgumentError: Unsupported dtype: complex64 [Op:TensorScatterUpdate]`
`InvalidArgumentError: Unsupported dtype: complex128 [Op:TensorScatterUpdate]`

when running tf2.3.0 on GPU, in eager mode, and using dtypes `complex64` and `complex128`.

You can reproduce this error by executing the following code on GPU with tf2.3.0: 

```python
import tensorflow as tf
var = tf.Variable(tf.zeros(10, dtype=tf.complex64))
up = tf.constant([1], dtype=tf.complex64) 
tf.tensor_scatter_nd_update(var, [[0]], up)
```
Note that the code works as expected if we compile the call with `tf.function`, and it also works fine on CPU.

**Describe the expected behavior**

The `tensor_scatter_nd_update` in tf2.3 should work on GPU if the dtype is complex64 or complex128, as in tf2.2 and tf2.1."
42580,Add missing packages to complete GPU installation page,"Hi, 

While following instructions on that page https://www.tensorflow.org/install/gpu, I needed to install 4 more packages to correctly install CUDA. It permits to avoid the `Depends: cuda-10-1 (>= 10.1.243) but it is not going to be installed` issue.

```
Ubuntu 18.04 (CUDA 10.1)
# Add NVIDIA package repositories
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub
sudo dpkg -i cuda-repo-ubuntu1804_10.1.243-1_amd64.deb
sudo apt-get update
wget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt install ./nvidia-machine-learning-repo-ubuntu1804_1.0.0-1_amd64.deb
sudo apt-get update

# Install NVIDIA driver
sudo apt-get install --no-install-recommends nvidia-driver-450
# Reboot. Check that GPUs are visible using the command: nvidia-smi

**# ADDED LINE : 
sudo apt-get install freeglut3 freeglut3-dev libxi-dev libxmu-dev**

# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-10-1 \
    libcudnn7=7.6.5.32-1+cuda10.1  \
    libcudnn7-dev=7.6.5.32-1+cuda10.1


# Install TensorRT. Requires that libcudnn7 is installed above.
sudo apt-get install -y --no-install-recommends libnvinfer6=6.0.1-1+cuda10.1 \
    libnvinfer-dev=6.0.1-1+cuda10.1 \
    libnvinfer-plugin6=6.0.1-1+cuda10.1
```
That solution is suggested by NVIDIA [here](https://forums.developer.nvidia.com/t/cuda-install-unmet-dependencies-cuda-depends-cuda-10-0-10-0-130-but-it-is-not-going-to-be-installed/66488/9). Maybe updating the doc could save few hours to data scientists."
42578,TF2.3 can't use GPU with CUDA11.0(windows),Do I have to reduce the CUDA version?
42577,[IOS TfLite] Missing Operations for TfLite GPU Delegate,"**System information**
In Podfile: TensorFlowLiteSwift', '~> 0.0.1-nightly', :subspecs => ['Metal']
Tensorflow Lite version: 2.3.0
Swift 5
Xcode ver 11.6

**Model information**
The model file is YOLOv4-tiny, training with darknet framework, the config file similar to: 
https://github.com/AlexeyAB/darknet/blob/master/cfg/yolov4-tiny-custom.cfg

Convert to model tflite using this following open project sourecode:
https://github.com/hunglc007/tensorflow-yolov4-tflite

**
I'm trying to use the GPU delegate for my custom tflite model, create the interpreter with GPU delegate using this code:

```
var metalOptions = MetalDelegate.Options()
metalOptions.waitType = .passive
let delegate = MetalDelegate(options: metalOptions)
var options = Interpreter.Options()
options.threadCount = threadCount
interpreter = try Interpreter(modelPath: modelPath, options: options,delegates: [delegate])
```
*****
After running, got this error:

```
TensorFlow Lite Error: Following operations are not supported by GPU delegate:
DEQUANTIZE: 
SPLIT: Operation is not supported.
SPLIT_V: Operation is not supported.
106 operations will run on the GPU, and the remaining 100 operations will run on the CPU
```
How can I fix this error?
Thanks.
"
42576,1st Time Model taking too long to start,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow==2.2
- TensorFlow version (use command below):2.2
- Python version:3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 10.1, cudnn 7.6.5
- GPU model and memory: NVIDIA MX110 2gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

2. `v2.2.0-rc4-8-g2b96f3662b 2.2.0`


**Describe the current behavior**
I Installed TF 2 and make a Sequential Model, That jupyter cell is taking forever to load. 

**Describe the expected behavior**
It should load that cell fastly
**Standalone code to reproduce the issue**
```python3
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target
from tensorflow.keras.models import Sequential
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.layers import Dense
y = to_categorical(y)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,y)
model1 = Sequential([
    Dense(512, activation='tanh', input_shape = X_train[0].shape),
    Dense(512//2, activation='tanh'),
    Dense(512//4, activation='tanh'),
    Dense(512//8, activation='tanh'),
    Dense(32, activation='relu'),
    Dense(3, activation='softmax')
])
```
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

What I can see on my COmmand Line is 
```terminal
[I 16:37:13.657 NotebookApp] Starting buffering for 427db0cf-ee0b-49fa-b075-ce24cdc456bb:6564a2664692489a8473d40138e004c7
2020-08-22 16:38:17.412089: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-22 16:38:17.465067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.465381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce MX110 computeCapability: 5.0
coreClock: 1.006GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s
2020-08-22 16:38:17.465580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-22 16:38:17.467132: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-22 16:38:17.468592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-22 16:38:17.468928: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-22 16:38:17.470530: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-22 16:38:17.471467: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-22 16:38:17.474846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-22 16:38:17.475036: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.475556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.475947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-22 16:38:17.476205: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2020-08-22 16:38:17.482259: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 1800000000 Hz
2020-08-22 16:38:17.482562: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa670000b20 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-22 16:38:17.482589: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-22 16:38:17.510525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.510905: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b17c39ce70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-22 16:38:17.510933: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce MX110, Compute Capability 5.0
2020-08-22 16:38:17.511104: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.511357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: GeForce MX110 computeCapability: 5.0
coreClock: 1.006GHz coreCount: 2 deviceMemorySize: 1.96GiB deviceMemoryBandwidth: 37.33GiB/s
2020-08-22 16:38:17.511403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-22 16:38:17.511423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-22 16:38:17.511441: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-22 16:38:17.511458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-22 16:38:17.511475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-22 16:38:17.511493: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-22 16:38:17.511512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-22 16:38:17.511562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.511828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.512056: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-22 16:38:17.512092: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-22 16:38:17.513132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-22 16:38:17.513144: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-08-22 16:38:17.513150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-08-22 16:38:17.513236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.513508: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2020-08-22 16:38:17.513756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1461 MB memory) -> physical GPU (device: 0, name: GeForce MX110, pci bus id: 0000:01:00.0, compute capability: 5.0)
[I 16:39:14.659 NotebookApp] Saving file at /Desktop/Ahmad/REgular/iristest.ipynb

```
It is more than 10 mins, while I am waiting for that jupyter cell to load.
"
42575,'retval_' must have the same nested structure in the main and else branches,"Tensorflow=2.3
python=3.7

I want to control the running process of call() through ‘mode’. but when the number of returned tensors is inconsistent, an error will be prompted. How can I fix this error?

My goal is to deploy the saved model to tensorflow serving.

```python
class MLP(tf.keras.Model):
    def __init__(self, **kwargs):
        super(MLP, self).__init__(**kwargs)
        self.dense = tf.keras.layers.Dense(1)
    @tf.function
    def call(self, inputs):
        a, b, mode = inputs
        if mode == 'predict':
            return a
        
        return self.dense(a + b),b
```
```python
if __name__ == '__main__':
    a = tf.ones((2, 10))
    b = tf.ones((2, 10))
    mode = 'train'
    mode = tf.cast(mode, tf.string)
    model = MLP()
    print(model((a, b, mode)))
    model.save('test_model/1/')
```

errors:
```python
    TypeError: 'retval_' must have the same nested structure in the main and else branches:
    
    The two structures don't have the same nested structure.
    
    First structure: type=Tensor str=Tensor(""inputs:0"", shape=(2, 10), dtype=float32)
    
    Second structure: type=tuple str=(<tf.Tensor 'cond/dense/BiasAdd:0' shape=(2, 1) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(2, 10) dtype=float32>)
    
    More specifically: Substructure ""type=tuple str=(<tf.Tensor 'cond/dense/BiasAdd:0' shape=(2, 1) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(2, 10) dtype=float32>)"" is a sequence, while substructure ""type=Tensor str=Tensor(""inputs:0"", shape=(2, 10), dtype=float32)"" is not
    Entire first structure:
    .
    Entire second structure:
    (., .)


```

"
42573,issues with tf.math.reduce_euclidean_norm,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
    Yes

-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
     macOS Mojave (10.14.5)

-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
    N/A

-   **TensorFlow installed from (source or binary)**:
    from anaconda

-   **TensorFlow version (use command below)**:
    1.14

-   **Python version**:
     3.7

-   **Bazel version (if compiling from source)**:
     N/A

-   **GCC/Compiler version (if compiling from source)**:
     N/A

-   **CUDA/cuDNN version**:
     No GPU needed

-   **GPU model and memory**:
     N/A
 
-   **Exact command to reproduce**:
     Just run the program

### Describe the problem
tf.math.reduce_euclidean_norm makes the model non-differentiable, I getting the error message ```ValueError: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [""<tf.Variable 'embedding_matrix:0' shape=(5, 3) dtype=float32_ref>""] and loss Tensor(""EuclideanNorm:0"", dtype=float32).``` for the following program

```
mport tensorflow as tf

index1 = tf.placeholder(tf.int32, None)
index2 = tf.placeholder(tf.int32, None)

embedding = tf.get_variable('embedding_matrix', [5, 3])

vector1 = tf.nn.embedding_lookup(embedding, index1)
vector2 = tf.nn.embedding_lookup(embedding, index2)

loss = tf.math.reduce_euclidean_norm(vector1-vector2, axis=1)
train_step = tf.train.AdamOptimizer(learning_rate=0.001, beta1=0.9, beta2=0.999).minimize(loss)

with tf.Session() as sess:            
    init = tf.global_variables_initializer()
    sess.run(init)
   
    for i in range(5):
        result = sess.run([loss, train_step], feed_dict={index1:1,index2:0})
        print(result[0])
```

Replace the line

```
loss = tf.math.reduce_euclidean_norm(vector1-vector2, axis=1)
```
with a custom implementation

```
loss = tf.math.sqrt(tf.reduce_sum((vector1-vector2)**2, axis=-1))
```

solves the problem. So we think this is a bug. It would be great if you can take a look or maybe let us know how we may use ```tf.math.reduce_euclidean_norm``` assuming we misused it.

Thanks"
42572,Tensor Cores no performance improvement CUBLAS tf.matmul(),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): see below
- OS Platform and Distribution: CentOS Linux 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0
- Python version: 3.7.4
- GCC/Compiler version (if compiling from source): N/A but it is GCC-8.3.0.
- CUDA/cuDNN version: 10.1.243 / 7.x.x
- GPU model and memory: RTX 2080Ti

**Describe the current behavior**
I don't see any performance difference running `tf.matmul()` when utilizing the Tensor Cores in Nvidia RTX 2080 Ti. Is my test setup wrong or is there really no performance improvement?

I run the test two times, setting environment variables for Tensor Cores differently each run.

As of Tensorflow 2.3.0 the use of environment variable `TF_DISABLE_CUBLAS_TENSOR_OP_MATH` has been removed (see [this commit](https://github.com/tensorflow/tensorflow/commit/32d63d0a3efb5e0b65dc6f590e54248054cf9f14)), making it impossible to switch between Tensor Core usage during runtime. So I used Tensorflow 2.2.0 using _float16_ datatype.

Tensor Cores disabled:
```
TF_DISABLE_CUBLAS_TENSOR_OP_MATH=1
TF_ENABLE_CUBLAS_TENSOR_OP_MATH_FP32=0
TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32=0
TF_ENABLE_CUDNN_RNN_TENSOR_OP_MATH_FP32=0
NUMBER=16384
DTYPE=float16            
2020-08-21 12:44:28.189777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-21 12:44:32.243417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:5a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-21 12:44:32.243729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 12:44:32.246376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-21 12:44:32.249129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-21 12:44:32.249554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-21 12:44:32.252491: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-21 12:44:32.254002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-21 12:44:32.259926: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-21 12:44:32.281312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-21 12:44:32.282146: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-08-21 12:44:32.291868: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2600000000 Hz
2020-08-21 12:44:32.292063: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3547ca0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-21 12:44:32.292081: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-21 12:44:32.694055: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x35ba180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-21 12:44:32.694078: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-08-21 12:44:32.696876: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:5a:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-21 12:44:32.696914: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 12:44:32.696931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-21 12:44:32.696947: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-21 12:44:32.696957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-21 12:44:32.696967: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-21 12:44:32.696976: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-21 12:44:32.696985: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-21 12:44:32.715821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-21 12:44:32.715858: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 12:44:32.718333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 12:44:32.718348: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-08-21 12:44:32.718356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-08-21 12:44:32.723419: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10204 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:5a:00.0, compute capability: 7.5)
2020-08-21 12:44:32.725151: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op StatelessRandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:41.983001: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:41.984023: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:42.230340: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:42.231872: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:42.232362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Num GPUs Available:  1
Generating a  16384 x 16384  matrix took:  0.0007753130048513412 s
Multiplying a  16384 x 16384  matrix took:  0.00017198268324136734 s
```

Tensor Cores enabled:

```
TF_DISABLE_CUBLAS_TENSOR_OP_MATH=0
TF_ENABLE_CUBLAS_TENSOR_OP_MATH_FP32=1
TF_ENABLE_CUDNN_TENSOR_OP_MATH_FP32=1
TF_ENABLE_CUDNN_RNN_TENSOR_OP_MATH_FP32=1
NUMBER=16384
DTYPE=float16         
2020-08-21 12:44:28.189996: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-08-21 12:44:32.153926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:62:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-21 12:44:32.155894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 12:44:32.173061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-21 12:44:32.177611: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-21 12:44:32.179239: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-21 12:44:32.184143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-21 12:44:32.186975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-21 12:44:32.195284: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-21 12:44:32.211230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-21 12:44:32.212211: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-08-21 12:44:32.223121: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2600000000 Hz
2020-08-21 12:44:32.223348: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3547d60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-21 12:44:32.223367: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-21 12:44:32.658935: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x35ba260 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-21 12:44:32.658973: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): GeForce RTX 2080 Ti, Compute Capability 7.5
2020-08-21 12:44:32.660135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:62:00.0 name: GeForce RTX 2080 Ti computeCapability: 7.5
coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.76GiB deviceMemoryBandwidth: 573.69GiB/s
2020-08-21 12:44:32.660175: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 12:44:32.660187: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-08-21 12:44:32.660197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-08-21 12:44:32.660206: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-08-21 12:44:32.660215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-08-21 12:44:32.660225: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-08-21 12:44:32.660235: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-08-21 12:44:32.662855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0
2020-08-21 12:44:32.662896: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 12:44:32.664538: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 12:44:32.664554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 
2020-08-21 12:44:32.664564: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N 
2020-08-21 12:44:32.694148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10204 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:62:00.0, compute capability: 7.5)
2020-08-21 12:44:32.696836: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op StatelessRandomUniform in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:41.882224: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op Sub in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:41.883265: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op Mul in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:42.230553: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op Add in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:42.233699: I tensorflow/core/common_runtime/eager/execute.cc:501] Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0
2020-08-21 12:44:42.234656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
Num GPUs Available:  1
Generating a  16384 x 16384  matrix took:  0.0015629231929779053 s
Multiplying a  16384 x 16384  matrix took:  0.00017150864005088806 s
```
**Describe the expected behavior**
Difference in execution time of multiplying 2 matrices.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import argparse
import logging
from time import perf_counter

parser = argparse.ArgumentParser(description='Process some integers.')
parser.add_argument('--number', dest='number', action='store', type=int,
                    default=1024,
                    help='Optional, integer')
parser.add_argument('--dtype', dest='dtype', action='store', type=str,
                    default='float32',
                    choices=['float64','float32','float16','bfloat16','int64','int32','int16','int8','uint64','uint32','uint16','uint8'],
                    help='Optional, dtype')

args = parser.parse_args()

size = args.number;

print(""Num GPUs Available: "", len(tf.config.experimental.list_physical_devices('GPU')))

tf.debugging.set_log_device_placement(True)
#initialize libraries
temp = tf.random_uniform_initializer(minval=0, maxval=1, seed=2)(shape=[2,2],dtype=args.dtype);

logging.info(""Generating a %dx%d randomUniform matrix, please standby"", size, size);
t1_start = perf_counter()
matrix = tf.random_uniform_initializer(minval=0, maxval=1, seed=2)(shape=[size,size],dtype=args.dtype);
t1_stop = perf_counter()
print(""Generating a "",size,""x"",size,"" matrix took: "", (t1_stop - t1_start), 's')

# Initialize cuBLAS....
temp2 = tf.matmul(temp,temp,transpose_a=False,transpose_b=True);

t1_start = perf_counter()
result = tf.matmul(matrix,matrix,transpose_a=False,transpose_b=True);
t1_stop = perf_counter()
print(""Multiplying a "",size,""x"",size,"" matrix took: "", (t1_stop - t1_start), 's')
```
**Other info / logs**"
42569,Improvements to the contribution guidelines,"@tensorflow/micro

Couple of comments that we have received that would improve the [TFLM Contribution Guidelines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/CONTRIBUTING.md):

 * More visibility on the internal testing (e.g. a short description of what is tested internally) would be good.
 * What are the guidelines around unit-testing?"
42568,Please Update Conda Install,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from conda then used pip to update tensorflow to 2.3.0):
- TensorFlow version (2.3.0):
- Python version 3.7:
- CUDA/cuDNN version (Latest conda install verison):
- GPU model and memory (V100 32gb VRAM):

**Describe the current behavior**
Tensorflow is unbelievable slow to start the training. Takes 30-40 min to initialize. Stops halfway through the first epoch for 10 minutes. Then every epoch after that is fine. I have run a similar code in Google Colab and it runs perfectly normal. Does not take 30-40 min to initialize. Everything is a fresh install (conda, venv, etc). 

**Describe the expected behavior**
For model.fit() to start training within a minute or two. Not take 40 minutes.

**Standalone code to reproduce the issue**

conda install -c anaconda tensorflow-gpu (I do this because it installs all the cuda drivers in one command. It is impossible to install cuda drivers on a cluster login node that does not contain an nvidia GPU inside of it. Conda is the only way to install TensorFlow with GPU acceleration on computational shared clusters.)

pip install tensorflow-gpu==2.3.0 (because your latest conda version does not install tensorflow-gpu==2.3.0)

Use the ImageDataGenerator:

batch_size=32

train_gen = train_datagen.flow(X_train, y_train, 
                                 batch_size=batch_size, 
                                 shuffle=True,
                                 subset='training', seed=7)


v_gen = val_datagen.flow(X_test, y_test,
                                 batch_size=batch_size,
                                 subset='training', seed=7)


ds = tf.data.Dataset.from_generator(lambda: train_gen, 
    output_types=(tf.float32, tf.float32), 
    output_shapes=([32,64,64,3], [32,3])
)


v = tf.data.Dataset.from_generator(lambda: v_gen, 
    output_types=(tf.float32, tf.float32), 
    output_shapes=([32,64,64,3], [32,3])
)


model.fit_generator(ds,
      epochs=100,  #### set repeat in training dataset
      steps_per_epoch=len(train_gen),
      validation_data=v,
      validation_steps=len(v_gen), 
      callbacks=[checkpoints, ], verbose=1)


You can use model.fit() as well. There is still a 40-minute wait for this to start training."
42566,Bandwidth on TPU v3 > 900 GB/s,"I've testing the performance of EmbeddingTPU on GCP TPU v3-8. I found that the computed memory access
bandwidth is beyond > 900 GB/s when using one core (900 GB/s is the theoretical upper bound). I think this may be
a tensorflow bug. The test code is developed in Python interface.

Here is the code TGTBT.py and the tf_env.txt (file is too big to put here, collected using tf_env_collect.sh).
https://github.com/shz0116/testTF/blob/master/TGTBT.py
https://github.com/shz0116/testTF/blob/master/tf_env.txt

The run command and output is listed at the top of TGTBT.py file. It's pretty simple.

~$ python3 -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
v1.12.1-34769-gfd2d4cdb70 2.3.0-dev20200620

**Describe the current behavior**
We see the computed memory bandwidth for one TPU v3 core is > 900 GB/s.
 
**Describe the expected behavior**
We expect the bandwidth number is below 900 GB/s.

**Standalone code to reproduce the issue**
See above.

Update: I found that if I add a statement res.numpy() into the timer, the performance will change from too good (> 900 GB/s) to too bad (1.4 GB/s)."
42565,Kohonen SOM,"Just wondering how come there is no ""factory"" implementation of Kohonen SOM in Tensorflow? Or is there? Is it planned to include it in next releases?"
42564,TFLite Inference Runtime Error with Models Containing Conv2dLstm,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf-nightly-gpu: v1.12.1-39890-gf74cc7a696 2.4.0-dev20200821
(problem also happens on tensorflow-gpu v2.3.0)
- Python version:
3.7.7
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
All model's containing Conv2DLstm layers (https://www.tensorflow.org/api_docs/python/tf/keras/layers/ConvLSTM2D) can be successfully converted to TFLite models, but then they always fail when running inference using the TFLite interpreter. 

Specifically, the following Runtime Error is thrown when the interpreter's `invoke` method is called:
`Runtime Error Caught: Fill dimensions must be >= 0Node number 7 (FILL) failed to invoke.`


**Describe the expected behavior**
All model's containing Conv2DLstm layers can successfully run inference using the TFLite interpreter.

**Standalone code to reproduce the issue**
Here is a standalone script to reproduce this issue with either tf-nightly or tensorflow r2.3.0.

```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np

# a function to convert the input tensorflow model to a tflite model and test the models
# with some test data
def convert_and_test(model, test_data):    
    converter = tf.lite.TFLiteConverter.from_keras_model(model)

    # this is needed to convert tf.Slice, see https://github.com/tensorflow/tensorflow/issues/35590
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,
                                           tf.lite.OpsSet.SELECT_TF_OPS]
    tflite_model = converter.convert()

    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    input_details = interpreter.get_input_details()

    # since model has dynamic input shape, we need to reshape the input tensor each time that shape changes
    interpreter.resize_tensor_input(input_details[0]['index'], test_data.shape)
    interpreter.allocate_tensors()
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()
    
    # print IO details after resizing and tensor allocation
    print(input_details)
    print(output_details)

    # Test regular model on data
    tf_results = model(test_data)
    
    # verify shape is correct
    print('Test data shape: ', test_data.shape)
    print('TF model output shape: ', tf_results.shape)
    
    # Test the model
    interpreter.set_tensor(input_details[0]['index'], test_data)
    
    try:
        interpreter.invoke()

        # get output if inference was successful
        tflite_results = interpreter.get_tensor(output_details[0]['index'])

        # verify shape is correct
        print('TF lite model output shape: ', tflite_results.shape)

        # Compare model results
        for tf_result, tflite_result in zip(tf_results, tflite_results):
            np.testing.assert_almost_equal(tf_result, tflite_result, decimal=5)

    except RuntimeError as err:
        print('Runtime Error Caught: %s' % err)


# define two helper functions for creating conv2d and conv2dlstms with some
# common features
def _conv_lstm(filters, kernel_size, dilation_rate, return_sequences):
    conv_layer = layers.ConvLSTM2D(filters=filters, 
                                    kernel_size=kernel_size,
                                    strides=(1, 1),
                                    padding='same',
                                    data_format='channels_last',
                                    dilation_rate=dilation_rate,
                                    activation='relu',
                                    recurrent_activation='hard_sigmoid',
                                    return_sequences=return_sequences)
    return conv_layer

def _conv(filters, kernel_size, dilation_rate):
    conv_layer = layers.Conv2D(filters=filters, 
                                    kernel_size=kernel_size,
                                    strides=(1, 1),
                                    padding='same',
                                    data_format='channels_last',
                                    dilation_rate=dilation_rate,
                                    activation='relu')
    return conv_layer

# not important for this example, some arbitrary value
num_classes = 3

# define a model that DOES work
working_inputs = keras.Input(shape=(None, None, 1), name='image_sequence')
working_conv_1 = _conv(filters=8, kernel_size=(3, 3), dilation_rate=(1, 1))(working_inputs)
working_conv_bn_1 = layers.BatchNormalization()(working_conv_1)
working_outputs = layers.Conv2D(filters=num_classes, 
                        kernel_size=(1, 1),
                        strides=(1, 1),
                        padding='same',
                        data_format='channels_last',
                        dilation_rate=(1, 1),
                        activation=None)(working_conv_bn_1)
working_model = keras.Model(inputs=working_inputs, outputs=working_outputs)
working_model.summary(line_length=140)

# create some data to test model with
working_data = np.random.rand(1, 256, 128, 1).astype(np.float32)

# show that model fails to invoke
print('About to convert and test working fully convolutional model')
convert_and_test(working_model, working_data)

# define a model that DOES NOT work
failing_inputs = keras.Input(shape=(None, None, None, 1), name='image_sequence')
failing_conv_lstm_1 = _conv_lstm(filters=8, kernel_size=(3, 3), dilation_rate=(1, 1), return_sequences=False)(failing_inputs)
failing_conv_lstm_bn_1 = layers.BatchNormalization()(failing_conv_lstm_1)
failing_outputs = layers.Conv2D(filters=num_classes, 
                                kernel_size=(1, 1),
                                strides=(1, 1),
                                padding='same',
                                data_format='channels_last',
                                dilation_rate=(1, 1),
                                activation=None)(failing_conv_lstm_bn_1)

failing_model = keras.Model(inputs=failing_inputs, outputs=failing_outputs)
failing_model.summary(line_length=140)

# create some data to test model with
# since we are using Conv2dLstms here, dimension 1 (0-based) is sequence length
failing_data = np.random.rand(1, 3, 256, 128, 1).astype(np.float32)

# show that model fails to invoke
print('About to convert and test failing fully convolutional LSTM model')
convert_and_test(failing_model, failing_data)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

One interesting thing I noticed is that the output_details for the model that fails does not seem to properly reshape the output after the call to allocate_tensors(). I'm not sure if that is related, but it was the one difference between the working model and failing model that I noticed (besides the RuntimeError, of course).

Please let me know if there is anything else I can do to help diagnose and fix this, or if you have any other questions. I'm very determined to solve this issue ASAP.

For completeness, here is total output of the script above:

```
2020-08-21 14:36:44.930663: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:45.796378: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2020-08-21 14:36:45.801517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:42:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2020-08-21 14:36:45.801546: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:45.803090: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:45.804734: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-21 14:36:45.804935: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-21 14:36:45.806469: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-21 14:36:45.807182: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-21 14:36:45.810354: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:45.812420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-21 14:36:45.812791: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2020-08-21 14:36:45.837505: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2994095000 Hz
2020-08-21 14:36:45.839632: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed46b621a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-21 14:36:45.839661: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-08-21 14:36:46.496091: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ed46bcdcb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-08-21 14:36:46.496148: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): TITAN RTX, Compute Capability 7.5
2020-08-21 14:36:46.498003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:42:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2020-08-21 14:36:46.498048: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:46.498078: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:46.498094: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-21 14:36:46.498110: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-21 14:36:46.498126: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-21 14:36:46.498141: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-21 14:36:46.498157: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:46.501695: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-21 14:36:46.501752: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:46.986756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 14:36:46.986826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-21 14:36:46.986834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-21 14:36:46.988998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21417 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:42:00.0, compute capability: 7.5)
WARNING:tensorflow:From /home/jatkinson/anaconda3/envs/tensorflow_2p3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-08-21 14:36:47.466889: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:tensorflow:From /home/jatkinson/anaconda3/envs/tensorflow_2p3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.
Instructions for updating:
This property should not be used in TensorFlow 2.0, as updates are applied automatically.
2020-08-21 14:36:47.753827: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-08-21 14:36:47.754022: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-08-21 14:36:47.755162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:42:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2020-08-21 14:36:47.755198: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:47.755226: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:47.755237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-21 14:36:47.755246: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-21 14:36:47.755256: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-21 14:36:47.755265: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-21 14:36:47.755275: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:47.755957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-21 14:36:47.755992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 14:36:47.755997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-21 14:36:47.756002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-21 14:36:47.756718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21417 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:42:00.0, compute capability: 7.5)
2020-08-21 14:36:47.763790: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-08-21 14:36:47.763842: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.008ms.
2020-08-21 14:36:47.763850: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-08-21 14:36:47.796767: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-08-21 14:36:47.796837: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
2020-08-21 14:36:47.800774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:42:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2020-08-21 14:36:47.800822: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:47.800846: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:47.800856: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-21 14:36:47.800864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-21 14:36:47.800873: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-21 14:36:47.800881: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-21 14:36:47.800890: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:47.801613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-21 14:36:47.801647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 14:36:47.801653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-21 14:36:47.801658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-21 14:36:47.802417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21417 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:42:00.0, compute capability: 7.5)
2020-08-21 14:36:47.816103: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:49.126173: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:51.254963: I tensorflow/core/grappler/devices.cc:69] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-08-21 14:36:51.255136: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session
2020-08-21 14:36:51.256116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:42:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2020-08-21 14:36:51.256160: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:51.256184: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:51.256194: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-21 14:36:51.256205: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-21 14:36:51.256227: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-21 14:36:51.256237: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-21 14:36:51.256248: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:51.256905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-21 14:36:51.256941: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 14:36:51.256946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-21 14:36:51.256952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-21 14:36:51.257671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21417 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:42:00.0, compute capability: 7.5)
2020-08-21 14:36:51.267953: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: graph_to_optimize
2020-08-21 14:36:51.268000: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 112 nodes (0), 133 edges (0), time = 1.388ms.
2020-08-21 14:36:51.268004: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: Graph size after: 112 nodes (0), 133 edges (0), time = 1.449ms.
2020-08-21 14:36:51.268008: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: functional_3_conv_lst_m2d_while_body_5034
2020-08-21 14:36:51.268012: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-08-21 14:36:51.268015: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-08-21 14:36:51.268019: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:816] Optimization results for grappler item: functional_3_conv_lst_m2d_while_cond_5033
2020-08-21 14:36:51.268022: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0.001ms.
2020-08-21 14:36:51.268026: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:818]   function_optimizer: function_optimizer did nothing. time = 0ms.
2020-08-21 14:36:51.342805: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:313] Ignored output_format.
2020-08-21 14:36:51.342872: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:316] Ignored drop_control_dependency.
INFO: Created TensorFlow Lite delegate for select TF ops.
2020-08-21 14:36:51.388179: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:42:00.0 name: TITAN RTX computeCapability: 7.5
coreClock: 1.77GHz coreCount: 72 deviceMemorySize: 23.65GiB deviceMemoryBandwidth: 625.94GiB/s
2020-08-21 14:36:51.388213: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2020-08-21 14:36:51.388235: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2020-08-21 14:36:51.388244: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2020-08-21 14:36:51.388252: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2020-08-21 14:36:51.388260: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2020-08-21 14:36:51.388268: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2020-08-21 14:36:51.388276: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2020-08-21 14:36:51.388956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2020-08-21 14:36:51.388990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 14:36:51.388996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2020-08-21 14:36:51.389000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2020-08-21 14:36:51.389756: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21417 MB memory) -> physical GPU (device: 0, name: TITAN RTX, pci bus id: 0000:42:00.0, compute capability: 7.5)
INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 16 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 0 nodes delegated out of 1 nodes with 0 partitions.

INFO: TfLiteFlexDelegate delegate: 2 nodes delegated out of 41 nodes with 1 partitions.

Model: ""functional_1""
____________________________________________________________________________________________________________________________________________
Layer (type)                                                   Output Shape                                            Param #              
============================================================================================================================================
image_sequence (InputLayer)                                    [(None, None, None, 1)]                                 0                    
____________________________________________________________________________________________________________________________________________
conv2d (Conv2D)                                                (None, None, None, 8)                                   80                   
____________________________________________________________________________________________________________________________________________
batch_normalization (BatchNormalization)                       (None, None, None, 8)                                   32                   
____________________________________________________________________________________________________________________________________________
conv2d_1 (Conv2D)                                              (None, None, None, 3)                                   27                   
============================================================================================================================================
Total params: 139
Trainable params: 123
Non-trainable params: 16
____________________________________________________________________________________________________________________________________________
About to convert and test working fully convolutional model
[{'name': 'image_sequence', 'index': 0, 'shape': array([  1, 256, 128,   1], dtype=int32), 'shape_signature': array([-1, -1, -1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'Identity', 'index': 9, 'shape': array([  1, 256, 128,   3], dtype=int32), 'shape_signature': array([-1, -1, -1,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Test data shape:  (1, 256, 128, 1)
TF model output shape:  (1, 256, 128, 3)
TF lite model output shape:  (1, 256, 128, 3)
Model: ""functional_3""
____________________________________________________________________________________________________________________________________________
Layer (type)                                                   Output Shape                                            Param #              
============================================================================================================================================
image_sequence (InputLayer)                                    [(None, None, None, None, 1)]                           0                    
____________________________________________________________________________________________________________________________________________
conv_lst_m2d (ConvLSTM2D)                                      (None, None, None, 8)                                   2624                 
____________________________________________________________________________________________________________________________________________
batch_normalization_1 (BatchNormalization)                     (None, None, None, 8)                                   32                   
____________________________________________________________________________________________________________________________________________
conv2d_2 (Conv2D)                                              (None, None, None, 3)                                   27                   
============================================================================================================================================
Total params: 2,683
Trainable params: 2,667
Non-trainable params: 16
____________________________________________________________________________________________________________________________________________
About to convert and test failing fully convolutional LSTM model
[{'name': 'image_sequence', 'index': 0, 'shape': array([  1,   3, 256, 128,   1], dtype=int32), 'shape_signature': array([-1, -1, -1, -1,  1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
[{'name': 'Identity', 'index': 37, 'shape': array([1, 1, 1, 3], dtype=int32), 'shape_signature': array([-1, -1, -1,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
Test data shape:  (1, 3, 256, 128, 1)
TF model output shape:  (1, 256, 128, 3)
Runtime Error Caught: Fill dimensions must be >= 0Node number 5 (FILL) failed to invoke.
```"
42563,TFLite Inference Runtime Error with Model' Conv2dLstm,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42561,Bazel fixes for other toolchains should be global,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Redhat 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.15.3+nv20.07
- Python version: 3.8.5
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source):9.3.0
- CUDA/cuDNN version:Cuda 11.0.207 cuDNN 8.0.1.13
- GPU model and memory: Nvidia A100

**Describe the current behavior**
```
(...)  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -shared -o bazel-out/k8-py2-opt/bin/tensorflow/python/_tf_stack.so '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,--version-script bazel-out/k8-py2-opt/bin/tensorflow/python/_tf_stack-version-script.lds -Wl,-no-as-needed -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/k8-py2-opt/bin/tensorflow/python/_tf_stack.so-2.params)
Execution platform: @bazel_tools//platforms:host_platform
/usr/bin/ld.gold: --push-state: unknown option
/usr/bin/ld.gold: use the --help option for usage information
collect2: error: ld returned 1 exit status
```

**Describe the expected behavior**
Fixed ld.gold versions work

**Code to reproduce the issue**
Compile with standard Redhat's GCC

**Other info / logs**
You describe the issue yourself here: https://github.com/tensorflow/tensorflow/blob/master/third_party/gpus/cuda_configure.bzl#L1192

But that does not help with other toolchains other than your own. So the solution is to use a properly patched libtool, and remove the hardcoded path to /usr/bin. The fine gentlemen of the EasyBuild project have a patch that does exactly this: https://github.com/easybuilders/easybuild-easyconfigs/blob/master/easybuild/easyconfigs/t/TensorFlow/TensorFlow-1.13.1_remove_usrbin_from_linker_bin_path_flag.patch"
42559,Error while converting from .pb to .tflite ,"The following error is being generated while I'm trying to convert my pb model file to tflite file.

```
2020-08-21 22:03:17.595865: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library nvcuda.dll
2020-08-21 22:03:17.638981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56
pciBusID: 0000:01:00.0
2020-08-21 22:03:17.645082: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-08-21 22:03:17.650409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-08-21 22:03:17.656789: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2020-08-21 22:03:17.663393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56
pciBusID: 0000:01:00.0
2020-08-21 22:03:17.668816: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-08-21 22:03:17.674059: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-08-21 22:03:21.070834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 22:03:21.074328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2020-08-21 22:03:21.076231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2020-08-21 22:03:21.080389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2927 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-08-21 22:03:27.639983: I tensorflow/core/grappler/devices.cc:55] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1
2020-08-21 22:03:27.644641: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2020-08-21 22:03:27.648974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties:
name: GeForce GTX 1650 major: 7 minor: 5 memoryClockRate(GHz): 1.56
pciBusID: 0000:01:00.0
2020-08-21 22:03:27.656568: I tensorflow/stream_executor/platform/default/dlopen_checker_stub.cc:25] GPU libraries are statically linked, skip dlopen check.
2020-08-21 22:03:27.663018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0
2020-08-21 22:03:27.665528: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-08-21 22:03:27.670558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0
2020-08-21 22:03:27.672619: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N
2020-08-21 22:03:27.674983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2927 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5)
2020-08-21 22:03:32.204885: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:716] Optimization results for grappler item: graph_to_optimize
2020-08-21 22:03:32.208626: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7082 nodes (-490), 10726 edges (-491), time = 1643.91504ms.
2020-08-21 22:03:32.212527: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:718]   constant folding: Graph size after: 7082 nodes (0), 10726 edges (0), time = 598.941ms.
Traceback (most recent call last):
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\Scripts\toco-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 503, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 499, in run_main
    _convert_tf1_model(tflite_flags)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 193, in _convert_tf1_model
    output_data = converter.convert()
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\python\lite.py"", line 898, in convert
    **converter_kwargs)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\python\convert.py"", line 404, in toco_convert_impl
    input_data.SerializeToString())
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\python\convert.py"", line 172, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\framework\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\framework\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\framework\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\framework\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\framework\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\framework\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([(""qint8"", np.int8, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([(""quint8"", np.uint8, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([(""qint16"", np.int16, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([(""quint16"", np.uint16, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([(""qint32"", np.int32, 1)])
C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorboard\compat\tensorflow_stub\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([(""resource"", np.ubyte, 1)])
2020-08-21 22:04:44.796956: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: FIFOQueueV2
2020-08-21 22:04:44.814263: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""NoOp"" device_type: ""CPU""') for unknown op: NoOp
2020-08-21 22:04:44.814667: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""NoOp"" device_type: ""GPU""') for unknown op: NoOp
2020-08-21 22:04:44.815133: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostRecv"" device_type: ""GPU"" host_memory_arg: ""tensor""') for unknown op: _HostRecv
2020-08-21 22:04:44.815574: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Send"" device_type: ""CPU""') for unknown op: _Send
2020-08-21 22:04:44.815923: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostRecv"" device_type: ""CPU""') for unknown op: _HostRecv
2020-08-21 22:04:44.816285: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Send"" device_type: ""GPU""') for unknown op: _Send
2020-08-21 22:04:44.816618: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Recv"" device_type: ""CPU""') for unknown op: _Recv
2020-08-21 22:04:44.816993: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostSend"" device_type: ""GPU"" host_memory_arg: ""tensor""') for unknown op: _HostSend
2020-08-21 22:04:44.817412: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_Recv"" device_type: ""GPU""') for unknown op: _Recv
2020-08-21 22:04:44.817697: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""_HostSend"" device_type: ""CPU""') for unknown op: _HostSend
2020-08-21 22:04:44.817961: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""CPU""') for unknown op: WrapDatasetVariant
2020-08-21 22:04:44.818266: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""WrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: WrapDatasetVariant
2020-08-21 22:04:44.818730: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""CPU""') for unknown op: UnwrapDatasetVariant
2020-08-21 22:04:44.819075: E tensorflow/core/framework/op_kernel.cc:1426] OpKernel ('op: ""UnwrapDatasetVariant"" device_type: ""GPU"" host_memory_arg: ""input_handle"" host_memory_arg: ""output_handle""') for unknown op: UnwrapDatasetVariant
2020-08-21 22:04:44.820166: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: FIFOQueueV2
2020-08-21 22:04:45.055786: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: QueueDequeueUpToV2
2020-08-21 22:04:45.062161: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: QueueDequeueUpToV2
2020-08-21 22:04:45.069979: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.070267: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.070687: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.071009: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.071499: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.071767: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.072164: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.072399: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.072714: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.072944: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.073255: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.073526: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.073978: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.074437: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.074677: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.075144: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.075363: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.075630: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.076161: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.076447: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.076664: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.076877: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.077287: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.077563: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.078077: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.078291: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.078502: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.078794: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.079051: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.079262: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.079752: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.080073: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.080325: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.080688: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.081058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.081355: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.081842: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.082050: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.082263: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.082747: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.082992: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.083223: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.083700: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.083932: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.084143: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.084413: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.084793: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.085070: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.085578: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.085910: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.086128: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.086342: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.086559: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.086765: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.087274: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.087485: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.087699: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.087904: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.088286: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.088499: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.089035: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.089361: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.089583: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.089795: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.090012: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.090223: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.090720: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.090931: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.091142: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.091416: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.091793: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.091999: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.093290: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.093599: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.093814: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.094021: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.094388: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.094594: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.094916: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.095183: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.095590: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.095799: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.096008: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.096213: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.096609: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.096825: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.097282: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.097495: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.098023: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.098249: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.098462: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.098668: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.099040: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.099255: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.099582: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.099787: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.100253: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.100544: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.100758: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.100966: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.101335: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.101541: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.101980: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.102187: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.102642: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.102850: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.103058: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.103263: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.103630: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.103835: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.104161: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.104467: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.104934: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.105140: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.105351: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.105557: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.105927: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.106136: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.106472: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.106679: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.107204: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.107507: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.107782: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.107990: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.108378: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.108586: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.108919: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.109127: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.109592: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.109802: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.110012: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.110216: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.110593: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.110798: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.111180: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.111388: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.111853: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.112062: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.112275: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.112476: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.112851: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.113054: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.113390: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.113596: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.114093: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.114326: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.114539: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.114744: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.115146: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.115354: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.115692: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.115906: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.116497: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.116740: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.116969: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.117274: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.118154: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.118448: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.119075: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.119301: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.120202: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.120668: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.120890: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.121102: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.121318: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.121527: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.122257: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.122470: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.122679: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.122890: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.123110: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.123320: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.124627: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.124903: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.125548: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.125929: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.126250: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.126544: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.127186: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.127442: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.127857: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.128069: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.128641: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.128930: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.129144: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.129352: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.129738: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.129946: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.130291: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.130494: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.130995: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.131269: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.131483: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.131689: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.132131: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.132339: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.132739: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.133007: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.133815: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.134120: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.134415: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.134699: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.135283: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.135572: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.136126: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.136414: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.137218: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.137537: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.137827: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.138114: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.138523: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.138732: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.139303: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.139513: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.140030: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.140245: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.140458: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.140889: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.141286: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.141602: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.141954: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.142212: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.142817: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: Reciprocal
2020-08-21 22:04:45.143028: I tensorflow/lite/toco/import_tensorflow.cc:1385] Unable to determine output type for op: Reciprocal
2020-08-21 22:04:45.961170: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 5663 operators, 8446 arrays (0 quantized)
2020-08-21 22:04:46.802380: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 4317 operators, 6315 arrays (0 quantized)
2020-08-21 22:04:47.543171: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 4317 operators, 6315 arrays (0 quantized)
2020-08-21 22:04:48.477362: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 3554 operators, 5619 arrays (0 quantized)
2020-08-21 22:04:49.187676: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 3554 operators, 5619 arrays (0 quantized)
2020-08-21 22:04:49.715892: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 3554 operators, 5619 arrays (0 quantized)
2020-08-21 22:04:50.394162: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2020-08-21 22:04:50.571496: E tensorflow/lite/toco/toco_tooling.cc:456] We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, FLOOR, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, MAX_POOL_2D, MEAN, MUL, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SQUARE, SQUARED_DIFFERENCE, SUB, SUM. Here is a list of operators for which you will need custom implementations: FIFOQueueV2, Merge, QueueDequeueUpToV2, RandomUniform, Reciprocal, Switch.
Traceback (most recent call last):
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\Scripts\toco_from_protos-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 59, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\absl\app.py"", line 299, in run
    _run_main(main, args)
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\absl\app.py"", line 250, in _run_main
    sys.exit(main(argv))
  File ""C:\Users\ghosh\anaconda3\envs\tf_gpu2\lib\site-packages\tensorflow\lite\toco\python\toco_from_protos.py"", line 33, in execute
    output_str = tensorflow_wrap_toco.TocoConvert(model_str, toco_str, input_str)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, AVERAGE_POOL_2D, CAST, CONCATENATION, CONV_2D, FLOOR, FULLY_CONNECTED, GATHER, L2_NORMALIZATION, MAX_POOL_2D, MEAN, MUL, REDUCE_PROD, RESHAPE, RSQRT, SHAPE, SQUARE, SQUARED_DIFFERENCE, SUB, SUM. Here is a list of operators for which you will need custom implementations: FIFOQueueV2, Merge, QueueDequeueUpToV2, RandomUniform, Reciprocal, Switch.
```"
42558,gen_nccl_ops.nccl_all_reduce will influence optimizer!!!,"My version is tf 1.13, I use multi-GPUs to  train a model, when i minimize the loss in 0-gpu, 1-gpu's is not minimized, there is someting strange.
so this is my code 

<img width=""701"" alt=""20200821-224654(WeLinkPC)"" src=""https://user-images.githubusercontent.com/34296901/90903315-58d97280-e400-11ea-9f58-a2a56ed87b76.png"">


then when i run the code , it will keep stopping at a point. After debuging, i find that when  i use nccl_all_reduce in syncBN, this will appear, like this

<img width=""588"" alt=""20200821-224658(WeLinkPC)"" src=""https://user-images.githubusercontent.com/34296901/90903324-5e36bd00-e400-11ea-87a9-bb44d21d66cd.png"">


when i comment the two line, everything is ok.
By th way, i minimize all losses in all gpus using compute_gradient and apply_gradient, when using nccl_reduce_all,  the model could't converge, after commentting it everything  is ok.

I think it's a bug."
42557,how to use tensorflow2 to identify  captcha image?,"the captcha image has  four characters, how to output?"
42556,Can't install tensorflow - numpy incompatibility?,"**System information**
- Windows10 x64
- Python version 3.8
- Tryed to install in a virtualenv? using conda and pip
- CUDA/cuDNN version: 9.2
- GPU model and memory: NVIDIA GeForce GTX 950M



**Problem**
I created a virtual environment and there I started to install necessary packages. When I tried to run a script that used tensorflow, it could not imported it.
After searching for this, I tried to import it in jupyter notebook, but it didn't work.
Then, I tried to add this to my code but it didn't work either:
tf.compat.v1.enable_eager_execution(
config=None, device_policy=None, execution_mode=None
)
I tried to install tensorflow in that environment using first conda and then pip.


**Sequence of commands / steps executed**
-Runing the script:

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\utils\json.py:64: ResourceWarning: unclosed file <_io.TextIOWrapper name='D:\\Software\\Anaconda_envs\\sweaver\\avgn_paper-vizmerge\\data\\processed\\sociable_weaver_damelio\\2020-08-21_09-35-13\\JSON\\2018-10-19_09-00-00-000001.JSON' mode='r' encoding='cp1252'>
  return json.load(open(json_loc), object_pairs_hook=OrderedDict)
ResourceWarning: Enable tracemalloc to get the object allocation traceback
HBox(children=(FloatProgress(value=0.0, description='loading json', max=1.0, style=ProgressStyle(description_w…
[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.

[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.0s finished
HBox(children=(FloatProgress(value=0.0, description='getting unique individuals', max=1.0, style=ProgressStyle…
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-43-e50f361ab3a6> in <module>
      1 # create a dataset object
----> 2 dataset = DataSet(DATASET_ID, hparams = hparams)

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\dataset.py in __init__(self, DATASET_ID, hparams, default_rate, build_mel_matrix)
     43 
     44         if build_mel_matrix:
---> 45             self.build_mel_matrix()
     46 
     47     def _get_wav_json_files(self):

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\dataset.py in build_mel_matrix(self, rate)
     66         if rate is None:
     67             rate = self.sample_json[""samplerate_hz""]
---> 68         self.mel_matrix = prepare_mel_matrix(self.hparams, rate)
     69 
     70     def _get_unique_individuals(self):

d:\software\anaconda_envs\sweaver\avgn_paper-vizmerge\avgn\signalprocessing\filtering.py in prepare_mel_matrix(hparams, rate, return_numpy, GPU_backend)
     71             os.environ[""CUDA_DEVICE_ORDER""] = ""PCI_BUS_ID""  # see issue #152
     72             os.environ[""CUDA_VISIBLE_DEVICES""] = """"
---> 73         import tensorflow as tf
     74 
     75     tf.compat.v1.enable_eager_execution(

ModuleNotFoundError: No module named 'tensorflow'

-Import it in jupyter notebook
import sys
!conda install --yes --prefix {sys.prefix} tensorflow

UnsatisfiableError: The following specifications were found to be incompatible with the existing python installation in your environment:
Specifications:
- tensorflow -> python[version'3.5.*|3.6.*|3.7.*'
Your python: python=3.8

The following specifications were found to be incompatible with your CUDA driver:
- feature:/win-64::__cuda==9.2=0
Your installed CUDA driver is: 9.2

-When I tried to install it through conda:

(D:\Software\Anaconda_envs\sweaver) D:\Software\Anaconda_envs\sweaver>conda install tensorflow
Collecting package metadata (current_repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.
Solving environment: -
Found conflicts! Looking for incompatible packages.
This can take several minutes.  Press CTRL-C to abort.
Examining @/win-64::__cuda==9.2=0:  67%|████████████████████████████████▋                | 2/3 [00:00<00:00, 18.17it/s]|Examining conflict for __cuda:  67%|███████████████████████████████████▎                 | 2/3 [00:00<00:00,  3.77it/s]|failed

UnsatisfiableError: The following specifications were found
to be incompatible with the existing python installation in your environment:

Specifications:

  - tensorflow -> python[version='3.5.*|3.6.*|3.7.*']

Your python: python=3.8

If python is on the left-most side of the chain, that's the version you've asked for.
When python appears to the right, that indicates that the thing on the left is somehow
not available for the python version you are constrained to. Note that conda will not
change your python version to a different minor version unless you explicitly specify
that.

The following specifications were found to be incompatible with your system:

  - feature:/win-64::__cuda==9.2=0
  - feature:|@/win-64::__cuda==9.2=0

Your installed version is: 9.2


-When I tried to install it using pip:

(D:\Software\Anaconda_envs\sweaver) D:\Software\Anaconda_envs\sweaver>pip install tensorflow
Collecting tensorflow
  Using cached tensorflow-2.3.0-cp38-cp38-win_amd64.whl (342.5 MB)
Requirement already satisfied: wrapt>=1.11.1 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (1.11.2)
Collecting keras-preprocessing<1.2,>=1.1.1
  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)
Collecting tensorboard<3,>=2.3.0
  Using cached tensorboard-2.3.0-py3-none-any.whl (6.8 MB)
Requirement already satisfied: grpcio>=1.8.6 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (1.31.0)
Collecting numpy<1.19.0,>=1.16.0
  Using cached numpy-1.18.5-cp38-cp38-win_amd64.whl (12.8 MB)
Collecting opt-einsum>=2.3.2
  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)
Requirement already satisfied: absl-py>=0.7.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (0.10.0)
Requirement already satisfied: wheel>=0.26 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (0.34.2)
Collecting scipy==1.4.1
  Using cached scipy-1.4.1-cp38-cp38-win_amd64.whl (31.0 MB)
Collecting astunparse==1.6.3
  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)
Requirement already satisfied: h5py<2.11.0,>=2.10.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (2.10.0)
Collecting protobuf>=3.9.2
  Using cached protobuf-3.13.0-py2.py3-none-any.whl (438 kB)
Collecting google-pasta>=0.1.8
  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)
Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (2.3.0)
Requirement already satisfied: gast==0.3.3 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (0.3.3)
Requirement already satisfied: six>=1.12.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorflow) (1.15.0)
Processing c:\users\bf\appdata\local\pip\cache\wheels\a0\16\9c\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\termcolor-1.1.0-py3-none-any.whl
Requirement already satisfied: werkzeug>=0.11.15 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)
Collecting markdown>=2.6.8
  Using cached Markdown-3.2.2-py3-none-any.whl (88 kB)
Collecting google-auth-oauthlib<0.5,>=0.4.1
  Using cached google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)
Requirement already satisfied: setuptools>=41.0.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.6.0.post20200814)
Collecting tensorboard-plugin-wit>=1.6.0
  Using cached tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)
Requirement already satisfied: requests<3,>=2.21.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)
Requirement already satisfied: google-auth<2,>=1.6.3 in d:\software\anaconda_envs\sweaver\lib\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.20.1)
Collecting requests-oauthlib>=0.7.0
  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)
Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.10)
Requirement already satisfied: certifi>=2017.4.17 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)
Requirement already satisfied: chardet<4,>=3.0.2 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)
Requirement already satisfied: idna<3,>=2.5 in d:\software\anaconda_envs\sweaver\lib\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)
Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\software\anaconda_envs\sweaver\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)
Requirement already satisfied: rsa<5,>=3.1.4; python_version >= ""3.5"" in d:\software\anaconda_envs\sweaver\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.5)
Requirement already satisfied: cachetools<5.0,>=2.0.0 in d:\software\anaconda_envs\sweaver\lib\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)
Collecting oauthlib>=3.0.0
  Using cached oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)
Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\software\anaconda_envs\sweaver\lib\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)
ERROR: Error while checking for conflicts. Please file an issue on pip's issue tracker: https://github.com/pypa/pip/issues/new
Traceback (most recent call last):
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 3021, in _dep_map
    return self.__dep_map
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 2815, in __getattr__
    raise AttributeError(attr)
AttributeError: _DistInfoDistribution__dep_map

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 3012, in _parsed_pkg_info
    return self._pkg_info
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 2815, in __getattr__
    raise AttributeError(attr)
AttributeError: _pkg_info

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_internal\commands\install.py"", line 535, in _determine_conflicts
    return check_install_conflicts(to_install)
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_internal\operations\check.py"", line 108, in check_install_conflicts
    package_set, _ = create_package_set_from_installed()
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_internal\operations\check.py"", line 50, in create_package_set_from_installed
    package_set[name] = PackageDetails(dist.version, dist.requires())
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 2736, in requires
    dm = self._dep_map
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 3023, in _dep_map
    self.__dep_map = self._compute_dependencies()
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 3032, in _compute_dependencies
    for req in self._parsed_pkg_info.get_all('Requires-Dist') or []:
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 3014, in _parsed_pkg_info
    metadata = self.get_metadata(self.PKG_INFO)
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1420, in get_metadata
    value = self._get(path)
  File ""D:\Software\Anaconda_envs\sweaver\lib\site-packages\pip\_vendor\pkg_resources\__init__.py"", line 1616, in _get
    with open(path, 'rb') as stream:
FileNotFoundError: [Errno 2] No such file or directory: 'd:\\software\\anaconda_envs\\sweaver\\lib\\site-packages\\numpy-1.19.1.dist-info\\METADATA'
Installing collected packages: numpy, keras-preprocessing, protobuf, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard-plugin-wit, tensorboard, opt-einsum, scipy, astunparse, google-pasta, termcolor, tensorflow
  Attempting uninstall: numpy
    Found existing installation: numpy 1.19.1
ERROR: Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'd:\\software\\anaconda_envs\\sweaver\\lib\\site-packages\\numpy-1.19.1.dist-info\\RECORD'


Maybe it's a problem of compatibility between tensorflow and numpy? But the file 'd:\\software\\anaconda_envs\\sweaver\\lib\\site-packages\\numpy-1.19.1.dist-info\\RECORD' actually does not exist.
I searched in my packages and there is a folder of numpy, numpy-1.18.5.dist-info and numpy-1.19.1.dist-info, but inside the last one, there are only two files: LICENSES_bundled.txt and REQUESTED.

Hope you can help me. I began using python just 2 weeks ago.
"
42554,Regarding RISCV-V Vector ISA extension on CPU / RISC-V,As I understood is RISC-V Vector ISA extension is available with Tensorflow lite for mobile/IOT device. Can we also use the same (RISC-V Vector ISA enabled) for RISC-V machines or CPU by cross-compiling the same for RISC-V for neural network applications?   
42553,StringLookup layer adds None to list of saved model variables which breaks hub.KerasLayer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-39890-gf74cc7a696 2.4.0-dev20200821
- Python version: Colab default
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
When loading (tf.saved_model.load) saved model (tf.saved_model.save) with StringLookup layer, None appears in list of model variables.
This breaks loading such model in tensorflow_hub.KerasLayer

**Describe the expected behavior**
Saved model should not contain None in list of variables

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1sVGGeJZ2rq6PofMXJ31gknhnYomHFyX7?usp=sharing
"
42552,Add a tf.feature_column like sklearn.preprocessing.MultiLabelBinarizer,"**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No):
If I can ......

**Describe the feature and the current behavior/state.**
```tf.feature_column``` doesn't have any column for dealing with multi-hot feature like ```sklearn.preprocessing.MultiLabelBinarizer``` now.

**Will this change the current api? How?**
I guess not

**Who will benefit with this feature?**
Everyone who has to deal with multi-hot feature 

**Any Other info.**
"
42550,Tensorflow Lite python interpreter for armv6,"Very simple question:

I'd like to install the Lite python interpreter ([https://www.tensorflow.org/lite/guide/python](https://www.tensorflow.org/lite/guide/python)) on an old RaspberryPi, that has an armv6.

At that page there is a list of downloadable Python wheels, but there is not the right one for my Pi, in fact, there are none for armv6. 

Ideally I would need: armv6, Py3.7, Tensorflow version 2.3 (actually I don't if version match between TF and TF Lite)

What should I do?

Thanks, have a great day"
42548,ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。,"gpu gtx660
only run pip install tensorflow-gpu
tensorflow-gpu2.1
pycharm
python3.7.6


Traceback (most recent call last):
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Program Files\python\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Program Files\python\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""D:/WorkSpace/py/tf/01.py"", line 1, in <module>
    import tensorflow
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow\__init__.py"", line 101, in <module>
    from tensorflow_core import *
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\__init__.py"", line 40, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow\__init__.py"", line 50, in __getattr__
    module = self._load()
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow\__init__.py"", line 44, in _load
    module = _importlib.import_module(self.__name__)
  File ""D:\Program Files\python\lib\importlib\__init__.py"", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""D:\Program Files\PyCharm\plugins\python\helpers\pydev\_pydev_bundle\pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""D:\WorkSpace\py\venv\lib\site-packages\tensorflow_core\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""D:\Program Files\python\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""D:\Program Files\python\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 动态链接库(DLL)初始化例程失败。
Failed to load the native TensorFlow runtime.
See https://www.tensorflow.org/install/errors
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
42547,TensorFlow nightly build 2.4.0.dev20200816 or newer breaks Horovod (0.19.1 and newer) installation.,"I'm trying to install `Horovod==0.19.1` with OpenMPI support using TensorFlow cpu nightly wheel and I noticed I'm getting the error: (full error log is attached)

```
# pip install --no-cache-dir --ignore-installed horovod==0.19.1
Collecting horovod==0.19.1
  Downloading horovod-0.19.1.tar.gz (2.9 MB)
     |████████████████████████████████| 2.9 MB 1.7 MB/s 
Collecting cloudpickle
  Downloading cloudpickle-1.5.0-py3-none-any.whl (22 kB)
Collecting psutil
  Downloading psutil-5.7.2.tar.gz (460 kB)
     |████████████████████████████████| 460 kB 17.6 MB/s 
Collecting pyyaml
  Downloading PyYAML-5.3.1.tar.gz (269 kB)
     |████████████████████████████████| 269 kB 15.8 MB/s 
Collecting six
  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)
Building wheels for collected packages: horovod, psutil, pyyaml
  Building wheel for horovod (setup.py) ... error
  ERROR: Command errored out with exit status 1:
   command: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-db1tsv5m/horovod/setup.py'""'""'; __file__='""'""'/tmp/pip-install-db1tsv5m/horovod/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-w2tr7faj
       cwd: /tmp/pip-install-db1tsv5m/horovod/
  Complete output (1999 lines):
  running bdist_wheel
  running build
  running build_py
  creating build
  creating build/lib.linux-x86_64-3.6
  creating build/lib.linux-x86_64-3.6/horovod
  copying horovod/__init__.py -> build/lib.linux-x86_64-3.6/horovod
  creating build/lib.linux-x86_64-3.6/horovod/run
  copying horovod/run/task_fn.py -> build/lib.linux-x86_64-3.6/horovod/run
  copying horovod/run/mpi_run.py -> build/lib.linux-x86_64-3.6/horovod/run
  copying horovod/run/run_task.py -> build/lib.linux-x86_64-3.6/horovod/run
  copying horovod/run/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run
  copying horovod/run/gloo_run.py -> build/lib.linux-x86_64-3.6/horovod/run
  copying horovod/run/run.py -> build/lib.linux-x86_64-3.6/horovod/run
  creating build/lib.linux-x86_64-3.6/horovod/spark
  copying horovod/spark/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark
  creating build/lib.linux-x86_64-3.6/horovod/torch
  copying horovod/torch/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/torch
  copying horovod/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch
  copying horovod/torch/compression.py -> build/lib.linux-x86_64-3.6/horovod/torch
  creating build/lib.linux-x86_64-3.6/horovod/_keras
  copying horovod/_keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/_keras
  copying horovod/_keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/_keras
  creating build/lib.linux-x86_64-3.6/horovod/common
  copying horovod/common/basics.py -> build/lib.linux-x86_64-3.6/horovod/common
  copying horovod/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/common
  copying horovod/common/util.py -> build/lib.linux-x86_64-3.6/horovod/common
  creating build/lib.linux-x86_64-3.6/horovod/keras
  copying horovod/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/keras
  copying horovod/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/keras
  creating build/lib.linux-x86_64-3.6/horovod/mxnet
  copying horovod/mxnet/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/mxnet
  copying horovod/mxnet/__init__.py -> build/lib.linux-x86_64-3.6/horovod/mxnet
  creating build/lib.linux-x86_64-3.6/horovod/tensorflow
  copying horovod/tensorflow/mpi_ops.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow
  copying horovod/tensorflow/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow
  copying horovod/tensorflow/compression.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow
  copying horovod/tensorflow/util.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow
  creating build/lib.linux-x86_64-3.6/horovod/run/driver
  copying horovod/run/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/driver
  copying horovod/run/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/driver
  creating build/lib.linux-x86_64-3.6/horovod/run/http
  copying horovod/run/http/http_client.py -> build/lib.linux-x86_64-3.6/horovod/run/http
  copying horovod/run/http/http_server.py -> build/lib.linux-x86_64-3.6/horovod/run/http
  copying horovod/run/http/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/http
  creating build/lib.linux-x86_64-3.6/horovod/run/util
  copying horovod/run/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/util
  copying horovod/run/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/util
  copying horovod/run/util/threads.py -> build/lib.linux-x86_64-3.6/horovod/run/util
  copying horovod/run/util/cache.py -> build/lib.linux-x86_64-3.6/horovod/run/util
  creating build/lib.linux-x86_64-3.6/horovod/run/task
  copying horovod/run/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/task
  copying horovod/run/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/task
  creating build/lib.linux-x86_64-3.6/horovod/run/common
  copying horovod/run/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common
  creating build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/network.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/secret.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/host_hash.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/codec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/env.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/settings.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/timeout.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/safe_shell_exec.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  copying horovod/run/common/util/config_parser.py -> build/lib.linux-x86_64-3.6/horovod/run/common/util
  creating build/lib.linux-x86_64-3.6/horovod/run/common/service
  copying horovod/run/common/service/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service
  copying horovod/run/common/service/__init__.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service
  copying horovod/run/common/service/task_service.py -> build/lib.linux-x86_64-3.6/horovod/run/common/service
  creating build/lib.linux-x86_64-3.6/horovod/spark/driver
  copying horovod/spark/driver/driver_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver
  copying horovod/spark/driver/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver
  copying horovod/spark/driver/mpirun_rsh.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver
  copying horovod/spark/driver/job_id.py -> build/lib.linux-x86_64-3.6/horovod/spark/driver
  creating build/lib.linux-x86_64-3.6/horovod/spark/torch
  copying horovod/spark/torch/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch
  copying horovod/spark/torch/remote.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch
  copying horovod/spark/torch/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch
  copying horovod/spark/torch/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/torch
  creating build/lib.linux-x86_64-3.6/horovod/spark/task
  copying horovod/spark/task/task_info.py -> build/lib.linux-x86_64-3.6/horovod/spark/task
  copying horovod/spark/task/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/task
  copying horovod/spark/task/mpirun_exec_fn.py -> build/lib.linux-x86_64-3.6/horovod/spark/task
  copying horovod/spark/task/task_service.py -> build/lib.linux-x86_64-3.6/horovod/spark/task
  creating build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/constants.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/_namedtuple_fix.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/params.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/serialization.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/store.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/backend.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  copying horovod/spark/common/cache.py -> build/lib.linux-x86_64-3.6/horovod/spark/common
  creating build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/bare.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/estimator.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/optimizer.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/remote.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/tensorflow.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  copying horovod/spark/keras/util.py -> build/lib.linux-x86_64-3.6/horovod/spark/keras
  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib
  copying horovod/torch/mpi_lib/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib
  creating build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl
  copying horovod/torch/mpi_lib_impl/__init__.py -> build/lib.linux-x86_64-3.6/horovod/torch/mpi_lib_impl
  creating build/lib.linux-x86_64-3.6/horovod/tensorflow/keras
  copying horovod/tensorflow/keras/__init__.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras
  copying horovod/tensorflow/keras/callbacks.py -> build/lib.linux-x86_64-3.6/horovod/tensorflow/keras
  running build_ext
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o
  x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.o -o build/temp.linux-x86_64-3.6/test_compile/test_cpp_flags.so
  x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/include/python3.6m -c build/temp.linux-x86_64-3.6/test_compile/test_link_flags.cc -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o
  x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -Wl,--version-script=horovod.lds build/temp.linux-x86_64-3.6/test_compile/test_link_flags.o -o build/temp.linux-x86_64-3.6/test_compile/test_link_flags.so
  INFO: Compilers /usr/bin/gcc and /usr/bin/g++ (version 8.4.0) selected for TensorFlow plugin build.
  -- The CXX compiler identification is GNU 8.4.0
  -- The C compiler identification is GNU 8.4.0
  -- Check for working CXX compiler: /usr/bin/g++
  -- Check for working CXX compiler: /usr/bin/g++ -- works
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  -- Check for working C compiler: /usr/bin/gcc
  -- Check for working C compiler: /usr/bin/gcc -- works
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- Found MPI_C: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so (found version ""3.1"")
  -- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version ""3.1"")
  -- Found MPI: TRUE (found version ""3.1"")
  -- MPI include path: /usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include/usr/lib/x86_64-linux-gnu/openmpi/include
  -- MPI libraries: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so
  -- Configuring done
  -- Generating done
  -- Build files have been written to: /tmp/pip-install-db1tsv5m/horovod/build/temp.linux-x86_64-3.6/gloo/tf
  Scanning dependencies of target gloo
  [  3%] Building CXX object gloo/CMakeFiles/gloo.dir/allgather.cc.o
  [  6%] Building CXX object gloo/CMakeFiles/gloo.dir/algorithm.cc.o
  [  9%] Building CXX object gloo/CMakeFiles/gloo.dir/allreduce.cc.o
  [ 12%] Building CXX object gloo/CMakeFiles/gloo.dir/allgatherv.cc.o
  [ 15%] Building CXX object gloo/CMakeFiles/gloo.dir/allreduce_local.cc.o
  [ 18%] Building CXX object gloo/CMakeFiles/gloo.dir/barrier.cc.o
  [ 21%] Building CXX object gloo/CMakeFiles/gloo.dir/broadcast.cc.o
  [ 24%] Building CXX object gloo/CMakeFiles/gloo.dir/context.cc.o
  [ 27%] Building CXX object gloo/CMakeFiles/gloo.dir/gather.cc.o
  [ 30%] Building CXX object gloo/CMakeFiles/gloo.dir/reduce.cc.o
  [ 33%] Building CXX object gloo/CMakeFiles/gloo.dir/scatter.cc.o
  [ 36%] Building CXX object gloo/CMakeFiles/gloo.dir/types.cc.o
  [ 39%] Building CXX object gloo/CMakeFiles/gloo.dir/common/logging.cc.o
  [ 42%] Building CXX object gloo/CMakeFiles/gloo.dir/common/linux.cc.o
  [ 45%] Building CXX object gloo/CMakeFiles/gloo.dir/mpi/context.cc.o
  [ 48%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/context.cc.o
  [ 51%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/file_store.cc.o
  In file included from /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc:16:
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc: In destructor ‘gloo::mpi::MPIScope::~MPIScope()’:
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:141:58: warning: throw will always call terminate() [-Wterminate]
             r.get_message_and_free(MakeString(__VA_ARGS__))); \
                                                            ^
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:150:3: note: in expansion of macro ‘GLOO_ENFORCE_THAT_IMPL’
     GLOO_ENFORCE_THAT_IMPL(Equals((x), (y)), #x "" == "" #y, __VA_ARGS__)
     ^~~~~~~~~~~~~~~~~~~~~~
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc:43:3: note: in expansion of macro ‘GLOO_ENFORCE_EQ’
     GLOO_ENFORCE_EQ(rv, MPI_SUCCESS);
     ^~~~~~~~~~~~~~~
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:141:58: note: in C++11 destructors default to noexcept
             r.get_message_and_free(MakeString(__VA_ARGS__))); \
                                                            ^
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/common/logging.h:150:3: note: in expansion of macro ‘GLOO_ENFORCE_THAT_IMPL’
     GLOO_ENFORCE_THAT_IMPL(Equals((x), (y)), #x "" == "" #y, __VA_ARGS__)
     ^~~~~~~~~~~~~~~~~~~~~~
  /tmp/pip-install-db1tsv5m/horovod/third_party/gloo/gloo/mpi/context.cc:43:3: note: in expansion of macro ‘GLOO_ENFORCE_EQ’
     GLOO_ENFORCE_EQ(rv, MPI_SUCCESS);
     ^~~~~~~~~~~~~~~
  [ 54%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/hash_store.cc.o
  [ 57%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/prefix_store.cc.o
  [ 60%] Building CXX object gloo/CMakeFiles/gloo.dir/rendezvous/store.cc.o
  [ 63%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/address.cc.o
  [ 66%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/buffer.cc.o
  [ 69%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/context.cc.o
  [ 72%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/device.cc.o
  [ 75%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/pair.cc.o
  [ 78%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/unbound_buffer.cc.o
  [ 81%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/address.cc.o
  [ 84%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/buffer.cc.o
  [ 87%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/context.cc.o
  [ 90%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/device.cc.o
  [ 93%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/pair.cc.o
  [ 96%] Building CXX object gloo/CMakeFiles/gloo.dir/transport/tcp/unbound_buffer.cc.o
  [100%] Linking CXX static library /tmp/pip-install-db1tsv5m/horovod/build/temp.linux-x86_64-3.6/lib/tf/libgloo.a
  [100%] Built target gloo
  building 'horovod.tensorflow.mpi_lib' extension
  creating build/temp.linux-x86_64-3.6/horovod
  creating build/temp.linux-x86_64-3.6/horovod/common
  creating build/temp.linux-x86_64-3.6/horovod/common/ops
  creating build/temp.linux-x86_64-3.6/horovod/common/optim
  creating build/temp.linux-x86_64-3.6/horovod/common/utils
  creating build/temp.linux-x86_64-3.6/horovod/common/mpi
  creating build/temp.linux-x86_64-3.6/horovod/common/ops/adasum
  creating build/temp.linux-x86_64-3.6/horovod/common/gloo
  creating build/temp.linux-x86_64-3.6/horovod/tensorflow
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/common.cc -o build/temp.linux-x86_64-3.6/horovod/common/common.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/controller.cc -o build/temp.linux-x86_64-3.6/horovod/common/controller.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/fusion_buffer_manager.cc -o build/temp.linux-x86_64-3.6/horovod/common/fusion_buffer_manager.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/logging.cc -o build/temp.linux-x86_64-3.6/horovod/common/logging.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/message.cc -o build/temp.linux-x86_64-3.6/horovod/common/message.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/mpi/mpi_context.h:25,
                   from horovod/common/operations.cc:47:
  horovod/common/mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/parameter_manager.cc -o build/temp.linux-x86_64-3.6/horovod/common/parameter_manager.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  horovod/common/parameter_manager.cc: In member function ‘virtual bool horovod::common::ParameterManager::BayesianParameter::IsDoneTuning() const’:
  horovod/common/parameter_manager.cc:466:21: warning: comparison of integer expressions of different signedness: ‘const uint32_t’ {aka ‘const unsigned int’} and ‘const int’ [-Wsign-compare]
     return iteration_ > max_samples_;
            ~~~~~~~~~~~^~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/response_cache.cc -o build/temp.linux-x86_64-3.6/horovod/common/response_cache.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/stall_inspector.cc -o build/temp.linux-x86_64-3.6/horovod/common/stall_inspector.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/thread_pool.cc -o build/temp.linux-x86_64-3.6/horovod/common/thread_pool.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/timeline.cc -o build/temp.linux-x86_64-3.6/horovod/common/timeline.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/tensor_queue.cc -o build/temp.linux-x86_64-3.6/horovod/common/tensor_queue.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/collective_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/collective_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/operation_manager.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/operation_manager.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/optim/bayesian_optimization.cc -o build/temp.linux-x86_64-3.6/horovod/common/optim/bayesian_optimization.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/optim/gaussian_process.cc -o build/temp.linux-x86_64-3.6/horovod/common/optim/gaussian_process.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/utils/env_parser.cc -o build/temp.linux-x86_64-3.6/horovod/common/utils/env_parser.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/half.cc -o build/temp.linux-x86_64-3.6/horovod/common/half.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/half.cc:16:
  horovod/common/half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/mpi/mpi_context.cc -o build/temp.linux-x86_64-3.6/horovod/common/mpi/mpi_context.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/mpi/mpi_context.h:25,
                   from horovod/common/mpi/mpi_context.cc:17:
  horovod/common/mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/mpi/mpi_controller.cc -o build/temp.linux-x86_64-3.6/horovod/common/mpi/mpi_controller.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/mpi/mpi_context.h:25,
                   from horovod/common/mpi/mpi_controller.h:19,
                   from horovod/common/mpi/mpi_controller.cc:16:
  horovod/common/mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/mpi_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/mpi_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/ops/../mpi/mpi_context.h:25,
                   from horovod/common/ops/mpi_operations.h:27,
                   from horovod/common/ops/mpi_operations.cc:17:
  horovod/common/ops/../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/ops/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/adasum/adasum_mpi.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/adasum/adasum_mpi.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/ops/adasum/../../mpi/mpi_context.h:25,
                   from horovod/common/ops/adasum/adasum_mpi.h:21,
                   from horovod/common/ops/adasum/adasum_mpi.cc:16:
  horovod/common/ops/adasum/../../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/ops/adasum/../../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/adasum_mpi_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/adasum_mpi_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/ops/adasum/../../mpi/mpi_context.h:25,
                   from horovod/common/ops/adasum/adasum_mpi.h:21,
                   from horovod/common/ops/adasum_mpi_operations.h:22,
                   from horovod/common/ops/adasum_mpi_operations.cc:16:
  horovod/common/ops/adasum/../../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/ops/adasum/../../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/gloo_context.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/gloo_context.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/gloo/../mpi/mpi_context.h:25,
                   from horovod/common/gloo/gloo_context.h:25,
                   from horovod/common/gloo/gloo_context.cc:16:
  horovod/common/gloo/../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/gloo/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/gloo_controller.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/gloo_controller.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/gloo/../mpi/mpi_context.h:25,
                   from horovod/common/gloo/gloo_context.h:25,
                   from horovod/common/gloo/gloo_controller.h:19,
                   from horovod/common/gloo/gloo_controller.cc:16:
  horovod/common/gloo/../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/gloo/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/http_store.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/http_store.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/gloo/memory_store.cc -o build/temp.linux-x86_64-3.6/horovod/common/gloo/memory_store.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  /usr/bin/gcc -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -DEIGEN_MPL2_ONLY=1 -DHAVE_MPI=1 -DHAVE_GLOO=1 -Ithird_party/HTTPRequest/include -Ithird_party/boost/assert/include -Ithird_party/boost/config/include -Ithird_party/boost/core/include -Ithird_party/boost/detail/include -Ithird_party/boost/iterator/include -Ithird_party/boost/lockfree/include -Ithird_party/boost/mpl/include -Ithird_party/boost/parameter/include -Ithird_party/boost/predef/include -Ithird_party/boost/preprocessor/include -Ithird_party/boost/static_assert/include -Ithird_party/boost/type_traits/include -Ithird_party/boost/utility/include -Ithird_party/eigen -Ithird_party/flatbuffers/include -Ithird_party/lbfgs/include -Ithird_party/gloo -I/usr/include/python3.6m -c horovod/common/ops/gloo_operations.cc -o build/temp.linux-x86_64-3.6/horovod/common/ops/gloo_operations.o -std=c++11 -fPIC -O2 -Wall -fassociative-math -ffast-math -ftree-vectorize -funsafe-math-optimizations -mf16c -mavx -mfma -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent -I/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi/opal/mca/event/libevent2022/libevent/include -I/usr/lib/x86_64-linux-gnu/openmpi/include -pthread -L/usr//lib -L/usr/lib/x86_64-linux-gnu/openmpi/lib -lmpi_cxx -lmpi -I/usr/local/lib/python3.6/dist-packages/tensorflow/include -D_GLIBCXX_USE_CXX11_ABI=0
  In file included from horovod/common/ops/../gloo/../mpi/mpi_context.h:25,
                   from horovod/common/ops/../gloo/gloo_context.h:25,
                   from horovod/common/ops/gloo_operations.h:20,
                   from horovod/common/ops/gloo_operations.cc:16:
  horovod/common/ops/../gloo/../mpi/../half.h: In function ‘void horovod::common::HalfBits2Float(short unsigned int*, float*)’:
  horovod/common/ops/../gloo/../mpi/../half.h:70:11: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     *res = *reinterpret_cast<float const*>(&f);
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  In file included from horovod/common/ops/gloo_operations.cc:22:
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = bool; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = bool]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
     for (auto i = 0; i < n; i++) {
                      ~~^~~
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = double; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = double]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = float; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = float]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = gloo::float16; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = gloo::float16]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = long int; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = long int]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = int; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = int]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = short int; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = short int]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
  third_party/gloo/gloo/math.h:20:22: warning: comparison of integer expressions of different signedness: ‘int’ and ‘size_t’ {aka ‘long unsigned int’} [-Wsign-compare]
  third_party/gloo/gloo/math.h: In instantiation of ‘void gloo::sum(void*, const void*, const void*, size_t) [with T = short unsigned int; size_t = long unsigned int]’:
  horovod/common/ops/gloo_operations.cc:69:10:   required from ‘void horovod::common::GlooAlgorithms<T>::Allreduce(void*, int) [with T = short unsigned int]’
  horovod/common/ops/gloo_operations.h:44:8:   required from here
 ...............
...............
...............
[horovod_install_fail.log](https://github.com/tensorflow/tensorflow/files/5107149/horovod_install_fail.log)

                         ^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:373:25: note: in definition of macro ‘MATCH_TYPE_AND_ENUM’
       struct DataTypeToEnum<TYPE> {                         \
                             ^~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:373:29: error: template argument 1 is invalid
       struct DataTypeToEnum<TYPE> {                         \
                                 ^
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:1: note: in expansion of macro ‘MATCH_TYPE_AND_ENUM’
     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);
     ^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:21: error: ‘bfloat16’ was not declared in this scope
     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);
                         ^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:379:26: note: in definition of macro ‘MATCH_TYPE_AND_ENUM’
       struct IsValidDataType<TYPE> {                        \
                              ^~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:21: note: suggested alternative: ‘float_t’
     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);
                         ^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:379:26: note: in definition of macro ‘MATCH_TYPE_AND_ENUM’
       struct IsValidDataType<TYPE> {                        \
                              ^~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:379:30: error: template argument 1 is invalid
       struct IsValidDataType<TYPE> {                        \
                                  ^
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:1: note: in expansion of macro ‘MATCH_TYPE_AND_ENUM’
     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);
     ^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:404:21: error: ‘bfloat16’ does not name a type; did you mean ‘float_t’?
     MATCH_TYPE_AND_ENUM(bfloat16, DT_BFLOAT16);
                         ^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:384:13: note: in definition of macro ‘MATCH_TYPE_AND_ENUM’
         typedef TYPE Type;                                  \
                 ^~~~
    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/device_base.h:26,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:27,
                     from horovod/tensorflow/mpi_ops.cc:24:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:224:27: error: expected ‘)’ before ‘scalar_value’
       explicit Tensor(bfloat16 scalar_value)
                      ~        ^~~~~~~~~~~~~
                               )
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/tensor.h:225:47: error: expected unqualified-id before ‘)’ token
           : Tensor(scalar_value, host_scalar_tag{}) {}
                                                   ^
    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../SpecialFunctions:63,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/Tensor:18,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:24,
                     from horovod/tensorflow/mpi_ops.cc:24:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_i0e<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:208:44:   required from ‘static Scalar Eigen::internal::bessel_i0e_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1892:54:   required from ‘typename Eigen::internal::bessel_i0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_i0e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_i0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:21:69:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:120:27: error: ‘pcmp_le’ was not declared in this scope
         return pselect(pcmp_le(y, pset1<T>(8.0f)), y_le_eight, y_gt_eight);
                        ~~~~~~~^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:96:17: warning: unused variable ‘A’ [-Wunused-variable]
         const float A[] = {-1.30002500998624804212E-8f, 6.04699502254191894932E-8f,
                     ^
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_i1e<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:403:44:   required from ‘static Scalar Eigen::internal::bessel_i1e_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1904:54:   required from ‘typename Eigen::internal::bessel_i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_i1e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_i1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:29:69:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:313:24: error: ‘pcmp_le’ was not declared in this scope
         y = pselect(pcmp_le(y, pset1<T>(8.0f)), y_le_eight, y_gt_eight);
                     ~~~~~~~^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:314:27: error: ‘pcmp_lt’ was not declared in this scope
         return pselect(pcmp_lt(x, pset1<T>(0.0f)), pnegate(y), y);
                        ~~~~~~~^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_j0<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1283:43:   required from ‘static Scalar Eigen::internal::bessel_j0_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1934:53:   required from ‘typename Eigen::internal::bessel_j0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_j0(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_j0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:32:68:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1168:16: error: ‘pcmp_lt’ was not declared in this scope
             pcmp_lt(y, pset1<T>(1.0e-3f)),
             ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1177:27: error: ‘pcmp_le’ was not declared in this scope
         return pselect(pcmp_le(y, pset1<T>(2.0)), y_le_two, y_gt_two);
                        ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_j1<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1672:43:   required from ‘static Scalar Eigen::internal::bessel_j1_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1946:53:   required from ‘typename Eigen::internal::bessel_j1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_j1(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_j1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:36:68:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1575:16: error: ‘pcmp_lt’ was not declared in this scope
             pcmp_lt(x, pset1<T>(0.0f)), pnegate(y_gt_two), y_gt_two);
             ~~~~~~~^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1576:27: error: ‘pcmp_le’ was not declared in this scope
         return pselect(pcmp_le(y, pset1<T>(2.0f)), y_le_two, y_gt_two);
                        ~~~~~~~^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_y0<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1481:43:   required from ‘static Scalar Eigen::internal::bessel_y0_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1940:53:   required from ‘typename Eigen::internal::bessel_y0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_y0(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_y0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:40:68:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:31: error: ‘pcmp_le’ was not declared in this scope
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1380:27: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
         return pselect(pcmp_le(x, pset1<T>(2.0)), x_le_two, x_gt_two);
                        ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1373:31: note: ‘pcmp_le’ declared here, later in the translation unit
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), NEG_MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_y1<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1875:43:   required from ‘static Scalar Eigen::internal::bessel_y1_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1952:53:   required from ‘typename Eigen::internal::bessel_y1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_y1(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_y1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:44:68:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1766:31: error: ‘pcmp_lt’ was not declared in this scope
         x_le_two = pselect(pcmp_lt(x, pset1<T>(0.0f)), NEG_MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1774:27: error: ‘pcmp_le’ was not declared in this scope
         return pselect(pcmp_le(x, pset1<T>(2.0)), x_le_two, x_gt_two);
                        ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_k0<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:761:43:   required from ‘static Scalar Eigen::internal::bessel_k0_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1910:53:   required from ‘typename Eigen::internal::bessel_k0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k0(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k0_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:47:68:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:31: error: ‘pcmp_le’ was not declared in this scope
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:675:27: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
         return pselect(pcmp_le(x, two), x_le_two, x_gt_two);
                        ~~~~~~~^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:668:31: note: ‘pcmp_le’ declared here, later in the translation unit
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:651:17: warning: unused variable ‘A’ [-Wunused-variable]
         const float A[] = {1.90451637722020886025E-9f, 2.53479107902614945675E-7f,
                     ^
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_k0e<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:589:44:   required from ‘static Scalar Eigen::internal::bessel_k0e_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1916:54:   required from ‘typename Eigen::internal::bessel_k0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k0e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k0e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:51:69:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:502:16: error: ‘pcmp_le’ was not declared in this scope
             pcmp_le(x, pset1<T>(0.0)),
             ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:504:24: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
             pselect(pcmp_le(x, two), x_le_two, x_gt_two));
                     ~~~~~~~^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:502:16: note: ‘pcmp_le’ declared here, later in the translation unit
             pcmp_le(x, pset1<T>(0.0)),
             ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:479:17: warning: unused variable ‘A’ [-Wunused-variable]
         const float A[] = {1.90451637722020886025E-9f, 2.53479107902614945675E-7f,
                     ^
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_k1<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1083:43:   required from ‘static Scalar Eigen::internal::bessel_k1_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1922:53:   required from ‘typename Eigen::internal::bessel_k1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k1(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k1_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:55:68:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:31: error: ‘pcmp_le’ was not declared in this scope
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:999:27: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
         return pselect(pcmp_le(x, two), x_le_two, x_gt_two);
                        ~~~~~~~^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:992:31: note: ‘pcmp_le’ declared here, later in the translation unit
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h: In instantiation of ‘static T Eigen::internal::generic_k1e<T, float>::run(const T&) [with T = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:917:44:   required from ‘static Scalar Eigen::internal::bessel_k1e_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:1928:54:   required from ‘typename Eigen::internal::bessel_k1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::bessel_k1e(const Scalar&) [with Scalar = float; typename Eigen::internal::bessel_k1e_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsHalf.h:59:69:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:31: error: ‘pcmp_le’ was not declared in this scope
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:838:27: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
         return pselect(pcmp_le(x, two), x_le_two, x_gt_two);
                        ~~~~~~~^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/BesselFunctionsImpl.h:833:31: note: ‘pcmp_le’ declared here, later in the translation unit
         x_le_two = pselect(pcmp_le(x, pset1<T>(0.0)), MAXNUM, x_le_two);
                            ~~~~~~~^~~~~~~~~~~~~~~~~~
    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../SpecialFunctions:69,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/Tensor:18,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:24,
                     from horovod/tensorflow/mpi_ops.cc:24:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h: In instantiation of ‘T Eigen::internal::generic_ndtri(const T&) [with T = float; ScalarType = float]’:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:652:41:   required from ‘static Scalar Eigen::internal::ndtri_impl<Scalar>::run(Scalar) [with Scalar = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:1989:49:   required from ‘typename Eigen::internal::ndtri_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type Eigen::numext::ndtri(const Scalar&) [with Scalar = float; typename Eigen::internal::ndtri_retval<typename Eigen::internal::global_math_functions_filtering_base<Scalar>::type>::type = float]’
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsHalf.h:34:64:   required from here
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:616:28: error: ‘pcmp_le’ was not declared in this scope
       should_flipsign = pcmp_le(a, psub(one, exp_neg_two));
                         ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:617:14: error: cannot convert ‘const float’ to ‘fd_set*’
       b = pselect(should_flipsign, a, psub(one, a));
           ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    In file included from /usr/include/x86_64-linux-gnu/sys/types.h:197,
                     from /usr/include/stdlib.h:394,
                     from /usr/include/c++/8/cstdlib:75,
                     from /usr/include/c++/8/ext/string_conversions.h:41,
                     from /usr/include/c++/8/bits/basic_string.h:6400,
                     from /usr/include/c++/8/string:52,
                     from /usr/include/c++/8/stdexcept:39,
                     from /usr/include/c++/8/array:39,
                     from /usr/include/c++/8/tuple:39,
                     from /usr/include/c++/8/bits/unique_ptr.h:37,
                     from /usr/include/c++/8/memory:80,
                     from horovod/tensorflow/mpi_ops.cc:18:
    /usr/include/x86_64-linux-gnu/sys/select.h:113:52: note:   initializing argument 2 of ‘int pselect(int, fd_set*, fd_set*, fd_set*, const timespec*, const __sigset_t*)’
     extern int pselect (int __nfds, fd_set *__restrict __readfds,
                                     ~~~~~~~~~~~~~~~~~~~^~~~~~~~~
    In file included from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../SpecialFunctions:69,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/Tensor:18,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/third_party/eigen3/unsupported/Eigen/CXX11/Tensor:1,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/numeric_types.h:20,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:26,
                     from /usr/local/lib/python3.6/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:24,
                     from horovod/tensorflow/mpi_ops.cc:24:
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:620:14: error: ‘pcmp_lt’ was not declared in this scope
           pcmp_lt(exp_neg_two, b),
           ~~~~~~~^~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:625:14: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
           pcmp_le(a, zero), neg_maxnum,
           ~~~~~~~^~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:616:28: note: ‘pcmp_le’ declared here, later in the translation unit
       should_flipsign = pcmp_le(a, psub(one, exp_neg_two));
                         ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:626:22: error: ‘pcmp_le’ was not declared in this scope, and no declarations were found by argument-dependent lookup at the point of instantiation [-fpermissive]
           pselect(pcmp_le(one, a), maxnum, ndtri));
                   ~~~~~~~^~~~~~~~
    /usr/local/lib/python3.6/dist-packages/tensorflow/include/unsupported/Eigen/CXX11/../src/SpecialFunctions/SpecialFunctionsImpl.h:616:28: note: ‘pcmp_le’ declared here, later in the translation unit
       should_flipsign = pcmp_le(a, psub(one, exp_neg_two));
                         ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
    error: command '/usr/bin/gcc' failed with exit status 1
    ----------------------------------------
ERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '""'""'/tmp/pip-install-db1tsv5m/horovod/setup.py'""'""'; __file__='""'""'/tmp/pip-install-db1tsv5m/horovod/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' install --record /tmp/pip-record-vpg897nc/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.6/horovod Check the logs for full command output.
``` 

Here is how I prepared my environment:
On Ubuntu 18.04 Docker with Python3.6 and Pip3.6 after I install the nightly wheel, I run these commands:

```
export HOROVOD_WITHOUT_PYTORCH=1
export HOROVOD_WITHOUT_MXNET=1
export HOROVOD_WITH_TENSORFLOW=1

apt-get update -y
apt-get install gcc-8 g++-8 cmake python-tk -y
update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 700 --slave /usr/bin/g++ g++ /usr/bin/g++-7
update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 800 --slave /usr/bin/g++ g++ /usr/bin/g++-8
apt-get install -y libsm6 libxext6
apt-get install google-perftools -y
apt-get install openmpi-bin openmpi-common openssh-client openssh-server libopenmpi-dev -y
pip install --no-cache-dir horovod==0.19.1
```
and that throws the above error.

I have tried with all wheels from 08/16 all the way up to 08/20 and they all fail but 08/15 wheel seem to work.

I've done a bit of research and seems like the regression was introduced somewhere between:
https://github.com/tensorflow/tensorflow/commit/0ec146da5fee12617a944127f18998acd74414d8
and
https://github.com/tensorflow/tensorflow/commit/58de5563920eaf3d1454e924270c8f9b140301fe

But I'm not sure about that."
42546,Mixed precision training on TensorFlow 2.3 doesn't speed-up training progress as TensorFlow 2.2 does,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.2.0/2.3.0
- Python version: 3.6/3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda 10.1, cuDNN 7.6.5
- GPU model and memory: 2080Ti/12Gb

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**

Mixed precision training on tensorflow 2.3 doesn't speed-up training progress as Tensorflow 2.2 does 

**Describe the expected behavior**

faster or at least equal

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

[nmt_with_attention.zip](https://github.com/tensorflow/tensorflow/files/5106684/nmt_with_attention.zip)

I modify a bit based on tensorflow offcial tutorial nmt here (https://www.tensorflow.org/tutorials/text/nmt_with_attention). You can see the flag `IS_MIXED_PRECISION` in the head of the notebook, let change it to True/False to check by urself.


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

[log.txt](https://github.com/tensorflow/tensorflow/files/5106690/log.txt)

here is my summary about the time consuming for tensorflow2.2/2.3 on both mixex precision and non-mixed precision.

I have many model on my framework TensorFlowTTS (https://github.com/TensorSpeech/TensorFlowTTS). All of my model training on mixed precsion Tensorflow2.3 have this problem. The above notebook is just example. 

BTW, colab gpu haven't support mixed precision so to run this notebook require a local GPU to reproduce my issue. 

"
42545,Tensorflow V2.2.0 boot fail on aarch64,"When I run a testcase on my `aarch64` platform, it reports error as:
```
tensorflow/core/platform/profile_utils/cpu_utils.cc:106] Failed to find bogomips or clock in /proc/cpuinfo; cannot determine CPU frequency
```

And I found `core/platform/profile_utils/cpu_utils.cc` not support `aarch64` yet as follows:
```
#if (defined(__powerpc__) || \
     defined(__ppc__) && (__BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__))
    retval = sscanf(line.c_str(), ""clock              : %lfMHz"", &cpu_freq);
    freq_factor = 1.0;
#else
    retval = sscanf(line.c_str(), ""bogomips : %lf"", &cpu_freq);
#endif
    if (retval > 0) {
      const double freq_ghz = cpu_freq / 1000.0 / freq_factor;
      if (retval != 1 || freq_ghz < 0.01) {
        LOG(WARNING) << ""Failed to get CPU frequency: "" << freq_ghz << "" GHz"";
        return INVALID_FREQUENCY;
      }
      const int64 freq_n =
          static_cast<int64>(freq_ghz * 1000.0 * 1000.0 * 1000.0);
      LOG(INFO) << ""CPU Frequency: "" << freq_n << "" Hz"";
      return freq_n;
    }
  }
  LOG(WARNING)
      << ""Failed to find bogomips or clock in /proc/cpuinfo; cannot determine ""
         ""CPU frequency"";
  return INVALID_FREQUENCY;
```

But my `proc/cpuinfo` is
```
BogoMIPS        : xxx.xx
```
which not match `bogomips : %lf`

Could anyone help to fix this issue?"
42544,[Estimator] bazel test failures,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: r2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.2/7
- GPU model and memory:

**Describe the problem**
Estimator bazel test failures
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test //tensorflow_estimator/... --test_output=errors --verbose_failures=true
  --keep_going --test_verbose_timeout_warnings

**Any other info / logs**
//tensorflow_estimator/python/estimator:rnn_test                         FAILED in 197.2s
//tensorflow_estimator/python/estimator:distribute_strategy_estimator_training_test FAILED in 48 out of 48 in 13.4s
//tensorflow_estimator/python/estimator:distribute_strategy_estimator_training_test_gpu FAILED in 48 out of 48 in 10.1s

-------------------------------------------------------------------
rnn_test failues:
[  FAILED  ] RNNClassifierEvaluationTest.testMultiClassEvaluationMetrics
[  FAILED  ] RNNClassifierPredictionTest.testBinaryClassPredictions
[  FAILED  ] RNNClassifierPredictionTest.testBinaryClassPredictionsSequential
[  FAILED  ] RNNClassifierPredictionTest.testMultiClassPredictions
[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromCheckpoint
[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromCheckpointSequential
[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromCheckpointSequentialWithWeights
[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassFromScratchWithDefaultOptimizer
[  FAILED  ] RNNClassifierTrainingTest.testBinaryClassWithExampleWeight
[  FAILED  ] RNNClassifierTrainingTest.testDefaultGradientClipping
[  FAILED  ] RNNClassifierTrainingTest.testFromScratchWithCustomRNNCellFn
[  FAILED  ] RNNClassifierTrainingTest.testMultiClassFromCheckpoint
[  FAILED  ] RNNClassifierTrainingTest.testMultiClassFromScratchWithDefaultOptimizer
[  FAILED  ] RNNClassifierTrainingTest.testMultiClassWithExampleWeight
[  FAILED  ] RNNLogitFnTest.testMultiDimLogitsSequential
[  FAILED  ] RNNLogitFnTest.testMultiDimLogitsStatic
[  FAILED  ] RNNLogitFnTest.testOneDimLogitsSequential
[  FAILED  ] RNNLogitFnTest.testOneDimLogitsSequentialInfer
[  FAILED  ] RNNLogitFnTest.testOneDimLogitsSequentialTrain
[  FAILED  ] RNNLogitFnTest.testOneDimLogitsStatic
[  FAILED  ] RNNLogitFnTest.testTrainingMode('train', True)
[  FAILED  ] RNNLogitFnTest.testTrainingMode('eval', False)
[  FAILED  ] RNNLogitFnTest.testTrainingMode('infer', False)

More details on testOneDimLogitsSequentialTrain failure
======================================================================
ERROR: testOneDimLogitsSequentialTrain (__main__.RNNLogitFnTest)
testOneDimLogitsSequentialTrain (__main__.RNNLogitFnTest)
testOneDimLogitsSequentialTrain(return_sequences=True, expected_logits=[[[-1.4388], [-0.6033]]], training=True)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1178, in decorated
    f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/absl/testing/parameterized.py"", line 263, in bound_param_test
    test_method(self, **testcase_params)
  File ""/root/.cache/bazel/_bazel_root/d226403c339549ea2db666353bf31144/sandbox/processwrapper-sandbox/902/execroot/org_tensorflow_estimator/bazel-out/k8-fastbuild/bin/tensorflow_estimator/python/estimator/rnn_test.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/rnn_test.py"", line 325, in testOneDimLogits
    training=training)
  File ""/root/.cache/bazel/_bazel_root/d226403c339549ea2db666353bf31144/sandbox/processwrapper-sandbox/902/execroot/org_tensorflow_estimator/bazel-out/k8-fastbuild/bin/tensorflow_estimator/python/estimator/rnn_test.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/rnn_test.py"", line 268, in _test_logits
    logits = logit_layer(features_fn(), training=training)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py"", line 776, in __call__
    outputs = call_fn(cast_inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py"", line 258, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    /root/.cache/bazel/_bazel_root/d226403c339549ea2db666353bf31144/sandbox/processwrapper-sandbox/902/execroot/org_tensorflow_estimator/bazel-out/k8-fastbuild/bin/tensorflow_estimator/python/estimator/rnn_test.runfiles/org_tensorflow_estimator/tensorflow_estimator/python/estimator/canned/rnn.py:221 call  *
        rnn_outputs = self._rnn_layer(
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:663 __call__  **
        return super(RNN, self).__call__(inputs, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:776 __call__
        outputs = call_fn(cast_inputs, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:1573 call
        inputs, mask=mask, training=training, initial_state=initial_state)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/recurrent.py:807 call
        zero_output_for_mask=self.zero_output_for_mask)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:201 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py:4327 rnn
        **while_loop_kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py:2696 while_loop
        back_prop=back_prop)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/while_v2.py:229 while_loop
        len_orig_loop_vars], expand_composites=True))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/while_v2.py:1151 _check_shapes_compat
        ""specify a less-specific shape."" % (input_t.name, shape, t.shape))

    ValueError: Input tensor 'rnn_model/simple_rnn/zeros_like:0' enters the loop with shape (1, 2), but has shape (None, 2) after one iteration. To allow the shape to vary across iterations, use the `shape_invariants` argument of tf.while_loop to specify a less-specific shape.
-------------------------------------------------------------------------
The distribute_strategy_estimator_training_test.* failed with the below error in the logs.
[  FAILED  ] setUpClass (__main__.DistributeCoordinatorIntegrationTest)
We tried to use portpicker but ran into segmentation failures.
-------------------------------------------------------------------
"
42542,WARNING:tensorflow:Gradients do not exist for variables  when minimizing the loss.,"Hello 

I am trying to run a tutorial of ES-RNN code found in this link. [timeseries forcasting tutorial](https://github.com/Azure/DeepLearningForTimeSeriesForecasting/blob/master/4_ES_RNN.ipynb)

When I run this in a conda environment where I installed required packages (tensorflow= 1.12.0, keras==2.2.4, etc), I was able to run model.fit function successfully.

However I wanted to run this in recent tensorflow version (2.2) and so I created another conda environment. Also I made minor changes in the ES class. For example, instead of `from keras import backend as K` , I used `from tensorflow.keras import backend as K`

Also there is no `K.slice` function so I used `tf.slice` instead.

Here is the original ES class code written in tensorflow=1.12.0 and keras=2.2.4. 

```
from keras import backend as K
from keras.layers import Layer
from keras import initializers

class ES(Layer):

    def __init__(self, horizon, m, batch_size, time_steps, **kwargs):
        self.horizon = horizon
        self.m = m
        self.batch_size = batch_size
        self.time_steps = time_steps
        
        super(ES, self).__init__(**kwargs)

    # initialization of the learned parameters of exponential smoothing
    def build(self, input_shape):
        self.alpha = self.add_weight(name='alpha', shape=(1,),
                                     initializer='uniform', trainable=True)
        self.gamma = self.add_weight(name='gamma', shape=(1,),
                                     initializer='uniform', trainable=True)
        self.init_seasonality = self.add_weight(name='init_seasonality', shape=(self.m,),
                                                initializer=initializers.Constant(value=0.8), trainable=True)
        self.init_seasonality_list = [K.slice(self.init_seasonality,(i,),(1,)) for i in range(self.m)]
        self.seasonality_queue = deque(self.init_seasonality_list, self.m)
        self.level = self.add_weight(name='init_level', shape=(1,),
                                     initializer=initializers.Constant(value=0.8), 
                                     trainable=True)
        super(ES, self).build(input_shape)  

    def call(self, x):

        # extract time-series from feature vector
        n_examples = K.int_shape(x)[0]
        if n_examples is None:
            n_examples = self.batch_size
        x1 = K.slice(x,(0,0,0),(1,self.time_steps,1))
        x1 = K.reshape(x1,(self.time_steps,))
        x2 = K.slice(x,(1,self.time_steps-1,0),(n_examples-1,1,1))
        x2 = K.reshape(x2,(n_examples-1,))
        ts = K.concatenate([x1,x2])
        
        x_norm = []  # normalized values of time-series
        ls = []      # coeffients for denormalization of forecasts
        
        l_t_minus_1 = self.level
        
        for i in range(n_examples+self.time_steps-1):
        
            # compute l_t
            y_t = ts[i]
            s_t = self.seasonality_queue.popleft()
            l_t = self.alpha * y_t / s_t + (1 - self.alpha) * l_t_minus_1
            
            # compute s_{t+m}
            s_t_plus_m = self.gamma * y_t / l_t + (1 - self.gamma) * s_t
            
            self.seasonality_queue.append(s_t_plus_m)
            
            # normalize y_t
            x_norm.append(y_t / (s_t * l_t))

            l_t_minus_1 = l_t

            if i >= self.time_steps-1:
                l = [l_t]*self.horizon
                l = K.concatenate(l)
                s = [self.seasonality_queue[i] for i in range(self.horizon)] # we assume here that horizon < m
                s = K.concatenate(s)
                ls_t = K.concatenate([K.expand_dims(l), K.expand_dims(s)])
                ls.append(K.expand_dims(ls_t,axis=0))  
       
        self.level = l_t
        x_norm = K.concatenate(x_norm)

        # create x_out
        x_out = []
        for i in range(n_examples):
            norm_features = K.slice(x_norm,(i,),(self.time_steps,))
            norm_features = K.expand_dims(norm_features,axis=0)
            x_out.append(norm_features)

        x_out = K.concatenate(x_out, axis=0)
        x_out = K.expand_dims(x_out)

        # create tensor of denormalization coefficients 
        denorm_coeff = K.concatenate(ls, axis=0)
        return [x_out, denorm_coeff]

    def compute_output_shape(self, input_shape):
        return [(input_shape[0], input_shape[1], input_shape[2]), (input_shape[0], self.horizon, 2)]
    
class Denormalization(Layer):
    
    def __init__(self, **kwargs):
        super(Denormalization, self).__init__(**kwargs)

    def build(self, input_shape):
        super(Denormalization, self).build(input_shape)  

    def call(self, x):
        return x[0] * x[1][:,:,0] * x[1][:,:,1]

    def compute_output_shape(self, input_shape):
        return input_shape[0]
````

However, the code failed to run at TF2.2 and the error message is below:

```
Train on 23328 samples, validate on 1488 samples
Epoch 1/10
WARNING:tensorflow:Gradients do not exist for variables ['es/init_seasonality:0'] when minimizing the loss.
WARNING:tensorflow:Gradients do not exist for variables ['es/init_seasonality:0'] when minimizing the loss.
   48/23328 [..............................] - ETA: 4:05:37WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: 

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-25-a1542188ddf7> in <module>
      6           validation_data=(valid_inputs['X'], valid_inputs['target']),
      7           callbacks=[earlystop],
----> 8           verbose=1)

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    817         max_queue_size=max_queue_size,
    818         workers=workers,
--> 819         use_multiprocessing=use_multiprocessing)
    820 
    821   def evaluate(self,

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in fit(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    340                 mode=ModeKeys.TRAIN,
    341                 training_context=training_context,
--> 342                 total_epochs=epochs)
    343             cbks.make_logs(model, epoch_logs, training_result, ModeKeys.TRAIN)
    344 

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2.py in run_one_epoch(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)
    126         step=step, mode=mode, size=current_batch_size) as batch_logs:
    127       try:
--> 128         batch_outs = execution_function(iterator)
    129       except (StopIteration, errors.OutOfRangeError):
    130         # TODO(kaftan): File bug about tf function and errors.OutOfRangeError?

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\keras\engine\training_v2_utils.py in execution_function(input_fn)
     96     # `numpy` translates Tensors to values in Eager mode.
     97     return nest.map_structure(_non_none_constant_value,
---> 98                               distributed_function(input_fn))
     99 
    100   return execution_function

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\def_function.py in __call__(self, *args, **kwds)
    566         xla_context.Exit()
    567     else:
--> 568       result = self._call(*args, **kwds)
    569 
    570     if tracing_count == self._get_tracing_count():

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\def_function.py in _call(self, *args, **kwds)
    630         # Lifting succeeded, so variables are initialized and we can run the
    631         # stateless function.
--> 632         return self._stateless_fn(*args, **kwds)
    633     else:
    634       canon_args, canon_kwds = \

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\function.py in __call__(self, *args, **kwargs)
   2361     with self._lock:
   2362       graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 2363     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   2364 
   2365   @property

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\function.py in _filtered_call(self, args, kwargs)
   1609          if isinstance(t, (ops.Tensor,
   1610                            resource_variable_ops.BaseResourceVariable))),
-> 1611         self.captured_inputs)
   1612 
   1613   def _call_flat(self, args, captured_inputs, cancellation_manager=None):

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1690       # No tape is watching; skip to running the function.
   1691       return self._build_call_outputs(self._inference_function.call(
-> 1692           ctx, args, cancellation_manager=cancellation_manager))
   1693     forward_backward = self._select_forward_and_backward_functions(
   1694         args,

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    543               inputs=args,
    544               attrs=(""executor_type"", executor_type, ""config_proto"", config),
--> 545               ctx=ctx)
    546         else:
    547           outputs = execute.execute_with_cancellation(

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     74           ""Inputs to eager execution function cannot be Keras symbolic ""
     75           ""tensors, but found {}"".format(keras_symbolic_tensors))
---> 76     raise e
     77   # pylint: enable=protected-access
     78   return tensors

~\Miniconda3\envs\TF2p2\lib\site-packages\tensorflow_core\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     59     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,
     60                                                op_name, inputs, attrs,
---> 61                                                num_outputs)
     62   except core._NotOkStatusException as e:
     63     if name is not None:

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: model/es/add_59:0
```

Could you please let me know what part of the code should be modified to be compatible with tensorflow 2.X ? 

Also model.summary shows the batch size (48) in es, gru and dense layer in TF2.X but it was ""None"" in the original tutorial. 
```
Model: ""model""
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 1)]    0                                            
__________________________________________________________________________________________________
es (ES)                         [(48, 6, 1), (48, 3, 26          input_1[0][0]                    
__________________________________________________________________________________________________
gru (GRU)                       (48, 5)              120         es[0][0]                         
__________________________________________________________________________________________________
dense (Dense)                   (48, 3)              18          gru[0][0]                        
__________________________________________________________________________________________________
denormalization (Denormalizatio (48, 3)              0           dense[0][0]                      
                                                                 es[0][1]                         
==================================================================================================
Total params: 164
Trainable params: 164
Non-trainable params: 0

```


Thank you. "
42535,TFLiteConverter Segmentation Fault during integer quantization representative_dataset --add_postprocessing_op=true,"I'm using tensorflow==1.15.3 and I'm hitting a segmentation fault attempting [int8 post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization#integer_only). The documentation for the 1.15 version of the TFLiteConverter can be found [here](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/lite/TFLiteConverter). 

I found [a similar issue on github](https://github.com/tensorflow/tensorflow/issues/29829), but their solution to provide `--add_postprocessing_op=true` has not solved the segmentation fault.

I've debugged it using PDB and found exactly where it crashes. It never reaches my `representative_dataset` function. It faults when running `CreateWrapperCPPFromBuffer(model_content)`

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.4 LTS
- TensorFlow installed from (source or binary): source/pip tensorflow==1.15.3
- TensorFlow version (or github SHA if from source): 1.15.3

**Command used to run the converter or code if you’re using the Python API**
If possible, please share a link to Colab/Jupyter/any notebook.

```
python -m pdb convert_model_to_tflite_int8.py --add_postprocessing_op=true
```

**The output from the converter invocation**

```
2020-08-20 20:25:22.552188: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-08-20 20:25:22.573942: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz
2020-08-20 20:25:22.574163: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x566ecb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-08-20 20:25:22.574183: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
2020-08-20 20:25:22.574411: I tensorflow/core/common_runtime/direct_session.cc:359] Device mapping:
/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device
2020-08-20 20:25:33.546206: I tensorflow/core/grappler/devices.cc:60] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0 (Note: TensorFlow was not compiled with CUDA support)
2020-08-20 20:25:33.546355: I tensorflow/core/grappler/clusters/single_machine.cc:356] Starting new session

2020-08-20 20:25:37.496345: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:786] Optimization results for grappler item: graph_to_optimize
2020-08-20 20:25:37.496382: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 433 nodes (-65), 484 edges (-65), time = 2221.83691ms.
2020-08-20 20:25:37.496394: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:788]   constant_folding: Graph size after: 433 nodes (0), 484 edges (0), time = 935.13ms.
> .../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py(51)__init__()
-> .CreateWrapperCPPFromBuffer(model_content))
(Pdb) s
Fatal Python error: Segmentation fault

Current thread 0x00007ff40ee9f740 (most recent call first):
  File "".../python3.6/site-packages/tensorflow_core/lite/python/optimize/calibrator.py"", line 51 in __init__
  File "".../python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 236 in _calibrate_quantize_model
  File "".../python3.6/site-packages/tensorflow_core/lite/python/lite.py"", line 993 in convert
  File "".../convert_model_to_tflite_int8.py"", line 97 in <module>
  File ""<string>"", line 1 in <module>
  File ""/usr/lib/python3.6/bdb.py"", line 434 in run
  File ""/usr/lib/python3.6/pdb.py"", line 1548 in _runscript
  File ""/usr/lib/python3.6/pdb.py"", line 1667 in main
  File ""/usr/lib/python3.6/pdb.py"", line 1694 in <module>
  File ""/usr/lib/python3.6/runpy.py"", line 85 in _run_code
  File ""/usr/lib/python3.6/runpy.py"", line 193 in _run_module_as_main
[1]    17668 segmentation fault (core dumped)  python -m pdb convert_model_to_tflite_int8.py  --add_postprocessing_op=true
```

```python
converter = tf.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file=pb_model_path,
  input_arrays=[""device_0/input_node_name:1""],
  output_arrays=[""device_0/output_node_name""],
  input_shapes={""device_0/input_node_name:1"": [100, 16384]}
)
converter.allow_custom_ops = True
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

def test():
  pdb.set_trace()
  print(' ! ! ! representative_dataset_gen ! ! ! ')
  zeros = np.zeros(shape=(1, 100, 16384), dtype='int8')
  ds = tf.data.Dataset.from_tensor_slices((zeros)).batch(1)
  for input_value in ds.take(1):
    yield [input_value]
converter.representative_dataset = test

pdb.set_trace()
tflite_model = converter.convert()

tflite_model_size = open(model_name, 'wb').write(tflite_model)
print('TFLite Model is %d bytes' % tflite_model_size)
```

**Failure details**
- int8 quantization with `representative_dataset`
- Segmentation fault occurs when running `CreateWrapperCPPFromBuffer(model_content)`

**Any other info / logs**
- tf.float16 conversion works (without representative_dataset property)
- asked on stack overflow [here](https://stackoverflow.com/questions/63514487/tfliteconverter-segmentation-fault-when-running-integer-quantization)
"
42534,Low performance of Object Detection Models in Coral Dev Board,"Hi 

Recently my company bought a Coral Board Dev and  Coral USB Accelerator to detect some objects in outdoor environment.

We have tried to train MobileNet V1 and V2 using TPU on the Cloud, as well using GPU RT2070 and finally we have implemented other models such as Inception Lite and Tiny Yolo V3 but the results are no good according the accuracy during the performance. 


For this reason, I would like to know what else could be done to improve this model in real Object detection in Coral edge device?"
42528,Model.save(...) throws exception ,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
Model.save() throws exceptions when having ""kernel_initializer=tf.initializers.zeros"" to Dense. If it is removed, the ""save()"" works.  The exception is:


```
/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/generic_utils.py in serialize_keras_object(instance)
...
    243     name = get_registered_name(instance.__class__)
    244     try:
--> 245       config = instance.get_config()
    246     except NotImplementedError as e:
    247       if _SKIP_FAILED_SERIALIZATION:

TypeError: get_config() missing 1 required positional argument: 'self'
```

The complete model is:

```
multi_lstm_model = tf.keras.Sequential([
    # Shape [batch, time, features] => [batch, lstm_units]
    # Adding more `lstm_units` just overfits more quickly.
    tf.keras.layers.LSTM(32, return_sequences=False),
    # Shape => [batch, out_steps*features]
    tf.keras.layers.Dense(OUT_STEPS*num_features,
                          kernel_initializer=tf.initializers.zeros),
    # Shape => [batch, out_steps, features]
    tf.keras.layers.Reshape([OUT_STEPS, num_features])
])
```

The save is: 
`multi_lstm_model.save('multi_lstm_model')`



The code is from: https://www.tensorflow.org/tutorials/structured_data/time_series

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42524,Error building custom op with g++,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`


**Describe the current behavior**
```
# /usr/bin/g++ -I /usr/local/lib/python3.7/site-packages/tensorflow_core/include disparity_prop.cc -std=c++11
Undefined symbols for architecture x86_64:
  ""tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)"", referenced from:
      ___cxx_global_var_init.4 in disparity_prop-a72fe1.o
  ""tensorflow::OpDefBuilder::SetShapeFn(std::__1::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>)"", referenced from:
      tensorflow::register_op::OpDefBuilderWrapper<true>::SetShapeFn(tensorflow::Status (*)(tensorflow::shape_inference::InferenceContext*)) in disparity_prop-a72fe1.o
  ""tensorflow::OpDefBuilder::Input(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)"", referenced from:
      tensorflow::register_op::OpDefBuilderWrapper<true>::Input(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) in disparity_prop-a72fe1.o
  ""tensorflow::OpDefBuilder::Output(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)"", referenced from:
      tensorflow::register_op::OpDefBuilderWrapper<true>::Output(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) in disparity_prop-a72fe1.o
  ""tensorflow::OpDefBuilder::OpDefBuilder(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >)"", referenced from:
      tensorflow::register_op::OpDefBuilderWrapper<true>::OpDefBuilderWrapper(char const*) in disparity_prop-a72fe1.o
  ""tensorflow::OpDef::~OpDef()"", referenced from:
      tensorflow::OpRegistrationData::~OpRegistrationData() in disparity_prop-a72fe1.o
  ""tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)"", referenced from:
      tensorflow::core::RefCounted::~RefCounted() in disparity_prop-a72fe1.o
  ""tensorflow::internal::LogMessageFatal::~LogMessageFatal()"", referenced from:
      tensorflow::core::RefCounted::~RefCounted() in disparity_prop-a72fe1.o
  ""tensorflow::internal::CheckOpMessageBuilder::ForVar2()"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in disparity_prop-a72fe1.o
  ""tensorflow::internal::CheckOpMessageBuilder::NewString()"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in disparity_prop-a72fe1.o
  ""tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in disparity_prop-a72fe1.o
  ""tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()"", referenced from:
      std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >* tensorflow::internal::MakeCheckOpString<int, int>(int const&, int const&, char const*) in disparity_prop-a72fe1.o
  ""_main"", referenced from:
     implicit entry/start for main executable
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)

```

**Describe the expected behavior**
The .cc should compile successfully.
**Standalone code to reproduce the issue**

```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;


class DisparityPropOp : public OpKernel {
public:
  explicit DisparityPropOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
  }
};

//REGISTER_KERNEL_BUILDER(Name(""DisparityProp"").Device(DEVICE_CPU), DisparityPropOp);

// Disparity propagation
REGISTER_OP(""DisparityProp"")
.Input(""to_disparity_prop: float32"")
.Output(""disparity_proped: float32"")
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
              c->set_output(0, c->input(0));
              return Status::OK();
            });
```"
42523,Error building custom op,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): True
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): tensorflow/tensorflow docker image on macOS Catalina
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.0
- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04) 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
```
# g++ -I /usr/local/lib/python3.6/dist-packages/tensorflow/include disparity_prop.cc

/usr/lib/gcc/x86_64-linux-gnu/7/../../../x86_64-linux-gnu/Scrt1.o: In function `_start':
(.text+0x20): undefined reference to `main'
/tmp/ccyNizdq.o: In function `__static_initialization_and_destruction_0(int, int)':
disparity_prop.cc:(.text+0x5a7): undefined reference to `tensorflow::register_op::OpDefBuilderReceiver::OpDefBuilderReceiver(tensorflow::register_op::OpDefBuilderWrapper<true> const&)'
/tmp/ccyNizdq.o: In function `tensorflow::OpRegistrationData::~OpRegistrationData()':
disparity_prop.cc:(.text._ZN10tensorflow18OpRegistrationDataD2Ev[_ZN10tensorflow18OpRegistrationDataD5Ev]+0x26): undefined reference to `tensorflow::OpDef::~OpDef()'
/tmp/ccyNizdq.o: In function `tensorflow::register_op::OpDefBuilderWrapper<true>::OpDefBuilderWrapper(char const*)':
disparity_prop.cc:(.text._ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EEC2EPKc[_ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EEC5EPKc]+0x52): undefined reference to `tensorflow::OpDefBuilder::OpDefBuilder(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
/tmp/ccyNizdq.o: In function `tensorflow::register_op::OpDefBuilderWrapper<true>::Input(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)':
disparity_prop.cc:(.text._ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EE5InputENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EE5InputENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0x4d): undefined reference to `tensorflow::OpDefBuilder::Input(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
/tmp/ccyNizdq.o: In function `tensorflow::register_op::OpDefBuilderWrapper<true>::Output(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)':
disparity_prop.cc:(.text._ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EE6OutputENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE[_ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EE6OutputENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE]+0x4d): undefined reference to `tensorflow::OpDefBuilder::Output(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)'
/tmp/ccyNizdq.o: In function `tensorflow::register_op::OpDefBuilderWrapper<true>::SetShapeFn(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>)':
disparity_prop.cc:(.text._ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EE10SetShapeFnESt8functionIFNS_6StatusEPNS_15shape_inference16InferenceContextEEE[_ZN10tensorflow11register_op19OpDefBuilderWrapperILb1EE10SetShapeFnESt8functionIFNS_6StatusEPNS_15shape_inference16InferenceContextEEE]+0x4d): undefined reference to `tensorflow::OpDefBuilder::SetShapeFn(std::function<tensorflow::Status (tensorflow::shape_inference::InferenceContext*)>)'
/tmp/ccyNizdq.o: In function `tensorflow::core::RefCounted::~RefCounted()':
disparity_prop.cc:(.text._ZN10tensorflow4core10RefCountedD2Ev[_ZN10tensorflow4core10RefCountedD5Ev]+0xf4): undefined reference to `tensorflow::internal::LogMessageFatal::LogMessageFatal(char const*, int)'
disparity_prop.cc:(.text._ZN10tensorflow4core10RefCountedD2Ev[_ZN10tensorflow4core10RefCountedD5Ev]+0x11c): undefined reference to `tensorflow::internal::LogMessageFatal::~LogMessageFatal()'
/tmp/ccyNizdq.o: In function `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >* tensorflow::internal::MakeCheckOpString<long, int>(long const&, int const&, char const*)':
disparity_prop.cc:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x33): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::CheckOpMessageBuilder(char const*)'
disparity_prop.cc:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x5d): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::ForVar2()'
disparity_prop.cc:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x7b): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::NewString[abi:cxx11]()'
disparity_prop.cc:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0x8a): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
disparity_prop.cc:(.text._ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc[_ZN10tensorflow8internal17MakeCheckOpStringIliEEPNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEERKT_RKT0_PKc]+0xad): undefined reference to `tensorflow::internal::CheckOpMessageBuilder::~CheckOpMessageBuilder()'
collect2: error: ld returned 1 exit status
```
**Describe the expected behavior**
The .cc source should compile successfully. 

**Standalone code to reproduce the issue**
```
#include ""tensorflow/core/framework/op.h""
#include ""tensorflow/core/framework/shape_inference.h""
#include ""tensorflow/core/framework/op_kernel.h""

using namespace tensorflow;


class DisparityPropOp : public OpKernel {
public:
  explicit DisparityPropOp(OpKernelConstruction* context) : OpKernel(context) {}

  void Compute(OpKernelContext* context) override {
    // Grab the input tensor
    const Tensor& input_tensor = context->input(0);

    // Create an output tensor
    Tensor* output_tensor = NULL;
    OP_REQUIRES_OK(context, context->allocate_output(0, input_tensor.shape(),
                                                     &output_tensor));
  }
};

//REGISTER_KERNEL_BUILDER(Name(""DisparityProp"").Device(DEVICE_CPU), DisparityPropOp);

// Disparity propagation
REGISTER_OP(""DisparityProp"")
.Input(""to_disparity_prop: float32"")
.Output(""disparity_proped: float32"")
.SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
              c->set_output(0, c->input(0));
              return Status::OK();
            });
```"
42522,GatherV2 checks batch_dims falsely in graph mode,"
**Describe the current behavior**
In graph mode, `GatherV2` checks `batch_dims` with the rank of `param`

**Describe the expected behavior**
By definition and the kernel implementation, should check
`batch_dims` with the rank of `indices`

**Standalone code to reproduce the issue**
```python
import tensorflow as tf
import numpy as np

@tf.function
def gather_fn(x, indices, axis, batch_dims):
    return tf.gather(x, indices, axis=axis, batch_dims=batch_dims)

# 2-D input with shape (2, 3)
x = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.int32)
# 3-D indices with shape (2, 1, 2)
indices = tf.constant([[[0,1]], [[1,0]]], dtype=tf.int32)
axis = 1
batch_dims = -3
# Eager mode computes correctly
print('Eager gather:', tf.gather(x, indices, axis=axis, batch_dims=batch_dims))
# Error in graph mode
print('Function gather', gather_fn(x, indices, axis=axis, batch_dims=batch_dims))
```

**Other info / logs** 
```
Eager gather: tf.Tensor(
[[[[0 1]]

  [[1 0]]]


 [[[3 4]]

  [[4 3]]]], shape=(2, 2, 1, 2), dtype=int32)
Traceback (most recent call last):
  File ""gather_function.py"", line 14, in <module>
    print('Function gather', gather_fn(x, indices, axis=axis, batch_dims=batch_dims))
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 580, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 627, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 506, in _initialize
    *args, **kwds))
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2446, in _get_concrete_function_internal_garbage_collected
    graph_function, _, _ = self._maybe_define_function(args, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2777, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py"", line 2667, in _create_graph_function
    capture_by_value=self._capture_by_value),
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 981, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py"", line 441, in wrapped_fn
    return weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py"", line 968, in wrapper
    raise e.ag_error_metadata.to_exception(e)
ValueError: in user code:

    gather_function.py:6 gather_fn  *
        return tf.gather(x, indices, axis=axis, batch_dims=batch_dims)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180 wrapper  **
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:4541 gather_v2
        batch_dims=batch_dims)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:180 wrapper
        return target(*args, **kwargs)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:4518 gather
        params, indices, axis, batch_dims=batch_dims, name=name)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/gen_array_ops.py:3762 gather_v2
        batch_dims=batch_dims, name=name)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper
        attrs=attr_protos, op_def=op_def)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal
        compute_device)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal
        op_def=op_def)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1817 __init__
        control_input_ops, op_def)
    /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/ops.py:1657 _create_c_op
        raise ValueError(str(e))

    ValueError: Shape must be at least rank 3 but is rank 2 for '{{node GatherV2}} = GatherV2[Taxis=DT_INT32, Tindices=DT_INT32, Tparams=DT_INT32, batch_dims=-3](x, indices, GatherV2/axis)' with input shapes: [2,3], [2,1,2], [] and with computed input tensors: input[2] = <1>.
```

This issue might come from https://github.com/tensorflow/tensorflow/blob/e7d27d850737cc11235df1a206ee8bd3efab1761/tensorflow/core/ops/array_ops.cc#L1219-L1224"
42521,tf.argmin across longer axis significantly slower than tf.reduce_min,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Amazon Linux AMI 2018.03
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0,2.1,2.2
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0, 10.1
- GPU model and memory: V100 16GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When finding the argmin across axis=1 on a large, two dimensional tensor, the performance significantly degrades on GPU. We are seeing almost 10x slower on GPU rather than on CPU. Argmin is okay when taking the global argmin, however when specifying axis=1, is very slow. You can even get around it by using tf.map_fn then taking the global argmin, but that is obviously not optimal.

**Describe the expected behavior**

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import time
begin = time.time()
for _ in range(1000):
    a = tf.random.uniform(shape=(20, 150000), 
                                 minval=0, 
                                 maxval=1, 
                                 dtype=tf.float64)
    c = tf.argmin(a, axis=1)
end = time.time()
end - begin
# around 25 seconds
```

Now do not use axis=1

```
import time
begin = time.time()
for _ in range(1000):
    a = tf.random.uniform(shape=(20, 150000), 
                                 minval=0, 
                                 maxval=1, 
                                 dtype=tf.float64)
    c = tf.argmin(a)
end = time.time()
end - begin
# around 0.5 seconds
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
42520,ReduceLROnPlateau not logging LR to tensorboard anymore,"I use the `ReduceLROnPlateau` callback and LR used to be logged to tensorboard. However since 2.3.0 it isn't showing up anymore. I tried to find a change that may explain the issue, but can't find anything. Was this some deliberate change?
"
42519,"'undefined reference' error to ""cxx11"" / ""std"" (Tensorflow-Lite C++ build for Android's NDK)","
**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: Samsung Galaxy S10
- TensorFlow installed from: binary
- TensorFlow version: 2.2 / 2.3


**Describe the problem**

Trying to build Tensorflow-Lite for Android Studio's NDK (C++ API) using Cross-compile for ARM64 with Make.

After creating `libtensorflow-lite.a` using the [Docker container](https://hub.docker.com/r/tensorflow/tensorflow/tags/), and copying all header files of `tensorflow`, `flatbuffers` and `absl` to Android project: trying to build results in `undefined reference` errors.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Created Android Studio C++ project
2. Created JNI Folders: `src/main/jni`, `src/main/jniLibs` (with `arm64-v8a`)
3. Placed static library `libtensorflow-lite.a` under `src/main/jni/arm64-v8a`
4. Placed all header files (in their directory structure) in `src/main/jniLibs/arm64-v8a`
5. In `build.gradle` (app) added the following:
```
...
android {
    compileSdkVersion 29
    buildToolsVersion ""29.0.3""

    defaultConfig {
        ...
        minSdkVersion 29
        targetSdkVersion 29
        ...
        externalNativeBuild {
            cmake {
                cppFlags ""-std=c++11 -frtti -fexceptions""
                arguments ""-DANDROID_ARM_NEON=ON""
            }
        }
        ndk {
            abiFilters 'arm64-v8a'
        }
    }
    ...
    sourceSets {
        main {
            jni.srcDirs = ['src/main/jni']
            jniLibs.srcDirs = ['src/main/jniLibs']
        }
    }
    splits {
        abi {
            enable true
            reset()
            include ""arm64-v8a""
            universalApk true
        }
    }
}
...
```
6. In `CMakeLists.txt` added the following:
```
set(CMAKE_CXX_STANDARD 11)
set(JNI_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../jni)
set(JNI_LIBS_DIR ${CMAKE_CURRENT_SOURCE_DIR}/../jniLibs)
add_library(
        tflite-lib
        STATIC
        IMPORTED)
set_target_properties(tflite-lib
        PROPERTIES IMPORTED_LOCATION
        ${JNI_DIR}/${ANDROID_ABI}/libtensorflow-lite.a)
include_directories(
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/flatbuffers
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/flatbuffers/include
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/absl
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/absl/absl
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/tensorflow
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/tensorflow/lite
        ${JNI_LIBS_DIR}/${ANDROID_ABI}/tensorflow/lite/c)
add_library(
        native-lib
        SHARED
        native-lib.cpp
        tfl-lib.cpp)
...
target_link_libraries(
        native-lib
        tflite-lib
        ${log-lib})
```
7. created `tfl-lib.cpp` and `tfl-lib.h` and changed header files to relative paths, for example `lib-tfl.h`:
```
...
#include ""../jniLibs/arm64-v84/tensorflow/lite/allocation.h""
#include ""../jniLibs/arm64-v84/tensorflow/lite/arena_planner.h""
#include ""../jniLibs/arm64-v84/tensorflow/lite/builtin_op_data.h""
...
```
and so on in the header files themselves, for example on `allocation.h`:
```
...
//#include ""tensorflow/lite/c/common.h""
#include ""c/common.h""
...
```
8. Include any dummy Tensorflow-Lite reference in `lib-tfl.cpp`, for example:
```
int load_model(char * filename, tflite::ErrorReporter * error_reporter){
    std::unique_ptr<tflite::FlatBufferModel> model =
            tflite::FlatBufferModel::BuildFromFile(filename);
    return 0;
}
```
9. Make Project
In this example the reference is to `BuildFromFile` from `model_builder.cc` as seen above - see ""Any other info / logs"" section for errors.

**Any other info / logs**

```
Build command failed.
Error while executing process /path/to/Android/Sdk/cmake/3.10.2.4988404/bin/ninja with arguments {-C /path/to/app/.cxx/cmake/debug/arm64-v8a native-lib}
ninja: Entering directory `/path/to/app/.cxx/cmake/debug/arm64-v8a'
[1/4] Building CXX object CMakeFiles/native-lib.dir/ocv-lib.cpp.o
[2/4] Building CXX object CMakeFiles/native-lib.dir/native-lib.cpp.o
[3/4] Building CXX object CMakeFiles/native-lib.dir/tfl-lib.cpp.o
[4/4] Linking CXX shared library /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so
FAILED: /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so 
: && /path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/bin/clang++ --target=aarch64-none-linux-android29 --gcc-toolchain=/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64 --sysroot=/path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/sysroot -fPIC -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -D_FORTIFY_SOURCE=2 -Wformat -Werror=format-security  -std=c++11 -frtti -fexceptions -O0 -fno-limit-debug-info  -Wl,--exclude-libs,libgcc_real.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -shared -Wl,-soname,libnative-lib.so -o /path/to/app/build/intermediates/cmake/debug/obj/arm64-v8a/libnative-lib.so CMakeFiles/native-lib.dir/native-lib.cpp.o CMakeFiles/native-lib.dir/tfl-lib.cpp.o CMakeFiles/native-lib.dir/ocv-lib.cpp.o  /path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflow-lite.a /path/to/Android/Sdk/ndk/21.0.6113669/toolchains/llvm/prebuilt/linux-x86_64/sysroot/usr/lib/aarch64-linux-android/29/liblog.so -latomic -lm && :
/path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflow-lite.a(model_builder.o): In function `tflite::FlatBufferModel::GetMinimumRuntime[abi:cxx11]() const':
model_builder.cc:(.text+0x278): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::compare(char const*) const'
model_builder.cc:(.text+0x3c0): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)'
model_builder.cc:(.text+0x4bc): undefined reference to `std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::_M_create(unsigned long&, unsigned long)'
model_builder.cc:(.text+0x514): undefined reference to `std::__throw_logic_error(char const*)'
/path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflow-lite.a(allocation.o): In function `tflite::FileCopyAllocation::FileCopyAllocation(char const*, tflite::ErrorReporter*)':
allocation.cc:(.text+0x130): undefined reference to `__fxstat'
/path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflow-lite.a(minimal_logging_default.o): In function `tflite::logging_internal::MinimalLogger::LogFormatted(tflite::LogSeverity, char const*, std::__va_list)':
minimal_logging_default.cc:(.text+0x3c): undefined reference to `__fprintf_chk'
minimal_logging_default.cc:(.text+0x60): undefined reference to `__vfprintf_chk'
/path/to/app/src/main/cpp/../jni/arm64-v8a/libtensorflow-lite.a(mmap_allocation.o): In function `tflite::MMAPAllocation::MMAPAllocation(char const*, tflite::ErrorReporter*)':
mmap_allocation.cc:(.text+0x114): undefined reference to `__fxstat'
clang++: error: linker command failed with exit code 1 (use -v to see invocation)
ninja: build stopped: subcommand failed.
```

**Question**

It looks like the static library matches a different `std` (not `clang++`?)
Am I missing some configuration on the Docker container for `clang++`?"
