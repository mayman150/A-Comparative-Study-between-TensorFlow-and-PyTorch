Issue Number,Issue Title,Issue Body
16764,Does it makes sense to use AdamOptimizer with Dropout?,"I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.
My network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.

I was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.

Here is the code

```
import numpy as np
import tensorflow as tf

# As input, 100 random numbers.
input_size = 100
output_size = 1

x = tf.placeholder(tf.float32,[None, input_size],name=""input"")
y = tf.placeholder(tf.float32,[None, output_size],name=""labels"")

with tf.variable_scope(""dense1"") as scope:
    W = tf.get_variable(""W"",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())
    b = tf.get_variable(""b"",initializer=tf.zeros([output_size]))
    dropped = tf.nn.dropout(x,0.8)
    dense = tf.matmul(dropped,W)+b

eval_pred = tf.nn.sigmoid(dense,name=""prediction"")

cost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))
#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)
train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)
train_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)


# 20 epochs, batch size of 1
epochs = 20

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    allWeights = []
    for i in range(epochs):

        x_raw = np.random.random((1,input_size))
        y_raw = np.random.random((1,output_size))
        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})
        #print(""Epoch {0}/{1}. Loss: {2}"".format(i+1,epochs,c))

        # Numbers will be around 20% of input_size (17-22)
        print(np.sum(d==0))
        allWeights.append(w)

print(""Calculate the difference between W_i and W_{i-1}"")
for wi in range(1,len(allWeights)):
    difference = allWeights[wi]-allWeights[wi-1]
    # I expect that there will be around 20 weights that won't be updated
    # so the difference between the current weight and the previous one
    # should be zero.
    print(np.sum(difference==0))
```

Just in case is not clear enough in the code, I'm printing two sets of numbers:
The first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.
The second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.

Again, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.

Question:
Is this a bug or is it supposed to be like this?
In the latter case, does it make sense to use Adam with Dropout?
"
16762,build android demo error,"ERROR: missing input file '@local_jdk//:jre/lib/resources.jar'
ERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1: @bazel_tools//tools/android:gen_java_lang_extras_jar: missing input file '@local_jdk//:jre/lib/resources.jar'
Target //tensorflow/examples/android:tensorflow_demo failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /home/wangmeng/.cache/bazel/_bazel_wangmeng/b0a6578298b02ab8f9d039326e51a46f/external/bazel_tools/tools/android/BUILD:104:1 1 input file(s) do not exist
INFO: Elapsed time: 22.408s, Critical Path: 0.03s
FAILED: Build did NOT complete successfully
"
16761,Core dumped after checking failed,"### System information
== cat /etc/issue ===============================================
Linux 84a2861a8739 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""7 (Core)""
VERSION_ID=""7""
CENTOS_MANTISBT_PROJECT_VERSION=""7""
REDHAT_SUPPORT_PRODUCT_VERSION=""7""

== are we in docker =============================================
Yes

== compiler =====================================================
c++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-4)
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 84a2861a8739 3.10.0-229.el7.x86_64 #1 SMP Fri Mar 6 11:36:42 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.0.post1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0
tf.GIT_VERSION = v1.5.0-0-g37aa430d84
tf.COMPILER_VERSION = v1.5.0-0-g37aa430d84
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I used keras using TensorFlow backend to train a batch of 300\*300\*3 RGB image.
The program occurred core dumped after printing check failed log as below.

### Source code / logs
F tensorflow/core/kernels/maxpooling_op.cc:177] Check failed: input_backprop_index >= in_start && input_backprop_index < in_end Invalid input backprop index: -1491167680, 2803712000, 2806515712

I traced tensorflow source code, it should be check operation. And backprop index shouldn't be negative. I don't know tensorflow well, why can appear such problem? I think this error is root cause of code dumped. Could you help me solve this problem?
"
16760,Feature request: Adding data_format argument to lstm2d.separable_lstm,"Since ndlstm is used for 2D data such as images, I think it would be nice to include a `data_format` argument in lstm2d.separable_lstm in case one decides to use the channels first format (NCHW) for their images (i.e. Using CNN having NCHW data format followed by NDLSTM)."
16758,import(ing) tensorflow fails to load at runtime.,"I have written custom code (as opposed to using a stock example script provided in TensorFlow):
using the Keras wrapper.  That works, but system failure and re-installation (new machine) provided the following:
--------------------------------------------------------------------------------------------------------------------------------
Python 2.7.12 (default, Dec  4 2017, 14:50:18) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import keras
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/keras/__init__.py"", line 3, in <module>
    from . import utils
  File ""/usr/local/lib/python2.7/dist-packages/keras/utils/__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""/usr/local/lib/python2.7/dist-packages/keras/utils/conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/__init__.py"", line 83, in <module>
    from .tensorflow_backend import *
  File ""/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
---------------------------------------------------------------------------------------------------------------------------------
"
16757,tf.contrib.layers.optimize_loss() to support mixed precision training,"ISSUE: Referring to [source code](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/layers/python/layers/optimizers.py),  it is evident that mixed precision gradients is not supported in `tf.contrib.layers.optimize_loss`. 
Here is the `snip` of assertion…
```
opt = tf.contrib.layers.optimize_loss(
    base_loss, global_step=global_step,
    clip_gradients=clip_grad, increment_global_step=True, **train_params)

TypeError: Tensors in list passed to 'values' of 'Pack' Op have types [float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16] that don't all match.
```
Description:
This was observed during training resnet50 (this involves mixed precision batch_norm). Just curious to know whether there is a roadmap to have mixed precision gradients support in `tf.contrib.layers.optimize_loss`.

System information
	•	**OS Platform and Distribution *: Linux Centos 7.2
	•	TensorFlow installed from (source or binary): 1.5.0
	•	TensorFlow version (use command below):  v1.5.0-0-g37aa430d84 1.5.0
	•	Python version: 3.4.5
	•	Bazel version (if compiling from source): No
	•	CUDA/CUDAnn version: CUDA 9.1 and CUDAnn 7.0 with latest Nvidia driver
	•	GPU model and memory: Volta 100, 16GiB
"
16755,Multilayer CNN Softmax Script Error,"Hi, I transferred your script for the Softmax regression and Multilayer CNN from your guide:
https://www.tensorflow.org/versions/r0.12/tutorials/mnist/pros/.

I get the following error whether I am running just the simple Softmax regression model or the full Multilayer CNN. Seems like an issue with the arguments used for the cross_entropy, but I don't know what the issue is...could you please help?

ValueError                                Traceback (most recent call last)
<ipython-input-2-997a50686e0d> in <module>()
     39     # between the target and the softmax activation function applied to the model's prediction.
     40 
---> 41 cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, y_))
     42 
     43     # Note that tf.nn.softmax_cross_entropy_with_logits internally applies the softmax

~\Anaconda2\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_ops.py in softmax_cross_entropy_with_logits(_sentinel, labels, logits, dim, name)
   1742   """"""
   1743   _ensure_xent_args(""softmax_cross_entropy_with_logits"", _sentinel,
-> 1744                     labels, logits)
   1745 
   1746   # TODO(pcmurray) Raise an error when the labels do not sum to 1. Note: This

~\Anaconda2\envs\tensorflow\lib\site-packages\tensorflow\python\ops\nn_ops.py in _ensure_xent_args(name, sentinel, labels, logits)
   1696   if sentinel is not None:
   1697     raise ValueError(""Only call `%s` with ""
-> 1698                      ""named arguments (labels=..., logits=..., ...)"" % name)
   1699   if labels is None or logits is None:
   1700     raise ValueError(""Both labels and logits must be provided."")

ValueError: Only call `softmax_cross_entropy_with_logits` with named arguments (labels=..., logits=..., ...)

Here is my script (essentially copied from the guide):

# Multilayer CNN using Tensorflow
# Load Data
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

# Start Tensorflow InteractiveSession
import tensorflow as tf
sess = tf.InteractiveSession()

# Build Softmax regression model. Define our model and training loss function.
      
    # creating nodes for the input images and target output classes

x = tf.placeholder(tf.float32, shape=[None, 784])
y_ = tf.placeholder(tf.float32, shape=[None, 10])
sess.run(tf.global_variables_initializer())


# Weight initialization
def weight_variable(shape):
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)

def bias_variable(shape):
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)

# Convolutional and pooling operations
# Our convolutions uses a stride of one and are zero padded so that
# the output is the same size as the input. Our pooling is plain 
# old max pooling over 2x2 blocks. To keep our code cleaner, let's 
# also abstract those operations into functions.

def conv2d(x, W):
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

def max_pool_2x2(x):
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')

# Implement first convolutional layer. t will consist of convolution, 
# followed by max pooling. The convolution will compute 32 features 
# for each 5x5 patch. Its weight tensor will have a shape of [5, 5, 1, 32].
# The first two dimensions are the patch size, the next is the number of 
# input channels, aTo apply the layer, we first reshape x to a 4d tensor, with the second and third dimensions corresponding to image width and height, and the final dimension corresponding to the number of color channels.nd the last is the number of output channels. We will 
# also have a bias vector with a component for each output channel.

W_conv1 = weight_variable([5, 5, 1, 32])
b_conv1 = bias_variable([32])

# To apply the layer, we first reshape x to a 4d tensor, with the second 
# and third dimensions corresponding to image width and height, and the 
# final dimension corresponding to the number of color channels.

x_image = tf.reshape(x, [-1,28,28,1])

# Then convolve x_image with the weight tensor, add the bias, apply the
# ReLU function, and finally max pool. The max_pool_2x2 method will reduce 
# the image size to 14x14.

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)

# Second convolutional layer.
# In order to build a deep network, we stack several layers of this type. 
# The second layer will have 64 features for each 5x5 patch.

W_conv2 = weight_variable([5, 5, 32, 64])
b_conv2 = bias_variable([64])

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
h_pool2 = max_pool_2x2(h_conv2)

# Densely connected layer. 
# Now that the image size has been reduced to 7x7, we add a fully-connected
# layer with 1024 neurons to allow processing on the entire image. We 
# reshape the tensor from the pooling layer into a batch of vectors, 
# multiply by a weight matrix, add a bias, and apply a ReLU.

W_fc1 = weight_variable([7 * 7 * 64, 1024])
b_fc1 = bias_variable([1024])

h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

# Dropout
# To reduce overfitting, we will apply dropout before the readout layer. 
# We create a placeholder for the probability that a neuron's output is 
# kept during dropout. This allows us to turn dropout on during training, 
# and turn it off during testing. TensorFlow's tf.nn.dropout op 
# automatically handles scaling neuron outputs in addition to masking them, 
# so dropout just works without any additional scaling.

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

# Readout layer.  one layer softmax regression.

W_fc2 = weight_variable([1024, 10])
b_fc2 = bias_variable([10])

y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

# Train and Evaluate model.

cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_conv, y_))

train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))

accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

sess.run(tf.global_variables_initializer())

for i in range(20000):
  batch = mnist.train.next_batch(50)
  if i%100 == 0:
    train_accuracy = accuracy.eval(feed_dict={
        x:batch[0], y_: batch[1], keep_prob: 1.0})
    print(""step %d, training accuracy %g""%(i, train_accuracy))
  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})
print(""test accuracy %g""%accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))

                                              "
16753,Virtual GPU config crashes TensorFlow after physical GPU loaded,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 9.3
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.5.0-2132-gbdea071e68', '1.5.0')
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**: (Debian 4.9.2-10) 4.9.2
- **CUDA/cuDNN version**: 9.1/7.0
- **GPU model and memory**: K40m, 11439 MB
- **Exact command to reproduce**: see the following script

### Describe the problem

Check failed when creating virtual GPU device after loading physical GPU information with `tensorflow.python.client.device_lib.list_local_devices`.

### Source code / logs

Source code:

```python
import tensorflow as tf
from tensorflow.core.protobuf import config_pb2
from tensorflow.python.client import device_lib

device_lib.list_local_devices()

virtual_device_gpu_options = config_pb2.GPUOptions(
    visible_device_list='0',
    experimental=config_pb2.GPUOptions.Experimental(virtual_devices=[
        config_pb2.GPUOptions.Experimental.VirtualDevices(
            memory_limit_mb=[200, 300])]))
config = config_pb2.ConfigProto(gpu_options=virtual_device_gpu_options)

with tf.Session(config=config) as sess:
    with tf.device('/gpu:1'):
        result = sess.run(tf.constant(42))
```

Logs:

```
2018-02-04 20:36:14.145943: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-02-04 20:36:23.613248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:02:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:23.806370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 1 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:03:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.019343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 2 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:82:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.341878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 3 with properties:
name: Tesla K40m major: 3 minor: 5 memoryClockRate(GHz): 0.745
pciBusID: 0000:83:00.0
totalMemory: 11.17GiB freeMemory: 11.09GiB
2018-02-04 20:36:24.342631: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1223] Device peer to peer matrix
2018-02-04 20:36:24.342769: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1229] DMA: 0 1 2 3
2018-02-04 20:36:24.342789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 0:   Y Y N N
2018-02-04 20:36:24.342803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 1:   Y Y N N
2018-02-04 20:36:24.342815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 2:   N N Y Y
2018-02-04 20:36:24.342827: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1239] 3:   N N Y Y
2018-02-04 20:36:24.342846: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0, 1, 2, 3
2018-02-04 20:36:25.856113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:0 with 10755 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-02-04 20:36:26.092252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:1 with 10753 MB memory) -> physical GPU (device: 1, name: Tesla K40m, pci bus id: 0000:03:00.0, compute capability: 3.5)
2018-02-04 20:36:26.329914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:2 with 10755 MB memory) -> physical GPU (device: 2, name: Tesla K40m, pci bus id: 0000:82:00.0, compute capability: 3.5)
2018-02-04 20:36:26.567499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/device:GPU:3 with 10753 MB memory) -> physical GPU (device: 3, name: Tesla K40m, pci bus id: 0000:83:00.0, compute capability: 3.5)
2018-02-04 20:36:26.852624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-04 20:36:26.852709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 200 MB memory) -> physical GPU (device: 0, name: Tesla K40m, pci bus id: 0000:02:00.0, compute capability: 3.5)
2018-02-04 20:36:26.852868: F tensorflow/core/common_runtime/gpu/gpu_id_utils.cc:42] Check failed: cuda_gpu_id.value() == result.first->second (0 vs. 1)Mapping the same TfGpuId to a different CUDA GPU id. TfGpuId: 1 Existing mapped CUDA GPU id: 1 CUDA GPU id being tried to map to: 0
```"
16752,Unable to run custom model (tensorflow) on android using Android studio ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
downloaded from Githhub
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
NA, not compiling from source
- **GCC/Compiler version (if compiling from source)**: 
NA
- **CUDA/cuDNN version**:
NA
- **GPU model and memory**:
NA
- **Exact command to reproduce**:
NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I have followed the instruction as per ""https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/index.html"" as were able to generate and successfully run APK for default model. I got problems when i replaced the default graph.pb file from rounded_graph.pb (retained name as graph only) file which is optimized version for android as per instructions in above mentioned link
Updated graphs are working using command line 

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
02-04 15:09:09.642: E/art(23153): No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)
02-04 15:09:10.105: E/AndroidRuntime(23153): FATAL EXCEPTION: main
02-04 15:09:10.105: E/AndroidRuntime(23153): Process: org.tensorflow.demo, PID: 23153
02-04 15:09:10.105: E/AndroidRuntime(23153): java.lang.RuntimeException: Failed to load model from 'file:///android_asset/graph.pb'
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.contrib.android.TensorFlowInferenceInterface.<init>(TensorFlowInferenceInterface.java:100)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.TensorFlowImageClassifier.create(TensorFlowImageClassifier.java:103)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.ClassifierActivity.onPreviewSizeChosen(ClassifierActivity.java:132)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.CameraActivity$1.onPreviewSizeChosen(CameraActivity.java:159)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.CameraConnectionFragment.setUpCameraOutputs(CameraConnectionFragment.java:421)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.CameraConnectionFragment.openCamera(CameraConnectionFragment.java:428)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.CameraConnectionFragment.access$000(CameraConnectionFragment.java:64)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at org.tensorflow.demo.CameraConnectionFragment$1.onSurfaceTextureAvailable(CameraConnectionFragment.java:95)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.TextureView.getHardwareLayer(TextureView.java:368)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.updateDisplayListIfDirty(View.java:15173)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:15969)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.drawChild(ViewGroup.java:3612)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.updateDisplayListIfDirty(View.java:15191)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:15969)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.drawChild(ViewGroup.java:3612)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:16202)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.updateDisplayListIfDirty(View.java:15196)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:15969)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.drawChild(ViewGroup.java:3612)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.updateDisplayListIfDirty(View.java:15191)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:15969)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.drawChild(ViewGroup.java:3612)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.updateDisplayListIfDirty(View.java:15191)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:15969)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.drawChild(ViewGroup.java:3612)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewGroup.dispatchDraw(ViewGroup.java:3402)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.draw(View.java:16202)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at com.android.internal.policy.PhoneWindow$DecorView.draw(PhoneWindow.java:2690)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.View.updateDisplayListIfDirty(View.java:15196)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ThreadedRenderer.updateViewTreeDisplayList(ThreadedRenderer.java:281)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ThreadedRenderer.updateRootDisplayList(ThreadedRenderer.java:287)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ThreadedRenderer.draw(ThreadedRenderer.java:322)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewRootImpl.draw(ViewRootImpl.java:2627)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewRootImpl.performDraw(ViewRootImpl.java:2446)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewRootImpl.performTraversals(ViewRootImpl.java:2079)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewRootImpl.doTraversal(ViewRootImpl.java:1119)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.ViewRootImpl$TraversalRunnable.run(ViewRootImpl.java:6060)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.Choreographer$CallbackRecord.run(Choreographer.java:858)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.Choreographer.doCallbacks(Choreographer.java:670)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.Choreographer.doFrame(Choreographer.java:606)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.view.Choreographer$FrameDisplayEventReceiver.run(Choreographer.java:844)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.os.Handler.handleCallback(Handler.java:746)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.os.Handler.dispatchMessage(Handler.java:95)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.os.Looper.loop(Looper.java:148)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at android.app.ActivityThread.main(ActivityThread.java:5443)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at java.lang.reflect.Method.invoke(Native Method)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:728)
02-04 15:09:10.105: E/AndroidRuntime(23153): 	at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:618)
02-04 15:09:10.105: E/AndroidRuntime(23153): Caused by: java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef mentions attr 'dilations' not in Op<name=Conv2D; signature=input:T, filter:T -> output:T; attr=T:type,allowed=[DT_HALF, DT_FLOAT]; attr=strides:list(int); attr=use_cudnn_on_gpu:bool,default=true; attr=padding:string,allowed=[""SAME"", ""VALID""]; attr=data_format:string,default=""NHWC"",allowed=[""NHWC"", ""NCHW""]>; NodeDef: Mobi

"
16751,CUDA Fail in Tensorflow Inference on Jetson TX2,"Hi,

I am getting CUDA fail error for model inference on Jetson TX2 aarch64. I have built the TF source (Version 1.3) for python 3.5 from this github repo:
https://github.com/jetsonhacks/installTensorFlowTX2

- Ubuntu 16.04
- Bazel 0.5.2
- CUDA 8 
- cuDNN 6. 

The relevant discussion on NVIDIA dev forum directed me to post this here:
https://devtalk.nvidia.com/default/topic/1029256/jetson-tx2/cuda-fail-when-running-tensorflow-inference/post/5236860/

TF does work for smaller sized models, but for larger sized models the inference fails. I appreciate if you can please take look at this.

"
16750,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory (when trying to run object detection),"Hello, I am trying to install the object-detection module of tensorflow but when running the following command: 
python3 object_detection/builders/model_builder_test.py

I get the following error. I have install CUDA 8.0,9.0,9.1, and cuDNN 6 and 7 but still have the following error. I appreciate your advice, thank you! 

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""object_detection/builders/model_builder_test.py"", line 18, in <module>
    import tensorflow as tf
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime."
16749,Cross Compiled For Rpi Successfully on Gentoo. No Scope and Session support.,"Hello everyone.
I cross compiled from tensorflow master using armv7a-hardfloat-linux-gnueabi-gcc built using crossdev on Gentoo AMD64

_**make -j9 -f tensorflow/contrib/makefile/Makefile HOST_OS=LINUX TARGET=PI OPTFLAGS=""-Os -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize"" CXX=armv7a-hardfloat-linux-gnueabi-g++**_

 I am able to execute on Rpi, the sample label_image.cc program by compiling it manually using

**_armv7a-hardfloat-linux-gnueabi-g++ -Wl,--whole-archive lib/libtensorflow-core.a -Wl,--whole-archive lib/libnsync.a -Wl,--no-whole-archive lib/libprotobuf.a -ldl -lm -lpthread -lz -I include/ -I include/public/ label_image.cc -std=c++11 `armv7a-hardfloat-linux-gnueabi-pkg-config --cflags --libs libjpeg` -o test_**

However, when trying to compile a program that uses tensorflow::Scope and tensorflow::ClientSession, i get undefined references to them


 _$ armv7a-hardfloat-linux-gnueabi-g++ -Wl,--whole-archive lib/libtensorflow-core.a -Wl,--whole-archive lib/libnsync.a -Wl,--no-whole-archive lib/libprotobuf.a -ldl -lm -lpthread -lz -I include/ -I include/public/ test.cpp -std=c++11 -o test
/tmp/ccZyvm2X.o: In function `main':
test.cpp:(.text+0x9c): undefined reference to `tensorflow::Scope::NewRootScope()'
test.cpp:(.text+0x114): undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'
test.cpp:(.text+0x128): undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'
test.cpp:(.text+0x1a4): undefined reference to `tensorflow::Input::Initializer::Initializer(std::initializer_list<tensorflow::Input::Initializer> const&)'
test.cpp:(.text+0x1b8): undefined reference to `tensorflow::ops::Const(tensorflow::Scope const&, tensorflow::Input::Initializer const&)'
test.cpp:(.text+0x220): undefined reference to `tensorflow::Scope::WithOpName(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const'
test.cpp:(.text+0x280): undefined reference to `tensorflow::ops::MatMul::MatMul(tensorflow::Scope const&, tensorflow::Input, tensorflow::Input, tensorflow::ops::MatMul::Attrs const&)'
test.cpp:(.text+0x2a4): undefined reference to `tensorflow::Scope::~Scope()'
test.cpp:(.text+0x2dc): undefined reference to `tensorflow::ClientSession::ClientSession(tensorflow::Scope const&)'
test.cpp:(.text+0x334): undefined reference to `tensorflow::ClientSession::Run(std::vector<tensorflow::Output, std::allocator<tensorflow::Output> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*) const'
test.cpp:(.text+0x448): undefined reference to `tensorflow::ClientSession::~ClientSession()'
test.cpp:(.text+0x484): undefined reference to `tensorflow::Scope::~Scope()'
test.cpp:(.text+0x53c): undefined reference to `tensorflow::Scope::~Scope()'
test.cpp:(.text+0x5e4): undefined reference to `tensorflow::ClientSession::~ClientSession()'
test.cpp:(.text+0x62c): undefined reference to `tensorflow::Scope::~Scope()'
collect2: error: ld returned 1 exit status_

Has support for **Scope** and **ClientSession** intentionally been left out of the Makefile?
Is there a way to add support for it?

Regards
Mandar Joshi"
16747,No documentation on the order of eigenvalues returned by tf.self_adjoint_eig ,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:('v1.4.0-rc1-11-g130a514', '1.4.0')
- **Python version**: 2.7.14


### Describe the problem
From the documentation of `tf.self_adjoint_eig` I cannot see what the order of eigenvalues is. I tried with several examples and found they were sorted in ascending order. Does this always hold?

"
16745,Consider supporting Microsoft Quantum,"Please consider supporting Microsoft Quantum as a Runtime just like GPUs and TPUs.

Here is the same issue on Microsoft Quantum's repo
https://github.com/Microsoft/Quantum/issues/30

It would be great to have an XLA device/target for Microsoft Quantum"
16743,MonitoredSession after_run hook returning empty SessionRunValues results,"### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Arch Linux 4.14.15-1
- **TensorFlow installed from**: source (master)
- **TensorFlow version**: v1.5.0-2123-g66105a6144
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.3.0
- **CUDA/cuDNN version**: 9.1.85/7.0.5
- **GPU model and memory**: Nvidia GTX 1080 8GB
- **Exact command to reproduce**: `python test.py`

### Describe the problem
When running a `MonitoredSession` with `after_run` hooks, the result passed to `run_values` is None, when there should be output.

### Source code / logs
`test.py`:
```python
import tensorflow as tf

one = tf.Variable(1)

class TestHook(tf.train.SessionRunHook):
    def __init__(self):
        super().__init__()
        self.result = None

    def after_run(self, run_context, run_values):
        # run_values.results should be 1 here
        self.result = run_values.results

hook = TestHook()
with tf.train.MonitoredSession(hooks=[hook]) as sess:
    print('Eval result: {}'.format(
        one.eval(session=sess)))
    print('Hook result: {}'.format(
        hook.result))
```

Expected output:
```
Eval result: 1
Hook result: 1
```

Actual output:
```
Eval result: 1
Hook result: None
```

I've changed this in my fork by replacing https://github.com/tensorflow/tensorflow/blob/3fb47614c4c3f29d59085c2eb6ad9a4f9adfa98e/tensorflow/python/training/monitored_session.py#L1176 with `results=outputs['caller'],`. However, this breaks training with an `Estimator` wrapping a `MonitoredSession`. If I'm misinterpreting the usage of the `after_run` hook please let me know!"
16740,error in code ,"    W_0 = utils.weight_variable([FLAGS.z_dim, 64 * GEN_DIMENSION / 2 * IMAGE_SIZE / 16 * IMAGE_SIZE / 16],
NameError: name 'utils' is not defined
can you help me please "
16737,Discrepancies between GPU and CPU in floating-point operations,"```
bs = 32
dim = 1024

tf.reset_default_graph()
with tf.device(""/cpu:0""):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))

tf.reset_default_graph()
with tf.device(""/gpu:0""):
  probs = tf.where(tf.greater(tf.random_normal((bs, dim)), 0.), tf.ones((bs, dim)), tf.zeros((bs, dim)))
  print(probs)
  logits = tf.log(probs / (1e-10 + 1 - probs))
  s = tf.Session(config=tf.ConfigProto(log_device_placement=True))
  print(s.run([logits]))
```

Running this graph on GPU results in positive infinities, whereas on CPU these tensor entries evaluate to ~88.72284. I could not quite figure out which operation is responsible for the difference. In both cases TensorFlow reports `probs` as `float32`. The difference does not occur when replacing `probs` with a `tf.ones` tensor in `float32` format."
16735,"incorrect logging formatting used in tensorflow / examples / image_retraining / retrain.py, causes error","In tensorflow -> examples -> image_retraining -> retrain.py, currently lines 347 / 348 look like this:

```
tf.logging.info('Successfully downloaded', filename, statinfo.st_size,
                    'bytes.')
```

This understandably causes an error since this function accepts strings and it is being fed an instance of statinfo.st_size which does not seem to be a string.  On my machine at least (TensorFlow 1.5, Windows 10) this causes the following error in function maybe_download_and_extract:

`TypeError: not all arguments converted during string formatting`

Here is a screenshot if that helps:

![error](https://user-images.githubusercontent.com/5672876/35772391-74f4433c-08f2-11e8-83d2-084605c14844.png)

The line numbers are slightly different in my screenshot because I moved a few lines around, but I can assure you the line above is causing the logging error.

I would suggest changing this line to the following, or similar:

`tf.logging.info('Successfully downloaded ' + str(filename) + ', statinfo.st_size = ' + str(statinfo.st_size) + ' bytes')`
"
16723,"Bug: Compile Tensorflow 1.5.0 Java from source failed on NVIDIA Jetson TX2 with error ""'@bazel_tools//tools/jdk:singlejar' must produce a single file""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04 aarch64
- **TensorFlow installed from (source or binary)**:
source on branch r1.5
- **TensorFlow version (use command below)**:
v1.5.0-1934-g9e7ce91 1.5.0
- **Python version**:
Python 3.5
- **Bazel version (if compiling from source)**:
0.9.0 and 0.10.0 (both tried with clean installation)
- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
- **CUDA/cuDNN version**:
CUDA 9.0, cuDNN 7.0
- **GPU model and memory**:
NVIDIA Tegra X2 major (Pascal™ architecture) 8G
- **JDK Version**:

```
root@tegra-ubuntu:/usr/src# java -version
openjdk version ""1.8.0_151""
OpenJDK Runtime Environment (build 1.8.0_151-8u151-b12-0ubuntu0.16.04.2-b12)
OpenJDK 64-Bit Server VM (build 25.151-b12, mixed mode)

root@tegra-ubuntu:/usr/src# javac -version
javac 1.8.0_151
```
- **Exact command to reproduce**:

```shell
root@tegra-ubuntu:/usr/src/tensorflow# ./configure
Extracting Bazel installation...
You have bazel 0.9.0- (@non-git) installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3


Found possible Python library paths:
  /usr/local/lib/python3.5/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python3.5/dist-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: 
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Apache Kafka Platform support? [y/N]: 
No Apache Kafka Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]6.2


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=tensorrt    	# Build with TensorRT support.
Configuration finished
```
The output from compile procedure is

```shell
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
.........................
ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file
ERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted
INFO: Elapsed time: 57.138s
FAILED: Build did NOT complete successfully (7 packages loaded)
    currently loading: tensorflow
```
You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

```
root@tegra-ubuntu:/usr/src# tensorflow/tools/tf_env_collect.sh
Collecting system information...
2018-02-03 09:51:47.561112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:865] ARM64 does not support NUMA - returning NUMA node zero
2018-02-03 09:51:47.561338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1208] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005
pciBusID: 0000:00:00.0
totalMemory: 7.66GiB freeMemory: 465.56MiB
2018-02-03 09:51:47.561450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1308] Adding visible gpu devices: 0
2018-02-03 09:51:48.341988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:989] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 52 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt
```

```
root@tegra-ubuntu:/usr/src# cat tf_env.txt

== cat /etc/issue ===============================================
Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu/Linaro 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux tegra-ubuntu 4.4.38-tegra #1 SMP PREEMPT Fri Dec 1 06:08:28 PST 2017 aarch64 aarch64 aarch64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow (1.5.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0
tf.GIT_VERSION = v1.5.0-1934-g9e7ce91
tf.COMPILER_VERSION = v1.5.0-1934-g9e7ce91
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tensorflow/tools/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/aarch64-linux/lib/libcudart.so.9.0.252
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
```

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

```
root@tegra-ubuntu:/usr/src# python3 -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-1934-g9e7ce91 1.5.0
```

### Describe the problem

I have successfully compiled tensorflow python from source using same configure procedure as above with the following command:

```
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
``` 
There is no error in output and I can install the .whl file with pip.

After that, I tried to compile the Java native library without the configure step (because I already configured it when compiling python version) using the command:

```
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
```

It failed with errors:

```shell
root@tegra-ubuntu:/usr/src/tensorflow# bazel build --config=opt --config=cuda //tensorflow/java:tensorflow //tensorflow/java:libtensorflow_jni
.........................
ERROR: /root/.cache/bazel/_bazel_root/cbe8b06b94787a6b39e59564d90f2497/external/bazel_tools/tools/jdk/BUILD:193:17: in singlejar attribute of java_toolchain rule @bazel_tools//tools/jdk:toolchain: '@bazel_tools//tools/jdk:singlejar' must produce a single file
ERROR: Analysis of target '//tensorflow/java:tensorflow' failed; build aborted: Analysis of target '@bazel_tools//tools/jdk:toolchain' failed; build aborted
INFO: Elapsed time: 57.138s
FAILED: Build did NOT complete successfully (7 packages loaded)
    currently loading: tensorflow
```

### Here are some ways I tried but faild with same error:

1. Configure again (With same configure settings) and compile
2. Remove the directory ~/.cache and do the step 1
3. Remove the directory ~/.cache and tensorflow source directory, git clone tensorflow from r1.5 branch then do step 1
4. Remove ~/.cache and the bazel binary, compile and install bazel from latest source release (0.10.0) without error. Then I use bazel 0.10.0 to compile tensorflow java. This produced same error.
"
16721,Customized loss in keras,"Dear all,

I can run properly with the following code:
### System Information ####
Have I written custom code : As following
OS Platform and Distribution: Linux Ubuntu16.04
TensorFlow installed from : conda script
TensorFlow version : '1.4.0'
Bazel version : N/A
CUDA/cuDNN version : CUDA8.0, cudnn6.0
GPU model and memory : 1070/8G

```
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model
from tensorflow.python.keras.layers import  Input

mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
img_size_flat = 28*28
batch_size = 64

def gen(batch_size=32):
    while True:
        batch_data, batch_label = mnist_data.train.next_batch(batch_size)
        yield batch_data, batch_label   


inputs = Input(shape=(img_size_flat,))
x = Dense(128, activation='relu')(inputs)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation
model = Model(inputs=inputs, outputs=preds)

model.compile(optimizer='rmsprop',
               loss='categorical_crossentropy',
               metrics=['accuracy'])


model.fit_generator(gen(batch_size), steps_per_epoch=len(mnist_data.train.labels)//batch_size, epochs=2)
```

But if I want to write loss function with my own code like:
```
preds_softmax = tf.nn.softmax(preds)
step1 = tf.cast(y_true, tf.float32) * tf.log(preds_softmax)
step2 = -tf.reduce_sum(step1, reduction_indices=[1])
loss = tf.reduce_mean(step2)       # loss
```

Is something like the following code on tensorflow?
```
inputs = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(inputs) # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x) # output layer with 10 units and a softmax activation

y_true = tf.placeholder(tf.float32, shape=(None, 10))
```

How can I do based on above code(part I)? Thanks for any help!!
"
16720,ImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_FalseStruct,"I update my tensorflow to 1.5, the last version is 1.3.0. But I got a issue about numpy, after lot of attempts, it was solved, numpy can be used. 

**But now, I have a new issue when I import tensorflow:**

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _Py_FalseStruct


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

I can't get any solutions on the Internet. Anyone can help me?"
16719,TypeError: 'NoneType' object is not callable,"My environment

python 3.5.2 anaconda (use pyenv)
keras 2.1.3
tensorflow 1.4.1
theano 1.0.1
cntk 2.3

When backend is tensorflow, TypeError occurs just before learning is finished.

TypeError: 'NoneType' object is not callable

Is there a solution?"
16718,add an interface to check if a variable is initialized,"Currently, if we create an Adam optimizer and minimize some loss, Adam will create some new variables that need to be initialized. Howerver, this is only a part of variables and we donot want to use `tf.global_variables_initializer()`.

If there is an interface to check if a variable is initialized, then we can filter global variables and initialize only what needs to be initialized!

the interface should look like `Variable.is_initialized() -> bool`"
16716,Linker Tools Error encountered when use StepStats,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10.0.16299
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5 release
- **Python version**: 
3.5.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 



### Describe the problem

Encounter link error when build the program (source code attached). Build went through well with TF 1.4 release 

Error	LNK2001	unresolved external symbol ""class tensorflow::StepStatsDefaultTypeInternal tensorflow::_StepStats_default_instance_"" (?_StepStats_default_instance_@tensorflow@@3VStepStatsDefaultTypeInternal@1@A)	ReprBug	c:\Users\xx\documents\visual studio 2015\Projects\ReprBug\ReprBug\Source.obj


### Source code / logs

```cpp 
#include ""tensorflow/cc/saved_model/tag_constants.h""
#include ""tensorflow/core/public/session_options.h""
#include ""tensorflow/core/util/stat_summarizer.h""
#include ""tensorflow/contrib/session_bundle/bundle_shim.h""

class SynchronizedStatSummarizer
{
public:
	SynchronizedStatSummarizer(const tensorflow::StatSummarizerOptions& options)
		: m_statSummarizer{ options }, m_mutex{}
	{
	}

	void AddStepStats(const tensorflow::StepStats& stepStats)
	{
		std::lock_guard<std::mutex> guard{ m_mutex };
		m_statSummarizer.ProcessStepStats(stepStats);
	}

private:
	// The TF stat summarizer.
	tensorflow::StatSummarizer m_statSummarizer;

	// Synchronizes access to m_statSummarizer.
	mutable std::mutex m_mutex;
};

int main() {

	tensorflow::SessionOptions sessionOptions;
	tensorflow::RunOptions runOptions{};
	tensorflow::ConfigProto& config = sessionOptions.config;
	
	std::unique_ptr<tensorflow::SavedModelBundle> m_bundle (new tensorflow::SavedModelBundle());

	const std::string path = ""somepath"";
	tensorflow::Status status = tensorflow::serving::LoadSessionBundleOrSavedModelBundle(
		sessionOptions, runOptions, path, { tensorflow::kSavedModelTagServe }, m_bundle.get());

	std::vector<std::pair<std::string, tensorflow::Tensor>> modifiedInputs;
	std::vector<std::string> modifiedOutputNames;
	std::vector<tensorflow::Tensor> tensorOutputs;
	tensorflow::RunMetadata runMetadata{};

	tensorflow::Status run_status = m_bundle->session->Run(
		runOptions, modifiedInputs, modifiedOutputNames, {}, &tensorOutputs, &runMetadata);

	std::unique_ptr<SynchronizedStatSummarizer> m_runTracingStats;

	if (run_status.ok())
	{
		m_runTracingStats->AddStepStats(runMetadata.step_stats());
	}

}
```"
16707,Can't initialize an all zero SparseTensor,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Kind of?
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.12.6 (not relevant)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: NA

### Describe the problem
It doesn't seem possible to initialize a `tf.SparseTensor` with all zero entries. 

A call doing this would look something like:

    tf.SparseTensor(indices=[], values=[], dense_shape=(10, 10))

However, attempting this initialization produces the error:

     ValueError: Shape (0,) must have rank 2


### Source code / logs
Current relevant section from `SparseTensor.__init__`:

    indices_shape = indices.get_shape().with_rank(2) # <--- .with_rank(2) is what causes the problem
    values_shape = values.get_shape().with_rank(1)
    dense_shape_shape = dense_shape.get_shape().with_rank(1)

    # Assert number of rows in indices match the number of elements in values.
    indices_shape[0].merge_with(values_shape[0])
    # Assert number of columns in indices matches the number of elements in
    # dense_shape.
    indices_shape[1].merge_with(dense_shape_shape[0])

Example solution:

    tf.cond(tf.equal(indices.get_shape()[0], 0),
            true_fn=lambda: None,
            false_fn=self._validate_input)

    def _validate_input(self):
        indices_shape = self._indices.get_shape().with_rank(2)
        values_shape = self._values.get_shape().with_rank(1)
        dense_shape_shape = self._dense_shape.get_shape().with_rank(1)

        # Assert number of rows in indices match the number of elements in values.
        indices_shape[0].merge_with(values_shape[0])
        # Assert number of columns in indices matches the number of elements in
        # dense_shape.
        indices_shape[1].merge_with(dense_shape_shape[0])

My only worry with the example solution is that `tf.cond` is too high level a function and there's some alternative that would be better. Is that the case? "
16703,tf.contrib.rnn.GLSTMCell is hilariously broken,"In 3f579020bab8f00e4621e9c7c740cbf13136a809 an ""if"" was added that caches linear transformation weights:
https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L2316

The problem is that this _linear is inside a loop. And so the change tied weights of all these linear transformations.

CC @okuchaiev 
"
16697,gcc: error: unrecognized command line option '--config=opt',"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: release 0.9.0- (@non-git) 
- **GCC/Compiler version (if compiling from source)**: Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/local/stow/gcc-4.9.2/libexec/gcc/x86_64-unknown-linux-gnu/4.9.2/lto-wrapper
Target: x86_64-unknown-linux-gnu
Configured with: /usr/src/nfs/gcc-4.9.2/configure --prefix=/usr/local/stow/gcc-4.9.2
Thread model: posix
gcc version 4.9.2 (GCC)
- **CUDA/cuDNN version**: None
- **GPU model and memory**: x86_64 GNU/Linux
- **Exact command to reproduce**:  bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
gcc: error: unrecognized command line option '--config=opt'

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
'''
ERROR: /usa/haoxu/.cache/bazel/_bazel_haoxu/95196ed5087168c723729aeb7fc160d9/external/flatbuffers/BUILD:22:1: C++ compilation of rule '@flatb
uffers//:flatbuffers' failed (Exit 1)
gcc: error: unrecognized command line option '--config=opt'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 57.329s, Critical Path: 13.22s
FAILED: Build did NOT complete successfully
'''"
16695,Padding algo is not working as doc says,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linus centos 7
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0
- **GCC/Compiler version (if compiling from source)**:0
- **CUDA/cuDNN version**:0
- **GPU model and memory**:0
- **Exact command to reproduce**:

In the following situation, TF [doc](https://www.tensorflow.org/api_guides/python/nn#Convolution) is not correct.
- Input tensor shape : [1, 5, 2, 1]
- Kernel shape:           [1, 3, 1, 1]
- Stride :                      [1, 5, 5, 1]
- Padding =                  ""SAME""

According to the formula we can compute : 
out_h = 1
out_w = 1

```
if (in_height % strides[1] == 0):
  pad_along_height = max(filter_height - strides[1], 0)
else:
  pad_along_height = max(filter_height - (in_height % strides[1]), 0)
if (in_width % strides[2] == 0):
  pad_along_width = max(filter_width - strides[2], 0)
else:
  pad_along_width = max(filter_width - (in_width % strides[2]), 0)
```
gives :
pad_along_height = 0
pad_along_width = 1

then 
```
pad_top = pad_along_height // 2
pad_bottom = pad_along_height - pad_top
pad_left = pad_along_width // 2
pad_right = pad_along_width - pad_left
```

gives:

pad_top = 0
pad_bottom = 0
pad_left = 0
pad_right = 1

How tensorflow do a convolution with a kernel of height 1 on a image of height 5 and which gives output of height 1 (stride = 5) ??? How TF do this ? The doc can't explain the method used ... 

Doing retro engineering, I saw that TF apply the filter on the middle of the input tensor (pad_top = -2 and pad_bottom=-2).
I agree with this method, but the formulas of the Convolution doc is doing max(.., 0) so padding could never be negative (according to the doc).

Could someone explain me clearly what is the formula used in tensorflow ?
Could someone update the doc ?"
16694,"Tensorflow 1.5.0 doesn't compile from source, linking issue with tf.contrib.lite.toco","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**:
3.5.2
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
gcc 5.4.0 20160609
- **CUDA/cuDNN version**:
Tried 9.0 and 9.1 with cuDNN 7.0.5
- **GPU model and memory**:
GeForce GTX 1080Ti
- **Exact command to reproduce**:
```
git clone https://github.com/tensorflow/tensorflow
cd tensorflow
git checkout v1.5.0
./configure  # selected yes for CUDA, no for other optional things
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
```

### Describe the problem
As described in #14573, this is the error I get from Bazel. It is quite long, but it starts with:
```
ERROR: ~/tensorflow/tensorflow/contrib/lite/toco/BUILD:326:1: Linking of rule '//tensorflow/contrib/lite/toco:toco' failed (Exit 1)
/usr/bin/ld: warning: libcublas.so.9.1, needed by bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcufft.so.9.1, needed by bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
/usr/bin/ld: warning: libcurand.so.9.1, needed by bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZsyr2k_v2@libcublas.so.9.1'
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasCgerc_v2@libcublas.so.9.1'
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasChemv_v2@libcublas.so.9.1'
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhemm_v2@libcublas.so.9.1'
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cufftCreate@libcufft.so.9.1'
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreate@libcudnn.so.7'
bazel-out/k8-py3-opt/bin/_solib_local/_U_S_Stensorflow_Scontrib_Slite_Stoco_Ctoco___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreatePoolingDescriptor@libcudnn.so.7'
(and so on for other symbols)
```

As suggested in #14573, adding the following option to Bazel fixes the issue:
```
--action_env=LD_LIBRARY_PATH=/path/to/cuda/lib64/stubs:${LD_LIBRARY_PATH}
```

Note that CUDA is properly installed and all necessary environment variables are set (CUDA_HOME, LD_LIBRARY_PATH). Installing e.g. Tensorflow 1.4.1 (and earlier versions) with exactly the same set up is no problem. Tried with both CUDA 9.0 and CUDA 9.1, both show the same issue.

CC: @kmhofmann @gunan 
"
16692,Bug: using pandas_input_fn with tensorflow.contrib.tensor_forest.client.random_forest.TensorForestEstimator,"### System information
- **Have I written custom code:  YES
- **OS Platform and Distribution : Linux Ubuntu 16.04 LTS
- **TensorFlow installed from : pip
- **TensorFlow version (use command below): 1.4.1
- **Python version: 2.7.12 

### Describe the problem


### Source code / logs

I am working on a simple Tensorflow programme, and build input pipeline with pandas.My code is below:

```
def train_rf(X,Y):
  print(X.shape) #output is  (53443, 131)
  print(Y.shape) #output is  (53443,)

  #features is the list of names of features
  params = tensor_forest.ForestHParams(
      num_classes=2,
      num_features=len(features),
      num_trees=config['rf']['num_trees'])

  est = random_forest.TensorForestEstimator(params, model_dir=config['rf']['model_dir'])

  train_input_fn = tf.estimator.inputs.pandas_input_fn(
      X,
      y=Y,
      batch_size=config['rf']['train_batch_size'],
      num_epochs=1,
      shuffle=True)

  est.fit(input_fn=train_input_fn, steps=100)
```

However, when I run this function, I got error like this:
```
    Traceback (most recent call last):
      File ""random_forest.py"", line 118, in <module>
        train_rf(train_ohd[features].iloc[X_1],l(y_1),train_ohd[features].iloc[X_2])
      File ""random_forest.py"", line 44, in train_rf
        est.fit(input_fn=train_input_fn, steps=100)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
        return func(*args, **kwargs)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 480, in fit
        loss = self._train_model(input_fn=input_fn, hooks=hooks)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 986, in _train_model
        model_fn_ops = self._get_train_ops(features, labels)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1202, in _get_train_ops
        return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.py"", line 1166, in _call_model_fn
        model_fn_results = self._model_fn(features, labels, **kwargs)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/client/random_forest.py"", line 171, in _model_fn
        features)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/python/tensor_forest.py"", line 489, in inference_graph
        data_ops.ParseDataTensorOrDict(input_data))
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/tensor_forest/python/ops/data_ops.py"", line 159, in ParseDataTensorOrDict
        processed_dense_features = array_ops.concat(dense_features, 1)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/array_ops.py"", line 1099, in concat
        return gen_array_ops._concat_v2(values=values, axis=axis, name=name)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_array_ops.py"", line 706, in _concat_v2
        ""ConcatV2"", values=values, axis=axis, name=name)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
        op_def=op_def)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2958, in create_op
        set_shapes_for_outputs(ret)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2209, in set_shapes_for_outputs
        shapes = shape_func(op)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 2159, in call_with_requiring
        return call_cpp_shape_fn(op, require_shape_fn=True)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 627, in call_cpp_shape_fn
        require_shape_fn)
      File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.py"", line 691, in _call_cpp_shape_fn_impl
        raise ValueError(err.message)
    ValueError: Shape must be at least rank 2 but is rank 1 for 'concat' (op: 'ConcatV2') with input shapes: [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [?], [] and with computed input tensors: input[131] = <1>.
```

The output of `print(X.shape)` and `print(Y.shape)` is `(53443, 131)`, `(53443,)` respectively. So I got confused why there will be 132 dimensions in input and why I got this `ValueError`?

What's more, when I used `tf.contrib.learn.LinearRegressor` to replace `TensorForestEstimator`, I can train and eval model with no error. So there's no problems in my `train_input_fn` and I assume this is a bug of Tensorflow.
"
16691,docs error in triplet_semihard_loss(),"Since ||AP|| + alpha < ||AN||, I think the docs in the following

def triplet_semihard_loss(labels, embeddings, margin=1.0):
  """"""Computes the triplet loss with semi-hard negative mining.
  The loss encourages the positive distances (between a pair of embeddings with
  the same labels) to be smaller than the minimum negative distance among
  which are at least greater than the positive distance **plus** the margin constant
  (called semi-hard negative) in the mini-batch. If no such negative exists,
  uses the largest negative distance instead.

should be

def triplet_semihard_loss(labels, embeddings, margin=1.0):
  """"""Computes the triplet loss with semi-hard negative mining.
  The loss encourages the positive distances (between a pair of embeddings with
  the same labels) to be smaller than the minimum negative distance among
  which are at least greater than the positive distance **minus** the margin constant
  (called semi-hard negative) in the mini-batch. If no such negative exists,
  uses the largest negative distance instead.
"
16689,Feature request: Support SparseTensor in Dataset.from_generator ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No

== cat /etc/issue ===============================================
Darwin c02ql2p2fvh7-c 16.7.0 Darwin Kernel Version 16.7.0: Mon Nov 13 21:56:25 PST 2017; root:xnu-3789.72.11~1/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin c02ql2p2fvh7-c 16.7.0 Darwin Kernel Version 16.7.0: Mon Nov 13 21:56:25 PST 2017; root:xnu-3789.72.11~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow (1.5.0)
tensorflow-tensorboard (1.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0
tf.GIT_VERSION = v1.5.0-0-g37aa430d84
tf.COMPILER_VERSION = v1.5.0-0-g37aa430d84
Sanity check: array([1], dtype=int32)
/Users/rbp/anaconda/envs/python3/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5
  return f(*args, **kwds)
/Users/rbp/anaconda/envs/python3/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

- **Exact command to reproduce**:

```
        def generator():
            indices = [(1, 1)]
            values = (1.,)
            shape = (3, 3)
            while True:
                yield tf.SparseTensor(indices, values, shape)

        iterator = tf.data.Dataset.from_generator(generator, tf.float32, (3,3)).make_one_shot_iterator()

        sample = iterator.get_next()
        ss = tf.sparse_reduce_sum(sample)
        with tf.Session() as sess:
            _ss = sess.run(ss)
```

### Describe the problem

Above code throws `AttributeError: 'Tensor' object has no attribute 'indices'`

It seems the `from_generator` always expects Tensors as output."
16688,how to assign the GPU device using C++?,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16686,Is there still a math_ops.h,"I followed the C++ API tutorial (https://www.tensorflow.org/api_guides/cc/guide). But I can't make it because I don't find ""tensorflow/cc/ops/math_ops.h"" (and many other header files). Then I go to the repo on master branch, didn't find them too. 
### System information
- **my code is very simple**
`   using namespace tensorflow; `
  `using namespace tensorflow::ops;`
 ` Scope root = Scope::NewRootScope();`
  `// Matrix A = [3 2; -1 0]`
 ` auto A = Const(root, { {3.f, 2.f}, {-1.f, 0.f} });`
 ` // Vector b = [3 5]`
 ` auto b = Const(root, { {3.f, 5.f} });`
 ` // v = Ab^T`
`  auto v = MatMul(root) ` 
**And the function MatMul can't be recognized.**

- **OS Platform and Distribution:** Linux Ubuntu 16.04
- **TensorFlow installed from**: source
"
16684,The link for the  tutorial on Google's Tensorflow SyntaxNet  page  gives 404 error,"Go tot the page 
https://www.tensorflow.org/versions/r0.12/tutorials/syntaxnet/

and click the ""tutorial"" link. It gets a 404 error.

The target of the link is
https://github.com/tensorflow/models/tree/master/syntaxnet#installation
"
16683,Tensorflow 1.5: failed to use tf.keras.applications.MobileNet(),"I updated my Tensorflow to 1.5, and I tried to run the codes as below:
`import tensorflow as tf`
`model = tf.keras.applications.MobileNet()`

But it raised an error  as below:
---------------------------------------------------------------------------
OSError                                   Traceback (most recent call last)
<ipython-input-14-2182e918e983> in <module>()
----> 1 model = tf.keras.applications.MobileNet()

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/applications/mobilenet.py in MobileNet(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes)
    538     K.set_image_data_format(old_data_format)
    539   elif weights is not None:
--> 540     model.load_weights(weights)
    541   return model
    542 

/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)
   1099     if h5py is None:
   1100       raise ImportError('`load_weights` requires h5py.')
-> 1101     f = h5py.File(filepath, mode='r')
   1102     if 'layer_names' not in f.attrs and 'model_weights' in f:
   1103       f = f['model_weights']

/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in __init__(self, name, mode, driver, libver, userblock_size, swmr, **kwds)
    267             with phil:
    268                 fapl = make_fapl(driver, libver, **kwds)
--> 269                 fid = make_fid(name, mode, userblock_size, fapl, swmr=swmr)
    270 
    271                 if swmr_support:

/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py in make_fid(name, mode, userblock_size, fapl, fcpl, swmr)
     97         if swmr and swmr_support:
     98             flags |= h5f.ACC_SWMR_READ
---> 99         fid = h5f.open(name, flags, fapl=fapl)
    100     elif mode == 'r+':
    101         fid = h5f.open(name, h5f.ACC_RDWR, fapl=fapl)

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/_objects.pyx in h5py._objects.with_phil.wrapper()

h5py/h5f.pyx in h5py.h5f.open()

OSError: Unable to open file (unable to open file: name = 'imagenet', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)

It seems like that something wrong with h5py
I try to upgrade h5py but it's still invalid
This works well in Tensorflow 1.4
How should I resolve it with Tensorflow 1.5, thanks
"
16682,[Recommendation] Expose tensorflow/core/kernels/dataset.h in wheel file,"Hi,

Right now, to build a new dataset op, you need to access to the header file `tensorflow/core/kernels/dataset.h`, but the tensorflow wheel does not expose this header.

Use case: I built new Dataset Ops to read Kaldi's ""Table"" I/O format to enable others to be able to move from Kaldi-based automatic speech recognition recipes to tensorflow-based ones without having to do a bunch of extra data-munging. Right now, I require users to build tensorflow from source code and point my build to the tensorflow source code path, so I can guarantee that I have access to a header file compatible with their binary. I'd prefer to be able to build my package by depending only on pip-installed tensorflow to make things easier on users.

I manually verified that adding 

```
""//tensorflow/core/kernels:dataset"",
```

to the deps of

```
transitive_hdrs(
    name = ""included_headers"",
    deps = [
        ""//tensorflow/core:core_cpu"",
        ""//tensorflow/core:framework"",
        ""//tensorflow/core:lib"",
        ""//tensorflow/core:protos_all_cc"",  
        ""//tensorflow/core:stream_executor"",
        ""//third_party/eigen3"",
    ],
)
```

in `tensorflow/tools/pip_package/BUILD` will include the right header file.

This is a pretty small change. Is there a particular reason why tensorflow does not already expose the header file? Is this an oversight or because you aren't ready to expose this interface publically?"
16677,Dear frinds,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16670,Tensorflow 1.5.0 import error under CUDA 8.0,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip install tensorflow-gpu
- **TensorFlow version (use command below)**:
1.5.0
- **Python version**: 
3.5.4
- **CUDA/cuDNN version**:
8.0/6.0
- **GPU model and memory**:
GTX1080ti
- **Exact command to reproduce**:
pip install tensorflow-gpu
python
import tensorflow as tf

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Tensorflow 1.4 works fine with the same setup as described above. However, after upgrade tensorflow to 1.5 using pip install tensorflow-gpu, it fails to import tensorflow package in python.

### Source code / logs
(tensorflow_1_5) C:\WINDOWS\system32>python
Python 3.5.4 |Continuum Analytics, Inc.| (default, Aug 14 2017, 13:41:13) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\ctypes\__init__.py"", line 351, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Program Files\Anaconda3\envs\tensorflow_1_5\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit"
16669,grappler: memory optimizer fails with: No output shape in Conv2DBackpropInput op / Conv2DBackpropFilter op.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  v1.5.0-0-g37aa430d84
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.1 / 7
- **GPU model and memory**: TITAN Xp, 12196MiB
- **Exact command to reproduce**: -

### Describe the problem
When I enable the memory optimizer in grappler, it fails with the following errors:
```
E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.
E tensorflow/core/grappler/costs/op_level_cost_estimator.cc:720] No output shape in Conv2DBackpropInput op.
```

My network is mostly a ResNet 34. I cannot share the complete code right now, but I can try to come up with a reduced example if it is necessary.

Is this a limitation of the current memory optimizer or should the output shape always be known at this point in the code?"
16665,Android: No OpKernel was registered to support Op 'Min' with these attrs when using custom TensorFlow library built with SELECTIVE_REGISTRATION,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.7.0-homebrew
- **GCC/Compiler version (if compiling from source)**: 4.2.1

### Describe the problem

When running a **custom TensorFlow library** built with `SELECTIVE_REGISTRATION`and running our **quantized model** on Android we see this crash log:

```Ruby
 java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Min' with these attrs.  Registered devices: [CPU], Registered kernels:
 <no registered kernels>

 [[Node: mul_2_eightbit/mul_2/y/min = Min[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](mul_2_eightbit/mul_2/y/reshape, mul_2_eightbit/mul_2/y/reduction_dims)]]
 at org.tensorflow.Session.run(Native Method)
 at org.tensorflow.Session.access$100(Session.java:48)
 at org.tensorflow.Session$Runner.runHelper(Session.java:298)
 at org.tensorflow.Session$Runner.run(Session.java:248)
 at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)
 at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
 at io.cometapp.tensortest.models.yolo.YoloClassifier.predict(YoloClassifier.java:81)
 at io.cometapp.tensortest.ClassifierActivity$4.run(ClassifierActivity.java:713)
 at java.lang.Thread.run(Thread.java:764)
```

Here is how we build the custom TensorFlow Library

`
bazel build -c opt --copt=""-DSELECTIVE_REGISTRATION"" --copt=""-DSUPPORT_SELECTIVE_REGISTRATION""  --copt=""-DTENSORFLOW_DISABLE_META"" --copt=""-D__ANDROID_TYPES_FULL__""  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a
`

If we do not use `--copt=""-DSELECTIVE_REGISTRATION"" --copt=""-DSUPPORT_SELECTIVE_REGISTRATION"" `, we are able to run the model successfully.


Here is the ops_to_register.h that we use

``` C
// This file was autogenerated by print_selective_registration_header.py
#ifndef OPS_TO_REGISTER
#define OPS_TO_REGISTER

    namespace {
      constexpr const char* skip(const char* x) {
        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;
      }

      constexpr bool isequal(const char* x, const char* y) {
        return (*skip(x) && *skip(y))
                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))
                   : (!*skip(x) && !*skip(y));
      }

      template<int N>
      struct find_in {
        static constexpr bool f(const char* x, const char* const y[N]) {
          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);
        }
      };

      template<>
      struct find_in<0> {
        static constexpr bool f(const char* x, const char* const y[]) {
          return false;
        }
      };
    }  // end namespace
    constexpr const char* kNecessaryOpKernelClasses[] = {
""ConcatV2Op<CPUDevice, float>"",
""ConstantOp"",
""DequantizeOp<CPUDevice, quint8>"",
""IdentityOp"",
""ReductionOp<CPUDevice, float, Eigen::internal::MaxReducer<float>>"",
""BinaryOp< CPUDevice, functor::maximum<float>>"",
""ReductionOp<CPUDevice, float, Eigen::internal::MinReducer<float>>"",
""NoOp"",
""PadOp<CPUDevice, float>"",
""PlaceholderOp"",
""QuantizeV2Op<CPUDevice, quint8>"",
""QuantizedBiasAddOp<quint8, quint8, qint32>"",
""QuantizedConv2DOp<quint8, quint8, qint32, Im2ColConvFunctor>"",
""QuantizedMaxPoolingOp<CPUDevice, quint8>"",
""QuantizedMulOp<quint8, qint32>"",
""BinaryOp< CPUDevice, functor::div<float>>"",
""RequantizationRangeOp"",
""RequantizeOp<qint32, quint8>"",
""ReshapeOp"",
""BinaryOp< CPUDevice, functor::sub<float>>"",
""RecvOp"",
""SendOp"",
};
#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || isequal(op, ""ConcatV2"")
     || isequal(op, ""Const"")
     || isequal(op, ""Dequantize"")
     || isequal(op, ""Identity"")
     || isequal(op, ""Max"")
     || isequal(op, ""Maximum"")
     || isequal(op, ""Min"")
     || isequal(op, ""NoOp"")
     || isequal(op, ""Pad"")
     || isequal(op, ""Placeholder"")
     || isequal(op, ""QuantizeV2"")
     || isequal(op, ""QuantizedBiasAdd"")
     || isequal(op, ""QuantizedConv2D"")
     || isequal(op, ""QuantizedMaxPool"")
     || isequal(op, ""QuantizedMul"")
     || isequal(op, ""RealDiv"")
     || isequal(op, ""RequantizationRange"")
     || isequal(op, ""Requantize"")
     || isequal(op, ""Reshape"")
     || isequal(op, ""Sub"")
     || isequal(op, ""_Recv"")
     || isequal(op, ""_Send"")
  ;
}
#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)

#define SHOULD_REGISTER_OP_GRADIENT false
#endif

```
"
16663,extract_glimpse padding with fixed value,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64 bit
- **TensorFlow installed from (source or binary)**: source (anaconda)
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.0
- **CUDA/cuDNN version**:
- **GPU model and memory**: not relevant
- **Exact command to reproduce**: not relevant


### Describe the problem
[tf documentation](https://www.tensorflow.org/api_docs/python/tf/image/extract_glimpse)
> Returns a set of windows called glimpses extracted at location offsets from the input tensor. If the windows only partially overlaps the inputs, the non overlapping areas will be filled with random noise.

the function `tf.image.extract_glimpse ` padds windows that reach outside of the input tensor with random values from either a Gaussian or a normal distribution.
However this prevents consistent classification in some cases.

Would it be possible to add the option of specifying a fixed padding value (i.e. zero padding)?


"
16662,"Current Bazel version is 0.10.0, expected at least 0.5.4","I get this error message when trying to build from source (r1.5) with the new bazel version published today.

Current Bazel version is 0.10.0, expected at least 0.5.4

I guess the version check is wrong. "
16660,GPU support for Java in windows,"I am working with Tensorflow in Java using the Maven dependency.

I would like to use the GPU version in my Java application but I notice there is only a supported Maven repository for Linux.

Will there be support for Tensorflow with Java in windows?
If so then what is the timeline for this?

Otherwise is there another way round it?

Thanks."
16658,Runtime Error with Qt GUI Application,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from source**:
- **TensorFlow version use master**:
- **Python version 2.7**: 
- **Bazel version 0.9.0**:
- **GCC/Compiler version 5.4.0**:
- **Without CUDA/cuDNN**:
- **Without GPU**:

### Describe the problem
When I used QtCreator to build GUI Application, if include ""tensorflow/core/lib/core/refcount.h"", it will throw The program has unexpectedly finished.

.pro like
####
    SOURCES += \
        main.cpp \
        mainwindow.cpp
    HEADERS += \
         mainwindow.h
    FORMS += \
         mainwindow.ui
    
    #tensorflow
    INCLUDEPATH += /home/face/Desktop/tensorflow/bazel-genfiles`
    INCLUDEPATH += /home/face/Desktop/tensorflow`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/gen/protobuf/include`
    INCLUDEPATH += /home/face/Desktop/tensorflow/tensorflow/contrib/makefile/downloads/nsync/public`
    INCLUDEPATH += /home/face/Desktop/eigen-eigen-5a0156e40feb`
    LIBS += -L/home/face/Desktop/tensorflow/bazel-bin/tensorflow -ltensorflow_cc -ltensorflow_framework

main.cpp
####
    #include ""mainwindow.h""
    #include <QApplication>
    #include <tensorflow/core/platform/env.h>
    #include <tensorflow/core/public/session.h>

    int main(int argc, char *argv[])
    {
        QApplication a(argc, argv);
        MainWindow w;
        w.show();
        return a.exec();
    }

then if ""tensorflow/core/lib/core/refcount.h"" line 79
####
    inline RefCounted::~RefCounted() {
        DCHECK_EQ(ref_.load(), 0); 
    }
to
####
    inline RefCounted::~RefCounted() {
        //DCHECK_EQ(ref_.load(), 0); 
    }
it will work.

### Source code / logs
debug log like:
####
    1  google::protobuf::internal::Mutex::Lock()                                    0x7fffde0c3516 
    2  google::protobuf::internal::OnShutdown(void ( *)())                          0x7fffde0c3833 
    3  call_init                                                     dl-init.c  72  0x7ffff7de76ba 
    4  call_init                                                     dl-init.c  30  0x7ffff7de77cb 
    5  _dl_init                                                      dl-init.c  120 0x7ffff7de77cb 
    6  dl_open_worker                                                dl-open.c  575 0x7ffff7dec8e2 
    7  _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    8  _dl_open                                                      dl-open.c  660 0x7ffff7debda9 
    9  dlopen_doit                                                   dlopen.c   66  0x7ffff18f0f09 
    10 _dl_catch_error                                               dl-error.c 187 0x7ffff7de7564 
    11 _dlerror_run                                                  dlerror.c  163 0x7ffff18f1571 
    12 __dlopen                                                      dlopen.c   87  0x7ffff18f0fa1 
    13 ??                                                                           0x7ffff33100e5 
    14 ??                                                                           0x7ffff3309975 
    15 QFactoryLoader::instance(int) const                                          0x7ffff32ff07e 
    16 QPlatformThemeFactory::create(QString const&, QString const&)                0x7ffff0b30231 
    17 QGuiApplicationPrivate::createPlatformIntegration()                          0x7ffff0b3aaf8 
    18 QGuiApplicationPrivate::createEventDispatcher()                              0x7ffff0b3b4bd 
    19 QCoreApplicationPrivate::init()                                              0x7ffff331ab3b 
    20 QGuiApplicationPrivate::init()                                               0x7ffff0b3cf7b 
    21 QApplicationPrivate::init()                                                  0x7ffff392d3b9 
    22 main                                                          main.cpp   103 0x402e3e  

"
16657,Tensorflow on banana-pi m64,"How to install TF on banana m64? OS: Linux bpi-iot-ros-ai 3.10.105-BPI-M64-Kernel 

When i trying install i have an error

> tensorflow-1.5.0-cp34-none-any.whl is not a supported wheel on this platform.
"
16656,Upgrade protobuf pip package version on Windows machine,"http://ci.tensorflow.org/job/tf-master-win-bzl/2410/console
The tests in the Bazel Windows build started to fail after https://github.com/tensorflow/tensorflow/commit/e818d10f84bad3faad398f4b55831064666af5df

After upgrade protobuf, you might also need to update protoc version in the CMake build.
@gunan "
16654,Bazel version comparison fails with bazel 0.10.0,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No, just commented 6 lines in the bzl files out
- **OS Platform and Distribution**: 16.04 on Jetson TX2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.10.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: TX2 GPU, 5GB (not sure)
- **Exact command to reproduce**: bazel build -c opt --local_resources 3072,4.0,1.0 --verbose_failures --config=cuda //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I cannot build tensorflow using bazel 0.10.0. It seems like the version checks in repositories.bzl and wokspace.bzl fail. Commenting them out solves the issue, even though I know that is no persistent solution. I think it is simply that bazel thinks that 0.10.0 is smaller 0.5.4 due to its string comparison, but I am no bazel expert.

### Source code / logs
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:15:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 15
		closure_repositories()
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 69, in closure_repositories
		_check_bazel_version(""Closure Rules"", ""0.4.5"")
	File ""/home/nvidia/.cache/bazel/_bazel_nvidia/01c445c7b00bca0241913a79fcd99718/external/io_bazel_rules_closure/closure/repositories.bzl"", line 172, in _check_bazel_version
		fail((""%s requires Bazel >=%s but was...)))
Closure Rules requires Bazel >=0.4.5 but was 0.10.0- (@non-git)
ERROR: Error evaluating WORKSPACE file
ERROR: /home/nvidia/git/tensorflow/WORKSPACE:41:1: Traceback (most recent call last):
	File ""/home/nvidia/git/tensorflow/WORKSPACE"", line 41
		tf_workspace()
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 48, in tf_workspace
		check_version(""0.5.4"")
	File ""/home/nvidia/git/tensorflow/tensorflow/workspace.bzl"", line 38, in check_version
		fail(""\nCurrent Bazel version is {}, ...))

Current Bazel version is 0.10.0- (@non-git), expected at least 0.5.4
ERROR: Error evaluating WORKSPACE file
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'external': Package 'external' contains errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'external': Package 'external' contains errors
INFO: Elapsed time: 3.145s
FAILED: Build did NOT complete successfully (0 packages loaded)
"
16652,v1.3 batch_norm layer,"I use the batch norm layer like this:
`def batch_norm_layer(x,train_phase,scope_bn):

	bn_train = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=True,
	reuse=None, # is this right?
	trainable=True,
	scope=scope_bn)
	bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,
	is_training=False,
	reuse=True, # is this right?
	trainable=True,
	scope=scope_bn)
	z = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)
	return z`
I don't know in v1.3.0 is the code worked?
I saw the [issue1122](https://github.com/tensorflow/tensorflow/issues/1122#issuecomment-232535426), someone said it would not work well.

thank you in advance."
16648,Error while building Tensorflow model from bazel ,"INFO: From Linking tensorflow/libtensorflow_framework.so [for host]:
LINK : warning LNK4044: unrecognized option '/Wl,-soname,libtensorflow_framework.so'; ignored
LINK : warning LNK4044: unrecognized option '/pthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
   Creating library bazel-out/host/bin/tensorflow/libtensorflow_framework.ifso and object bazel-out/host/bin/tensorflow/libtensorflow_framework.exp
ERROR: C:/courses/tensorflow/tensorflow/cc/BUILD:422:1: Linking of rule '//tensorflow/cc:ops/array_ops_gen_cc' failed (Exit 1181)
LINK : warning LNK4044: unrecognized option '/pthread'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : warning LNK4044: unrecognized option '/lm'; ignored
LINK : fatal error LNK1181: cannot open input file 'tensorflow_framework.obj'
Target //tensorflow/contrib/android:libtensorflow_inference.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3434.951s, Critical Path: 365.14s
FAILED: Build did NOT complete successfully

Is LNK4044 warning related to c++ path, i've given VisualStudio.exe path while installing"
16646,can tf.estimator.Estimator's  parameters be modified by hand? ,"TF's  high level API  is very convenient to defined a new model. 
However, many DNN Machine Learning task has to reuse some old model's parameter to fill a new model and then  fine-tune it in new tasks. 
I have read the tf.estimator.Estimator'API  carefully, but cann't find any API to set It's parameters. Hope  TF developer  add this function to the high level API. 
Thank very much!"
16645,What's the difference between Univariate prediction and Multivariate prediction?,My understanding is that paramenters of neurals are shared in Multivariate prediction and they can learn some correlations between series. There is less training time in Multivariate prediction. I wonder if that's right. Could you please explain any basic principles of Multivariate prediction with LSTM or recommend related papers to me? Thank you.
16644,clear,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16641,ImportError after compiling,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

Yes. I followed this guide in an attempt to gain GPU support under macOS. (https://tweakmind.com/tensorflow-1-5-macos-10-13-2/). The code changes are made by these commands:

sed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc
sed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/split_lib_gpu.cu.cc
sed -i.bu 's/__align__(sizeof(T)) //g' tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc


- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
macOS 10.13.2 High Sierra

- **TensorFlow installed from (source or binary)**:
Source

- **TensorFlow version (use command below)**:
1.5

- **Python version**: 
3.6.4

- **Bazel version (if compiling from source)**:
0.9.0-homebrew

- **GCC/Compiler version (if compiling from source)**:
xcode 8.3.3

- **CUDA/cuDNN version**:
9.1 / 7

- **GPU model and memory**:
Nvidia GTX 1080 Ti

- **Exact command to reproduce**:
python
import tensorflow as tf


### Describe the problem
After following the guide at the URL above, building w/bazel and creating the wheel, I installed the tensorflow package with pip3 install --upgrade --force-reinstall <package name> and was successful. However, when I try to import tensorflow, I get an ImportError, Symbol not found: _PyCObject_Type

### Source code / logs

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyCObject_Type
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so, 6): Symbol not found: _PyCObject_Type
  Referenced from: /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
  Expected in: flat namespace
 in /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so


Failed to load the native TensorFlow runtime.
"
16638,Feature request: use padded_batch with tf.estimator.export.build_parsing_serving_input_receiver_fn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6
- **TensorFlow installed from (source or binary)**: I forget
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.8
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
As far as I can tell, tf.estimator.export.build_parsing_serving_input_receiver_fn() doesn't allow you to control the way examples are batched. So if I have an Estimator that I've trained by using Dataset.padded_batch() in the input_fn, there doesn't seem to be a way for me to use that model with TensorFlow Serving.

### Source code / logs
N/A?"
16631,Allow variable_overwrites on scope level,"This is a request for allowing to pass a dict of `variable_overwrites` to variable scopes which to be returned when `tf.get_variable` is called instead of the usual procedure, if they are provided, otherwise do the usual procedure. A simple example of this beahviour is:
```
    import numpy as np
    with tf.variable_scope(""one""):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
            x1 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c1 = tf.sqrt(tf.abs(x1 + a))

    variables_overwrites = {x1._shared_name: c1}
    with tf.variable_scope(""one"", reuse=tf.AUTO_REUSE, variables_overwrites=variables_overwrites):
        a = tf.ones((5, 5), tf.float32)
        with tf.variable_scope(""two""):
           // x2 here is in fact the value of c1
            x2 = tf.get_variable(""x"", initializer=np.random.randn(5, 5).astype(""float32""))
            c2 = tf.sqrt(tf.abs(x2 + a))
```
This is particularly usefull for being able easily to bootstrap neural network parameters coming from inside the layers trough a standard function interface. My specific usage is for HMC for NN parameters. This is a question on whether you guys are interested in this so that I spend more time on doing this properly."
16629,How to get RunMetadata for tf.data.Dataset ops?,"Tensorflow version: 1.5 (pip)

I am interested in the runtime of ops when using the `tf.data.Dataset` api.
Information like how long shuffle, repeat or batch took.
When running the code below and visualizing it in tensoboard the intersting ops are marked as ""unused substructure"".

Is my approach the right one?
I find it a little confusing that they show up, but are not traceable.

Example code
```python
import numpy as np
import tensorflow as tf

tf.set_random_seed(0)

data = np.array([[10] * 10] * 3 + [[4] * 10]).astype(np.float32)

with tf.Graph().as_default():
  dataset = tf.data.Dataset.from_tensor_slices(data)
  sess = tf.Session()

  dataset = dataset.cache()
  dataset = dataset.shuffle(10, seed=0)
  dataset = dataset.repeat(5)
  dataset = dataset.batch(2)

  iterator = tf.data.Iterator.from_structure(
      output_types=tf.float32)
  batch = iterator.get_next()

  init1 = iterator.make_initializer(dataset)

  sess.run(init1)

  writer = tf.summary.FileWriter('tmp/datasettest/', sess.graph)
  run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)

  i = 0
  while True:
    try:
      run_metadata = tf.RunMetadata()
      sess.run(batch, options=run_options, run_metadata=run_metadata)
      writer.add_run_metadata(run_metadata, 'step{}'.format(i))
      i += 1
    except tf.errors.OutOfRangeError as ex:
      break
    except Exception as ex:
      raise ex
```
This will result in the following visualization in Tensorboard.

![image](https://user-images.githubusercontent.com/9438971/35647492-054aa5b0-06d3-11e8-8394-9b821fb79603.png)
"
16628,Tensorflow switches to CPU when using Variable.assign,"### System information
- **OS**:Windows 10
- **TensorFlow installed from**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6.4
- **CUDA/cuDNN version**: 8.0 / 64
- **GPU model and memory**: GeForce GTX 1080 8 GB
- **Exact command to reproduce**: run the provided code below

### Describe the problem
I'm using a `tf.Variable` for the learning rate of a optimizer. If I change its value with `sess.run(var.assign(0.1))` the performance drops extremly and it seems tensorflow switches from GPU use to only CPU use (no workload on the GPU and more load on the CPU).

### Source code / logs

I did write a minimal working example (training a network with XOR). **To see the difference just comment the line `sess.run(learning_rate.assign(0.1))` out** and it will run much much faster using the GPU.

```
import tensorflow as tf


def XOR(x_, y_):
    Theta1 = tf.Variable(tf.random_uniform([2, 2], -1, 1), name=""Theta1"")
    Theta2 = tf.Variable(tf.random_uniform([2, 1], -1, 1), name=""Theta2"")

    Bias1 = tf.Variable(tf.zeros([2]), name=""Bias1"")
    Bias2 = tf.Variable(tf.zeros([1]), name=""Bias2"")

    with tf.name_scope(""layer2""):
        A2 = tf.sigmoid(tf.matmul(x_, Theta1) + Bias1)

    with tf.name_scope(""layer3""):
        Hypothesis = tf.sigmoid(tf.matmul(A2, Theta2) + Bias2)

    with tf.name_scope(""cost""):
        cost = tf.reduce_mean(((y_ * tf.log(Hypothesis)) +
                               ((1 - y_) * tf.log(1.0 - Hypothesis))) * -1)

    return cost


if __name__ == ""__main__"":
    x_ = tf.placeholder(dtype=tf.float32, shape=[4, 2], name='x-input')
    y_ = tf.placeholder(dtype=tf.float32, shape=[4, 1], name='y-input')
    xor_cost = XOR(x_, y_)

    learning_rate = tf.Variable(0.1, dtype=tf.float32)

    with tf.name_scope(""train""):
        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(xor_cost)

    XOR_X = [[0, 0], [0, 1], [1, 0], [1, 1]]
    XOR_Y = [[0], [1], [1], [0]]

    sess = tf.Session()
    init = tf.global_variables_initializer()
    sess.run(init)

    for i in range(40001):
        sess.run(learning_rate.assign(0.1))

        _, loss = sess.run(fetches=[train_step, xor_cost], feed_dict={x_: XOR_X, y_: XOR_Y})

        if i % 100 == 0:
            print('iteration: {0:5}, loss: {1:15.10f}'.format(i, loss))


```"
16627,"TF consumes all available RAM with a particular combination of conv2d, batch_norm and LSTM","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.5.0-0-g37aa430d84 1.5.0
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0.176 / 7.0.5
- **GPU model and memory**: Tesla V100 (AWS P3), 16GB
- **Exact command to reproduce**: python debug_tf.py

### Describe the problem
When the following code is run on a p3.2xlarge, the python process starts consuming RAM indefinitely, until the entire server RAM is used and the server dies. That's system RAM, not GPU memory. 

It doesn't look it, but the code below is the smallest I could find that produces the bad behavior.

1. Replacing residual_conv with a simple convolution makes the code work (i.e not hang).
2. Reducing the repetition number in `layers.repeat` (to e.g 2) makes the code work.
3. Removing the batch normalization from residual_conv makes the code work.
4. Removing the LSTM makes the code work.
5. The weirdest of all, if I set `is_training=True` in batch_norm instead of `is_training=is_training_var` whose value is set to `True` in the feed_dict, then the code works.

The same code runs successfully on a AWS P2 server with TensorFlow 1.3.

### Source code / logs

```python
import numpy as np
import tensorflow as tf
import tensorflow.contrib.layers as layers
from tensorflow.contrib.framework import arg_scope


def residual_conv(incoming, num_filters, scope, bn=True):
	with tf.variable_scope(scope):
	    input_filters = incoming.get_shape().as_list()[-1]
	    if input_filters != num_filters:
	        incoming = layers.conv2d(incoming, num_filters, scope='adjust_conv')

	    after_conv1 = layers.conv2d(incoming, num_filters)
	    after_conv2 = layers.conv2d(after_conv1, num_filters, normalizer_fn=None, activation_fn=None)

	    net = incoming + after_conv2

	    if bn:
	        net = layers.batch_norm(net)

	    return net

def tf_bilstm(incoming, n_units, name):
	net = incoming

	lstm_f = tf.contrib.rnn.LSTMCell(n_units)
	lstm_b = tf.contrib.rnn.LSTMCell(n_units)

	with tf.variable_scope(name):
	    results, _ = tf.nn.bidirectional_dynamic_rnn(lstm_f, lstm_b, net, 
	                                                 dtype=tf.float32, time_major=True)
	return tf.concat(results, axis=2)

def main():
	x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))
	is_training_var = tf.placeholder(dtype=bool)

	net = x_var

	with arg_scope([layers.batch_norm], is_training=is_training_var, decay=0.99, scale=True
	               ), \
	     arg_scope([layers.conv2d], padding='SAME', kernel_size=(3, 3)), \
	     arg_scope([layers.max_pool2d], padding='SAME', kernel_size=(2, 2), stride=(2, 2)):

	    net = layers.repeat(net, 20, residual_conv, 64, scope='block1')
	    net = layers.max_pool2d(net)

	    net = tf.squeeze(net, 2)
	    net = tf.transpose(net, [1, 0, 2])
	    net = tf_bilstm(net, 512, 'lstm1')

	sess = tf.Session()
	sess.run(tf.global_variables_initializer())
	
	X = np.zeros([10, 100, 2, 512])

	print('Right before TF call!')

	a = sess.run(net, {x_var: X, is_training_var: True})

	print(a)


if __name__ == '__main__':
	main()
```

Edited: Simplified the code."
16626,"example script multivariate.py throws ""UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.  This may consume a large amount of memory.""","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am using the example script `tensorflow/contrib/timeseries/examples/multivariate.py`
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac OS High Sierra (darwin)
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
v1.5.0-rc1-1781-g86c10063c8 1.5.0-rc1
- **Python version**: 
3.6.4
- **Bazel version (if compiling from source)**:
0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**:
Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**:
n/a compiled without CUDA support
- **GPU model and memory**:
n/a GPU not supported on Mac with SIP
- **Exact command to reproduce**:
$ python $GOPATH/src/github.com/tensorflow/tensorflow/tensorflow/contrib/timeseries/examples/multivariate.py

### Describe the problem
The script throws warning:
`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.`

### Source code / logs
Here is the traceback of the warning on my system (I have tensorflow installed in a virtual environment called ""tf3"".:
```
  File ""multivariate.py"", line 59, in multivariate_train_and_sample
    estimator.train(input_fn=train_input_fn, steps=training_steps)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 352, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 809, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 790, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 228, in create_estimator_spec
    return self._train_ops(features)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/timeseries/python/timeseries/head.py"", line 85, in _train_ops
    learning_rate=None)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py"", line 241, in optimize_loss
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 458, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in gradients
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 376, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 610, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py"", line 589, in _PadGrad
    x_grad = array_ops.slice(grad, begin, sizes)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 640, in slice
    return gen_array_ops._slice(input_, begin, size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 4591, in _slice
    ""Slice"", input=input, begin=begin, size=size, name=name)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1036, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 97, in _IndexedSlicesToTensor
    ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
  File ""~/.pyenv/versions/3.6.4/lib/python3.6/warnings.py"", line 99, in _showwarnmsg
    msg.file, msg.line)
~/.pyenv/versions/tf3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
```
I see discussion about this warning on SO:
https://stackoverflow.com/questions/35892412/tensorflow-dense-gradient-explanation/35893467
But the problem seems to happen in the estimator ""train"" method, not in any custom code I have written."
16624,Crash caused by depthwise_conv on iOS,"Hi

I integrated the same tflite mode to android and iOS app, on android it always works fine, but on iOS the result is incorrect and sometimes app crashed, seems like there's an issue at depthwise_conv. 
The crash log is as follows:

```
Hardware Model:      iPhone7,1
Process:             Demo [287]
Path:                /private/var/containers/Bundle/Application/D412E130-5C9E-45B1-A84D-FD93FFAB9025/Demo.app/Demo
Identifier:          Sensteer.Demo2
Version:             1.0.0 (1.0)
Code Type:           ARM-64 (Native)
Role:                Non UI
Parent Process:      launchd [1]
Coalition:           Sensteer.Demo2 [457]


Date/Time:           2018-01-31 19:08:36.5843 +0800
Launch Time:         2018-01-31 19:06:58.3105 +0800
OS Version:          iPhone OS 11.1.2 (15B202)
Baseband Version:    6.17.00
Report Version:      104

Exception Type:  EXC_BAD_ACCESS (SIGSEGV)
Exception Subtype: KERN_INVALID_ADDRESS at 0x0000000102ea41dc
VM Region Info: 0x102ea41dc is not in any region.  Bytes after previous region: 16861  Bytes before following region: 15908
      REGION TYPE                      START - END             [ VSIZE] PRT/MAX SHRMOD  REGION DETAIL
      mapped file            0000000102e64000-0000000102ea0000 [  240K] r--/r-- SM=ALI  
--->  GAP OF 0x8000 BYTES
      MALLOC_LARGE           0000000102ea8000-0000000102ec8000 [  128K] rw-/rwx SM=PRV  

Termination Signal: Segmentation fault: 11
Termination Reason: Namespace SIGNAL, Code 0xb
Terminating Process: exc handler [0]
Triggered by Thread:  0

Filtered syslog:
None found

Thread 0 name:  Dispatch queue: com.apple.main-thread
Thread 0 Crashed:
0   libsystem_platform.dylib      	0x0000000185d21b60 _platform_memmove + 176
1   Demo                          	0x0000000100fcd238 tflite::optimized_ops::DepthwiseConv+ 463416 (float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, float const*, tflite::Dims<4> const&, int, int, int, int, int, float, float, float*, tflite::Dims<4> const&) + 820
2   Demo                          	0x0000000100fd435c void tflite::ops::builtin::depthwise_conv::EvalFloat<(tflite::ops::builtin::depthwise_conv::KernelType)2>+ 492380 (TfLiteContext*, TfLiteNode*, TfLiteDepthwiseConvParams*, tflite::ops::builtin::depthwise_conv::OpData*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*) + 744
3   Demo                          	0x0000000100fcbc60 TfLiteStatus tflite::ops::builtin::depthwise_conv::Eval<(tflite::ops::builtin::depthwise_conv::KernelType)2>+ 457824 (TfLiteContext*, TfLiteNode*) + 96
4   Demo                          	0x0000000100f827d4 tflite::Interpreter::Invoke+ 157652 () + 316
```


**System information**

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra 10.13.2
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): cloned on 2018.1.24
Python version: 2.7.10
Bazel version (if compiling from source): 0.7.0-homebrew"
16623,toco crashed while change model to lite,"I use toco to change model to lite format, it crashed like the following statck (PS: toco was compiled in my own machine with the tensorflow code. The git commit is 9fb9ac66ce5236ff045630cad6793f0531bf9d2c, with date Sun Jan 21 07:43:08 2018 +0800 )

Program received signal SIGSEGV, Segmentation fault.
0x000000000046bdf8 in void toco::(anonymous namespace)::EvaluateBinaryOperatorOnConstantInputs<(toco::ArrayDataType)2, (toco::ArrayDataType)2>(toco::Model*, toco::Operator const*) ()
(gdb) bt
#0  0x000000000046bdf8 in void toco::(anonymous namespace)::EvaluateBinaryOperatorOnConstantInputs<(toco::ArrayDataType)2, (toco::ArrayDataType)2>(toco::Model*, toco::Operator const*) ()
#1  0x000000000046e3e9 in toco::ResolveConstantBinaryOperator::Run(toco::Model*, unsigned long) ()
#2  0x0000000000489dd3 in toco::(anonymous namespace)::GraphTransformationsPass(int, toco::Model*, toco::GraphTransformationsSet const&) ()
#3  0x000000000048aae0 in toco::RunGraphTransformations(toco::Model*, std::string const&, toco::GraphTransformationsSet const&) ()
#4  0x000000000042b608 in toco::Transform(toco::TocoFlags const&, toco::Model*) ()
#5  0x000000000041c5e8 in main ()


It can reproduce by unzip my attached file
[test.zip](https://github.com/tensorflow/tensorflow/files/1681710/test.zip)
, and run the command 
toco --input_file=/home/jeyawn/ML/test.pb --output_file=/home/jeyawn/ML/test.lite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=FLOAT --input_shape=1,368,368,3 --input_array=image --input_data_type=FLOAT --output_arrays=Openpose/MConv_Stage6_L1_5_pointwise/BatchNorm/FusedBatchNorm,Openpose/MConv_Stage6_L2_5_pointwise/BatchNorm/FusedBatchNorm


Following is my env:

== cat /etc/issue ===============================================
Linux jeyawn-virtual-machine 3.13.0-128-generic #177-Ubuntu SMP Tue Aug 8 11:40:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux jeyawn-virtual-machine 3.13.0-128-generic #177-Ubuntu SMP Tue Aug 8 11:40:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.8.2)
protobuf (2.5.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
nvidia-smi: command not found

== cuda libs  ===================================================
----------------------------------------------------------------------------------------------------



"
16622,update tensorflow to 1.5 ,"I update tensorflow to 1.5,and reinstall cuda to 9.1,now  I run my program get the error:
Traceback (most recent call last):
  File ""/home/chris/tensorflowDemo/7_1_Word2Vec.py"", line 24, in <module>
    import tensorflow as tf
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/chris/.local/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/usr/lib/python3.5/imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""/usr/lib/python3.5/imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory"
16621,Using tf.train.SyncReplicasOptimizer with multiple optimizers,"I am trying to run the DeepLab Resnet (https://github.com/DrSleep/tensorflow-deeplab-resnet) in a distributed setup. I opted Synchronous Data parallel training approach similar to the one demonstrated in the Inception distributed training example.(https://github.com/tensorflow/models/tree/master/research/inception).

In the Inception example, a single RMS optimizer is used to reduce the loss. The tf.train.SyncReplicasOptimizer function wraps the optimizer and becomes responsible for synchronization, aggregation and application of gradients to various workers. Also, it takes care of updating the global_step variable. In my case, the DeepLab Resnet makes use of three optimizers each handling specific portions of the network. Following snippet explains the case:

    `#Three optimizers declared with different learning rates
     opt_conv = tf.train.MomentumOptimizer(learning_rate, args.momentum)
     opt_fc_w = tf.train.MomentumOptimizer(learning_rate * 10.0, args.momentum)
     opt_fc_b = tf.train.MomentumOptimizer(learning_rate * 20.0, args.momentum)`

    #Scope for every optimizer 
    grads = tf.gradients(reduced_loss, conv_trainable + fc_w_trainable + fc_b_trainable)
    grads_conv = grads[:len(conv_trainable)]
    grads_fc_w = grads[len(conv_trainable) : (len(conv_trainable) + len(fc_w_trainable))]
    grads_fc_b = grads[(len(conv_trainable) + len(fc_w_trainable)):]

    #Gradients applied to various portions of the network
    train_op_conv = opt_conv.apply_gradients(zip(grads_conv, conv_trainable))
    train_op_fc_w = opt_fc_w.apply_gradients(zip(grads_fc_w, fc_w_trainable))
    train_op_fc_b = opt_fc_b.apply_gradients(zip(grads_fc_b, fc_b_trainable))
 
    `#tf.group to combine all three operations
    train_op = tf.group(train_op_conv, train_op_fc_w, train_op_fc_b)`
   
I don't have any clue about using the tf.train.SyncReplicasOptimizer for multiple optimizers to achieve synchronous data parallel training. Also, I don't have an idea about updating the global_step variable and using the chief_queue_runner for this case. Please help me on this.


 


"
16620,How to use model.summary() when using placeholder instead of Input(keras),"Dear all, 
I follow  post in ""https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html""
The little modified code I use is:
----------------------------------------------------------------------------------------------------------------------------------
import tensorflow as tf
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.backend import categorical_crossentropy
from tensorflow.examples.tutorials.mnist import input_data
from tensorflow.python.keras.models import Model


sess = tf.Session()
img = tf.placeholder(tf.float32, shape=(None, 784))
x = Dense(128, activation='relu')(img)  # fully-connected layer with 128 units and ReLU activation
x = Dense(128, activation='relu')(x)
preds = Dense(10, activation='softmax')(x)  # output layer with 10 units and a softmax activation

labels = tf.placeholder(tf.float32, shape=(None, 10))
loss = tf.reduce_mean(categorical_crossentropy(labels, preds))
mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(loss)

init_op = tf.global_variables_initializer()
sess.run(init_op)
with sess.as_default():
    for i in range(100):
        batch = mnist_data.train.next_batch(50)
        train_step.run(feed_dict={img: batch[0],
                                  labels: batch[1]})

----------------------------------------------------------------------------------------------------------------------------------
It work fine until I use model.summary :  

model = Model(inputs=img, outputs=preds)

The error message show"" Input tensors to a Model must come from `tf.layers.Input`""
I can use tf.layers.Input to solve this problem.
But I really want to use tf.placeholder so I can feed data as I like.
Can anyone help me?  Thanks!!"
16619,add new feature for tfdbg that ignoring specific layers,"Using tfdbg, `has_inf_or_nan` filter is very useful. But when I need `inf` at some calculations in the network explicitly, this filter catches `inf` every time. (e.g. https://github.com/keras-team/keras/issues/9161 )

So I want to suggest new feature, `ignore option`, to `add_tensor_filter`. (I think it's around [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/wrappers/local_cli_wrapper.py#L193). right?)

This option can take layer's names and layers which is selected to this option are ignored explicitly when tfdbg check `inf_or_nan` in the network.

This is also useful other than in my case above. If my understanding is not correct, let me know.

Thanks."
16617,tf.tensordot on GPU,"Hi, I came across a problem when using eager execution with GPU.
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution **:  Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: Python 2.7.12
- **CUDA/cuDNN version**: cuda-9.0
- **GPU model and memory**:  GeForce GTX 1080 Ti, 11Gb RAM
- **Exact command to reproduce**:

```python
from __future__ import division
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT)

with tf.device(""/gpu:0""):
    a = tf.ones([1, 2])
    b = tf.ones([2, 1])
    c = tf.tensordot(a, b, axes=1)`
```

I get the error
```
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'ListDiff' OpKernel for GPU devices compatible with node ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](dummy_input, dummy_input)  
```
If I remove `device_policy=tfe.DEVICE_PLACEMENT_SILENT`, there is still an error
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu(), or transparently copied by using tfe.enable_eager_execution(tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: Tensordot/Cast/
```
"
16614,AttributeError: module 'tensorflow' has no attribute 'keras',"(tensorflow) admins-Mac-Pro:get_started admin$ python premade_estimator.py
Traceback (most recent call last):
  File ""premade_estimator.py"", line 88, in <module>
    tf.app.run(main)
  File ""/Users/admin/Work/tensorflow/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 43, in run
    sys.exit(main(sys.argv[:1] + flags_passthrough))
  File ""premade_estimator.py"", line 34, in main
    (train_x, train_y), (test_x, test_y) = iris_data.load_data()
  File ""/Users/admin/Work/tensorflow/models/samples/core/get_started/iris_data.py"", line 19, in load_data
    train_path, test_path = maybe_download()
  File ""/Users/admin/Work/tensorflow/models/samples/core/get_started/iris_data.py"", line 12, in maybe_download
    train_path = tf.keras.utils.get_file(TRAIN_URL.split('/')[-1], TRAIN_URL)
AttributeError: module 'tensorflow' has no attribute 'keras'"
16613,tensorflow upgrade made the spyder-ide editor auto-complite fail in python 3.6.4,"the enviroment is in the python 3.6.4, with the tensorflow upgrade to the new version 1.5.0. The function in spyder editor auto-complitation was failed. After I uninstall the package future and futures, the function was work again. 
I also made a test in python 3.5.4, and there was no problem."
16611,Bug: tf.train.monitoredtrainingsession non-chief worker does not start,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: GTX-1080 11GB (chief) and M2000 4GB (slave)
- **Exact command to reproduce**:
CUDA_VISIBLE_DEVICES='' python3 test.py --job-name ps --task-index 0
CUDA_VISIBLE_DEVICES=0 python3 test.py --job-name worker --task-index 0
CUDA_VISIBLE_DEVICES='' python3 test.py --job-name ps --task-index 1
CUDA_VISIBLE_DEVICES=0 python3 test.py --job-name worker --task-index 1

### Describe the problem
I am trying to run the [distributed training](https://www.tensorflow.org/deploy/distributed) and use [tf.train.MonitoredTrainingSession](https://www.tensorflow.org/api_docs/python/tf/train/MonitoredTrainingSession) with 2 PCs. PC1 has one GTX-1080 11GB and is set as chief, while PC2 has one M2000 4GB and is set as non-chief. They are connected back-to-back without switch/router. The chief worker was running okay but the non-chief worker was stuck at `tf.train.MonitoredTrainingSession` and did not proceed to execute the code within the session.

### Source code / logs
```
import argparse
import tensorflow as tf

def parse_command():
    parser = argparse.ArgumentParser(description='Monitor Training Session Test.')
    parser.add_argument('--job-name', dest='job_name', default=""worker"", nargs='?', help='job name [worker|ps]')
    parser.add_argument('--task-index', dest='task_index', type=int, default=0, help='task index')
    return parser.parse_args()

if __name__ == '__main__':
    print(""Test started..."")

    cluster = {
        ""ps"" : [
             ""192.168.0.2:2221"",
             ""192.168.0.1:2221""
             ],
        ""worker"" : [
             ""192.168.0.2:2222"",
             ""192.168.0.1:2222""
             ]}

    options = parse_command()
    cluster_spec = tf.train.ClusterSpec(cluster)
    server = tf.train.Server(server_or_cluster_def=cluster_spec,
                             job_name=options.job_name,
                             task_index=options.task_index)

    if options.job_name == ""ps"":
        server.join()
        sys.exit(0)

    is_chief = (options.task_index == 0)
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    config.log_device_placement = True
    step_size = 5

    print(""Running "" + options.job_name + "":"" + str(options.task_index))

    with tf.device(tf.train.replica_device_setter(
        worker_device = ""/job:worker/task:%d"" % options.task_index,
        cluster = cluster_spec)) :

        global_step = tf.train.get_or_create_global_step()
        learning_rate = tf.train.exponential_decay(0.1, global_step, step_size, 0.94, staircase=True)

        with tf.train.MonitoredTrainingSession(master=server.target,
                                               is_chief=is_chief) as session:

            print(""MonitoredTrainingSession started"")

            for i in range(10):
                for j in range(step_size):
                    lr, gstep = session.run([learning_rate, global_step])
                    print(""learning rate="" + str(lr) + "", global step="" + str(gstep))
```

**PC1 (GTX-1080) Logs**

```
...
Running worker:0
2018-01-31 10:29:44.888497: I tensorflow/core/distributed_runtime/master_session.cc:1004] Start master session 9a6571b5ba45d49d with config:
MonitoredTrainingSession started
learning rate=0.1, global step=0
learning rate=0.1, global step=0
learning rate=0.1, global step=0
...
```

**PC2 (M2000) Logs**
```
...
2018-01-31 10:29:37.753239: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
Running worker:1
(...wait for 1800 secs)
MonitoredTrainingSession ""Session was not ready after waiting 1800 secs""
```

I was using the [tf.train.SyncReplicasOptimizer](https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer) example to implement between-graph and synchronous training but found that the non-chief worker has never printed `MonitoredTrainingSession started`. Then I slowly remove all the unnecessary code (which becomes the code provided above) and found that `tr.train.MonitoredTrainingSession` does not seem to work for the bare minimum configuration. Please can you kindly have a look? 

Many thanks!"
16609,Tensorflow on AMD - specific chips,"Hi

I live abroad so the choice in hardware is more limited here. I only have the below option for graphics card, but from reading i find it hard to figure out if tensorflow would work w this or not. Some threads say that it works with ""newer ones from AMD"", others say ""it requires a lot of setup"". Can anyone say if tensorflow can run of the gpu of the below ""out of the box""?

AMD Radeon Pro WX 4150 w/4GB GDDR5"
16608,Can't find Stochastic Tensors class,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **Ubuntu 16.04**:
- **source**:
- **TensorFlow version 1.5**:
- **3.6**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
Apparently, `stochastic_tensor` is not present in `tf.contrib.bayesflow` which doesn't reflect here in the [api_guides](https://www.tensorflow.org/api_guides/python/contrib.bayesflow.stochastic_tensor). I have not mentioned my complete system information as I think it might not be needed, please let me know if it is required to further investigate the issue. Any help/suggestions would be highly appreciated.

### Source code / logs
```
In [7]: import tensorflow as tf

In [8]: st=tf.contrib.bayesflow.stochastic_tensor
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-8-0979a5c2af2d> in <module>()
----> 1 st=tf.contrib.bayesflow.stochastic_tensor

AttributeError: module 'tensorflow.contrib.bayesflow' has no attribute 'stochastic_tensor'

In [9]: tf.__version__
Out[9]: '1.5.0-rc1'
```"
16606,tf.contrib.data.rejection_resample not balancing class/freq on random crops,"randomly sampling / cropping data seems to break rejection_resample ==> meaning it won't do any re balancing of the class probability -- see these two simple feeders as example:

The first randomly samples data with tf.random_uniform and breaks rejection_resample -- the second with static random data having  the same distribution is correctly resampled (output has p(class=0)=0)

This creates issues when trying to real-time sample/crop/data-augment and at the same time rebalance classes with on the same pipeline

```python
def get_data_breaks(self, batch_size, iihook, this_set='train'):
    def sample(data, label): # sample detection window inside chunk
        xx = tf.cast(tf.random_uniform([1])*self.class_num, tf.int32)[0]
        with tf.control_dependencies([xx]): tf.Print(xx , [xx], 'xx>>')
        return xx, xx

    initial_dist=[1.0/self.class_num for cc in range(self.class_num)]
    classes = np.random.choice(self.class_num,20000,p=initial_dist)

    data_ph = tf.placeholder(classes.dtype, classes.shape)
    labels_ph = tf.placeholder(classes.dtype, classes.shape)
    dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))

    dataset = dataset.map(sample, num_parallel_calls=1)

    target_dist=[1.0/self.class_num for cc in range(self.class_num)]
    target_dist[1]+=target_dist[0] ; target_dist[0]=0
    print('target-dist>>', target_dist)
    initial_dist = None

    dataset = dataset.apply(tf.contrib.data.rejection_resample(
                class_func=lambda c, _: c,
                target_dist=target_dist,
                initial_dist=initial_dist,
                seed=42)).map(lambda a,b: b)

    dataset = dataset.repeat(None)
    iterator = dataset.make_initializable_iterator()
    iihook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,
                    feed_dict={data_ph: classes, labels_ph: classes})

    return iterator.get_next()


def get_data_works(self, batch_size, iihook, this_set='train'):

    initial_dist=[1.0/self.class_num for cc in range(self.class_num)]
    classes = np.random.choice(self.class_num,20000,p=initial_dist)

    data_ph = tf.placeholder(classes.dtype, classes.shape)
    labels_ph = tf.placeholder(classes.dtype, classes.shape)
    dataset = tf.data.Dataset.from_tensor_slices((data_ph, labels_ph))

    target_dist=[1.0/self.class_num for cc in range(self.class_num)]
    target_dist[1]+=target_dist[0] ; target_dist[0]=0
    print('target-dist>>', target_dist)
    initial_dist = None

    dataset = dataset.apply(tf.contrib.data.rejection_resample(
                class_func=lambda c, _: c,
                target_dist=target_dist,
                initial_dist=initial_dist,
                seed=42)).map(lambda a,b: b)

    dataset = dataset.repeat(None)
    iterator = dataset.make_initializable_iterator()
    iihook.iterator_initializer_func = lambda sess: sess.run(iterator.initializer,
                    feed_dict={data_ph: classes, labels_ph: classes})

    return iterator.get_next()
"
16605,Description in docs of one-hot vector for mnist deep example confusing and/or wrong,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:n/a
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Windows
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Documentation found at https://www.tensorflow.org/versions/r1.4/get_started/mnist/pros describes one of the two parameters passed to the training process as a 2d tensor of ""one-hot"" 10-dimensional vectors specifying the classes of the samples in the other 2d tensor parameter. But clearly the placeholders define 2d and 1d tensors, not 2d and 2d. There is no ""one-hot"" representation used at all as far as I can tell by using print() statements - if the class if a sample is class 3, then the corresponding entry is simple 3, not the one-hot representation of it. If this class is subsequently converted into a one-hot representation, it does not happen in the mnist_deep.py source file. I'm too much of a beginner to say what the documentation should say, but it seems at best confusing, and at worst completely wrong.

### Source code / logs
n/a
"
16603,1.3.0-py3 flagged by security issue CVE-2017-5754,Any chance a rebuild of the 1.3.0-py3 docker image is easy enough to pick up security patches?  It would save a team a lot of work.  Thank you.
16594,Layers created with tf.layers not listed with tf.contrib.framework.get_model_variables,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: Not Applicable
- **GCC/Compiler version (if compiling from source)**: Not Applicable
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: GeForce 940MX

### Describe the problem
Currently I'm using [slim models](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) to build my networks, but for extra layers I'm using the [tf.layers](https://www.tensorflow.org/api_docs/python/tf/layers) API. When I try to retrieve the list of variables with the function [`tf.contrib.framework.get_model_variables`](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/get_model_variables)
only the layers created inside slim models are retrieved.

### Minimal code to reproduce the issue

```python
import tensorflow as tf
x = tf.random_normal([2,5],name='x')
y = tf.contrib.layers.fully_connected(x,10,scope='Contrib_Fully_Connected') # Retrieved
z = tf.layers.dense(x,10, name='Layers_Fully_Connected') # Not Retrieved
print(tf.contrib.framework.get_model_variables())
```

Output:

```bash
[<tf.Variable 'Contrib_Fully_Connected/weights:0' shape=(5, 10) dtype=float32_ref>, <tf.Variable 'Contrib_Fully_Connected/biases:0' shape=(10,) dtype=float32_ref>]
```

### Workaround:

Use [`tf.get_collection`](https://www.tensorflow.org/api_docs/python/tf/get_collection)

```python
import tensorflow as tf
x = tf.random_normal([2,5],name='x')
y = tf.contrib.layers.fully_connected(x,10,scope='Contrib_Fully_Connected') # Retrieved
z = tf.layers.dense(x,10, name='Layers_Fully_Connected') # Not Retrieved
print(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES))
```
the output is:
```bash
[<tf.Variable 'Contrib_Fully_Connected/weights:0' shape=(5, 10) dtype=float32_ref>,
 <tf.Variable 'Contrib_Fully_Connected/biases:0' shape=(10,) dtype=float32_ref>,
 <tf.Variable 'Layers_Fully_Connected/kernel:0' shape=(5, 10) dtype=float32_ref>,
 <tf.Variable 'Layers_Fully_Connected/bias:0' shape=(10,) dtype=float32_ref>]
```

Is it worth to bring this issue as a bug and improve the funcionality of `tf.contrib.framework.get_model_variables` or will this contrib function be discontinued?"
16593,"contrib.tfgan: batch_norm is_training=True for both training and inferencing, non-slim version","Hi, I am exploring contrib.tfgan, such a great work @joel-shor .

### batch_norm is_training=True for both training and inferencing
However, when I see the example in source code of both generator and discriminator of MNIST, as below.

https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb

`with slim.arg_scope(
        [layers.fully_connected, layers.conv2d_transpose],
        activation_fn=tf.nn.relu, normalizer_fn=layers.batch_norm,
        weights_regularizer=layers.l2_regularizer(weight_decay)):
        net = layers.fully_connected(noise, 1024)
        net = layers.fully_connected(net, 7 * 7 * 256)
        net = tf.reshape(net, [-1, 7, 7, 256])`

The default argument of layers.batch_norm is set to True, and this gen_fn and dis_fn are used for  both training phase and generating test images phase (inferencing).

Is it a bug or it is intended? If it is intended, can you explain why is that?

### non-slim implementation
In addition, I don't really like slim, and I believe some people don't either. Can I use other model construction libraries like tf.layers or keras to build the network. Is tfslim a must?

Thank you,

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.5 and 1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.7
- **GCC/Compiler version (if compiling from source)**: 4.2
- **CUDA/cuDNN version**:NA (CPU)
- **GPU model and memory**:NA
- **Exact command to reproduce**:"
16592,"Freeze-graph not working, allocating too much memory for inference","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: ('v1.5.0-rc1-1783-g7d7dce1', '1.5.0-rc1')
- **Python version**: Python 2.7.12
- **Bazel version (if compiling from source)**: Build label: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0
- **CUDA/cuDNN version**: N/A (CPU only)
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

save_alexnet_checkpoint.py
```
import os

import numpy as np
import tensorflow as tf

from alexnet import AlexNet
from datagenerator import ImageDataGenerator
from datetime import datetime
from tensorflow.contrib.data import Iterator

""""""
Configuration Part.
""""""

num_classes = 1000
checkpoint_path = ""path/to/ckpt""


# Create parent path if it doesn't exist
if not os.path.isdir(checkpoint_path):
    os.mkdir(checkpoint_path)

# TF placeholder for graph input and output


# Initialize model
x = tf.placeholder(tf.float32, [1, 227, 227, 3],name=""input"")
keep_prob=tf.placeholder(tf.float32,[],name=""keepProbs"")
model = AlexNet(x, keep_prob, num_classes, [])
softmax = tf.nn.softmax(model.fc8,name=""softmax"")

# Initialize an saver for store model checkpoints
saver = tf.train.Saver()

# Start Tensorflow session
with tf.Session() as sess:

	# Validate the model on the entire validation set
	sess.run(tf.global_variables_initializer())

	# Load the pretrained weights into the non-trainable layer
	model.load_initial_weights(sess)

	# save checkpoint of the model
	checkpoint_name = os.path.join(checkpoint_path,
		                       'original_alexnet.ckpt')
	save_path = saver.save(sess, checkpoint_name)

	tf.train.write_graph(sess.graph_def, checkpoint_path, 'alexnet_def.pb',as_text=False)

	names=[]
	for n in tf.get_default_graph().as_graph_def().node:
		names.append(str(n.name))

    	names = sorted(names, key=str.lower)
    	for n in names:
		print n


	print(""Model checkpoint saved at {}"".format(checkpoint_name))
```
<br><br>

Freeze saved graph_def and weights:
```
bazel build tensorflow/python/tools:freeze_graph
bazel-bin/tensorflow/python/tools/freeze_graph --input_graph=/path/to/alexnet_def.pb -- input_checkpoint=path/to/ckpt/original_alexnet.ckpt --output_graph=/path/to/frozen_alexnet.pb --output_node_names='softmax' --input_binary=True
```
<br><br>

Forward Inference:
```
import os
import cv2
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from alexnet import AlexNet
from tensorflow.python.client import timeline
from caffe_classes import class_names

frozen_graph='/path/to/frozen_alexnet.pb '

def load_graph(frozen_graph_filename):
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    with tf.Graph().as_default() as graph:
        tf.import_graph_def(graph_def, name=""prefix"")
    return graph

imagenet_mean = np.array([104., 117., 124.], dtype=np.float32)

current_dir = os.getcwd()
image_dir = os.path.join(current_dir, 'images')

img_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.jpeg')]

imgs = []
for f in img_files:
    imgs.append(cv2.imread(f))

g=load_graph(frozen_graph)
with tf.Session(graph=g) as sess:

    softmax=g.get_tensor_by_name('prefix/softmax:0')
    x = g.get_tensor_by_name(""prefix/input:0"")
    keep_prob = g.get_tensor_by_name(""prefix/keepProbs:0"")

    for i, image in enumerate(imgs):
        
        img = cv2.resize(image.astype(np.float32), (227,227))
        img -= imagenet_mean
        img = img.reshape((1,227,227,3))

	options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
	run_metadata = tf.RunMetadata()
        
        print 'Begin session:'
        probs = sess.run(softmax, feed_dict={x: img, keep_prob: 1}, \
			options=options, run_metadata=run_metadata)
        print 'Ended session'
	print 'Class: ' + str(class_names[np.argmax(probs)]) \
	+ ', Prob: ' + str(probs[0,np.argmax(probs)])
	fetched_timeline = timeline.Timeline(run_metadata.step_stats)
	chrome_trace = fetched_timeline.generate_chrome_trace_format(show_memory=True)
	with open('./forward_timeline.json', 'w') as f:
		f.write(chrome_trace)

```

log_alexnet.sh
```
#!/bin/bash

logpid() { while sleep 0.1; do ps -p $1 -o pcpu= -o pmem= ; done; }
python frozen-infer.py &
logpid $! | tee ./pid.log
```
### Describe the problem

I ran my own Alexnet frozen model and the forward inference script I had required a large amount of RAM, around 2.466 GB rather than the memory required to load model size which was (243.9 MB).

Essentially I took scripts from [finetune_alexnet_with_tensorflow](https://github.com/kratzert/finetune_alexnet_with_tensorflow), including ```alexnet.py```, ```datagenerator.py```, ```caffe_classes.py```, and heavily modifying ```validate_alexnet_on_imagenet.ipynb``` to save a checkpoint of the Alexnet-implementation for TensorFlow. The weights were taken from this [link](http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/). I ran ```save_alexnet_checkpoint.py``` first to generate a binary proto graph definition of the AlexNet file and corresponding checkpoint files. Using the default ```freeze_graph``` tool from TensorFlow compiled by Bazel, I created a frozen graph. I then ran ```log_alexnet.sh``` to see how much of my RAM a forward inference uses on my laptop. The image I used was from the above mentioned github repo, llama. The script outputs the percentage of CPU and memory the python script ```frozen-infer.py```  uses. 

My CPU: Intel(R) Core(TM) i5-8250U CPU @ 1.60GHz

My RAM is 8 GB, but according to the free command, it is 7.909 GB, where htop reports it as 7.54 GB. The maximum RAM allocated by ```sess.run``` and the python script was thus, 0.327*7.54 = 2.466 GB (see below log). This is insanely larger than the 243.9 MB model I had. The ```chrome_trace``` I generated reports a maximum of around 235 MB, which is no where near what was actually happening.
[forward_timeline.json.tar.gz](https://github.com/tensorflow/tensorflow/files/1678731/forward_timeline.json.tar.gz)

My question is why does the session run require such memory, and how am I able to reduce it down to only consume the required memory to load the model, which would be ~250 MB? I am concerned about this because I will to transferring this code to a Raspberry Pi 3 soon.

It seems someone else had the same issue on stackoverflow [here](https://stackoverflow.com/questions/46531213/after-freeze-graph-the-inference-is-slower-and-requires-more-memory).

### Source code / logs
The output I got is such from log_alexnet.sh:

```
 0.0  0.7
 0.0  1.0
 0.0  1.2
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
 0.0  1.4
 0.0  1.8
 0.0  2.0
 0.0  2.2
 0.0  7.7
 194  5.3
Begin session:
 204 10.4
 215 16.8
 226 17.7
 236 22.1
 248 25.8
 259 30.7
 269 32.7
 280 27.7
 145 29.4
 151 27.3
 156 29.2
 162 24.6
 167 28.9
 172 27.0
 178 27.8
 183 17.1
 188 18.5 
 194 18.1
Ended session
Class: llama, Prob: 0.99927825
 134 21.3
```
"
16590,how to save model for tensroflwo serving for lstm in tensorflow/contrib/timeseries/examples/lstm.py,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
when I run lstm in tensorflow/contrib/timeseries/examples/lstm.py, I tried to add methods to save model into savedModel, but it gives back errors.

  File ""/Users/yang/.local/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 504, in export_savedmodel
    serving_input_receiver = serving_input_receiver_fn()
  File ""/Users/yang/.local/lib/python3.4/site-packages/tensorflow/contrib/timeseries/python/timeseries/estimators.py"", line 133, in _serving_input_receiver_fn
    self._model.initialize_graph()
TypeError: initialize_graph() missing 1 required positional argument: 'input_statistics'

The issue I guess is that, in self._model.initialize_graph(), no parameters are given, but in 

    def initialize_graph(self, input_statistics):
        """"""Save templates for components, which can then be used repeatedly.
        This method is called every time a new graph is created. It's safe to start
        adding ops to the current default graph here, but the graph should be
        constructed from scratch.
        Args:
          input_statistics: A math_utils.InputStatistics object.
        """"""
        super(_LSTMModel, self).initialize_graph(input_statistics=input_statistics)
        with tf.variable_scope("""", use_resource=True):
          # Use ResourceVariables to avoid race conditions.
          self._lstm_cell = tf.nn.rnn_cell.LSTMCell(num_units=self._num_units)
          # Create templates so we don't have to worry about variable reuse.
          self._lstm_cell_run = tf.make_template(
              name_=""lstm_cell"",
              func_=self._lstm_cell,
              create_scope_now_=True)
          # Transforms LSTM output into mean predictions.
          self._predict_from_lstm_output = tf.make_template(
              name_=""predict_from_lstm_output"",
              func_=functools.partial(tf.layers.dense, units=self.num_features),
              create_scope_now_=True)

one param input_statistics is asked. But how to fix this issue

### Source code / logs
    
    serving_input_receiver_fn = estimator.build_raw_serving_input_receiver_fn()
    estimator.export_savedmodel(
        ""../model"",
        serving_input_receiver_fn
    )
"
16589,"TensorFlowLite cannot locate symbol ""__atomic_store_8"" crash on Android x86 devices ","### System information

- **OS Platform and Distribution** : Android x86 v5.0+
- **TensorFlow installed from** : built using gradle from maven repository: https://google.bintray.com/tensorflow
- **TensorFlow version** : 1.2.0-rc0

### Describe the problem
We migrated our app to use tensorflow lite but it crashes on x86 upon invoking the inference module. It works perfect on ARM v7a devices.

### Source code / logs
Here's the exception:
```
01-15 03:39:47.138  2020  2056 W System.err: TensorFlowLite: failed to load native library: dlopen failed: cannot locate symbol ""__atomic_store_8"" referenced by ""/data/app/com.XXXXXXX.XXXXXXXXXXXXXXXX-1/lib/x86/libtensorflowlite_jni.so""...
01-15 03:39:47.138  2020  2056 E art     : No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: FATAL EXCEPTION: XXXXXServiceThread
01-15 03:39:47.183  2020  2056 E AndroidRuntime: Process: com.XXXXXXX.XXXXXXXXXXXXXXXX, PID: 2020
01-15 03:39:47.183  2020  2056 E AndroidRuntime: java.lang.UnsatisfiedLinkError: No implementation found for long org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(int) (tried Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter and Java_org_tensorflow_lite_NativeInterpreterWrapper_createErrorReporter__I)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.createErrorReporter(Native Method)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:47)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:77)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at com.XXXXXXX.XXXXXXXXXXXXXXXX.utils.classifier.TensorflowLiteClassifier.<init>(TensorflowLiteClassifier.java:46)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at com.XXXXXXX.XXXXXXXXXXXXXXXX.service.classifier.TensorFlowFileClassifier.initialize(TensorFlowFileClassifier.java:41)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at com.XXXXXXX.XXXXXXXXXXXXXXXX.service.servicethread.ServiceThreadModel.onFileSystemScanStarted(ServiceThreadModel.java:92)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at com.XXXXXXX.XXXXXXXXXXXXXXXX.service.servicethread.ServiceThread$1.handleMessage(ServiceThread.java:82)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:102)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:154)
01-15 03:39:47.183  2020  2056 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:61)
01-15 03:39:47.1
```"
16588,IllegalArgumentException: Retval[0] does not have value,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: iMac (27-inch, Late 2013) OS: 10.13.3 (17D47)
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**:  Using TensorFlow backend.
1.5.0-rc1
- **Python version**: Python 3.6.4
- **Bazel version (if compiling from source)**: 
Build label: 0.9.0-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Sun Jul 12 12:24:01 +49936 (1513677414241)
Build timestamp: 1513677414241
Build timestamp as int: 1513677414241

- **GCC/Compiler version (if compiling from source)**: Xcode 9.2
Build version 9C40b
- **CUDA/cuDNN version**: No (CPU only)
- **GPU model and memory**: No
- **Exact command to reproduce**:

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

if I add GRU or LSTM to my code and try to export this model to Android I have got the exception:
```
Successfully loaded model from 'file:///android_asset/frozen_opt.pb'
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/TensorFlowImageClassif: Read 24 labels, output layer size is 32
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Camera orientation relative to screen canvas: 90
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow I/tensorflow: ClassifierActivity: Initializing at size 640x480
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow E/art: No implementation found for void ru.rimidalv.tensorflow.env.ImageUtils.convertYUV420SPToARGB8888(byte[], int[], int, int, boolean) (tried Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888 and Java_ru_rimidalv_tensorflow_env_ImageUtils_convertYUV420SPToARGB8888___3B_3IIIZ)
01-30 07:45:35.424 25635-25635/ru.rimidalv.tensorflow W/tensorflow: ImageUtils: Native YUV420SP -> RGB implementation not found, falling back to Java implementation
01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[the_input], outputs:[softmax/truediv]
01-30 07:45:35.714 25635-25666/ru.rimidalv.tensorflow E/AndroidRuntime: FATAL EXCEPTION: inference
                                                                        Process: ru.rimidalv.tensorflow, PID: 25635
                                                                        java.lang.IllegalArgumentException: Retval[0] does not have value
                                                                            at org.tensorflow.Session.run(Native Method)
                                                                            at org.tensorflow.Session.access$100(Session.java:48)
                                                                            at org.tensorflow.Session$Runner.runHelper(Session.java:298)
                                                                            at org.tensorflow.Session$Runner.run(Session.java:248)
                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)
                                                                            at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)
                                                                            at ru.rimidalv.tensorflow.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:171)
                                                                            at ru.rimidalv.tensorflow.ClassifierActivity$2.run(ClassifierActivity.java:175)
                                                                            at android.os.Handler.handleCallback(Handler.java:739)
                                                                            at android.os.Handler.dispatchMessage(Handler.java:95)
                                                                            at android.os.Looper.loop(Looper.java:158)
                                                                            at android.os.HandlerThread.run(HandlerThread.java:61)
```

code to export my model:

```
bazel build tensorflow/python/tools:freeze_graph
bazel-bin/tensorflow/python/tools/freeze_graph \
--input_graph=$PB_MAIN/protobuf_path.pbtxt \
--input_checkpoint=$PB_MAIN/checkpoint_path.ckpt \
--output_graph=$PB_MAIN/frozen_graph.pb \
--output_node_names=softmax/truediv \
```
and than

```
!bazel build tensorflow/tools/graph_transforms:transform_graph
!bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
--in_graph=$PB_MAIN/frozen_graph.pb \
--out_graph=$PB_MAIN/frozen_opt.pb \
--inputs='the_input:0' \
--outputs='softmax/truediv:0' \
--transforms='add_default_attributes strip_unused_nodes(type=float, shape=""-1,128,64,1"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_nodes round_weights strip_unused_nodes sort_by_execution_order'
```


### Source code / logs
My model:

```python
    input_shape = (128, 64, 1)
    latent_dim = 128
    decoder_inputs = Input(shape=(input_shape), name='the_input')
    conv_to_rnn_dims = (32, 128*2 )
    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(decoder_inputs)
    inner = GRU(latent_dim, return_sequences=True, kernel_initializer='he_normal')(inner)
    decoder_dense = Dense(10, activation='softmax', name=""softmax"")(inner)
    model = Model(inputs=[decoder_inputs], outputs=decoder_dense)
    model.compile(loss={'softmax': lambda y_true, y_pred: y_pred}, optimizer=""adam"")
```

Save the model:

```python

    K.set_learning_phase(0)
    model = load_model(os.path.join('model_data', 'model.h5'), compile=False)
    model.load_weights(os.path.join('model_data', 'weights00.h5'))
    sess = K.get_session()
   
    protobuf_path = os.path.join('tf-exports', 'protobuf_path.pbtxt')
    checkpoint_path = os.path.join('tf-exports', 'checkpoint_path.ckpt')

    tf.train.write_graph(sess.graph_def, '.', protobuf_path)

    saver = tf.train.Saver()
    saver.save(sess, save_path = checkpoint_path)

```
"
16587,Feature deprecated in h5py is used in TF1.5,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux, OS X
- **TensorFlow installed from (source or binary)**: source and binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 
- **CUDA/cuDNN version**: 9.0 - 7.0
- **GPU model and memory**: GTX1060, GTX 1050Ti 
- **Exact command to reproduce**:
`sudo pip3 install h5py`
run python3, from there, type:
`import tensorflow as tf`


### Describe the problem
A feature of h5py used in TF 1.5 is deprecated, in particular: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated.

### Source code / logs
Warning message: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
"
16585,TensorFlow with CUDA or Python might rebuilds more than necessary instead of re-using bazel cache,"Context: for DeepSpeech, we perform tensorflow builds and then keep the cache in a tar (capturing the whole of the home directory of the build user). We then untar it and the deepspeech build through `bazel build` picks the proper cached items so it does not rebuild anything.

Recently, we started to have increased (2.5x) build time on CUDA-enabled builds. Debugging with Bazel showed that it was rebuilding because the `actionKey` computed for `stream_executor_impl` was different. Instrumenting Bazel to get more informations, I could get down to the reason of the different actionKey: the ordering of the CUDA includes was different. The list itself contained the exact same content, just a different ordering.

Those includes are symlinks, and they are generated from a genrule. This is all taken care of by https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L915-L1035 which generated shell script for the genrules, that actually do perform the symlinks. Checking those shell scripts revealed the exact same and different ordering.

Checking more carefully, one will see that the headers are discovered by `_read_dir` function: https://github.com/tensorflow/tensorflow/blob/ba64f5334d4bba31d22c30e09a96f806ea0e2f7e/third_party/gpus/cuda_configure.bzl#L891-L894, it does directly get the output of `find`. This is dependant on the ordering provided by `readdir` syscall.

In our case, the ordering on the filesystem before making the tar archive, and after untarring it would be different.

One simple fix for that is to force ordering the list of headers, this way we are sure the order is always the same and we are not dependant on what `readdir` is going to get us.

In the past, Bazel would force the ordering of the elements considered to compute the actionKey. This was removed with 0.3.0 but it might have make the issue hidden https://github.com/bazelbuild/bazel/commit/9dc32111d5b6c1c7c5eaf39efad5fef75327ee75"
16584,TensorFlow op to copy weights of Keras model,"I am doing a distributed calibration of an LSTM model (keras 2.0 + TensorFlow 1.0)

    with tf.device(tf.train.replica_device_setter(...):
          model = ##create model by keras
          clone_model = ## create the same model by keras but now a stateful one

after calibration, I want my chief worker to use the clone_model, copy the weights the calibration reached in model, and make predictions on some test set, but simply calling

     clone_model.set_weights(model.get_weights())

does not work.
I understand I need to define this weight copy as an op and then call session(run) of that op

Can you please help with a TensorFlow op copying weights of a keras model to another (identical architecture) Keras model?

"
16581,TFLite : Slice operation while using tf.nn.conv2d,"input = tf.get_variable(""input"",dtype = tf.float32,shape=(1,256,256,3))
kernel = tf.get_variable(""kernel"",initializer=[[[[ 0.0,  1,  2],[ 3,  4,  5],[ 6,  7,  8]],[[ 9, 10, 11],[12, 13, 14],[15, 16, 17]]],[[[18, 19, 20],[21, 22, 23],[24, 25, 26]],[[27, 28, 29],[30, 31, 32],[33, 34, 35]]]],dtype=tf.float32)
A = tf.nn.conv2d(input=input,filter=kernel,strides=[1,1,1,1],padding=""SAME"")
A = tf.nn.softmax(A)
output = tf.add(A,1,name=""output"")

I am trying to convert a simple model to tflite. However I am hitting. 
**If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Slice.
Abort trap: 6**

Does conv2d internally calls Slice"
16578,Different result between python and C++ with same pb file and same data.,"I got different result between python and C++ using same pb file and same data, I print result below, the fisrt 10000 number's diff is close to zero, but it's get bigger then.

|line|C++ result|Python result|diff|sum diff|
|----|:----:|:----:|:----:|:----:|
|0|-0.010966|-0.010966|-0.000000|-0.000000|
|1|-0.050666|-0.050666|0.000000|0.000000|
|2|-0.007573|-0.007573|0.000000|0.000000|
|3|-0.498266|-0.498266|0.000000|0.000000|
|4|-0.079290|-0.079290|-0.000000|0.000000|
|5|0.044778|0.044778|-0.000000|0.000000|
|6|0.003472|0.003472|-0.000000|0.000000|
|7|-0.542518|-0.542518|-0.000000|-0.000000|
|8|-0.087951|-0.087951|0.000000|-0.000000|
|9|-0.035723|-0.035723|0.000000|-0.000000|
|10000|-0.041655|-0.041655|0.000001|-0.000050|
|10001|0.059743|0.059742|0.000001|-0.000049|
|10002|0.003410|0.003410|-0.000000|-0.000049|
|10003|-0.671292|-0.671294|0.000001|-0.000048|
|10004|-0.117169|-0.117170|0.000001|-0.000047|
|10005|0.144822|0.144821|0.000001|-0.000045|
|10006|-0.001332|-0.001332|0.000000|-0.000045|
|10007|-0.795729|-0.795729|0.000001|-0.000045|
|10008|-0.058562|-0.058562|0.000000|-0.000045|
|10009|0.080222|0.080223|-0.000000|-0.000045|
|40000|-0.056272|0.261853|-0.318125|83.999641|
|40001|0.001396|0.006732|-0.005336|83.994308|
|40002|0.000540|-0.004921|0.005461|83.999771|
|40003|-0.601443|-0.382969|-0.218474|83.781296|
|40004|-0.088934|0.222222|-0.311156|83.470139|
|40005|0.048725|0.100866|-0.052141|83.417999|
|40006|-0.002078|-0.011184|0.009106|83.427109|
|40007|-0.652765|-0.507083|-0.145682|83.281425|
|40008|-0.079999|0.267384|-0.347384|82.934044|
|40009|-0.027125|0.091063|-0.118188|82.815857|
|64470|-0.001395|-0.003402|0.002008|673.607727|
|64471|-0.001808|-0.013858|0.012051|673.619751|
|64472|0.003523|-0.001226|0.004749|673.624512|
|64473|0.001785|0.015721|-0.013935|673.610596|
|64474|0.001010|-0.002219|0.003228|673.613831|
|64475|-0.000174|-0.000121|-0.000053|673.613770|
|64476|0.002843|0.005779|-0.002935|673.610840|
|64477|0.002199|-0.007970|0.010169|673.621033|
|64478|-0.002163|0.000151|-0.002314|673.618713|
|64479|0.018321|0.215750|-0.197428|673.421265|"
16576,Feature request: tf.estimator hyperparameter tuning,"I'm making an issue here because I'm sure this is being worked on somewhere in this huge repo and I've failed to find it by search.

Almost all models have hyperparameters that cannot be set by gradient descent (number of layers, for example). These needs to be tuned, preferably programatically with a smart strategy.

**What's the canonical way of doing hyperparameter tuning with the tf.estimator API?**

(also, how can we do early stopping with tf.estimator?)

I'm currently wrapping around scikit-optimize which is ok, but then I'll never be able to run parallel experiments across workers, and it's a bit tricky to know if the hyperparameters will lead to OOM aside from using tf.profile.

```python
import os

from skopt import gp_minimize
from skopt.space import Real, Categorical, Integer
from skopt.utils import use_named_args

logdir = 'tensorboard/'
space = [
    Real(0.0, 0.1, name='learning_rate'),
    Categorical([True, False], name='skip_connections'),
    Integer(1, 9, name='layers')]


@use_named_args(space)
def score(**params):
    model_dir = os.path.join(logdir, str(params))
    estimator = tf.estimator.Estimator(model_fn, model_dir, params=params)
    trainspec = tf.estimator.TrainSpec(train_input_fn)
    evalspec = tf.estimator.EvalSpec(eval_input_fn)
    try:
        tf.estimator.train_and_evaluate(estimator, trainspec, evalspec)
        metrics = estimator.evaluate(test_input_fn)
        return metrics['loss']
    except (tf.errors.ResourceExhaustedError, tf.train.NanLossDuringTrainingError):
        return 1e9


gp_minimize(score, space)
```"
16575,"There's no problem running on window, and there's a problem on Ubuntu","Traceback (most recent call last):

  File ""<ipython-input-1-fc398a1d9321>"", line 1, in <module>
    runfile('/home/lab326/songpeng/anacoda项目/tflearn-vgg1.py', wdir='/home/lab326/songpeng/anacoda项目')

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/lab326/songpeng/anacoda项目/tflearn-vgg1.py"", line 59, in <module>
    files_extension=['.jpg'], filter_channel=True)

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py"", line 512, in image_preloader
    flags=files_extension, filter_channel=filter_channel)

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py"", line 732, in directory_to_samples
    classes = sorted(os.walk(directory).__next__()[1])

StopIteration

There's no problem running on window, and there's a problem on Ubuntu

code:
from tflearn.data_utils import image_preloader
data_dir = ""/home/songpeng/dataset""
X, Y = image_preloader(data_dir, image_shape=(224, 224), mode='folder',
                       categorical_labels=True, normalize=True,
                       files_extension=['.jpg'], filter_channel=True)


"
16574,tf.contrib.metrics.streaming_precision raise an exception Attempting to use uninitialized value model/precision/true_positives/count,"Please go to Stack Overflow for help and support:



### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16572,tf.dynamic_placeholder gives inconsistent results in Tensorflow 1.5,"I'm seeing inconsistencies when calling tf.dynamic_partition(...) with Tensorflow 1.5.

I'm using tf.dynamic_partition(...) to select rows from two tensors. First is a 1D tensor of spare logits, second is a 2D tensor of one-hot encoded labels. My selector is a 1D tensor with entries made of 0s and 1s.

Say `matches_mask` has shape (268800,) and 320 of its elements are set to 1, rest are 0.
`scalar_labels` has shape(268800,) and `one_hot_encoded_logits` has shape (268800, 2).

The expected behaviour for

```python
_, selected_scalar_labels = tf.dynamic_partition(scalar_labels, matches_mask, num_partitions=2)
_, selected_one_hot_encoded_logits = tf.dynamic_partition(one_hot_encoded_logits, matches_mask, num_partitions=2)

```

is that `selected_scalar_labels` has shape (320, ) and `selected_one_hot_encoded_logits` has shape (320, 2).

This code works perfectly fine on two machines, one running Ubuntu 16.4, Tensorflow 1.3 and Tensorflow 1.4 (tested both versions) and CUDA 8 and the other running Ubuntu 16.4, Tensorflow 1.4 and CUDA 8.
However today I upgraded first machine to Ubuntu 16.4, Tensorflow 1.5 and CUDA 9 and above breaks.
For some reason while `selected_scalar_labels` are as expected, `selected_one_hot_encoded_logits` have some strange shapes, such as (268352, 2).

I tried writing a minimal test script, such as 

```python
labels_placeholder = tf.placeholder(dtype=tf.int32, shape=[None])
logits_placeholder = tf.placeholder(dtype=tf.float32, shape=[None, 2])

mask_placeholder = tf.placeholder(dtype=tf.int32, shape=[None])

_, selected_labels_op = tf.dynamic_partition(labels_placeholder, mask_placeholder, num_partitions=2)
_, selected_logits_op = tf.dynamic_partition(logits_placeholder, mask_placeholder, num_partitions=2)

with tf.Session() as session:

    for _ in range(100):

        print()

        size = 268800

        labels = np.random.randint(0, 2, size=size)
        logits = np.random.uniform(0, 1, size=(size, 2))

        mask = np.random.binomial(1, np.random.uniform(0, 1), size)

        feed_dictionary = {labels_placeholder: labels, logits_placeholder: logits, mask_placeholder: mask}

        selected_labels, selected_logits = session.run([selected_labels_op, selected_logits_op], feed_dictionary)

        print(""selected_labels: {}"".format(selected_labels.shape))
        print(""selected_logits: {}"".format(selected_logits.shape))
```

but that gave correct results.
Problem occurs in a more complicated pipeline that is a part of Single Shot Detector's loss computations.
Unfortunately computing `matches_mask` is a complex task that depends on image being used on input and SSD configuration, so I can't really provide here a complete test case without posting a few hundreds lines of code that isn't for public use + images. `matches_mask` itself is very simple though, just a 1D tensor of 0s and 1s.

I appreciate that without presenting a test case that fails it's unlikely anyone will look into this problem, but I decided to post the issue in case someone else sees similar behaviour - from my point of view it seems to be a new bug introduced by Tensorflow 1.5 or its dependencies.

I have of course double checked that my`matches_mask`, `scalar_labels` and `one_hot_encoded_logits` have expected dimensions and dtypes before `dynamic_partition` is called and that `matches_mask` is made of 0s and 1s only.

Environments on which code works as expected:
1) Ubuntu 16.04, tensorflow 1.3 installed with `pip install tensorflow-gpu`, CUDA 8.0, cuDNN 6.0, GPU: Tesla V100
2) Ubuntu 16.04, tensorflow 1.4 installed with `pip install tensorflow-gpu`, CUDA 8.0, cuDNN 6.0, GPU: GTX 1080 Ti
	tf.GIT_VERSION: v1.4.0-19-ga52c8d9
	tf.VERSION: 1.4.1
3) Ubuntu 16.04, tensorflow 1.4 installed with `pip install tensorflow-gpu`, CUDA 8.0, cuDNN 6.0, GPU: Tesla V100
	tf.GIT_VERSION: v1.4.0-rc1-11-g130a514
	tf.VERSION: 1.4.0

Environments on which code fails:
4) Ubuntu 16.04, tensorflow 1.5 installed with `pip install tensorflow-gpu`, CUDA 9.0, cuDNN 7.0, GPU: Tesla V100
	tf.GIT_VERSION: v1.5.0-0-g37aa430d84
	tf.VERSION: 1.5.0

Environments 1, 3 and 4 are the same machine, the only differences are in Tensorflow and CUDA versions and dependencies they install."
16567,"Protobuf 3.5 currently need golang and asm compiler, how can I add that to CI environemnt?","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Please refer to the pull request on https://github.com/tensorflow/tensorflow/pull/16480
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10/ Jenkins
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: MSVC 2015
- **CUDA/cuDNN version**: CUDA 9.0 with CUDNN 7
- **GPU model and memory**:
- **Exact command to reproduce**: by pull request

### Describe the problem
I want to update the cmake build with protobuf 3.5.1.1 with grpc support. However the latest grpc requires golang and asm compiler. Build fail on Jenkin's environment

### Source code / logs
https://source.cloud.google.com/results/invocations/76cb8643-d30a-45d6-9536-5372feb88e6d/log
"
16565,Feature Request: Separated Name Spaces of RNN Cells for hidden weights and recurrent states,"This feature request is based on the following question of StackOverflow, if it's not appropriate I will close this issue.

https://stackoverflow.com/questions/48506422/indices-and-slicing-of-tensorflows-global-variables-of-kernels-for-hidden-weigh

    placeholders = {""inputs"":tf.placeholder(tf.float32, shape=[None, None, 1000])}
    cell = tf.nn.rnn_cell.BasicLSTMCell(80)
    outs, states = tf.nn.dynamic_rnn(cell=cell, inputs=placeholders[""inputs""], dtype=tf.float32)

This graph building gives us tf.global_variables() list as follows:

    [<tf.Variable 'rnn/basic_lstm_cell/kernel:0' shape=(1080, 320) dtype=float32_ref>, <tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(320,) dtype=float32_ref>]

The proposed new feature separates two name spaces of ""kernel"": hidden weights and recurrent states
The expected tf.gloabal_variables() list with new feature is as follows:

     [<tf.Variable 'rnn/basic_lstm_cell/kernel/hidden_weights:0' shape=(1000, 320) dtype=float32_ref>,<tf.Variable 'rnn/basic_lstm_cell/kernel/recurrent_state:0' shape=(80, 320) dtype=float32_ref>,<tf.Variable 'rnn/basic_lstm_cell/bias:0' shape=(320,) dtype=float32_ref>]

In the new feature, a kernel name space has two sub spaces.
rnn/basic_lstm_cell/kernel/hidden_weights
rnn/basic_lstm_cell/kernel/recurrent_state

"
16561,Parse toco generated file (.tflite) in python?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.5.4
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem

I am using toco to optimize a frozen model (.pb). How do I read the .tflite file in python - something similar to tf.gfile.GFile('frozen.pb', 'rb')?
"
16556,tf.argmax appears to be functioning incorrectly on occasion,"**EDIT**

[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
Manjaro 17.1.3 Kernel 4.14

- **TensorFlow installed from (source or binary)**:
python pip

- **TensorFlow version (use command below)**:
tensorflow-gpu 1.5.0

- **Python version**:
3.6.4

- **CUDA/cuDNN version**:
CUDA 9.0
cuDNN 7.0

- **GPU model and memory**: 
Nvidia GeForce GTX 1050 8GB

### Describe the problem
tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.

### Source code / logs
To debug this, I have printed out the following operations:

```
print(self.session.run(
    tf.equal(tf.argmax(self.predictions, axis=-1),
             tf.argmax(self.labelsUnrolled, axis=-1)),
    self.batchDict))
print("""")
print(self.session.run(self.predictions, self.batchDict))
print("""")
print(self.session.run(self.labelsUnrolled, self.batchDict))
print(""\n********\n"")
```

Which on two consecutive iterations output the following:

```
[[ True  True  True]
 [ True  True  True]]

[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],
        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],
        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), 
array([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],
        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],
        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********

[[False False  True]
 [ True  True  True]]

 [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],
         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],
         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),
 array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],
         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],
         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 0., 1., 0.]], dtype=float32), 
 array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]

********
```

Isn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?

Printing out the same operations, but not inside the session gives the following shapes:

```
Tensor(""Equal_175:0"", shape=(2, ?), dtype=bool)

[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]

[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]
```

Is it a problem that the number associated with the tensor name ""Equal_XXX:0"" is incrementing each iteration?

I have also tried changing the axis argument in both argmax functions to `axis=2`, giving the ""Equal"" tensor a shape of (2, 3) again, but there are still similar errors.

Here is an example:

```
[[False False  True]
 [ True  True False]]

[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],
        [0.04962843, 0.43777955, 0.46654516, 0.04604685],
        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),
 array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],
        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],
        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [1., 0., 0., 0.],
        [0., 0., 1., 0.]], dtype=float32),
 array([[0., 1., 0., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

I would expect this to be:

```
[[ True False  True]
 [ True False False]]
```

I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:

```
[[False  True False]
 [False False False]]

[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],
        [0.04910654, 0.44086066, 0.46013904, 0.04989377],
        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), 
array([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],
        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],
        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]

[array([[0., 0., 1., 0.],
        [0., 0., 1., 0.],
        [0., 1., 0., 0.]], dtype=float32),
array([[0., 1., 0., 0.],
        [0., 1., 0., 0.],
        [0., 1., 0., 0.]], dtype=float32)]
```

"
16555,LSTMBlockFusedCell#call no longer supports list of tensors,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Unbuntu
- **TensorFlow installed from (source or binary)**: soruce
- **TensorFlow version (use command below)**: 'v1.5.0-0-g37aa430', '1.5.0'
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: Build label: 0.9.0

### Describe the problem
[Documentation](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMBlockFusedCell#call) says, ""inputs: 3-D tensor with shape [time_len, batch_size, input_size] or a list of time_len tensors of shape [batch_size, input_size]"". However, as of 1.5.0, this call only works with a 3-D tensor. 

### Source code / logs
When attempting to run with a list of 2-D tensors, I get the following:
```
...
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 629, in __call__
    self._assert_input_compatibility(inputs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py"", line 1112, in _assert_input_compatibility
    ' input tensors. Inputs received: ' + str(inputs))
ValueError: Layer rnn/lstm_cell expects 1 inputs, but it received 34 input tensors. Inputs received: ... 
```
"
16553,Any way to pass a feedable placeholder for num_spectrogram_bins  in linear_to_mel_weight_matrix?,"I need to run `auido->spectrogram->mel` graph for three different frame sizes. Ideally, I would have `frame_size` input tensor of the `stft` function be a placeholder and feed it in a loop. However, when computing the mel scale, tf does a value check for the number of bins, which according to my understanding, means that the number of bins needs to be specified during the graph construction.
Any suggestions?

Below is the link to the problematic code
https://github.com/tensorflow/tensorflow/blob/f7cbb757cf51143a1c7d9db5a812ac165941adf4/tensorflow/contrib/signal/python/ops/mel_ops.py#L72

**Edit**: I am referencing the official tutorial.
https://www.tensorflow.org/api_guides/python/contrib.signal#Computing_log_mel_spectrograms
The very first line essentially begins the problem chain. 

**Edit**: I just commented out the line below that initiates the value checks, and everything ran smoothly. I was able to feed in `frame_length`. 
https://github.com/tensorflow/tensorflow/blob/f7cbb757cf51143a1c7d9db5a812ac165941adf4/tensorflow/contrib/signal/python/ops/mel_ops.py#L151
"
16552,Sampled softmax loss stops gradients on sampled classes,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
Binary
- **TensorFlow version (use command below)**:
1.3
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA 8 / cuDNN 6
- **GPU model and memory**:
4 x TITAN X (Pascal)

### Describe the problem

The backbone of TensorFlow's sampled loss functions `nce_loss` and `sampled_softmax_loss` is a helper function called `_compute_sampled_logits`https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L961-L1139.

`_compute_sampled_logits` takes as input:

- weights and biases of the final layer,
- the output labels
- the inputs to the final layer inputs
- the sampled values of the output layer
- a few other things

and returns the logits and labels of only the requested sampled labels.

One of the first ops executed is https://github.com/tensorflow/tensorflow/blob/8fb12848d3a81a010714a4612ffd735106ea83d8/tensorflow/python/ops/nn_impl.py#L1046-L1047 

This line seems like it is stopping gradients flowing back through the sampled values if i'm reading it correctly.

Shouldn't the gradients be stopped from flowing back through the _non-sampled values_ as opposed to the _sampled values_? Why are gradients being stopped at the sampled values? 
"
16548,AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.13.2
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0
- **Python version**: 3.6.3
- **GPU model and memory**: no GPU
- **Exact command to reproduce**:

```python
class my_RNNCell(tf.nn.rnn_cell.RNNCell):
    def __init__(self):
        super(my_RNNCell, self).__init__()
        self._output_size = 2
        self._state_size = 2

    def __call__(self, tensor_in, state):

        output = tf.layers.conv_2d(tensor_in, 1, [1, 10])

        return output, output
    
    @property
    def output_size(self):
        return self._output_size
    @property
    def state_size(self):
        return self._state_size


tf.nn.dynamic_rnn(my_RNNCell(), inputs=tf.placeholder(shape=[None,2,100,3], dtype=tf.float32), initial_state= tf.placeholder(shape=[None,2], dtype=tf.float32))

>>>
AttributeError: module 'tensorflow.python.layers.layers' has no attribute 'conv_2d'
```
"
16545,Graph transform : fold_constant issue since tf v 1.5,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux centos 7
- **TensorFlow installed from (source or binary)**: with pip
- **TensorFlow version (use command below)**: 1.5.0 (vs 1.4.1)
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: No
- **GCC/Compiler version (if compiling from source)**: No
- **CUDA/cuDNN version**: No
- **GPU model and memory**: No
- **Exact command to reproduce**:

Clone the tensorflow/models repo.
then 
```
cd models/research/slim/
python export_inference_graph.py \
  --model_name=inception_v1 \
  --image_size=224 \
  --output_file=/tmp/inception_v1.pb
```
Then freeze the graph : 
```
cd tensorflow/tensorflow/python/tools/
python freeze_graph.py \
--input_graph /tmp/inception_v1.pb \
--input_checkpoint /tmp/inception_v1.ckpt \
--output_graph /tmp/inception_v1_frozen.pb \
--input_binary True \
--output_node_names ""InceptionV1/Logits/Predictions/Reshape_1""
```

Then in python do : 
```
import tensorflow as tf
from tensorflow.core.framework import graph_pb2
from tensorflow.python.platform import gfile
import tensorflow.tools.graph_transforms as graph_transforms
graph = graph_pb2.GraphDef()
with open(""/tmp/inception_v1_frozen.pb"", 'rb') as f:
    s = f.read()
    graph.ParseFromString(s)
graph = graph_transforms.TransformGraph(graph,
            [""input""], # inputs nodes
            [""InceptionV1/Logits/Predictions/Reshape_1""], # outputs nodes
            ['fold_constants()'])
with gfile.FastGFile(""/tmp/inception_v1_frozen""+""_optimized.pbtxt"", ""w"") as f:
    f.write(str(graph))
```

### Describe the problem
I am using the graph_transform to fold constants in by graph saved as .pb.
When I use the fold_constants() transformation, some inputs of some nodes are renamed but not the corresponding nodes in the whole graph. So the graph is no longer valid... 

I have an ""input"" placeholder in the graph.
And the node connected to this placeholder as an input name ""input:0"" instead of ""input"".

With the version 1.4.1 of tensorflow, I didn't have this issue.

To reproduce, follow the instructions below, and take a look to the /tmp/inception_v1_frozen_optimized.pbtxt graph. And search le node named ""input"" it is the input placeholder. Then search node which has an input named ""input:0"". This name (""input:0"") is node a node of the graph."
16544,"deconv_output_length(input_length, filter_size, padding, stride)","deconv_output_length(input_length, filter_size, padding, stride) is defined [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/layers/utils.py#L159)

When padding is valid, input_length += max(filter_size - stride, 0), why to use `max` function?
See more details on #2118

For conv2d:
```
output = (input - filter + stride) // stride  # VALID
output = (input + stride - 1) // stride  # SAME
```

For conv2d_transpose:
```
output = input * stride + filter - stride  # VALID
output = input * stride - stride + 1  # SAME 
```

Even when filter_size is less than stride, I think output is also `input * stride + filter - stride` rather `input * stride`, so why to use `max`?

 
"
16542,Cannot merge devices with incompatible jobs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04(kernel 4.10)
- **TensorFlow installed from (source or binary)**: source code
- **TensorFlow version (use command below)**: r1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: No CUDA
- **GPU model and memory**: No GPU
- **Exact command to reproduce**: var_rep = tf.Variable(var_concat, name=var_name, collections=[tf.GraphKeys.LOCAL_VARIABLES], trainable=False)

### Describe the problem
I am running distributed tensorflow with 2 parameter servers and 2 workers. I created some partioned variables by using ""tf.create_partitioned_variables()"". So the different parts of a variable will be placed on different ps. Then I want to concatenate them by using ""tf.concat()"" and assign the result of tf.concat() to a untrainable variable which lies in worker. But the error occurs as follows:

```
Traceback (most recent call last):
  File ""distributed_vgg19.py"", line 230, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""distributed_vgg19.py"", line 188, in main
    sess = sv.prepare_or_wait_for_session(server.target, config=sess_config)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py"", line 726, in prepare_or_wait_for_session
    max_wait_secs=max_wait_secs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 400, in wait_for_session
    sess)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 483, in _try_run_local_init_op
    sess.run(self._local_init_op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0' and 'fc8/fc8_biases/part_1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'
         [[Node: fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0 = Identity[T=DT_FLOAT, _class=[""loc:@fc8/fc8_biases/part_1""], _device=""/job:worker/task:1""](fc8/fc8_biases_1/cond_1/Merge)]]

Caused by op u'fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0', defined at:
  File ""distributed_vgg19.py"", line 230, in <module>
    tf.app.run()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""distributed_vgg19.py"", line 136, in main
    vgg.build(x, train_mode)
  File ""/home/shuai/distributed_vgg19_part/vgg19.py"", line 96, in build
    self.fc8 = self.fc_layer(self.relu7, 4096, 1000, ""fc8"")
  File ""/home/shuai/distributed_vgg19_part/vgg19.py"", line 120, in fc_layer
    weights, biases = self.get_fc_var(in_size, out_size, name)
  File ""/home/shuai/distributed_vgg19_part/vgg19.py"", line 143, in get_fc_var
    biases = self.get_var(initial_value, name, 1, name + ""_biases"")
  File ""/home/shuai/distributed_vgg19_part/vgg19.py"", line 171, in get_var
    var_rep = tf.Variable(var_concat, name=var_name, collections=[tf.GraphKeys.LOCAL_VARIABLES], trainable=False)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 229, in __init__
    constraint=constraint)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 378, in _init_from_args
    self._build_initializer_expr(self._initial_value),
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 811, in _build_initializer_expr
    new_op = self._build_initializer_expr(initial_value.op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 830, in _build_initializer_expr
    new_tensor = self._build_initializer_expr(tensor)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 811, in _build_initializer_expr
    new_op = self._build_initializer_expr(initial_value.op)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variables.py"", line 847, in _build_initializer_expr
    attrs=initial_value.node_def.attr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3042, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1521, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): Cannot colocate nodes 'fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0' and 'fc8/fc8_biases/part_1: Cannot merge devices with incompatible jobs: '/job:ps/task:0' and '/job:worker/task:1'
         [[Node: fc8/fc8_biases_1/fc8/fc8_biases/part_1/read_fc8/fc8_biases_1_0 = Identity[T=DT_FLOAT, _class=[""loc:@fc8/fc8_biases/part_1""], _device=""/job:worker/task:1""](fc8/fc8_biases_1/cond_1/Merge)]]
```

there are some relevant code:
```
            slice_list = []
            for dim_index in range(value.get_shape().ndims):
                if dim_index == 0:
                    slice_list.append(self.num_of_ps)
                else:
                    slice_list.append(1)
            var_list = tf.create_partitioned_variables(shape=value.get_shape(), 
                                                       slicing=slice_list, 
                                                       initializer=value,
                                                       name=var_name)
            var_concat = var_list[0]
            for ps_index in range(self.num_of_ps - 1):
                var_concat = tf.concat([var_concat, var_list[ps_index + 1]], 0)
            with tf.device('/job:worker/task:%d' % self.task_index):
                var = tf.Variable(var_concat, name=var_name, collections=[tf.GraphKeys.LOCAL_VARIABLES], trainable=False)
```

Anybody knows why this error happened? Is it caused by an bug? If not, what i can do to assign the value of a variable lying in ps to another variable lying in worker?
Thanks a lot!"
16541,Looking for a way to compile TF without SSE.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
NO.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Slackware 14.2 (32bit)

- **TensorFlow installed from (source or binary)**:
source

- **TensorFlow version (use command below)**:
1.5.0

- **Python version**: 
3.6.4

- **Bazel version (if compiling from source)**:
0.5.4

- **GCC/Compiler version (if compiling from source)**:
5.3.0

- **CUDA/cuDNN version**:
N/A

- **GPU model and memory**:
N/A

- **Exact command to reproduce**:
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

#### Problem
 Pardon me first to bother you. This is neither about a bug, nor a feature request. This is about the 32-bit architecture, and I know that TF do not officially support 32-bit machines.

I also use decent NVIDIA workstations at my lab. Thank you for your work. However, sometimes, I open my old laptop to write some code, and it is a 32-bit machine.

So far, (up to TF 1.4.1), I could somehow manage to compile the source code to get the things right. But, in TF 1.5, I receive next error messages here and there:

""The TensorFlow library was compiled to use "" << SSE
"" instructions, but these aren't available on your machine.""


I think that if `__SSE__` flag is disabled, then TF 1.5 can be available to the 32-bit architecuture, so I append -march=i686 to the configure script and bazel argument, but it's no worth. I'm trying to figure out what turns on SSE flag, but still I got no effort.

Currently I add to the bazel command this:

bazel build --config=opt --copt=-march=""i686"" //tensorflow/tools/pip_package:build_pip_package




### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16540,//tensorflow/python:nn_test fails with AssertionError,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 s390x
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:nn_test 

### Describe the problem
While running `nn_test`, 2 sub tests (`testOutput2DInput01`,`testOutput4DInput123`), fail on s390x with : 
```
AssertionError:
Not equal to tolerance rtol=0.0001, atol=0.0001
None
(mismatch 100.0%)
 x: array([[ 0.00111]], dtype=float32)
 y: array([[ 0.000837]])
```

The test passes if the [tolerance](https://github.com/tensorflow/tensorflow/blob/v1.4.1/tensorflow/python/ops/nn_test.py#L865) is slightly increased:
```
-  def doOutputTest(self, input_shape, moments_axes, tol=1e-4,
+  def doOutputTest(self, input_shape, moments_axes, tol=4e-4,
```

Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
======================================================================
```
FAIL: testOutput2DInput01 (__main__.MomentsTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py"", line 911, in testOutput2DInput01
    self.doOutputTest((10, 300), (0, 1))
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/ops/nn_test.py"", line 896, in doOutputTest
    self.assertAllClose(variance, expected_var, rtol=tol, atol=tol)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 972, in assertAllClose
    self._assertArrayLikeAllClose(a, b, rtol=rtol, atol=atol)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/nn_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 942, in _assertArrayLikeAllClose
    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol, err_msg=msg)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 1395, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 778, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=0.0001, atol=0.0001
None
(mismatch 100.0%)
 x: array([[ 0.00111]], dtype=float32)
 y: array([[ 0.000837]])

```
"
16539,How to use tensorflow library in c programs??,"I build shared libraries in tensorflow-serving with bazel.
However, I can not build C programs with the shared libraries.

I build C programs like this:
g++ main.cc -L../../bazel-bin/tensorflow_serving/rnnlm -lrnnlm_client_run -ldata_generator

Details is shown below:

../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::Stub::Predict(grpc::Cli
    entContext*, tensorflow::serving::PredictRequest const&, tensorflow::serving::PredictResponse*)'
 12 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::fixed_address_empty_string'
 13 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::CheckTypeAndIsAligned(tensorflow::DataType)
     const'
 14 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::_slow_mutable_model_spec()
    '
 15 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::FromProto(tensorflow::TensorProto const&)'
 16 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictResponse::PredictResponse()'
 17 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShape::CheckDimsEqual(int) const'
 18 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::Tensor::Tensor()'
 19 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictionService::NewStub(std::shared_ptr
    <grpc::ChannelInterface> const&, grpc::StubOptions const&)'
 20 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim(google::
    protobuf::Arena*)'
 21 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::TensorProto(google::protobuf::Arena*)'
 22 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::ArenaImpl::AllocateAligned(unsigned
     long)'
 23 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeBase<tensorflow::TensorShape>::dim_size(
    int) const'
 24 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::~PredictRequest()'
 25 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::internal::RepeatedPtrFieldBase::Reserve(int)'
 26 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorShapeProto_Dim::TensorShapeProto_Dim()'
 27 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::serving::PredictRequest::PredictRequest()'
 28 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `google::protobuf::Arena::OnArenaAllocation(std::type_info const
    *, unsigned long) const'
 29 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::~TensorProto()'
 30 ../../bazel-bin/tensorflow_serving/rnnlm/librnnlm_client_run.so: undefined reference to `tensorflow::TensorProto::_slow_mutable_tensor_shape()'"
16538,Keras ProgBar is not working properly anymore with TF 1.5.0,"If you run the following code with TF 1.4.*, this code works fine. However, if you run it with the newly released TF 1.5.0, the output is abnormal:

```python
import time
import tensorflow as tf

progress_bar = tf.contrib.keras.utils.Progbar(target=600)

for _ in range(600):
    progress_bar.update(_)
    time.sleep(0.01)
```

#### Screenshot Effect:

##### In Jupyter NB:
![image](https://user-images.githubusercontent.com/10923599/35502790-65112c36-04de-11e8-980f-59f1bc375d60.png)"
16536,Feature Request: global average pooling layer in tf.layers,"Hello,

Can we add an implementation of global_average_pooling to tf.layers or tf.contrib.layers? It can look much like the Keras implementation here: https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool3D
and essentially just requires calling tf.reduce_mean.

I suspect lots of people have written functions called global_pooling that just call reduce mean, and it would be nice to have a tf.layers function that just does this for consistency/readability.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: AWS Deep Learning AMI (Conda)
- **TensorFlow version (use command below)**: 1.5
- **Python version**:  3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16535,only merge specific kind of summary,"Currently, I need to gitve `tf.summary.merge` an input arg to specify what summaries to merge. 
But it will be more convinient if there is a function called `tf.summary.merge_scalar` / `tf.summary.merge_image` cause scalars and images are often logged with different frequencies, and they need to be merged seperately."
16534,warning with tf.losses.softmax_cross_entropy,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows and Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9/7
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
tf.losses.softmax_cross_entropy calls tf.nn.softmax_cross_entropy_with_logits, in which there is a warning. It's better also provide tf.losses.softmax_cross_entropy_v2 to call tf.nn.softmax_cross_entropy_with_logits_v2.
"
16532,Keras/TF1.5.0 does not consider TensorBoard embeddings_freq/embeddings_metadata,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) describe in following**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.5.0**:
- **Python version 3.5.1**: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version NOT USED**:
- **GPU model and memory NOT USED**:
- **Exact command to reproduce python imdb_fasttext.py**:

### Describe the problem
TF1.5.0/Keras does not consider TensorBoard embeddings_freq/embeddings_metadata.

This parameter is added in 10 month ago and removed in 5 month ago.
And currently it does not exist 
-  Current code
    https://github.com/tensorflow/tensorflow/blame/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L606
-  Apr 12, 2017 (10month ago) (add argument)
    https://github.com/tensorflow/tensorflow/commit/b8b8ebcf851df71ebb5209ae27d75e2befc50f0d
- Sep 6, 2017 (5month ago) (remove argument)
    https://github.com/tensorflow/tensorflow/commit/eaaa0b93852054dee086a3ed5373cf8bbe3d2fb3

Which is already supported on Keras 2.1.3.
https://github.com/keras-team/keras/blame/7d1e0bc5872855af5bf35a725025d3bdb6f07d6c/keras/callbacks.py#L641

### Source code / logs
Current outputs are following. It reports the keyword ""embeddings_metadata"" does not exist. 
```
C:\Users\sakaia\work\tensorflow\keras>python imdb_fasttext.py
Using TensorFlow backend.
Loading data...
25000 train sequences
25000 test sequences
Average train sequence length: 238
Average test sequence length: 230
Pad sequences (samples x time)
x_train shape: (25000, 400)
x_test shape: (25000, 400)
Build model...
WARNING:tensorflow:From C:\Program Files\Python35\lib\site-packages\tensorflow\p
ython\keras\_impl\keras\backend.py:1557: calling reduce_mean (from tensorflow.py
thon.ops.math_ops) with keep_dims is deprecated and will be removed in a future
version.
Instructions for updating:
keep_dims is deprecated, use keepdims instead
Traceback (most recent call last):
  File ""imdb_fasttext.py"", line 146, in <module>
    embeddings_metadata= embeddingsMetadata
TypeError: __init__() got an unexpected keyword argument 'embeddings_metadata'
```

Source code is follows (based on https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py)
```Python
'''This example demonstrates the use of fasttext for text classification

Based on Joulin et al's paper:

Bags of Tricks for Efficient Text Classification
https://arxiv.org/abs/1607.01759

Results on IMDB datasets with uni and bi-gram embeddings:
    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.
    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.
'''

from __future__ import print_function
import numpy as np

# Changed from keras to tensorflow.python.keras
from tensorflow.python.keras.preprocessing import sequence
from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import Embedding
from tensorflow.python.keras.layers import GlobalAveragePooling1D
from tensorflow.python.keras.callbacks import TensorBoard
# Followings are workaround for https://github.com/tensorflow/tensorflow/issues/16358
from keras.datasets import imdb

def create_ngram_set(input_list, ngram_value=2):
    """"""
    Extract a set of n-grams from a list of integers.

    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)
    {(4, 9), (4, 1), (1, 4), (9, 4)}

    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)
    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]
    """"""
    return set(zip(*[input_list[i:] for i in range(ngram_value)]))


def add_ngram(sequences, token_indice, ngram_range=2):
    """"""
    Augment the input list of list (sequences) by appending n-grams values.

    Example: adding bi-gram
    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}
    >>> add_ngram(sequences, token_indice, ngram_range=2)
    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]

    Example: adding tri-gram
    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]
    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}
    >>> add_ngram(sequences, token_indice, ngram_range=3)
    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]
    """"""
    new_sequences = []
    for input_list in sequences:
        new_list = input_list[:]
        for i in range(len(new_list) - ngram_range + 1):
            for ngram_value in range(2, ngram_range + 1):
                ngram = tuple(new_list[i:i + ngram_value])
                if ngram in token_indice:
                    new_list.append(token_indice[ngram])
        new_sequences.append(new_list)

    return new_sequences

# Set parameters:
# ngram_range = 2 will add bi-grams features
ngram_range = 1
max_features = 20000
maxlen = 400
batch_size = 32
embedding_dims = 50
epochs = 5

print('Loading data...')
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
print(len(x_train), 'train sequences')
print(len(x_test), 'test sequences')
print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))
print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))

if ngram_range > 1:
    print('Adding {}-gram features'.format(ngram_range))
    # Create set of unique n-gram from the training set.
    ngram_set = set()
    for input_list in x_train:
        for i in range(2, ngram_range + 1):
            set_of_ngram = create_ngram_set(input_list, ngram_value=i)
            ngram_set.update(set_of_ngram)

    # Dictionary mapping n-gram token to a unique integer.
    # Integer values are greater than max_features in order
    # to avoid collision with existing features.
    start_index = max_features + 1
    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}
    indice_token = {token_indice[k]: k for k in token_indice}

    # max_features is the highest integer that could be found in the dataset.
    max_features = np.max(list(indice_token.keys())) + 1

    # Augmenting x_train and x_test with n-grams features
    x_train = add_ngram(x_train, token_indice, ngram_range)
    x_test = add_ngram(x_test, token_indice, ngram_range)
    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))
    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))

print('Pad sequences (samples x time)')
x_train = sequence.pad_sequences(x_train, maxlen=maxlen)
x_test = sequence.pad_sequences(x_test, maxlen=maxlen)
print('x_train shape:', x_train.shape)
print('x_test shape:', x_test.shape)

print('Build model...')
model = Sequential()

# we start off with an efficient embedding layer which maps
# our vocab indices into embedding_dims dimensions
model.add(Embedding(max_features,
                    embedding_dims,
                    input_length=maxlen))

# we add a GlobalAveragePooling1D, which will average the embeddings
# of all words in the document
model.add(GlobalAveragePooling1D())

# We project onto a single unit output layer, and squash it with a sigmoid:
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
embeddingsMetadata = {'embedding': 'metadata.tsv'}
model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          validation_data=(x_test, y_test),
# problem occured here!!!
          callbacks=[TensorBoard(log_dir=""."", histogram_freq=1, embeddings_freq=1,
                                 embeddings_metadata= embeddingsMetadata
                                ),
          ])
```
"
16531,Running problems when building a VGG model through tflearn on Ubuntu to train your own data,"
  File ""<ipython-input-15-fc398a1d9321>"", line 1, in <module>
    runfile('/home/lab326/songpeng/anacoda项目/tflearn-vgg1.py', wdir='/home/lab326/songpeng/anacoda项目')

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/spyder/utils/site/sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""/home/lab326/songpeng/anacoda项目/tflearn-vgg1.py"", line 59, in <module>
    files_extension=['.jpg'], filter_channel=True)

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py"", line 512, in image_preloader
    flags=files_extension, filter_channel=filter_channel)

  File ""/home/lab326/anaconda3/lib/python3.5/site-packages/tflearn/data_utils.py"", line 732, in directory_to_samples
    classes = sorted(os.walk(directory).__next__()[1])

StopIteration



from tflearn.data_utils import image_preloader

X, Y = image_preloader(files_list, image_shape=(224, 224), mode='folder',
                       categorical_labels=True, normalize=False,
                       files_extension=['.jpg'], filter_channel=True)"
16526,TF1.5 configure overwrites export TF_CUDA_VERSION=9.1,"It seems `configure` overwrites the BASH flag:
```bash
export TF_CUDA_VERSION=""9.1""
```
Here is the 1.5 branch of my [tensorflow.sh](https://github.com/ahundt/robotics_setup/blob/tf-1.5/tensorflow.sh) install script.

Here are the key code lines from the [tensorflow.sh specific hash to reproduce this error](https://github.com/ahundt/robotics_setup/blob/8a7940f74945f09d2a6bfc1a08d06f156a4c79f9/tensorflow.sh) install script,  and 161 at the time of writing:
```
    # line 134 at the time of writing
    export TF_NEED_CUDA=1
    export TF_CUDA_VERSION=""9.1""
    export TF_CUDNN_VERSION=7
    export CUDA_TOOLKIT_PATH=/usr/local/cuda-9.1/targets/x86_64-linux/lib/
    # ...snip... to line 161 at the time of writing
    yes """" | ./configure
```

I believe the problem is either in the `configure` script, [configure.py set_tf_cuda_version() ask_cuda_version](https://github.com/tensorflow/tensorflow/blob/r1.5/configure.py#L805) or in the call to [set_tf_cuda_version](https://github.com/tensorflow/tensorflow/blob/r1.5/configure.py#L1320), none of which seem to check the environment variable first and possibly overwrite it.

The result is the following error:
```
Invalid path to CUDA 9.0 toolkit. /usr/local/cuda/lib64/libcudart.so.9.0 cannot be found
Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]:

Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:

Invalid path to CUDA 9.0 toolkit. /usr/local/cuda/lib64/libcudart.so.9.0 cannot be found
Traceback (most recent call last):
  File ""configure.py"", line 1365, in <module>
    main()
  File ""configure.py"", line 1320, in main
    set_tf_cuda_version(environ_cp)
  File ""configure.py"", line 850, in set_tf_cuda_version
    _DEFAULT_PROMPT_ASK_ATTEMPTS)
__main__.UserInputError: Invalid TF_CUDA_SETTING setting was provided 10 times in a row. Assuming to be a scripting mistake.
-> [1]
ahundt@femur|~/src/robotics_setup on tf-1.5!?
± ls /usr/local/cuda/lib64/
stubs                  libcusparse.so.9.1     libnppist.so.9.1
libaccinj64.so         libcusparse.so.9.1.85  libnppist.so.9.1.85
libaccinj64.so.9.1     libcusparse_static.a   libnppist_static.a
libaccinj64.so.9.1.85  libnppc.so             libnppisu.so
libcublas_device.a     libnppc.so.9.1         libnppisu.so.9.1
libcublas.so           libnppc.so.9.1.85      libnppisu.so.9.1.85
libcublas.so.9.1       libnppc_static.a       libnppisu_static.a
libcublas.so.9.1.128   libnppial.so           libnppitc.so
libcublas.so.9.1.85    libnppial.so.9.1       libnppitc.so.9.1
libcublas_static.a     libnppial.so.9.1.85    libnppitc.so.9.1.85
libcudadevrt.a         libnppial_static.a     libnppitc_static.a
libcudart.so           libnppicc.so           libnpps.so
libcudart.so.9.1       libnppicc.so.9.1       libnpps.so.9.1
libcudart.so.9.1.85    libnppicc.so.9.1.85    libnpps.so.9.1.85
libcudart_static.a     libnppicc_static.a     libnpps_static.a
libcufft.so            libnppicom.so          libnvblas.so
libcufft.so.9.1        libnppicom.so.9.1      libnvblas.so.9.1
libcufft.so.9.1.85     libnppicom.so.9.1.85   libnvblas.so.9.1.128
libcufft_static.a      libnppicom_static.a    libnvblas.so.9.1.85
libcufftw.so           libnppidei.so          libnvgraph.so
libcufftw.so.9.1       libnppidei.so.9.1      libnvgraph.so.9.1
libcufftw.so.9.1.85    libnppidei.so.9.1.85   libnvgraph.so.9.1.85
libcufftw_static.a     libnppidei_static.a    libnvgraph_static.a
libcuinj64.so          libnppif.so            libnvrtc-builtins.so
libcuinj64.so.9.1      libnppif.so.9.1        libnvrtc-builtins.so.9.1
libcuinj64.so.9.1.85   libnppif.so.9.1.85     libnvrtc-builtins.so.9.1.85
libculibos.a           libnppif_static.a      libnvrtc.so
libcurand.so           libnppig.so            libnvrtc.so.9.1
libcurand.so.9.1       libnppig.so.9.1        libnvrtc.so.9.1.85
libcurand.so.9.1.85    libnppig.so.9.1.85     libnvToolsExt.so
libcurand_static.a     libnppig_static.a      libnvToolsExt.so.1
libcusolver.so         libnppim.so            libnvToolsExt.so.1.0.0
libcusolver.so.9.1     libnppim.so.9.1        libOpenCL.so
libcusolver.so.9.1.85  libnppim.so.9.1.85     libOpenCL.so.1
libcusolver_static.a   libnppim_static.a      libOpenCL.so.1.0
libcusparse.so         libnppist.so           libOpenCL.so.1.0.0
```"
16523,"Feature request: tf.Print should either print (not log), or accept a log level","### System information
- **Have I written custom code**: no
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: n/a
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

### Feature Request
Currently tf.Print produces INFO logs.
If log level is set to get rid of INFO logs (given the noise level), it renders tf.Print as non-functional.

It would be nice to have tf.Print work either by making it actually work like print(), i.e. it is not a mechanism to generate logs, or if it should remain that way, then have it accept a log level optional parameter."
16522,"TF 1.5 fails on Windows 10 with ""Security check failure or stack buffer overrun""","The error is reproducible on simple MNIST tutorial from https://www.tensorflow.org/tutorials/layers
I can upload the minidump, if necessary.

**TF version**
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'unknown' 1.5.0

**The TF log is**
Extracting MNIST-data\train-images-idx3-ubyte.gz
Extracting MNIST-data\train-labels-idx1-ubyte.gz
Extracting MNIST-data\t10k-images-idx3-ubyte.gz
Extracting MNIST-data\t10k-labels-idx1-ubyte.gz
INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_model_dir': '.\\mnist_convnet_model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001F02AEF5860>, '_task_type': 'worker', '_task_id': 0, '_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
INFO:tensorflow:Create CheckpointSaverHook.
2018-01-28 19:09:17.733213: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 770 major: 3 minor: 0 memoryClockRate(GHz): 1.1105
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.64GiB
2018-01-28 19:09:17.733455: I C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 770, pci bus id: 0000:01:00.0, compute capability: 3.0)
2018-01-28 19:09:19.732272: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2018-01-28 19:09:19.732452: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_dnn.cc:389] error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2018-01-28 19:09:19.732900: E C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\stream_executor\cuda\cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2018-01-28 19:09:19.733034: F C:\tf_jenkins\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\kernels\conv_ops.cc:717] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)

**Exception analysis from WinDbg**
(42f0.1198): Security check failure or stack buffer overrun - code c0000409 (!!! second chance !!!)
*** WARNING: Unable to verify checksum for C:\Users\nobody\Anaconda3\lib\site-packages\tensorflow\python\_pywrap_tensorflow_internal.pyd
*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\Users\nobody\Anaconda3\lib\site-packages\tensorflow\python\_pywrap_tensorflow_internal.pyd - 
ucrtbase!abort+0x4e:
00007ffc`d74eb70e cd29            int     29h
0:034> !analyze -v
*** WARNING: Unable to verify checksum for C:\Users\nobody\Anaconda3\python36.dll
*** WARNING: Unable to verify checksum for python.exe
*** WARNING: Unable to verify checksum for C:\Users\nobody\Anaconda3\lib\site-packages\numexpr\interpreter.cp36-win_amd64.pyd
*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\Users\nobody\Anaconda3\lib\site-packages\numexpr\interpreter.cp36-win_amd64.pyd - 
*** ERROR: Symbol file could not be found.  Defaulted to export symbols for C:\WINDOWS\SYSTEM32\nvcuda.dll - 
GetUrlPageData2 (WinHttp) failed: 12002.

KEY_VALUES_STRING: 1


TIMELINE_ANALYSIS: 1

Timeline: !analyze.Start
    Name: <blank>
    Time: 2018-01-28T17:19:28.120Z
    Diff: 120 mSec

Timeline: Dump.Current
    Name: <blank>
    Time: 2018-01-28T17:19:28.0Z
    Diff: 0 mSec

Timeline: Process.Start
    Name: <blank>
    Time: 2018-01-28T17:18:34.0Z
    Diff: 54000 mSec

Timeline: OS.Boot
    Name: <blank>
    Time: 2018-01-12T10:16:05.0Z
    Diff: 1407803000 mSec


DUMP_CLASS: 2

DUMP_QUALIFIER: 0

FAULTING_IP: 
ucrtbase!abort+4e
00007ffc`d74eb70e cd29            int     29h

EXCEPTION_RECORD:  (.exr -1)
ExceptionAddress: 00007ffcd74eb70e (ucrtbase!abort+0x000000000000004e)
   ExceptionCode: c0000409 (Security check failure or stack buffer overrun)
  ExceptionFlags: 00000001
NumberParameters: 1
   Parameter[0]: 0000000000000007
Subcode: 0x7 FAST_FAIL_FATAL_APP_EXIT

FAULTING_THREAD:  00004cb0

PROCESS_NAME:  python.exe

ERROR_CODE: (NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.

EXCEPTION_CODE: (NTSTATUS) 0xc0000409 - The system detected an overrun of a stack-based buffer in this application. This overrun could potentially allow a malicious user to gain control of this application.

EXCEPTION_CODE_STR:  c0000409

EXCEPTION_PARAMETER1:  0000000000000007

WATSON_BKT_PROCSTAMP:  5a5e439a

WATSON_BKT_PROCVER:  3.6.4150.1013

PROCESS_VER_PRODUCT:  Python

WATSON_BKT_MODULE:  ucrtbase.dll

WATSON_BKT_MODSTAMP:  70f70cc4

WATSON_BKT_MODOFFSET:  6b70e

WATSON_BKT_MODVER:  10.0.16299.15

MODULE_VER_PRODUCT:  Microsoft® Windows® Operating System

BUILD_VERSION_STRING:  10.0.16299.15 (WinBuild.160101.0800)

MODLIST_WITH_TSCHKSUM_HASH:  d39c6bc550850a285511eb4ca73187e2bfcc8d5c

MODLIST_SHA1_HASH:  c6d888e2558848c801875139c919412641504368

NTGLOBALFLAG:  70

APPLICATION_VERIFIER_FLAGS:  0

PRODUCT_TYPE:  1

SUITE_MASK:  272

DUMP_TYPE:  fe

ANALYSIS_SESSION_HOST:  DESKTOP-V7DQHVH

ANALYSIS_SESSION_TIME:  01-28-2018 19:19:28.0120

ANALYSIS_VERSION: 10.0.17061.1000 amd64fre

THREAD_ATTRIBUTES: 
OS_LOCALE:  ENU

PROBLEM_CLASSES: 

    ID:     [0n277]
    Type:   [FAIL_FAST]
    Class:  Primary
    Scope:  DEFAULT_BUCKET_ID (Failure Bucket ID prefix)
            BUCKET_ID
    Name:   Add
    Data:   Omit
    PID:    [Unspecified]
    TID:    [Unspecified]
    Frame:  [0]

    ID:     [0n266]
    Type:   [FATAL_APP_EXIT]
    Class:  Addendum
    Scope:  DEFAULT_BUCKET_ID (Failure Bucket ID prefix)
            BUCKET_ID
    Name:   Add
    Data:   Omit
    PID:    [Unspecified]
    TID:    [Unspecified]
    Frame:  [0]

BUGCHECK_STR:  FAIL_FAST_FATAL_APP_EXIT

DEFAULT_BUCKET_ID:  FAIL_FAST_FATAL_APP_EXIT

PRIMARY_PROBLEM_CLASS:  FAIL_FAST

LAST_CONTROL_TRANSFER:  from 00007ffc623b447f to 00007ffcd74eb70e

STACK_TEXT:  
000000be`c3a3da80 00007ffc`623b447f : 000000be`00000003 00000000`00000003 000000be`c3a3dbf0 000001f6`6f45afa0 : ucrtbase!abort+0x4e
000000be`c3a3dab0 00007ffc`62dd4ea9 : 000000be`c3a3e640 00000000`00000000 00000000`00000000 000001f6`00000001 : _pywrap_tensorflow_internal!tensorflow::internal::LogMessageFatal::~LogMessageFatal+0x4f
000000be`c3a3daf0 00007ffc`62db3d62 : 00000000`00000001 000000be`c3a3ec50 000001f6`8ebb94e8 000000be`c3a3f6f0 : _pywrap_tensorflow_internal!tensorflow::LaunchConv2DOp<Eigen::GpuDevice,float>::operator()+0x18c9
000000be`c3a3eb50 00007ffc`6251e5b4 : 000001f6`6f942d30 000001f6`6f461900 000001f6`6f461900 000000be`c3a3f0a0 : _pywrap_tensorflow_internal!tensorflow::Conv2DOp<Eigen::GpuDevice,float>::Compute+0x742
000000be`c3a3eda0 00007ffc`6251ddca : ffffffff`fffffffe 000000be`c3a3f050 000001f6`6f3ddc80 000001f6`6fcd2db0 : _pywrap_tensorflow_internal!tensorflow::BaseGPUDevice::ComputeHelper+0x4f4
000000be`c3a3ef70 00007ffc`623edaa5 : 00000000`00000000 00000000`00000000 000000be`c3a3f100 00000000`00000000 : _pywrap_tensorflow_internal!tensorflow::BaseGPUDevice::Compute+0x18a
000000be`c3a3f000 00007ffc`623f1068 : 000001f6`6f8dde30 00007ffc`62384a75 000001f6`6f8dde30 000001f6`6f4635c0 : _pywrap_tensorflow_internal!tensorflow::NewLocalExecutor+0x1065
000000be`c3a3f9c0 00007ffc`62384e79 : 000001f6`6f8dde30 00000000`00000000 000000be`c3a3fa58 000001f6`6f4633f0 : _pywrap_tensorflow_internal!?_Copy@?$_Func_impl@V?$_Binder@U_Unforced@std@@P8ExecutorState@?A0x4f36ba0d@tensorflow@@EAAXUTaggedNode@345@_J@ZQEAV345@AEBU6345@AEA_J@std@@V?$allocator@H@2@X$$V@std@@EEBAPEAV?$_Func_base@X$$V@2@PEAX@Z+0x78
000000be`c3a3fa10 00007ffc`62385020 : 000001f6`6f8dde30 00007ffc`00001198 00007ffc`aa528b40 00000000`00000000 : _pywrap_tensorflow_internal!Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x3d9
000000be`c3a3faa0 00007ffc`623b6ab5 : 000001f6`6f480000 00007ffc`00000000 000001f6`6f4843c0 00000000`00000000 : _pywrap_tensorflow_internal!Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop+0x580
000000be`c3a3fae0 00007ffc`623b6a09 : 000001f6`6f4845b0 00000000`00000000 00000000`00000000 00000000`00000000 : _pywrap_tensorflow_internal!tensorflow::WindowsFileSystem::Utf8ToWideChar+0x175
000000be`c3a3fb20 00007ffc`d749d885 : 00000000`fc960000 000001f6`6f4845b0 00000000`00000000 00000000`00000000 : _pywrap_tensorflow_internal!tensorflow::WindowsFileSystem::Utf8ToWideChar+0xc9
000000be`c3a3fb50 00007ffc`d9c21fe4 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : ucrtbase!thread_start<unsigned int (__cdecl*)(void * __ptr64)>+0x35
000000be`c3a3fb80 00007ffc`da2eef91 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : KERNEL32!BaseThreadInitThunk+0x14
000000be`c3a3fbb0 00000000`00000000 : 00000000`00000000 00000000`00000000 00000000`00000000 00000000`00000000 : ntdll!RtlUserThreadStart+0x21

THREAD_SHA1_HASH_MOD_FUNC:  a995353e639442c8476b335aedd6ecded1b41211
THREAD_SHA1_HASH_MOD_FUNC_OFFSET:  b900a9e6afe48f2fd80e72effda8ef8748a73296
THREAD_SHA1_HASH_MOD:  216c770e1f463298a8482ae892957bad45a82609
FOLLOWUP_IP: 
ucrtbase!abort+4e
00007ffc`d74eb70e cd29            int     29h
FAULT_INSTR_CODE:  15ba29cd
SYMBOL_STACK_INDEX:  0
SYMBOL_NAME:  ucrtbase!abort+4e
FOLLOWUP_NAME:  MachineOwner
MODULE_NAME: ucrtbase
IMAGE_NAME:  ucrtbase.dll
DEBUG_FLR_IMAGE_TIMESTAMP:  70f70cc4
STACK_COMMAND:  ~34s ; .cxr ; kb
BUCKET_ID:  FAIL_FAST_FATAL_APP_EXIT_ucrtbase!abort+4e
FAILURE_EXCEPTION_CODE:  c0000409
FAILURE_IMAGE_NAME:  ucrtbase.dll
BUCKET_ID_IMAGE_STR:  ucrtbase.dll
FAILURE_MODULE_NAME:  ucrtbase
BUCKET_ID_MODULE_STR:  ucrtbase
FAILURE_FUNCTION_NAME:  abort
BUCKET_ID_FUNCTION_STR:  abort
BUCKET_ID_OFFSET:  4e
BUCKET_ID_MODTIMEDATESTAMP:  70f70cc4
BUCKET_ID_MODCHECKSUM:  fbc7a
BUCKET_ID_MODVER_STR:  10.0.16299.15
BUCKET_ID_PREFIX_STR:  FAIL_FAST_FATAL_APP_EXIT_
FAILURE_PROBLEM_CLASS:  FAIL_FAST
FAILURE_SYMBOL_NAME:  ucrtbase.dll!abort
FAILURE_BUCKET_ID:  FAIL_FAST_FATAL_APP_EXIT_c0000409_ucrtbase.dll!abort
WATSON_STAGEONE_URL:  http://watson.microsoft.com/StageOne/python.exe/3.6.4150.1013/5a5e439a/ucrtbase.dll/10.0.16299.15/70f70cc4/c0000409/0006b70e.htm?Retriage=1
TARGET_TIME:  2018-01-28T17:19:44.000Z
OSBUILD:  16299
OSSERVICEPACK:  15
SERVICEPACK_NUMBER: 0
OS_REVISION: 0
OSPLATFORM_TYPE:  x64
OSNAME:  Windows 10
OSEDITION:  Windows 10 WinNt SingleUserTS
USER_LCID:  0
OSBUILD_TIMESTAMP:  1976-06-22 09:45:20
BUILDDATESTAMP_STR:  160101.0800
BUILDLAB_STR:  WinBuild
BUILDOSVER_STR:  10.0.16299.15
ANALYSIS_SESSION_ELAPSED_TIME:  9166
ANALYSIS_SOURCE:  UM
FAILURE_ID_HASH_STRING:  um:fail_fast_fatal_app_exit_c0000409_ucrtbase.dll!abort
FAILURE_ID_HASH:  {e31753ac-c98a-8055-3663-47e707543d20}"
16521,Failed to load native Tensorflow runtine,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information : 
raspberry pi raspbian 4.9.24-v27
Tensorflow installed from source (version 1.5)
Python version : 3.5.4
gcc 4.9.2
Python was build from sources
Bazel version 0.9.0

Followin this link to build bazel and tensor flow (with last version of bazel and tf) : 
<img width=""437"" alt=""image"" src=""https://user-images.githubusercontent.com/30391054/35484892-572990c2-0457-11e8-8980-6389246bb2d0.png"">
with minimal support (only jmalloc)


### Describe the problem
When trying import tensorflow in python3 i have the following error message 👍 
importError tensorflow/python/_pywrap_tensorflow_internal.so undefined symbol: _ZN10tensorflow9ConcatCPUINS_8bfloat16EEEvPNS_10DeviceBaseERKSt6vectorISt10unique_ptrINS_6TTypesIT_Li2EiE11ConstMatrixESt14default_deleteIS9_EESaISC_EEPNS8_6MatrixE

failed to load the native Tensorflow runtime

I am in doubt to revert to python3.4 and using the tensorflow available without compiling ...

Thanks in advance.
"
16518,./configure [--help|-h] does not work,"
```
./configure --help
```

starts some interactive configuration.

It should output a list of possible configuration options or indicate where to find more information.

It should *not* start interactive configuration."
16517,ou must feed a value for placeholder tensor 'import/Placeholder when i test my frozen model,"Hello , I trying create mobile app for object recognition for my own created model. I fallow this tutorial https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2/#2
But when i even get a testing model from step 3 i get error 

```
InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'import/Placeholder' with dtype float
	 [[Node: import/Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]
```

I'm aware that is wrong node problem, but i trying with other and always i get failure 

My code for model creation 
```

x = tf.placeholder(tf.float32,
                   shape=[None, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3], name=""x"")
y_ = tf.placeholder(tf.float32, shape=[None, cons.LABELS_NUMB], name=""labels"")

K = 4
L = 8
M = 12
N = 200

x_image = tf.reshape(x, [-1, cons.IMAGE_SIZE, cons.IMAGE_SIZE, 3])
tf.summary.image('input', x_image, 3)
print(""X image "")
print(tf.shape(x_image))

################## first ##############

W_conv1 = weight_variable([5, 5, 3, 32], ""weight1"")
b_conv1 = bias_variable([32], ""bias1"")

h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
h_pool1 = max_pool_2x2(h_conv1)
print(""W_conv1 "")
print(tf.shape(W_conv1))
print(""b_conv1 "")
print(tf.shape(b_conv1))
print(""h_conv1 "")
print(tf.shape(h_conv1))
print(""h_pool1 "")
print(tf.shape(h_pool1))

################## second ##############

W_conv2 = weight_variable([5, 5, 32, 64], ""weight2"")
b_conv2 = bias_variable([64], ""bias2"")

h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
tf.summary.histogram(""activations"", h_conv2)


h_pool2 = max_pool_2x2(h_conv2)

print(""W_conv2 "")
print(tf.shape(W_conv2))
print(""b_conv2 "")
print(tf.shape(b_conv2))
print(""h_conv2 "")
print(tf.shape(h_conv2))
print(""h_pool2 "")
print(tf.shape(h_pool2))

################## fully connected 3 ##############

W_fc1 = weight_variable([8 * 8 * 64, 1024], ""Weight3"")
b_fc1 = bias_variable([1024], ""bias3"")

h_pool2_flat = tf.reshape(h_pool2, [-1, 8 * 8 * 64])
h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)

print(""W_fc1 "")
print(tf.shape(W_fc1))
print(""b_fc1 "")
print(tf.shape(b_fc1))
print(""h_pool2_flat "")
print(tf.shape(h_pool2_flat))
print(""h_fc1 "")
print(tf.shape(h_fc1))

################ dropout  4 #################

keep_prob = tf.placeholder(tf.float32)
h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

################## fully connected 5 ##############

W_fc2 = weight_variable([1024, cons.LABELS_NUMB], ""weight5"")
b_fc2 = bias_variable([cons.LABELS_NUMB], ""bias5"")

Y = tf.matmul(h_fc1_drop, W_fc2) + b_fc2
tf.summary.histogram(""final"", Y)


print(""W_fc2 "")
print(tf.shape(W_fc2))
print(""b_fc2 "")
print(tf.shape(b_fc2))
print(""Y "")
print(tf.shape(Y))

with tf.name_scope(""cross_entropy""):
    cross_entropy = tf.reduce_mean(
        tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=Y))
    tf.summary.scalar(""xent"", cross_entropy)

with tf.name_scope(""train_step""):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

with tf.name_scope(""Acuracy""):
    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(y_, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))
    tf.summary.scalar(""accuracy"", accuracy)

summ = tf.summary.merge_all()
saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    writer = tf.summary.FileWriter(""/home/damian/api/mnist_demo/10"")
    writer.add_graph(sess.graph)
    for i in range(1000):
        img, lb = fileCreation.next_batch(100, images32, labels)
        if i % 5 == 0:
            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
        writer.add_summary(s, i)
        if i % 100 == 0:
            saver.save(sess, '/home/damian/api/checkpoint/my_test_model', global_step=i)
            train_accuracy = accuracy.eval(feed_dict={
                x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 1.0})
            print('step %d, training accuracy %g' % (i, train_accuracy))
        train_step.run(feed_dict={x: img, y_: fileCreation.dense_to_one_hot(lb, cons.LABELS_NUMB), keep_prob: 0.5})

    print('test accuracy %g' % accuracy.eval(feed_dict={x: test_images32,
                                                        y_: fileCreation.dense_to_one_hot(test_labels,
                                                                                          cons.LABELS_NUMB),
                                                        keep_prob: 0.1}))
```


I'm freeze this model with and i point output_node to 'final'
next i call script from tutorial


```
python -m scripts.label_image \
  --graph=tf_files/frozen_model3.pb  \
  --input_layer=x \
  --output_layer=final \
  --image=tf_files/stop.png  \
  --input_height=32 \
  --input_width=32 \
  --input_mean=16  \
  --input_std=16  
```









"
16515,Unable to install/ upgrade tensorflow1.5 on window10,"Exception:
Traceback (most recent call last):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\commands\install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\req\req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 811, in get_page
    inst = cls(resp.content, resp.url, resp.headers)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\pip\index.py"", line 731, in __init__
    namespaceHTMLElements=False,
TypeError: parse() got an unexpected keyword argument 'transport_encoding'"
16514,Possibility to compile Tensorflow Native C++ Library to support Windows 10 UWP?,"Hello all,

When I was checking Deep Learning framework for Windows 10 UWP development, somehow I came into this: https://anyline.com/news/tensorflow-implem/

It was not hard to see their UWP example app as well: https://github.com/Anyline/anyline-ocr-examples-windows-uwp

Which really brings up the question: Is it possible to compile the Tensorflow C++ library to add support for Windows 10 UWP development now? Or at least to get some codes enough for the Model Evaluation to work? Since it has been achieved by others. Some similar cases are like OpenCV, Numpy, etc. It can be really tricky solving dependencies and other unknown issues, but since it's done before there's hope?

Many thanks in advance!"
16513,TF1.5.0 not working with CUDA 8.0,"After upgrading to TF 1.5.0, when I import tensorflow, it raises:

```
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory
```

- System: Ubuntu 14.04.5 LTS (64 bit)
- Python: 2.7.6
- TensorFlow: tensorflow-gpu-1.5.0
- GPU: GeForce GTX TITAN
- CUDA: 8.0"
16512,how to install ffmpeg in tensorflow 1.4 binary,"hi
i want to install ffmpeg in tensorflow 1.4 binary , python 3.5 on ubuntu 16.04 , please help me how do i do ? 
the output type python -c ""from tensorflow.contrib import ffmpeg"" is ok dont have anly error , but i dont know why : 
from tensorflow.contrib import ffmpeg

i get error , 
>>>  from tensorflow.contrib import ffmpeg
  File ""<stdin>"", line 1
    from tensorflow.contrib import ffmpeg
    ^
IndentationError: unexpected indent
"
16510,Feature Request: Make lstm2d.separable_lstm accept Dynamic Batch Sizes,"Apparently, lstm2d.separable_lstm doesn't accept dynamic batch sizes (number of images). Whenever I set the shape of a `placeholder` to `(None, height, width, depth)` to be fed into the network , I get this error:

```
Traceback (most recent call last):
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 159, in separable_lstm
    hidden = horizontal_lstm(images, nhidden)
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 82, in horizontal_lstm
    sequence = images_to_sequence(images)
  File ""...\tensorflow\contrib\ndlstm\python\lstm2d.py"", line 47, in images_to_sequence
    [width, num_image_batches * height, depth])
TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'
```

I guess it would be nice to have `separable_lstm` accept dynamic batch sizes so it can be used effectively."
16507,"ResourceExhaustedError, when running UNET","My computer has a gpu GeForce 940MX installed. It has the Memory bandwidth 16.02 GB/s. I'm trying to train LUNA dataset using UNET model using following code.

	from __future__ import print_function

	import numpy as np
	from keras.models import Model
	from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
	from keras.layers import concatenate
	from keras.optimizers import Adam
	from keras.optimizers import SGD
	from keras.callbacks import ModelCheckpoint, LearningRateScheduler
	from keras import backend as K


	K.set_image_dim_ordering('th')  # Theano dimension ordering in this code

	img_rows = 512
	img_cols = 512

	smooth = 1.


	def dice_coef(y_true, y_pred):
		y_true_f = K.flatten(y_true)
		y_pred_f = K.flatten(y_pred)
		intersection = K.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

	def dice_coef_np(y_true,y_pred):
		y_true_f = y_true.flatten()
		y_pred_f = y_pred.flatten()
		intersection = np.sum(y_true_f * y_pred_f)
		return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)

	def dice_coef_loss(y_true, y_pred):
		return -dice_coef(y_true, y_pred)


	def get_unet():
		inputs = Input((1,img_rows, img_cols))
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
		pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)

		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
		conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
		pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)

		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
		conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
		pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)

		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
		conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)
		pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)

		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)
		conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)

		#up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)
		up6 = concatenate([UpSampling2D(size=(2, 2))(conv5), conv4], axis=1)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)
		conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)

		#up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)
		up7 = concatenate([UpSampling2D(size=(2, 2))(conv6), conv3], axis=1)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)
		conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)

		#up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)
		up8 = concatenate([UpSampling2D(size=(2, 2))(conv7), conv2], axis=1)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)
		conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)

		#up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)
		up9 = concatenate([UpSampling2D(size=(2, 2))(conv8), conv1], axis=1)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)
		conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)

		conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)

		model = Model(inputs=inputs, outputs=conv10)

		model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])

		return model


	def train_and_predict(use_existing):
		print('-'*30)
		print('Loading and preprocessing train data...')
		print('-'*30)
		imgs_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainImages.npy"").astype(np.float32)
		imgs_mask_train = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""trainMasks.npy"").astype(np.float32)

		imgs_test = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testImages.npy"").astype(np.float32)
		imgs_mask_test_true = np.load(""C:/Users/hirplk/Desktop/unet/Luna2016-Lung-Nodule-Detection-master_new/DATA_PROCESS/scratch/cse/dual/cs5130287/Luna2016/output_final/""+""testMasks.npy"").astype(np.float32)
		
		mean = np.mean(imgs_train)  # mean for data centering
		std = np.std(imgs_train)  # std for data normalization

		imgs_train -= mean  # images should already be standardized, but just in case
		imgs_train /= std

		print('-'*30)
		print('Creating and compiling model...')
		print('-'*30)
		model = get_unet()
		# Saving weights to unet.hdf5 at checkpoints
		model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)
		#
		# Should we load existing weights? 
		# Set argument for call to train_and_predict to true at end of script
		if use_existing:
			model.load_weights('./unet.hdf5')
			
		# 
		# The final results for this tutorial were produced using a multi-GPU
		# machine using TitanX's.
		# For a home GPU computation benchmark, on my home set up with a GTX970 
		# I was able to run 20 epochs with a training set size of 320 and 
		# batch size of 2 in about an hour. I started getting reseasonable masks 
		# after about 3 hours of training. 
		#
		print('-'*30)
		print('Fitting model...')
		print('-'*30)
		model.fit(imgs_train, imgs_mask_train, batch_size=50, epochs=10, verbose=1, shuffle=True,
				  callbacks=[model_checkpoint])

		# loading best weights from training session
		print('-'*30)
		print('Loading saved weights...')
		print('-'*30)
		model.load_weights('./unet.hdf5')

		print('-'*30)
		print('Predicting masks on test data...')
		print('-'*30)
		num_test = len(imgs_test)
		imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)
		for i in range(num_test):
			imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]
		np.save('masksTestPredicted.npy', imgs_mask_test)
		mean = 0.0
		for i in range(num_test):
			mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])
		mean/=num_test
		print(""Mean Dice Coeff : "",mean)

	if __name__ == '__main__':
		train_and_predict(False)
		
But when running it using GPU I'm getting the following error.

	Warning (from warnings module):
	  File ""C:\Research\Python_installation\lib\site-packages\h5py\__init__.py"", line 36
		from ._conv import register_converters as _register_converters
	FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
	Using TensorFlow backend.
	------------------------------
	Loading and preprocessing train data...
	------------------------------
	------------------------------
	Creating and compiling model...
	------------------------------
	------------------------------
	Fitting model...
	------------------------------
	Epoch 1/10
	Traceback (most recent call last):
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1327, in _do_call
		return fn(*args)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1306, in _run_fn
		status, run_metadata)
	  File ""C:\Research\Python_installation\lib\contextlib.py"", line 66, in __exit__
		next(self.gen)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
		pywrap_tensorflow.TF_GetCode(status))
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	During handling of the above exception, another exception occurred:

	Traceback (most recent call last):
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 127, in train_and_predict
		callbacks=[model_checkpoint])
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1657, in fit
		validation_steps=validation_steps)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\training.py"", line 1213, in _fit_loop
		outs = f(ins_batch)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2357, in __call__
		**self.session_kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 895, in run
		run_metadata_ptr)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1124, in _run
		feed_dict_tensor, options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1321, in _do_run
		options, run_metadata)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\client\session.py"", line 1340, in _do_call
		raise type(e)(node_def, op, message)
	tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]

	Caused by op 'conv2d_1/convolution', defined at:
	  File ""<string>"", line 1, in <module>
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 124, in main
		ret = method(*args, **kwargs)
	  File ""C:\Research\Python_installation\lib\idlelib\run.py"", line 351, in runcode
		exec(code, self.locals)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 150, in <module>
		train_and_predict(False)
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 106, in train_and_predict
		model = get_unet()
	  File ""C:\Users\hirplk\Desktop\unet\DSB3Tutorial-master\tutorial_code\LUNA_train_unet.py"", line 39, in get_unet
		conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\engine\topology.py"", line 603, in __call__
		output = self.call(inputs, **kwargs)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\layers\convolutional.py"", line 164, in call
		dilation_rate=self.dilation_rate)
	  File ""C:\Research\Python_installation\lib\site-packages\keras\backend\tensorflow_backend.py"", line 3195, in conv2d
		data_format=tf_data_format)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 672, in convolution
		op=op)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 338, in with_space_to_batch
		return op(input, num_spatial_dims, padding)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 664, in op
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\nn_ops.py"", line 131, in _non_atrous_convolution
		name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\ops\gen_nn_ops.py"", line 397, in conv2d
		data_format=data_format, name=name)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
		op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 2630, in create_op
		original_op=self._default_original_op, op_def=op_def)
	  File ""C:\Research\Python_installation\lib\site-packages\tensorflow\python\framework\ops.py"", line 1204, in __init__
		self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

	ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[50,32,512,512]
		 [[Node: conv2d_1/convolution = Conv2D[T=DT_FLOAT, data_format=""NCHW"", padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/gpu:0""](_arg_input_1_0_2/_261, conv2d_1/kernel/read)]]
		 [[Node: loss/mul/_273 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/cpu:0"", send_device=""/job:localhost/replica:0/task:0/gpu:0"", send_device_incarnation=1, tensor_name=""edge_3022_loss/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/cpu:0""]()]]


Can someone please kindly explain me the reason behind this error, ResourceExhaustedError. Is it because that the memory of GPU is not enough to load the dataset. This worked fine without GPU. But took around 6 hours to finish one epoch"
16506,Feature request: Have Estimator display Loss and Metrics for Every Epoch and not Every Step,"Most of the papers I’ve read measure the time it takes to train a model with every epoch and not every step. If it isn’t possible to display the loss only for every epoch, I think it would be nice to print when an epoch has passed."
16504,Issue propagating gradients through tf.while_loop,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu wheezy

- **TensorFlow installed from (source or binary)**:
pip

- **TensorFlow version (use command below)**:
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-4-g9283868
tf.COMPILER_VERSION = v1.4.0-4-g9283868
Sanity check: array([1], dtype=int32)

- **Python version**: 
3.5

### Describe the problem

I've found a few issues when trying to propagate gradients through tf.while_loops.

One issue is ops like this inside the loop body break gradients 
```k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )```

but more concerningly, conjoined conditions such as this:
```tf.logical_and( k < max_its-1, chg_w > tol  )```
or even this
``` tf.cast( max_its-k, DTYPE) *(chg_w - tol)```
breaks the differentiablity across the while loop.


### Source code / logs
``` python
tf.reset_default_graph()
sess = tf.InteractiveSession()
g = tf.Graph().as_default()

max_its = 10
tol = 1e-3

c = tf.constant( np.arange(100), dtype=DTYPE)
w = tf.Variable( initial_value=np.ones(100),  dtype=DTYPE)/100
k = tf.Variable( 0, dtype=tf.int32 )
chg_w = tf.constant( np.inf, dtype=DTYPE )


def _eg_step( k, w, chg_w): 
    grad = tf.gradients( -tf.reduce_sum( w * c ) , w )[0]
    w_n = w * tf.exp( -0.1  * grad )
    w_n = w_n / tf.reduce_sum( w_n ) 
    chg_w = tf.reduce_sum( tf.abs( w_n - w) ) / tf.reduce_sum( tf.abs( w ) )
    k = k + 1
    **# !! this busts the differentiablity !!
    # k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )**
    return k, w_n, chg_w

def _continue_cond( k, w, chg_w, *args ):
    **# NOTE either of this conjoined conditions
    #        tf.logical_and( k < max_its-1, chg_w > tol  )
    # OR     tf.cast( max_its-k, DTYPE) *(chg_w - tol)
    # do no propagate gradients correctly**
    return  k < max_its # tf.logical_and( k < max_its-1, chg_w > tol  )

k, w, chg_w = tf.while_loop(
    cond=_continue_cond,  body=_eg_step,
    loop_vars=[k, w, chg_w],
    name='while_loop', parallel_iterations=1
)

# see if the gradient is propagated
tf.gradients( w, c)

```"
16502,Java Android API: No OpKernel was registered to support Op 'ListDiff',"### System information
Android with Java API releases 1.4.0 and 1.5.0-rc1 (1.5 is not available yet)

### What I did
I have a graph that is applying a `tf.layers.dense` on a three dimensional tensor, which is applying a dense operation to the last dimension. Learning and execution on a Windows 10 and Ubuntu work fine.

Now I froze the model and put it on Android and receive the following error.

```
01-27 20:06:59.628 10481-11380/de.test.local W/System.err: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'ListDiff' with these attrs.  Registered devices: [CPU], Registered kernels:
01-27 20:06:59.628 10481-11380/de.test.local W/System.err:   <no registered kernels>
01-27 20:06:59.628 10481-11380/de.test.local W/System.err: 	 [[Node: model/logits/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](model/logits/Tensordot/range, model/logits/Tensordot/add_1)]]
01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at java.util.concurrent.FutureTask.report(FutureTask.java:94)
01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at java.util.concurrent.FutureTask.get(FutureTask.java:164)
01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at de.test.service.TaskWorkerLoop$Loop.run(TaskWorkerLoop.java:71)
01-27 20:06:59.628 10481-11380/de.test.local W/System.err:     at java.lang.Thread.run(Thread.java:762)
01-27 20:06:59.629 10481-11380/de.test.local W/System.err: Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'ListDiff' with these attrs.  Registered devices: [CPU], Registered kernels:
01-27 20:06:59.629 10481-11380/de.test.local W/System.err:   <no registered kernels>
01-27 20:06:59.630 10481-11380/de.test.local W/System.err: 	 [[Node: model/logits/Tensordot/ListDiff = ListDiff[T=DT_INT32, out_idx=DT_INT32](model/logits/Tensordot/range, model/logits/Tensordot/add_1)]]
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java.lang.reflect.Constructor.newInstance0(Native Method)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java.lang.reflect.Constructor.newInstance(Constructor.java:430)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:565)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:646)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:704)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ForEachOps$ForEachOp.evaluateParallel(ForEachOps.java:195)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ForEachOps$ForEachOp$OfRef.evaluateParallel(ForEachOps.java:210)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:459)
01-27 20:06:59.630 10481-11380/de.test.local W/System.err:     at java8.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:624)
...
```

Is there a reason for this kernel to be missing on Android? If not, can you please add it? And what could I do in the mean time to replace the dense layer?

Many thanks in advance!"
16499,Extend tf.unique_with_counts to multi-dimensional tensors,"I'm trying to solve KNN using tensorflow. After I get the K neighbours for N vectors, I have a N by K tensor. Now, for each vector in N, I need to use [```tf.unique_with_counts```][1] to find the majority vote. However, I cannot iterate in a tensor and I cannot run [```tf.unique_with_counts```][1] with a multi-dimensional tensor. It keeps giving me ```InvalidArgumentError (see above for traceback): unique expects a 1D vector.```

Why can't tf support multi-demsional input?

Example:

    def knnVote():
    '''
    KNN using majority vote
    '''
    #nearest indices
    A = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])
    nearest_k_y, idx, votes = tf.unique_with_counts(A)
    print(""y"", nearest_k_y.eval())
    print(""idx"", idx.eval())
    print(""votes"", votes.eval())
    majority = tf.argmax(votes)
    predict_res = tf.gather(nearest_k_y, majority)
    
    
    print(""majority"", majority.eval())
    print(""predict"", predict_res.eval())
    return predict_res

Result:

    y [1 2 4 7 8]
    idx [0 0 1 2 2 2 3 4 4]
    votes [2 1 3 1 2]
    majority 2
    predict 4

But how can I extend this to N by D input A, such as the case when ```A = tf.constant([[1, 1, 2, 4, 4, 4, 7, 8, 8],
[2, 2, 3, 3, 3, 4, 4, 5, 6]])```

  [1]: https://www.tensorflow.org/api_docs/python/tf/unique_with_counts"
16498,Bounding box do not remove,"Hi there,
After I detected my tv, it showed up but when i move it to other place, it do not remove the bounding box even though i put my camera on the table. Is this a bug?

![2018-01-28-00-34-50](https://user-images.githubusercontent.com/32919949/35474032-9eb4827e-03c3-11e8-8768-c4c81f9ad391.png)
"
16497,SpatialConvolution in tflite is very slow,"Hi, i build tflite ,using bazel build --cxxopt='--std=c++11' //tensorflow/contrib/lite/java:tensorflowlite 
--crosstool_top=//external:android/crosstool \
--host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
--cpu=armeabi-v7a
I test my CNN model, but it's very slow in google piexl. I analyze time cost of my CNN model. SpatialConvolution function is very slow, only 20%  of the peak flops of my hardware.
my model's data type is kTfLiteFloat32. 
Is build script for android right ？I doubt my build script is wrong , so run the SpatialConvolution function with ARM NEON.  (I don't know how Eigen is organized in tflite, i can't add log code in the source code to prove my suspicion)"
16496,"Feature Suggestion: ""Float-bit-strings""","### System information (Not really relevant ...)
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 8 (?)
- **GPU model and memory**: GTX 1070
- **Exact command to reproduce**: NA

### Summary

This proposes the use of what I call ""float-bit-strings"" or ""float-bits"" instead of one-hot-encoded arrays so as to greatly reduce the memory and computational usage e.g. in language models.

I don't think this preliminary discussion belongs on StackOverflow so I hope it is OK to post it here. It is a new feature that could be added to TensorFlow. There's quite likely somebody on the TensorFlow dev-team or in the community who has already thought of this. But I have searched the internet and cannot find any mentioning of a similar idea.


### Background

I have started looking at language-models using e.g. LSTM and encoder-decoder architectures. There are some aspects that seem to be incredibly wasteful and limiting. Let me briefly describe this and please forgive me if I am ignorant, I have only spent a week or two on studying LSTM and language models so far :-)

For example in Machine Translation we typically have the text-data for the source- and target-languages as lists of integer-tokens, where each integer maps to a word in the vocabulary. There may be e.g. 100k different words so these integer-tokens can take on values between zero and 100k. This data cannot be input directly to a Neural Network so we use an embedding layer to convert these integers to n-dimensional vectors with values between zero and one, according to a mapping-function that may either be loaded from disk or trained along with the rest of the Neural Network; if I understand correctly.

For the decoder in a language model, we have a similar problem where we must somehow convert integer-tokens to data that the neural network can work on. A typical way of doing this seems to be a one-hot encoding; if I understand correctly. (This could also be done for the encoder-part, but it doesn't seem to be necessary).

I can't figure out what the max-size of one-hot encodings are in TensorFlow and whether it can even handle 100k one-hot encoded tensors. But it is obviously an extremely wasteful data-mapping. For example, for a vocab of 100k words we only need 17-bits (log2(100k)) to represent each integer-token - but for a one-hot encoding using 32-bit floats we need 32 x 100k bits!

I can't figure out what people normally do, but it seems like the common practice is to limit the vocab to a smaller number of words, e.g. 1k or 10k. It appears that Google Translate runs on multiple GPU's and maybe that's why they can handle extremely large vocabs with one-hot encoded tensors?


### Float-bit-strings

I thought it might be possible to use a bit-string-like representation inside a TensorFlow model. I have searched the internet and cannot find anyone who has proposed a similar idea.

The idea is to convert each integer-token to what I call a ""float-bit-string"" or ""float-bits"". For example, the number 123 has the bit-string 01111011. We can then make a corresponding tensor with floats [0., 1., 1., 1., 1., 0., 1., 1.] and input this to the TensorFlow model.

In a language model we would then have to input and output these ""float-bits"" instead of one-hot encoded arrays. This would dramatically reduce the memory and computational requirements of the models.


### Test

I have hacked together a little test using numpy and Keras / TensorFlow. The idea is to see if we can learn to map integers x with values between 0 and 10k to y = 123 * x using these ""float-bit"" encodings. And it works as you can see by running the code further below! That is perhaps not a surprise as neural networks are general function approximators, but it's not always that they work according to theory :-)

However, the network cannot learn the arithmetic mapping of e.g. y = 123 * x when x and y are ""float-bits"". This means it cannot generalize to data it hasn't seen during training in the arithmetic manner we might expect. But I don't think that is necessary for use in e.g. language models where we merely want to be able to map some tensor from e.g. an LSTM to an integer-token from the vocabulary.


### Loss Functions

I have tested this with both MSE and binary cross-entropy in Keras, which unfortunately isn't documented so I'm not completely sure what it does. But in both cases it works and the model trains to get the bit-wise mapping correct.

There might be cases where you are more concerned about the MSE between the actual integer-values instead of their ""float-bit-string"" representations, in which case we would need a TensorFlow method to convert ""float-bits"" to integers and then take the MSE of the resulting integer and the true integer from the data-set. This is not relevant for language models, because the proximity of integer-keys do not correspond to words that are necessarily similar in meaning. But it could be useful in other applications.


### TensorFlow Implementation

In order to make this work in TensorFlow it seems that we just need a couple of TensorFlow-methods for converting between integers and ""float-bit-strings"". I have hacked this together using numpy but I'm sure somebody on the dev-team can make a super-fast native TensorFlow implementation. Then we just need a wrapper in Keras and that might be enough to do e.g. language models with gigantic vocabs.


### Test-Code

    import numpy as np
    from tensorflow.python.keras.models import Sequential
    from tensorflow.python.keras.layers import InputLayer
    from tensorflow.python.keras.layers import Dense
    from tensorflow.python.keras.optimizers import RMSprop
    
    
    # Number of bits to use in our ""float-bit-strings"".
    num_bits = 32
    
    def int_to_floatbits(value):
        """"""
        Convert a single integer value to an array of 0.0 and 1.0 floats
        corresponding to the bit-string.
    
        Example: value==123 gives [0.  ... 0.  1.  1.  1.  1.  0.  1.  1.]
        """"""
    
        # Convert the integer value to a bit-string.
        # NOTE: This has been fixed to 32-bit length.
        bitstr = ""{0:032b}"".format(value)
    
        # Convert the bit-string to an array of equivalent float-values.
        floatbits = np.array([1.0 if bit == '1' else 0.0 for bit in bitstr])
    
        return floatbits
    
    
    def floatbits_to_strbits(floatbits):
        """"""
        Convert an array of floats to a bit-string.
        A float value greater than 0.5 results in 1.0
        and a float value less or equal to 0.5 results in 0.0
    
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] gives ""001110""
        """"""
    
        # Convert the float-array to a list of bit-characters '0' or '1'.
        charbits = ['1' if floatbit > 0.5 else '0' for floatbit in floatbits]
    
        # Convert the bit-characters to a string.
        strbits = """".join(charbits)
    
        return strbits
    
    def floatbits_to_int(floatbits):
        """"""
        Convert a float-array to an integer, assuming each element
        of the float-array corresponds to a bit.
        
        Example: [0.1, 0.49, 0.51, 0.9, 1.1, -2.3] corresponds to
        the bit-string ""001110"" which is the integer 14.
        """"""
    
        # Convert the float-array to a bit-string.
        strbits = floatbits_to_strbits(floatbits=floatbits)
    
        # Convert the bit-string to an integer value.
        value = int(strbits, base=2)
    
        return value
    
    
    # Various tests of the above functions.
    if True:
        foo = int_to_floatbits(123)
        print(foo)
        print(floatbits_to_strbits(foo))
        print(floatbits_to_int(foo))
    
        bar = [0.3,  0.9,  0.8,  0.51,  0.501,  0.4999,  0.999,  1.1]
        print(floatbits_to_strbits(bar))
        print(floatbits_to_int(bar))
    
        baz = [0.1, 0.49, 0.51, 0.9, 1.1, -2.3]
        print(floatbits_to_strbits(baz))
        print(floatbits_to_int(baz))
    
    # quit()
    
    # We will now train a TensorFlow / Keras model
    # that maps integers between 0 and 10000 to
    # the same numbers multiplied by 123.
    # If we were to use one-hot encoding then we would
    # need 10000 inputs to the Neural Network and
    # 1230000 outputs if using the full output range.
    # Using ""bit-strings"" encoded as floats, we only need
    # 14 bits for the input and 21 bits for the output.
    # We round it up to 32-bits.
    
    # The dataset as integers,
    # we want the Neural Network to map from x to y.
    x_int = np.arange(10000, dtype=int)
    y_int = 123 * x_int
    
    # Convert the dataset to ""float-bit-strings"" (aka. float-bits).
    x = np.array(list(map(int_to_floatbits, x_int)))
    y_true = np.array(list(map(int_to_floatbits, y_int)))
    
    # Check the mapping is correct. E.g. if the number of required bits
    # exceeds num_bits then these may not create numpy matrices correctly.
    if False:
        print(x.shape)
        print(y_true.shape)
        print(x[0:10])
        print(y_true[0:10])
    
    # Start construction of the Keras Sequential model.
    model = Sequential()
    
    # Add an input layer to the model.
    model.add(InputLayer(input_shape=(num_bits,)))
    
    # Fully-connected / dense layers with ReLU-activation.
    model.add(Dense(512, activation='relu'))
    model.add(Dense(512, activation='relu'))
    
    # Last fully-connected / dense layer with sigmoid-activation
    # so the output is between 0.0 and 1.0
    model.add(Dense(num_bits, activation='sigmoid'))
    
    optimizer = RMSprop(lr=1e-3)
    
    if True:
        # Loss is MSE.
        model.compile(optimizer=optimizer,
                      loss='mean_squared_error')
    else:
        # Loss is Binary Crossentropy, but also report MSE.
        model.compile(optimizer=optimizer,
                      loss='binary_crossentropy',
                      metrics=['mse'])
    
    epochs = 50
    
    if True:
        # Fit the model using the entire data-set.
        model.fit(x, y_true, epochs=epochs)
    else:
        # Fit the model using the data-set split into training and validation.
        # You will see that the validation-error is high so the model
        # has not learned the arithmetic function of the data-set.
        model.fit(x, y_true, epochs=epochs, validation_split=0.2)
    
    # Use the model to predict the output for a part of the data-set.
    y_pred = model.predict(x[0:10])
    
    # The true output for this part of the data-set.
    y_true_subset = y_true[0:10]
    
    # Map the ""float-bit-strings"" to integers.
    y_pred_int = list(map(floatbits_to_int, y_pred))
    y_true_int = list(map(floatbits_to_int, y_true_subset))
    
    # Print the predicted and true integers.
    print(*zip(y_pred_int, y_true_int))
    
    # Round the float-bit-strings to 2 decimals for pretty printing.
    def rounded(numbers):
        return np.array([[""{:.2f}"".format(x) for x in row] for row in numbers])
    y_pred_rounded = rounded(y_pred)
    y_true_rounded = rounded(y_true_subset)
    
    # Print the predicted and true float-bit-strings.
    # (I know it is bad to reuse the same variable-names here ...)
    for y_pred_int, y_true_int, y_pred_rounded, y_true_rounded \
        in zip(y_pred_int, y_true_int, y_pred_rounded, y_true_rounded):
    
        print(y_true_int, ""\t"", y_true_rounded)
        print(y_pred_int, ""\t"", y_pred_rounded)
        print()


### Output

True integer and true ""float-bit-string"" (note that the numbers are all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '1.00' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']

Predicted integer and predicted ""float-bit-string"" (note that the numbers a **not** all exactly 0.00 or 1.00):

	738 	 ['0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.00' '0.03' '0.00' '0.00' '0.00' '0.01' '0.00' '0.00' '0.00' '0.00' '0.99' '0.00' '1.00' '1.00' '1.00' '0.00' '0.00' '0.00' '1.00' '0.00']
"
16493,"Keras ""Output missing from loss dictionary""","I think that this warning can be rephrased:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/_impl/keras/engine/training.py#L640-L641

Some outputs could be consumed as TB summary and not involved in any loss. 
Especially when you use an estimator converted from a tf.keras model  you have not the explicit control of the model_fn for placing summaries. 

So I suppose that one of the entry point for connecting summaries to the graph is the tf.train.SessionRunHook.

I.e. you can produce the output from the Dataset api, consume it as TB summary in the SessionRunHook without using it in any loss.

See also https://github.com/tensorflow/tensorflow/issues/14879#issuecomment-351996902 

/cc @fchollet "
16488,Travis trusty Ubuntu 14.04.5: module compiled against API version 0xc but this version of numpy is 0xb,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Travis trusty, Ubuntu 14.04.5
- **TensorFlow installed from (source or binary)**: binary, pip
- **TensorFlow version (use command below)**: latest pip (I guess 1.5.0)
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: GCC 4.8.4
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: ...

### Describe the problem

```
$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
RuntimeError: module compiled against API version 0xc but this version of numpy is 0xb
ImportError: numpy.core.multiarray failed to import
ImportError: numpy.core.umath failed to import
ImportError: numpy.core.umath failed to import
2018-01-26 23:12:12.304782: F tensorflow/python/lib/core/bfloat16.cc:664] Check failed: PyBfloat16_Type.tp_base != nullptr 
/home/travis/.travis/job_stages: line 57:  2555 Aborted                 (core dumped) python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Source code / logs

See the [Travis log](https://travis-ci.org/rwth-i6/returnn/jobs/333916514).

"
16483,Test case for session_partial_run_test.py is getting failed ,"I tried running file `session_partial_run_test.py`, It throws an error with following two failed exception.

```
======================================================================
FAIL: testRunAndPartialRunDist (__main__.PartialRunTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""g:/tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 258, in testRunAndPartialRunDist
    self.RunTestRunAndPartialRun(session.Session(server.target))
  File ""g:/tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 123, in RunTestRunAndPartialRun
    self.assertEqual(r1, r2)
AssertionError: Lists differ: [4.0, 12.0] != [array([], dtype=float32), 12.0]

- [4.0, 12.0]
+ [array([], dtype=float32), 12.0]

======================================================================
FAIL: testRunAndPartialRunDist (__main__.PartialRunWithCApiTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""g:/tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 258, in testRunAndPartialRunDist
    self.RunTestRunAndPartialRun(session.Session(server.target))
  File ""g:/tensorflow/tensorflow/python/client/session_partial_run_test.py"", line 123, in RunTestRunAndPartialRun
    self.assertEqual(r1, r2)
AssertionError: Lists differ: [4.0, 12.0] != [array([], dtype=float32), 12.0]

- [4.0, 12.0]
+ [array([], dtype=float32), 12.0]

----------------------------------------------------------------------
Ran 50 tests in 2.933s

FAILED (failures=2)
```

However, I tried debugging the code and found some scenarios in below function:

```
def RunTestRunAndPartialRun(self, sess):
    a = constant_op.constant(2.0, dtypes.float32)
    b = a * 2
    c = b * 3
    r1 = sess.run([b, c])
    h = sess.partial_run_setup([b, c], [])
    r2 = sess.partial_run(h, [b, c])
    self.assertEqual(r1, r2)
```

In 1st scenario, when `testRunAndPartialRunDirect()` is executed, the test gets succeed with h value:

`h = '->mul:0,mul_1:0//1/;0'`

In 2nd scenario, when `testRunAndPartialRunDist()` is executed, the test gets failed with h value:

`h = '0'`

and below following list are different since `assertEqual` is throwing an exception:

```
Lists differ: [4.0, 12.0] != [array([], dtype=float32), 12.0]

- [4.0, 12.0]
+ [array([], dtype=float32), 12.0] 
```

Looks like due to `zero` handle value, it started throwing an exception. Just need your suggestion if you are facing the same issue. Can I fix it by myself (I'd be happy to contribute)?

Guidance will be appreciated.

"
16481,Container localhost does not exist.,"Hi,

I upgraded from 1.5.0-rc1 to the current master branch and I started receiving the following error:

```
2018-01-27 02:48:38.928667: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at lookup_table_op.cc:656 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
2018-01-27 02:48:38.928786: W tensorflow/core/framework/op_kernel.cc:1201] OP_REQUIRES failed at iterator_ops.cc:855 : Not found: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
Exception in thread ""main"" org.platanios.tensorflow.jni.NotFoundException: Container localhost does not exist. (Could not find resource: localhost/hash_table_/Users/anthony/Development/GitHub/symphony-mt/temp/data/iwslt-15/vocab.vi_WHOLE_LINE_LINE_NUMBER)
	 [[Node: Lookup_1/LookupTableFind = LookupTableFindV2[Tin=DT_STRING, Tout=DT_INT64](lookup_1_placeholder, input_1, lookup_1_placeholder_1)]]
	 [[Node: Model/Model/Iterator/Next = IteratorGetNext[output_shapes=[[?,?], [?], [?,?], [?,?], [?]], output_types=[DT_INT32, DT_INT32, DT_INT32, DT_INT32, DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](Model/Model/Iterator)]]
```

It's hard to reproduce this error but a summary of the context is that I have a lookup table op inside a dataset map operator and I get this error when I try to execute the corresponding iterator ""GetNext"" op. I'm looking for information in how to parse and debug this error. I never explicitly set any containers for my variables or lookup tables (i.e., leave them to the default value; an empty string). Were there any changes introduced recently that could result in this error? Note that this happens with my Scala API but not with the Python API and so it may be that I haven't updated something in my code. I just don't really know where to look for this.

Thanks!"
16479,Does 1.5.0 not suppurt CUDA 9.1? It worked with CUDA 9.0 but not 9.1,"I installed 1.5.0, and tying to import tensorflow, but it said that 'cannot find cudart64_90.dll'. 
Then I installed the CUDA 9.0, and then everything works fine. 
So I want to make sure than does 1.5.0 not support CUDA 9.1 or I have something installed wrong? 
"
16478,Failed install on Windows,"Python 3.6.4

There is a strange error when I install tensorflow 1.5.

```
Collecting tensorflow
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/34/96/11f048eca7b4d6da3084ca49c636b9e720e9dd1483c0c4e9ba3cf5037564/tensorflow-1.5.0-cp36-cp36m-win_amd64.whl
Requirement already up-to-date: wheel>=0.26 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: numpy>=1.12.1 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting absl-py>=0.1.6 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/42/3c/1985d86a44bfe44fd060c02807336f840a509bfaa2d340860fba7d22da39/absl-py-0.1.9.tar.gz
Requirement already up-to-date: protobuf>=3.4.0 in d:\python\python36\lib\site-packages (from tensorflow)
Requirement already up-to-date: six>=1.10.0 in d:\python\python36\lib\site-packages (from tensorflow)
Collecting tensorflow-tensorboard<1.6.0,>=1.5.0 (from tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/43/69/82e2a368076c94edbba3cd15804103bf1f31486d69e11551b71fa1d1f384/tensorflow_tensorboard-1.5.0-py3-none-any.whl
Requirement already up-to-date: setuptools in d:\python\python36\lib\site-packages (from protobuf>=3.4.0->tensorflow)
Requirement already up-to-date: bleach==1.5.0 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: markdown>=2.6.8 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: werkzeug>=0.11.10 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Requirement already up-to-date: html5lib==0.9999999 in d:\python\python36\lib\site-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
Collecting futures>=3.1.1 (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow)
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/1f/9e/7b2ff7e965fc654592269f2906ade1c7d705f1bf25b7d469fa153f7d19eb/futures-3.2.0.tar.gz
Unknown requires Python '>=2.6, <3' but the running Python is 3.6.4
```

Why the dependency is *futures*? It doesn't have a verion of Python 3.6.4."
16477,"Windows Installation tutorial has wrong cuda version requirement, 9.0 required for latest version.","I just ran the installation validation and it's telling me I need 9.0, the tutorial says we must use 8.0. I don't have cheap access to Internet, now I have to find 1GB+ plus of data without paying $15 to use my phone's data. Please update the page to recommend 9.0.

Thank you."
16468,Keras multi input and estimator,"If I've interpreted it correctly seems that there is some strange behavior with Keras multi inputs and the estimator.

- Why input layers are renamed with `_1` suffix?
- Why TB display a `_2` suffixed parallel sub-graph?

I've attached a snippet runnable on [colab](http://colab.research.google.com/) and the TB rendered image.

 
```import tensorflow as tf
from tensorflow import keras as ks
import numpy as np
from IPython.display import clear_output, Image, display, HTML

def strip_consts(graph_def, max_const_size=32):
    """"""Strip large constant values from graph_def.""""""
    strip_def = tf.GraphDef()
    for n0 in graph_def.node:
        n = strip_def.node.add() 
        n.MergeFrom(n0)
        if n.op == 'Const':
            tensor = n.attr['value'].tensor
            size = len(tensor.tensor_content)
            if size > max_const_size:
                tensor.tensor_content = ""<stripped %d bytes>""%size
    return strip_def

def show_graph(graph_def, max_const_size=32):
    """"""Visualize TensorFlow graph.""""""
    if hasattr(graph_def, 'as_graph_def'):
        graph_def = graph_def.as_graph_def()
    strip_def = strip_consts(graph_def, max_const_size=max_const_size)
    code = """"""
        <script src=""//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js""></script>
        <script>
          function load() {{
            document.getElementById(""{id}"").pbtxt = {data};
          }}
        </script>
        <link rel=""import"" href=""https://tensorboard.appspot.com/tf-graph-basic.build.html"" onload=load()>
        <div style=""height:600px"">
          <tf-graph-basic id=""{id}""></tf-graph-basic>
        </div>
    """""".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))

    iframe = """"""
        <iframe seamless style=""width:1200px;height:620px;border:0"" srcdoc=""{}""></iframe>
    """""".format(code.replace('""', '&quot;'))
    display(HTML(iframe))

class ExampleHook(tf.train.SessionRunHook):
    def __init__(self):
        print('Starting the session.')
        return

    def begin(self):
        g = tf.get_default_graph()
        show_graph(g)
        print('Starting the session.')
        
        #for op in tf.get_default_graph().get_operations():
          #print(str(op.name) )
        
my_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""input_rgb"": np.array(np.random.rand(5,5,3).astype(np.float32)), ""input_gray"": np.array(np.random.rand(5,5,1).astype(np.float32)), 
       ""input_mix"": np.array(np.random.rand(5,5,1).astype(np.float32))},
    y= np.array(np.random.rand(5,5,1)),
      batch_size=1,
      num_epochs=1,
      shuffle=False)

input_rgb = ks.layers.Input(shape=(1,5, 5, 3), name=""input_rgb"")
input_gray = ks.layers.Input(shape=(1,5, 5, 1), name=""input_gray"")
input_mix = ks.layers.Input(shape=(1,5, 5, 1), name=""input_mix"")
rgb_gray = ks.layers.concatenate([input_rgb, input_gray, input_mix], name=""rbg_gray"")
x = ks.layers.Dense(1, activation='relu',name=""Dense_1"")(rgb_gray)
x = ks.layers.Dense(1, activation='softmax',name=""softmax"")(x)
model = ks.models.Model(
        inputs=[input_rgb, input_gray, input_mix],
        outputs=[x])
model.compile(loss={ 'softmax': 'binary_crossentropy'},optimizer=tf.keras.optimizers.Adam())


est = ks.estimator.model_to_estimator(
            keras_model=model)

model.summary()
print(model.input_names)
pred = list(est.predict(
    input_fn=my_input_fn,
    predict_keys=None,
    hooks=[ExampleHook()],
))
```
![tb](https://user-images.githubusercontent.com/1710528/35459923-5eca90b8-02e2-11e8-8248-764671141850.png)"
16466,[Feature request] Adding a PR curves to canned estimators for (binary) classifiers,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
This is a feature request, and I'd be happy to **contribute** if you think it's a valuable addition. 
I have been using estimators both pre-made and custom for classification tasks. I like that sharing the use of a `head`  as defined in [tensorflow/python/estimator/canned/head.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py) allows both the canned models and the custom ones to share prediction and evaluation metrics for comparison, however currently it feels that some key metrics are missing, mainly PR curves which are fully supported by tensorboard.  Currently, the `head` constructor allows a list of thresholds, although they are not used by default. The problem is that when used, it creates scalar summaries for the precision and recall at each threshold, which is not that useful as in general one wants to compare how different models compare precision wise while fixing recall and the other way round.

Adding the PR summary op from [here](https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/pr_curve) would make the eval metrics more informative IMO.

Thanks for taking the time to read this!

### Source code / logs
Any code that uses pre-made estimator classifiers relies on the same head. [This](https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep.py) is one example."
16465,bug with frame_step in tf.contrib.signal overlap_and_add inverse_stft,"### System information
- Based on example 
- Linux Ubuntu 16.04
- installed from binary
- v1.4.0-19-ga52c8d9, 1.4.1; also 1.5.0
- Python 2.7.14 |Anaconda custom (64-bit)| (default, Oct 16 2017, 17:29:19). IPython 5.4.1
- Cuda release 8.0, V8.0.61, cuDNN 6; also Cuda release 9.0, V9.0.176, cuDNN 7.0.5
- Geforce GTX 970M, also GTX 1070, Driver Version: 384.111

### Describe the problem

A.) When I create frames from a signal with `frame_length=1024` and  `frame_step=256` (i.e. 25% hop size, 75% overlap) using a hann window (also tried hamming), and then I reconstruct with `overlap_and_add`, I'd expect the signal to be reconstructed correctly (because of COLA etc). But instead it comes out exactly double the amplitude. I need to divide the resulting signal by two for it to be correct. 

B.) If I use STFT to create a series of overlapping spectrograms, and then reconstruct with inverse STFT, again with `frame_length=1024` and `frame_step=256`, the signal is again reconstructed at double amplitude. 

I realise why these might be the case (unity gain at 50% overlap for hann, so 75% overlap will double the signal). But is it not normal for the reconstruction function to take this into account? E.g. librosa istft does return signal with correct amplitude while tensorflow returns double.

C.) 
At any other frame_step there is severe amplitude modulation going on. See images below. This doesn't seem right at all. 

**UPDATE**: If I explicitly set `window_fn=tf.contrib.signal.inverse_stft_window_fn(frame_step)` in `inverse_stft` the output is correct. So it seems the `frame_step` parameter in `inverse_stft` is not being passed into the window function (which is also what the results hint at).

### Source code / logs

original data:
![22050 orig](https://user-images.githubusercontent.com/144230/35579363-64e51ef6-05de-11e8-8dc2-f4220265c2f8.png)

tensorflow output from frames + overlap_and_add:
![tensorflow 22050 frame l1024 s256](https://user-images.githubusercontent.com/144230/35579392-7907d798-05de-11e8-8971-64597d3e06d3.png)
![tensorflow 22050 frame l1024 s512](https://user-images.githubusercontent.com/144230/35579393-792fb060-05de-11e8-93af-efc2bd30d058.png)
![tensorflow 22050 frame l1024 s768](https://user-images.githubusercontent.com/144230/35579394-794b9898-05de-11e8-82e7-ecbb41feed9a.png)
![tensorflow 22050 frame l1024 s1024](https://user-images.githubusercontent.com/144230/35579397-7a3116fc-05de-11e8-815d-d4df17e86dc5.png)

tensorflow output from stft+istft:
![tensorflow 22050 stft l1024 s256](https://user-images.githubusercontent.com/144230/35579401-7ed0727a-05de-11e8-9cf2-e8dd06df9e05.png)
![tensorflow 22050 stft l1024 s512](https://user-images.githubusercontent.com/144230/35579402-7ee9d29c-05de-11e8-93e0-a6d676c7d5ae.png)
![tensorflow 22050 stft l1024 s768](https://user-images.githubusercontent.com/144230/35579403-7f01f818-05de-11e8-8779-e50b824b8aec.png)
![tensorflow 22050 stft l1024 s1024](https://user-images.githubusercontent.com/144230/35579404-7f272a34-05de-11e8-9b55-a41942db7eb1.png)

librosa output from stft+istft:
![librosa 22050 stft l1024 s256](https://user-images.githubusercontent.com/144230/35579408-834faee2-05de-11e8-8f44-e6ef0e797fb4.png)
![librosa 22050 stft l1024 s512](https://user-images.githubusercontent.com/144230/35579409-8385fa6a-05de-11e8-8d36-b96c35ba862a.png)
![librosa 22050 stft l1024 s768](https://user-images.githubusercontent.com/144230/35579410-83a5767e-05de-11e8-89bd-6d01c9a7cb8c.png)
![librosa 22050 stft l1024 s1024](https://user-images.githubusercontent.com/144230/35579411-83c791c8-05de-11e8-8b6c-7ef5508a66e5.png)

tensorflow code:

    from __future__ import print_function
    from __future__ import division
    
    import numpy as np
    import scipy.io.wavfile
    import math
    import random
    import matplotlib.pyplot as plt
    
    import tensorflow as tf
    out_prefix = 'tensorflow'
    
    
    def plot(data, title, do_save=True):
        plt.figure(figsize=(20,5))
        plt.plot(data[:3*frame_length])
        plt.ylim([-1, 1])
        plt.title(title)
        plt.grid()
        if do_save: plt.savefig(title + '.png')
        plt.show()
    
    
    def reconstruct_from_frames(x, frame_length, frame_step):
        name = 'frame'
        frames_T = tf.contrib.signal.frame(x, frame_length=frame_length, frame_step=frame_step)
        windowed_frames_T = frames_T * tf.contrib.signal.hann_window(frame_length, periodic=True)
        output_T = tf.contrib.signal.overlap_and_add(windowed_frames_T, frame_step=frame_step)
        return name, output_T
    
    
    def reconstruct_from_stft(x, frame_length, frame_step):
        name = 'stft'
        spectrograms_T = tf.contrib.signal.stft(x, frame_length, frame_step)
        output_T = tf.contrib.signal.inverse_stft(spectrograms_T, frame_length, frame_step)
        return name, output_T
    
    
    def test(fn, input_data):
        print('-'*80)
        tf.reset_default_graph()
        input_T = tf.placeholder(tf.float32, [None]) 
        name, output_T = fn(input_T, frame_length, frame_step)
    
        title = ""{}.{}.{}.l{}.s{}"".format(out_prefix, sample_rate, name, frame_length, frame_step)
        print(title)
    
        with tf.Session():
            output_data =  output_T.eval({input_T:input_data})
    
    #    output_data /= frame_length/frame_step/2 # tensorflow needs this to normalise amp
        plot(output_data, title)
        scipy.io.wavfile.write(title+'.wav', sample_rate, output_data)
    
    
    def generate_data(duration_secs, sample_rate, num_sin, min_freq=10, max_freq=500, rnd_seed=0, max_val=0):
        '''generate signal from multiple random sin waves'''
        if rnd_seed>0: random.seed(rnd_seed)
        data = np.zeros([duration_secs*sample_rate], np.float32)
        for i in range(num_sin):
            w = np.float32(np.sin(np.linspace(0, math.pi*2*random.randrange(min_freq, max_freq), num=duration_secs*sample_rate)))
            data += random.random() * w
        if max_val>0:
            data *= max_val / np.max(np.abs(data))
        return data
        
    
    frame_length = 1024
    sample_rate = 22050
    
    input_data = generate_data(duration_secs=1, sample_rate=sample_rate, num_sin=1, rnd_seed=2, max_val=0.5)
    
    title = ""{}.orig"".format(sample_rate)
    plot(input_data, title)
    scipy.io.wavfile.write(title+'.wav', sample_rate, input_data)
    
    for frame_step in [256, 512, 768, 1024]:
        test(reconstruct_from_frames, input_data)
        test(reconstruct_from_stft, input_data)
    
    print('done.')

librosa code:

    from __future__ import print_function
    from __future__ import division
    
    import numpy as np
    import scipy.io.wavfile
    import math
    import random
    import matplotlib.pyplot as plt
    
    import librosa.core as lc
    out_prefix = 'librosa'
    
    
    def plot(data, title, do_save=True):
        plt.figure(figsize=(20,5))
        plt.plot(data[:3*frame_length])
        plt.ylim([-1, 1])
        plt.title(title)
        plt.grid()
        if do_save: plt.savefig(title + '.png')
        plt.show()
    
    
    def reconstruct_from_stft(x, frame_length, frame_step):
        name = 'stft'
        stft = lc.stft(x, n_fft=frame_length, hop_length=frame_step)
        istft = lc.istft(stft, frame_step)
        return name, istft
    
    
    def test(fn, input_data):
        print('-'*80)
        name, output_data = fn(input_data, frame_length, frame_step)
    
        title = ""{}.{}.{}.l{}.s{}"".format(out_prefix, sample_rate, name, frame_length, frame_step)
        print(title)
    
    #    output_data /= frame_length/frame_step/2 # tensorflow needs this to normalise amp
        plot(output_data, title)
        scipy.io.wavfile.write(title+'.wav', sample_rate, output_data)
    
    
    def generate_data(duration_secs, sample_rate, num_sin, min_freq=10, max_freq=500, rnd_seed=0, max_val=0):
        '''generate signal from multiple random sin waves'''
        if rnd_seed>0: random.seed(rnd_seed)
        data = np.zeros([duration_secs*sample_rate], np.float32)
        for i in range(num_sin):
            w = np.float32(np.sin(np.linspace(0, math.pi*2*random.randrange(min_freq, max_freq), num=duration_secs*sample_rate)))
            data += random.random() * w
        if max_val>0:
            data *= max_val / np.max(np.abs(data))
        return data
        
    
    frame_length = 1024
    sample_rate = 22050
    
    input_data = generate_data(duration_secs=1, sample_rate=sample_rate, num_sin=1, rnd_seed=2, max_val=0.5)
    
    title = ""{}.orig"".format(sample_rate)
    plot(input_data, title)
    scipy.io.wavfile.write(title+'.wav', sample_rate, input_data)
    
    for frame_step in [256, 512, 768, 1024]:
        test(reconstruct_from_stft, input_data)
    
    print('done.')
"
16464,AssignAddVariableOp has no output,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (Using Go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: 9.1 / 7.0
- **GPU model and memory**: GTX 1060 6GB
- **Exact command to reproduce**: See below


### Describe the problem
According to the docs, AssignAddVariableOp ""Outputs the incremented value, which can be used to totally order the increments to this variable."". Without this feature, I get non deterministic behavior when reading the value of the variable at the same time as I update it. However, at least in the Go bindings, it returns an operation which has no outputs. I can work around this problem by using two calls to `sess.Run()`, but this is inelegant.

### Source code / logs
```
package main

import (
	""fmt""

	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	value1 := op.Const(s.SubScope(""zero""), float32(0))
	value2 := op.Const(s, float32(3.1415))
	handle := op.VarHandleOp(s, tf.Float, tf.ScalarShape())
	init := op.AssignVariableOp(s, handle, value1)
	update := op.AssignAddVariableOp(s, handle, value2)
	fmt.Println(""NumOutputs:"", update.NumOutputs())
	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, []tf.Output{update.Output(0)}, nil)
	if err != nil {
		panic(err)
	}
}
```
```
$ go run assign_demo.go 
NumOutputs: 0
panic: Tried to fetch data for 'AssignAddVariableOp:0', which produces no output.  To run to a node but not fetch any data, pass 'AssignAddVariableOp:0' as an argument to the 'target_node_names' argument of the Session::Run API.

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/assign_demo.go:32 +0x448
exit status 2
```"
16462,How to create a model checkpoint based on each step rather than time interval.? using TensorFlow-Slim api.,"slim.learning.train(
    train_op,
    logdir,
    number_of_steps=1000,
    **save_summaries_secs=300**,
    **save_interval_secs=600**):

The above api only supports to capture model checpoints periodically, but I need to checkpoint based on each step. How do achieve this using TesorFlow-Slim API.?

I am looking for parameters like this:

save_summaries_steps = 10,
save_interval_steps=10
    where the value 10 is the number of steps and that should be configurable."
16461,macOS mnist download error,"```
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(""MNIST_data/"", one_hot=True)
```
MacOS python3.6 is wrong
windows is right.


```
Traceback (most recent call last):
  File ""/Users/funny/Documents/AIML/test.py"", line 8, in <module>
    from tensorflow.examples.tutorials.mnist import input_data
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 63, in <module>
    from tensorflow.python.framework.framework_lib import *
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/framework_lib.py"", line 76, in <module>
    from tensorflow.python.framework.ops import Graph
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 37, in <module>
    from tensorflow.python.eager import context
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/eager/context.py"", line 27, in <module>
    from tensorflow.python.framework import errors
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors.py"", line 22, in <module>
    from tensorflow.python.framework import errors_impl as _impl
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 27, in <module>
    from tensorflow.python.util import compat
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 130, in <module>
    remove_undocumented(__name__, _allowed_symbols)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/all_util.py"", line 103, in remove_undocumented
    should_have = make_all(module_name, doc_string_modules)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/all_util.py"", line 55, in make_all
    for m in _reference_pattern.finditer(doc_module.__doc__)
TypeError: expected string or bytes-like object
```"
16458,How to parse multivalve feature using tf.feature_column and tf.data API ??,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16456,'InputFnOps' object has no attribute 'receiver_tensors',"
### System information
==TensorFlow installed from (source or binary)==
Source
== Python version ==
Python 2.7.13
== cat /etc/issue ==
Linux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""
== are we in docker ==
No
== compiler ==
c++ (Debian 6.3.0-18) 6.3.0 20170516
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
== uname -a ==
Linux orion 4.9.0-3-amd64 #1 SMP Debian 4.9.30-2+deb9u5 (2017-09-19) x86_64 GNU/Linux
== check pips ==
numpy (1.12.1)
protobuf (3.5.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.8)
tensorflow-transform (0.3.1)
== check for virtualenv ==
True
== tensorflow import ==
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)
== env ==
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

### Describe the problem
I ran into some incompatibility issues running tf.estimator.DNNClassifier with tf.contrib.learn.InputFnOps and tf.contrib.learn.Experiment.  
There has been a separate [solved issue](https://github.com/tensorflow/transform/issues/36): 
I have tried the bundle version, but it doesn't work for me:
tensorflow==1.3
tensorflow_transform==0.3.1
six==1.10.0
However, if I switch to tf.contrib.learn.DNNClassifier, the issue go away.
It is also suggested to use tf.estimator.DNNClassifier rather than tf.contrib.learn.DNNClassifier.  I would like to get tf.estimator.DNNClassifier work.

### Source code / logs
```
def build_estimator(config):
    m = tf.estimator.DNNClassifier(
   # m = tf.contrib.learn.DNNClassifier(
    					config=config,
    					feature_columns=deep_columns,
                        hidden_units=[100, 100, 100],
                        n_classes=2,
                        optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01)
                        )
    return m
```
```
def json_serving_input_fn():
	""""""Build the serving inputs.""""""
	inputs = {}
	for feat in INPUT_COLUMNS:
		inputs[feat.name] = tf.placeholder(shape=[None], dtype=feat.dtype)

	features = {
	  key: tf.expand_dims(tensor, -1)
	  for key, tensor in inputs.iteritems()
	}
	return tf.contrib.learn.InputFnOps(features, None, inputs)
```
```
def _experiment_fn(run_config, hparams):
    # num_epochs can control duration if train_steps isn't
    # passed to Experiment
    train_input = lambda: model.generate_input_fn(
        hparams.train_files,
        num_epochs=hparams.num_epochs,
        batch_size=hparams.train_batch_size,
    )
    # Don't shuffle evaluation data
    eval_input = lambda: model.generate_input_fn(
        hparams.eval_files,
        batch_size=hparams.eval_batch_size,
        shuffle=False
    )
    return tf.contrib.learn.Experiment(
        model.build_estimator(
            config=run_config
        )
```


Error message:
local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 440, in export_savedmodel
    serving_input_receiver.receiver_tensors,
AttributeError: 'InputFnOps' object has no attribute 'receiver_tensors'
"
16455,Set training=True in BatchNormalization layer causes evaluation error in custom Estimator model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu16.04
- **TensorFlow installed from (source or binary)**: pip install 
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**:  3.5.2
- **CUDA/cuDNN version**: 8.0/6
- **GPU model and memory**: GeForce 1080ti, 11G

### Describe the problem
I define a custom estimator model for classification following [this document](https://www.tensorflow.org/extend/estimators). **Cifar10** dataset is used for test and network framework is **xception** rewritten in tensorflow. But when using `estimator.train_and_evaluate()` to train and evaluate the model repeatedly, I find evaluation accuracy dones't improve while training accuracy is normally increasing with training. Inspired by tensorflow official [resnet estimator example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/resnet.py): <br>
```python
  with tf.variable_scope('conv_layer1'):
    net = tf.layers.conv2d(
        x,
        filters=64,
        kernel_size=7,
        activation=tf.nn.relu)
    net = tf.layers.batch_normalization(net)   # no training status, default is False
```
 
I turn off `training=is_training` in `tf.layers.batch_normalization()`, both training and evaluation do work normally. For estimator model_fn is used multiple times (see [issue 13895](https://github.com/tensorflow/tensorflow/issues/13895)),  so is this issue related to graph reuse in BN layer and if training status option could be set to enable BN layer to act differently during training and evaluation/predict?

BTW, same issue occurs when using `keras` or `slim` instead of `tf.layers` to construct network architecture.

### Source code / logs
**Network architecture:** <br>
```python
def tf_xception(features, input_shape, pooling=None, classes=2, is_training=True):
    # is_training = False  # manually set False to disable training option
    x = tf.layers.conv2d(features, 32, (3, 3), strides=(2, 2), use_bias=False, name='block1_conv1')
    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv1_bn')
    x = tf.nn.relu(x, name='block1_conv1_act')
    x = tf.layers.conv2d(x, 64, (3, 3), use_bias=False, name='block1_conv2')
    x = tf.layers.batch_normalization(x, training=is_training, name='block1_conv2_bn')
    x = tf.nn.relu(x, name='block1_conv2_act')

    residual = tf.layers.conv2d(x, 128, (1, 1), strides=(2, 2),
                      padding='same', use_bias=False)
    residual = tf.layers.batch_normalization(residual, training=is_training)

    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv1')
    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv1_bn')
    x = tf.nn.relu(x, name='block2_sepconv2_act')
    x = tf.layers.separable_conv2d(x, 128, (3, 3), padding='same', use_bias=False, name='block2_sepconv2')
    x = tf.layers.batch_normalization(x, training=is_training, name='block2_sepconv2_bn')

    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block2_pool')
    x = tf.add(x, residual, name='block2_add')

    residual = tf.layers.conv2d(x, 256, (1, 1), strides=(2, 2),
                      padding='same', use_bias=False)
    residual = tf.layers.batch_normalization(residual, training=is_training)

    x = tf.nn.relu(x, name='block3_sepconv1_act')
    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv1')
    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv1_bn')
    x = tf.nn.relu(x, name='block3_sepconv2_act')
    x = tf.layers.separable_conv2d(x, 256, (3, 3), padding='same', use_bias=False, name='block3_sepconv2')
    x = tf.layers.batch_normalization(x, training=is_training, name='block3_sepconv2_bn')

    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block3_pool')
    x = tf.add(x, residual, name=""block3_add"")

    residual = tf.layers.conv2d(x, 728, (1, 1), strides=(2, 2),
                      padding='same', use_bias=False)
    residual = tf.layers.batch_normalization(residual, training=is_training)

    x = tf.nn.relu(x, name='block4_sepconv1_act')
    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv1')
    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv1_bn')
    x = tf.nn.relu(x, name='block4_sepconv2_act')
    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block4_sepconv2')
    x = tf.layers.batch_normalization(x, training=is_training, name='block4_sepconv2_bn')

    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block4_pool')
    x = tf.add(x, residual, name=""block4_add"")

    for i in range(8):
        residual = x
        prefix = 'block' + str(i + 5)

        x = tf.nn.relu(x, name=prefix + '_sepconv1_act')
        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv1')
        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv1_bn')
        x = tf.nn.relu(x, name=prefix + '_sepconv2_act')
        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv2')
        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv2_bn')
        x = tf.nn.relu(x, name=prefix + '_sepconv3_act')
        x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')
        x = tf.layers.batch_normalization(x, training=is_training, name=prefix + '_sepconv3_bn')

        x = tf.add(x, residual, name=prefix+""_add"")

    residual = tf.layers.conv2d(x, 1024, (1, 1), strides=(2, 2),
                      padding='same', use_bias=False)
    residual = tf.layers.batch_normalization(residual, training=is_training)

    x = tf.nn.relu(x, name='block13_sepconv1_act')
    x = tf.layers.separable_conv2d(x, 728, (3, 3), padding='same', use_bias=False, name='block13_sepconv1')
    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv1_bn')
    x = tf.nn.relu(x, name='block13_sepconv2_act')
    x = tf.layers.separable_conv2d(x, 1024, (3, 3), padding='same', use_bias=False, name='block13_sepconv2')
    x = tf.layers.batch_normalization(x, training=is_training, name='block13_sepconv2_bn')

    x = tf.layers.max_pooling2d(x, (3, 3), strides=(2, 2), padding='same', name='block13_pool')
    x = tf.add(x, residual, name=""block13_add"")

    x = tf.layers.separable_conv2d(x, 1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')
    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv1_bn')
    x = tf.nn.relu(x, name='block14_sepconv1_act')

    x = tf.layers.separable_conv2d(x, 2048, (3, 3), padding='same', use_bias=False, name='block14_sepconv2')
    x = tf.layers.batch_normalization(x, training=is_training, name='block14_sepconv2_bn')
    x = tf.nn.relu(x, name='block14_sepconv2_act')
    # replace conv layer with fc
    x = tf.layers.average_pooling2d(x, (3, 3), (2, 2), name=""global_average_pooling"")
    x = tf.layers.conv2d(x, 2048, [1, 1], activation=None, name=""block15_conv1"")
    x = tf.layers.conv2d(x, classes, [1, 1], activation=None, name=""block15_conv2"")
    x = tf.squeeze(x, axis=[1, 2], name=""logits"")
    return x
```

**model_fn:** <br>
```python
def model_fn(features, labels, mode, params):
    # check if training stage
    if mode == tf.estimator.ModeKeys.TRAIN:
        is_training = True
    else:
        is_training = False
    input_tensor = features[""input""]
    logits = tf_xception(input_tensor, input_shape=(96, 96, 3), classes=10, is_training=is_training)
    probs = tf.nn.softmax(logits, name=""output_score"")
    predictions = tf.argmax(probs, axis=-1, name=""output_label"")
    onehot_labels = tf.one_hot(tf.cast(labels, tf.int32), 10)
    predictions_dict = {""score"": probs,
                        ""label"": predictions}
    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions_output = tf.estimator.export.PredictOutput(predictions_dict)
        return tf.estimator.EstimatorSpec(mode=mode,
                                          predictions=predictions_dict,
                                          export_outputs={
                                              tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: predictions_output
                                          })
    # calculate loss
    loss = tf.losses.softmax_cross_entropy(onehot_labels, logits)
    accuracy = tf.metrics.accuracy(labels=labels,
                                   predictions=predictions)
    if mode == tf.estimator.ModeKeys.TRAIN:
        lr = params.learning_rate
        # train optimizer
        optimizer = tf.train.RMSPropOptimizer(learning_rate=lr, decay=0.9)
        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
        tensors_to_log = {'batch_accuracy': accuracy[1]}
        logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=1000)
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op,
                                          training_hooks=[logging_hook])
    else:
        eval_metric_ops = {""accuracy"": accuracy}
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          eval_metric_ops=eval_metric_ops)
```

**If the training status set True in training and False in evaluation**<br>
**train logs:**<br>
```
INFO:tensorflow:Saving checkpoints for 1 into train_episode5/model.ckpt.
INFO:tensorflow:loss = 2.3025837, step = 1
INFO:tensorflow:batch_accuracy = 0.109375
INFO:tensorflow:global_step/sec: 3.50983
INFO:tensorflow:loss = 2.3048878, step = 101 (28.492 sec)
INFO:tensorflow:Saving checkpoints for 185 into train_episode5/model.ckpt.
INFO:tensorflow:Loss for final step: 2.3093615.
...
INFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-185
INFO:tensorflow:Saving checkpoints for 186 into train_episode5/model.ckpt.
INFO:tensorflow:loss = 2.2975698, step = 186
INFO:tensorflow:batch_accuracy = 0.09375
INFO:tensorflow:global_step/sec: 3.47248
INFO:tensorflow:loss = 2.3078504, step = 286 (28.798 sec)
INFO:tensorflow:Saving checkpoints for 374 into train_episode5/model.ckpt.
INFO:tensorflow:Loss for final step: 2.290754.
...
INFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-374
INFO:tensorflow:Saving checkpoints for 375 into train_episode5/model.ckpt.
INFO:tensorflow:loss = 2.2987142, step = 375
INFO:tensorflow:batch_accuracy = 0.140625
INFO:tensorflow:global_step/sec: 3.50966
INFO:tensorflow:loss = 2.0407405, step = 475 (28.493 sec)
INFO:tensorflow:Saving checkpoints for 560 into train_episode5/model.ckpt.
INFO:tensorflow:Loss for final step: 2.1280906.
...
INFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-560
INFO:tensorflow:Saving checkpoints for 561 into train_episode5/model.ckpt.
INFO:tensorflow:loss = 2.0747793, step = 561
INFO:tensorflow:batch_accuracy = 0.203125
INFO:tensorflow:global_step/sec: 3.31447
INFO:tensorflow:loss = 2.1767468, step = 661 (30.171 sec)
INFO:tensorflow:Saving checkpoints for 740 into train_episode5/model.ckpt.
INFO:tensorflow:Loss for final step: 1.9530052.
...
INFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-740
INFO:tensorflow:Saving checkpoints for 741 into train_episode5/model.ckpt.
INFO:tensorflow:loss = 1.9676144, step = 741
INFO:tensorflow:batch_accuracy = 0.296875
INFO:tensorflow:global_step/sec: 3.50441
INFO:tensorflow:loss = 1.8766258, step = 841 (28.536 sec)
INFO:tensorflow:Saving checkpoints for 930 into train_episode5/model.ckpt.
INFO:tensorflow:Loss for final step: 1.884157.
...
INFO:tensorflow:Restoring parameters from train_episode5/model.ckpt-930
INFO:tensorflow:Saving checkpoints for 931 into train_episode5/model.ckpt.
INFO:tensorflow:loss = 1.8624167, step = 931
INFO:tensorflow:batch_accuracy = 0.296875
INFO:tensorflow:global_step/sec: 3.30778
INFO:tensorflow:loss = 1.7580669, step = 1031 (30.232 sec)
INFO:tensorflow:Saving checkpoints for 1112 into train_episode5/model.ckpt.
INFO:tensorflow:Loss for final step: 1.9509349.
...
```

**eval log:**
```
INFO:tensorflow:Saving dict for global step 170: accuracy = 0.099306434, global_step = 170, loss = 2.3029344
INFO:tensorflow:Saving dict for global step 348: accuracy = 0.11751261, global_step = 348, loss = 2.2920265
INFO:tensorflow:Saving dict for global step 528: accuracy = 0.13106872, global_step = 528, loss = 2.5031097
INFO:tensorflow:Saving dict for global step 697: accuracy = 0.085986756, global_step = 697, loss = 30.668789
INFO:tensorflow:Saving dict for global step 871: accuracy = 0.10009458, global_step = 871, loss = 47931.96
...
```
**accuracy round initial 0.1 and loss increase ridiculously !!!!**

**If training status set False in both stage:**<br>
**train log:** similar to above train log 

**eval log:** <br>
```
INFO:tensorflow:Saving dict for global step 185: accuracy = 0.1012768, global_step = 185, loss = 2.3037012
INFO:tensorflow:Saving dict for global step 374: accuracy = 0.10001576, global_step = 374, loss = 2.3124988
INFO:tensorflow:Saving dict for global step 560: accuracy = 0.20081967, global_step = 560, loss = 2.0881999
INFO:tensorflow:Saving dict for global step 740: accuracy = 0.26134932, global_step = 740, loss = 2.0297167
INFO:tensorflow:Saving dict for global step 930: accuracy = 0.26379257, global_step = 930, loss = 1.9529407
INFO:tensorflow:Saving dict for global step 1112: accuracy = 0.3454445, global_step = 1112, loss = 1.832811
```
"
16454,ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible,"Traceback (most recent call last):
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 558, in merge_with
    new_dims.append(dim.merge_with(other[i]))
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 133, in merge_with
    self.assert_is_compatible_with(other)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 106, in assert_is_compatible_with
    other))
ValueError: Dimensions 1069539296 and 13528529576648672 are not compatible

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""Train.py"", line 117, in <module>
    train()
  File ""Train.py"", line 54, in train
    train_op=Evaluation.trainning(loss=loss1, learning_rate=0.0001)
  File ""/home/lihua/Documents/Projects/Project2018/trafficSignClassification/Evaluation.py"", line 36, in trainning
    train_op = optimizer.minimize(loss, global_step= global_step)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 315, in minimize
    grad_loss=grad_loss)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/training/optimizer.py"", line 386, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py"", line 560, in gradients
    in_grad.set_shape(t_in.get_shape())
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py"", line 443, in set_shape
    self._shape = self._shape.merge_with(shape)
  File ""/home/lihua/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py"", line 561, in merge_with
    raise ValueError(""Shapes %s and %s are not compatible"" % (self, other))
ValueError: Shapes (128, 4, 4, 1069539296) and (128, 4, 4, 13528529576648672) are not compatible


ubuntu16.04
tensorflow 1.4


"
16453,Updated roadmap?,The [TensorFlow roadmap](https://www.tensorflow.org/about/roadmap) was last updated a year ago (January 2017); could there be an update from the team on where TensorFlow is going in 2018?
16452,Semantic Segmentation API,"Hello, is there any plan to include Semantic Segmentation API in future Tensorflow releases, similar to [Tensorflow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection#tensorflow-object-detection-api) ?

There are other semantic segmentation repositories in Github, an awesome list is [here](https://github.com/mrgloom/awesome-semantic-segmentation), but I would like to see the implementation from Tensorflow organization.
Thanks"
16451,Parameter parsing error messages,"Parameter parsing error messages probably can be improved, e.g. 

`bazel-bin/tensorflow/core/profiler/profiler --profile_path=/tmp/for_tfprof/profile_20`

runs ok, but if cd to bazel-bin/tensorflow/core/profiler/ and

`./profiler --profile_path /tmp/for_tfprof/profile_20`

results in 

> ./profiler
> --profile_path
> /tmp/for_tfprof/profile_20
> Reading Files...
> Try to use a single --profile_path instead of graph_path,op_log_path,run_meta_path
> 2018-01-26 01:43:29.458032: F tensorflow/core/profiler/profiler.cc:206] Non-OK-status: ReadProtoFile(Env::Default(), FLAGS_graph_path, graph.get(), false) status: Not found: ; No such file or directory
> Aborted (core dumped)

"
16450,InvalidArgumentError (see above for traceback): sequence_length(0) <= 80 thrown by ctc_loss,"I am encountering this error thrown by `ctc_loss` and I have no idea what it means nor how to resolve it.

```
InvalidArgumentError (see above for traceback): sequence_length(0) <= 80
	 [[Node: CTCLoss = CTCLoss[ctc_merge_repeated=true, ignore_longer_outputs_than_inputs=false, preprocess_collapse_repeated=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](transpose_1/_455, Where/_445, GatherNd, reshape_1/_457)]]
	 [[Node: CTCLoss/_459 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_7988_CTCLoss"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

```"
16448,embedding lookup table in tensorflow serving,"Hi, I am trying to serve a NLP model in tensorflow serving. I am wondering how embedding matrix is being stored in tensorflow serving. If I deploy model to two servers, will the embedding matrix be a distributed table with sharding for looking up?"
16445,Unable to build image_retraining:label_image no such target ,"SUCCESS: bazel build --config opt tensorflow/examples/image_retraining:retrain
SUCCESS: bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos
FAILED : bazel build tensorflow/examples/image_retraining:label_image
ERROR: Skipping 'tensorflow/examples/image_retraining:label_image': no such target '//tensorflow/examples/image_retraining:label_image': target 'label_image' not declared in package 'tensorflow/examples/image_retraining' defined by /Users/jeff/tensorflow/tensorflow/examples/image_retraining/BUILD
WARNING: Target pattern parsing failed.
ERROR: no such target '//tensorflow/examples/image_retraining:label_image': target 'label_image' not declared in package 'tensorflow/examples/image_retraining' defined by /Users/jeff/tensorflow/tensorflow/examples/image_retraining/BUILD
INFO: Elapsed time: 0.320s
FAILED: Build did NOT complete successfully (0 packages loaded)
"
16444,Documentation update,"The mean squared error is described as following

mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=tf.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)

The default reduction method is MEAN in v1.4"
16442,"why save model and deploy in android device, the outputs are not the same as in ubuntu?","OS Platform and Distribution :Ubuntu 14.04.5 LTS   && Android 8.0
TensorFlow installed from source
TensorFlow version :1..4.0
Python version : 2.7.6
Bazel version :0.4.5
GCC/Compiler version:4.8.4
CUDA/cuDNN version:8.0
GPU model and memory: GTX1080, 8G
Exact command to reproduce:

step 1. clone code from https://github.com/davidsandberg/facenet
step 2. in the file src/compare.py, add the follow code after line 90, after align.detect_face.create_mtcnn 
            output_node_names=['pnet/prob1','pnet/conv4-2/BiasAdd','pnet/conv1/BiasAdd','rnet/prob1','rnet/conv5-2/conv5-2','onet/prob1','onet/conv6-2/conv6-2','onet/conv6-3/conv6-3']
            output_graph_def = tf.graph_util.convert_variables_to_constants(sess, sess.graph.as_graph_def(), output_node_names) #sess.graph_def,
            with tf.gfile.FastGFile(""mtcnn.pb"", mode = 'wb') as f:
                f.write(output_graph_def.SerializeToString())

step 3. run the command:  python src/compare.py ./data/20170512-110547 ./data/images/Anthony_Hopkins_0001.jpg ./data/images/Anthony_Hopkins_0002.jpg
will create the model file mtcnn.pb

step 4 deploy the file mtcnn.pb to android app, validate with the file Anthony_Hopkins_0001.jpg, indeed it can fetch the results for the outputs such as 'pnet/prob1','pnet/conv4-2/BiasAdd',  but the values are difference with the results from the facenet project run on ubuntu,

the query is what is the cause to the difference? the  way to create the mtcnn.pb is wrong? still need to optimize it to adapt android device? or there is something wrong with Tensorflow for mobile device?

the follow is the log show the difference:
with the same input:00.28515625,-0.24609375,-0.59765625
but the output is difference

Android output
 	Line 5555: 01-26 11:34:20.793 I/lxr     (22967): img00.28515625,-0.24609375,-0.59765625
	Line 5795: 01-26 11:34:21.326 I/lxr     (22967): mapWidth 70 mapHeight 70
	Line 5796: 01-26 11:34:21.327 I/lxr     (22967): outValue:0.9998832,1.16751995E-4
	Line 5797: 01-26 11:34:21.327 I/lxr     (22967): outReg:-0.068167016,-0.2052449,0.06884944,0.1512082

Ubuntu output
img_y (1, 150, 150, 3)
img_y0 [ 0.28515625 -0.24609375 -0.59765625]
out0 shape (1, 70, 70, 4)
out1 shape (1, 70, 70, 2)
out0 [-0.07926445 -0.20101449  0.06468102  0.16017048]
out1 [  9.99792397e-01   2.07666759e-04]
"
16428,"want keras to work on GPU, but actually on CPU","System information:
OS Platform and Distribution: Linux Ubuntu 16.04:
TensorFlow installed from anaconda:
Keras installed from pip
TensorFlow version 1.4.1
Bazel version : None:
CUDA/cuDNN version : CUDA V7.5.17, CuDNN v6.0:
GPU: GeForce GeForce GTX 1050 Ti(3.94GB):
Code: [mnist_mlp.py](https://github.com/antoniosehk/keras-tensorflow-windows-installation/blob/master/examples/mnist_mlp.py)

When run the code, it shows the following log:

> Using TensorFlow backend.
2018-01-25 17:31:16.572013: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-25 17:31:16.679268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-25 17:31:16.679499: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:
name: GeForce GTX 1050 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.468
pciBusID: 0000:01:00.0
totalMemory: 3.94GiB freeMemory: 3.46GiB
2018-01-25 17:31:16.679513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)

Have no idea why it shows: 

> 2018-01-25 17:31:16.679268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero

And the code is actually running on the CPU, although the log shows the info of GPU

What have tried: stackoverflow :)"
16424,Dependency on old version of bleach (1.5),"Bleach 1.5 came out Nov 4th 2016 and this is old enough to cause dependency issues for projects that stayed up to date with Bleach.

In particular, this causes issues for Jupyter users."
16423,Windows 10 Cmake GPU nvcc.exe error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
r1.5
- **Python version**: 
3.6
- **GCC/Compiler version (if compiling from source)**:
Visual Studio 2017
- **CUDA/cuDNN version**:
9.1
- **GPU model and memory**:
1080Ti
- **Exact command to reproduce**:

**Cmake Command:**
```
cmake -G ""Visual Studio 15 2017 Win64"" -T host=x64 -DCMAKE_BUILD_TYPE=""
Release"" -DSWIG_EXECUTABLE='C:\ProgramData\Chocolatey\bin\swig.exe' -Dtensorflow_ENABLE_GPU=ON -Dtensorflow_CUDA_VERSION
=9.1 -Dtensorflow_CUDNN_VERSION=7 -Dtensorflow_WIN_CPU_SIMD_OPTIONS=""/arch:AVX2"" -DCUDA_CUDART_LIBRARY=D:\NVIDIA\CUDA\v9
.1 -DCUDNN_HOME='D:\NVIDIA\CUDA\v9.1' ..
```

**Build command**
```
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\amd64\MSBuild.exe"" /m:4 /p:Configuration=Release .\tf_core_gpu_kernels.vcxproj
```
### Describe the problem
Cmake creates a bad command to send to nvcc.exe

In tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:202L
there is an error with the resulting command.

There is issues with spacing, "";"" in between arguments and others.

**Command Ran**
`C:/NVIDIA/CUDA/v9.1/bin/nvcc.exe -M -D__CUDACC__ D:/tensorflow/tensorflow/core/kernels/adjust_contrast_op_gpu.cu.cc -o D:/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.NVCC-depend -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1 -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true;  -DNVCC -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus`

Multable invalid cmake varables

```
${CCBIN} = -ccbin;C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin 
${nvcc_flags} = -m64;-DSQLITE_OMIT_LOAD_EXTENSION;-DEIGEN_AVOID_STL_ARRAY;-DNOMINMAX;-D_WIN32_WINNT=0x0A00;-DLANG_CXX11;-DCOMPILER_MSVC;-DWIN32;-DOS_WIN;-D_MBCS;-DWIN64;-DWIN32_LEAN_AND_MEAN;-DNOGDI;-DPLATFORM_WINDOWS;-DTENSORFLOW_USE_EIGEN_THREADPOOL;-DEIGEN_HAS_C99_MATH;-DTF_COMPILE_LIBRARY;-DGRPC_ARES=0;-DTF_USE_SNAPPY;-DGOOGLE_CUDA=1;-DTF_EXTRA_CUDA_CAPABILITIES=6.1
${nvcc_host_compiler_flags} = -Xcompiler;,""/DWIN32"",""/D_WINDOWS"",""/W3"",""/GR"",""/EHsc"",""/MP"",""/arch:AVX2"",""/MD"",""/O2"",""/Ob2"",""/DNDEBUG"",""/D_ITERATOR_DEBUG_LEVEL=0""  
${depends_CUDA_NVCC_FLAGS} = -gencode;arch=compute_61,code=""sm_61,compute_61"";--include-path;D:/tensorflow/tensorflow/contrib/cmake/build/Release;--expt-relaxed-constexpr;-ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include ;-ID:/tensorflow ;-ID:/tensorflow/tensorflow/contrib/cmake/build ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive ;-ID:/tensorflow/third_party/eigen3 ;-ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash ;-ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public ;-ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src ;-ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite ;-ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include ;-ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy ;-IC:/NVIDIA/CUDA/v9.1 ;-IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include ;-ID:/tensorflow/third_party/gpus
```


Should be
```
${CCBIN} = -ccbin ""C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/bin""
${nvcc_flags} = -m64 -DSQLITE_OMIT_LOAD_EXTENSION -DEIGEN_AVOID_STL_ARRAY -DNOMINMAX -D_WIN32_WINNT=0x0A00 -DLANG_CXX11 -DCOMPILER_MSVC -DWIN32 -DOS_WIN -D_MBCS -DWIN64 -DWIN32_LEAN_AND_MEAN -DNOGDI -DPLATFORM_WINDOWS -DTENSORFLOW_USE_EIGEN_THREADPOOL -DEIGEN_HAS_C99_MATH -DTF_COMPILE_LIBRARY -DGRPC_ARES=0 -DTF_USE_SNAPPY -DGOOGLE_CUDA=1 -DTF_EXTRA_CUDA_CAPABILITIES=6.1 
${nvcc_host_compiler_flags} = -Xcompiler ""/DWIN32,/D_WINDOWS,/W3,/GR,/EHsc,/MP,/arch:AVX2,/MD,/O2,/Ob2,/DNDEBUG,/D_ITERATOR_DEBUG_LEVEL=0"" 
${depends_CUDA_NVCC_FLAGS} = -gencode arch=compute_61,code=\""sm_61,compute_61\"" --include-path D:/tensorflow/tensorflow/contrib/cmake/build/Release --expt-relaxed-constexpr -ftz=true
${CUDA_NVCC_INCLUDE_ARGS} = -IC:/NVIDIA/CUDA/v9.1/include -ID:/tensorflow -ID:/tensorflow/tensorflow/contrib/cmake/build -ID:/tensorflow/tensorflow/contrib/cmake/build/external/zlib_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/gif_archive/giflib-5.1.4 -ID:/tensorflow/tensorflow/contrib/cmake/build/external/png_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/jpeg_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/lmdb -ID:/tensorflow/tensorflow/contrib/cmake/build/external/eigen_archive -ID:/tensorflow/third_party/eigen3 -ID:/tensorflow/tensorflow/contrib/cmake/build/gemmlowp/src/gemmlowp -ID:/tensorflow/tensorflow/contrib/cmake/build/jsoncpp/src/jsoncpp -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive -ID:/tensorflow/tensorflow/contrib/cmake/build/external/farmhash_archive/util -ID:/tensorflow/tensorflow/contrib/cmake/build/external/highwayhash -ID:/tensorflow/tensorflow/contrib/cmake/build/cub/src/cub -ID:/tensorflow/tensorflow/contrib/cmake/build/external/nsync/public -ID:/tensorflow/tensorflow/contrib/cmake/build/protobuf/src/protobuf/src -ID:/tensorflow/tensorflow/contrib/cmake/build/re2/install/include -ID:/tensorflow/tensorflow/contrib/cmake/build/external/sqlite -ID:/tensorflow/tensorflow/contrib/cmake/build/grpc/src/grpc/include -ID:/tensorflow/tensorflow/contrib/cmake/build/snappy/src/snappy -IC:/NVIDIA/CUDA/v9.1 -IC:/NVIDIA/CUDA/v9.1/extras/CUPTI/include -ID:/tensorflow/third_party/gpus

```"
16412,Documentation on build from source is unclear,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh


python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.4.1-7-gaa03bfc', '1.4.1')
built and installed from source with
git checkout r1.4
bazel build -c opt --copt=-march=""haswell"" --config=cuda --verbose_failures --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package >pip_package_build2.log 2>&1
note: incompatible path flag is required with R1.4 at this time per https://github.com/tensorflow/tensorflow/issues/15492
ubuntu 16.04
Cuda 9.1, cudnn 7.0.4
gcc --version
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
uname -r
4.4.0-104-generic
Bazel 0.9

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
It is unclear how to build and install the entire package purely from source
I will attempt to log what I have done so far
clone and build TF R1.4 for cuda
install wheel into local directory  (sudo pip install /tmp/tensorflow-pkg/tensorflow*.whl -t ~/mytf_r1.4_c9.1
export PYTHONPATH=~/mytf_r1.4_c9.1
move tensorflow directory

install common_voice files to ~/Common_voice

per native client build from source instructions: https://github.com/mozilla/DeepSpeech/blob/master/native_client/README.md
git clone tensorflow
cd tensorflow
git checkout r1.4
ln -s ../DeepSpeech/native_client ./
./configure
edit native_client/BUILD
comment out the following:
#    tfcompile_flags = select({
#        ""//tensorflow:rpi3"": str('--target_triple=""armv6-linux-gnueabihf"" --target_cpu=""cortex-a53"" --target_features=""+neon-fp-armv8""'),
#        ""//conditions:default"": str('')
#    }),
bazel build -c opt --copt=-O3 --incompatible_load_argument_is_label=false //tensorflow:libtensorflow_cc.so //tensorflow:libtensorflow_framework.so //native_client:deepspeech //native_client:deepspeech_utils //native_client:libctc_decoder_with_kenlm.so //native_client:generate_trie
at this point all the native client binaries are in
~/tensorflow/bazel-bin/native_client
levinth@zt-gpu-lin:~/DeepSpeech/native_client$ ls ~/tensorflow/bazel-bin/native_client/
generate_trie
generate_trie-2.params
generate_trie.runfiles
generate_trie.runfiles_manifest
libctc_decoder_with_kenlm.so
libctc_decoder_with_kenlm.so-2.params
libctc_decoder_with_kenlm.so.runfiles
libctc_decoder_with_kenlm.so.runfiles_manifest
libdeepspeech.a
libdeepspeech.a-2.params
libdeepspeech.pic.a
libdeepspeech.pic.a-2.params
libdeepspeech.so
libdeepspeech.so-2.params
libdeepspeech_utils.a
libdeepspeech_utils.a-2.params
libdeepspeech_utils.pic.a
libdeepspeech_utils.pic.a-2.params
libdeepspeech_utils.so
libdeepspeech_utils.so-2.params
_objs

cd ../Deepspeech/native_client
export TFDIR ~/tensorflow
make deepspeech


at this point however the native client shared objects are still in bazel-bin/native client and have not been installed. the invocation of Deepspeech.py fails as it cannot find the shared objects
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv >deepspeech_1.log 2>&1
tensorflow.python.framework.errors_impl.NotFoundError: native_client/libctc_decoder_with_kenlm.so: cannot open shared object file: No such file or directory

the native_client/Makefile has sections for bindings and install..so try
sudo make install
and this still generates the error as install does not put
~/tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so
into /usr/local/lib 
though deepspeech.so and deepspeech_utils.so are installed there.

manually copy /tensorflow/bazel-bin/native_client/libctc_decoder_with_kenlm.so to ~/DeepSpeech/native_client and set permissions
at this point the invocation now starts running but complains about
------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
even though /usr/local/lib is in the $LD_LIBRARY_PATH

invoking
python DeepSpeech.py --train_files ../Common_voice/cv-valid-train.csv,../Common_voice/cv-other-train.csv --dev_files ../Common_voice/cv-valid-dev.csv --test_files ../Common_voice/cv-valid-test.csv --display_step 1 --validation_step 10

------------------------------------------------------------------------
WARNING: libdeepspeech failed to load, resorting to deprecated code
         Refer to README.md for instructions on installing libdeepspeech
------------------------------------------------------------------------
I STARTING Optimization
Loading the LM will be faster if you build a binary file.
Reading data/lm/lm.binary
----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100
terminate called after throwing an instance of 'lm::FormatLoadException'
  what():  native_client/kenlm/lm/read_arpa.cc:65 in void lm::ReadARPACounts(util::FilePiece&, std::vector<long unsigned int>&) threw FormatLoadException.
first non-empty line was ""version https://git-lfs.github.com/spec/v1"" not \data\. Byte: 43

I clearly have not figured this out
:-)
 
"
16411,Source code / logs,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16410,..,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16407,Creating placeholder with `np.uint32` dtype fails,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes custom snippet below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
not applicable
- **GCC/Compiler version (if compiling from source)**:
not applicable
- **CUDA/cuDNN version**:
CUDA 8.5
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
np.uint32 dtype is not supported while creating placeholder

### Source code / logs
```python
import tensorflow as tf
import numpy as np
print(tf.placeholder(np.int32, [None], 'ph1'))
print(tf.placeholder(np.uint32, [None], 'ph2'))
```
Line 3 works, line 4 fails.

Console Output:
```txt
Tensor(""ph1:0"", shape=(?,), dtype=int32)
Traceback (most recent call last):
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 126, in make_type
    v = dtypes.as_dtype(v).base_dtype
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\framework\dtypes.py"", line 595, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value <class 'numpy.uint32'> to a TensorFlow DType.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""E:/scaffold-ext/scaffold_ext/analysis/ann/bug.py"", line 4, in <module>
    print(tf.placeholder(np.uint32, [None], 'ph2'))
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 1599, in placeholder
    return gen_array_ops._placeholder(dtype=dtype, shape=shape, name=name)
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 3083, in _placeholder
    dtype = _execute.make_type(dtype, ""dtype"")
  File ""...\AppData\Local\Continuum\anaconda3\lib\site-packages\tensorflow\python\eager\execute.py"", line 129, in make_type
    (arg_name, repr(v)))
TypeError: Expected DataType for argument 'dtype' not <class 'numpy.uint32'>.
```"
16405,tf.contrib.framework.sort failing with <<...has no attribute 'sort'>> in TF windows ,"### [Problem] : Can't use tf.contrib.framework.sort in my tensorflow code as it's failing to find the 'sort' attribute

Source code:
```
import tensorflow as tf

x = tf.placeholder(tf.float32, shape=(10, 10))
y = tf.contrib.framework.sort(x)

with tf.Session() as sess:
    rand_array = np.random.rand(10, 10)
    print(sess.run(y, feed_dict={x: rand_array})) 
```

Error log:
```
 <module>
    y = tf.contrib.framework.sort(x)
AttributeError: module 'tensorflow.contrib.framework' has no attribute 'sort'
```

### System information
- I've been trying to use the tf.contrib.framework.sort  within a custom loss function but issue being reproduced with a simple call to the tf.contrib.framework.sort
- Windows 10 64 bit
- TF installed with native pip3
- TF version: 1.4.0
- Python version: 3.6.2 
- TF with CPU support only
"
16403,1D Convolution in Tensorflow Serving,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux
- **TensorFlow installed from (source or binary)**: tensorflow binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: 3.6
- **CUDA/cuDNN version**: 9.0, 7.0
- **GPU model and memory**: GTX 1050

### Describe the problem
The Problem is a little bit hard to reproduce, I guess because so many steps are involved.
So, the basic scenario is, that I am using keras to train a model in python. Here is the model I am using:

`
           input = Input(shape=(200, 8))
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(input)
            x = Conv1D(filters=128, kernel_size=7, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=128, kernel_size=3, activation=""relu"", padding=""same"")(x)
            x = Conv1D(filters=2, kernel_size=1, activation=""softmax"")(x)

`

Now, I extract the graph and I am saving graph and weights with the ModelBundleBuilder:

`
session = K.get_session()

        signature = tf.saved_model.signature_def_utils.build_signature_def(
            inputs={'input': tf.saved_model.utils.build_tensor_info(self._get_model().inputs[0])},
            outputs={'output': tf.saved_model.utils.build_tensor_info(self._get_model().outputs[0])},
            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME
        )

        b = tf.saved_model.builder.SavedModelBuilder(filename)
        legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')
        b.add_meta_graph_and_variables(session,
                                       [tf.saved_model.tag_constants.SERVING],
                                       signature_def_map={
                                           tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature},
                                       legacy_init_op=legacy_init_op)
        b.save()
`

If I am loading the model via python, everything works as expected.

Now I am deploying the model into TF serving and using protobuf / gRPC to make the prediction via Java. I am converting a 3D float array to a TensorProto like this:

`
TensorShapeProto.Dim dim1 = TensorShapeProto.Dim.newBuilder()
                .setSize(data.length).build();

        TensorShapeProto.Dim dim2 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0].length).build();

        TensorShapeProto.Dim dim3 = TensorShapeProto.Dim.newBuilder()
                .setSize(data[0][0].length).build();

        TensorShapeProto shape = TensorShapeProto.newBuilder()
                .addDim(dim1).addDim(dim2).addDim(dim3).build();

        TensorProto.Builder builder = TensorProto.newBuilder()
                .setDtype(DataType.DT_FLOAT)
                .setTensorShape(shape);

        for(int i = 0; i < data.length; i++) {
            for(int j = 0; j < data[0].length; j++) {
                for(int k = 0; k < data[0][0].length; k++) {
                    builder.addFloatVal(data[k][j][i]);
                }
            }
        }

        return builder.build();
`

And do the predicition like this:

`
public class ModelClientImpl implements ModelClient {

    private String host;
    private Integer port;
    private ManagedChannel channel;
    private PredictionServiceGrpc.PredictionServiceBlockingStub stub;

    public void init() {
        channel = ManagedChannelBuilder
                .forAddress(getHost(), getPort())
                .usePlaintext(true)
                .build();

        stub = PredictionServiceGrpc.newBlockingStub(channel);
    }

    @Override
    public Map<String, TensorProto> predict(final String signatureName, Map<String, TensorProto> inputs) {
        final Predict.PredictResponse response = stub.predict(createRequest(signatureName, inputs));

        return response.getOutputsMap();
    }

    protected Predict.PredictRequest createRequest(final String signatureName, final Map<String, TensorProto> inputs) {
        final Model.ModelSpec modelSpec = Model.ModelSpec.newBuilder()
                .setName(signatureName)
                .setSignatureName(""serving_default"").build();

        final Predict.PredictRequest.Builder builder = Predict.PredictRequest.newBuilder()
                .setModelSpec(modelSpec)
                .putAllInputs(inputs);

        return builder.build();
    }

    public String getHost() {
        return host;
    }

    public void setHost(String host) {
        this.host = host;
    }

    public Integer getPort() {
        return port;
    }

    public void setPort(Integer port) {
        this.port = port;
    }

    @Override
    public void close() throws Exception {
        channel.shutdown().awaitTermination(5, TimeUnit.DAYS);
    }
}

`

But the prediction is totally different from python. Does anybody know if this is a bug or is something wromg with 1dconv?
"
16400,"[doc] link to ""How to Use t-SNE Effectively"" from embeddings is broken","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)  NO (since Web page problem)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:
- **TensorFlow installed from (source or binary) binary**:
- **TensorFlow version (use command below) 1.5.0rc0 **:
- **Python version  3.5.1**: 
- **Bazel version (if compiling from source) NOT USED**:
- **GCC/Compiler version (if compiling from source) NOT USED**:
- **CUDA/cuDNN version NOT USED**:
- **GPU model and memory NOT USED **:
- **Exact command to reproduce DOC Problem. Just look https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/**:

### Describe the problem
- Link to to ""How to Use t-SNE Effectively"" is broken.
- The page link is follows (before junmping)
  - https://www.tensorflow.org/programmers_guide/embedding
  - 404 page is following URL 
     - https://www.tensorflow.org/programmers_guide/distill.pub/2016/misread-tsne/
- Original page should be follows. (the URL in embedding.md should rewrite to follows)
  -   https://distill.pub/2016/misread-tsne/

### Source code / logs
- The problem code is follows.
  - https://github.com/tensorflow/tensorflow/blame/v1.5.0-rc1/tensorflow/docs_src/programmers_guide/embedding.md#L123

"
16399,Does TensorFlow 1.5 support CUDA 9.1?,"My notebook has an MX150 display adapter, someone said that it's available with CUDA 9.1"
16397,"S3 accessing reports ""Curl returned error code 6"" after AWS SDK upgrading to 1.3.15","### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0rc1 (tag) and master (2e5ff39e)
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 4.8
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: /
- **Exact command to reproduce**: 

### Describe the problem

TensorFlow 1.4.X was working well with S3 in my environment. After upgrading to 1.5.0rc1, I found that S3 could not be accessed. ""Curl returned error code 6"" is reported.

I noticed that AWS SDK had been upgraded from 1.0.90 to 1.3.15 in r1.5 and master branches. Thus, I pulled the master (2e5ff39e) and tried to change AWS SDK 1.3.15 into 1.0.90 in `tensorflow/workspace.bzl`. After this modification, it works well!

I tried with both AWS S3 (with http proxy) and Minio (localhost), and the results are the same. (AWS SDK 1.0.90 is Ok, but 1.3.15 reports error)

I guess there might be some incompatible changes after AWS SDK 1.3.15. Could you please take a look? Thanks! @yongtang

I noticed that AWS SDK required gcc 4.9, but I was using 4.8. Thus, this issue might be related to the old versions of gcc and glib on my server. I will try with some new systems as well.

### Source code / logs

Logs when using TensorFlow master (2e5ff39e):

```
>>> import tensorflow as tf
>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')
2018-01-25 15:34:45.145317: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1
2018-01-25 15:34:45.145354: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0
2018-01-25 15:34:45.145367: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default
2018-01-25 15:34:45.145383: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections2 and scheme http
2018-01-25 15:34:45.145401: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-01-25 15:34:45.145414: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-01-25 15:34:45.145456: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/credentials for reading.
2018-01-25 15:34:45.145468: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:45.145479: I tensorflow/core/platform/s3/aws_logging.cc:54] Unable to open config file /home/xxxxxxxx//.aws/config for reading.
2018-01-25 15:34:45.145487: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:45.145495: I tensorflow/core/platform/s3/aws_logging.cc:54] Credentials have expired attempting to repull from EC2 Metadata Service.
2018-01-25 15:34:45.145614: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-01-25 15:34:45.145628: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.146625: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 28
2018-01-25 15:34:46.146655: E tensorflow/core/platform/s3/aws_logging.cc:60] Http request to Ec2MetadataService failed.
2018-01-25 15:34:46.146666: I tensorflow/core/platform/s3/aws_logging.cc:54] Failed to reload configuration.
2018-01-25 15:34:46.146715: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-01-25 15:34:46.146849: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool grown by 2
2018-01-25 15:34:46.146863: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.147677: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6
2018-01-25 15:34:46.147704: W tensorflow/core/platform/s3/aws_logging.cc:57] If the signature check failed. This could be because of a time skew. Attempting to adjust the signer.
2018-01-25 15:34:46.147716: W tensorflow/core/platform/s3/aws_logging.cc:57] Request failed, now waiting 0 ms before attempting again.
2018-01-25 15:34:46.147802: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:34:46.148107: E tensorflow/core/platform/s3/aws_logging.cc:60] Curl returned error code 6
...
False
```

Logs after replacing AWS SDK 1.3.15 with 1.0.90:

```
>>> import tensorflow as tf
>>> tf.gfile.Exists('s3://xxxxxxxxx/f.txt')
2018-01-25 15:44:29.134077: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/config and using profilePrefix = 1
2018-01-25 15:44:29.134108: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing config loader against fileName /home/xxxxxxxx//.aws/credentials and using profilePrefix = 0
2018-01-25 15:44:29.134121: I tensorflow/core/platform/s3/aws_logging.cc:54] Setting provider to read credentials from /home/xxxxxxxx//.aws/credentials for credentials file and /home/xxxxxxxx//.aws/config for the config file , for use with profile default
2018-01-25 15:44:29.134133: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating HttpClient with max connections -864887560 and scheme Creating HttpClient with max connections %d and scheme %s
2018-01-25 15:44:29.134147: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 2
2018-01-25 15:44:29.134157: I tensorflow/core/platform/s3/aws_logging.cc:54] Creating Instance with default EC2MetadataClient and refresh rate 900000
2018-01-25 15:44:29.134176: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:29.134184: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:29.134223: I tensorflow/core/platform/s3/aws_logging.cc:54] Initializing CurlHandleContainer with size 25
2018-01-25 15:44:29.134264: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:29.134273: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:29.134429: I tensorflow/core/platform/s3/aws_logging.cc:54] Pool successfully grown by 2
2018-01-25 15:44:29.134442: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
2018-01-25 15:44:30.048051: I tensorflow/core/platform/s3/aws_logging.cc:54] Found credential in environment with access key id ********************
2018-01-25 15:44:30.048084: I tensorflow/core/platform/s3/aws_logging.cc:54] Found secret key
2018-01-25 15:44:30.048159: I tensorflow/core/platform/s3/aws_logging.cc:54] Connection has been released. Continuing.
True
```"
16396,tf.image.resize_image_with_crop_or_pad() is taking too much time to process an image or batch of images,"ISSUE: 
I am stuck in fine tuning imagenet data rendering speed for the reason being tf.image.resize_image_with_crop_or_pad() is taking too much time to process an image or batch of images. 
```Testing was done by preprocessing ONE image (no batching) and an observed latency was `~160msec`. ```

NOTE: I did not observe memory pressure or any possible compute bottleneck during this time. 
Description:
I have a tool to load ImageNet dataset which reads the dataset from processed TFRecords and returns an iterator object to iterate over the datasets with a shuffle and repeat.
I have been observing slow rendering of data due to some overtime by certain steps in image (batch of images) pre-preprocessing. This is really impacting in data rendering speed and hence the slow training of models(Reference here is AlexNet). 

Here is the `snip` of time taken for each operation,
<img width=""1425"" alt=""imagenet_loader_perf_profile"" src=""https://user-images.githubusercontent.com/35795681/35406079-d4ec3584-01bc-11e8-9ba6-50e4dabba29e.png"">

Here is a `snip`  of data preprocess methods,
```
def _parse_example_proto(dataset, height, width, depth, dtype, num_parallel_calls):
    def _input_parser(record):
        keys_to_features = {
            ""image/encoded"": tf.FixedLenFeature((), tf.string, default_value=""""),
            ""image/format"": tf.FixedLenFeature((), tf.string, default_value=""jpeg""),
            ""image/class/label"": tf.FixedLenFeature([], dtype=tf.int64, default_value=-1),
        }
        parsed = tf.parse_single_example(record, keys_to_features)
        image = tf.image.decode_jpeg(parsed[""image/encoded""], channels=depth)
        image = tf.image.convert_image_dtype(image, dtype=dtype)
        label = tf.cast(parsed[""image/class/label""], tf.int32)
        return image, label
    return dataset.map(_input_parser, num_parallel_calls=num_parallel_calls)

def _dataset_preprocess_fn(dataset, height, width, num_parallel_calls):
    def _preprocess_fn(image, label):
        image = tf.image.resize_image_with_crop_or_pad(image, height + 8, width + 8 )
        image = tf.random_crop(image, [height, width, 3])
        image = tf.image.random_flip_left_right(image)
        image = (ISTD * image) + MEAN  #dataset level MEAN
        return image, label
    return dataset.map(_preprocess_fn, num_parallel_calls=num_parallel_calls)
```
### System information
- **OS Platform and Distribution *:  Linux Centos 7.2
- **TensorFlow installed from (source or binary)**: 1.4.0 rc1 ( commit hash badd356)
- **TensorFlow version (use command below)**:  b'v1.3.0-rc1-4546-gef196f3' 1.4.0-rc1
- **Python version**:  3.4.5
- **Bazel version (if compiling from source)**: Build label: 0.8.1
- **CUDA/CUDAnn version**: CUDA 9 and  CUDAnn 7.0
- **GPU model and memory**: Volta 100, 16GiB


I am using Amazon Instance and here are the details,
```
GPUs - Tesla V100
GPU Memory (GB): 16
vCPUs : 8
Memory (GB):61
Network Bandwidth: Upto 10Gbps
EBS Bandwidth: 1.5 Gbps
```
"
16393,tensorflow object detection issue,"i developed the custom object detector with some 96 images (train 76 and test 20) after trained 200k steps my losses(1) goes down correctly....after that i create the interference graph all ok..........but my question is when i run my code to detect the object from video it not able to find the correct object ........ thanks in advance.....

actually i think we have a enough image for the dataset...because my dataset can able to detect the object from pre built video....so....it can detect the object from webcam also but not ...i dont kw the exact problem.....i think the resolution may b differ...thats y it not able to detect the object....﻿


use predefined ""object detection"" code with my own data set
ubuntu 16.04
tensorflow installed in anaconda environment
tensorflow 1.4.0
bazel 0.4.5
cpu version"
16392,tensorflow object detection issue,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16390,Maybe bug in quantize_graph.py,"In line 489 of quantize_graph.py
```python
def quantize_nodes_recursively(self, current_node):
    """"""The entry point for quantizing nodes to eight bit and back.""""""
    if self.already_visited[current_node.name]:
      return
```
The initial value of dic *self.already_visited* is empty ,so the statement
```python
if self.already_visited[current_node.name]:
```
may raise exception.
So I think it would be better to update to ：
```python
if self.already_visited.has_key(current_node.name):
``` 
Same issue also in ：
```python
def round_nodes_recursively(self, current_node)
```
Please take a look, thanks!"
16388,java.lang.InternalError: Cannot find requested resource bundle for locale en_US,"bazel version  0.9.0
java version ""1.8.0_161""
os：Ubuntu 16
编译之后出现这个错误，
 Building tensorflow/examples/android/libtensorflow_demo.jar (24 source files) failed (Exit 1)
java.lang.InternalError: Cannot find requested resource bundle for locale en_US

应该如何修改呢，谢谢啦
"
16387,Tensorflow not using GPU,"I use windows in my laptop. Initially tensorflow worked well with GPU. I don't know why suddenly it's not detecting the GPU. I am using tensorflow-gpu 1.4 version (installed with pip install tensorflow-gpu) with CUDA 8.0 and cudnn 6.0. I also tried with other versions but the problem persists. Attached is the error message shown. I appreciate any help :)

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: just called a session
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows
- **TensorFlow installed from (source or binary)**: pip install tensorflow-gpu
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: NVIDIA GeForce 1050 2GB
- **Exact command to reproduce**: sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

(https://user-images.githubusercont
![tferror](https://user-images.githubusercontent.com/19821962/35368823-48f11022-0153-11e8-87c3-41e41118a3fa.png)
ent.com/19821962/35368747-e2d4ba8c-0152-11e8-871a-a6a5aedf6663.png)
"
16386,Using keras layers within an Estimator either causes training where it shouldn't or corrupts weights,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

Hey there,

So I have a `tf.keras` model that for use on Amazon SageMaker I'm trying to convert into an Estimator. I know there's `tf.keras.estimator.model_to_estimator` but, I'm having separate [issues with that](https://github.com/tensorflow/tensorflow/issues/16385).

In the [easily run reproduction here](https://gist.github.com/zmjjmz/667d0d7e6b6c97c49b3aaf4d67b03d2c) I have (as a demonstration) a `tf.keras.layers.Embedding` which is initialized with all zeros and has `trainable=False`. Followed by that is a `Dense` layer with `use_bias=False` because I couldn't figure out how to get predictions out of an Estimator without training it first (and I can't train nothing apparently). Since all of the embeddings are zero however and can't be trained, the `Dense` layer should always produce a zero, even after training it. Instead, it produces garbage!

In fact, I've taken a few steps to ensure that no training takes place, although ideally I'd be able to just run the estimator without training:
1) I've set the loss to be 0 initially (l2_norm of what should start out as 0)
2) Optimize with SGD using a learning rate of 0
3) One training example that should have zero loss...

The output I actually get is very much non-zero. If I inspect the `embed/embeddings:0` tensor in `tfbdg`, I see this:
```
array([[ 0.14387012,  0.83495581,  0.44025695,  0.25154734,  0.7214781 ,  0.40229702,  0.82108581,  0.12210274,  0.43861651,  0.39615464],                
       [ 0.81636655,  0.48157215,  0.48987687,  0.48775947,  0.62187696,  0.25421095,  0.64555049,  0.97305572,  0.53352964,  0.34286666],                
       [ 0.82881641,  0.80365777,  0.4596678 ,  0.21614265,  0.22256434,  0.07986271,  0.92880177,  0.64946997,  0.89239001,  0.13793337],                
       [ 0.98491704,  0.15281868,  0.77106941,  0.30048406,  0.86042607,  0.88010466,  0.64362776,  0.70185173,  0.49912012,  0.61521161]], dtype=float32)
```

Even though it shouldn't have budged from all zeroes! 

So, something about how I'm doing this is fundamentally broken. I suspect that the issue is in line `61` where I start a new Session -- however this appears to be necessary, since I need to ensure that the `keras` backend is using the same `Graph` as `tensorflow.get_default_graph()` due to the peculiarities of how the Estimator calls the `model_fn`.

Notably those values hold between:
1) Runs of the estimator
2) Successive runs with the same `tf.set_random_seed` value

The latter makes me think that somehow the `Embedding` layer is receiving *a* gradient despite my best efforts, although it's hard to test this versus some sort of memory corruption. 

If you need me to provide any more information let me know -- I'm sure I've left something out.

"
16385,Estimator built with keras.estimator.model_to_estimator fails on Estimator.export_savedmodel,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Debian 3.16.36
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.9
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: See gist

### Describe the problem

If I create a model with `tf.keras`, compile it and then turn it into an estimator by simply passing it thru to `tf.keras.estimator.model_to_estimator`, I am able to train and evaluate the model just fine -- however when I got to export it with `Estimator.export_savedmodel`, I get the following error:

```
Traceback (most recent call last):
  File ""endtoend_noweights_trainer_keras.py"", line 302, in <module>
    get_serving_input_fn(hyperparameters),
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 517, in export_savedmodel
    serving_input_receiver.receiver_tensors_alternatives)
  File ""/home/u1/zach/proj/dataplayground2/local/lib/python2.7/site-packages/tensorflow/python/estimator/export/export.py"", line 193, in build_all_signature_defs
    raise ValueError('export_outputs must be a dict.')
ValueError: export_outputs must be a dict.
```

I'm not sure what `export_outputs` is, but if I had to guess it should be a mapping of output names to output tensors from the `keras` model.

Here's the (very unclean) [code](https://gist.github.com/zmjjmz/8e3a7e5430f2e700a9e89bf2b4f6259b) I'm using to get to this, although there's a lot of dependencies that won't work for y'all. If you need a repro I can take the time to put it together, just let me know. Notably the error occurs on line `300`."
16381,Distributed TensorFlow without shared directory,"It seems like there is an inherent assumption in Distributed TensorFlow that all nodes must share a common file system, such as google cloud or NFS. 

I've found in testing that models will train just fine without a common file system, but the final trained model doesn't save properly when you try something like: 
`builder = tf.saved_model_builder.SavedModelBuilder(export_dir) 
...
builder.save()`

The issue seems to be that the parameter server has the variables and the chief node has the graph. 

It'd be great if TensorFlow added a function to allow us to consolidate the graph and variables at the end of training onto the chief node in order to save the trained model. Right now there doesn't seem to be an easy way to do this.

Thanks."
16379,No module named 'tensorflow'. Anaconda+windows10+tensorflow-gpu+cuda8+cudnn6,"I had anaconda on my windows 10.
I installed CUDA 8.0 with cuDNN 6 and then followed [http://blog.nitishmutha.com/tensorflow/2017/01/22/TensorFlow-with-gpu-for-windows.html](url) to activate tensorflow-gpu environment. Now when I import tensorflow in the console, it works but with jupyter notebook opened right in this environment it throws the error. I even upgraded setuptools as mentioned in a previous issue.
![capture5](https://user-images.githubusercontent.com/10391022/35360420-a5cd8f98-0183-11e8-919a-53da56df5ee8.JPG)
![capture6](https://user-images.githubusercontent.com/10391022/35360450-c27e44e8-0183-11e8-9c0a-5aaaa05000c4.JPG)
![capture7](https://user-images.githubusercontent.com/10391022/35360455-c7e24a42-0183-11e8-964b-7186fb233078.JPG)
![capture8](https://user-images.githubusercontent.com/10391022/35360549-174292c2-0184-11e8-9d6a-93ba6139a275.JPG)


"
16374,Windows does not build tflite: No module named 'tensorflow.contrib.lite.toco.python',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Build 16299.192  and Windows 7
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0rc1 and tf-nightly  1.6.0.dev20180124
- **Python version**: 3.6.2 and 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: Nvidia GT 740M 2GB
- **Exact command to reproduce**: toco --help

### Describe the problem
I am trying to run the codelab tutorial of tensorflow lite. After installing tf-nightly, when I try to run the command ""toco --help"", I get the error ModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python'.

I have tried this on 3 computers( all Windows) and the same problem persists.


### Source code / logs
C:\Users\HP\Downloads>toco --help
Traceback (most recent call last):
  File ""c:\programdata\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\programdata\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\ProgramData\Anaconda3\Scripts\toco.exe\__main__.py"", line 5, in <module>
ModuleNotFoundError: No module named 'tensorflow.contrib.lite.toco.python'
"
16371,Tegra Nvidia Jetson TX2 build python 2.7 new CUDA and CUDNN,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux4Tegra 28.2
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: 5.4
- **CUDA/cuDNN version**: 9.0/7.0
- **GPU model and memory**: Denver2 8GB
- **Exact command to reproduce**: import tensorflow

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
Clean installation with the new CUDA 9 and cudnn 7 from nvidia
### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
Collecting system information...
Traceback (most recent call last):
  File ""/tmp/check_tf.py"", line 1, in <module>
    import tensorflow as tf;
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: undefined symbol: _ZN3Aws4Time9LocalTimeEP2tml


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Wrote environment to tf_env.txt. You can review the contents of that file.
and use it to populate the fields in the github issue template.

cat tf_env.txt

```
"
16370,add hooks for mutate variables in tf.Estimator,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:


== cat /etc/issue ===============================================
Darwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64
Mac OS X 10.12.6

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 8.0.0 (clang-800.0.42.1)
Target: x86_64-apple-darwin16.7.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin MTL-PengYu 16.7.0 Darwin Kernel Version 16.7.0: Wed Oct  4 00:17:00 PDT 2017; root:xnu-3789.71.6~1/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.13.3)
protobuf (3.4.0)
tensorflow (1.3.0)
tensorflow-serving-api (1.3.0)
tensorflow-tensorboard (0.1.8)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = v1.3.0-rc2-20-g0787eee
tf.COMPILER_VERSION = v1.3.0-rc2-20-g0787eee
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
('v1.3.0-rc2-20-g0787eee', '1.3.0')

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Right now there is no elegant way we can mutate variable in the `tf.estimator.Estimator` 
And we do have some situation that we want to discard the checkpoints but save the variable in other format fits better with our infra.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

"
16369,Unittesting Models with Tensorflow - How to clear the existing graph ?,"Hello dear tensorflowers,

I have already asked the question of [StackOverflow](https://stackoverflow.com/questions/48421308/tensorflow-and-unittests-layer-already-defined), however it seams like nobody can answer my question.

So I hope you will forgive me about reposting it here:

I am developing unittests for a product I implemented with TF.

Each part of the model is tested separately then all together in different conditions.

Let's take the example of a simple GAN, I have the following tests:

 - **GeneratorTest** Class: With all tests concerning G inside
 - **DiscriminatorTest** Class: With all tests concerning D inside
 - **GAN_Train_Test** Class: G and D connected all together: 1 training step is tested.
 - **GAN_Inference_Test** Class: G and D connecteed all together: 1 inference run is tested.

------------

When the files are executed independently, everything is working nicely and fine. Tests are all fine.

Problems start occuring when I try to create one file to launch them all from one master file.

**master_test_launcher.py:**

```python
import unittest
import time
    
import tensorflow as tf
    
from tests.test_generator import GeneratorTest
from tests.test_discriminator import DiscriminatorTest
from tests.test_anovae_model import GAN_Train_Test
from tests.test_inference import GAN_Inference_Test
    
runner = unittest.TextTestRunner(verbosity=2)
    
if __name__ == '__main__':
   tf.logging.set_verbosity(tf.logging.DEBUG)
    
    for test in [GeneratorTest, DiscriminatorTest, GAN_Train_Test, GAN_Inference_Test]:
        tf.logging.debug(""Running tests for: %s ..."" % test.__str__())
    
        tf.reset_default_graph()
    
        time.sleep(2)
    
        test_suite = unittest.TestSuite()
        test_suite.addTest(unittest.makeSuite(test))
    
        runner.run(test_suite)
```

I repeatedly obtain the same error when I run the tests related to G and D connected together: 

```python
Exception: Layer 'encoder/input' already exists, please choice other 'name' or reuse this layer
Hint : Use different name for different 'Layer' (The name is used to control parameter sharing)
```

The error is quite simple to understand, each test file is independant and thus create its own session and graph. While testing only G or D, there is no problem because they have different name_scope/variable_scope. However, when testing the whole model *Layers* already have been defined by previous tests and thus leading to issue.

I would like to find a way to completely drop the graph and reset the whole TF state as **brand new and clean**. However, everything I try seem to  fail.

I would like to avoid creating a new graph for each test, leaving the old one in memory (could lead to very high amount of memory waste after a few tests).


So my question is easy: ** How can I reset the whole TF state and internal vars as ""clean"" as if you relaunch a new python shell ? By some black-magic I can't find any way doing it (after looking for it for hours).

For information here are the things I tried and which failed:
- tf.reset_default_graph()
- cleaning everything in graph collections
- creating a new graph + new session before executing each Test File: A graph is still built somewhere containing my Layers and I can't manage to find it.
- reading the TF code and trying to find any __exit__ or close function which I didn't find

Thanks a lot,

Jonathan D."
16366,I have an issue with http://projector.tensorflow.org/ always getting stuck,"Can anyone help me with this issue? I start up the _**Visualizing High-Dimensional Space**_ webstie and it loads until it says something about metadata and never does anything from there. Plese, help. I'm so confused?"
16365,Include grpc_tensorflow_std_server in Docker image,"It would be nice if the grpc_tensorflow_std_server was included in the Docker image.

This would prevent users from having to write code just to launch a parameter server because they could just run the stock binary.

Some context in: tensorflow/k8s#16"
16364,Standardizing the saved format and/or converting to big-endian on read,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 s390x
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/python:framework_meta_graph_test which invokes meta_graph.import_scoped_meta_graph.

### Describe the problem
The testdata for //tensorflow/python:framework_meta_graph_test is not platform independent and causes test to fail on Big Endian systems.

As per discussion in #16003 , correct approach would be standardizing the stored format and/or conversion on load based on endianness. Can someone have a look?  
"
16363,Warning: Table trying to initialize from file ... is already initialized,"Python: 3.6.2
Tensorflow: 1.5.0rc1 pip
OS: Windows 10
No CUDA, just CPU

I get this when creating a lookup table with `tf.contrib.lookup.index_table_from_file`.
As I have multiple graphs I create that table in each graph (from the same file) I need it in. This results in the warning from the title of this issue.

1. Are tables shared across graphs?
2. How to figure out if a table from a specific file is already existing/initialized?

This also occurs with `tensorflow/nmt`:
https://github.com/tensorflow/nmt/issues/234"
16362,`tf.foldl` should have more robust input handling (like `tf.scan`),"### System information
- Windows 10 x64
- Installed from binary
- TensorFlow 1.4.0 (Cpu version)
- Python 3.6.1

### Bug Description
`tf.foldl` (and `tf.foldr`) are conceptually very very similar to `tf.scan`. Therefore the implementations are also very similar. However, `tf.scan` accepts initializer lists or tuples with varying type arguments, while `tf.foldl` does not. I think this is a simple oversight, and it seems that cutting and pasting some code from `tf.scan` to `tf.foldl` fixes this problem. Specifically. the master `tf.foldl `code is (after removing the docstring):

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
  if not callable(fn):
    raise TypeError(""fn must be callable."")

  with ops.name_scope(name, ""foldl"", [elems]):
    # Any get_variable calls in fn will cache the first call locally
    # and not issue repeated network I/O requests for each iteration.
    varscope = vs.get_variable_scope()
    varscope_caching_device_was_none = False
    if varscope.caching_device is None:
      # TODO(ebrevdo): Change to using colocate_with here and in other methods.
      varscope.set_caching_device(lambda op: op.device)
      varscope_caching_device_was_none = True

    # Convert elems to tensor array.
    elems = ops.convert_to_tensor(elems, name=""elems"")
    n = array_ops.shape(elems)[0]
    elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                            dynamic_size=False,
                                            infer_shape=True)
    elems_ta = elems_ta.unstack(elems)

    if initializer is None:
      a = elems_ta.read(0)
      i = constant_op.constant(1)
    else:
      a = ops.convert_to_tensor(initializer)
      i = constant_op.constant(0)

    def compute(i, a):
      a = fn(a, elems_ta.read(i))
      return [i + 1, a]
    _, r_a = control_flow_ops.while_loop(
        lambda i, a: i < n, compute, [i, a],
        parallel_iterations=parallel_iterations,
        back_prop=back_prop,
        swap_memory=swap_memory)

    if varscope_caching_device_was_none:
      varscope.set_caching_device(None)
    return r_a

```

Modifying the code in the following manner seems to allow non-homgoenous initializer lists (tuples do not work for some reason). Note that you can toggle the mofidication with the ""useModifications"" flag:

```
def foldl(fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
          swap_memory=False, name=None):
    if not callable(fn):
        raise TypeError(""fn must be callable."")

    with ops.name_scope(name, ""foldl"", [elems]):
        # Any get_variable calls in fn will cache the first call locally
        # and not issue repeated network I/O requests for each iteration.
        varscope = vs.get_variable_scope()
        varscope_caching_device_was_none = False
        if varscope.caching_device is None:
            # TODO(ebrevdo): Change to using colocate_with here and in other methods.
            varscope.set_caching_device(lambda op: op.device)
            varscope_caching_device_was_none = True

        # Convert elems to tensor array.
        elems = ops.convert_to_tensor(elems, name=""elems"")
        n = array_ops.shape(elems)[0]
        elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,
                                                dynamic_size=False,
                                                infer_shape=True)
        elems_ta = elems_ta.unstack(elems)

        if initializer is None:
            a = elems_ta.read(0)
            i = constant_op.constant(1)
        else:
            useModifications = True
            if useModifications:
                output_is_sequence = nest.is_sequence(initializer)
                output_flatten = lambda x: nest.flatten(x) if output_is_sequence else [x]
                initializer_flat = output_flatten(initializer)
                a = [ops.convert_to_tensor(init) for init in initializer_flat]
            else:
                a = ops.convert_to_tensor(initializer)

            i = constant_op.constant(0)

        def compute(i, a):
            a = fn(a, elems_ta.read(i))
            return [i + 1, a]

        _, r_a = control_flow_ops.while_loop(
            lambda i, a: i < n, compute, (i, a),
            parallel_iterations=parallel_iterations,
            back_prop=back_prop,
            swap_memory=swap_memory)

        if varscope_caching_device_was_none:
            varscope.set_caching_device(None)
        return r_a
```

Here is a MWE:

```
import tensorflow as tf

a = tf.constant( 1, dtype = tf.float32 )
b = tf.constant( 2, dtype = tf.int64   )

useTuple = False

def body( ab, i ):
    a = ab[0]
    b = ab[1]
    if useTuple:
        return (a,b)
    else:
        return [a,b]

N = 3
with tf.Session() as sess:
    if useTuple:
        ab = (a,b)
    else:
        ab = [a,b]
    print( ""new foldl :"", sess.run(   foldl(  body, tf.range(N), ab ) ) )  
    print( ""tf.scan   :"", sess.run( tf.scan(  body, tf.range(N), ab ) ) )
    print( ""tf.foldl  :"", sess.run( tf.foldl( body, tf.range(N), ab ) ) )
```

with useTuple = False, this returns 

```
new foldl : [1.0, 2]
tf.scan   : [array([1., 1., 1.], dtype=float32), array([2, 2, 2], dtype=int64)]
# Crash for tf.foldl with error: 
TypeError: Cannot convert a list containing a tensor of dtype <dtype: 'int64'> to <dtype: 'float32'> (Tensor is: <tf.Tensor 'Const_5:0' shape=() dtype=int64>)

```"
16360,"Python: Make an alias for ""tf.variable"" (with a lower ""v"") so the naming of it is consistent with ""tf.placeholder""/""tf.constant""","
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
Many developers learn the naming standards of the software so they can write code faster. It does not make any sense to have to things ""tf.placeholder"" and ""tf.Variable"" named using different schema. Constant, Placeholder and Variable are similar entities and can be used interchangeably. They should be named in same style even if tf.Variable is a class. 

https://www.tensorflow.org/api_docs/python/tf/placeholder
https://www.tensorflow.org/api_docs/python/tf/Variable
https://www.tensorflow.org/api_docs/python/tf/constant

### Source code / logs
N/A
"
16358,Request for updating keras/datasets files to r1.5,"### System information
- **executes Keras sample code imdb_fasttext.py https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py**:
- **Windows 7**:
- **TensorFlow installed from binary**:
- **TensorFlow version 1.5.0rc0**:
- **Python version 3.5.1**: 

### Describe the problem
Keras sample program does not work.
 There is a bug for numpy arange method wrong usage.
   (Need to fix from arrange to arange) 
This issue is already solved on master branch. (not in 1.5.0rc1)
Would you update these source codes?

### Source code / logs
Error messages are follows
===
C:\Users\sakaia\work\tensorflow\keras>python imdb_fasttext.py
Loading data...
Traceback (most recent call last):
  File ""imdb_fasttext.py"", line 75, in <module>
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features
)
  File ""C:\Program Files\Python35\lib\site-packages\tensorflow\python\keras\_imp
l\keras\datasets\imdb.py"", line 77, in load_data
    indices = np.arrange(len(x_train))
AttributeError: module 'numpy' has no attribute 'arrange'
===

Following are just checking np.arrange (not np.arange)
>git branch r1.5
>grep -rn np.arrange *
tensorflow/python/keras/_impl/keras/datasets/boston_housing.py:51:  indices = np.arrange(len(x))
tensorflow/python/keras/_impl/keras/datasets/reuters.py:76:  indices = np.arrange(len(xs))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:77:  indices = np.arrange(len(x_train))
tensorflow/python/keras/_impl/keras/datasets/imdb.py:82:  indices = np.arrange(len(x_test))
>git branch -
>grep -rn np.arrange *
(This line is intentionally blank)"
16356,How leave only 1 app and how edit drawing boxes?,
16354,Dataset map func shape inference ,"
------------------------

### System information
```
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)
```

### Describe the problem

I use dataset api to build  input_fn  in train and evaluate process， the input_fn is as follows
```
def input_fn(data_dir, num_epochs, shuffle, batch_size):
  """"""Generate an input function for the Estimator.""""""
  data_files = tf.gfile.Glob(data_dir)
  assert len(data_files), ('%s has no files!' % data_files) 
  
  def parse_line(value):
    columns = module_decode_file.decode_file(value, 
            record_defaults=_COLUMN_DEFAULTS,
            output_size=_COLUMN_SIZESt,
            field_outer_delim=',',
            field_inner_delim='%')
    features = dict(zip(_COLUMN_NAMES, columns))
    labels = features.pop('label')
    return features, labels

  # Extract lines from input files using the Dataset API.
  dataset = tf.data.TextLineDataset(data_files)
  if shuffle:
    dataset = dataset.shuffle(buffer_size=batch_size*3)
  dataset = dataset.map(parse_line, num_parallel_calls=1)
  dataset = dataset.repeat(num_epochs)
  dataset = dataset.batch(batch_size)

  iterator = dataset.make_one_shot_iterator()
  features, labels = iterator.get_next(""iterator"")
  return features, labels
```
I use custom op decode_file, which is like tf.decode_csv.

the decode_file's SetShapeFn is as follows: 
```
    .SetShapeFn([](InferenceContext* c) {
      std::vector<int> output_size;
      c->GetAttr(""output_size"", &output_size);

      for (int i = 0; i < c->num_outputs(); ++i) {
        ShapeHandle s = c->MakeShape({DimensionOrConstant(output_size[i])});
        c->set_output(i, s); 
      }   
      return Status::OK();
    })  
```
this custom op  can work well  in train and evaluate ,  but  when it is used for export_savedmodel api, i need modify  SetShapeFn as follows ( to support batch predict)
``` 
 .SetShapeFn([](InferenceContext* c) {
      std::vector<int> output_size;
      c->GetAttr(""output_size"", &output_size);
      for (int i = 0; i < c->num_outputs(); ++i) {
          c->set_output(i, c->Matrix(c->Dim(c->input(0), 0), output_size[i]));
      }
      return Status::OK();
}

# my serving_input_fn
def string_decode_serving_input_receiver_fn():
  string_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.string)

  columns = module_decode_file_serve.decode_file_serve(string_placeholder,
          record_defaults=_COLUMN_DEFAULTS,
          output_size=_COLUMN_SIZESt, 
          field_outer_delim=',',
          field_inner_delim='%')
  features = dict(zip(_COLUMN_NAMES, columns))
  features.pop('label')
  return tf.estimator.export.ServingInputReceiver(features, string_placeholder)
```

Why does dataset map api (dataset = dataset.map(parse_line)) only handle single record?  and pase_line's shape inference does not include batch dimmension
At present I need use different SetShapeFn in train and export_savedmodel
Can dataset map function add new feature to  handle batch dimmension ?
@mrry  thanks! (sorry for bad description ...）
"
16352,OrderedEnqueuer not imported in keras/utils/__init__.py,"OrderedEnqueuer is not imported with the remaining util objects.

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16351,TfLiteCameraDemo failed to work with NNAPI after commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1,"I am testing NNAPI by forcing TfLiteCameraDemo to invoking libneuralnetworks.so. It worked correctly though slower. But since commit e6ff665dbe4888aa5fdff8f34c44405acca2ddd1, TfLiteCameraDemo crashes with error message like,

	01-24 03:39:36.393 19136 19153 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: Process: com.example.android.tflitecamerademo, PID: 19136
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: java.lang.IllegalArgumentException: Failed to run on the given Interpreter: NNAPI was requested, but dependent sized tensors being used.
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:95)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:123)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:104)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:114)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:790)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:99)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:164)
	01-24 03:39:36.393 19136 19153 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)
	01-24 03:39:36.396   626   871 W ActivityManager:   Force finishing activity com.example.android.tflitecamerademo/.CameraActivity

Here is my patch

	 index e44c5ae..1ed88eb 100644
	 ---a/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 +++b/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java
	 @@ -91,7 +91,7 @@ public class ImageClassifier {
		
	   /** Initializes an {@code ImageClassifier}. */
	   ImageClassifier(Activity activity) throws IOException {
	-    tflite = new Interpreter(loadModelFile(activity));
	+    tflite = new Interpreter(loadModelFile(activity), true);
	     labelList = loadLabelList(activity);
	     imgData =
	         ByteBuffer.allocateDirect(
	diff --git a/tensorflow/contrib/lite/java/demo/build.gradle b/tensorflow/contrib/lite/java/demo/build.gradle
	index dd883d6..9361c71 100644
	--- a/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	+++ b/tensorflow/contrib/lite/java/src/main/java/org/tensorflow/lite/Interpreter.java
	@@ -66,6 +66,13 @@ public final class Interpreter implements AutoCloseable {
	     }
	     wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	   }
	+  public Interpreter(@NotNull File modelFile, boolean nn) {
	+    if (modelFile == null) {
	+      return;
	+    }
	+    wrapper = new NativeInterpreterWrapper(modelFile.getAbsolutePath());
	+    wrapper.setUseNNAPI(nn);
	+  }
	
	   /**
	    * Initializes a {@code Interpreter} with a {@code MappedByteBuffer} to the model file.
	@@ -76,6 +83,10 @@ public final class Interpreter implements AutoCloseable {
	   public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer) {
	     wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	   }
	+  public Interpreter(@NotNull MappedByteBuffer mappedByteBuffer,  boolean nn) {
	+    wrapper = new NativeInterpreterWrapper(mappedByteBuffer);
	+    wrapper.setUseNNAPI(nn);
	+  }
	 
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
	   /**
	    * Runs model inference if the model takes only one input, and provides only one output.
"
16350,"There is an issue with your ""new issue"" page","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- All systems/system independent

### Describe the problem
If there are documentation issues, the ""New Issue"" button will direct many people to StackOverflow. The notions of ""bugs and features"" are not universal in that they only apply to software, not to documentation as far as some people are concerned. IMHO the text above would be better if it stated that ""1. It must be a bug or a feature request, or a correction/clarification to documentation""

### Source code / logs
No source involved
"
16348,Change RELEASE.md to specify CUDA version,"The RELEASE.md states that ""Prebuilt binaries are now built against CUDA 9 and cuDNN 7.""

https://github.com/tensorflow/tensorflow/issues/15604 says that CUDA 9.1 is not supported.

Could we change the RELEASE.md so that it says ""Prebuilt binaries are now built against CUDA 9.0 and cuDNN 7."" until later versions are supported?
"
16347,Golang API to serialize data into tf.Example protos(tfrecords),"The Golang api [WriteContentsTo](https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go#Tensor.WriteContentsTo) can be used to writes the serialized contents of a tensor to io.Writer, where the tensor is built from golang scalars, slices, and arrays. 

Yet there's not a Golang API to serialize data into tf.Example protos(tfrecords).

For example, when i want serialize a libsvm into tf.Example protos, i can do this by:
```
def libsvm2tfrecords(data_source, target_dir, delimiter='\t'):
    """"""
    a single file should not contain lines more than 1000,000, or we should use libsvm2proto_par
    :param data_source: libsvm file path
    :param target_dir: dir to storage the serialize proto file
    :param delimiter: delimiter for csv reader
    :return:
    """"""
    if not os.path.isfile(data_source):
        raise ValueError('data file passed do not exist or not a file')

    file_name = os.path.join(target_dir, os.path.splitext(
        os.path.basename(data_source))[0] + '.tfrecords')
    writer = tf.python_io.TFRecordWriter(file_name)
    start = datetime.now()
    line_c = 0
    with open(data_source, 'rb') as rf:
        f_reader = csv.reader(rf, delimiter=delimiter, quotechar='|')
        for row in f_reader:
            line_c += 1
            feature = dict()
            indexes = []
            values = []
            feature.update({'label': _float_feature([float(row[0])])})
            for e in row[1:]:
                index, value = e.split(':')
                indexes.append(int(index))
                values.append(float(value))
                feature.update({'index': _int64_feature(indexes)})
                feature.update({'value': _float_feature(values)})

            example = tf.train.Example(features=tf.train.Features(feature=feature))
            writer.write(example.SerializeToString())

        writer.close()
        end = datetime.now()

        print(""- consumed time: %ds for %s"" % ((end-start).seconds, data_source))
```
BUT Golang api does not seem to be able to achieve this. "
16345,tf.nn.sparse_softmax_cross_entropy_with_logits get error:  ValueError: Rank mismatch: Rank of labels (received 1) should equal rank of logits minus 1 (received 4).,
16344,Variance of weights initialized with tf.variance_scaling_initializer is somewhat surprising,"If `tf.variance_scaling_initializer` is called with `distribution='uniform'`, then the weights have variance `scale / n`. However, if `tf.variance_scaling_initializer` is called with `distribution='normal'`, the weights have variance `scale / n * (1 - (4 * norm.pdf(2))/(2 * norm.cdf(2) - 1))`, because the weights are drawn from a normal distribution truncated at +/- 2 std and the scale is not adjusted for this truncation.

While I understand that there are reasons to use a truncated normal distribution, I would argue that the distribution should be scaled to adjust for the truncation, or the fact that the scale is not adjusted for truncation should feature more prominently in the documentation. The current statement that:

> With `distribution=""normal""`, samples are drawn from a truncated normal distribution centered on zero, with `stddev = sqrt(scale / n)`

is not totally clear, since it's not obvious that `stddev` is referring to the standard deviation of the normal distribution before truncation and not after.

To make matters more confusing, unlike `tf.variance_scaling_initializer`, `tf.contrib.layers.variance_scaling_initializer` performs the appropriate correction so that the weights have variance `scale / n`."
16343,tf.data.Dataset doesn't provide a good workflow for generating custom samples from large files,"We're given hundreds of data files, each containing many gigabytes worth of sample data in a custom format. As far as I can tell there are only two approaches to extract samples from this using `Dataset`:

1) `tf.data.Dataset.from_generator(generator=my_custom_reader, ...)`

Create a generator which produces samples. This approach is not ideal because this method must be the first dataset in the chain. The generator cannot accept a tensor. Therefore you can't batch and shuffle your list of 100's of filenames (or anything more complex). You also can't make use of `interleave(...)` because the generator can't accept a tensor, and this use case is begging to use `interleave(...)`.

A solution here might be to provide a method for a generator to accept a tensor, as `tf.py_func(...)` does for functions.

2) `tf.data.Dataset.map(map_func=tf.py_func(my_custom_reader, ...), ...)`

The map function does allow us to shuffle and parallelize the filenames using all of the functionality of the Dataset pipeline, however, with `map`, the files must be read into memory completely, and these files are large. Reading numerous files into memory is infeasible.

A solution here might be to extend the `map` function to support generators.

Unless there's an alternative approach, which I didn't glean from the docs or stackoverflow, then this seems to be an inherent limitation and a seemingly reasonable use case on which to base a feature request.
"
16338,Obtain tower id when using tf.contrib.estimator.replicate_model_fn(),"Since tensorflow 1.5, a new API tf.contrib.estimator.replicate_model_fn() is introduced to to replicate a model over GPUs. However, the current API hides the tower id information from the model_fn(). Without knowning tower id, it is difficult to create non-shared local variables/ops per tower. Is it possible to propagate the tower id information to model_fn? Thanks."
16334,save_steps in MoniteredTrainingSession,"Any reason to omit the `save_steps` in favor of `save_secs` for `CheckpointSaverHook` in `MoniteredTrainingSession`?

https://github.com/tensorflow/tensorflow/blob/abf3c6d745c34d303985f210bf9e92cac99ba744/tensorflow/python/training/monitored_session.py#L363

The previous `SummarySavorHook` has both `save_steps` and `save_secs` considered

https://github.com/tensorflow/tensorflow/blob/abf3c6d745c34d303985f210bf9e92cac99ba744/tensorflow/python/training/monitored_session.py#L358

- [x] Have I written custom code: no
- [x] OS Platform and Distribution: N/A
- [x] TensorFlow installed from: pip
- [x] TensorFlow version: 1.5
- [x] Bazel version: N/A
- [x] CUDA/cuDNN version: N/A
- [x] GPU model and memory: N/A
- [x] Exact command to reproduce: N/A"
16333,Computing gradients of loop variables return None,"### System information
- Windows 10 x64
- Installed from binary
- TensorFlow 1.3.0
- Python 3.6.3
- CuDNN 6.4.6, CUDA 8.0
- NVIDIA GeForce 940M

### Bug Description

Issue [15403](https://github.com/tensorflow/tensorflow/issues/15403) was closed since it was labelled as ""not a bug or feature request"". I believe it is indeed a bug. 

The fundamental problem is that gradients do not seem to be working inside of `tf.while_loop`s. Here is a demonstration of code run inside and outside of a while loop. The code outside of the loop produces a result, whereas the gradient inside of the loop returns None: 
```
import tensorflow as tf

i = tf.constant(0)
x = tf.constant(3.0)
print(""External gradient:"", tf.gradients(x, x)[0])     # Prints Tensor(""gradients/Fill:0"", shape=(), dtype=float32)

def loop_body(i, x, y):
    print(""internal gradient:"", tf.gradients(x, y)[0]) # Prints None
    return i + 1, x, y

tf.while_loop( lambda i,x,y: tf.less(i, 5), loop_body, [i, x, x]);
```"
16331,Compilation failure with gcc-6.4 (gcc-7.2 and clang-4) in ubuntu 17.10,"When compiling master in ubuntu 17.10, compilation fails due to 'too perfect forwarding' in variant_op_registry
`
std::unordered_map<std::tuple<VariantXOp, StringPiece, StringPiece>, 
                     VariantXOpFn, TupleHash>
`
A workaround, by replacing tuple with a struct, provided in PR #16309 for your review.

Thanks,
Sami
"
16329,tf.nn.seq2seq.embedding_rnn_seq2seq,"Have I written custom code: Yes
OS Platform and Distribution: Windows 10
TensorFlow installed from anaconda prompt
TensorFlow version 0.12 & 1.4
Bazel version NA
CUDA/cuDNN version NA
GPU model and memory: Floydhub
Exact command to reproduce:

self.outputs, self.states = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(self.enc,self.dec, stacked_lstm, xvocab_size, yvocab_size, emb_dim)

THE PROBLEM.
I am upgrading working tf0.12 code so I can train on floydhub.  I have replaced  tf.nn.seq2seq.embedding_rnn_seq2seq  with tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq. It produces the following error log:


  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\site-packages\tensorflow\contrib\legacy_seq2seq\python\ops\seq2seq.py"", line 358, in embedding_rnn_seq2seq
    encoder_cell = copy.deepcopy(cell)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 166, in deepcopy
    y = copier(memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\site-packages\tensorflow\python\layers\base.py"", line 655, in __deepcopy__
    setattr(result, k, copy.deepcopy(v, memo))

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 218, in _deepcopy_list
    y.append(deepcopy(a, memo))

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 182, in deepcopy
    y = _reconstruct(x, rv, 1, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 297, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 182, in deepcopy
    y = _reconstruct(x, rv, 1, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 297, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 182, in deepcopy
    y = _reconstruct(x, rv, 1, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 297, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 182, in deepcopy
    y = _reconstruct(x, rv, 1, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 297, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 182, in deepcopy
    y = _reconstruct(x, rv, 1, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 297, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 218, in _deepcopy_list
    y.append(deepcopy(a, memo))

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 223, in _deepcopy_tuple
    y = [deepcopy(a, memo) for a in x]

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 223, in <listcomp>
    y = [deepcopy(a, memo) for a in x]

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 182, in deepcopy
    y = _reconstruct(x, rv, 1, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 297, in _reconstruct
    state = deepcopy(state, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 155, in deepcopy
    y = copier(x, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 243, in _deepcopy_dict
    y[deepcopy(key, memo)] = deepcopy(value, memo)

  File ""C:\Users\BC\AppData\Local\conda\conda\envs\tf1.4\lib\copy.py"", line 174, in deepcopy
    rv = reductor(4)

TypeError: cannot serialize '_io.TextIOWrapper' object"
16328,Gradient computation across multi-GPU,"
------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7.6
- **CUDA/cuDNN version**: 8.0/6.0

I am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. 

However, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.

My question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?

I tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.

    import tensorflow as tf
    import numpy as np
    from tensorflow.python.client import timeline
    
    N_SAMPLES = 100000000
    
    
    def all_reduce(gpu_num):
        means = []
        x2s = []
        axs = []
        for i in range(gpu_num):
            with tf.device('/cpu:0'):
                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)
            with tf.device('/gpu:%d'%i):
                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)
                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)
                x2 = tf.square(ax, name='local_square_%d'%i)
                axs.append(ax)
                means.append(mean)
                x2s.append(x2)
    
        with tf.device('/gpu:0'):
            global_mean = tf.reduce_mean(means, name='global_mean')
            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),
                                     tf.square(global_mean, name='global_mean_square'),
                                     name='global_sub')
            print global_var.get_shape()
    
        gs = []
        # manually
        # for i in range(gpu_num):
        #     with tf.device('/gpu:%d'%i):
        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])
        #         gradient_wrt_var = tf.gradients(global_var, axs[i])
        #         gs.append(gradient_wrt_mean)
        #         gs.append(gradient_wrt_var)
    
        # auto by tf
        gradient_wrt_mean = tf.gradients(global_mean, axs)
        gradient_wrt_var = tf.gradients(global_var, axs)
        gs.append(gradient_wrt_var)
        gs.append(gradient_wrt_mean)
    
        for n in tf.get_default_graph().as_graph_def().node:
            print [n.name, n.device]
    
        return global_mean, global_var, axs, gs
    
    
    def main(_):
        gpu_num = 2
        mean_op, var_op, xs, gs = all_reduce(gpu_num)
        x = np.random.randn(N_SAMPLES*gpu_num)
        print np.mean(x), np.var(x)
        feed_dict = dict()
        for i in range(gpu_num):
            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]
    
        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
        run_metadata = tf.RunMetadata()
        gpu_options = tf.GPUOptions(allow_growth=False)
        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)
        sess = tf.Session(config=config)
    
        # mean, var, g = sess.run([
        #     mean_op, var_op, gs
        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
        # print mean, var
    
        g = sess.run([
            gs
        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)
    
        # Create the Timeline object, and write it to a json
        tl = timeline.Timeline(run_metadata.step_stats)
        ctf = tl.generate_chrome_trace_format()
        with open('timeline.json', 'w') as f:
            f.write(ctf)
    
    
    if __name__ == '__main__':
        tf.app.run()

Two figures:
auto, without specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)

manually specifying GPU device.
![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)

If using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?


"
16327,iOS does not support a simple tensorflow network,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A 


### Steps I followed:

- I have trained resnet_v2_50 using slim.

- I created a script just to run inference, so the input image is a placeholder with name ""input_1"" and the output is the softmax with name ""softmax"". 

- I exported the .pb graph, then I ran 
```
python python/tools/freeze_graph.py --input_graph=resnet_v2_50.pb --input_checkpoint=model.ckpt-1 --output_graph=frozen_resnet_v2_50.pb --input_binary=True --output_node_names=""softmax""

``` 
to freeze my graph using my checkpoint.

- I ran 
```
bazel-bin/tensorflow/tools/graph_transforms/transform_graph  --inputs=input_1 --in_graph=frozen_resnet_v2_50.pb --outputs=softmax --out_graph=quantized_resnet_v2_50.pb --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,180,180,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) quantize_weights strip_unused_nodes sort_by_execution_order'

```
 to quantize it.

- I imported the graph and also my labels file into `tensorflow/examples/ios/camera/data` and I ran the model on my iphone 5s (ios 10.3) using xcode 9.2. 

### Problem:
The app is running on my iphone but I am getting wrong labels and probabilities while the `tensorflow/examples/label_image/label_image.py` script gives me right results. I tested also the same image using label_image.py and xcode simulator and the prediction was different. Is there any bug? I changed this snippet because I don't need normalization of the image and it's right when running with python:

```
const int wanted_input_width = 180;
const int wanted_input_height = 180;
const int wanted_input_channels = 3;
const float input_mean = 0.0f;
const float input_std = 1.0f;
```

You can see the node names and the operations of resnet 50 below (here the output node is v/tower_0/resnet_v2_50/predictions/Reshape_1):

input_1=>Placeholder
v/tower_0/Reshape/shape=>Const
v/tower_0/Reshape=>Reshape
v/tower_0/split/split_dim=>Const
v/tower_0/split=>Split
v/tower_0/sub/y=>Const
v/tower_0/sub=>Sub
v/tower_0/sub_1/y=>Const
v/tower_0/sub_1=>Sub
v/tower_0/sub_2/y=>Const
v/tower_0/sub_2=>Sub
v/tower_0/concat/axis=>Const
v/tower_0/concat=>ConcatV2
v/tower_0/Reshape_1/shape=>Const
v/tower_0/Reshape_1=>Reshape
v/tower_0/resnet_v2_50/Pad/paddings=>Const
v/tower_0/resnet_v2_50/Pad=>Pad
v/resnet_v2_50/conv1/weights_quantized_max=>Const
v/resnet_v2_50/conv1/weights_quantized_min=>Const
v/resnet_v2_50/conv1/weights_quantized_const=>Const
v/resnet_v2_50/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/conv1/Conv2D=>Conv2D
v/resnet_v2_50/conv1/biases=>Const
v/tower_0/resnet_v2_50/conv1/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/pool1/MaxPool=>MaxPool
v/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_2/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block1/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_3/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/biases=>Const
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block2/unit_4/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_4/bottleneck_v2/add=>Add
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_5/bottleneck_v2/add=>Add
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/shortcut/MaxPool=>MaxPool
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv1/Relu=>Relu
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad/paddings=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/Pad=>Pad
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block3/unit_6/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/shortcut/BiasAdd=>BiasAdd
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_1/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_2/bottleneck_v2/add=>Add
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/gamma=>Dequantize
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/beta=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/preact/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv1/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/gamma=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/beta=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/Const_1=>Const
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/BatchNorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv2/Relu=>Relu
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/weights=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/Conv2D=>Conv2D
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_max=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_min=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases_quantized_const=>Const
v/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/biases=>Dequantize
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/conv3/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/block4/unit_3/bottleneck_v2/add=>Add
v/resnet_v2_50/postnorm/gamma_quantized_max=>Const
v/resnet_v2_50/postnorm/gamma_quantized_min=>Const
v/resnet_v2_50/postnorm/gamma_quantized_const=>Const
v/resnet_v2_50/postnorm/gamma=>Dequantize
v/resnet_v2_50/postnorm/beta_quantized_max=>Const
v/resnet_v2_50/postnorm/beta_quantized_min=>Const
v/resnet_v2_50/postnorm/beta_quantized_const=>Const
v/resnet_v2_50/postnorm/beta=>Dequantize
v/tower_0/resnet_v2_50/postnorm/Const=>Const
v/tower_0/resnet_v2_50/postnorm/Const_1=>Const
v/tower_0/resnet_v2_50/postnorm/FusedBatchNorm=>FusedBatchNorm
v/tower_0/resnet_v2_50/postnorm/Relu=>Relu
v/tower_0/resnet_v2_50/pool5/reduction_indices=>Const
v/tower_0/resnet_v2_50/pool5=>Mean
v/resnet_v2_50/logits/weights_quantized_max=>Const
v/resnet_v2_50/logits/weights_quantized_min=>Const
v/resnet_v2_50/logits/weights_quantized_const=>Const
v/resnet_v2_50/logits/weights=>Dequantize
v/tower_0/resnet_v2_50/logits/Conv2D=>Conv2D
v/resnet_v2_50/logits/biases_quantized_max=>Const
v/resnet_v2_50/logits/biases_quantized_min=>Const
v/resnet_v2_50/logits/biases_quantized_const=>Const
v/resnet_v2_50/logits/biases=>Dequantize
v/tower_0/resnet_v2_50/logits/BiasAdd=>BiasAdd
v/tower_0/resnet_v2_50/SpatialSqueeze=>Squeeze
v/tower_0/resnet_v2_50/predictions/Reshape/shape=>Const
v/tower_0/resnet_v2_50/predictions/Reshape=>Reshape
v/tower_0/resnet_v2_50/predictions/Softmax=>Softmax
v/tower_0/resnet_v2_50/predictions/Shape=>Const
v/tower_0/resnet_v2_50/predictions/Reshape_1=>Reshape"
16323,Does Broadcast in TF copy first or just do ops along the axis,"For example, we have
tensor a with shape (100, 100, 5) and tensor b with shape (1, 1, 5)
when running
c = tf.multiply(a, b)

Is b first copied 100 * 100 times for the **big** dot multiply with a (GPU memory consuming),
or the dot multiply is done with the original b along axis 0 and 1?

The tf.multiply page refers to **numpy multiply** that says it won't copy, just loop.
https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html

I guess it's copied first that I run into GPU memory problem by adjusting a bit the b.
How's it implemented in TF? Couldn't find the source gen_math_ops

Issue template update:
Have I written custom code No
OS Platform and Distribution: Windows 10 x64 Home version
TensorFlow installed from pip (anaconda with python 3.6.3)
TensorFlow version: 1.4.1
Bazel version: N/A
CUDA/cuDNN version: CUDA 8.0, cuDNN 6
GPU model and memory: GTX 1050Ti, 4 GB memory (3.3 GB available)
Exact command to reproduce N/A (not relevant to the question)"
16320,BUG: py_func don't support unicode string results for python2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Y
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac 10.11
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

When I investigated #14116, I found that `py_func` converts unicode strings result to bytes only for  Python3, while raise an exception for Python 2.

I'm curious why we don't the same thing for Python 2.

### Source code / logs

script:

```python
  def testReturnUnicodeString(self):
    with self.test_session():
      correct = u""你好 世界""

      def unicode_string():
        return correct

      z, = script_ops.py_func(unicode_string, [], [dtypes.string])
      self.assertEqual(z.eval(), correct.encode(""utf8""))
```

logs:

```python
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
-----------------------------------------------------------------------------
2018-01-23 09:43:04.611108: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
..2018-01-23 09:43:06.545687: W tensorflow/core/framework/op_kernel.cc:1181] Unimplemented: Unsupported numpy type 20
.2018-01-23 09:43:06.548797: W tensorflow/core/framework/op_kernel.cc:1181] Unimplemented: Unsupported object type dict
.....2018-01-23 09:43:08.129501: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.ValueError: blah
2018-01-23 09:43:08.131595: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.TypeError: blah
2018-01-23 09:43:08.134736: W tensorflow/core/framework/op_kernel.cc:1181] Resource exhausted: exceptions.MemoryError: blah
2018-01-23 09:43:08.136335: W tensorflow/core/framework/op_kernel.cc:1181] Unimplemented: exceptions.NotImplementedError: blah
2018-01-23 09:43:08.138000: W tensorflow/core/framework/op_kernel.cc:1181] Unknown: WeirdError: blah
2018-01-23 09:43:08.138882: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.ValueError: blah
2018-01-23 09:43:08.139022: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.TypeError: blah
2018-01-23 09:43:08.139236: W tensorflow/core/framework/op_kernel.cc:1181] Resource exhausted: exceptions.MemoryError: blah
2018-01-23 09:43:08.139349: W tensorflow/core/framework/op_kernel.cc:1181] Unimplemented: exceptions.NotImplementedError: blah
2018-01-23 09:43:08.139492: W tensorflow/core/framework/op_kernel.cc:1181] Unknown: WeirdError: blah
.....2018-01-23 09:43:10.036466: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.ValueError: blah
2018-01-23 09:43:10.038219: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.TypeError: blah
2018-01-23 09:43:10.041279: W tensorflow/core/framework/op_kernel.cc:1181] Resource exhausted: exceptions.MemoryError: blah
2018-01-23 09:43:10.044136: W tensorflow/core/framework/op_kernel.cc:1181] Unimplemented: exceptions.NotImplementedError: blah
2018-01-23 09:43:10.046983: W tensorflow/core/framework/op_kernel.cc:1181] Unknown: WeirdError: blah
............2018-01-23 09:43:11.081837: W tensorflow/core/framework/op_kernel.cc:1181] Invalid argument: exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)
E..........
======================================================================
ERROR: testReturnUnicodeString (__main__.PyFuncTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/py_func_test.py"", line 227, in testReturnUnicodeString
    self.assertEqual(z.eval(), correct.encode(""utf8""))
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 639, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 4795, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
InvalidArgumentError: exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)
	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_STRING], token=""pyfunc_2049"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

Caused by op u'PyFunc', defined at:
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/py_func_test.py"", line 476, in <module>
    test.main()
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/platform/test.py"", line 74, in main
    return _googletest.main(argv)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 99, in main
    benchmark.benchmarks_main(true_main=main_wrapper)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/platform/benchmark.py"", line 336, in benchmarks_main
    true_main()
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 98, in main_wrapper
    return app.run(main=g_main, argv=args)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 124, in run
    _sys.exit(main(argv))
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/platform/googletest.py"", line 69, in g_main
    return unittest_main(argv=argv)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/main.py"", line 95, in __init__
    self.runTests()
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/main.py"", line 232, in runTests
    self.result = testRunner.run(self.test)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/runner.py"", line 151, in run
    test(result)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/suite.py"", line 70, in __call__
    return self.run(*args, **kwds)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/suite.py"", line 108, in run
    test(result)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/case.py"", line 393, in __call__
    return self.run(*args, **kwds)
  File ""/usr/home/facai/workshop/anaconda2/lib/python2.7/unittest/case.py"", line 329, in run
    testMethod()
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/py_func_test.py"", line 226, in testReturnUnicodeString
    z, = script_ops.py_func(unicode_string, [], [dtypes.string])
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/ops/script_ops.py"", line 300, in py_func
    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/ops/script_ops.py"", line 209, in _internal_py_func
    input=inp, token=token, Tout=Tout, name=name)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/ops/gen_script_ops.py"", line 93, in _py_func
    ""PyFunc"", input=input, token=token, Tout=Tout, name=name)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 3172, in create_op
    op_def=op_def)
  File ""/data1/users/facai/.cache/bazel/_bazel_facai/3338df3cdc4fd0e5fdd3f3ae6490e0be/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/python/kernel_tests/py_func_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-1: ordinal not in range(128)
	 [[Node: PyFunc = PyFunc[Tin=[], Tout=[DT_STRING], token=""pyfunc_2049"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


----------------------------------------------------------------------
Ran 36 tests in 6.497s

FAILED (errors=1)
```
"
16317,tensorflow crash on android mobile with libtensorflowlite_jni.so of arm-v7a ,"
### System information
- **OS Platform and Distribution ( android 5.1 )**:
- **TensorFlow installed from (source )**:
- **TensorFlow version ( # Release 1.5.0 )**:
- **Python version** 2.7 : 
- **Bazel version ( 0.9.0-homebrew )**:
- **GCC/Compiler version
 (Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin17.3.0 )**:


You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
after using tensorflow source to build the libtensorflowlite_jni.so of arm-v7a , it crash in the android 
mobile frequently with the following message ;  somehow came with the libtensorflowlite_jni.so of arm just work fine but too long; someone can help ? 


01-23 16:17:25.292 414-414/? I/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-23 16:17:25.292 414-414/? I/DEBUG: Build fingerprint: 'nubia/NX529J/NX529J:5.1.1/LMY47V/eng.nubia.20160612.203636:user/release-keys'
01-23 16:17:25.292 414-414/? I/DEBUG: Revision: '0'
01-23 16:17:25.292 414-414/? I/DEBUG: ABI: 'arm'
01-23 16:17:25.292 414-414/? I/DEBUG: pid: 8073, tid: 8930, name: AsyncTask #6  >>> com.test.tensorflow <<<
01-23 16:17:25.292 414-414/? I/DEBUG: signal 7 (SIGBUS), code -6 (SI_TKILL), fault addr 0xac66df18
01-23 16:17:25.372 414-414/? I/DEBUG:     r0 ac66df18  r1 00000004  r2 b5ba5050  r3 000003e9
01-23 16:17:25.372 414-414/? I/DEBUG:     r4 b6df239c  r5 00000002  r6 ac66df18  r7 d7744d20
01-23 16:17:25.372 414-414/? I/DEBUG:     r8 00000002  r9 b5ba5040  sl 00000000  fp b5ba5fe4
01-23 16:17:25.372 414-414/? I/DEBUG:     ip b5ba5fec  sp d7744cc8  lr 000003e8  pc c3e5ae9c  cpsr 000f0030
01-23 16:17:25.372 414-414/? I/DEBUG: backtrace:
01-23 16:17:25.372 414-414/? I/DEBUG:     #00 pc 00067e9c  /data/app/com.test.tensorflow-1/lib/arm/libtensorflowlite_jni.so
01-23 16:17:25.372 414-414/? I/DEBUG:     #01 pc 0004870f  /data/app/com.test.tensorflow-1/lib/arm/libtensorflowlite_jni.so
01-23 16:17:25.372 414-414/? I/DEBUG:     #02 pc 0004894b  /data/app/com.test.tensorflow-1/lib/arm/libtensorflowlite_jni.so
01-23 16:17:25.372 414-414/? I/DEBUG:     #03 pc 0005d333  /data/app/com.test.tensorflow-1/lib/arm/libtensorflowlite_jni.so
01-23 16:17:25.372 414-414/? I/DEBUG:     #04 pc 00007205  /data/app/com.test.tensorflow-1/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1280)
01-23 16:17:25.372 414-414/? I/DEBUG:     #05 pc 00011bf3  /data/dalvik-cache/arm/data@app@com.test.tensorflow-1@base.apk@classes.dex
01-23 16:17:28.882 414-414/? I/DEBUG: Tombstone written to: /data/tombstones/tombstone_07


"
16316,Lack of clarity in tf.while_loop documentation,"I believe that the documentation for tf.while_loop is lacking usage clarity, and actually provides contradictory statements. 

Specifically, it seems that many people are using the tf.while_loop as a ""for loop"" ([see stackoverflow](https://stackoverflow.com/questions/35330117/how-can-i-run-a-loop-with-a-tensor-as-its-range-in-tensorflow)). However, the [tf.while_loop](https://www.tensorflow.org/versions/r0.12/api_docs/python/control_flow_ops/control_flow_operations#while_loop) docs state:

> For correct programs, while_loop should return the same result for any parallel_iterations > 0.

A loop counter inside of the ""while loop"" body, seems to violate this constraint despite the fact that this is given as an example usage in the docs:

> python i = tf.constant(0) c = lambda i: tf.less(i, 10) b = lambda i: tf.add(i, 1) r = tf.while_loop(c, b, [i])

So it seems that there are two bad outcomes here:

1. If this is indeed the canonical way of creating a ""for loop"", then the example explicitly creates a dependency between iterations, meaning that the ""while loop"" iterations cannot be run in parallel. 

1. The example is incorrect? 

It seems like the while_loop docs should have an example which better illustrates how to use it as a ""for loop"", if such usage is indeed intended, or a warning on the implications of the provided example.  
"
16315,Remove Variables from a TF Server (e.g.),"I have a cluster of long-lived TensorFlow servers  (//tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server).
My problem is how to reset variables on these server. 

There is a behavior in distributed TensorFlow in which a variable defined on a worker (e.g. PS) outlives the session which defines it. I understand this behavior is intentional to support between graph model-replica.

However, In my use case this behavior causes unexpected problem. I have not found a mechanism to override this. It there is, I believe it is helpful to better reflect it in the documentation, if there is not, I hope I can make a case to motivate its existence.

In my use case different training jobs are ran _sequentially_ (i.e. one training job at a time) on this cluster, each using one client (which connects to only one master). 

The problem I have is if a variable is defined in two training job with a same name but different shape sizes, the latter client gets the following error on ""Session"" creation:
```
InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [100] rhs shape= [200]%0A%09 [[Node: a/Assign = Assign[T=DT_FLOAT, _class=[""loc:@a""], use_locking=true, validate_shape=true, _device=""/job:worker/replica:0/task:0/device:GPU:0""](a, a/Initializer/random_uniform)]]
```

`tf.reset_default_graph` does not help. The solution to this problem could be a mechanism similar `tf.reset_default_graph` that resets variables in all the workers.

To replicate this problem let say we have two workers: (one PS, and on Worker)

Worker 1:
```bash
#worker 1
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0 &
#worker 2
./bazel-bin/tensorflow/core/distributed_runtime/rpc/grpc_tensorflow_server --cluster_spec=""ps|localhost:2222,worker|localhost:2223"" --job_name=worker --task_id=0
```
(Same result with `tf.train.Server` workers.)

Then run the simple code:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(100,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
It should work just fine.
Then when this code (which is identical except the variable shape) is ran:
```python
import tensorflow as tf
var = tf.get_variable(""A"", shape=(5000,))
with tf.train.MonitoredTrainingSession(master=""localhost:2223"") as sess:
   pass
```
This example fails.
"
16314,Published libtensorflow_framework.so binaries ABI Problem,"The distributed `libtensorflow_framework.so` included in the JAR files published to Maven are built using the C++ 11 ABI, in contrast to the main TF build. I think that affects all continuous integration builds of the shared objects. I believe the `-D_GLIBCXX_USE_CXX11_ABI=0` compiler flag should be used for the CI builds as is done for the main build. An example of its use is shown in commit 550df413158b32645ca5df4dcaabc67f1a48964d. This causes some trouble when using these shared objects and developing custom ops, as those are required to be built using that compiler flag. It would be great if the use of the flag was consistent and all binaries were built using the same ABI.

Thanks! "
16313,Bug of tf.data.TFRecordDataset? Couldn't use tf.reshape after the operations of tf.data.TFRecordDataset,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia GeForce GTX TITAN X 12GB
- **Exact command to reproduce**:


### Describe the problem
I want to use  function **tf.profiler.ProfileOptionBuilder.float_operation** to show the flops of the model. But it need a certain input shape while the the output shape of **tf.data.TFrecordDataset** is like (?, 32,32,3). When I want to use tf.reshape to reshape the output of **tf.data.TFrecordDataset**, it generates an error ""Input to reshape is a tensor with 64512 values, but the requested shape has 98304"".  

### Source code 

  def dataset_input(self, dataset_type):
    with tf.variable_scope(""batch_"" + dataset_type):
        def parser(record):
            features = tf.parse_single_example(
                record,
                features={
                    'image': tf.FixedLenFeature([], tf.string),
                    'label': tf.FixedLenFeature([], tf.int64)
                })
            image, label = features['image'], features['label']
            height, width, channels = self.input_size, self.input_size, self.input_dim
            image = tf.decode_raw(image, tf.uint8)
            image = tf.reshape(image, [height, width, channels])
            return image, label
        dataset = tf.data.TFRecordDataset([self.dataset_dir[dataset_type]])
        dataset = dataset.map(parser)
        dataset = dataset.shuffle(buffer_size=50000)
        dataset = dataset.batch(self.batch_size)
        dataset = dataset.repeat()
        iterator = dataset.make_one_shot_iterator()
        features, labels = iterator.get_next()
        features = tf.reshape(features, [self.batch_size, self.input_size, self.input_size, self.input_dim])
        return features, labels
  "
16311,Segmentation fault in _pywrap_tensorflow_internal.so,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.3 LTS (Xenial)
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: Cuda8, cudnn6
- **GPU model and memory**: Titan Xp (with driver 387.26)
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh
```
== cat /etc/issue ===============================================
Linux ubuntu 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux ubuntu 4.4.0-87-generic #110-Ubuntu SMP Tue Jul 18 12:55:35 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow-gpu (1.4.1)
tensorflow-tensorboard (0.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda/lib64/
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Tue Jan 23 11:09:12 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:04:00.0 Off |                  N/A |
| 40%   66C    P2   182W / 250W |  11763MiB / 12189MiB |     73%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 00000000:05:00.0 Off |                  N/A |
| 23%   30C    P8     8W / 250W |  11591MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  TITAN Xp            Off  | 00000000:06:00.0 Off |                  N/A |
| 28%   48C    P0    62W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  TITAN Xp            Off  | 00000000:07:00.0 Off |                  N/A |
| 26%   46C    P0    63W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   4  TITAN Xp            Off  | 00000000:08:00.0 Off |                  N/A |
| 26%   46C    P0    63W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  TITAN Xp            Off  | 00000000:0C:00.0 Off |                  N/A |
| 23%   43C    P0    62W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   6  TITAN Xp            Off  | 00000000:0E:00.0 Off |                  N/A |
| 25%   44C    P0    62W / 250W |      0MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   7  TITAN Xp            Off  | 00000000:0F:00.0 Off |                  N/A |
| 42%   69C    P2   167W / 250W |  11833MiB / 12189MiB |     31%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0     19682      C   /home/peiliang/tensorflow/bin/python       11751MiB |
|    1     19682      C   /home/peiliang/tensorflow/bin/python       11579MiB |
|    7     27581      C   python                                     11823MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/lib64/libcudart.so.9.1.85
/usr/local/cuda-9.1/lib64/libcudart_static.a
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
```

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
My custom learning code works perfectly on my older workstation with 2 GPU cards. But am having issue with our new workstation which has 8 GPU cards. I get a Segmentation fault. 


### Source code / logs

The entire source code is: https://github.com/mpkuse/cartwheel_train/tree/config-files

The main-script is `train_netvlad.py`. Currently my learning data is private, 
if you really need it to test, I can provide the data as well (~100 GB). 

My code basically builds a network with tf.slim. I have a custom operations to build a layer. Have a custom loss function. Can be found in `CartWheelFlow.py/ class VGGDescriptor`. It uses tf.while. 
Data is managed by `class TimeMachineRender`

stack trace for the crash. 

```
$ gdb --args python train_netvlad.py -t tfsuper.logs/test 
(gdb) run
.
.
.
Thread 1 ""python"" received signal SIGSEGV, Segmentation fault.
0x00007ffef7f61a2c in std::__detail::_Map_base<std::string, std::pair<std::string const, unsigned long>, std::allocator<std::pair<std::string const, unsigned long> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
(gdb) where
#0  0x00007ffef7f61a2c in std::__detail::_Map_base<std::string, std::pair<std::string const, unsigned long>, std::allocator<std::pair<std::string const, unsigned long> >, std::__detail::_Select1st, std::equal_to<std::string>, std::hash<std::string>, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true>, true>::operator[](std::string const&) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#1  0x00007ffef7f6c4d9 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007ffef5ed94ea in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorfl---Type <return> to continue, or q <return> to quit---
ow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007ffef5ed9824 in TF_Run ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007ffef5bf701a in tensorflow::TF_Run_wrapper_helper(TF_DeprecatedSession*, char const*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007ffef5bf7411 in tensorflow::TF_Run_wrapper(TF_DeprecatedSession*, TF_Buffer const*, _object*, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/---Type <return> to continue, or q <return> to quit---
_pywrap_tensorflow_internal.so
#6  0x00007ffef5bbb6f1 in _wrap_TF_Run ()
   from /usr/local/lib/python2.7/dist-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00000000004c45fa in PyEval_EvalFrameEx ()
#8  0x00000000004c2705 in PyEval_EvalCodeEx ()
#9  0x00000000004de69e in ?? ()
#10 0x00000000004b0c93 in PyObject_Call ()
#11 0x00000000004c6ef6 in PyEval_EvalFrameEx ()
#12 0x00000000004c2705 in PyEval_EvalCodeEx ()
#13 0x00000000004ca7df in PyEval_EvalFrameEx ()
#14 0x00000000004c2705 in PyEval_EvalCodeEx ()
#15 0x00000000004ca7df in PyEval_EvalFrameEx ()
#16 0x00000000004c2705 in PyEval_EvalCodeEx ()
#17 0x00000000004ca7df in PyEval_EvalFrameEx ()
#18 0x00000000004c2705 in PyEval_EvalCodeEx ()
#19 0x00000000004ca088 in PyEval_EvalFrameEx ()
#20 0x00000000004c2705 in PyEval_EvalCodeEx ()
#21 0x00000000004c24a9 in PyEval_EvalCode ()
#22 0x00000000004f19ef in ?? ()
#23 0x00000000004ec372 in PyRun_FileExFlags ()
#24 0x00000000004eaaf1 in PyRun_SimpleFileExFlags ()
#25 0x000000000049e208 in Py_Main ()
#26 0x00007ffff7810830 in __libc_start_main (main=0x49db30 <main>, argc=4, 
    argv=0x7fffffffe558, init=<optimized out>, fini=<optimized out>, 
    rtld_fini=<optimized out>, stack_end=0x7fffffffe548) at ../csu/libc-start.c:291
#27 0x000000000049da59 in _start ()
```
"
16310,Bug of tf.data.TFRecordDataset ?  couldnot use tf.reshape to reshape the output of tf.data.TFRecordDataset,
16308,A crash found on tensorflow_jni.so when create interpreter using byteBufferMode,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
### System information
Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
•OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04.4 LTS
•TensorFlow installed from (source or binary):use the pip install
•TensorFlow version (use command below):1.4.0
•Python version: Python 2.7.6
•Bazel version (if compiling from source):0.9.0
•GCC/Compiler version (if compiling from source):(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
•CUDA/cuDNN version:NA
•GPU model and memory:NA
•Exact command to reproduce:NAYou can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I put my training model to the tflitecamerademo projects, but I encounter a crash issue.
The crash stack is

01-23 10:46:45.292 21514 21536 E AndroidRuntime: Process: android.example.com.tflitecamerademo, PID: 21514
01-23 10:46:45.292 21514 21536 E AndroidRuntime: java.lang.IllegalArgumentException: Invalid handle to Interpreter.
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.getInputDims(Native Method)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:82)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:112)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at org.tensorflow.lite.Interpreter.run(Interpreter.java:93)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at com.example.android.tflitecamerademo.ImageClassifier.classifyFrame(ImageClassifier.java:117)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.classifyFrame(Camera2BasicFragment.java:663)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment.access$900(Camera2BasicFragment.java:69)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at com.example.android.tflitecamerademo.Camera2BasicFragment$5.run(Camera2BasicFragment.java:558)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at android.os.Handler.handleCallback(Handler.java:789)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at android.os.Handler.dispatchMessage(Handler.java:98)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at android.os.Looper.loop(Looper.java:180)
01-23 10:46:45.292 21514 21536 E AndroidRuntime: 	at android.os.HandlerThread.run(HandlerThread.java:65)

I see the similar issue in https://groups.google.com/a/tensorflow.org/forum/?hl=es-VE#!topic/discuss/jJSH5RQO4Mo, but no one answer.

P.S. My training tflite file size is about 248293KB

Could you kindly to help?

Thanks

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16298,Bug of tf.data.TFRecordDataset? or my codes wrong?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Windows 10
- **TensorFlow installed from (source or binary)**:  binary
- **TensorFlow version (use command below)**:  1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0/6.0
- **GPU model and memory**: Nvidia Quadro K4000
- **Exact command to reproduce**: 


I tested to write dynamic numbers of variables into tfrecord. But when I use the tf.data.TFRecordDataset to read VarLenFeature, the program crashes. However, if I do not use dataset, but just tf.python_io.tf_record_iterator. The program works without problem. I wonder whether this is a bug of tf.data.TFRecordDataset, or there is something wrong in my codes?

My writing codes are 

    def test_write():
      writer = tf.python_io.TFRecordWriter('test.tfrecord')

      for i in range(3):
        val_list = []
        for j in range(i+1):
          val_list.append(i+j)
        feature_dict = {
          'val': tf.train.Feature(int64_list=tf.train.Int64List(value=val_list)),
        }
    
        example = tf.train.Example(features=tf.train.Features(feature=feature_dict))
        writer.write(example.SerializeToString())

      writer.close()

The reading codes using tf.data.TFRecordDataset and causing error are

	def parse_test(example):
	  features = {
		'val': tf.VarLenFeature(dtype=tf.int64)
	  }
	  parsed_features = tf.parse_single_example(example, features)

	  return parsed_features

	def test_read():
	  dataset = tf.data.TFRecordDataset(['test.tfrecord'])
	  dataset = dataset.map(parse_test)
	  dataset = dataset.batch(1)

	  iterator = dataset.make_one_shot_iterator()
	  feature_dict =  iterator.get_next()

	  with tf.Session() as sess:
		for _ in range(3):
		  curr_dict = sess.run(feature_dict)
		  print([curr_dict['val']])

The error message is:

	TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""ParseSingleExample/Slice_Indices_val:0"", shape=(?, 1), dtype=int64), values=Tensor(""ParseSingleExample/ParseExample/ParseExample:1"", shape=(?,), dtype=int64), dense_shape=Tensor(""ParseSingleExample/Squeeze_Shape_val:0"", shape=(1,), dtype=int64)). Consider casting elements to a supported type.


 The successful reading codes without using tf.data.TFRecordDataset are as below

	def test_read2():
	  with tf.Session() as sess:
		for serialized_example in tf.python_io.tf_record_iterator('test.tfrecord'):
		  features = tf.parse_single_example(serialized_example,
			features={
			  'val': tf.VarLenFeature(dtype=tf.int64),
			}
		  )

		  temp = features['val']

		  values = sess.run(temp)
		  print(values)

This code successfully print out

	SparseTensorValue(indices=array([[0]], dtype=int64), values=array([0], dtype=int64), dense_shape=array([1], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1]], dtype=int64), values=array([1, 2], dtype=int64), dense_shape=array([2], dtype=int64))
	SparseTensorValue(indices=array([[0],
		   [1],
		   [2]], dtype=int64), values=array([2, 3, 4], dtype=int64), dense_shape=array([3], dtype=int64))

However, I am still hoping to use the dataset structure to deal with the VarLenFeature. Is there anything wrong with my reading codes or there is a bug in tf.data.TFRecordDataset? Thank you.

"
16294,ScipyOptimizer SLSQP supporting callback,"The callback is deprecated when `SLSQP` method in scipy optimizer is selected (see [here](https://github.com/tensorflow/tensorflow/blob/04b5c75aae4bdbdac7c713714a369f9b360daf70/tensorflow/contrib/opt/python/training/external_optimizer.py#L400)). Actually, `SLSQP` does support callback, so 
```python 
if method == 'SLSQP':
  # SLSQP doesn't support step callbacks. Obviate associated warning
  # message.
  del minimize_kwargs['callback']
```
in the above linked file could be removed. 

The following example shows that `SLSQP` do support callback. 

```python 
from scipy.optimize import minimize, rosen, rosen_der

def callback(xk, step=[0]):
  print step[0], xk[0]
  step[0] += 1
  
x0 = [1.3, 0.7, 0.8, 1.9, 1.2]
res = minimize(rosen, x0, callback=callback, method='SLSQP',
    options={'ftol': 1e-6, 'disp': True})
 
print res.x[0]
```






"
16293,tf.errors.OutOfRangeError error not raised when using tf.train.MonitoredTrainingSession,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
I am providing a mini snippet to reproduce bug.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 7 - 64 bit
- **TensorFlow installed from (source or binary)**:
From binary for windows (tensorflow_gpu)
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.6
- **Bazel version (if compiling from source)**:
Not Applicable
- **GCC/Compiler version (if compiling from source)**:
Not Applicable
- **CUDA/cuDNN version**:
CUDA 8.5
- **GPU model and memory**:
Titan X
- **Exact command to reproduce**:
Just run the snippet as it is.

### Describe the problem

When Dataset iterator reaches at the end it should raise `tf.errors.OutOfRangeError`. But when using `tf.train.MonitoredTrainingSession` with custom hook I do not get the error. Ideally I need to get the Error.

### Source code / logs

Reproducible bug snippet:

```python
# coding=utf-8
import tensorflow as tf
import numpy as np

print(tf.GIT_VERSION, tf.VERSION)


class MyHook(tf.train.SessionRunHook):

    def __init__(
            self,
            place_holders,
            batch_size, epochs
    ):
        super(MyHook, self).__init__()
        self._create_iterator(place_holders, batch_size, epochs)
        self._session = None
        self._handle = None

    def _create_iterator(self, place_holders, batch_size, epochs):

        self._dataset = tf.data.Dataset.from_tensor_slices(
            place_holders
        ).batch(
            batch_size
        ).repeat(
            epochs
        )  # type: tf.data.Dataset

        self._iterator = self._dataset.make_initializable_iterator()
        self._next_op = self._iterator.get_next()

    def reinit(self, feed_dict):
        self._session.run(self._iterator.initializer, feed_dict=feed_dict)

    @property
    def next(self):
        return self._session.run(self._next_op)

    def after_create_session(self, session, coord):
        self._session = session


if __name__ == ""__main__"":

    data_dict = {
        'labels': np.arange(20, dtype=np.int32)
    }

    placeholders_dict = {
        'labels': tf.placeholder(
            dtype=data_dict['labels'].dtype,
            shape=data_dict['labels'].shape,
            name='labels')
    }

    feed_dict = {}
    for k, v in placeholders_dict.items():
        feed_dict[v] = data_dict[k]

    hook_twice = MyHook(place_holders=placeholders_dict, batch_size=10, epochs=2)
    hook_infinite = MyHook(place_holders=placeholders_dict, batch_size=10, epochs=None)

    with tf.train.MonitoredTrainingSession(
        hooks=[hook_twice, hook_infinite]
    ):
        hook_twice.reinit(feed_dict=feed_dict)
        hook_infinite.reinit(feed_dict=feed_dict)
        while True:
            print(""..."")
            print(hook_twice.next)
            print(hook_infinite.next)

```
As you can see `hook_twice` should repeat twice on dataset and then throw Error. Whereas `hook_infinite` should repeat infinitely but because of `hook_twice` should be abrupt-ed. But the program runs without throwing error after two iterations because of `hook_twice`.

Output:

```txt
b'unknown' 1.4.0
2018-01-22 19:55:01.223557: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2018-01-22 19:55:01.519963: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.076
pciBusID: 0000:03:00.0
totalMemory: 12.00GiB freeMemory: 11.78GiB
2018-01-22 19:55:01.519963: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\36\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0, compute capability: 5.2)
...
{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}
{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}
...
{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}
{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}
...
{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}
{'labels': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])}
...
{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}
{'labels': array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19])}
...

Process finished with exit code 0

```
"
16291,[bug] Specify GPU device error when using session_options.config.mutable_gpu_options()->set_visible_device_list,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. 
session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");
session->reset(tensorflow::NewSession(session_options)); // Error in this line
Status session_create_status = (*session)->Create(graph_def); 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.9.0
- **GCC/Compiler version (if compiling from source)**:
5.4
- **CUDA/cuDNN version**:
CUDA 8.0  /  cuDNN 6.0.21
- **GPU model and memory**:
Quadro P6000, 24G GPU memory
- **Exact command to reproduce**:


### Describe the problem
I built the tensorflow C++ API from the source using Bazel 0.9.0. When I link the shared library libtensorflow_cc.so, my code works fine. (I do not need to link libtensorflow_framework.so, since I used '--config=monolithic' when I build tensorflow using bazel.)
However, I want to specify the GPU device in my code using this function to set gpu options:
session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");
session->reset(tensorflow::NewSession(session_options)); // **Error in this line**

### Source code / logs
Logs:
2018-01-22 09:39:57.262843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1202] Found device 0 with properties: 
name: Quadro P6000 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:02:00.0
totalMemory: 23.87GiB freeMemory: 22.46GiB
2018-01-22 09:39:57.262897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1296] Adding visible gpu device 0
2018-01-22 09:39:57.584754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21801 MB memory) -> physical GPU (device: 0, name: Quadro P6000, pci bus id: 0000:02:00.0, compute capability: 6.1)
2018-01-22 09:39:57.584802: E tensorflow/core/common_runtime/gpu/process_state.cc:130] Invalid allocator type: 0
Segmentation fault (core dumped)


Source Codes:
// Reads a model graph definition from disk, and creates a session object you
// can use to run it.
Status LoadGraph(const string& graph_file_name, std::unique_ptr<tensorflow::Session>* session) 
{
    tensorflow::GraphDef graph_def;
    Status load_graph_status = ReadBinaryProto(tensorflow::Env::Default(), graph_file_name, &graph_def);
    if (!load_graph_status.ok()) 
    {
        return tensorflow::errors::NotFound(""Failed to load compute graph at '"",
                                        graph_file_name, ""'"");
    }
    //tensorflow::SessionOptions options;
    tensorflow::SessionOptions session_options;
    session_options.config.mutable_gpu_options()->visible_device_list();
    std::cout<<""list GPU done""<<std::endl;
    session_options.config.mutable_gpu_options()->set_visible_device_list(""0"");
    std::cout<<""GPU assign is done""<<std::endl;
   
    session->reset(tensorflow::NewSession(session_options));
    std::cout<<""new session is created. ""<<std::endl;
    Status session_create_status = (*session)->Create(graph_def);
    std::cout<<""Graph is loaded. ""<<std::endl;
    if (!session_create_status.ok()) 
    {
        return session_create_status;
    }
    return Status::OK();
}"
16290,Bug in session initialization?,"Initializing a variable depending on a placeholder does not work:

```python
import tensorflow as tf
import numpy as np

x = np.random.normal(loc=5, size=[10, 25])

sample_data = tf.placeholder(tf.float32)

mu = tf.Variable(tf.reduce_mean(sample_data, axis=0),
                 dtype=tf.float32,
                 validate_shape=False,
                 name=""mu"")

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer(), feed_dict={sample_data: x})
    print(sess.run(mu))

########################################### REPEAT:

x = np.random.normal(loc=5, size=[10, 25])

sample_data = tf.placeholder(tf.float32)

mu = tf.Variable(tf.reduce_mean(sample_data, axis=0),
                 dtype=tf.float32,
                 validate_shape=False,
                 name=""mu"")

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer(), feed_dict={sample_data: x})
    print(sess.run(mu))
```

### Error Message:
> tensorflow.python.framework.errors_impl.InvalidArgumentError: You must feed a value for placeholder tensor 'Placeholder' with dtype float
> 	 [[Node: Placeholder = Placeholder[dtype=DT_FLOAT, shape=<unknown>, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]

The interesting part about this is that the first initialization works, but the second one does not.
This is really annoying if you don't know that behaviour and you try to build some application with interactive python shells.

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 17.10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
3.6.3"
16289,pip package build error,"------------------------

### System information
- **OS Platform and Distribution**:

> Linux Ubuntu 16.04


- **TensorFlow installed from**:

> source

- **TensorFlow version**:

> 1.4.0


- **Python version**: 

> 2.7.12


- **Bazel version :**

> Build label: 0.9.0
> Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
> Build time: Tue Dec 19 09:31:58 2017 (1513675918)
> Build timestamp: 1513675918
> Build timestamp as int: 1513675918

- **GCC/Compiler version**

> gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5)

### Describe the problem

I am trying to build the pip package after configuration by using : 

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`

but I am getting a weird error. I have been using tensorflow but trying a fresh install gives me this output ..

### Terminal log
```
.....................
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /home/konmon01/.cache/bazel/_bazel_konmon01/4b0cf0ce18b83a1ab9f12165a3d0df4f/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /home/konmon01/tensorflow/tensorflow/core/BUILD:2215:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//tensorflow/core:sycl_runtime'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 6.310s
FAILED: Build did NOT complete successfully (100 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching https://bitbucket.org/eigen/eigen/get/429aa5254200.tar.gz; 32,768b
```

"
16288,Tensorflow works in command prompt but not in Spyder,"Hello.
I'm new to Python so maybe I've missed something but anyway, here is my problem.
I've installed tensorflow in Anaconda prompt by using 
```
C:\WINDOWS\system32> conda create -n tensorflow python=3.6
C:\WINDOWS\system32> activate tensorflow
(tensorflow)C:\WINDOWS\system32> pip install --ignore-installed --upgrade tensorflow
```
Instalation was succesful, then I opened python and tried to import tensorflow to verify installation
```
(base) C:\WINDOWS\system32>python
Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ModuleNotFoundError: No module named 'tensorflow'
```

After activating tensorflow, it works.
```
(tensorflow) C:\WINDOWS\system32>python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 10:22:32) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
>>>
```
But I cannot find a way to make it work in Spyder IDE. I always get error:
```ModuleNotFoundError: No module named 'tensorflow'```"
16287,[Bug] LuongMonotonicAttention in contrib/seq2seq/python/ops/attention_wrapper.py,"`LuongMonotonicAttention.__init__(...)` calls its parent `_BaseAttentionMechanism` with `query_layer` as follows:
```
        query_layer=layers_core.Dense(
            num_units, name=""query_layer"", use_bias=False),
```
But, it doesn't apply it on query in `LuongMonotonicAttention.__call__(...)`.
```
  def __call__(self, query, previous_alignments):
    """"""...
    """"""
    with variable_scope.variable_scope(None, ""luong_monotonic_attention"",
                                       [query]):
      score = _luong_score(query, self._keys, self._scale)
      score_bias = variable_scope.get_variable(
          ""attention_score_bias"", dtype=query.dtype,
          initializer=self._score_bias_init)
      score += score_bias
    alignments = self._probability_fn(score, previous_alignments)
    return alignments
```
Guessing from the way `LuongAttention` works, there should be `query_layer=None` in `LuongMonotonicAttention.__init__(...)`."
16285,"tf-seq2seq, nmt, tensorflow's seq2seq diff","google-seq2seq, nmt, tensorflow's seq2seq
what is the diff about them"
16284,[bug?] Tensorflow accepts CUDA_VISIBLE_DEVICES but still allocates memory on multiple GPUs,"### System information
- **I have written a custom script, but effect is also visible for just `import tensorflow as tf; sess = tf.Session()`**:
- **Windows 7 Professional**:
- **TensorFlow installed from binary (pip, in Anaconda environment)**:
- **TensorFlow version 1.4.0**:
- **Python version 3.5.4**: 
- **CUDA/cuDNN version 8.0/6.0**:
- **4 GeForce GTX 1080Ti, 11GB**:

### Problem description
I am facing the following issue:

If I set
`CUDA_VISIBLE_DEVICES=1`
and start my python script, everything works as expected, only GPU 1 is used/only memory from GPU 1 is allocated. GPU1 hosts the Desktop Window Manager.

If I set
`CUDA_VISIBLE_DEVICES=0 # or 2 or 3 `
before running my python script
`sess = tf.Session()`
faithfully reports only one available GPU (with the expected PCI bus id), see attached file ipython.txt.

However, nvidia_smi.exe shows that memory on all remaining GPUs except for GPU1 is allocated. GPU-Util shows that expected GPU is used for actual computation, see attached file nvidia_smi_output.txt.

I see this effect both for my actual tensorflow script as well for simple interactive python with

```
import tensorflow as tf
sess = tf.Session()
```

Unfortunately, the dual boot Ubuntu is not working at the moment, but once it is running again, I can try to check whether a similar effect presents itself there.
Could this be a bug, possibly related to Windows? Or a driver or hardware issue?

Attached files:
[ipython.txt](https://github.com/tensorflow/tensorflow/files/1651908/ipython.txt): ipython script and output.
[nvidia_smi_output.txt](https://github.com/tensorflow/tensorflow/files/1651905/nvidia_smi_output.txt): output of nvidia-smi.exe
"
16283,tf.edit_distance work wrang,"### System information

== cat /etc/issue ===============================================
Linux asr-eval 4.4.77-1.el7.elrepo.x86_64 #1 SMP Sat Jul 15 11:17:37 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux asr-eval 4.4.77-1.el7.elrepo.x86_64 #1 SMP Sat Jul 15 11:17:37 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
tensorflow-gpu (1.5.0rc1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc0-8-gc678970
tf.COMPILER_VERSION = v1.5.0-rc0-8-gc678970
Sanity check: array([1], dtype=int32)
/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters

== env ==========================================================
LD_LIBRARY_PATH /usr/local/nvidia/lib64:/usr/local/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Mon Jan 22 10:28:25 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.98                 Driver Version: 384.98                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  TITAN Xp            Off  | 00000000:0C:00.0 Off |                  N/A |
| 23%   29C    P0    68W / 250W |     10MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176


You can obtain the TensorFlow version with

('v1.5.0-rc0-8-gc678970', '1.5.0-rc1')

### Describe the problem
```
(<tf.Tensor: id=1, shape=(2, 7), dtype=int32, numpy=
 array([[1, 2, 3, 4, 5, 0, 0],
        [2, 2, 3, 5, 4, 0, 0]], dtype=int32)>,
 <tf.Tensor: id=81, shape=(2, 7), dtype=int32, numpy=
 array([[1, 2, 1, 3, 4, 0, 0],
        [2, 2, 1, 2, 3, 0, 0]], dtype=int32)>)

In [160]: edit
Out[160]: <tf.Tensor: id=154, shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)>
```
The truth must [3, 3]

### Source code / logs
```
t: (<tf.Tensor: id=1, shape=(2, 7), dtype=int32, numpy=
 array([[1, 2, 3, 4, 5, 0, 0],
        [2, 2, 3, 5, 4, 0, 0]], dtype=int32)>,
l: <tf.Tensor: id=81, shape=(2, 7), dtype=int32, numpy=
 array([[1, 2, 1, 3, 4, 0, 0],
        [2, 2, 1, 2, 3, 0, 0]], dtype=int32)>)

edit = tf.edit_distance(st, sl, normalize=False)

In [160]: edit
Out[160]: <tf.Tensor: id=154, shape=(2,), dtype=float32, numpy=array([2., 3.], dtype=float32)>
```"
16282,Adding go_package to proto definition files necessary for Tensorflow serving,"I've went through guidelines about calling Tensorflow serving in Python.
Then I've decided to make it Go.

You can find my repo here:
https://github.com/datainq/go-mnist-client

Manually preparing files is unmaintainable. What about we add `go_package` to proto files?
e.g. for a `github.com/tensorflow/tensorflow/tensorflow/core/protobuf/saver.proto` it would be:
```
go_package = ""github.com/tensorflow/tensorflow/tensorflow/go/core/protobuf"";
```
I think keeping the generated files in go' subpath is a good idea.

Does it make sense? 
(@jhseu was the author of the initial Go code)


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: N/A
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A"
16281,"After Working with Tensorflow cpu version for 2 days, it gave me an error on installation today","I had installed and used Tensorflow successfully but today when I opened my computer it gave me this error message:
Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/asus/PycharmProjects/untitled3/CNNCIFARTFNEW.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\imp.py"", line 297, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\asus\AppData\Local\Programs\Python\Python36-32\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ModuleNotFoundError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.


Please help. This is urgent."
16279,"While using tf.while_loop , the _Slicehelper chooses strided_slice op(req 4 args) instead of slice op(req 3 args)","I am facing this issue while creating a decoder using tf.while_loop

`
train_decoder = tf.while_loop(self.coarse_decoder_condition, self.coarse_decoder_function,
[self.iter_decoder_c, tf.zeros([1,self.c_dec_size]), tf.ones([self.c_dec_size])] ,
				shape_invariants =[self.iter_decoder_c.get_shape(),tf.TensorShape([None,self.c_dec_size]),tf.TensorShape([self.c_dec_size]) ] )`


here is what comes up in the cmd

```
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2816, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2640, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\control_flow_ops.py"", line 2576, in _BuildLoop
    c = ops.convert_to_tensor(pred(*packed_vars))

  File ""C:\Users\HP\Music\Final\nn.py"", line 200, in coarse_decoder_condition
    return it[0] < self.oplen_c[0]

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 538, in _SliceHelper
    name=name)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\array_ops.py"", line 706, in strided_slice
    shrink_axis_mask=shrink_axis_mask)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 5429, in strided_slice
    name=name)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-
packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_he
lper
    op_def=op_def)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2958, in create_op
    set_shapes_for_outputs(ret)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2209, in set_shapes_for_outputs

    shapes = shape_func(op)
  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 2159, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 627, in call_cpp_shap
e_fn
    require_shape_fn)

  File ""C:\Users\HP\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\common_shapes.py"", line 691, in _call_cpp_sha
pe_fn_impl
    raise ValueError(err.message)
ValueError: Index out of range using input dim 0; input has only 0 dims for 'Coarse_Decoder/while/strided_slice' (op: 'StridedSlice') with input shape
s: [], [1], [1], [1] and with computed input tensors: input[3] = <1>.


```

Essentially what is happening is that the while_loop => buildloop => _SliceHelper

the _sliceHelper chooses strided_slice op(which requires 4 args instead of 3) instead of just slice op

This is causing me to get an error while passing 3 args , which is what my requirement is.

A similar issue highlighted [here ](https://github.com/tensorflow/models/issues/817)

I can't manually choose slice() over strided_slice()

Any help would be appreciated @michaelisard"
16278,__init__() got multiple values for argument 'strides',"    model = Sequential()
    model.add(ZeroPadding2D((1, 1), input_shape=(img_width, img_height, 3)))
    print(model.output_shape)
    model.add(Convolution2D(64, 3, 3, strides=(
        1, 1), activation='relu', name='conv1_1'))
above is my code, I got error:
__init__() got multiple values for argument 'strides'
If i don't use 'strides', it's fine. but the stride is 3. How should I set strides?"
16277,Control dependency does not ensure write observed by read,"TF version 1.3.0

```python
def sleep(t):
    '''TF sleep'''
    import time
    def f(t):
        time.sleep(t)
        return np.array([], dtype=np.float32)
    return tf.py_func(f, [t], [tf.float32])[0]

with tf.device('gpu'):
    x = tf.Variable(0.)
with tf.control_dependencies([tf.identity(sleep(0.1))]):
    with tf.device('gpu'):
        mod = tf.assign(x, 100.)
with tf.device('cpu'):
    a = x+1.
    with tf.control_dependencies([tf.identity(mod)]):
        b = x+2.
        with tf.device('gpu'):
            c = x+3.

x.initializer.run()
sess.run([a, b, c])
# [1.0, 2.0, 103.0]
```

When a variable is read on another device, TF seems to copy once regardless of dependencies. I understand this is how TF works, but I think it would be nice to have dependencies ensure memory access order."
16276,Linking against system-installed cuda and cudnn,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian sid
- **TensorFlow installed from (source or binary)**: source (trying)
- **TensorFlow version (use command below)**: git master (commit 9fb9ac66ce) or any version before.
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.0
- **CUDA/cuDNN version**: CUDA 9.0, cuDNN 6.0
- **GPU model and memory**: GeForce GTX 1070
- **Exact command to reproduce**:
`bazel build --config=opt --config=mkl --config=cuda //tensorflow/tools/pip_package:build_pip_package`

### Describe the problem
The build system currently require that all the libraries from the CUDA toolkit are stored in a specific directory called `cuda_toolkit_path`, both in the bazel scripts and in the `configure.py` script. However, some systems (like Debian) have a packaged version of CUDA which installs the libraries in the standard path which cannot be found by `configure.py`.

The compiler can find those libraries just right with nothing more than `-lcuda`. It would be nice if the build system could rely on the compiler's ability to find its libraries instead of relying on the knowledge of their full path.

### Source code / logs
As a feature-request / enhancement-request, this section seems irrelevant."
16271,"tf.pow(x, y) edge case with negative x (Bug)","I am using tf.pow for my project, but my losses are 'nan', so I setup the test cases as shown below.
I found that whenever x is negative, tf.pow seems to output nan instead of the correct answer.

>>> r = tf.pow(0.4,0.4)
>>> r2 = tf.pow(-0.4,-0.4)
>>> r3 = tf.pow(0.4,-0.4)
>>> r4 = tf.pow(-0.4,0.4)
>>> sess.run(r)
0.69314486
>>> sess.run(r2)
nan
>>> sess.run(r3)
1.4426999
>>> sess.run(r4)
nan

I appreciate for anyone of the community who can address this issue.

Respectfully,"
16269,ContentTooShortError: <urlopen error retrieval incomplete: got only 246506328 out of 247336696 bytes>,"I have started UDACITY deep learning course.
I was copying the assignment 1 codes then I got the this error during downloading the notMNIST_large.tar.gz
Here are some screenshots of the code.
I am doing this on jupyter python3 notebook in ubuntu.

![1](https://user-images.githubusercontent.com/25321783/35194207-ca56cdc6-fed5-11e7-9fe7-3232c3741689.png)
![2](https://user-images.githubusercontent.com/25321783/35194208-cacbbc6c-fed5-11e7-9ff0-600527e4990a.png)
![error](https://user-images.githubusercontent.com/25321783/35194209-cb3fbd6a-fed5-11e7-8162-9b999b6240bb.png)
"
16268,"ValueError: Protocol message RewriterConfig has no ""layout_optimizer"" field.","I just start learning tensorflow object detection API.And now,I can use the train.py script to train my model,and use the eval.py script to evaluate normally,but when I use the export_inference_graph.py script to export .pb file,the following error occurred.my tf version is 1.4 and python version is 3.5,ubuntu14.04.thank you very much.

Traceback (most recent call last):
  File ""export_inference_graph.py"", line 110, in <module>
    tf.app.run()
  File ""/home/yj/anaconda3/envs/tensorflow3.4/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""export_inference_graph.py"", line 106, in main
    FLAGS.output_directory)
  File ""/home/yj/anaconda3/envs/tensorflow3.4/models/research/object_detection/exporter.py"", line 427, in export_inference_graph
    input_shape, optimize_graph, output_collection_name)
  File ""/home/yj/anaconda3/envs/tensorflow3.4/models/research/object_detection/exporter.py"", line 391, in _export_inference_graph
    initializer_nodes='')
  File ""/home/yj/anaconda3/envs/tensorflow3.4/models/research/object_detection/exporter.py"", line 72, in freeze_graph_with_def_protos
    layout_optimizer=rewriter_config_pb2.RewriterConfig.ON)
ValueError: Protocol message RewriterConfig has no ""layout_optimizer"" field.

I guess I installed a wrong protobuf version,but I tried 2.6.0 2.6.1 3.5.1,and the same error occured."
16266,No OpKernel was registered to support Op 'RandomShuffleQueueV2' with these attrs. ,"I build a graph.pb by python.

```
  classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,
                                          hidden_units=[60, 60, 60],
                                          n_classes=numbTrainClass,
                                          model_dir=os.path.curdir + ""/tmp/app_predict_mode1-12"")
```

When I load it on android, 

```
tf = new TensorFlowInterface(mContext.getAssets(), mPbFileName);
...
tf.run(null).
```
This is error.
```
java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'RandomShuffleQueueV2' with these attrs.  Registered devices: [CPU], Registered kernels:
  <no registered kernels>

	 [[Node: enqueue_input/random_shuffle_queue = RandomShuffleQueueV2[capacity=1000, component_types=[DT_INT64, DT_FLOAT, DT_INT32], container="""", min_after_dequeue=250, seed=0, seed2=0, shapes=[[], [9], []], shared_name="""", _device=""/device:CPU:0""]()]]
```
"
16265,ImportError: cannot import name tf,"### System information
- **Os version**: Linux Ubuntu 14.04
- **TensorFlow installed from**: binary (sudo pip  install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp27-none-linux_x86_64.whl
)
- **TensorFlow version** :1.4.1
- **Python version**: 2.7

### Describe the problem
After Install when i run the following command it throws error;
`from tensorflow import tf`

It throws
`Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name tf
`
"
16264,No module named tensorflow.python.platform,"### System information
**OS**
Linux 4.4.0-109-generic 132-Ubuntu SMP Tue Jan 9 19:52:39 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial
**Tensorflow**
Installed from source (git clone from this repository):
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc1-1379-g20f6af3
tf.COMPILER_VERSION = v1.5.0-rc1-1379-g20f6af3
Sanity check: array([1], dtype=int32)
**Python version**
Python 2.7.12
**Bazel version **
Build label: 0.9.0
**GCC/Compiler version**
gcc (Ubuntu 5.4.1-2ubuntu1~16.04) 5.4.1 20160904
nvcc Cuda compilation tools, release 9.1, V9.1.85
**CUDA/cuDNN version**
CUDA Version 9.1.85
cuDNN Version 7.0.5
**GPU model and memory**
GeForce 1080 Ti 11Gb
Driver Version: 387.26 
**Exact command to reproduce**

1. git clone https://github.com/tensorflow/tensorflow 
2. ./configure

```
WARNING: Running Bazel server needs to be killed, because the startup options are different.
You have bazel 0.9.0 installed.
Please specify the location of python. [Default is /usr/bin/python]: 


Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]
/usr/lib/python2.7/dist-packages
Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: n
No jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: n
No Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: 
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: n
No Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: 
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: 
No GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: 
No VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use, e.g. 7.0. [Leave empty to default to CUDA 9.0]: 9.1


Please specify the location where CUDA 9.1 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.0.5


Please specify the location where cuDNN 7.0.5 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
Configuration finished
```

3. bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""

```

Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed time: 1505.551s, Critical Path: 92.22s
INFO: Build completed successfully, 6091 total actions 
```

4. bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

`Sun Jan 21 09:39:44 +04 2018 : === Output wheel file is in: /tmp/tensorflow_pkg`

5. sudo pip install /tmp/tensorflow_pkg/tensorflow-1.5.0rc1-cp27-cp27mu-linux_x86_64.whl

```
Installing collected packages: tensorflow-tensorboard, tensorflow
Successfully installed tensorflow-1.5.0rc1 tensorflow-tensorboard-0.4.0
```
6.  start python & post 'import tensorflow as tf' command

```
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform
```

"
16263,losses.softmax_cross_entropy documentation on tensor rank,"### Documentation Issue
It seems `losses.softmax_cross_entropy()` works just fine with any tensor rank/shape (as long as the last dimension is classes) and not just shapes of `[batch_size, num_classes]` as the documentation indicates. (The documentation also indicates weights should be rank 0 or 1; also not true)

If this is the expected behavior, fixing the documentation would help people avoid doing unnecessary reshapes."
16262,"Cannot opened include file ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"": no such file or directory","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: source 
- **TensorFlow version (use command below)**: current git master branch, should be v1.4.1 or v1.5.0rc1?
- **Python version**: N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: MSVC2015
- **CUDA/cuDNN version**: CPU build only, gpu function is off
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Trying a minimal build with cmake, with only snappy support and optimize for native arch turned on

### Describe the problem
Build failing due to missing header files ""tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h"".

Everything build succesful except for tpu project. I don't really know how to generate the pb.h file from protoc manually. I trying to fix the problem by chaning .cmake files, but not sure which one is for tpu.

### Source code / logs
133>D:\MSVC-source\tensorflow\tensorflow\contrib\tpu\ops\tpu_embedding_ops.cc(16): fatal error C1083: Cannot open include file: 'tensorflow/contrib/tpu/proto/tpu_embedding_config.pb.h': No such file or directory

"
16261,Tensorflow Debug tfdbg ValueError with combined loss functions.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. Included
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Home x64, 1709
- **TensorFlow installed from (source or binary)**: binary (pip3)
- **TensorFlow version (use command below)**:  1.4.0
- **Python version**:  3.5.2
- **Exact command to reproduce**: See source code.

### Describe the problem
When attempting to combine two loss functions, tfdbg fails to properly grab the gradients due to a ValueError when executing run.

### Source code / logs
[consolelog.txt](https://github.com/tensorflow/tensorflow/files/1649424/consolelog.txt) Log of the error.
[not_working.txt](https://github.com/tensorflow/tensorflow/files/1649426/not_working.txt) Code example that generates the error.
[working.txt](https://github.com/tensorflow/tensorflow/files/1649427/working.txt) Code example where the two loss functions are split that does not generate the error.




"
16260,easier installation debugging,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 with Java API
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.3.0
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8 with cudnn64_6.dll
- **GPU model and memory**: 1080
- **Exact command to reproduce**: HelloTF.java

### Describe the problem
Default error message from NativeLibrary load() method is not helpful enough.   
Simple suggested improvement: please print the contents in the string variable ""frameworkResourceName"", which is the missing resource, when throwing a new UnsatisfiedLinkError exception.

### Source code / logs
Suggested Source Code Improvement for NativeLibrary.java:

    if (jniResource == null) {
      throw new UnsatisfiedLinkError(
          String.format(
              ""Cannot find TensorFlow native library %s for OS: %s, architecture: %s. See ""
                  + ""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/java/README.md""
                  + "" for possible solutions (such as building the library from source). Additional""
                  + "" information on attempts to find the native library can be obtained by adding""
                  + "" org.tensorflow.NativeLibrary.DEBUG=1 to the system properties of the JVM."",
                  frameworkResourceName,
              os(), architecture()));
    }
"
16257,feature request: KL distance for Gaussian Mixture Model,"I am hoping that **tf.contrib.distributions** module is expanded so that we can calculate **KL** divergence between **multivariate Gaussian Mixture Models(GMM)** ,with its paramter list such as weight, mean, covariance given as Tensor Array. Because I think there is going to be a more need for that for many applications. Thank you.

With current version, either  we can calculate KL divergence for a single gauss, or create GMM object, but not KL for GMM.
https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/Mixture
https://www.tensorflow.org/api_docs/python/tf/distributions/kl_divergence

I tried as shown below, but it didn'T work.

    import tensorflow as tf
    print('tensorflow ',tf.__version__)  # for Python 3
    import numpy as np
    import matplotlib.pyplot as plt

    ds = tf.contrib.distributions
    kl_divergence=tf.contrib.distributions.kl_divergence

    # Gaussian Mixure1
    mix = 0.3# weight
    bimix_gauss1 = ds.Mixture(
    cat=ds.Categorical(probs=[mix, 1.-mix]),#weight
    components=[
       ds.Normal(loc=-1., scale=0.1),
       ds.Normal(loc=+1., scale=0.5),
    ])

    # Gaussian Mixture2
    mix = 0.4# weight
    bimix_gauss2 = ds.Mixture(
        cat=ds.Categorical(probs=[mix, 1.-mix]),#weight
        components=[
            ds.Normal(loc=-0.4, scale=0.2),
            ds.Normal(loc=+1.2, scale=0.6),
    ])

    # KL between GM1 and GM2
    kl_value=kl_divergence(
        distribution_a=bimix_gauss1,
        distribution_b=bimix_gauss2,
        allow_nan_stats=True,
        name=None
    )
     sess = tf.Session() # 
     with sess.as_default():
        x = tf.linspace(-2., 3., int(1e4)).eval()
        plt.plot(x, bimix_gauss1.prob(x).eval(),'r-')
        plt.plot(x, bimix_gauss2.prob(x).eval(),'b-')
        plt.show()

        print('kl_value=',kl_value.eval())`

Then I got this error... **NotImplementedError: No KL(distribution_a || distribution_b) registered for distribution_a type Mixture and distribution_b type Mixture**

I know that with python sklearn without Tensorflow, we can calculate KL for GMM as shown below.
https://stackoverflow.com/questions/48335823/tensorflow-kl-divergence-for-a-gaussian-mixure

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:1.4
- **Python version**: 3
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8
- **GPU model and memory**:
- **Exact command to reproduce**:"
16255,tf.scatter_update Error ,"Hi, 

I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. 
Because of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. 
However, I am getting error:
LookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). 
Here are the lines of the code that I update AO and AS that gives the error:

self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, 
                    self.curTime + leadTimeIn, 
                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   

self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, 
                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], 
                    self.players[k].actionValue(self.curTime, self.playType))
                    , name='handle_scat_j')

Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? 
Is it something that you can fix it, or is there any reason behind this behavior? 

BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. 

Thanks, 
Afshin
"
16254,adding placeholder_with_default in order to feed both via dataset and placeholders produces error on GPU,"
Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: 3.6.2
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1.85
- **GPU model and memory**:Titan V, 12G
- **Exact command to reproduce**: See below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

The issue is when you want to train a classifier that takes both placeholder and dataset via queue to feed input. The reason one may want to do that is to run inference via placeholders. 

I added the following line under define the model section of train_image_classifier.py of slim library

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.


_images = tf.placeholder_with_default(image, shape=[...], name='input)
 I get an error of the following kind when running on GPU:
Cannot assign a device for operation 'input': Could not satisfy explicit device specification '/device:GPU:0' 


"
16251,Tensorflow Lite (tf-nightly) toco error for python3,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra 10.13.2
- **TensorFlow installed from (source or binary)**: yes
- **TensorFlow version (use command below)**:  1.6.0-dev20180119
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:-
- **GPU model and memory**: - 
- **Exact command to reproduce**: toco --help

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
I'm following the current tensorflow codelab https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/index.html?index=..%2F..%2Findex#3 however it seems that toco doesn't run under a python 3.6 env according to the output of the terminal

### Source code / logs
command input/output :

```
└─[$] <git:(master)> toco --help
2018-01-19 22:15:19.449190: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
Traceback (most recent call last):
  File ""/Users/macuser/Projects/Tensorflow/bin/toco"", line 11, in <module>
    sys.exit(main())
  File ""/Users/macuser/Projects/Tensorflow/lib/python3.6/site-packages/tensorflow/contrib/lite/toco/python/toco_wrapper.py"", line 35, in main
    os.execvp(binary, sys.argv)
  File ""/Users/macuser/Projects/Tensorflow/bin/../lib/python3.6/os.py"", line 559, in execvp
    _execvpe(file, args)
  File ""/Users/macuser/Projects/Tensorflow/bin/../lib/python3.6/os.py"", line 583, in _execvpe
    exec_func(file, *argrest)
PermissionError: [Errno 13] Permission denied

```

using sudo also doesn't help. There is also a stackoverflow issue logged here: 
https://stackoverflow.com/questions/43322964/permission-denied-when-installing-tensorflow


"
16248,[bug?] Error in `python': malloc(): memory corruption,"Error in `python': malloc(): memory corruption: 0x00000000723f9040

Strangely encountered this error when the training was going, it happened after a certain number of iterations (~7000 iterations with image batch size of 1 using coco dataset).  I have also attached the memory map that showed up after the backtrace.

```
*** Error in `python': malloc(): memory corruption: 0x00000000723f9040 ***
======= Backtrace: =========
/lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f9d5715a7e5]
/lib/x86_64-linux-gnu/libc.so.6(+0x8213e)[0x7f9d5716513e]
/lib/x86_64-linux-gnu/libc.so.6(__libc_malloc+0x54)[0x7f9d57167184]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x1ec51)[0x7f9d560f0c51]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x842c8)[0x7f9d561562c8]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x844a4)[0x7f9d561564a4]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x84920)[0x7f9d56156920]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x85105)[0x7f9d56157105]
/usr/local/lib/python2.7/dist-packages/numpy/core/multiarray.so(+0x11b16d)[0x7f9d561ed16d]
/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x9406)[0x7f9d1f276406]
/home/user/tools/../data/coco/PythonAPI/pycocotools/_mask.so(+0x1c05f)[0x7f9d1f28905f]
python(PyEval_EvalFrameEx+0x615e)[0x4ca15e]
python(PyEval_EvalFrameEx+0x5d8f)[0x4c9d8f]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x68d1)[0x4ca8d1]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalFrameEx+0x6099)[0x4ca099]
python(PyEval_EvalCodeEx+0x255)[0x4c2765]
python(PyEval_EvalCode+0x19)[0x4c2509]
python[0x4f1def]
python(PyRun_FileExFlags+0x82)[0x4ec652]
python(PyRun_SimpleFileExFlags+0x191)[0x4eae31]
python(Py_Main+0x68a)[0x49e14a]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f9d57103830]
python(_start+0x29)[0x49d9d9]
```


```
System Information
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

Compiler:
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609

PIPs:
msgpack-numpy (0.4.2)
protobuf (3.5.0.post1)
tensorflow-gpu (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

TensorFlow:
tf.VERSION = 1.4.0
tf.GIT_VERSION = v1.4.0-rc1-11-g130a514
tf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514
Sanity check: array([1], dtype=int32)

Env:
LD_LIBRARY_PATH /usr/lib/x86_64-linux-gnu:/usr/local/lib:/usr/local/cuda/lib64:
DYLD_LIBRARY_PATH is unset

GPU:
TITAN Xp

CUDA Lib:
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
```
[memory map.txt](https://github.com/tensorflow/tensorflow/files/1647515/memory.map.txt)

"
16247,[doc][feature request] Graphical representation of operations,"It would be cool if the documentation of TF operations would contain graphical examples.

E.g. the ""tf.dynamic_partition"" operation contains such a visualization:
![image](https://user-images.githubusercontent.com/1200058/35163918-880ec02e-fd48-11e7-944f-7d3aac7cadc5.png)

This would be especially helpful to understand the different slicing/joining operations."
16246,Failed to build error: mismatched argument pack lenghts...,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 4.14.13-1-ARCH
- **TensorFlow installed from (source or binary)**: git
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0-1
- **GCC/Compiler version (if compiling from source)**: 6.4.1
- **CUDA/cuDNN version**:  9.1.85-1 / 7.0.5-2
- **Exact command to reproduce**:
./configure
bazel build --config=opt --config=cuda --jobs 12 //tensorflow/tools/pip_package:build_pip_package


### Describe the problem
failed to build

### Source code / logs

> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:489:65: error: mismatched argument pack lengths while expanding 'std::is_convertible<_UElements&&, _Elements>'
>        return __and_<is_convertible<_UElements&&, _Elements>...>::value;
>                                                                  ^~~~~
> /usr/lib/gcc/x86_64-pc-linux-gnu/6.4.1/include/c++/tuple:490:1: error: body of constexpr function 'static constexpr bool std::_TC<<anonymous>, _Elements>::_ImplicitlyMoveConvertibleTuple() [with _UElements = {const std::tuple<tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece>}; bool <anonymous> = true; _Elements = {tensorflow::VariantBinaryOp, tensorflow::StringPiece, tensorflow::StringPiece}]' not a return-statement
>      }
>  ^
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: output 'tensorflow/core/kernels/_objs/list_kernels_gpu/tensorflow/core/kernels/list_kernels.cu.pic.o' was not created
> ERROR: /home/user/dev/git/tensorflow/tensorflow/core/kernels/BUILD:1884:1: not all outputs were created or valid
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 29.727s, Critical Path: 28.35s
> FAILED: Build did NOT complete successfully
> "
16244,Benchmarking GPU ops in Tensorflow Graphs,"I tried using the tool for benchmarking Tensorflow Graphs at
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/benchmark

Seems it only gives the memory profile for RAM per ops. How can I get the GPU memory and utilization profile per ops using this tool?

"
16240,graph_metrics.py does not work well,"I want to use function in graph_metrics.py, I run corresponding test file graph_metrics_test.py, but I get the following assertion error for ""weight_parameters"" metric
```

line 32, in testGraphMetrics
    self.assertEqual(expected[statistic_type], current_stats.value)
AssertionError: 100 != None
```
has anyone encounter this problem?"
16239,Not supported for GpuManagedAllocator,"GpuManagedAllocator was early supported at tensorflow/core/common_runtime/gpu/gpu_managed_allocator.cc.
But it seems there is no choice for users to use it according to the source code?
The GpuManagedAllocator can help to enlarge virtual GPU memory to fit huge training models, so it is very important to support them.
So what is the plan next about this feature?"
16238,//tensorflow/contrib/gan:losses_impl_test fails with AssertionError ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04  s390x
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.7.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: No GPU
- **GPU model and memory**: NA
- **Exact command to reproduce**: bazel test -c opt //tensorflow/contrib/gan:losses_impl_test

### Describe the problem
One of the sub-test `test_stable_global_norm_unchanged` fails on s390x with 
`AssertionError: 110.709068 != 110.709084 +/- 0.000010`

Seems like a minor difference, so I tried changing the tolerance slightly as below:
```
-        self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
+        self.assertNear(gnorm_np, precond_gnorm_np, 2e-5)
```
with this the test is passing.

Is it ok to create a PR with this change? Could you please share your thoughts on this.

### Source code / logs
```
.......................F..................................................................................
======================================================================
FAIL: test_stable_global_norm_unchanged (__main__.CombineAdversarialLossTest)
Test that preconditioning doesn't change global norm value.
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/contrib/gan/python/losses/python/losses_impl_test.py"", line 602, in test_stable_global_norm_unchanged
    self.assertNear(gnorm_np, precond_gnorm_np, 1e-5)
  File ""/home/test/.cache/bazel/_bazel_test/774d974934abfb88e6e5d6a13042805c/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/gan/losses_impl_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 879, in assertNear
    if msg is not None else """"))
AssertionError: 110.709068 != 110.709084 +/- 0.000010

----------------------------------------------------------------------
Ran 106 tests in 9.119s

FAILED (failures=1)
```

"
16235,Feature Request: Make NDLSTM use state_is_tuple=True,"I have successfully used [NDLSTM](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/ndlstm) (specifically lstm2d.separable_lstm) in my own project but whenever I use it, I enocunter this warning: `""Using a concatenated state is slower and will soon be deprecated. use state_is_tuple=true.""`

The warning is caused by  `ndlstm_base_dynamic` in lstm1d.py. Specifically, this line: `lstm_cell = rnn_cell.BasicLSTMCell(noutput, state_is_tuple=False)`

I modified the code such that the deprecation warning won't appear:

```
with variable_scope.variable_scope(scope, ""SeqLstm"", [inputs]):
    lstm_cell = rnn_cell.BasicLSTMCell(noutput)
    if reverse:
      inputs = array_ops.reverse_v2(inputs, [0])
    outputs, _ = rnn.dynamic_rnn(
        lstm_cell, inputs, time_major=True, dtype=inputs.dtype)
    if reverse:
      outputs = array_ops.reverse_v2(outputs, [0])
    return outputs
```

Before I make any pull requests, is there a reason why the `state_is_tuple` argument is set to `False` in the code?"
16234,The recognized result is not correct when converting the frozen graph to tflite for android device use ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: mac High Sierra
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:c++/4.2.1
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

The detailed system information, you can check the url:
https://drive.google.com/file/d/19oKikJ0PcGHx9daauub28IYb8J3hA-rw/view?usp=sharing

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi, I covert the frozen graph:mobilenet_v1_224 to tflite, and put it in the tflitecamerademo app, but the regonization result is not correct. If I use the tflite file which download from https://storage.googleapis.com/download.tensorflow.org/models/tflite/mobilenet_v1_224_android_quant_2017_11_08.zip

The regonization result is correct, I don't know what steps is not correct when I covert the frozeon graph to tflite file, could you help me to review what steps is the wrong?

I put the frozen graph, coverting tflite file and the regonized picture in the https://drive.google.com/drive/folders/12h9O2AtcnDuQZXogAdmexRSjxIQVc1Ej?usp=sharing

The correct result should be ""malamute"", but I use the my coverting tflite file, the result is ""shower curtain""

I use the command to do the covert
bazel run --config=opt //tensorflow/contrib/lite/toco:toco -- '--input_file=/tmp/mobilenet_frozen_graph.pb' '--output_file=/tmp/mobilenet_quant_20180117.tflite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=QUANTIZED_UINT8' '--inference_input_type=QUANTIZED_UINT8' '--input_shapes=1,224,224,3' '--input_arrays=input' '--output_arrays=MobilenetV1/Predictions/Reshape_1' '--mean_values=128' '--std_values=128' '--default_ranges_min=0' '--default_ranges_max=6'

Thanks

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16231,x86_64 compilation failed,"### System information

- **MacOS High Sierra 10.13.2**:
- **Python 3.6.3**:
- **TensorFlow Latest Pull from 1/17/18**:

### Describe the problem
I am following Pete Warden's TensorFlow for Mobile Poets guide and seem to have a found an error. When I run ""tensorflow/contrib/makefile/build_all_ios.sh"" after about 20 minutes it returns an error. 

I have tried running lipo -info /Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a

and this returns: 

Architectures in the fat file:
/Users/ryan/Downloads/tensorflow2/tensorflow/contrib/makefile/gen/protobuf_ios/lib/libprotobuf.a are: i386 

I have the entire error script here:
https://drive.google.com/file/d/1JovTMGBJKbqzRPBzXy3cIQ-hbz76n0ab/view?usp=sharing

### Source code / logs
ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see 
invocation)
make: *** [/Users/ryan/Desktop/tensorflow-
master/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1
"
16230,A new S3Client is created with all file operations.,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X 10.12.6
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: commit 4595f1cff635ce024e875f0f3d480172731b0b22
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.5.4-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 9.0.0 (clang-900.0.39.2)
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A

### Describe the problem

The [S3 filesystem](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/platform/s3/s3_file_system.cc) creates a new `Aws::S3::S3Client` object with all interactions with S3. This is a heavyweight object, and takes relatively large amount of time to create and destroy.

This should be a singleton associated with the filesystem object.

Fix shortly."
16228,tf.contrib.rnn.LSTMCell()  dtype not defined. Error when creating initializer for bias variable. ,"- **OS Platform and Distribution**: Mac OSX 10.10.5
- **TensorFlow installed from**: binary
- **TensorFlow version**: 1.5.0rc
- **Python version**:   3.6
- **Have I written custom code**: NA
- **Bazel version**: NA
- **CUDA/cuDNN version**: NA
- **GPU model and memory**:NA

- **Exact command to reproduce**:

import tensorflow as tf
lstm = tf.contrib.rnn.LSTMCell(10)
input_tensor = tf.ones([10,50])
lstm.build(input_tensor.get_shape())


Traceback (most recent call last):
  File ""/Applications/PyCharm CE.app/Contents/helpers/pydev/_pydevd_bundle/pydevd_exec2.py"", line 3, in Exec
    exec(exp, global_vars, local_vars)
  File ""<input>"", line 1, in <module>
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 719, in build
    initializer=init_ops.zeros_initializer(dtype=self.dtype))
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py"", line 88, in __init__
    self.dtype = dtypes.as_dtype(dtype)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py"", line 627, in as_dtype
    ""Cannot convert value %r to a TensorFlow DType."" % type_value)
TypeError: Cannot convert value None to a TensorFlow DType.

### Describe the problem
It seems that LSTMCell does get the dtype in __init__ and does not pass to parent object. Then lstm._dtype is always None. 
A workearound is to add:
lstm._dtype = 'float32'
before:
lstm.build(input_tensor.get_shape())
"
16227,tf.contrib.factorization.KMeansClustering cannot save model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux
- **TensorFlow version**: 1.4
- **Python version**: 2.7.6
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
In [tf.contrib.factorization.KMeansClustering](https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering#model_fn ), the TensorFlow 1.4 version of the KMeans Estimator ([previous version](https://www.tensorflow.org/api_docs/python/tf/contrib/learn/KMeansClustering)), the [export_savedmodel](https://www.tensorflow.org/api_docs/python/tf/contrib/factorization/KMeansClustering#export_savedmodel) function throws an error: 

`ValueError: export_outputs must be a dict and not<type 'NoneType'>`

As far as I can tell, the older version used the function [here](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/contrib/learn/python/learn/estimators/model_fn.py#L236) to populate export_outputs from the prediction values. The newer version does not do this, rendering it impossible to create a saved model. Instead the [model_fn](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/python/ops/kmeans.py#L210) returns an EstimatorSpec with no export_outputs. 

To replicate the issue, create and train a simple contrib.factorization.KMeansClustering Estimator and try to save it.

`kmeans = tf.contrib.factorization.KMeansClustering(num_clusters = num_clusters)`
`kmeans.train(input_fn = inputFn)`
`kmeans.export_savedmodel(export_dir, exportFn)`"
16226,Include netstat in the tensorflow docker container,"### Describe the problem
This is a feature request to add net-tools to the Tensorflow docker containers.  Having netstat in the Tensorflow container will make it easier to find open ports in a multi-tenant environment when launching Tensorflow Distributed or Tensorboard.

Note, I have found how to add netstat (see URL below), but would prefer not having to change or maintain a modified version of the Tensorflow container.

https://stackoverflow.com/questions/41961217/installing-netstat-on-docker-linux-container

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:NA
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:tensorflow/tensorflow:1.3.0 docker container
- **TensorFlow installed from (source or binary)**:docker container
- **TensorFlow version (use command below)**:1.3.0
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**:NA
- **GCC/Compiler version (if compiling from source)**:NA
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:netstat
"
16225,maxout lose the number of features in the shape of its output,"In tf.contrib.layers.maxout(), when the shape of ""inputs"" is not completely specified, the shape of its output will be completely unknown, such as [None, None, None] in the 3d case.
Since ""num_units"" has specified the final number of features in the maxout axis, the output should set its shape accordingly:
https://github.com/tensorflow/tensorflow/pull/16114"
16221,Meaning of report_tensor_allocations_upon_oom output,"Python: 3.6.2
OS: Ubuntu 16.04
Tensorflow: 1.5.0rc1

When running a session with `tf.RunOptions` and `report_tensor_allocations_upon_oom=True` I get the following output at the end of my log.

1. I am wondering why some entries occur multiple times? How can a single node have multiple allocations? Why are they not summed?
2. Does `Remaining 1252 nodes with 98.80MiB` mean that all 1252 nodes together use 98.80MiB or each single one uses that amount?
3. When summing up all values I get `10.607822265625GiB` but my free GPU space when starting my program is `11.92GiB` so shouldn't there still be enough space??

```
Current usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc
  250.78MiB from network/convolutions/conv2d_5/Conv2D
  217.34MiB from network/convolutions/conv2d_5/Conv2D
  203.75MiB from network/convolutions/conv2d_5/Conv2D
  192.91MiB from network/convolutions/conv2d_11/Conv2D
  168.05MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_2/Conv2D
  167.19MiB from network/convolutions/conv2d_3/Conv2D
  167.19MiB from network/convolutions/conv2d_4/Conv2D
  167.19MiB from network/convolutions/conv2d_5/Conv2D
  167.19MiB from network/convolutions/conv2d_11/Conv2D
  163.84MiB from network/convolutions/conv2d_7/Conv2D
  160.50MiB from network/convolutions/conv2d_7/Conv2D
  140.99MiB from network/convolutions/conv2d_12/Conv2D
  133.75MiB from network/convolutions/conv2d_6/Conv2D
  133.75MiB from network/convolutions/conv2d_7/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_11/Conv2D
  133.75MiB from network/convolutions/conv2d_12/Conv2D
  103.66MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_6/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_7/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_8/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_9/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  83.59MiB from network/convolutions/conv2d_10/Conv2D
  Remaining 1252 nodes with 98.80MiB
```"
16219,build&link tensorflow lite c++ library Error,"System information

Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
TensorFlow installed from (source or binary): source
TensorFlow version (use command below): v1.4.0-19-ga52c8d9 1.4.1
Python version: 2.7.12
Bazel version (if compiling from source): 0.8.1
GCC/Compiler version (if compiling from source): g++ 5.4.0
CUDA/cuDNN version: none
GPU model and memory: none
Exact command to reproduce: g++ -std=c++11 -I...tensorflow -L. -lframework demo.cpp
Describe the problem

I run 'bazel build //tensorflow/contrib/lite:framework' and get libframework.so. Then I use libframework.so in my own code, but get undefined reference error when compile with g++:
/temp/ccYTZw2h.o: In function 'main':
demo.cpp:(.text+0x46): undefined reference to 'tflite::DefaultErrorReporter()'
demo.cpp:(.text+0x6a): undefined reference ro 'tflite::FlatBufferModel::BuildFromFile(char const*, tflite::ErrorReporter*)'
......

I get following lines by 'nm libframework.so | grep 'DefaultErrorReporter'':
000000000001b1b0 b _ZGVZN6tflite20DefaultErrorReporterEvE14error_reporter
0000000000007990 T _ZN6tflite20DefaultErrorReporterEv
000000000001b1a8 b _ZZN6tflite20DefaultErrorReporterEvE14error_reporter

I'm not familiar with how to use tensorflow lite. Where is the problem could be?"
16218,[feature request ? ]  How to return SparseTensor when custom ops,"------------------------
### Describe the problem
there exists some ops (eg: decode_libsvm) that can return SparseTensor by three dense tensor
```
#see: tensorflow/tensorflow/contrib/libsvm/
REGISTER_OP(""DecodeLibsvm"")
    .Input(""input: string"")
    .Output(""label: label_dtype"")
    .Output(""feature_indices: int64"")
    .Output(""feature_values: dtype"")
    .Output(""feature_shape: int64"")
    .Attr(""dtype: {float, double, int32, int64} = DT_FLOAT"")
    .Attr(""label_dtype: {float, double, int32, int64} = DT_INT64"")
    .Attr(""num_features: int >= 1"")
```
Can I define my custom ops which can return SparseTensor directly? ,  

What I want to do is modify tf.decode_csv ,  some column can return Tensor,  some column can return SparseTensor. 
 If i return SparseTensor by (indices,values, shape) dense tensor,  it would be  diffcult . 
does there exists SparseTensor class in c++ api ?
@all, @mrry @yongtang  
Thanks

```
  def parse_csv(value):
    print('Parsing', data_file)
    columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)
    features = dict(zip(_CSV_COLUMNS, columns))
    labels = features.pop('income_bracket')
    return features, tf.equal(labels, '>50K')
```

### Have I written custom code
### OS Platform and Distribution
### TensorFlow installed from
### TensorFlow version
### Bazel version
### CUDA/cuDNN version
### GPU model and memory
### Exact command to reproduce"
16216,Resize tensor in tflite,"hi,I save my cnn model into pb file, and  change it into tflite model.  In android I can run the model well with tensorflow lite. 

 I have a problem that the input tensor size of inference must be euqal to the input tensor size of training, because the intermediate variables  size of the model are determined by the input tensor size of training. But for my  cnn model, it can process the input tensor size that is not euqal to the input tensor size of training. 

 so  How can  i reshape the intermeidate varibales shape to match the  input tensor size of inference before inference? In other words， How can i do to let my cnn model process the input tensor whose size is not euqal to the input tensor size of traning? thx."
16215,Tensorflow doesn't delete previous checkpoints,"### System information
- Linux Ubuntu 16.04:
- Tensorflow version 1.4.1*:
- Python 3.5.2: 

### Describe the problem

A brief summary is that, if I run multiple times my training script tensorflow doesn't delete the checkpoints created in previous runs of the script.

I am preparing a automatic script that every X days runs and train with the new data collected. But I am facing a problem, even that I have configured the saver to keep the 2 last checkpoints, it doesn't work as I expected. 

Example:
I configure to run 100.000 iterations and each 10.000 to save the checkpoint. The system works and starts saving 10.000, 20.000, ... And when get to 30.000 starts deleting the firsts checkpoints. When the script ends I have the 2 last checkpoints(90.000 and 100.000). 

Then when I train again the system starts from the last checkpoint, in this example the 100.000, and do the same as the previous, 110.000, 120.000,.. and when gets to the 130.000 starts to delete the 100.000 and so on. But the 2 checkpoints from the previous run(90.000 and 100.000) remain there even that in the checkpoint txt are not listed there.

This will be repeated in every run of the script, creating files that I don't need anymore and growing during the time.

This is an intended behavior(expecting to the user to delete or manage manually) or it is really a problem?
It exist any workaround?

Thank you for your time and amazing work. 
 "
16214,Unable to locate package cuda-command-line-tools,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:r1.5
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:9.0
- **GPU model and memory**:
- **Exact command to reproduce**:
### Describe the problem
In the official installing guide of r1.5, the libcupti-dev library is required to run tensorflow with GPU support. When issue the following command line for CUDA Toolkit >= 8.0:
`$ sudo apt-get install cuda-command-line-tools`
I got this error:
`$ E: Unable to locate package cuda-command-line-tools`
It can't be solved after updating source list.
I have tried  on my desktop and a VM instance on Google Cloud Platform, both with Linux Ubuntu 16.04.
### Source code / logs"
16213,Non-chief replicas freeze after chief completes training,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9, 1.4.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 6.0
- **GPU model and memory**: Titan X (Pascal), 12 GiB
- **Exact command to reproduce**: Custom Script

### Describe the problem
Non-chief replicas freeze after chief completes training, when in synchronous mode.
The problem appears to occur immediately after the chief shuts down.

See attached source code for a complete example of multi-GPU demonstrating the problem on a toy dataset. Modify the cluster variable and start the PS first, followed by workers then the chief node last. This is somewhat broken out in run_distributed.sh.

``
        sv = tf.train.Supervisor(
            is_chief=(FLAGS.task_index == 0),
            global_step = global_step,
            init_op = init_op
        )
        ...
        with sv.prepare_or_wait_for_session(server.target, config=config) as sess:
            # is chief
            if FLAGS.task_index == 0:
                sv.start_queue_runners(sess, [chief_queue_runner])
                sess.run(init_token_op)
        ...
``

### Source code / logs
[rnn-multi-gpu.zip](https://github.com/tensorflow/tensorflow/files/1641799/rnn-multi-gpu.zip)

"
16209,process blocked in session.run,"when i use a process to read a frame from queue what an other process put the frame in from video stream。 but process blocked in session.run。 but the issue doesn't occurred when i used just one process。

below is my code：
process of handle frame 
```
def parse_origin_video_frame(origin_frame, session, detection_graph, category_index):
    image_np_expanded = np.expand_dims(origin_frame, axis=0)
    image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

    # Each box represents a part of the image where a particular object was detected.
    boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

    # Each score represent how level of confidence for each of the objects.
    # Score is shown on the result image, together with the class label.
    scores = detection_graph.get_tensor_by_name('detection_scores:0')
    classes = detection_graph.get_tensor_by_name('detection_classes:0')
    num_detections = detection_graph.get_tensor_by_name('num_detections:0')

    # Actual detection.
    (boxes, scores, classes, num_detections) = session.run(
        [boxes, scores, classes, num_detections],
        feed_dict={image_tensor: image_np_expanded})

    # Visualization of the results of a detection.
    vis_util.visualize_boxes_and_labels_on_image_array(
        origin_frame,
        np.squeeze(boxes),
        np.squeeze(classes).astype(np.int32),
        np.squeeze(scores),
        category_index,
        use_normalized_coordinates=True,
        line_thickness=2)

    return origin_frame, scores, classes, boxes

 def run(self):
        # Load a (frozen) Tensorflow model into memory.
        detection_graph = tf.Graph()
        with detection_graph.as_default():
            od_graph_def = tf.GraphDef()
            with tf.gfile.GFile(PathManager.get_ckpt_path(), 'rb') as fid:
                serialized_graph = fid.read()
                od_graph_def.ParseFromString(serialized_graph)
                tf.import_graph_def(od_graph_def, name='')

            sess = tf.Session(graph=detection_graph)

        n = 0
        t = time.time()
        while True:
            operation_id, frame_id, origin_frame = self.__in_queue.get()
            # try:
            #     operation_id, frame_id, origin_frame = self.__in_queue.get(False, 0.1)
            # except Empty:
            #     print 'video parser queue is empty'
            #     time.sleep(1)
            #     continue

            n += 1
            now = time.time()
            if now - t > 1:
                print ""parse FPS: "", n, "" time: "", now - t,
                print "" __in_queue: "", self.__in_queue.qsize(), "" __detected_queue: "", self.__detected_queue.qsize()
                t = now
                n = 0

            updated_frame, score, classes, boxes = parse_origin_video_frame(origin_frame,
                                                                            sess,
                                                                            detection_graph,
                                                                            self.__category_index)
```
process of read video capture：
```
 def run(self):
        thread.start_new_thread(self.__recv_msg_from_main,(""recv_main_process"", ""11""))
        # thread.start_new_thread(self.__get_out_queue,(""get_out_queue"", ""11""))

        video_capture = WebcamVideoStream(ConfigManager.get_sources(),
                                          ConfigManager.get_width(),
                                          ConfigManager.get_height()).start()
        fps = FPS().start()
        n = 0
        t = time.time()
        while True:
            if self.__gate_open:
                origin_frame = video_capture.read()
                time.sleep(0.1)
                n += 1
                now = time.time()
                if now - t > 1:
                    print ""read FPS: "", n, "" time: "", now - t,
                    print "" __in_queue: "", self.__in_queue.qsize(), "" output_q: "", self.__out_queue.qsize()
                    t = now
                    n = 0

                frame_rgb = cv2.cvtColor(origin_frame, cv2.COLOR_BGR2RGB)
                self.__in_queue.put((self.__operation_id, self.__frame_id, frame_rgb))
                self.__frame_id += 1
            else:
                time.sleep(0.1)
```

below is the stack：
```
(gdb) thread apply all bt

Thread 17 (Thread 0x7fa347eba700 (LWP 31711)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58
#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#3  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 16 (Thread 0x7fa3476b9700 (LWP 31712)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58
#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#3  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 15 (Thread 0x7fa346eb8700 (LWP 31713)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58
#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#3  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 14 (Thread 0x7fa3466b7700 (LWP 31714)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa348579218 in th_worker (tidptr=<optimized out>) at numexpr/module.cpp:58
#2  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#3  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 13 (Thread 0x7fa32d393700 (LWP 31715)):
#0  0x00007fa3710e5a0b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007fa3710e5a9f in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007fa3710e5b3b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007fa371403856 in PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#4  0x00007fa371407941 in lock_PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#5  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#6  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#7  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#9  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#10 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#11 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#12 0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#13 0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#14 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#15 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#16 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#17 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#18 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#19 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#20 0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 12 (Thread 0x7fa32cb92700 (LWP 31716)):
#0  0x00007fa3710e6a9b in recv () from /lib64/libpthread.so.0
#1  0x00007fa35afee3b6 in sock_recv_guts () from /root/anaconda2/lib/python2.7/lib-dynload/_socket.so
#2  0x00007fa35afee5c1 in sock_recv () from /root/anaconda2/lib/python2.7/lib-dynload/_socket.so
#3  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#4  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#5  0x00007fa3713610c7 in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#6  0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#7  0x00007fa3713d14d0 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#9  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#10 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#11 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#12 0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#13 0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#14 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#15 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#16 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#17 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#18 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
---Type <return> to continue, or q <return> to quit---
#19 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#20 0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 11 (Thread 0x7fa327fff700 (LWP 31717)):
#0  0x00007fa3710e670d in read () from /lib64/libpthread.so.0
#1  0x00007fa3680e247f in conn_recv_string.isra.2 () from /root/anaconda2/lib/python2.7/lib-dynload/_multiprocessing.so
#2  0x00007fa3680e26f0 in connection_recv_obj () from /root/anaconda2/lib/python2.7/lib-dynload/_multiprocessing.so
#3  0x00007fa3713d6192 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#4  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#5  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#6  0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#7  0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#9  0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#10 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#11 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#12 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#13 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#14 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#15 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#16 0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 10 (Thread 0x7fa324c14700 (LWP 31720)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 9 (Thread 0x7fa31bfff700 (LWP 31721)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 8 (Thread 0x7fa31b7fe700 (LWP 31722)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 7 (Thread 0x7fa31affd700 (LWP 31723)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
---Type <return> to continue, or q <return> to quit---
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 6 (Thread 0x7fa31a7fc700 (LWP 31724)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 5 (Thread 0x7fa319ffb700 (LWP 31725)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 4 (Thread 0x7fa3197fa700 (LWP 31726)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 3 (Thread 0x7fa318ff9700 (LWP 31727)):
#0  0x00007fa3710e3945 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007fa35b678a6c in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /lib64/libstdc++.so.6
#2  0x00007fa35bf9e727 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WaitForWork(Eigen::EventCount::Waiter*, tensorflow::thread::EigenEnvironment::Task*) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007fa35bf9f19e in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(int) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#4  0x00007fa35bf9ded2 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()
   from /root/anaconda2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#5  0x00007fa35b67c2b0 in ?? () from /lib64/libstdc++.so.6
#6  0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#7  0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 2 (Thread 0x7fa3269fe700 (LWP 31731)):
#0  0x00007fa3710e5a0b in do_futex_wait.constprop.1 () from /lib64/libpthread.so.0
#1  0x00007fa3710e5a9f in __new_sem_wait_slow.constprop.0 () from /lib64/libpthread.so.0
#2  0x00007fa3710e5b3b in sem_wait@@GLIBC_2.2.5 () from /lib64/libpthread.so.0
#3  0x00007fa371403856 in PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#4  0x00007fa371407941 in lock_PyThread_acquire_lock () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#5  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#6  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#7  0x00007fa3713d5482 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#8  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
---Type <return> to continue, or q <return> to quit---
#9  0x00007fa3713610c7 in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#10 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#11 0x00007fa3713d14d0 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#12 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#13 0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#14 0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#15 0x00007fa371360fda in function_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#16 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#17 0x00007fa37134b50d in instancemethod_call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#18 0x00007fa37133c773 in PyObject_Call () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#19 0x00007fa3713ce6d8 in PyEval_CallObjectWithKeywords () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#20 0x00007fa371407d46 in t_bootstrap () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#21 0x00007fa3710dfe25 in start_thread () from /lib64/libpthread.so.0
#22 0x00007fa37070434d in clone () from /lib64/libc.so.6

Thread 1 (Thread 0x7fa3718d7740 (LWP 31705)):
#0  0x00007fa3706fb7a3 in select () from /lib64/libc.so.6
#1  0x00007fa369cda144 in time_sleep () from /root/anaconda2/lib/python2.7/lib-dynload/time.so
#2  0x00007fa3713d6615 in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#3  0x00007fa3713d6dac in PyEval_EvalFrameEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#4  0x00007fa3713d84e9 in PyEval_EvalCodeEx () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#5  0x00007fa3713d870a in PyEval_EvalCode () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#6  0x00007fa3713f193d in run_mod () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#7  0x00007fa3713f2ab8 in PyRun_FileExFlags () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#8  0x00007fa3713f3cd8 in PyRun_SimpleFileExFlags () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#9  0x00007fa371405d3c in Py_Main () from /root/anaconda2/bin/../lib/libpython2.7.so.1.0
#10 0x00007fa37062dc05 in __libc_start_main () from /lib64/libc.so.6
#11 0x000055e5ed1a387f in _start ()
```
"
16208,using string_input_producer with train dataset and validate dataset,"I have two datasets(files), for train and validate respectively. I can successfully load training set thru tf.train.string_input_producer, set num_epochs=5. Then I can iteratively get batch of data to optimize my model.
But, I got stuck when trying to load my validation set by the same way, the program keeps saying ""OutOfRange Error"" even I didn't set num_epochs in string_input_producer.
Can you supply an example that using string_input_producer  with two or more dataset?
same as the question on stackoverflow: [here](https://stackoverflow.com/questions/37068324/read-big-train-validation-test-datasets-in-tensorflow)
Please help me solve the problem. Thank you very much.
"
16207,Socket issue of run whl,"Hi ,
I have a problem about run tensorflow .whl.  as follow:

Exception:
Traceback (most recent call last):
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/basecommand.py"", line 215, in main
    status = self.run(options, args)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/commands/install.py"", line 335, in run
    wb.build(autobuilding=True)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/wheel.py"", line 749, in build
    self.requirement_set.prepare_files(self.finder)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py"", line 380, in prepare_files
    ignore_dependencies=self.ignore_dependencies))
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/req/req_set.py"", line 554, in _prepare_file
    require_hashes
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/req/req_install.py"", line 278, in populate_link
    self.link = finder.find_requirement(self, upgrade)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py"", line 465, in find_requirement
    all_candidates = self.find_all_candidates(req.name)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py"", line 423, in find_all_candidates
    for page in self._get_pages(url_locations, project_name):
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py"", line 568, in _get_pages
    page = self._get_page(location)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py"", line 683, in _get_page
    return HTMLPage.get_page(link, session=self.session)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/index.py"", line 792, in get_page
    ""Cache-Control"": ""max-age=600"",
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
    return self.request('GET', url, **kwargs)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/download.py"", line 386, in request
    return super(PipSession, self).request(method, url, *args, **kwargs)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
    resp = self.send(prep, **send_kwargs)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
    r = adapter.send(request, **kwargs)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
    resp = super(CacheControlAdapter, self).send(request, **kw)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/adapters.py"", line 390, in send
    conn = self.get_connection(request.url, proxies)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/adapters.py"", line 290, in get_connection
    proxy_manager = self.proxy_manager_for(proxy)
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/adapters.py"", line 184, in proxy_manager_for
    **proxy_kwargs
  File ""/home/CORPUSERS/xp022898/anaconda3/lib/python3.6/site-packages/pip/_vendor/requests/packages/urllib3/contrib/socks.py"", line 154, in __init__
    ""Unable to determine SOCKS version from %s"" % proxy_url
ValueError: Unable to determine SOCKS version from socks:*********"
16205,Graph Transform Tool unable to build in TF source r1.5?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source 
- **TensorFlow version (use command below)**: r1.5
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 0.5.4
- **CUDA/cuDNN version**: 9.0/7.0.5
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**:
```
sudo sh -c ""echo '/usr/local/cuda-8.0/lib64' >> /etc/ld.so.conf.d/nvidia.conf""
sudo ldconfig
bazel clean
bazel build tensorflow/tools/graph_transforms:transform_graph --verbose_failures
```


### Describe the problem
I'm having issue trying to build the graph transform tool with bazel although I've look at existing solutions to similar problem such as #13481. I have been able to build the graph transform tool in previous versions but not in this version, so I'm not too sure what went wrong. Note that previously I got a similar problem when I installed TF from source but it was related to CUDA and I solved it after reinstalling nvcc.

I also rebooted my comp just in case it was a temporary system error, but the error still persists.

Looking at the error, does it have anything to do with ""JEMALLOC""? I enabled this option when configuring tensorflow as seen in the official installation guide.


### Source code / logs
```
ERROR: /home/kwotsin/tensorflow/tensorflow/core/kernels/BUILD:3945:1: C++ compilation of rule '//tensorflow/core/kernels:scatter_nd_op' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /home/kwotsin/.cache/bazel/_bazel_kwotsin/041f6cc3555a2d9f6211c6d126ede477/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.0 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.0 \
    TF_CUDNN_VERSION=7.0.5 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \

  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -fPIE -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++11' -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.o' -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_JEMALLOC -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/k8-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-opt/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/k8-opt/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/k8-opt/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/k8-opt/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/genfiles/external/nsync -iquote external/jemalloc -iquote bazel-out/k8-opt/genfiles/external/jemalloc -iquote external/gif_archive -iquote bazel-out/k8-opt/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/k8-opt/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/k8-opt/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/k8-opt/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/k8-opt/genfiles/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/k8-opt/genfiles/external/local_config_cuda -isystem external/bazel_tools/tools/cpp/gcc3 -isystem external/eigen_archive -isystem bazel-out/k8-opt/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/k8-opt/genfiles/external/nsync/public -isystem external/jemalloc/include -isystem bazel-out/k8-opt/genfiles/external/jemalloc/include -isystem external/gif_archive/lib -isystem bazel-out/k8-opt/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/k8-opt/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/k8-opt/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/k8-opt/genfiles/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/genfiles/external/local_config_cuda/cuda/cuda/include -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' -msse3 -pthread '-DGOOGLE_CUDA=1' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fno-canonical-system-headers -c tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/_objs/scatter_nd_op/tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.o)
In file included from tensorflow/core/kernels/scatter_nd_op_cpu_impl_5.cc:18:0:
./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h: In instantiation of 'Index tensorflow::functor::ScatterNdFunctor<Eigen::ThreadPoolDevice, T, Index, OP, IXDIM>::operator()(const CPUDevice&, Index, Eigen::array<long int, IXDIM>, typename tensorflow::TTypes<T, 2>::Tensor, typename tensorflow::TTypes<T, 2>::ConstTensor, typename tensorflow::TTypes<T, 2>::ConstTensor, typename tensorflow::TTypes<T, 2>::Tensor) [with T = float; Index = int; tensorflow::scatter_nd_op::UpdateOp OP = (tensorflow::scatter_nd_op::UpdateOp)0; int IXDIM = 5; tensorflow::CPUDevice = Eigen::ThreadPoolDevice; typename tensorflow::TTypes<T, 2>::Tensor = Eigen::TensorMap<Eigen::Tensor<float, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T, 2>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const int, 2, 1, long int>, 16, Eigen::MakePointer>; typename tensorflow::TTypes<T, 2>::ConstTensor = Eigen::TensorMap<Eigen::Tensor<const float, 2, 1, long int>, 16, Eigen::MakePointer>]':
./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:162:1:   required from here
./tensorflow/core/kernels/scatter_nd_op_cpu_impl.h:137:3: internal compiler error: Segmentation fault
   }
   ^
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-5/README.Bugs> for instructions.
Target //tensorflow/tools/graph_transforms:transform_graph failed to build
INFO: Elapsed time: 147.229s, Critical Path: 24.74s
FAILED: Build did NOT complete successfully

```"
16203,Feature Request: Add CheckpointSaverListener to tf.contrib.learn.Experiment,"Hello, 

I'm using the `tf.contrib.learn.Experiment` system to manage experiments built using the `tf.Estimator` framework. This setup allows me to specify checkpoint saving frequencies very easily, using just the `min_eval_frequency` argument to `tf.contrib.learn.Experiment`. However, I would like to be able to add a `tf.train.CheckpointSaverListener` (for example, to upload files to AWS after each checkpoint). Can there be a param added to `tf.contrib.learn.Experiment` to pass an optional `CheckpointSaverListener` object, which is then passed to the underlying `CheckpointSaverHook` object?

Thanks!


### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Amazon Deep Learning AMI
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8
- **GPU model and memory**: NVIDIA K80
- **Exact command to reproduce**: N/A

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16202,regarding the deconvolution operation for conv1d,There are `conv2d_transpose` for` conv2d` and `conv3d_transpose `for `conv3d` respectively.  How about deconvolution operation for `tf.nn.conv1d`?
16198,Unable to build Tensorflow Benchmark model for Android,"I've been trying to benchmark the model on mobile but I'm not able to build the model for Android. For desktop, I have been able to build and run the benchmark model.

The machine I'm using is a MacBook pro 15 inch with High Sierra and tensorflow v.1.4

I've been following the directions given at the following links:

(https://www.tensorflow.org/mobile/optimizing#how_to_profile_your_model
https://github.com/tensorflow/tensorflow/tree/r1.4/tensorflow/tools/benchmark)

Edit: Updated answer to the issue template

Have I written custom code

- No custom code was written

OS Platform and Distribution

- Mac OS High Sierra

TensorFlow installed from

- Tensorflow installed from source

TensorFlow version

- 1.4

Bazel version

- Bazel version 0.9.0

CUDA/cuDNN version

- N/A

GPU model and memory

- N/A

Exact command to reproduce

```
bazel build -c opt --cpu=armeabi-v7a \
  --crosstool_top=//external:android/crosstool \
  --host_crosstool_top=@bazel_tools//tools/cpp:toolchain \
  tensorflow/tools/benchmark:benchmark_model
```

Error message received

```
ERROR: /private/var/tmp/_bazel_abhisheksehgal/486c675b3107dc770b2281b905f670fe/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1)
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
In file included from external/highwayhash/highwayhash/sip_hash.h:25:
external/highwayhash/highwayhash/state_helpers.h:76:3: error: use of undeclared identifier 'static_assert'; did you mean 'static_cast'?
  static_assert((kPacketSize & (kPacketSize - 1)) == 0, ""Size must be 2^i."");
  ^
In file included from external/highwayhash/highwayhash/sip_hash.cc:15:
external/highwayhash/highwayhash/sip_hash.h:33:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
  using Key = HH_U64[2];
              ^
external/highwayhash/highwayhash/sip_hash.h:104:22: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHashState = SipHashStateT<2, 4>;
                     ^
external/highwayhash/highwayhash/sip_hash.h:105:24: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using SipHash13State = SipHashStateT<1, 3>;
                       ^
external/highwayhash/highwayhash/sip_hash.cc:20:13: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key = highwayhash::SipHashState::Key;
            ^
external/highwayhash/highwayhash/sip_hash.cc:21:15: warning: alias declarations are a C++11 extension [-Wc++11-extensions]
using Key13 = highwayhash::SipHash13State::Key;
              ^
5 warnings and 1 error generated.
Target //tensorflow/tools/benchmark:benchmark_model failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 2.060s, Critical Path: 1.60s
FAILED: Build did NOT complete successfully
```"
16197,add broadcasting to `softmax_cross_entropy_with_logits`,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yeah
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip3 binary
- **TensorFlow version (use command below)**:
```
>>> tf.__git_version__
'v1.4.0-rc1-11-g130a514'
>>> tf.__version__
'1.4.0'
```
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**: tf.nn.softmax_cross_entropy_with_logits(labels=tf.constant(1., shape=(2,)), logits=tf.constant(1., shape=(50,2)))


### Describe the problem
FEATURE REQUEST: `softmax_cross_entropy_with_logits` should broadcast, maybe?  I'm reading groups of data that all have the same label.  Seems a waste to have to replicate the label a gazillion times.

### Source code / logs
```
>>> a = tf.nn.softmax_cross_entropy_with_logits(labels=tf.constant(1., shape=(2,)), logits=tf.constant(1., shape=(50,2)))

Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 686, in _call_cpp_shape_fn_impl
    input_tensors_as_shapes, status)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimension 0 in both shapes must be equal, but are 50 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [50,2], [1,2].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2958, in create_op
    set_shapes_for_outputs(ret)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2209, in set_shapes_for_outputs
    shapes = shape_func(op)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 2159, in call_with_requiring
    return call_cpp_shape_fn(op, require_shape_fn=True)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 627, in call_cpp_shape_fn
    require_shape_fn)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/common_shapes.py"", line 691, in _call_cpp_shape_fn_impl
    raise ValueError(err.message)
ValueError: Dimension 0 in both shapes must be equal, but are 50 and 1 for 'SoftmaxCrossEntropyWithLogits_3' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [50,2], [1,2].
```
"
16194,"Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.","Hello,

I try to get the output of each layer of my CNN. Here is the full example:
```
`from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import sys
import tempfile
import os
import DatasetReader as dr
import numpy as np
import matplotlib.pyplot as plt


import tensorflow as tf
import utils
FLAGS = None
PLOT_DIR = './output/plots'


def deepnn(x):

  with tf.name_scope('reshape'):

    x_image = tf.reshape(x, [-1, 100, 100, 1])#(x, [-1, 28, 28, 1])

  # First convolutional layer - maps one grayscale image to 32 feature maps.
  with tf.name_scope('conv1'):
    W_conv1 = weight_variable([5, 5, 1, 32])#([5, 5, 1, 32])
    b_conv1 = bias_variable([32])
    # conv1dis = conv2d(x_image, W_conv1) + b_conv1
    h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)
    tf.add_to_collection('conv_weights', conv2d(x_image, W_conv1))

  # Pooling layer - downsamples by 2X.
  with tf.name_scope('pool1'):
    h_pool1 = max_pool_2x2(h_conv1)

  # Second convolutional layer -- maps 32 feature maps to 64.
  with tf.name_scope('conv2'):
    W_conv2 = weight_variable([5, 5, 32, 64])
    b_conv2 = bias_variable([64])
    # conv2dis = conv2d(h_pool1, W_conv2) + b_conv2
    h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)
    # tf.add_to_collection('conv_weights', h_conv2)

  # Second pooling layer.
  with tf.name_scope('pool2'):
    h_pool2 = max_pool_2x2(h_conv2)

  # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image
  # is down to 7x7x64 feature maps -- maps this to 1024 features.
  with tf.name_scope('fc1'):
    W_fc1 = weight_variable([25 * 25 * 64, 1024])
    b_fc1 = bias_variable([1024])
    h_pool2_flat = tf.reshape(h_pool2, [-1, 25*25*64])
    h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)
    # tf.add_to_collection('conv_weights', W_fc1)

  # Dropout - controls the complexity of the model, prevents co-adaptation of
  # features.
  with tf.name_scope('dropout'):
    keep_prob = tf.placeholder(tf.float32)
    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)

  # Map the 1024 features to 10 classes, one for each digit
  with tf.name_scope('fc2'):
    W_fc2 = weight_variable([1024, 2])
    b_fc2 = bias_variable([2])

  y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2

  return y_conv, keep_prob


def conv2d(x, W):
  """"""conv2d returns a 2d convolution layer with full stride.""""""
  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')


def max_pool_2x2(x):
  """"""max_pool_2x2 downsamples a feature map by 2X.""""""
  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                        strides=[1, 2, 2, 1], padding='SAME')


def weight_variable(shape):
  """"""weight_variable generates a weight variable of a given shape.""""""
  initial = tf.truncated_normal(shape, stddev=0.1)
  return tf.Variable(initial)


def bias_variable(shape):
  """"""bias_variable generates a bias variable of a given shape.""""""
  initial = tf.constant(0.1, shape=shape)
  return tf.Variable(initial)


def main(_):
  # Import data
  V0Dataset = dr.read_data_sets(FLAGS.data_dir, one_hot=True)

  datasize = 10000
  # Create the model
  x = tf.placeholder(tf.float32, [None, datasize])#224*172])

  # Define loss and optimizer
  y_ = tf.placeholder(tf.float32, [None, 2])
  print(""logits shape {}"".format(y_))


  # # Build the graph for the deep net
  y_conv, keep_prob = deepnn(x)

  with tf.name_scope('loss'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_,
                                                            logits=y_conv)
  cross_entropy = tf.reduce_mean(cross_entropy)

  with tf.name_scope('adam_optimizer'):
    train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)

  with tf.name_scope('accuracy'):
    correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))
    correct_prediction = tf.cast(correct_prediction, tf.float32)

  accuracy = tf.reduce_mean(correct_prediction)

  print('cross_entropy {}'.format(cross_entropy))
  print('accuracy {}'.format(accuracy))



  with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(2):#500):
      batch = V0Dataset.train.next_batch(10)
      a = batch[1];
      a = a.reshape(10,2)
      train_step.run(feed_dict={x: batch[0], y_: a, keep_prob: 0.5})



    graph_location = tempfile.mkdtemp()
    print('Saving graph to: %s' % graph_location)
    train_writer = tf.summary.FileWriter(""/tmp/tensorflow/"")
    train_writer.add_graph(tf.get_default_graph())

    conv0 = sess.graph.get_tensor_by_name('conv1/Conv2D:0')
    print(""conv0 {}"".format(conv0))

    predictions0 = sess.run(conv0,
                           {'DecodeJpeg/contents:0': batch[0]}) # Error!!!!
    print(""predictions0 {}"".format(predictions0))
    print(""predictions0 {}"".format(predictions0.size))

```

Here are the errors I get:
```
`Traceback (most recent call last):
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1064, in _run
    allow_operation=False)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3035, in as_graph_element
    return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py"", line 3077, in _as_graph_element_locked
    ""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.""

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""./deep_charging_station_train.py"", line 309, in <module>
    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""./deep_charging_station_train.py"", line 297, in main
    {'DecodeJpeg/contents:0': batch[0]})
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py"", line 1067, in _run
    + e.args[0])
TypeError: Cannot interpret feed_dict key as Tensor: The name 'DecodeJpeg/contents:0' refers to a Tensor which does not exist. The operation, 'DecodeJpeg/contents', does not exist in the graph.
`



```

I don't understand why this appends. I looked with Tensorboard I don't know where should I get the DecodeJpeg informations of the layer

Edit:
Have I written custom code : I use deep mnist tutorial example and I modify the size of the input image
OS Platform and Distribution : Ubuntu 16.04
TensorFlow installed from
TensorFlow version 1.4.0
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce
"
16193,Performance issues with TF1.5 on CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: NA
- **GPU model and memory**: NA
- **Exact command to reproduce**:

Hello,
I'm facing performance issues with the last releases of TF using a CPU.
I'm using the [benchmark tool](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/benchmark/benchmark_model.cc) to calculate mean inference time of a model.

For example, in order to evaluate mobilenet (trained on a custom dataset), I'm using this command :
`bazel-bin/tensorflow/tools/benchmark/benchmark_model --graph=""path to mobilenet graph"" --input_layer=""input"" --input_layer_shape=""1,224,224,3"" --input_layer_type=""float"" --output_layer=""MobilenetV1/Predictions/Reshape_1""`
After setting CUDA_VISIBLE_DEVICES to """" in order to run on CPU.

With TF 1.4.1, I obtain a mean inference time equals to 26ms (13ms if I compile with optimization flags).
Using tf 1.5.*, I obtain a mean inference time equals to 51ms (45ms if I compile with optimization flags).

The loss is very important, so I'm wondering if it's a known issue and how I can improve this.

I tried with tags/v1.5.0-rc0, tags/v1.5.0-rc1 and master, and the problem is the same.

Thank you"
16192,how to set ignore_label in tensorflow?,"when i set the label=-1, there is an error: Received a label value of -1 which is outside the valid range of [0, 8)"
16190,ValueError: Inputs to `Dense` should have rank >= 2.,"##error

Traceback (most recent call last):
  File ""firstGANtf.py"", line 90, in <module>
    G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)
  File ""firstGANtf.py"", line 55, in __init__
    self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 177, in func_with_args
    return func(*args, **current_args)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1409, in fully_connected
    outputs = layer.apply(inputs)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 303, in apply
    return self.__call__(inputs, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/base.py"", line 269, in __call__
    self.build(input_shapes[0])
  File ""/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/tensorflow/python/layers/core.py"", line 110, in build
    raise ValueError('Inputs to `Dense` should have rank >= 2.')
ValueError: Inputs to `Dense` should have rank >= 2.


##code

       self.map1 = tf.contrib.layers.linear(inputs=input_size , num_outputs=hidden_size)
        self.map2 = tf.contrib.layers.linear(inputs=hidden_size,  num_outputs=hidden_size)
        self.map3 = tf.contrib.layers.linear(inputs=hidden_size, num_outputs=output_size)"
16189,"Will tf-Lite have GPU support , if the answer is yes ,the compute API will be which one ,OpenCL or gles ?",
16188,benchmark_model tool not build successfully for android version,"Hello,

I try to build the benchmark_model for the android, but I encounter some errors.
Please help, is any setting not correct?

The configuration of the SDK and NDK in the WORKSPACE is
android_sdk_repository(
    name = ""androidsdk"",
    api_level = 23,
    build_tools_version = ""26.0.1"",
    path = ""/home/kk/android_sdk/android-sdk-linux"",
)

android_ndk_repository(
    name=""androidndk"",
    path=""/home/kk/android_sdk/ndk/android-ndk-r14"",
    api_level=14)

Use the command to build:
bazel build --cxxopt='--std=c++11' -c opt 
--crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain tensorflow/tools/benchmark:benchmark_model

There are three errors
1. external/gif_archive/lib/openbsd-reallocarray.c:33:19: error: use of undeclared identifier 'SIZE_MAX'
2. tensorflow/core/common_runtime/gpu/gpu_debug_allocator.cc:172:53: error: no member named 'nanf' in namespace 'std'; did you mean simply 'nanf'?
3. external/androidndk/ndk/toolchains/arm-linux-androideabi-4.9/prebuilt/linux-x86_64/lib/gcc/arm-linux-androideabi/4.9.x/../../../../arm-linux-androideabi/bin/ld: error: cannot find -lpthread

Thanks

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04.4 LTS
- **TensorFlow installed from (source or binary)**:use the pip install
- **TensorFlow version (use command below)**:1.4.0
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:(Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
- **CUDA/cuDNN version**:NA
- **GPU model and memory**:NA
- **Exact command to reproduce**:NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16187,Faster R-CNN: too many resources requested for launch,"I am trying to deploy the pretrained Faster-RCNN Inception V2 from the object detection API on a Jetson TX2. 
I am running CUDA 8, cuDNN 6 and have tested with both TF 1.3 and 1.5 in a Jupyter Notebook environment. 
When I monitor the GPU memory it starts out by having 4.8 GB free and when launching these fills up immediately. When I run on my GTX1060 6 GB GPU I have effectively the same amount of memory free but are having no issues running.
Smaller models as SSD MobileNet runs without problems.

From tests performed today, I can supply the following dumps.

Jupyter Notebook terminal output:

```
2018-01-17 16:16:19.584106: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:859] ARM64 does not support NUMA - returning NUMA node zero
2018-01-17 16:16:19.584261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: 
name: NVIDIA Tegra X2 major: 6 minor: 2 memoryClockRate(GHz): 1.3005
pciBusID: 0000:00:00.0
totalMemory: 7.67GiB freeMemory: 4.97GiB
2018-01-17 16:16:19.584312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0
2018-01-17 16:16:20.824479: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4437 MB memory) -> physical GPU (device: 0, name: NVIDIA Tegra X2, pci bus id: 0000:00:00.0, compute capability: 6.2)
2018-01-17 16:17:09.816477: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:09.816703: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:09.816771: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:09.816912: E tensorflow/stream_executor/cuda/cuda_dnn.cc:2456] failed to enqueue convolution on stream: CUDNN_STATUS_EXECUTION_FAILED
2018-01-17 16:17:10.174651: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:10.174772: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:10.174806: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:10.174836: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
2018-01-17 16:17:10.174865: E tensorflow/stream_executor/event.cc:33] error destroying CUDA event in context 0x7f001959b0: CUDA_ERROR_LAUNCH_FAILED
```

Error dump from printout inside the notebook:

```
Exception in thread Thread-4:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""<ipython-input-5-a51933cd03d8>"", line 19, in worker
    im, t_elapsed = detect_objects(frame_rgb, sess, detection_graph)
  File ""<ipython-input-4-6c8da66803e2>"", line 19, in detect_objects
    feed_dict={image_tensor: image_np_expanded})
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 895, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1128, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1344, in _do_run
    options, run_metadata)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py"", line 1363, in _do_call
    raise type(e)(node_def, op, message)
InternalError: cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])
	 [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]
	 [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_10508...ield/Equal"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]

Caused by op u'FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D', defined at:
  File ""/usr/lib/python2.7/threading.py"", line 774, in __bootstrap
    self.__bootstrap_inner()
  File ""/usr/lib/python2.7/threading.py"", line 801, in __bootstrap_inner
    self.run()
  File ""/usr/lib/python2.7/threading.py"", line 754, in run
    self.__target(*self.__args, **self.__kwargs)
  File ""<ipython-input-5-a51933cd03d8>"", line 10, in worker
    tf.import_graph_def(od_graph_def, name='')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/importer.py"", line 548, in import_graph_def
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3176, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1617, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InternalError (see above for traceback): cuDNN launch failure : input shape([1,64,138,256]) filter shape([3,3,64,192])
	 [[Node: FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2c_3x3/Conv2D = Conv2D[T=DT_FLOAT, data_format=""NHWC"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](FirstStageFeatureExtractor/InceptionV2/InceptionV2/Conv2d_2b_1x1/Relu, FirstStageFeatureExtractor/InceptionV2/Conv2d_2c_3x3/weights/read/_47__cf__53)]]
	 [[Node: BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal/_883 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_10508...ield/Equal"", tensor_type=DT_BOOL, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^_cloopBatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/non_max_suppression/iou_threshold/_1)]]
```

Output of tegrastats at the point of error:
```
RAM 3151/7851MB (lfb 915x4MB) cpu [2%@345,100%@2034,99%@2034,1%@348,3%@348,6%@349] EMC 5%@1866 APE 150 GR3D 0%@114
RAM 3151/7851MB (lfb 915x4MB) cpu [0%@345,100%@1981,100%@1988,3%@348,5%@348,4%@349] EMC 5%@1866 APE 150 GR3D 0%@114
RAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2021,100%@2021,4%@348,5%@348,2%@349] EMC 5%@1866 APE 150 GR3D 0%@114
RAM 3152/7851MB (lfb 915x4MB) cpu [2%@345,100%@2035,100%@2034,3%@349,4%@348,2%@348] EMC 5%@1866 APE 150 GR3D 0%@114
RAM 3152/7851MB (lfb 915x4MB) cpu [1%@345,100%@2016,100%@2019,2%@345,1%@349,3%@348] EMC 5%@1866 APE 150 GR3D 0%@114
RAM 3181/7851MB (lfb 898x4MB) cpu [21%@806,100%@2021,56%@2024,8%@499,10%@500,3%@500] EMC 5%@1866 APE 150 GR3D 24%@114
RAM 3210/7851MB (lfb 887x4MB) cpu [8%@345,100%@2018,32%@2026,7%@345,24%@345,13%@349] EMC 5%@1866 APE 150 GR3D 99%@114
RAM 3327/7851MB (lfb 838x4MB) cpu [2%@1573,100%@1987,31%@1992,35%@1574,13%@1575,5%@1573] EMC 5%@1866 APE 150 GR3D 8%@114
RAM 3578/7851MB (lfb 758x4MB) cpu [19%@1806,100%@2080,0%@2035,7%@2035,2%@2035,56%@1727] EMC 5%@1866 APE 150 GR3D 10%@114
RAM 3732/7851MB (lfb 715x4MB) cpu [2%@345,100%@2034,83%@2035,5%@348,21%@345,2%@346] EMC 7%@1866 APE 150 GR3D 99%@624
RAM 3732/7851MB (lfb 715x4MB) cpu [94%@2036,100%@2035,97%@2034,87%@1987,13%@2035,1%@2035] EMC 4%@1866 APE 150 GR3D 43%@1032
RAM 3659/7851MB (lfb 727x4MB) cpu [2%@653,81%@2022,20%@2027,28%@652,2%@655,4%@655] EMC 3%@1866 APE 150 GR3D 0%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [1%@345,100%@2033,0%@2035,1%@346,2%@348,3%@349] EMC 3%@1866 APE 150 GR3D 0%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2035,0%@2034,0%@348,3%@348,0%@348] EMC 2%@1866 APE 150 GR3D 0%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [3%@345,100%@2034,0%@2035,1%@348,1%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [2%@345,100%@2034,0%@2034,2%@348,4%@348,1%@348] EMC 2%@1866 APE 150 GR3D 0%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@1988,0%@1987,2%@346,2%@345,1%@345] EMC 2%@1866 APE 150 GR3D 9%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [4%@345,100%@2026,0%@2026,1%@347,0%@348,3%@348] EMC 2%@1866 APE 150 GR3D 0%@114
RAM 3661/7851MB (lfb 727x4MB) cpu [8%@345,100%@2024,0%@2028,5%@345,8%@345,1%@345] EMC 2%@1866 APE 150 GR3D 0%@114
```
As you can see the RAM are nowhere near full at the moment of the error.

Can anybody suggest a solution to this?"
16186,A bug when applying MultiRNNCell?,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: This code is very similar to an official example
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: b'unknown' 1.4.0
- **Python version**: Python 3.5.2 :: Anaconda 4.2.0 (64-bit)
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
tf.nn.MultiRNNCell sometimes doesn't work.

It raises an issue like this:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

### Source code / logs
  import tensorflow as tf
  import numpy as np

  hidden_layer_size = 32
  embed = tf.zeros((128, 6, 64), dtype=tf.float32)

  num_LSTM_layers = 2
  with tf.variable_scope(""lstm""):
    
    lstm_cell = tf.contrib.rnn.BasicLSTMCell(hidden_layer_size, forget_bias=1.0)
    cell = tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell]*num_LSTM_layers, state_is_tuple=True)
    outputs, states = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)
   
Error:
ValueError: Dimensions must be equal, but are 64 and 96 for 'lstm/rnn/while/rnn/multi_rnn_cell/cell_0/cell_0/basic_lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [128,64], [96,128].

"
16184,Tensorflow and libcuda.so.1,"Tensorflow 1.4.1
OS:  CentOS 6/7  (we use customized gcc 4.9.2 build for CentOS 6)

We have a number of computational servers.  Some are with GPU but some are not.  For the ease of maintenance, we build tensorflow from source code (bazel build) and install the modules under
/usr/local/... that all computational servers mount to the same /usr/local by means of NFS.

In the past (Tensorflow 1.0.0), the module could be built without linking with libcuda.so.1.  When a computational server without GPU runs tensorflow, it could run as CPU mode without problems.  When the computational server with GPU runs tensorflow, it could detect the GPU and load up libcuda.so.1 (and libcudart.so and libcudnn.so) by using dso_loader.  This works great for supporting both GPU and non-GPU servers while sharing the same module.

But I think that since 1.2.1 (at least, still 1.4.1), it seems that linking libcuda.so.1 is mandatory.  This is bad when non-GPU server would fail loading the module (missing libcuda.so.1), unless we explicitly putting libcuda.so.1 under /usr/lib64 (but this is a non-GPU server...!).  

Wonder if it is possible to make use of the old method of dso_loader instead of linking libcuda.so.1 for bazel building.  Thanks.
"
16182,"What are the relation ship between TF.Slim, TF high level API and Keras","I am very confused. What are the relationships between TF.Slim, TF high level API and Keras. I just want to know which one has the long term evolution. Fragmentation, like Android OS, is a very bad and dangerous thing. At least for me, I am not comfortable with TF.Slim at all. Why TF cannot have a unified and Standardized API? The benefits are so obvious. It should not become different political parties fight each other."
16181,How to control the number of threads on Tensorflow and TF-Slim?,"Do we have any options to control the number of threads in TF-Slim both in training and evaluation processes?

Specifically, I use [this network](https://github.com/pudae/tensorflow-densenet) for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like [this code](https://github.com/mnuke/tf-slim-mnist). I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error ""resource temporarily unavailable"".

This error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:

    INFO:tensorflow:Restoring parameters from ./model.ckpt-0
    INFO:tensorflow:Starting evaluation at
    I tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]
    I tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]
    INFO:tensorflow:Evaluation [1/60]

However, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:

    INFO:tensorflow:Restoring parameters from ./model.ckpt-0
    terminate called after throwing an instance of 'std::system_error'
    what():  Resource temporarily unavailable

I tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:

      FLAGS.num_preprocessing_threads=1

      config = tf.ConfigProto()
      config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads
      config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads
    
        slim.evaluation.evaluation_loop(
            master=FLAGS.master,
            checkpoint_path=each_ckpt,
            logdir=FLAGS.eval_dir,
            num_evals=num_batches,
            eval_op=list(names_to_updates.values()) + print_ops,
            variables_to_restore=variables_to_restore,
            session_config=config)
But unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:


    slurm_script─┬─python───128*[{python}]
                 └─python───8*[{python}]

Training script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).  

P.S. I'm using Python 2.7.13 and Tensorflow 1.3.0."
16180,Tensorboard is down after upgrading the tensorflow?,"Hello everyone:

I meet a issue about tensorboard after upgrading the tensorflow. It runs nicely before, but I want maintain some Python2.7 codes in Python3.4. That is why I install tensorflow .whl file of Python 3.4 and modify some grammer from Python2.7 to Python3.4. Then codes still run fine, but tensorboard is donw. The error message as following:

![image](https://user-images.githubusercontent.com/12611573/35029640-d1981942-fb96-11e7-9b89-c3c14ffaa54b.png)

OS Platform: Ubuntu 14.04
TensorFlow installed from: pip instll .whl file
TensorFlow version: tensorflow 1.2.1 for Python2, but can not check the version for Python 3

What should I do for this issue? degrade tensorflow or upgrade CUDA?
Can anybody give me any help? Thank you!
"
16179,ProfilerHook and loading libcupti.so cause Ubuntu to completely freeze,"1. OS Platform and Distribution: Ubuntu 14.04 LTE
2. TensorFlow version: 1.14
3. Bazel version: 0.9.0
4. CUDA/cuDNN version: 8.0/7.0.5
5. GPU model and memory: GeForce GTX1060 - 6070MB
6. Exact command to reproduce: python3.4 -m music_modeling 
--

I added ""ProfilerHook"" to Estimator for recording GPU memory consumption; but it always causes my Ubuntu to freeze indefinitely and Ubuntu never makes away with it.

Here is the source code:

```
import tensorflow as tf

from tensorflow.python.layers.core import dense
from tensorflow.python import debug as tf_debug

from data.music_data_reader import MusicDataReader
from model.tf_msa_rnn import dynamic_msa_rnn


FLAGS = tf.app.flags.FLAGS
tf.app.flags.DEFINE_string(""mode"", tf.estimator.ModeKeys.TRAIN,
                           """"""""Is training or testing mode"""""")
tf.app.flags.DEFINE_string(""model_dir"", './msa_model',
                           """"""""Directory in where checkpoints are stored"""""")
tf.app.flags.DEFINE_integer(""batch_size"", 20,
                            """"""Number of samples in a batch"""""")
tf.app.flags.DEFINE_integer(""num_epochs"", 100,
                            """"""""How many times the whole training set has to be fed into network"""""")
tf.app.flags.DEFINE_string(""log_directory"", './log_dir',
                           """"""""Directory in where logs and checkpoints are stored"""""")
# *************************************
# *************************************Configuration options for the network
# *************************************
tf.app.flags.DEFINE_integer(""num_units"", 30,
                            """"""""# of hidden units in an LSTM cell"""""")
tf.app.flags.DEFINE_integer(""num_msa_feats"", 10,
                            """"""""# of MS features to be learned"""""")
tf.app.flags.DEFINE_integer(""signal_len"", 100,
                            """"""""Length of the signal at a time to be processed by 
                                multi-scale analyzer
                            """""")
tf.app.flags.DEFINE_integer(""dim_pitch"", 88,
                            """"""""# of hidden units in an LSTM cell"""""")
# *************************************
# *************************************Configuration options for dataset
# *************************************
tf.app.flags.DEFINE_string(""dir_path"",
                           './music_samples/MuseData',
                           """"""""Absolute path to the music files for reading training/testing samples"""""")
tf.app.flags.DEFINE_integer(""pitch_low"",
                            21,
                            """"""""Low pitch value"""""")
tf.app.flags.DEFINE_integer(""pitch_high"",
                            109,
                            """"""""High pitch value"""""")
tf.app.flags.DEFINE_float(""dt"",
                          0.3,
                          """"""""Not sure yet..."""""")


def loss_fn(y_pred, y_true):
    '''

    :param y_pred: Logits predicted by the model
    :param y_true: Correct values corresponding each prediction
    :return:
    '''

    y_pred = tf.log(tf.nn.softmax(y_pred, name=""probs_tensor""))  # [BSxMTxOS] -- Probabilities
    p_trun = y_pred[:, 0:-1, ...]  # x'[1], x'[2], ..., x'[N-1]
    t_trun = y_true[:, 1:, ...]  # x[1], x[2], ..., x[N-1]
    loss = tf.reduce_sum(p_trun*t_trun, axis=2)  # [BSxMT] -- Dot product between 3rd dimensions
    loss = tf.reduce_mean(loss, name=""piano_roll_loss"")  # loss function -- returns a scalar
    tf.summary.scalar(""loss_fn"", loss)  # Add summary for the loss
    loss = tf.Print(loss, [loss], ""Loss: "")
    return loss


def model_fn(features,
             mode=tf.estimator.ModeKeys.TRAIN,
             params=None):

    print(""Creating Model..."")
    input_ph = tf.reshape(features['input_ph'], [FLAGS.batch_size, params['max_seq'], FLAGS.dim_pitch])
    seq_len_ph = tf.reshape(features['seq_len_ph'], [FLAGS.batch_size])
    outputs, state = dynamic_msa_rnn(FLAGS.batch_size,
                                     input_ph,
                                     seq_len_ph,
                                     params['max_seq'],
                                     FLAGS.signal_len,
                                     [20, 10],  # Number of filters per layer
                                     [11, 13],  # Kernel size for each layer
                                     [3],  # Pooling size for each layer
                                     FLAGS.num_msa_feats,
                                     FLAGS.num_units,
                                     activation=tf.nn.tanh,
                                     initializer=tf.glorot_normal_initializer())  # [BSxMTxOS], [BSxSS]
    # Create fully connected layer to generate output for piano keys
    outs = dense(outputs,
                 FLAGS.dim_pitch,
                 kernel_initializer=tf.glorot_normal_initializer())  # BSxMTxID
    loss = loss_fn(outs, features['input_ph'])
    print(""Creating Estimator Spec for %s ..."" % mode)
    # For training
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer()
        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op)
    # For evaluation
    eval_metric_ops = {
        ""accuracy"": tf.metrics.accuracy(
            labels=tf.argmax(features['input_ph'][:, 1:, ...], axis=-1),
            predictions=tf.argmax(outs[:, 0:-1, ...], axis=-1)
        )
    }
    return tf.estimator.EstimatorSpec(mode=mode,
                                      loss=loss,
                                      eval_metric_ops=eval_metric_ops)


def do_train(tr_data, vl_data):

    # Create Estimator
    sess_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)
    sess_conf.gpu_options.allow_growth = True
    config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,  # CheckpointSaverHook
                                    save_checkpoints_steps=100,  # CheckpointSaverHook
                                    log_step_count_steps=10,  # SummarySaverHook
                                    session_config=sess_conf)
    music_classifier = tf.estimator.Estimator(model_fn,
                                              config=config,
                                              params={'max_seq': tr_data.max_seq})
    # Prepare input data
    tr_input_fn = tf.estimator.inputs.numpy_input_fn(
        {'input_ph': tr_data.data, 'seq_len_ph': tr_data.seq_len},
        batch_size=FLAGS.batch_size,
        num_epochs=FLAGS.num_epochs,
        num_threads=1,
        shuffle=True
    )
    # Extra Hooks
    logging_hook = tf.train.LoggingTensorHook(
        tensors={'probabilities': 'probs_tensor'},
        every_n_secs=60
    )
    # debugging_hook = tf_debug.LocalCLIDebugHook(thread_name_filter=""MainThread$"", dump_root=""./dump"")
    profiler_hook = tf.train.ProfilerHook(save_steps=1,
                                          output_dir=""./profile"",
                                          show_dataflow=False,
                                          show_memory=True)
    # Train
    music_classifier.train(tr_input_fn, hooks=[profiler_hook])
    print(""Training is over..."")


def do_test(te_data):

    print(""Start testing..."")


def main(_):

    if FLAGS.mode == tf.estimator.ModeKeys.TRAIN:
        tr_data = MusicDataReader(FLAGS.dir_path,
                                  'train',
                                  (FLAGS.pitch_low, FLAGS.pitch_high),
                                  FLAGS.dt)
        vl_data = MusicDataReader(FLAGS.dir_path,
                                  'valid',
                                  (FLAGS.pitch_low, FLAGS.pitch_high),
                                  FLAGS.dt)
        print(""Number of training samples: %d"" % tr_data.data_num)
        print(""Number of validation samples: %d"" % vl_data.data_num)
        do_train(tr_data, vl_data)
    else:
        te_data = MusicDataReader(FLAGS.dir_path,
                                  'test',
                                  (FLAGS.pitch_low, FLAGS.pitch_high),
                                  FLAGS.dt,)
        print(""Number of testing samples: %d"" % te_data.data_num)
        do_test(te_data)


if __name__ == ""__main__"":
    tf.app.run(main=main)
```
I cannot attach the output of my console for it is impossible due to the indefinite freezing of my Ubuntu. All I can say is that it tells me that it loads **libcupti.so** and computes a step of training process. Then, everything totally messes up.

Thank you for your support in advance."
16178,Crash in TF lite demo android app when using preprocessing layer,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04):** Linux Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: v1.4.0-rc0-21-g1e25994 1.4.0-rc1
- **Python version**: Python 3.6.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: Titan X (Pascal), 12 GB
- **Exact command to reproduce**:


### Describe the problem
I have a problem adding preprocessing layers to MobileNetV1 model that is quantized afterward. As preprocessing method I would like to use inception preprocessing, but TF lite does not support several operations (sub, div, broadcasting, ...), so I modified following preprocessing

```python
images = tf.divide(images, tf.constant(255.0))
images = tf.subtract(images, tf.constant(0.5))
images = tf.multiply(images, tf.constant(2.0))
```

to

```python
shape = images.get_shape()
c1 = tf.constant(1.0/255.0, shape=shape)
c1 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c2 = tf.constant(-0.5, shape=shape)
c2 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)
c3 = tf.constant(2.0, shape=shape)
c3 = tf.fake_quant_with_min_max_args(c1, min=-1, max=1)

images = tf.multiply(images, c1)
images = tf.fake_quant_with_min_max_args(images, min=0, max=1)
images = tf.add(images, c2)
images = tf.fake_quant_with_min_max_args(images, min=-0.5, max=0.5)
images = tf.multiply(images, c3)
images = tf.fake_quant_with_min_max_args(images, min=-1.0, max=1.0)
```

Quantization is performed with
```python
fold_batch_norms.FoldBatchNorms(graph)
quantize.Quantize(graph, is_training=is_training)
```
 and can be trained and evaluated.

Further, graph is frozen.
```bash
bazel-bin/tensorflow/python/tools/freeze_graph \
  --input_graph=MobileNetV1-4.pbtxt \
  --input_checkpoint=MobileNetV1-4.ckpt \
  --output_node_names=output/softmax \
  --output_graph=MobileNetV1-4-frozen.pb
```

Finally, frozen graph is converted to TF lite model using command.
```bash
bazel-bin/tensorflow/contrib/lite/toco/toco \
 --input_file=MobileNetV1-4-frozen.pb \
 --input_format=TENSORFLOW_GRAPHDEF \
 --output_format=TFLITE \
 --output_file=model.tflite \
 --inference_type=QUANTIZED_UINT8 \
 --inference_input_type=QUANTIZED_UINT8 \
 --input_array=input/image \
 --output_array=output/softmax \
 --input_shape=1,224,224,3
```

During conversion no error occurs.
```
2018-01-17 11:25:52.905034: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 604 operators, 896 arrays (0 quantized)
2018-01-17 11:25:53.301108: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.302502: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 66 operators, 127 arrays (1 quantized)
2018-01-17 11:25:53.303020: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.303601: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 35 operators, 96 arrays (1 quantized)
2018-01-17 11:25:53.326761: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 1: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327269: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After quantization graph transformations pass 2: 34 operators, 95 arrays (94 quantized)
2018-01-17 11:25:53.327854: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 1756160 bytes, theoretical optimal value: 1204224 bytes.
2018-01-17 11:25:53.328080: I tensorflow/contrib/lite/toco/toco_tooling.cc:269] Estimated count of arithmetic ops: 1.14175 billion (note that a multiply-add is counted as 2 ops).
```

When I upload generated model to TF lite demo application, app crashes logcat prints this error.
```
01-17 11:56:52.190 10923-10923/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-17 11:56:52.190 10923-10923/? A/DEBUG: Build fingerprint: 'samsung/dreamlteks/dreamlteks:7.0/NRD90M/G950NKSU1AQL3:user/release-keys'
01-17 11:56:52.191 10923-10923/? A/DEBUG: Revision: '11'
01-17 11:56:52.191 10923-10923/? A/DEBUG: ABI: 'arm64'
01-17 11:56:52.191 10923-10923/? A/DEBUG: pid: 10865, tid: 10881, name: CameraBackgroun  >>> android.example.com.tflitecamerademo <<<
01-17 11:56:52.191 10923-10923/? A/DEBUG: signal 6 (SIGABRT), code -6 (SI_TKILL), fault addr --------
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x0   0000000000000000  x1   0000000000002a81  x2   0000000000000006  x3   0000000000000008
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x4   0000007e81515040  x5   0000007e815166c0  x6   0000ffffffffffff  x7   ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x8   0000000000000083  x9   ffffffffffffffdf  x10  0000000000000000  x11  ffffffffffffffff
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x12  0000000000000000  x13  ffffffffffff0000  x14  00000000000002e0  x15  000000000000044d
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x16  0000007e9533aed0  x17  0000007e952e29f4  x18  0000000000000001  x19  0000007e817524f8
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x20  0000000000000006  x21  0000007e81752450  x22  000000000000000b  x23  0000007e858340f0
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x24  0000007e817524e8  x25  0000000000000000  x26  0000000000000080  x27  0000007e81516740
01-17 11:56:52.191 10923-10923/? A/DEBUG:     x28  0000000000000001  x29  0000007e81750730  x30  0000007e952dfd14
01-17 11:56:52.191 10923-10923/? A/DEBUG:     sp   0000007e81750710  pc   0000007e952e29fc  pstate 0000000060000000
01-17 11:56:52.198 10923-10923/? A/DEBUG: backtrace:
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #00 pc 000000000006f9fc  /system/lib64/libc.so (tgkill+8)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #01 pc 000000000006cd10  /system/lib64/libc.so (pthread_kill+64)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #02 pc 0000000000025078  /system/lib64/libc.so (raise+24)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #03 pc 000000000001cc04  /system/lib64/libc.so (abort+52)
01-17 11:56:52.198 10923-10923/? A/DEBUG:     #04 pc 00000000000881a0  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #05 pc 0000000000071ce4  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #06 pc 00000000000707fc  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #07 pc 000000000007f99c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #08 pc 0000000000011c5c  /data/app/android.example.com.tflitecamerademo-1/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+1628)
01-17 11:56:52.199 10923-10923/? A/DEBUG:     #09 pc 0000000000384abc  /data/app/android.example.com.tflitecamerademo-1/oat/arm64/base.odex (offset 0x329000)
```

This error doesn't seem to be related to added preprocessing layer, but without adding preprocessing layer, no error occurs and app can run.
"
16177,"RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6   return f(*args, **kwds)","### System information
- **OS Platform and Distribution :**  Linux Ubuntu 17.10
- **TensorFlow installed from :**  Anaconda [followed this tutorial](https://www.tensorflow.org/install/install_linux#InstallingAnaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: Python 3.6.4 :: Anaconda, Inc.
- **CUDA/cuDNN version**: not using GPU version
- **GPU model and memory**: 2GB GT720

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**result :** 
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
v1.4.0-19-ga52c8d9 1.4.1

### Describe the problem
Followed Official tensorflow documentation to install tensorflow on Ubuntu 17.10, python3 (python 3.6) and with CPU support. Used conda environment. Followed [this](https://www.tensorflow.org/install/install_linux#InstallingAnaconda) and in the 4th step this is the command I used `pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.4.1-cp36-cp36m-linux_x86_64.whl`
it installed successfully. But when I try to import tensorflow in python I'm getting this error.

```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)

```

### Source code / logs
Activate Conda environment
`source activate tensorflow`

**command :** `python`
**log :** 
```
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```

**command :** `import tensorflow`
**log :**
```
/home/pankaja/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
```


Why can't I use the tensorflow for python 3.6 in python 3.6 . How to fix this ?"
16174,Failed to Create Session: CUDA_ERROR_UNKNOWN,"Failed to create session:
```
import tensorflow as tf
tf.Session()
```
Error info:
E tensorflow/core/common_runtime/direct_session.cc:170] Internal: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_UNKNOWN
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1482, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File ""/home/ubuntu/envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 622, in __init__
    self._session = tf_session.TF_NewDeprecatedSession(opts, status)
  File ""/home/ubuntu/envs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: Failed to create session.

------------------------

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: tensorflow-gpu ('v1.4.0-19-ga52c8d9', '1.4.1')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 6.0
- **GPU model and memory**: Tesla M40 24GB
- **Exact command to reproduce**: N/A
"
16170,"tf.train.latest_checkpoint fails when paths have ""//"" ","```tf.train.latest_checkpoint``` returns the error 
```
ERROR:tensorflow:Couldn't match files for checkpoint
``` 
when paths in the ""checkpoint"" file have ""//"" instead of ""/"". Usually, good practice of using ```os.path.join``` will help avoid this situation, but I believe TensorFlow should account for '//'s in paths as several developers do not use ```os.path.join```

### To reproduce this error
- Create directory ```/home/user/model```
- Create file ```/home/user/model/checkpoint``` whose contents are 

```
model_checkpoint_path: ""/home/user/model//model_1""
all_model_checkpoint_paths: ""/home/user/model//model_0""
all_model_checkpoint_paths: ""/home/user/model//model_1""
```
- Create empty files ```model/model_1.data-00000-of-00001```, ```model/model_1.index```, ```model/model_1.meta```
- Run ```tf.train.latest_checkpoint('/home/user/model')```. 
Expected output is ```u'/home/user/model//model_1'```, but TF returns an error ```ERROR:tensorflow:Couldn't match files for checkpoint /home/user/model//model_1```




### System information
- OS: Ubuntu 16.04
- TF installed via ```pip install tensorflow-gpu```
- TF version: 1.4.1
- Python version: 2.7 
- CUDA/cuDNN version: 8.0
- GPU model and memory: GeForce GTX 1080, 8GB

### Have I written custom code
Yes

### OS Platform and Distribution
Ubuntu 16.04

### TensorFlow installed from
Installed thru ```pip install tensorflow-gpu```

### TensorFlow version
1.4.1

### Bazel version
N/A

### Exact command to reproduce
N/A


"
16167,Documentation Method Templates Improvement,"### System information
N/A

### Describe the problem
The method/class templates in documentation should include a full, functioning path to the method instead of just truncating to the method's name.

I.e. this is what we have at present (bad): 
<img width=""399"" alt=""screen shot 2018-01-16 at 2 23 08 pm"" src=""https://user-images.githubusercontent.com/9597721/35007940-0511d55c-fac9-11e7-9d0c-4be2db021533.png"">

This is a more practical and copy/paste-friendly version:
<img width=""426"" alt=""screen shot 2018-01-16 at 2 22 49 pm"" src=""https://user-images.githubusercontent.com/9597721/35007976-2970cdc2-fac9-11e7-80b8-0ec1e2334734.png"">

I'm constantly just grabbing method templates, pasting to my text editor and then coming back to docs to copy/paste the package path which is now the header of the page; which is an awful workflow.

### Source code / logs
N/A"
16166,import error cudnnSetRNNDescriptor_v6 in tensorflow,"hi,
I have installed tensorflow-gpu on ubuntu server.at the beginning, my tensorflow work well but recently it gives an error when I logging to the python console and import tensorflow as tf.(server has python 3.5)
![tensor](https://user-images.githubusercontent.com/22906072/35007461-c60ec326-fb34-11e7-84f2-3ff8ca685895.jpg)
"
16165,Error when building from source Fedora 27 CUDA 9.1,"### System information
- OS Platform and Distribution: Fedora 27
- TensorFlow installed from (source or binary): binary
- TensorFlow version: r1.4
- Python version: 3.6.3
- Bazel version: 0.8.1
- GCC/Compiler version: 7.2.1
- CUDA/cuDNN version: CUDA 9.1 cuDNN 7.0.5
- **GPU model and memory**: NVidia Geforce GTX 960 4GB
- Exact command to reproduce: bazel build -c opt --config=cuda --verbose_failures //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
So I'm attempting to build tensorflow from source on fedora with the version of CUDA and cuDNN I already had installed to avoid have to also install an older version. The build however errors with the following message:

```
ERROR: .cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/external/nccl_archive/BUILD:33:1: error while parsing .d file: /home/xd009642/.cache/bazel/_bazel_xd009642/f9f5dea1a139b69420e1045d339dda45/execroot/org_tensorflow/bazel-out/k8-py3-opt/bin/external/nccl_archive/_objs/nccl/external/nccl_archive/src/all_reduce.cu.pic.d (No such file or directory)
<command-line>:0:15: warning: ISO C++11 requires whitespace after the macro name
<command-line>:0:1: error: macro names must be identifiers
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 0.392s, Critical Path: 0.12s
FAILED: Build did NOT complete successfully
```
I also tried the command `bazel build --config=opt --config=cuda --incompatible_load_argument_is_label=false //tensorflow/tools/pip_package:build_pip_package` from [here](http://www.python36.com/install-tensorflow141-gpu/) with the same end result.

Any guidance is appreciated as this is my first time using bazel (and also trying to compile tensorflow)."
16164,"Accidentally cancelled inceptionV3 during install, now can't install at all","Hello,
i was setting up tensorflow for image classification, and after i ran : 

python -m scripts.retrain \
  --bottleneck_dir=tf_files/bottlenecks \
  --model_dir=tf_files/models/""${ARCHITECTURE}"" \
  --summaries_dir=tf_files/training_summaries/""${ARCHITECTURE}"" \
  --output_graph=tf_files/retrained_graph.pb \
  --output_labels=tf_files/retrained_labels.txt \
  --architecture=""${ARCHITECTURE}"" \
  --image_dir=tf_files/flower_photos

It automatically started installing inception, i realized that i needed to change some options so i cancelled the install of inception.
Now i believe that i have a half install that doesn't let me install the full package or use the half package.

I may be wrong, but any suggestions would be appreciated.
FYI: i've run :
pip install inception, to which i receive a ""python setup.py egg_info"" failed with error code 1 in {my local/temp dir}

I also just tried running the scripts.retrain again, to which i receive a ""EOFError: compressed file ended before the end-of-stream marker was reached""

Running on Windows 7"
16163,Dataset.from_generator doesn't release memory after recreating the session,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 17.10
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5.0-rc0
- **Python version**: Python 3.6
- **Bazel version (if compiling from source)**: -
- **GCC/Compiler version (if compiling from source)**: -
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: see below

### Describe the problem

After closing the session and creating new one an iterator creates the generator instance but doesn't free the memory of the previous one.

Every calling of the line `session.run(x)` (see below) increases memory consumption of the script:

- 519 MiB after the first,
- 600 MiB after the second,
- 681 MiB after the third and so on.

As you can see the delta is equal to 80 MiB = N * sizeof(data.dtype). (data.dtype is float64 here)

### Source code / logs
```python
import numpy as np
import tensorflow as tf

N = 10 * 1024 * 1024

def generate():
  data = np.random.rand(N)
  for k in range(N):
    yield data[k].copy()

graph = tf.Graph()
with graph.as_default():
  x = tf.data.Dataset\
    .from_generator(generate, tf.float32)\
    .make_one_shot_iterator()\
    .get_next()

while True:
  session = tf.Session(graph=graph)
  session.run(x) # <--- PUT A BREAKPOINT HERE!
                 #  Be careful running the code without it!
  session.close()
```"
16162,segmentation fault when calling help on GraphNodeProto,"### System information
- **Have I written custom code**:
Nothing beside the example code below.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux RBSylaptop 4.14.0-2-amd64 #1 SMP Debian 4.14.7-1 (2017-12-22) x86_64 GNU/Linux

- **TensorFlow installed from**:
`    $ pip3 install tensorflow-gpu`

- **TensorFlow version**:
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9

- **Python version**:
Python 3.6.4

- **Bazel version**:
N/A

- **GCC/Compiler version**:
N/A

- **CUDA/cuDNN version**:
8.0

- **GPU model and memory**:
GeForce GTX 1070
8192 MB

- **Exact command to reproduce**:
python3 -c ""import tensorflow as tf; help(tf.profiler.profile(tf.get_default_graph()))""

You can collect some of this information using our environment capture script:

### Problem description
Calling help on the value returned by `tf.profiler.profile` generate a segmentation fault. However it might be related to the warning about the python version.

### Source code / logs
Here is the full python session:
```
$ python3                                                                                
Python 3.6.4 (default, Jan  5 2018, 02:13:53) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
/usr/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6
  return f(*args, **kwds)
>>> help(tf.profiler.profile(tf.get_default_graph()))
Parsing Inputs...

=========================Options=============================
-max_depth                  10000
-min_bytes                  0
-min_peak_bytes             0
-min_residual_bytes         0
-min_output_bytes           0
-min_micros                 0
-min_accelerator_micros     0
-min_cpu_micros             0
-min_params                 0
-min_float_ops              0
-min_occurrence             0
-step                       -1
-order_by                   name
-account_type_regexes       _trainable_variables
-start_name_regexes         .*
-trim_name_regexes          
-show_name_regexes          .*
-hide_name_regexes          
-account_displayed_op_only  true
-select                     params
-output                     stdout:

==================Model Analysis Report======================
node name | # parameters
_TFProfRoot (--/0 params)

======================End of Report==========================
zsh: segmentation fault  python3
```"
16161,tf.case raising IllegalArgumentError 'None of the conditions evaluated as True' when used with Dataset,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10 64bit
- **TensorFlow installed from (source or binary)**:
via pip
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.5

When I use tf.case within a tf.data.Dataset, I get an IllegalArgumentError 'None of the conditions evaluated as True'. However, when I use the same code without the Dataset, it works fine. Furthermore, if I understand the error message correctly, it already tells me that one condition evaluated to true (see the end of the first line):

```
InvalidArgumentError (see above for traceback): assertion failed: [None of the conditions evaluated as True. Conditions: (Equal_3:0, Equal_4:0, Equal_5:0), Values:] [1 0 0]
	 [[Node: case/If_0/Assert_1/AssertGuard/Assert = Assert[T=[DT_STRING, DT_BOOL], summarize=3](case/If_0/Assert_1/AssertGuard/Assert/Switch, case/If_0/Assert_1/AssertGuard/Assert/data_0, case/If_0/Assert_1/AssertGuard/Assert/Switch_1)]]
	 [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[]], output_types=[DT_INT32], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](OneShotIterator)]]
```

The following code reproduces the issue:
```
import tensorflow as tf


def random_map(i):
	random_int = tf.random_uniform([], minval=0, maxval=3, dtype=tf.int32)
	random_int = tf.Print(random_int, [random_int, tf.equal(random_int, 0), tf.equal(random_int, 1), tf.equal(random_int, 2)], 'random_int')

	result = tf.case([
		(tf.equal(random_int, 0), lambda: i * 10000),
		(tf.equal(random_int, 1), lambda: i * 20000),
		(tf.equal(random_int, 2), lambda: i * 30000)
	], exclusive=True)

	return result


print('working =========================================================================')
with tf.Session() as sess:
	input_pl = tf.placeholder(dtype=tf.int32)
	result = random_map(input_pl)
	for i in range(5):
		result_value = sess.run(result, feed_dict={input_pl: i})
		print(result_value)

print('not working =====================================================================')
with tf.Session() as sess:
	dataset = tf.data.Dataset.from_tensor_slices(tf.range(5))
	dataset = dataset.map(random_map)
	iterator = dataset.make_one_shot_iterator()
	next_result = iterator.get_next()

	for i in range(5):
		result_value = sess.run(next_result)
		print(result_value)
```

I also found [this question]( http://www.programfaqs.com/faq/tensorflow-case-error-invalid-argument-assertion-failed-none-of-the-conditions-evaluated-as-true/), which seems to be the same problem."
16160,tf.contrib.lookup.HashTable(kv_initializer) does not work in eager mode.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------
```
== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named tensorflow

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================

== cat /etc/issue ===============================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64
Mac OS X 10.11.4

== are we in docker =============================================
No

== compiler =====================================================
Apple LLVM version 7.3.0 (clang-703.0.31)
Target: x86_64-apple-darwin15.4.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin

== uname -a =====================================================
Darwin Xiaoyuns-MBP 15.4.0 Darwin Kernel Version 15.4.0: Fri Feb 26 22:08:05 PST 2016; root:xnu-3248.40.184~3/RELEASE_X86_64 x86_64

== check pips ===================================================
numpy (1.11.2)
protobuf (3.4.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc1
tf.GIT_VERSION = v1.5.0-rc0-9-gf9472619f6
tf.COMPILER_VERSION = v1.5.0-rc0-9-gf9472619f6
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH /usr/local/cuda/lib:/usr/local/cuda/lib:

== nvidia-smi ===================================================
/Users/xiaoyun/tf14py3/bin/tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
v1.5.0-rc0-9-gf9472619f6 1.5.0-rc1

### Describe the problem
Can not use hashtable in eager mode.

### Source code / logs
```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

keyfile = ""./key_dict""
kv_initializer = tf.contrib.lookup.TextFileInitializer(
    keyfile, tf.string, 0, tf.int64, 1, delimiter=""\t"")
table = tf.contrib.lookup.HashTable(kv_initializer, 0)
table.init.run()

filenames = [""./data1""]
dataset = tf.data.TextLineDataset(filenames)
#dataset = dataset.map(lambda tkns:table.lookup(tkns))
for x in tfe.Iterator(dataset):
    print(x)

Traceback (most recent call last):
  File ""torch/textline.py"", line 13, in <module>
    table = tf.contrib.lookup.HashTable(kv_initializer, 0)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 282, in __init__
    super(HashTable, self).__init__(table_ref, default_value, initializer)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 168, in __init__
    self._init = initializer.initialize(self)
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py"", line 531, in initialize
    if constant_op.is_constant(filename):
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 224, in is_constant
    op = tensor_or_op.op
  File ""/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 825, in op
    raise AttributeError(""op not supported for Eager Tensors."")
AttributeError: op not supported for Eager Tensors.
```"
16159,:q,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16156,fixed_address_empty,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16155,[Bug] slim.tfexample_decoder.TFExampleDecoder() crashes if RAW image with float type is used.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 'unkown', '1.4.0-rc0'
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7.0.3
- **GPU model and memory**: Pascal Titan X
- **Exact command to reproduce**:

### Describe the problem

To feed raw images with floating type saved in tfrecord, I am using slim.tfexample_decoder.TFExampleDecoder().

The problem is if I set the dtype of raw image to tf.float32, the function doesn't work with following error messages. 

```
Traceback (most recent call last):
  File ""train.py"", line 475, in <module>
    tf.app.run()
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""train.py"", line 304, in main
    common_queue_min=10 * FLAGS.batch_size)
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/dataset_data_provider.py"", line 97, in __init__
    tensors = dataset.decoder.decode(data, items)
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 427, in decode
    outputs.append(handler.tensors_to_item(keys_to_tensors))
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 324, in tensors_to_item
    return self._decode(image_buffer, image_format)
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/slim/python/slim/data/tfexample_decoder.py"", line 353, in _decode
    pred_fn_pairs, default=decode_image, exclusive=True)
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3262, in case
    case_seq = _build_case()
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3257, in _build_case
    strict=strict, name=""If_%d"" % i)
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 316, in new_func
    return func(*args, **kwargs)
  File ""/home/jhkang/tools/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1902, in cond
    (val_x.dtype.name, val_y.dtype.name))
ValueError: Outputs of true_fn and false_fn must have the same type: float32, uint8

```

To see what the problem is, I checked the ``slim/data/tfexample_decoder.py``.
I figured out that there is a ``case()`` function in `` _decode()`` of ``class Image``, and if I set dtype for the raw image as other than uint8, then, the returning tensors' dtypes from ``decode_image()`` and ``decode_raw()`` used in ``case()`` are always mis-matched. You can check the code in below.

```
 def _decode(self, image_buffer, image_format):
    """"""Decodes the image buffer.
    Args:
      image_buffer: The tensor representing the encoded image tensor.
      image_format: The image format for the image in `image_buffer`. If image
        format is `raw`, all images are expected to be in this format, otherwise
        this op can decode a mix of `jpg` and `png` formats.
    Returns:
      A tensor that represents decoded image of self._shape, or
      (?, ?, self._channels) if self._shape is not specified.
    """"""
    def decode_image():
      """"""Decodes a png or jpg based on the headers.""""""
      return image_ops.decode_image(image_buffer, self._channels)

    def decode_raw():
      """"""Decodes a raw image.""""""
      return parsing_ops.decode_raw(image_buffer, out_type=self._dtype)

    pred_fn_pairs = {
        math_ops.logical_or(
            math_ops.equal(image_format, 'raw'),
            math_ops.equal(image_format, 'RAW')): decode_raw,
    }
    image = control_flow_ops.case(
        pred_fn_pairs, default=decode_image, exclusive=True)

    image.set_shape([None, None, self._channels])
    if self._shape is not None:
      image = array_ops.reshape(image, self._shape)

    return image
```
The returning tensor of `` return image_ops.decode_image(image_buffer, self._channels)`` only supports uint8 type of tensors.  
To fix this problem, I changed the previous code into the following code. 
``return math_ops.cast(image_ops.decode_image(image_buffer, self._channels), self._dtype)``

### Source code / logs
I am attaching my source code for feeding dataset from tfrecord using slim.

```
from __future__ import absolute_import, division, print_function

import os

import tensorflow as tf

from datasets import dataset_utils

slim = tf.contrib.slim

_FILE_PATTERN = '%s_*.tfrecord'

SPLITS_TO_SIZES = {'train': 24523, 'validation': 6130} # lineGT 20180112
_NUM_CLASSES = 10

_ITEMS_TO_DESCRIPTIONS = {
    'intensity': 'an intensity map',
    'heightmap': 'an heightmap map',
    'label': 'Ground truth segmentation mask',
}


def get_split(split_name, dataset_dir, file_pattern=None, reader=None):
  if split_name not in SPLITS_TO_SIZES:
    raise ValueError('split name %s was not recognized.' % split_name)

  if not file_pattern:
    file_pattern = _FILE_PATTERN
  file_pattern = os.path.join(dataset_dir, file_pattern % split_name)
  print(file_pattern)
  # Allowing None in the signature so that dataset_factory can use the
  # default.
  if reader is None:
    reader = tf.TFRecordReader

  keys_to_features = {
      'image/intensity':
          tf.FixedLenFeature(
              (), tf.string, default_value=''),
      'image/format':
          tf.FixedLenFeature(
              (), tf.string, default_value='raw'),
      'image/height':
          tf.FixedLenFeature(
              (), tf.int64, default_value=0),
      'image/width':
          tf.FixedLenFeature(
              (), tf.int64, default_value=0),
      'image/mask':
          tf.FixedLenFeature(
              (), tf.string, default_value=''),
      'image/mask/format':
          tf.FixedLenFeature(
              (), tf.string, default_value='raw'),
      'image/heightmap':
          tf.FixedLenFeature(
              (), tf.string, default_value=''),
      'image/heightmap/format':
          tf.FixedLenFeature(
              (), tf.string, default_value='raw'),
      'image/filename':
          tf.FixedLenFeature(
              (), tf.string, default_value=''),
  }

  items_to_handlers = {
      'intensity':
          slim.tfexample_decoder.Image(
            'image/intensity', 'image/format', channels=1, dtype=tf.float32),
      'heightmap':
          slim.tfexample_decoder.Image(
            'image/heightmap', 'image/heightmap/format', channels=1, dtype=tf.float32),
      'label':
          slim.tfexample_decoder.Image(
            'image/mask', 'image/mask/format', channels=1),
      'height':
          slim.tfexample_decoder.Tensor('image/height'),
      'width':
          slim.tfexample_decoder.Tensor('image/width'),
      'fileid':
          slim.tfexample_decoder.Tensor('image/filename'),
  }

  decoder = slim.tfexample_decoder.TFExampleDecoder(keys_to_features,
                                                    items_to_handlers)

  labels_to_names = None

  return slim.dataset.Dataset(
      data_sources=file_pattern,
      reader=reader,
      decoder=decoder,
      num_samples=SPLITS_TO_SIZES[split_name],
      items_to_descriptions=_ITEMS_TO_DESCRIPTIONS,
      num_classes=_NUM_CLASSES,
      labels_to_names=labels_to_names)
```"
16152,DeprecationWarning from `inspect.getargspec()`,"`inspect.getargspec` is deprecated in Python 3
https://docs.python.org/3/library/inspect.html#inspect.getargspec

I solved the problem in keras like this:
https://github.com/keras-team/keras/pull/7035

### System information
- Using tensorflow as a keras backend (keras 2.1.2)
- Linux Ubuntu 16.04
- installed from conda
- version 1.3.0
- python 3.6.4

### Describe the problem
We recently switched from theano to tensorflow and this warning message is filling up my test output.

### Source code / logs
```
/home/<name>/.conda/envs/<env>/lib/python3.6/site-packages/tensorflow/python/util/tf_inspect.py:45: DeprecationWarning:
  
  inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()
```
"
16151,ValueError: Labels are incompatible with given information. ,"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4

I am having problems in adding validation monitors to `Estimator.fit`. With this code I have:

```
def main(_):
    image_paths, labels = dataset_utils.read_dataset_list('../test/dummy_labels_file.txt')
    data_dir = ""../test/dummy_data/""
    images = dataset_utils.read_images(data_dir=data_dir, image_paths=image_paths, image_extension='png')
    print('Done reading images')
    images = dataset_utils.resize(images, (1596, 48))
    images = dataset_utils.transpose(images)
    labels = dataset_utils.encode(labels)
    x_train, x_test, y_train, y_test = dataset_utils.split(features=images, test_size=0.5, labels=labels)
    print(x_test)
    x_train_seq_lens = dataset_utils.get_seq_lens(x_train)
    x_test_seq_lens = dataset_utils.get_seq_lens(x_test)

    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_train),
           ""seq_lens"": np.array(x_train_seq_lens)},
        y=np.array(y_train),
        num_epochs=1,
        shuffle=True,
        batch_size=1
    )

    validation_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={""x"": np.array(x_test),
           ""seq_lens"": np.array(x_test_seq_lens)},
        y=np.array(y_test),
        shuffle=True
    )

    validation_monitor = learn.monitors.ValidationMonitor(
        input_fn=validation_input_fn,
        every_n_steps=1
    )

    model = GridRNNModelFn(num_time_steps=1596, num_features=48, num_hidden_units=128, num_classes=80,
                           learning_rate=0.001, optimizer=Optimizers.MOMENTUM)

    classifier = learn.Estimator(model_fn=model.model_fn, params=model.params, model_dir=""/tmp/grid_rnn_ocr_model"")
    classifier.fit(input_fn=train_input_fn, monitors=[validation_monitor])


if __name__ == '__main__':
    tf.app.run(main=main)
```

It throws this error:

`ValueError: Labels are incompatible with given information. Given labels: Tensor(""random_shuffle_queue_DequeueUpTo:3"", shape=(?, 37), dtype=int32), required signatures: TensorSignature(dtype=tf.int32, shape=TensorShape([Dimension(None), Dimension(33)]), is_sparse=False).`

Which leads me to think that the dynamic label lengths are not accepted. To reproduce this, simply clone this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr) and run the script specified in the readme."
16150,Tensorflow Debugger with Multithreading,"
## System information

1. OS Platform: Ubuntu 14.04 
2. TensorFlow installed from source : GPU-Version 1.14 branch 
3. cuDNN: 7.0
4. Python version: 3.4

--

I'm using **tf.Estimator** together with **numpy_input_fn**. Moreover, I set all num_threads to 1. However, tfdbg somehow doesn't get along well with child threads auto-created by new estimator API.

Here is how I defined Estimator:

```
def do_train(tr_data, vl_data):

    # Create Estimator
    sess_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)
    sess_conf.gpu_options.allow_growth = True
    config = tf.estimator.RunConfig(model_dir=FLAGS.model_dir,  # CheckpointSaverHook
                                    save_checkpoints_steps=100,  # CheckpointSaverHook
                                    log_step_count_steps=10,  # SummarySaverHook
                                    session_config=sess_conf)
    music_classifier = tf.estimator.Estimator(model_fn,
                                              config=config,
                                              params={'max_seq': tr_data.max_seq})
    # Prepare input data
    tr_input_fn = tf.estimator.inputs.numpy_input_fn(
        {'input_ph': tr_data.data, 'seq_len_ph': tr_data.seq_len},
        batch_size=FLAGS.batch_size,
        num_epochs=FLAGS.num_epochs,
        num_threads=1,
        shuffle=True
    )
    # Extra Hooks
    logging_hook = tf.train.LoggingTensorHook(
        tensors={'probabilities': 'probs_tensor'},
        every_n_iter=10
    )
    debugging_hook = tf_debug.LocalCLIDebugHook(thread_name_filter=""MainThread$"", dump_root=""./dump"")
    # Train
    music_classifier.train(tr_input_fn, hooks=[debugging_hook])
```


Although the code runs fine; whenever I run it in debug mode, it fails at executing run command on tfdbg command line. Following is the exception I got:

```
2018-01-16 10:25:26.587936: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-16 10:25:26.588715: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 1060 major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:01:00.0
totalMemory: 5.93GiB freeMemory: 5.38GiB
2018-01-16 10:25:26.588732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0, compute capability: 6.1)
2018-01-16 10:25:41.373907: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434
Traceback (most recent call last):
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434
     [[Node: rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _class=[""loc:@rnn/while/TensorArrayReadV3""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter, rnn/while/rnn/msarnn_cell/Identity/Enter/_259, rnn/while/TensorArrayReadV3, rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter_1, ^rnn/while/rnn/msarnn_cell/cond/Merge/_263)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 202, in <module>
    tf.app.run(main=main)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 191, in main
    do_train(tr_data, vl_data)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 169, in do_train
    music_classifier.train(tr_input_fn, hooks=[debugging_hook])
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 783, in _train_model
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 521, in run
    run_metadata=run_metadata)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 892, in run
    run_metadata=run_metadata)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 967, in run
    raise six.reraise(*original_exc_info)
  File ""/usr/local/lib/python3.4/dist-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 952, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 1024, in run
    run_metadata=run_metadata)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/training/monitored_session.py"", line 827, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434
     [[Node: rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _class=[""loc:@rnn/while/TensorArrayReadV3""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter, rnn/while/rnn/msarnn_cell/Identity/Enter/_259, rnn/while/TensorArrayReadV3, rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter_1, ^rnn/while/rnn/msarnn_cell/cond/Merge/_263)]]

Caused by op 'rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3', defined at:
  File ""/usr/lib/python3.4/runpy.py"", line 170, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.4/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 202, in <module>
    tf.app.run(main=main)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/platform/app.py"", line 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 191, in main
    do_train(tr_data, vl_data)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 169, in do_train
    music_classifier.train(tr_input_fn, hooks=[debugging_hook])
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/music_modeling.py"", line 116, in model_fn
    initializer=tf.glorot_normal_initializer())  # [BSxMTxOS], [BSxSS]
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py"", line 253, in dynamic_msa_rnn
    dtype=tf.float32)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py"", line 614, in dynamic_rnn
    dtype=dtype)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py"", line 777, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2816, in while_loop
    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2640, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2590, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py"", line 760, in _time_step
    skip_conditionals=True)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py"", line 236, in _rnn_step
    new_output, new_state = call_cell()
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn.py"", line 748, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 183, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/layers/base.py"", line 575, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py"", line 204, in call
    ms_analyzer = self._extract_features(inputs)
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py"", line 151, in _extract_features
    sub_sig = self._gen_sub_sig(inputs)  # BSxTxIN
  File ""/home/ilithefallen/Documents/phdStudies/coding/temizelRepo/MultiScaleRNN/model/tf_msa_rnn.py"", line 105, in _gen_sub_sig
    self.__ta_signal = self.__ta_signal.write(self.__time, inputs)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/util/tf_should_use.py"", line 107, in wrapped
    return _add_should_use_warning(fn(*args, **kwargs))
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/tensor_array_ops.py"", line 310, in write
    name=name)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/ops/gen_data_flow_ops.py"", line 5038, in _tensor_array_write_v3
    flow_in=flow_in, name=name)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/ilithefallen/tensorflow/lib/python3.4/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): TensorArray ta_signal_0: Tried to write to index 2434 but array is not resizeable and size is: 2434
     [[Node: rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3 = TensorArrayWriteV3[T=DT_FLOAT, _class=[""loc:@rnn/while/TensorArrayReadV3""], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter, rnn/while/rnn/msarnn_cell/Identity/Enter/_259, rnn/while/TensorArrayReadV3, rnn/while/rnn/msarnn_cell/TensorArrayWrite/TensorArrayWriteV3/Enter_1, ^rnn/while/rnn/msarnn_cell/cond/Merge/_263)]]
```

Thank you for your support in advance."
16149,Dataset from string generator raises Exception with python 2,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS X 10.12.6
- **TensorFlow installed from (source or binary)**: binary cpu version from pypi
- **TensorFlow version (use command below)**: ('v1.5.0-rc0-9-gf9472619f6', '1.5.0-rc1')
- **Python version**: 2.7.14

### Describe the problem
I'm reading text data from file in generator. Encoding: UTF-8.
After some preprocessing i return it in generator manner.
Next, i'm trying to create Dataset from this generator.

Code below produce exception in both Python 2&3 for TensorFlow 1.4.
For TF 1.5.rc1 & Python 3 there is no errors.
For TF 1.5.rc1 & Python 2 error exist.

### Source code / logs
```python
# -*- coding: utf-8 -*-
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import tensorflow as tf


def generator():
    data = [
        [u'Простой', u'тест', u'юникода'],
        [u'никогда', u'не', u'бывает', u'простым']
    ]

    for seq in data:
        yield seq, [0, 1, 2, 3]


def dataset():
    dataset = tf.data.Dataset.from_generator(
        generator,
        (tf.string, tf.int32),
        (tf.TensorShape([None]), tf.TensorShape([None]))
    )
    dataset = dataset.padded_batch(2, padded_shapes=([None], [None]), padding_values=('', 0))

    return dataset


iterator = dataset().make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    value = sess.run(next_element)
    print(value)
```"
16148,non_max_suppression is on CPU?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
     Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
      Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
      binary(By pip)
- **TensorFlow version (use command below)**:
      1.4.1
- **Python version**: 
      3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
    8.0.61/6.0.21
- **GPU model and memory**:
    GTX 1080 Ti, 11172MiB
- **Exact command to reproduce**:
     python main.py

### Describe the problem
    
I train my RFCN by tensorflow. My project need very high speed. So I use the profile and I find that non_max_suppression is on CPU? Is there a GPU version?I think if you calculate all pairs of boxes IOU first, then just for-loop once will ultimately boost speed, there have some trick in it, just see the source code in [https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms](https://github.com/rbgirshick/py-faster-rcnn/tree/master/lib/nms). I think cuda version of NMS is faster than CPU version.
"
16147,Inference on V100 with TF1.5 is extremely slow. ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source and Virtual Env, problem doesn't change
- **TensorFlow version (use command below)**: 1.5.0-rc1 (Makes no difference on 1.4)
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: CUDA 9/7.0.5
- **GPU model and memory**: V100 - 16GB
- **Exact command to reproduce**:

### Describe the problem

- Inference using the V100 is very slow. For example performing object detection with SSD Mobilenet is achieving a max frame rate of ~8, compared to ~45 on a GTX1080
- Initialization with a warm up image is extremely slow - up to 2 mins for the first image. 
- I have tried model quantization using graph_transforms/transform_graph (in an attempt to use the FP16 mode) and various combinations of CUDA, cuDNN and Tensorflow versions with no difference. 

Is there some recommended environment setup for the V100?
I am successfully running Darknet (https://pjreddie.com/darknet/) with a massive increase of speed. "
16146,Documentation for GridLSTMCell is lacking and does not match the paper,"### Describe the problem

The documentation for tf.contrib.rnn.GridLSTMCell cites the paper ""Grid Long Short-Term Memory"", by Kalchbrenner et al.
The paper describes an architecture, called the 2D Grid LSTM, to replace a stack of LSTM cells. In a 2D Grid LSTM, 2 state components are passed from one layer to the next vertically.

In Tensorflow RNN parlance, one would expect both the state and the output of the cell to be an LSTMStateTuple, which would allow seamless integration with a MultiRNNCell.
In the current implementation, instead it appears that the vertical unrolling is done internally to the GridLSTMCell.
I say it appears, because I can't quite make sense of the arguments and their documentation: specifically, there is a required ""num_frequency_block"" argument whose meaning is quite obscure.
Looking at the implementation also did not help me understand what value is actually expected in that parameter, and the related parameters.
Note that the above mentioned paper does not talk about frequencies anywhere.

Would it be possible to expand on the documentation for the cell, as well as provide a code example on how to replicate the 2D Grid LSTM from the paper?
"
16144,Is it possible to train CNN model by using tensorflow JAVA API?,"Hello, TF.
I have plane to train my CNN model by using tensorflow JAVA API.
I got success on simple model( with a simple matmul operation between weights and bias)
BUT I failed to train CNN model.
"
16143,"Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""","compiled tensorflow r.15 from source , when import tensorflow in python got following error:
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
ImportError: /usr/local/lib/python2.7/site-packages/tensorflow-1.5.0rc1-py2.7-freebsd-11.0-RELEASE-p1-i386.egg/tensorflow/python/_pywrap_tensorflow_internal.so: Undefined symbol ""_ZN3Aws8Security14SecureMemClearEPhj""


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/install_sources#common_installation_problems

for some common reasons and solutions.  Include the entire stack trace

thanks in advance !!!"
16139,Segmentation fault when running optimization step with 3d convolution,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux (4.14.13-1 linux kernel version)
- **TensorFlow installed from (source or binary)**: source (using the package here: https://www.archlinux.org/packages/community/x86_64/python-tensorflow-cuda/)
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: 9.1.85-1/7.0.5-2
- **GPU model and memory**: NVidia Quadro K4200, 4028MiB
- **Exact command to reproduce**: `python test.py`
Note that the same code also fails in a Ubuntu docker container (Dockerfile attached).

### Describe the problem
I set up a computation graph with a 3d convolution. I can evaluate the result of this graph, but when I attempt to optimize the parameters of the graph (`train_step.run(feed_dict={x: sample, y_: label})`), tensorflow segfaults.

In a jupyterlab notebook running on Ubuntu, if I run the same code, the notebook hangs indefinitely at the same line. In both cases, the last line of the program is never run - ""ran train step"" is never printed.

I also tried running this on my CPU with `os.environ['CUDA_VISIBLE_DEVICES'] = '-1'`. I get the same segfault.

The segfault goes away if I do any of the following:
- Remove the 3d convolution
- Reduce the input size significantly (e.g. 100x smaller to 1 x 41 x 96 x 128 x 1)
- Reduce the kernel size significantly

### Source code / logs
Minimal example code (test.py):
```python
import numpy as np
import tensorflow as tf

sample = np.zeros((1, 41, 960, 1280, 1))
label = np.zeros((1,))

rc_kernel = np.ones((31,))

x = tf.placeholder(tf.float64, shape=[None, 41, 960, 1280, 1])
y_ = tf.placeholder(tf.float64, shape=[None])

W_conv_r = tf.Variable(rc_kernel.reshape((1, -1, 1, 1, 1)))
h_blur = tf.nn.conv3d(x, W_conv_r, [1, 1, 1, 1, 1], ""VALID"")

h_sum = tf.reduce_sum(tf.reduce_sum(tf.reduce_sum(h_blur, axis=3), axis=2), axis=1)
y = tf.sigmoid(h_sum)

sq_err = (y - y_) ** 2

train_step = tf.train.GradientDescentOptimizer(0.1).minimize(sq_err)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    E = sq_err.eval(feed_dict={x: sample, y_: label})
    print(f'E = {E}')
    train_step.run(feed_dict={x: sample, y_: label})  # fails here
    print('ran train step')
```

Dockerfile:
[Dockerfile.txt](https://github.com/tensorflow/tensorflow/files/1633128/Dockerfile.txt)"
16138,Build fails with Visual Studio 2017,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
Source
- **TensorFlow version (use command below)**:
1.5.0-dev20180103
https://github.com/tensorflow/tensorflow/commit/25d275280dfb163674f81c7681c2c1d34545a155
- **Python version**: 
3.5.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
9.0 with cuDNN 7
- **GPU model and memory**:
GeForce GTX 1060 6GB
- **Exact command to reproduce**:
Open Visual Studio x64 Native Tools Command Prompt with admin rights. 
using MSBuild 15.5.180.51428,

```
cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release ^
-DSWIG_EXECUTABLE=C:/tools/swigwin-3.0.12/swig.exe ^
-DPYTHON_EXECUTABLE=C:\Users\csemp\AppData\Local\Programs\Python\Python35\python.exe ^
-DPYTHON_LIBRARIES=C:\Users\csemp\AppData\Local\Programs\Python\Python35\libs\python35.lib ^
-Dtensorflow_ENABLE_GPU=ON ^
-DCUDNN_HOME=""C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0"" ^
-Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX ^
-Dtensorflow_BUILD_CC_EXAMPLE=OFF

set PreferredToolArchitecture=x64
""C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\MSBuild\15.0\Bin\amd64\MSBuild.exe"" /m:2 /p:Configuration=Release tf_python_build_pip_package.vcxproj /v:diag > diag.log
```

### Describe the problem
The build fails with the error 
```
 133>CustomBuild: (TargetId:8893)
                     CMake Error at tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj.Release.cmake:222 (message): (TaskId:3323)
                       Error generating (TaskId:3323)
                       C:/Users/csemp/dev/tensorflowbuild/tensorflow/tensorflow/contrib/cmake/build/CMakeFiles/tf_core_gpu_kernels.dir/__/__/core/kernels/Release/tf_core_gpu_kernels_generated_adjust_contrast_op_gpu.cu.cc.obj (TaskId:3323)
                      (TaskId:3323)
                      (TaskId:3323)
```
It also has the error 
```
(ClCompile target) -> 
  C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file C:\Users\csemp\dev\tensorflowbuild2\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [C:\Users\csemp\dev\tensorflowbuild2\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2100: illegal indirection (compiling source file C:\Users\csemp\dev\tensorflowbuild2\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [C:\Users\csemp\dev\tensorflowbuild2\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
```
the latter of which can be fixed by a simple change, as noted in https://github.com/tensorflow/tensorflow/issues/15925#issuecomment-356275963 .
Creating this new issue per @gunan's request
https://github.com/tensorflow/tensorflow/issues/14691#issuecomment-356846982

### Source code / logs
The log file is way too big to post apparently. I'm open to suggestions

"
16137,download_dependencies.sh fails for r1.5 and master.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**:  3.6 - Anaconda
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: 5.4.1 or 4.8.4 (tried both)
- **CUDA/cuDNN version**: CUDA 8.0 and cuDNN 6.0.21 or cuDNN 5.0.5 (tried both)
- **GPU model and memory**: 4x NVIDIA GTX Titan X 12GB
- **Exact command to reproduce**: `sh ./tensorflow/contrib/makefile/download_dependencies.sh`

### Describe the problem
I have put together a script to build a distribution of TensorFlow for C++ development. Part of this script invokes `tensorflow/contrib/makefile/download_dependencies.sh`. However, this fails with the following output(truncated, error is the same for each subsequent download):

`
downloading https://bitbucket.org/eigen/eigen/get/034b6c3e1017.tar.gz
./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
downloading https://mirror.bazel.build/github.com/google/gemmlowp/archive/010bb3e71a26ca1d0884a167081d092b43563996.zi
p
./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
downloading https://github.com/google/googletest/archive/release-1.8.0.tar.gz
./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
downloading https://mirror.bazel.build/github.com/google/nsync/archive/8502189abfa44c249c01c2cad64e6ed660a9a668.tar.g
z
./tensorflow/contrib/makefile/download_dependencies.sh: 61: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
./tensorflow/contrib/makefile/download_dependencies.sh: 63: ./tensorflow/contrib/makefile/download_dependencies.sh: [
[: not found
downloading https://mirror.bazel.build/github.com/google/protobuf/archive/b04e5cba356212e4e8c66c61bbe0c3a20537c5b9.tar.gz
`
It appears that maybe the download URL's are out of date. If this is indeed a bug, it should be reproducible on Linux with the attached script(which should be sufficiently commented.).

### Source code / logs
See attached script.
[build_tf.txt](https://github.com/tensorflow/tensorflow/files/1632802/build_tf.txt)

"
16136,"Error while using cuda-9.1, libcublas.so.8.0: cannot open shared object file: No such file or directory","Hi,
I am having the import problem. I installed cuda 9.1 and set the path as suggested https://stackoverflow.com/questions/36159194/tensorflow-libcudart-so-7-5-cannot-open-shared-object-file-no-such-file-or-di
Should i install cuda 8.0 for resolving this problem?
Issue :tensorflow not supporting cuda version greater than 8.

The error is:

            import tensorflow
            Traceback (most recent call last):
            File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in
            from tensorflow.python.pywrap_tensorflow_internal import *
            File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in
            _pywrap_tensorflow_internal = swig_import_helper()
            File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
            _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
            File ""/home/honeypot/tensorflow/lib/python3.5/imp.py"", line 242, in load_module
            return load_dynamic(name, filename, file)
            File ""/home/honeypot/tensorflow/lib/python3.5/imp.py"", line 342, in load_dynamic
            return _load(spec)
            ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File """", line 1, in
File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/init.py"", line 24, in
from tensorflow.python import *
File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/init.py"", line 49, in
from tensorflow.python import pywrap_tensorflow
File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 72, in
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in
from tensorflow.python.pywrap_tensorflow_internal import *
File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in
_pywrap_tensorflow_internal = swig_import_helper()
File ""/home/honeypot/tensorflow/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""/home/honeypot/tensorflow/lib/python3.5/imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""/home/honeypot/tensorflow/lib/python3.5/imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: libcublas.so.8.0: cannot open shared object file: No such file or directory

Failed to load the native TensorFlow runtime"
16135,Distributed Tensorflow  using MPI,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

I have tried stackflow and Google group discussion forum but could  get any reply or comment

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Redhat 7.4

- **TensorFlow installed from (source or binary)**:
from source with MPI
- **TensorFlow version (use command below)**:
1.41
- **Python version**: 
2.7.14
- **Bazel version (if compiling from source)**:

- **GCC/Compiler version (if compiling from source)**:
GCC 6.0
- **CUDA/cuDNN version**:
8.0/6.5
- **GPU model and memory**:
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.81                 Driver Version: 384.81                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla K20Xm         Off  | 00000000:08:00.0 Off |                    0 |
| N/A   34C    P0    61W / 235W |      0MiB /  5699MiB |     72%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I am using the following  script to launch distributed computing.


#! /bin/bash

module load openmpi/3.0.0-gnu

host=$(hostname -s)
if [[ $host == ""node06"" ]]; then
        echo ""statring Node 6""
        python tf_dis_2.py --job_name=""ps"" --task_index=0
elif [[ $host == ""node07"" ]]; then
        echo ""starting Node 7 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=0
elif [[ $host == ""node08"" ]]; then
        echo ""starting Node 8 as worker""
        python tf_dis_2.py --job_name=""worker"" --task_index=1
fi

-----

I am running it on slurm  with three nodes.

srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh

I am using MPI instead of GPRC.

I am getting the following message:

---------------------------------------------------
srun -N 3 -n 3 --gres=gpu:1 -w node[06-08] test.sh
statring Node 6
starting Node 8 as worker
starting Node 7 as worker
2018-01-15 11:34:59.961617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:34:59.961674: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.020327488   36133 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.026716: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:35:00.026760: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> localhost:2224}
2018-01-15 11:35:00.029261: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2224
2018-01-15 11:35:00.439045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:35:00.439124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:35:00.497022377   13701 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:35:00.503585: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}
2018-01-15 11:35:00.503622: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> node07:2223, 1 -> node08:2224}
2018-01-15 11:35:00.505803: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2222
2018-01-15 11:33:39.681311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: Tesla K20Xm major: 3 minor: 5 memoryClockRate(GHz): 0.732
pciBusID: 0000:08:00.0
totalMemory: 5.57GiB freeMemory: 5.49GiB
2018-01-15 11:33:39.681375: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K20Xm, pci bus id: 0000:08:00.0, compute capability: 3.5)
E0115 11:33:39.739196190   46236 ev_epoll1_linux.c:1051]     grpc epoll fd: 22
2018-01-15 11:33:39.745655: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> node06:2222}
2018-01-15 11:33:39.745697: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> node08:2224}
2018-01-15 11:33:39.747692: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:324] Started server with target: grpc://localhost:2223
Abid Malik
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 102, in <module>
    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 336, in __init__
    self._verify_setup()
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 885, in _verify_setup
    ""their device set: %s"" % op)
ValueError: When using replicas, all Variables must have their device set: name: ""weights/Variable""
op: ""VariableV2""
attr {
  key: ""container""
  value {
    s: """"
  }
}
attr {
  key: ""dtype""
  value {
    type: DT_FLOAT
  }
}
attr {
  key: ""shape""
  value {
    shape {
      dim {
        size: 784
      }
      dim {
        size: 100
      }
    }
  }
}
attr {
  key: ""shared_name""
  value {
    s: """"
  }
}

2018-01-15 11:33:41.719083: E tensorflow/core/distributed_runtime/master.cc:269] Master init: Unavailable: Endpoint read failed
Extracting MNIST_data/train-images-idx3-ubyte.gz
Extracting MNIST_data/train-labels-idx1-ubyte.gz
Extracting MNIST_data/t10k-images-idx3-ubyte.gz
Extracting MNIST_data/t10k-labels-idx1-ubyte.gz
Variables initialized ...
Traceback (most recent call last):
  File ""tf_dis_2.py"", line 114, in <module>
    with sv.prepare_or_wait_for_session(server.target) as sess:
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py"", line 708, in prepare_or_wait_for_session
    init_feed_dict=self._init_feed_dict, init_fn=self._init_fn)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 273, in prepare_session
    config=config)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/session_manager.py"", line 205, in _restore_checkpoint
    saver.restore(sess, ckpt.model_checkpoint_path)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/amalik/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.UnavailableError: Endpoint read failed
srun: error: node08: task 2: Exited with exit code 1
srun: error: node07: task 1: Exited with exit code 1
---------------------------------------------------------------------------------

Why is it crashing? I have been trying to solve this for the last three weeks by putting it on different forums and groups. However, could not get any reply. I would be grateful if someone can guide me. I apologize in advance if this is not the right forum.





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:

``
from __future__ import print_function

import tensorflow as tf
import sys
import time


print(""Abid Malik"")


parameter_servers = [""node06:2222""]
workers = [""node07:2223"",""node08:2224""]
cluster = tf.train.ClusterSpec({""ps"":parameter_servers, ""worker"":workers})



tf.app.flags.DEFINE_string(""job_name"", """", ""Either 'ps' or 'worker'"")
tf.app.flags.DEFINE_integer(""task_index"", 0, ""Index of task within the job"")
FLAGS = tf.app.flags.FLAGS





server = tf.train.Server(
    cluster,
    job_name=FLAGS.job_name,
    task_index=FLAGS.task_index)


batch_size = 100
learning_rate = 0.0005
training_epochs = 20
logs_path = ""/tmp/mnist/1""


from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets('MNIST_data', one_hot=True)

if FLAGS.job_name == ""ps"":
    server.join()
elif FLAGS.job_name == ""worker"":

        with tf.device(tf.train.replica_device_setter(worker_device=""/job:worker/task:%d"" % FLAGS.task_index,cluster=cluster)):
              
                global_step = tf.get_variable('global_step',[],initializer = tf.constant_initializer(0), trainable = False)

              
        with tf.name_scope('input'):
              
                  x = tf.placeholder(tf.float32, shape=[None, 784], name=""x-input"")
               
                  y_ = tf.placeholder(tf.float32, shape=[None, 10], name=""y-input"")

                
        tf.set_random_seed(1)
        with tf.name_scope(""weights""):
                        W1 = tf.Variable(tf.random_normal([784, 100]))
                        W2 = tf.Variable(tf.random_normal([100, 10]))

               
        with tf.name_scope(""biases""):
                        b1 = tf.Variable(tf.zeros([100]))
                        b2 = tf.Variable(tf.zeros([10]))

               
        with tf.name_scope(""softmax""):
                        # y is our prediction
                        z2 = tf.add(tf.matmul(x,W1),b1)
                        a2 = tf.nn.sigmoid(z2)
                        z3 = tf.add(tf.matmul(a2,W2),b2)
                        y  = tf.nn.softmax(z3)

               
        with tf.name_scope('cross_entropy'):
                        # this is our cost
                        cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))

             
        with tf.name_scope('train'):
                       
                                                                                                                                                                                                                                                                

grad_op = tf.train.GradientDescentOptimizer(learning_rate)
                        train_op = grad_op.minimize(cross_entropy, global_step=global_step)


        with tf.name_scope('Accuracy'):
                        # accuracy
                        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))
                        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

   
        tf.summary.scalar(""cost"", cross_entropy)
        tf.summary.scalar(""accuracy"", accuracy)

        saver = tf.train.Saver()
       
        summary_op = tf.summary.merge_all()
        init_op = tf.global_variables_initializer()
        print(""Variables initialized ..."")

        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),logdir=""/tmp/train_logs"",global_step=global_step,init_op=init_op)


        begin_time = time.time()
        frequency = 100
        with sv.prepare_or_wait_for_session(server.target) as sess:
                # create log writer object (this will log on every machine)
                writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())

                # perform training cycles
                start_time = time.time()
                for epoch in range(training_epochs):

                        # number of batches in one epoch
                        batch_count = int(mnist.train.num_examples/batch_size)

                        count = 0
                        for i in range(batch_count):
                                batch_x, batch_y = mnist.train.next_batch(batch_size)

                                # perform the operations we defined earlier on batch
                                _, cost, summary, step = sess.run([train_op, cross_entropy, summary_op, global_step], feed_dict={x: batch_x, y_: batch_y})
                                writer.add_summary(summary, step)

                                count += 1
                                if count % frequency == 0 or i+1 == batch_count:
                                        elapsed_time = time.time() - start_time
                                        start_time = time.time()
                                        print(""Step: %d,"" % (step+1),
                                                                "" Epoch: %2d,"" % (epoch+1),
                                                                "" Batch: %3d of %3d,"" % (i+1, batch_count),
                                                                "" Cost: %.4f,"" % cost,
                                                                "" AvgTime: %3.2fms"" % float(elapsed_time*1000/frequency))
                                        count = 0


                print(""Test-Accuracy: %2.2f"" % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))
                print(""Total Time: %3.2fs"" % float(time.time() - begin_time))
                print(""Final Cost: %.4f"" % cost)

        sv.stop()
        print(""done"")
                                                                                                                                                                                                                                                                 

``"
16132,Bug while printing parameters and gradients,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary (anaconda)
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: using CPU
- **GPU model and memory**: using CPU
- **Exact command to reproduce**: see below

### Describe the problem
The model is very simple, I do digits classification with MNIST. There is only one parameter matrix W, no bias and no non-linearities. The model show convergence since the loss is decreasing. I checked predictions and accuracy but I do not copy paste useless code here. If I print the parameters before and after training they are the same, however, it shouldn't be the case. Moreover, the gradient of the loss w.r.t. parameters are zero but again it shouldn't be the case since the model converges so there should be a non-zero gradient. I cannot explain why and my implementation seems correct, that's why I am posting my code here.

### Source code / logs
```
import numpy as np
import tensorflow as tf

tf.set_random_seed(42)

from tensorflow.examples.tutorials.mnist import input_data

mnist = input_data.read_data_sets('data/', one_hot=True)

x = tf.placeholder(tf.float32, shape=(None, 784))
y = tf.placeholder(tf.float32, shape=(None, 10))

W = tf.get_variable('W0', (784, 10))
pred = tf.matmul(x, W)
loss = tf.reduce_sum((y - pred) ** 2)
grads = tf.gradients(loss, W)
train_step = tf.train.AdamOptimizer().minimize(loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

for _ in range(1000):
    x_mb, y_mb = mnist.train.next_batch(32)
    loss_, _ = sess.run([loss, train_step], {x: x_mb, y: y_mb})
    print('loss: {:2.5}'.format(loss_))

>>> I won't print uselss log here but the loss is decreasing

print(sess.run(W))

>>> [[-0.0823722  -0.01139299 -0.04053238 ... -0.03432762 -0.05707605
  -0.01042821]
 [ 0.06725802  0.07879441  0.05811419 ... -0.05443887 -0.03835129
  -0.0796528 ]
 [-0.06725079 -0.00356448  0.0823487  ...  0.0006832  -0.01058736
  -0.04312544]
 ...
 [ 0.04159895  0.01873457  0.05547244 ... -0.04325137 -0.00306174
   0.06578781]
 [ 0.05061891 -0.07273331  0.06083969 ...  0.0548989  -0.01343339
  -0.02337921]
 [ 0.02918045 -0.05145956  0.0042838  ...  0.05564766 -0.04886324
  -0.02436799]]

print(sess.run(grads, {x: x_mb, y: y_mb}))

>>> [array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)]

```
"
16128,Improvement Proposal - tf.alphas_like (merging tf.ones_like and tf.zeros_like into one),"Hello dear tensorflowers,

In a research project, I encountered the need to create tensors of the same shape than any other tensor with a custom value (not just 0 or 1), could be Boolean, Floats, Integers and so on.

The functions prototypes are the following and will be implemented in [ tensorflow/python/ops/array_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py): 
- **tf.alphas** (shape, alpha_value, name=None)
- **tf.alphas_like** (tensor, alpha_value, name=None, optimize=True)

The code is not created from scratch. It is **highly** inspired by the functions tf.ones, tf.zeros for tf.alphas and by tf.zeros_like, tf.ones_like for tf.alphas_like.

The code use the latest implementation and has been designed to work with *eager_mode*.
The number of modification is relatively small, thus I am relatively confident on the robustness of the new implementation (largely based on the existing one).

The idea is to reproduce and merge the functions while enabling to set any custom value in the tensor:

- **tf.alphas merges:**
  - tf.ones
  - tf.zeros
- **tf.alphas_like merges:**
  - tf.zeros_like
  - tf.ones_like

### How is the API Working ?

My new functions take a parameter _alpha_value_ and fill the tensor with this value. This allows me to run such a script:

```python
import tensorflow as tf

a = tf.constant([
    [
        [4, 5, 6],
        [1, 2, 3]
    ],
    [
        [4, 5, 6],
        [1, 2, 3]
    ]
])

b1 = tf.alphas_like(a, 0.5431)
b2 = tf.alphas_like(a, 5)
b3 = tf.alphas_like(a, -5)
b4 = tf.alphas_like(a, True)

with tf.Session() as sess:
    _b1, _b2, _b3, _b4 = sess.run([b1, b2, b3, b4])
    
print(""b1:"", _b1)
print(""b2:"", _b2)
print(""b3:"", _b3)
print(""b4:"", _b4)

############### OUTPUTS ###############

>>> b1: [
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
  [
    [ 0.5431  0.5431  0.5431]
    [ 0.5431  0.5431  0.5431]
  ]
]

>>> b2: [
  [
    [5 5 5]
    [5 5 5]
  ]
  [
    [5 5 5]
    [5 5 5]
  ]
]

>>> b3: [
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
  [
    [-5 -5 -5]
    [-5 -5 -5]
  ]
]

>>> b4: [
  [
    [ True  True  True]
    [ True  True  True]
  ]
  [
    [ True  True  True]
    [ True  True  True]
  ]
]
```

### How can you help ?

Before submitting a PR, I would like to know a few things:
  - Is it something that would be any kind of interest and worth a PR?
  - Are the names I have chosen (tf.alphas and tf.alphas_like) okay with everyone ?
  - In my PR, should I delete the implementation of tf.zeros_likes and tf.ones_likes and replace them as an alias of my new function which basically does the same job, just in a more flexible way ?

Thanks for your time and attention,

Best Regards,

Jonathan"
16126,Error 	LNK1181	cannot open input file 'To.obj' ConsoleApplication1,"The tensorflow Library built in 
windows 10; Visual Studio 2015(Update 3);Python 3.5, 
with CMake, 
now I try to run an example and I get this error, 
I don't find any To.obj in my Tensorflow build folder.
this is my code (with name: ConsoleApplication1): 
`// matmul.cpp
#define _ITERATOR_DEBUG_LEVEL 0  

#include <vector>
#include <eigen/Dense>

#include ""matmul.h""
#include ""tensorflow/core/public/session.h""
#include ""tensorflow/cc/ops/standard_ops.h""


using namespace tensorflow;

// Build a computation graph that takes a tensor of shape [?, 2] and
// multiplies it by a hard-coded matrix.
GraphDef CreateGraphDef()
{
	Scope root = Scope::NewRootScope();

	auto X = ops::Placeholder(root.WithOpName(""x""), DT_FLOAT,
		ops::Placeholder::Shape({ -1, 2 }));
	auto A = ops::Const(root, { { 3.f, 2.f },{ -1.f, 0.f } });

	auto Y = ops::MatMul(root.WithOpName(""y""), A, X,
		ops::MatMul::TransposeB(true));

	GraphDef def;
	TF_CHECK_OK(root.ToGraphDef(&def));

	return def;
}

int main()
{
	GraphDef graph_def = CreateGraphDef();

	// Start up the session
	SessionOptions options;
	std::unique_ptr<Session> session(NewSession(options));
	TF_CHECK_OK(session->Create(graph_def));

	// Define some data.  This needs to be converted to an Eigen Tensor to be
	// fed into the placeholder.  Note that this will be broken up into two
	// separate vectors of length 2: [1, 2] and [3, 4], which will separately
	// be multiplied by the matrix.
	std::vector<float> data = { 1, 2, 3, 4 };
	auto mapped_X_ = Eigen::TensorMap<Eigen::Tensor<float, 2, Eigen::RowMajor>>
		(&data[0], 2, 2);
	auto eigen_X_ = Eigen::Tensor<float, 2, Eigen::RowMajor>(mapped_X_);

	Tensor X_(DT_FLOAT, TensorShape({ 2, 2 }));
	X_.tensor<float, 2>() = eigen_X_;

	std::vector<Tensor> outputs;
	TF_CHECK_OK(session->Run({ { ""x"", X_ } }, { ""y"" }, {}, &outputs));

	// Get the result and print it out
	Tensor Y_ = outputs[0];
	std::cout << Y_.tensor<float, 2>() << std::endl;

	session->Close();
}`"
16124,How can I batch images of arbitrary sizes in tensorflow?,I want to realize arbitrary inputs that I can batch them in one batch.
16116,Implement Scale Operate?? ,"------------------------

### System information
- **Have I written custom code: N/A**:
- **OS Platform and Distribution: openSUSE Leap 42.3**:
- **TensorFlow installed from: binary**:
- **TensorFlow version: tensorflow_gpu-1.4.0**:
- **Python version: 2.7.13**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version: 8.0/6.0**:
- **GPU model and memory: 11GB**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In Caffe, Scale layer could do this:
top = alpha∗bottom+beta
where bottom is the input, top is the output, alpha and beta are the learnable params.
In Tensorflow, what operate could implement above layer?
Thanks, Help a lot."
16115,Gradients w.r.t. eigenvalues/eigenvectors ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: YES
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.1
- **TensorFlow installed from (source or binary)**: pip install tensorflow
- **TensorFlow version (use command below)**: v1.2.0-5-g435cdfc 1.2.1
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: see code

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

Hi,

I'm having issues with evaluating the gradients of eigenvalues/eigenvectors with respect to the underlying matrix. Tensorflow evaluates the gradients, but these are not correct when compared to the true gradients.

We are using tf.self_adjoint_eig to evaluate the spectral decomposition for the tensorflow variable input. We have initialised this with a symmetric matrix to satisfy the self adjoint operator property. We wish to take the derivative of individual eigenvalues or eigenvector with respect to the original matrix (note for eigenvectors we take one element from one eigenvector, e.g. element 2 in eigenvector 1 to avoid the issue of gradient aggregation for now).

The methodology for evaluating true gradients of eigenvalues and vectors of a symmetric real matrix can be found in the paper ""On differentiating Eigenvalues and Eigenvectors"" by Magnus (1985), Theorem 1 eqn (6) + (7). We evaluated using our input matrix the gradients under this paper and compared it to tensorflow evaluated gradients and the gradients from finite difference approximation. For the eigenvalues, the gradients are similar (identical on diagonal entries, off by a factor of 2 on off-diagonal elements), however the eigenvectors are off by quite a bit outside the diagonal entries. To start, define a matrix A as (excuse the matrix output formatting from Python)

A=[[-3 -2  4]
      [-2  1  1]
      [ 4  1  5]]

using np.linalg.eig and tf.self_adjoint_eig on A (I simply initialised a variable with A and computed the gradient for the tf implementation)

Tensorflow eigenvalues: [-5.43071561  1.76904987  6.66166575]
Python eigenvalues: [-5.43071561  6.66166575  1.76904987]

I now wish to evaluate the gradient of eigenvalue 1 (-5.43071561) w.r.t. A. 

Analytical gradient:
[[ 0.75896178  0.28555906 -0.31842553]
 [ 0.28555906  0.10744148 -0.11980748]
 [-0.31842553 -0.11980748  0.13359674]]

Tensorflow gradient:
[[[ 0.75896178,  0.        ,  0.        ],
   [ 0.57111812,  0.10744148,  0.        ],
  [-0.63685107, -0.23961495,  0.13359674]]]

The diagonal entries are the same but the off-diagonal entries are clearly off by a factor of 2.  Now we try and evaluate gradients for the eigenvectors.

Tensorflow eigenvectors:
[[-0.87118413 -0.31452619 -0.37697678]
 [-0.32778267  0.94426587 -0.0303395 ]
 [ 0.36550888  0.09713517 -0.92572567]]
Python eigenvectors:
[[ 0.87118413  0.37697678 -0.31452619]
 [ 0.32778267  0.0303395   0.94426587]
 [-0.36550888  0.92572567  0.09713517]]

We try and find the gradient of the eigenvector 1 element 2 (+/-0.32778267). We expect the tensorflow gradient to the equivalent to the analytical gradient (after taking into account the sign difference).

Analytical gradient:
[[ 0.03511309 -0.10795607 -0.01312188]
 [ 0.01321128 -0.04061843 -0.0049371 ]
 [-0.01473184  0.04529341  0.00550534]]

Tensorflow gradient:
[[[-0.03511309,  0.        ,  0.        ],
   [ 0.09474478,  0.04061843,  0.        ],
   [ 0.02785372, -0.04035631, -0.00550534]]]

Besides the entries being different between the tensorflow evaluated and real gradients, one issue is that tensorflow only returns the lower triangle of the gradient. Despite A being symmetric, the gradient matrix is not symmetric (as per the true gradient) and so the upper right triangle shouldn't be zeros. I believe this to be a bug, however I understand there may be reasons for the differences. Thank you for reading!

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

Here is a script that reproduces the above.

[eigen_decomp_examplev2.py.zip](https://github.com/tensorflow/tensorflow/files/1546631/eigen_decomp_examplev2.py.zip)
"
16112,"Define gradient for tf.linspace and make it work with higher-rank tensors, not just scalars.","I needed to feed-forward through `tf.linspace`, but it seems that it does not have gradient defined.

I don't know how to define gradient for existing op, but I've implemented my own version of `tf.linspace` in python using tensorflow with so that automatically defined gradient.
```python
            def linspace(start, end, num):
                range = end - start
                num_steps = num - 1
                h = range / num_steps

                def cond(ta, x, k):
                    return tf.less(x, end)

                def body(ta, x, k):
                    x = x + h
                    ta = ta.write(k, x)
                    return ta, x, k+1

                k = tf.constant(0)
                ta = tf.TensorArray(dtype=tf.float32, size=num)
                ta = ta.write(k, start)
                ta = tf.while_loop(cond, body, [ta, start, k+1])[0]
                return ta.stack()
```

One more feature I can suggest adding is improve to `linspace` so it would work with higher-rank tensors.
The function I wrote is also very short and simple, but is interesting as a generalization of `linspace`.
```python
            def linspace_vectors(start, end, num):
                cnct = tf.concat([start, end], 1)
                seq = tf.map_fn(
                    lambda row_i: linspace(row_i[0], row_i[1], num), cnct)
                splits = tf.split(seq, num, 1)
                return tf.stack(splits)
```
The function is taken from my project and returns 3-rank tensor with shapes [num, r, 1]. Inputs are 2-rank tensors with shapes [r, 1].
So that I linspaced vectors-columns, not just scalars as `tf.linspace` do.
What do you think? Is it worth adding?

Have I written custom code: Yes
OS Platform and Distribution: Ubuntu 16.04
TensorFlow installed from: pip nightly build
TensorFlow version: 1.4.1
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A"
16111,Implement Scale Operate?? ,"------------------------

### System information
- **Have I written custom code: No**:
- **OS Platform and Distribution: openSUSE Leap 42.3**:
- **TensorFlow installed from: binary**:
- **TensorFlow version: tensorflow_gpu-1.4.0**:
- **Python version: 2.7.13**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version: 8.0/6.0**:
- **GPU model and memory: 11GB**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

In Caffe, Scale layer could do this:
top = alpha∗bottom+beta
where bottom is the input, top is the output, alpha and beta are the learnable params.
In Tensorflow, what operate could implement above layer?
Thanks, Help a lot."
16110,"Using tf1.4 to restore a model from tf0.8, a NotFoundError appeared","When using tf1.4 to restore a model from tf0.8, I met a NotFoundError, the related code as flow:

ema = tf.train.ExponentialMovingAverage(1.0)
saver = tf.train.Saver(ema.variables_to_restore())
model_checkpoint_path='./model_check_point/model-20160506.ckpt-500000'
saver.restore(sess, model_checkpoint_path)

The error as flow:
NotFoundError (see above for traceback): Tensor name ""incept3a/in3_conv5x5_8/batch_norm/moments/Squeeze/ExponentialMovingAverage"" not found in checkpoint files ./model_check_point/model-20160506.ckpt-500000
	 [[Node: save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_46/tensor_names, save/RestoreV2_46/shape_and_slices)]]
	 [[Node: save/RestoreV2_315/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_702_save/RestoreV2_315"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

How can I solve this problem?

This is a project I download from the github, the code is not writen by me. I run it on ubuntu14.04,
with TF1.4.1 installed from source code, bazel0.8 , CUDA8.0, cudnn6.0, GTX1060 6G memory.

"
16109,R1.4 restore a model from r0.8 encounter NotFoundError (see above for traceback): Tensor name ,"When using tf1.4 to restore a model from tf0.8, I met a NotFoundError, the related code as flow:

ema = tf.train.ExponentialMovingAverage(1.0)
saver = tf.train.Saver(ema.variables_to_restore())
model_checkpoint_path='./model_check_point/model-20160506.ckpt-500000'
saver.restore(sess, model_checkpoint_path)

The error as flow:
NotFoundError (see above for traceback): Tensor name ""incept3a/in3_conv5x5_8/batch_norm/moments/Squeeze/ExponentialMovingAverage"" not found in checkpoint files ./model_check_point/model-20160506.ckpt-500000
	 [[Node: save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save/Const_0_0, save/RestoreV2_46/tensor_names, save/RestoreV2_46/shape_and_slices)]]
	 [[Node: save/RestoreV2_315/_35 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device_incarnation=1, tensor_name=""edge_702_save/RestoreV2_315"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]

How can I solve this problem?"
16108,No tf.metrics.true_negatives,"**TensorFlow version**: 1.4.1

Is there any particular reason for why there is no `tf.metrics.true_negatives` method? I know it's simple to calculate from other confusion metrics that are available, but I was wondering why the developers chose to let this one method out."
16106,Eager: Invalid placement of vars/consts depending on their types and not the tf.device,"Hi,
I'm currently testing eager execution on TF 1.5.0-rc1 (built it with XLA and CUDA enabled) and observe strange behavior: variables and constants get created either on GPU or CPU depending on their types, and not `with tf.device(...):` block. Moreover, on creation of int32 variable it fails completely. For example, when I run the following code

```
import tensorflow as tf
import tensorflow.contrib.eager as tfe

tfe.enable_eager_execution()

print('TensorFlow version:', tf.__version__)

with tf.device('/gpu:0'):
    A = tf.constant([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Const A is placed on:', A.device)

    B = tf.constant([1, 2, 3], dtype=tf.int32)
    print('Const B is placed on:', B.device)

    C = tfe.Variable([1.0, 2.0, 3.0], dtype=tf.float32)
    print('Variable C is placed on:', C.device)

    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
    print('Variable D is placed on:', D.device)
```

I get the following output:

```
TensorFlow version: 1.5.0-rc1
2018-01-14 01:16:06.385929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-14 01:16:06.386198: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 10.91GiB freeMemory: 363.06MiB
2018-01-14 01:16:06.386223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)
Const A is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Const B is placed on: CPU:0
Variable C is placed on: /job:localhost/replica:0/task:0/device:GPU:0
Traceback (most recent call last):
  File ""tf_bug.py"", line 18, in <module>
    D = tfe.Variable([1, 2, 3], dtype=tf.int32)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 277, in __init__
    constraint=constraint)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 422, in _init_from_args
    graph_mode=self._in_graph_mode)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 53, in _eager_safe_variable_handle
    container=container)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/ops/gen_resource_variable_ops.py"", line 396, in var_handle_op
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/home/kpot/pyves/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: No registered 'VarHandleOp' OpKernel for GPU devices compatible with node VarHandleOp = VarHandleOp[container=""eager-execution-2/"", dtype=DT_INT32, shape=[3], shared_name=""11""]()
	 (OpKernel was found, but attributes didn't match)
	.  Registered:  device='GPU'; dtype in [DT_VARIANT]
  device='GPU'; dtype in [DT_COMPLEX128]
  device='GPU'; dtype in [DT_COMPLEX64]
  device='GPU'; dtype in [DT_BOOL]
  device='GPU'; dtype in [DT_DOUBLE]
  device='GPU'; dtype in [DT_FLOAT]
  device='GPU'; dtype in [DT_HALF]
  device='CPU'
  device='XLA_GPU'
  device='XLA_CPU'
 [Op:VarHandleOp] name: Variable/
```

As you can see, the constants and variables get placed either on GPU:0 or CPU:0 despite all of them gathered inside the same `tf.device('/gpu:0')` block."
16104,Feature request: Allow the build to use the system-installed protobuf lib,"### Describe the problem
Currently, it's impossible to use the system-installed protobuf library because the tensorflow build always uses the `protobuf_archive` version. There should be an option to use the one installed in the system.

Background: I package tensorflow for Arch Linux and we run into symbol conflicts if a user wants to use protobuf and tensorflow together in a binary because tensorflow's protobuf symbols conflict with the one installed in the system already.

Original Arch bug report: https://bugs.archlinux.org/task/56943"
16103,No OpKernel was registered to support Op 'AssignVariableOp' with DT_BFLOAT16,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch linux
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.5.0-rc1
- **Python version**: NA (go bindings)
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 7.2.1
- **CUDA/cuDNN version**: na (CPU)
- **GPU model and memory**: na
- **Exact command to reproduce**: See below

### Describe the problem
`AssignVariableOp` does not appear to appear to have a kernel for `DT_BFLOAT16`.


### Source code / logs
```
package main

import (
	tf ""github.com/tensorflow/tensorflow/tensorflow/go""
	""github.com/tensorflow/tensorflow/tensorflow/go/op""
)

func main() {
	s := op.NewScope()
	bfloat := op.Cast(s, op.Const(s, float32(0.1234)), tf.Bfloat16)
	variable := op.VarHandleOp(s, tf.Bfloat16, tf.ScalarShape())
	init := op.AssignVariableOp(s, variable, bfloat)

	graph, err := s.Finalize()
	if err != nil {
		panic(err)
	}
	sess, err := tf.NewSession(graph, nil)
	if err != nil {
		panic(err)
	}
	_, err = sess.Run(nil, nil, []*tf.Operation{init})
	if err != nil {
		panic(err)
	}
}
```
```
go run bfloat_demo.go 
panic: No OpKernel was registered to support Op 'AssignVariableOp' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; dtype in [DT_VARIANT]
  device='CPU'; dtype in [DT_QINT32]
  device='CPU'; dtype in [DT_QUINT8]
  device='CPU'; dtype in [DT_QINT8]
  device='CPU'; dtype in [DT_RESOURCE]
  device='CPU'; dtype in [DT_STRING]
  device='CPU'; dtype in [DT_BOOL]
  device='CPU'; dtype in [DT_COMPLEX128]
  device='CPU'; dtype in [DT_COMPLEX64]
  device='CPU'; dtype in [DT_DOUBLE]
  device='CPU'; dtype in [DT_FLOAT]
  device='CPU'; dtype in [DT_HALF]
  device='CPU'; dtype in [DT_INT8]
  device='CPU'; dtype in [DT_UINT8]
  device='CPU'; dtype in [DT_INT16]
  device='CPU'; dtype in [DT_UINT16]
  device='CPU'; dtype in [DT_INT32]
  device='CPU'; dtype in [DT_INT64]

	 [[Node: AssignVariableOp = AssignVariableOp[dtype=DT_BFLOAT16](VarHandleOp, Cast)]]

goroutine 1 [running]:
main.main()
	/home/isaac/go/src/github.com/is8ac/gotf/bfloat_demo.go:24 +0x250
exit status 2
```"
16102,[Bug]: Unable to update the batch_normalization layer moving_mean/moving_variance of keras ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: Tensorflow 1.4.1
- **Bazel version (if compiling from source)**: N/A
- **Python version**: Python 3.5
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: 1080 Ti
- **Exact command to reproduce**: see below

It is following up this [issue](https://github.com/tensorflow/tensorflow/issues/15367#issuecomment-357088739) about training models by mixing Tensorflow with Keras using this style, exampled [here](https://blog.keras.io/keras-as-a-simplified-interface-to-tensorflow-tutorial.html). Basically, using Inception architectures in keras.applications.InceptionV3 and training using sess.run() with a placeholder K.learning_phase() to indicate the training/testing mode.

_At beginning, I found the evaluation accuracy in testing mode (loaded back from a checkpoint) is different from the results when evaluation on the fly during training. But when I evaluated using training mode, it can give me reasonable but not good results. So I knew the problem happened in batch normalization._

**I tried either using tf.keras and keras independent version (2.1.2). But both not work.**

After some investigations, I found there should an issue of Keras of using batch normalization. I track the moving_mean and moving_variance and found they never update (maintaining the initial values, zero and one). In tf. keras, I notice its batchnorm inherits tf.layers.BatchNormalization (see [here](https://github.com/tensorflow/tensorflow/blob/ac8e67399d75edce6a9f94afaa2adb577035966e/tensorflow/python/keras/_impl/keras/layers/normalization.py#L26)). However, by checking the tutorial of this tf.layers.BatchNormalization,
  _It required to add a **update_ops** in optimizer. But it never mentioned in Keras (which should be 
  clarified)._ 
I did not see Keras using update_ops anywhere. **So i believe if you want to fine-tune a keras predefined model in applications, you never be able to update your moving_mean and moving_variance for your new data**.

**Note that I also tried keras independent version. It never worked. moving_mean and moving_variance are always not changed. I  tracked the value of batch_mean which is used to update moving_mean. batch_mean has values but not on moving_mean.**

In addition, I found moving_mean and moving_variance are in tf.trainable_variables() when using keras but not in tf.keras. I am not sure if this matters.

Here is an example code

```
image = keras.preprocessing.image
def preprocess_input(x):
    # the same as keras.applications.inception_v3
    with tf.name_scope('preprocess_input'):
        x /= 255.
        x -= 0.5
        x *= 2.
        return x

def get_main_network(name, input_tensor, use_weights=False):

    processed_tensor = preprocess_input(input_tensor)

    if name == 'inception':
        base_model = keras.applications.InceptionV3(include_top=True,
                                                    weights='imagenet' if use_weights else None,
                                                    pooling='avg',
                                                    input_tensor=processed_tensor)
    
    model = keras.models.Model(inputs=base_model.input, outputs=base_model.output)
 
    return model

img_shape=[299,299]
img = tf.placeholder(tf.float32, shape=[None]+img_shape+[3])

with tf.name_scope('model'):
    model = get_main_network('inception', input_tensor=img, use_weights=False)
    output = model.output
    logit = tf.cast(tf.argmax(output, axis=1), np.float32)

# define loss
with tf.name_scope('cross_entropy'):
    cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=label, logits=output)  

# define optimizer
with tf.name_scope('learning_rate'):
    global_step = tf.Variable(0, name='global_step', trainable=False)
    learning_rate = tf.train.exponential_decay(opt.learning_rate, global_step,
                                        iter_epoch*opt.lr_decay_epoch, opt.lr_decay, staircase=True)
    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9) #.minimize(cross_entropy_loss, global_step=global_step)

''' **This matters a lot but not working for independent keras version'**''
update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)
with tf.control_dependencies(update_ops):
    train_step = optimizer.minimize(cross_entropy_loss, global_step=global_step)

''' Initialization '''
init_op = tf.variables_initializer([]) # a fake one, variables already initialized in keras
sess.run(init_op)

img_path = 'elephant.jpg'
imgs= image.load_img(img_path, target_size=img_shape)
x = image.img_to_array(imgs)
x = np.expand_dims(x, axis=0)
saver = tf.train.Saver(max_to_keep=20) # must be added in the end
with sess.as_default():
    
    feed_dict = {   
                        img: x_batch,
                        label: y_batch,
                        K.learning_phase(): True
                    }
        _, loss = sess.run([train_step, 
                                    cross_entropy_loss, 
                                    ], feed_dict=feed_dict)

```
@tensorflowbutler responsed"
16100,Exception when not providing optional parameter frequency_skip in TimeFreqLSTMCell,"### System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution: 
- TensorFlow installed from: `pip3 install --user tensorflow-gpu`
- TensorFlow version: 1.4.1
- Python version: 3.5.2
- CUDA: 8.0
- GPU: NVidia Titan X

### Describe the problem

Using a `TimeFreqLSTMCell` in a `dynamic_rnn` or `static_rcnn` without providing the optional parameter `frequency_skip` results in an exception:

```
TypeError: unsupported operand type(s) for /: 'int' and 'NoneType'
```

The line which throws this exception is https://github.com/tensorflow/tensorflow/blob/8b78c23c161c9d0bec462d5f4c73f0fca413bc8b/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L474-L475
`frequency_skip` has it's default value `None` here.

Maybe the default should be changed to `1`?

### Source code / logs

Sadly I am not allowed to share my full source code. However, this is how I create the RNN layers:

```
lstmcell = tf.contrib.rnn.TimeFreqLSTMCell(lstm_input.shape.as_list()[2], forget_bias = self.lstm_forget_bias, feature_size = lstm_input_rev.shape.as_list()[2])
                
stacked_lstm = tf.nn.rnn_cell.MultiRNNCell([lstmcell] * self.layers_lstm)
                
lstm_output, lstm_state = tf.nn.dynamic_rnn(stacked_lstm, lstm_input_rev, dtype=""float32"", time_major=True)
```"
16098,Question about r1.5's default ptxas GPU back-end,"From TF r1.5's release notes, it is said that ""GPU back-end now uses ptxas to compile generated PTX.""  I have searched through the entire git repo of r1.5 code base with only finding that ptxas will be invoked in XLA flow.
Since we are working on some XLA and GPU optimization stuffs, I just want to make sure the meaning of this sentence since I am a little bit puzzle about it and want to ensure our development flow will align with the community major direction.

Thanks"
16097,terminate called after throwing an instance of 'std::bad_alloc',"### System information
- **Have I written custom code**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7
- **TensorFlow installed from**: source
- **TensorFlow version**: 1.3
- **Python version**: 2.7.13
- **Bazel version**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 8.0.61 / 6.0.21
- **GPU model and memory**: Quadro P5000 16GB
- **Exact command to reproduce**:

### Describe the problem
When I try to alloc a tf.constant variable as a [90000, 4096] matrix with float32 type. It seems that the graphics have enough memory. But I still got an memory error.

### Source code / logs
The error information is as follows:

```
name: Quadro P5000
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:03:00.0
Total memory: 15.89GiB
Free memory: 15.66GiB
2018-01-13 19:18:10.558228: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x39a1500 exists before initializing the StreamExecutor. We haven't verified StreamExecutor works with that.
2018-01-13 19:18:10.559218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:
name: Quadro P5000
major: 6 minor: 1 memoryClockRate (GHz) 1.7335
pciBusID 0000:82:00.0
Total memory: 15.89GiB
Free memory: 15.43GiB
2018-01-13 19:18:10.559297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 0 and 1
2018-01-13 19:18:10.559306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:847] Peer access not supported between device ordinals 1 and 0
2018-01-13 19:18:10.559317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1
2018-01-13 19:18:10.559321: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y N
2018-01-13 19:18:10.559324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   N Y
2018-01-13 19:18:10.559333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Quadro P5000, pci bus id: 0000:03:00.0)
2018-01-13 19:18:10.559338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -> (device: 1, name: Quadro P5000, pci bus id: 0000:82:00.0)
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted
```

The test code is as follows:

```
import numpy as np
import tensorflow as tf

a = tf.constant(np.ones([90000, 4096], dtype = np.float32), dtype = tf.float32)

sess = tf.Session()
sess.run(a)
```"
16095,Implement Scale Operate??,"In Caffe, Scale layer could do this:
top=alpha∗bottom+beta
where bottom is the input, top is the output, alpha and beta are the learnable params.
In Tensorflow, what operate could implement above layer?
Thanks, Help a lot"
16094,Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss'),"Have I written custom code: yes
OS: Windows 8.1
Tensorflow installed from: conda
Tensorflow version: 1.4


I've successfully converted a Tensor into a SparseTensor with this code:

```
def dense_to_sparse(dense_tensor, out_type):
    indices = tf.where(tf.not_equal(dense_tensor, tf.constant(0, dense_tensor.dtype)
    values = tf.gather_nd(dense_tensor, indices)
    shape = tf.shape(dense_tensor, out_type=out_type)
    return tf.SparseTensor(indices, values, shape)
```

I want to try out using a `SparseTensor` converted from a dense one: 

```
input_layer = tf.placeholder(tf.float32, [None, 1596, 48])
dense_labels = tf.placeholder(tf.int32)
sparse_from_dense = dense_to_sparse(dense_lables, out_type=tf.int64)
cell_fw = grid_rnn.Grid2LSTMCell(num_units=128)
cell_bw = grid_rnn.Grid2LSTMCell(num_units=128)
bidirectional_grid_rnn = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, input_layer, dtype=tf.float32)
outputs = tf.reshape(bidirectional_grid_rnn[0], [-1, 256])

W = tf.Variable(tf.truncated_normal([256, 80], stddev=0.1, dtype=tf.float32), name='W')
b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[80], name='b'))

logits = tf.matmul(outputs, W) + b
logits = tf.reshape(logits, [tf.shape(input_layer)[0], -1, 80])
logits = tf.transpose(logits, (1, 0, 2))

loss = tf.nn.ctc_loss(inputs=logits, labels=sparse, sequence_length=320)
```

Unfortunately, when I do this, I encounter this error:

`Shape must be rank 1 but is rank 0 for 'CTCLoss' (op: 'CTCLoss') with input shapes: [?,?,80], [?,1], [?], [].`

"
16093,when will we have multi gpu support under eager mode? Pytorch has it.,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16091,Failed to convert tf pb file (MTCNN model) to tflite format,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.2 LTS
- **TensorFlow installed from (source or binary)**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**:gcc (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**: bazel run --config=opt --copt=-msse4.1 --copt=-msse4.2  //tensorflow/contrib/lite/toco:toco --  --input_file=/home/xxx/facenet/ks/onet.pb   --output_file=/home/xxx/facenet/ks/onet.tflite  --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --inference_type=FLOAT   --input_shape=1,48,48,3   --input_array=onet/input --output_array=onet/prob1

### Describe the problem
I failed to covert my pb file to tflite format because some operations were not supported. Is there any workaround for the issue? 

### Source code / logs
2018-01-13 09:47:41.439557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.439661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.439687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.439736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.439767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.439786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.439824: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.439864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.439884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.439940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.439970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.439987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440055: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440073: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440259: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440392: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440423: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440596: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440689: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440828: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440886: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.440918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.440937: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.440987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441018: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441090: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441118: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441196: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441254: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Exp
2018-01-13 09:47:41.441295: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441324: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441340: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441372: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441419: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: RandomUniform
2018-01-13 09:47:41.441586: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: VariableV2
2018-01-13 09:47:41.441604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441701: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441961: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.441990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.442071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.442095: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.442120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.442142: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443280: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443433: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443475: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.443517: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1126] Converting unsupported operation: Assign
2018-01-13 09:47:41.446177: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 231 operators, 319 arrays (0 quantized)
2018-01-13 09:47:41.449197: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 68 operators, 76 arrays (0 quantized)
2018-01-13 09:47:41.449990: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 68 operators, 77 arrays (0 quantized)
2018-01-13 09:47:41.450763: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 68 operators, 77 arrays (0 quantized)
2018-01-13 09:47:41.451432: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.

"
16087,  tf.estimator.train_and_evaluate list  of eval_spec,"Sometimes could be useful to evaluate on different clusters of the evaluation set with distinct metrics, summaries etc.
Do you plan that this interface will accept a list of `eval_spec` in the future?"
16083,Tensorflow and Bazle build in Docker Container on Windows,"Hello, i'm trying to build the following Dockerfile:

```
FROM ubuntu:17.10

ADD https://bazel.build/bazel-release.pub.gpg /bazel-release.pub.gpg
RUN apt-key add /bazel-release.pub.gpg && rm /bazel-release.pub.gpg
RUN echo ""deb [arch=amd64] http://storage.googleapis.com/bazel-apt stable jdk1.8"" | tee /etc/apt/sources.list.d/bazel.list

RUN apt-get update && apt-get install -y git build-essential bazel openjdk-8-jdk python3-dev python3-pip python3-numpy python3-wheel && ln --symbolic python3 /usr/bin/python

RUN git clone --recursive --branch r1.4 --depth 1 --shallow-submodules https://github.com/tensorflow/tensorflow
RUN cd /tensorflow && (echo ""\n\ny\nn\nn\nn\ny\nn\nn\nn\nn\nn\n\n"" | ./configure)
RUN cd /tensorflow && bazel build --config=opt //tensorflow/compiler/aot:tfcompile
```




Im getting the following Error on the last RUN operation:

```
Step 8/8 : RUN cd /tensorflow && bazel build --config=opt //tensorflow/compiler/aot:tfcompile
 ---> Running in ef36db7c22d1
..................
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
Analyzing: target //tensorflow/compiler/aot:tfcompile (1 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (4 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (5 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (6 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (19 packages loaded)
Analyzing: target //tensorflow/compiler/aot:tfcompile (25 packages loaded)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD"", line 27
                cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
        File ""/root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
                sycl_library_path
name 'sycl_library_path' is not defined
Analyzing: target //tensorflow/compiler/aot:tfcompile (35 packages loaded)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:118:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:120:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:122:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:124:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:126:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?)
Analyzing: target //tensorflow/compiler/aot:tfcompile (45 packages loaded)
ERROR: /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /tensorflow/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/compiler/aot:tfcompile' failed; build aborted: Loading failed
INFO: Elapsed time: 18.318s
FAILED: Build did NOT complete successfully (46 packages loaded)
```

My Docker Version:
```
Client:
 Version:       17.12.0-ce
 API version:   1.35
 Go version:    go1.9.2
 Git commit:    c97c6d6
 Built: Wed Dec 27 20:05:22 2017
 OS/Arch:       windows/amd64

Server:
 Engine:
  Version:      17.12.0-ce
  API version:  1.35 (minimum version 1.12)
  Go version:   go1.9.2
  Git commit:   c97c6d6
  Built:        Wed Dec 27 20:12:29 2017
  OS/Arch:      linux/amd64
  Experimental: true
```


Any ideas why this doesn't work?
The Dockerfile works fine on linux."
16082,Connect Apache Beam/Spark to TensorFlow (MonitoredTrainingSession) in a streaming manner?,"### Describe the problem
I have a lengthy question on [SO](https://stackoverflow.com/questions/47986410/optimal-data-streaming-and-processing-solution-for-enormous-datasets-into-tf-dat) about this. But in short, is there a way (or a best practice) to pipe big training datasets directly into a distributed setting (e.g. GKE),  especially if they are subjected to a heavy preprocessing? 
I'm basically reaching the limit of what can be sanely stored in TFRecords (they are verbose and heavy).
The closest issue was this one (https://github.com/tensorflow/tensorflow/issues/12903) and this guide (https://github.com/GoogleCloudPlatform/dataflow-prediction-example) but I do not see a healthy way to implement it (last one with a `@singleton` looks like a hack and not usable with the `tf.Dataset` or `MonitoredTrainingSession`).

I believe this is a useful issue/feature request for a decent amount of Tensorflow users. "
16076,Using self.test_session() in setUp() in tf.test.TestCase runs setUp but not tearDown,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, see below
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: From binary with pip
- **TensorFlow version (use command below)**: v1.4.1
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 8.0
- **GPU model and memory**: 1080Ti
- **Exact command to reproduce**: python mwe.py

### Describe the problem
https://github.com/tensorflow/tensorflow/blob/1cc4ec4c5a08cadae87fa02222c5b5c3e81dedbb/tensorflow/python/framework/test_util.py#L874

If you use `tf.test.TestCase.test_session()` in `setUp()`, there will be one ""skipped"" test for which `setUp()` is run, but not `tearDown()`. I believe this has to do with `tf.TestCase.test_session()` trying to take care of automatic test discovery on the line above, but I'm not sure.

### Source code / logs

In the following MWE (mwe.py), after running this test there will be a tmp.txt file left in the current directory.

MWE:

    import tensorflow as tf

    import os
    import unittest

    class FooTest(tf.test.TestCase):
        def setUp(self):
            with open(""tmp.txt"", ""w"") as f:
                f.write(""Hello"")

            with self.test_session() as sess:
                pass

        def tearDown(self):
            os.unlink(""tmp.txt"")

        def testExample(self):
            self.assertEqual(1, 1)

    if __name__==""__main__"":
        unittest.main()
"
16073,Feature request: (optionally) return all audio streams in tf.contrib.ffmpeg.decode_audio,"I'm trying to read [musdb18](https://sigsep.github.io/musdb.html) with `tf.data` and it comes in the form of mp4 files with multiple audio streams so `ffmpeg -map` is needed.

`tf.contrib.ffmpeg.decode_audio` cannot be configured to choose the audio stream. I wonder if we could have a `tf.contrib.ffmpeg.decode_audios` that returns every audio stream in the file, or if we could have a new argument in `tf.contrib.ffmpeg.decode_audio` for choosing streams.

Being able to choose the audio stream is also important for getting the right language in a movie file's audio, and similarly `tf.contrib.ffmpeg.decode_video` could need the same extension (though multiple video streams is not as common AFAIK)."
16072,Dynamic Bi-directional RNN vs Dynamic RNN.- Not working as expected.,"I am trying to use Bidirectional RNN and pass the output through a CNN for text classification. However, I am getting all sorts of shape errors with bidirectional RNN. Although, If I use two dynamic rnn with reverse op in the second layer, it appears to work fine:

Here is bidirectional RNN code that DOES NOT work for me:

```
    # Bidirectional LSTM layer
    with tf.name_scope(""bidirectional-lstm""):
        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)

        self.lstm_outputs, _ = tf.nn.bidirectional_dynamic_rnn(
            lstm_fw_cell, 
            lstm_bw_cell, 
            self.embedded_chars, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)
        self.lstm_outputs = tf.concat(self.lstm_outputs, axis=2)
```


Here is the two layer dynamic rnn that DOES work for me:


```
  # Bidirectional LSTM layer
    with tf.name_scope(""bidirectional-lstm""):
        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(hidden_size, forget_bias=1.0)
    with tf.variable_scope(""lstm-output-fw""):
        self.lstm_outputs_fw, _ = tf.nn.dynamic_rnn(
            lstm_fw_cell, 
            self.embedded_chars, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)

    with tf.variable_scope(""lstm-output-bw""):
        self.embedded_chars_rev = array_ops.reverse_sequence(self.embedded_chars, seq_lengths=self.seqlen, seq_dim=1)
        tmp, _ = tf.nn.dynamic_rnn(
            lstm_bw_cell, 
            self.embedded_chars_rev, 
            sequence_length=self.seqlen, 
            dtype=tf.float32)
        self.lstm_outputs_bw = array_ops.reverse_sequence(tmp, seq_lengths=self.seqlen, seq_dim=1)

    Concatenate outputs
    self.lstm_outputs = tf.add(self.lstm_outputs_fw, self.lstm_outputs_bw, name=""lstm_outputs"")
```

I am passing the output of this to CNN and error occurs when computing the

Here is the rest of the code:

# Convolution + maxpool layer for each filter size
        pooled_outputs = []
        for i, filter_size in enumerate(filter_sizes):
            with tf.name_scope(""conv-maxpool-%s"" % filter_size):
                # Convolution Layer
                filter_shape = [filter_size, hidden_size, 1, num_filters]
                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=""W"")
                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=""b"")

                conv = tf.nn.conv2d(
                    self.lstm_outputs_expanded, 
                    W,
                    strides=[1, 1, 1, 1], 
                    padding=""VALID"",
                    name=""conv"")

                # Apply nonlinearity
                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=""relu"")

                # Maxpooling over the outputs
                pooled = tf.nn.max_pool(
                    h, 
                    ksize=[1, sequence_length - filter_size + 1, 1, 1],
                    strides=[1, 1, 1, 1], 
                    padding='VALID',
                    name=""pool"")
                pooled_outputs.append(pooled)

        # Combine all the pooled features
        num_filters_total = num_filters * len(filter_sizes)
        self.h_pool = tf.concat(axis=3, values=pooled_outputs)
        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])


        # Dropout layer
        with tf.name_scope(""dropout""):
            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)


```
        # Final (unnormalized) scores and predictions
        with tf.name_scope(""output""):
            # Standard output weights initialization
            W = tf.get_variable(
                ""W"", 
                shape=[num_filters_total, num_classes], 
                initializer=tf.contrib.layers.xavier_initializer())
            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=""b"")

            # # Initialized output weights to 0.0, might improve accuracy
            # W = tf.Variable(tf.constant(0.0, shape=[num_filters_total, num_classes]), name=""W"")
            # b = tf.Variable(tf.constant(0.0, shape=[num_classes]), name=""b"")

            l2_loss += tf.nn.l2_loss(W)
            l2_loss += tf.nn.l2_loss(b)

            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=""scores"")

            self.predictions = tf.argmax(self.scores, 1, name=""predictions"")

        # Calculate mean cross-entropy loss
        with tf.name_scope(""loss""):
            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss

        # Accuracy
        with tf.name_scope(""accuracy""):
            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))
            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, ""float""), name=""accuracy"")
```


here are the errors I am getting.

```
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_upgraded.py"", line 209, in <module>
    train_step(x_batch, seqlen_batch, y_batch)
  File ""train_upgraded.py"", line 177, in train_step
    feed_dict)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]

Caused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:
  File ""train_upgraded.py"", line 87, in <module>
    l2_reg_lambda=FLAGS.l2_reg_lambda)
  File ""/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py"", line 138, in __init__
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]
During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""train_upgraded.py"", line 209, in <module>
    train_step(x_batch, seqlen_batch, y_batch)
  File ""train_upgraded.py"", line 177, in train_step
    feed_dict)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]

Caused by op 'loss/SoftmaxCrossEntropyWithLogits', defined at:
  File ""train_upgraded.py"", line 87, in <module>
    l2_reg_lambda=FLAGS.l2_reg_lambda)
  File ""/media/hemant/MVV/MyValueVest-local/learning/Initial Embeddings/STEP 2 lstm-context-embeddings-master/model_upgraded.py"", line 138, in __init__
    losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 1783, in softmax_cross_entropy_with_logits
    precise_logits, labels, name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 4364, in _softmax_cross_entropy_with_logits
    name=name)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/hemant/anaconda3/envs/tf14/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

InvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[7550,2] labels_size=[50,2]
         [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](loss/Reshape, loss/Reshape_1)]]
```"
16069,Key generator/encoder_8/conv/filter not found in checkpoint,"I'm using python 3.6.3 win 10 64bit and tensorflow 1.2.1 and now I'm working on https://github.com/datitran/face2face-demo project and **part 4.export model** I'm taking this error:NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint 

how can I solve this problem ?

what I run
`C:\Users\hajum>python C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py --model-input C:\Users\hajum\Desktop\face2face-model --model-output C:\Users\hajum\Desktop\face2face-reduced-model`

same folder names with project but I have my own models

what it shows
```
2018-01-12 13:24:05.337267: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.337407: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338476: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.338779: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339070: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339369: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339659: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.339962: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-01-12 13:24:05.779044: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/conv/filter not found in checkpoint
2018-01-12 13:24:05.779482: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/deconv/filter not found in checkpoint
2018-01-12 13:24:05.779562: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_1/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780339: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/deconv/filter not found in checkpoint
2018-01-12 13:24:05.780354: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.781063: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781066: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.781015: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.784971: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.785849: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.785928: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_4/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787052: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.787346: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/deconv/filter not found in checkpoint
2018-01-12 13:24:05.787687: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.791739: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.793255: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.793508: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_6/deconv/filter not found in checkpoint
2018-01-12 13:24:05.794303: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.795123: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.795823: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.796067: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_7/deconv/filter not found in checkpoint
2018-01-12 13:24:05.797352: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.798112: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/conv/filter not found in checkpoint
2018-01-12 13:24:05.800556: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_1/conv/filter not found in checkpoint
2018-01-12 13:24:05.801703: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.801868: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/decoder_8/deconv/filter not found in checkpoint
2018-01-12 13:24:05.801974: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.801977: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_2/conv/filter not found in checkpoint
2018-01-12 13:24:05.804154: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.805983: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.806160: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.807834: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.808628: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_3/conv/filter not found in checkpoint
2018-01-12 13:24:05.808808: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.809721: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.809836: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_4/conv/filter not found in checkpoint
2018-01-12 13:24:05.810842: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.811998: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_5/conv/filter not found in checkpoint
2018-01-12 13:24:05.812062: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_7/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812846: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/offset not found in checkpoint
2018-01-12 13:24:05.812889: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/batchnorm/scale not found in checkpoint
2018-01-12 13:24:05.813195: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\framework\op_kernel.cc:1158] Not found: Key generator/encoder_6/conv/filter not found in checkpoint
Traceback (most recent call last):
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1139, in _do_call
    return fn(*args)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1121, in _run_fn
    status, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\contextlib.py"", line 88, in __exit__
    next(self.gen)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py"", line 466, in raise_exception_on_not_ok_status
    pywrap_tensorflow.TF_GetCode(status))
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 215, in <module>
    saver.restore(sess, checkpoint)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1548, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 789, in run
    run_metadata_ptr)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 997, in _run
    feed_dict_string, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1132, in _do_run
    target_list, options, run_metadata)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.NotFoundError: Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]

Caused by op 'save/RestoreV2_43', defined at:
  File ""C:\Users\hajum\Desktop\face2face-demo-master\reduce_model.py"", line 213, in <module>
    saver = tf.train.Saver()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1139, in __init__
    self.build()
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 1170, in build
    restore_sequentially=self._restore_sequentially)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 691, in build
    restore_sequentially, reshape)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 407, in _AddRestoreOps
    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\training\saver.py"", line 247, in restore_op
    [spec.tensor.dtype])[0])
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\ops\gen_io_ops.py"", line 640, in restore_v2
    dtypes=dtypes, name=name)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 767, in apply_op
    op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 2506, in create_op
    original_op=self._default_original_op, op_def=op_def)
  File ""C:\Users\hajum\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1269, in __init__
    self._traceback = _extract_stack()

NotFoundError (see above for traceback): Key generator/encoder_8/conv/filter not found in checkpoint
         [[Node: save/RestoreV2_43 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/cpu:0""](_arg_save/Const_0_0, save/RestoreV2_43/tensor_names, save/RestoreV2_43/shape_and_slices)]]
```"
16068,Empty tensorflow/python/tools directory after build tf 1.5 from source,"I rebuild Tensorflow-GPU Version 1.5.0rc0 from source with Bazel Version 0.9.0 with CUDA 9 and cuDNN 7 to see changes from my previous version 1.4 also build from source with same prerequesites.

But now the tensorflow/python/tools directory is empty? Has anybody experienced the same?
Or Did these scripts get moved to another directory?"
16063,`tools/ci_builds/pi/build_raspberry_pi.sh`: '__PTHREAD_SPINS' was not declared in this scope,"Hi, 
I'm trying to build TensorFlow for RasperryPi from my computer (Ubuntu 16:04) 
When I'm running `tensorflow/tools/ci_builds/pi/build_raspberry_pi.sh`
from the docker container build from `tensorflow/tools/ci_builds/Dockerfile.pi-python3`, some compilation commands from `bazel build` failed ! 

My goal is to use TensorFlow on my RaspberryPi (with a PiCamera) with the C++ API (compiled with bazel ? or makefile ?) ! I started to run some code on my RPi, but the compilation step is so long (6hours to compile/install TensorFlow from sources with Bazel !). I would like to save time, by compiling my code on my laptop, then sending it to the RPi for execution ! 

Here my commands : 

```bash
# get tensorflow 1.5
git clone ......
cd tensorflow 
git checkout r1.5
cd tensorflow/tools/ci_build

# this docker file was added with `r1.5`!
docker build -t tf_ci_buid/pi-py3 -f Dockerfile.pi-python3 .
cd ../../../

# run the docker image
docker run -it -v ""$PWD"":/workspace -w /workspace tf_ci_buid/pi-py3:latest

# then, from the docker container, run the *.sh code 
# this script contains the `bazel build` command ! 
root@88db37534dff:/workspace# ./tensorflow/tools/ci_build/pi/build_raspberry_pi.sh 
```

But during the execution of `bazel build`, the compilation of code from `external/highwayhash/highwayhash` seems to fail ... (see error message bellow)

Someone has already encountered this issue ? Or it's due to the new version (r1.5) ?

Error message : 
```
ERROR: /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/highwayhash/BUILD:8:1: C++ compilation of rule '@highwayhash//:sip_hash' failed (Exit 1): arm-linux-gnueabihf-gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/arm-linux-gnueabihf-gcc -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -DRASPBERRY_PI -g0 -O2 -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=armv7-a' '-mfpu=neon-vfpv4' '-std=gnu11' '-DS_IREAD=S_IRUSR' '-DS_IWRITE=S_IWUSR' -O3 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_1 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_2 -U__GCC_HAVE_SYNC_COMPARE_AND_SWAP_8 -funsafe-math-optimizations -ftree-vectorize -fomit-frame-pointer '-std=c++11' -isystem /usr/include/arm-linux-gnueabihf -isystem /usr/include/python2.7 -isystem /usr/include/ -MD -MF bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.d '-frandom-seed=bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o' -iquote external/highwayhash -iquote bazel-out/armeabi-opt/genfiles/external/highwayhash -iquote external/bazel_tools -iquote bazel-out/armeabi-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -no-canonical-prefixes -fno-canonical-system-headers -c external/highwayhash/highwayhash/sip_hash.cc -o bazel-out/armeabi-opt/bin/external/highwayhash/_objs/sip_hash/external/highwayhash/highwayhash/sip_hash.o)
cc1plus: warning: command line option '-std=gnu11' is valid for C/ObjC but not for C++
In file included from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/arm-linux-gnueabihf/bits/gthr-default.h:35:0,
                 from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/arm-linux-gnueabihf/bits/gthr.h:148,
                 from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/ext/atomicity.h:35,
                 from /root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/memory:73,
                 from external/highwayhash/highwayhash/state_helpers.h:23,
                 from external/highwayhash/highwayhash/sip_hash.h:25,
                 from external/highwayhash/highwayhash/sip_hash.cc:15:
/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/ext/concurrence.h:122:34: error: '__PTHREAD_SPINS' was not declared in this scope
     __gthread_mutex_t _M_mutex = __GTHREAD_MUTEX_INIT;
                                  ^
/root/.cache/bazel/_bazel_root/eab0d61a99b6696edb3d2aff87b585e8/external/arm_compiler/bin/../lib/gcc/arm-linux-gnueabihf/4.9.3/../../../../arm-linux-gnueabihf/include/c++/4.9.3/ext/concurrence.h:177:44: error: '__PTHREAD_SPINS' was not declared in this scope
     __gthread_recursive_mutex_t _M_mutex = __GTHREAD_RECURSIVE_MUTEX_INIT;
                                            ^
INFO: Elapsed time: 275.112s, Critical Path: 9.55s
FAILED: Build did NOT complete successfully
```"
16062,Which CUDA/cuDNN version should I chose for tf-1.50,"![eo hqbj __vt _49a35ztc](https://user-images.githubusercontent.com/30284428/34855697-10dd4c5e-f77c-11e7-8174-3d1925170698.png)
"
16061,OSError: [Errno 12] Cannot allocate memory on deep Q learning model,"I was having fun with the Deep Q learning model, which came from https://github.com/dennybritz/reinforcement-learning/blob/master/DQN. Then after 750 episodes this error popped up.
![1](https://user-images.githubusercontent.com/20869223/34855218-6ae8778a-f779-11e7-9a7f-bbe7670f47b0.png)
![2](https://user-images.githubusercontent.com/20869223/34855219-6b1bc496-f779-11e7-9183-24306a413c01.png)
![3](https://user-images.githubusercontent.com/20869223/34855220-6b4a6436-f779-11e7-84de-e66f157d5a68.png)
![4](https://user-images.githubusercontent.com/20869223/34855221-6b77d754-f779-11e7-90c8-2b700fd10d81.png)
![5](https://user-images.githubusercontent.com/20869223/34855222-6baf6d18-f779-11e7-9963-6a860225a0f2.png)
![6](https://user-images.githubusercontent.com/20869223/34855224-6bdfce68-f779-11e7-8a9a-16ddc709895e.png)
![7](https://user-images.githubusercontent.com/20869223/34855225-6c110014-f779-11e7-8738-c60e50059330.png)
I was running this code on Ubuntu 14.04 with a 8G graphic card(GTX 1070). I have had libav-tools installed before I ran the code.
I will try to reduce the batch size to see whether similar error will pop up again.
Please help me solve this problem."
16058,How to initialize embeddings layer within Estimator API?,"I'm trying to use existing embeddings within tensorflow model, the size of embedding is greater than 2Gb and this makes my original try of doing this unsuccessful:

```
embedding_var = tf.get_variable(
        ""embeddings"", 
        shape=GLOVE_MATRIX.shape, 
        initializer=tf.constant_initializer(np.array(GLOVE_MATRIX))
)
```
Which gave me this error:

` Cannot create a tensor proto whose content is larger than 2GB.`

I'm using AWS SageMaker, which based on the Estimator API, and the actual running of the graph in session happens behind the scene, so I'm not sure how to initialize some placeholders for embedding given that. Would be helpful if someone will be able to share the way how to do such initialization in term of EstimatorAPI.

--------------------------

Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
16054,Lack of Complex64 support for Java API,"This issue is not about a bug, but I will fill in the form anyhow ;-)
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Mac 10.13.2
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
master branch at commit: b86dc365ebbef64daceced37026518696ede5b7b
- **Python version**:
N/A Using Java version ""1.8.0_152""
- **Bazel version (if compiling from source)**:
Build label: 0.8.1-homebrew
Build target: bazel-out/darwin-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Dec 5 19:29:04 2017 (1512502144)
- **GCC/Compiler version (if compiling from source)**:
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX10.13.sdk/usr/include/c++/4.2.1
Apple LLVM version 9.0.0 (clang-900.0.39.2)
Target: x86_64-apple-darwin17.3.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
- **CUDA/cuDNN version**:
N/A not using CUDA
- **GPU model and memory**:
N/A not using GPU
- **Exact command to reproduce**:
There is no bug to reproduce

### Describe the problem
There is not really a problem. I have need to build TensorFlow computations in Java and have support for complex numbers. Because the C API already supports COMPLEX64 tensors, it was a straight forward effort to expose them in the Java API. Along with unit tests, I have also added a Java example based on the Python tutorial that builds an image that displays the Mandelbrot fractal to show that the complex number support works correctly.

### Source code / logs
Here is a very short example of using the API, clearly there are functions in the example that are not available defined in the example, but it gets the point across. This example is basically lifted from a new java example I created called MandelbrotExample.java
```
Tensor<Complex64> resultZ = null;
try(Graph g = new Graph()) {
    //We create a meshgrid based on two numeric ranges, by wrapping the ranges
    //in Tensors and then pulling out sub grids to make complex numbers
    try (Tensor<Float> meshGridT = buildMeshGrid(range1Spec, range2Spec);
         Tensor<Integer> zeroT = Tensors.create(new int[]{0});
         Tensor<Integer> oneT = Tensors.create(new int[]{1});
         Tensor<Complex64> jTensor = Tensors.create(0.0f, 1.0f)) {
    
        Output<Integer> zero = buildConstant(g, ""0"", zeroT);
        Output<Integer> one = buildConstant(g, ""1"", oneT);

        Output<Float> meshGrid = buildConstant(g, ""meshgrid"", meshGridT);

        Output<Complex64> j = buildConstant(g, ""imagUnit"", jTensor);

        //We use GatherNd to pull out the two parts of the original mesh grid
        //the Z complex tensor
        Output<Float> Yfloat = g.opBuilder(""GatherNd"", ""get_y"")
                .addInput(meshGrid)
                .addInput(zero) //use zero
                .build().output(0);
        Output<Float> Xfloat = g.opBuilder(""GatherNd"", ""get_x"")
                .addInput(meshGrid)
                .addInput(one) //use one
                .build().output(0);

        Output<Complex64> Y = g.opBuilder(""Cast"", ""castYtoComplex"")
                .addInput(Yfloat)
                .setAttr(""DstT"", DataType.COMPLEX64)
                .build().output(0);

        Output<Complex64> X = g.opBuilder(""Cast"", ""castXtoComplex"")
                .addInput(Xfloat)
                .setAttr(""DstT"", DataType.COMPLEX64)
                .build().output(0);

        //Z is constructed by X + Yj
        Output<Complex64> mulYj = g.opBuilder(""Mul"", ""mulYj"")
                .addInput(Y)
                .addInput(j)
                .build()
                .output(0);
        Output<Complex64> Z = g.opBuilder(""Add"", ""addZ"")
                .addInput(X)
                .addInput(mulYj)
                .build()
                .output(0);

        try(Session s = new Session(g)){
            resultZ = s.runner().fetch(Z).run().get(0).expect(Complex64.class);
        }
    }
}
```
I would like to get my local branched pushed to GitHub and then a pull request put in with these changes. I have attached a diff file for the curious. I am still working on getting a corporate CLA put into place.
Thanks for your time and consideration!
[java-complex64.patch.txt](https://github.com/tensorflow/tensorflow/files/1624251/java-complex64.patch.txt)

"
16053,"java.lang.IndexOutOfBoundsException: Invalid index 0, size is 0, TensorFlow on Android","Hello,

Updated the following info:
Have I written custom code No its modification of the code from here https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample 
OS Platform and Distribution: Windows 10
TensorFlow installed from: anaconda
TensorFlow version: 1.2.0
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A

I created  my custom model in keras to recognize happy faces and loaded the model into android and ran into this issue of 
java.lang.IndexOutOfBoundsException: Invalid index 0, size is 0 at runtime and my app crashed. I have modified the code from here https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample 
to suit my model. Is it the problem with protobuf file creation? I have tested my model and it works well in python. Below is the log file and the source code. Please help with this issue? Thanks!

### Source code / logs


01-11 16:21:12.508 18038-18078/com.sridhar.deepak.objectdetection D/OpenGLRenderer: endAllStagingAnimators on 0xab6c06f0 (ListView) with handle 0xab7000d8
01-11 16:21:18.135 18038-18038/com.sridhar.deepak.objectdetection E/TensorFlowInferenceInterface: Failed to run TensorFlow session: java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Switch' with these attrs.  Registered devices: [CPU], Registered kernels:
                                                                                                    device='GPU'; T in [DT_STRING]
                                                                                                    device='GPU'; T in [DT_BOOL]
                                                                                                    device='GPU'; T in [DT_INT32]
                                                                                                    device='GPU'; T in [DT_FLOAT]
                                                                                                    device='CPU'; T in [DT_FLOAT]
                                                                                                    device='CPU'; T in [DT_INT32]
                                                                                                  
                                                                                                  	 [[Node: bn0/cond/Switch = Switch[T=DT_BOOL](bn0/keras_learning_phase, bn0/keras_learning_phase)]]
01-11 16:21:18.135 18038-18038/com.sridhar.deepak.objectdetection D/AndroidRuntime: Shutting down VM
01-11 16:21:18.136 18038-18038/com.sridhar.deepak.objectdetection E/AndroidRuntime: FATAL EXCEPTION: main
                                                                                    Process: com.sridhar.deepak.objectdetection, PID: 18038
                                                                                    java.lang.IndexOutOfBoundsException: Invalid index 0, size is 0
                                                                                        at java.util.ArrayList.throwIndexOutOfBoundsException(ArrayList.java:255)
                                                                                        at java.util.ArrayList.get(ArrayList.java:308)
                                                                                        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.getTensor(TensorFlowInferenceInterface.java:473)
                                                                                        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.readNodeIntoFloatBuffer(TensorFlowInferenceInterface.java:320)
                                                                                        at org.tensorflow.contrib.android.TensorFlowInferenceInterface.readNodeFloat(TensorFlowInferenceInterface.java:275)
                                                                                        at com.sridhar.deepak.objectdetection.TensorFlowImageClassifier.recognizeImage(TensorFlowImageClassifier.java:161)
                                                                                        at com.sridhar.deepak.objectdetection.HappyFaceDetector$2.onPictureTaken(HappyFaceDetector.java:82)
                                                                                        at com.flurgle.camerakit.CameraView$CameraListenerMiddleWare.onPictureTaken(CameraView.java:296)
                                                                                        at com.flurgle.camerakit.Camera1$2.onPictureTaken(Camera1.java:185)
                                                                                        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1118)
                                                                                        at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                                        at android.os.Looper.loop(Looper.java:154)
                                                                                        at android.app.ActivityThread.main(ActivityThread.java:5527)
                                                                                        at java.lang.reflect.Method.invoke(Native Method)
                                                                                        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:739)
                                                                                        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:629)
01-11 16:21:18.136 18038-18038/com.sridhar.deepak.objectdetection E/MQSEventManagerDelegate: failed to get MQSService.
01-11 16:21:19.470 18038-18038/com.sridhar.deepak.objectdetection I/Process: Sending signal. PID: 18038 SIG: 9

TensorFlowImageClassifier file

    @Override
    public List<Recognition> recognizeImage(final Bitmap bitmap,int s) {
        // Log this method so that it can be analyzed with systrace.
        Trace.beginSection(""recognizeImage"");

        Trace.beginSection(""preprocessBitmap"");
        // Preprocess the image data from 0-255 int to normalized float based
        // on the provided parameters.
        if (s==1) {
            bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
            for (int i = 0; i < intValues.length; ++i) {
                final int val = intValues[i];
                floatValues[i * 3 + 0] = (((val >> 16) & 0xFF) - imageMean) / imageStd;
                floatValues[i * 3 + 1] = (((val >> 8) & 0xFF) - imageMean) / imageStd;
                floatValues[i * 3 + 2] = ((val & 0xFF) - imageMean) / imageStd;
            }
        }else {
            bitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());
            for (int i = 0; i < intValues.length; ++i) {
                final int val = intValues[i];
                floatValues[i * 3 + 0] = (((val >> 16) & 0xFF))/imageStd;
                floatValues[i * 3 + 1] = (((val >> 8) & 0xFF))/imageStd;
                floatValues[i * 3 + 2] = ((val & 0xFF))/imageStd;
                floatValues[i * 3 + 0] = floatValues[i * 3 + 0] - 1;
                floatValues[i * 3 + 1] = floatValues[i * 3 + 1] - 1;
                floatValues[i * 3 + 2] = floatValues[i * 3 + 2] - 1;

            }
        }
        Trace.endSection();

        // Copy the input data into TensorFlow.
        Trace.beginSection(""fillNodeFloat"");
        inferenceInterface.fillNodeFloat(
                inputName, new int[]{1, inputSize, inputSize, 3}, floatValues);
        Trace.endSection();

        // Run the inference call.
        Trace.beginSection(""runInference"");
        inferenceInterface.runInference(outputNames);
        Trace.endSection();

        // Copy the output Tensor back into the output array.
        Trace.beginSection(""readNodeFloat"");
        inferenceInterface.readNodeFloat(outputName, outputs);
        Trace.endSection();

MainActivity
    private static final int INPUT_SIZE = 64;
    private static final int IMAGE_MEAN = 128;
    private static final float IMAGE_STD = 128;
    private static final String INPUT_NAME = ""input_1"";
    private static final String OUTPUT_NAME = ""fc/Sigmoid"";

    private static final String MODEL_FILE = ""file:///android_asset/happy_model.pb"";
    private static final String LABEL_FILE =
            ""file:///android_asset/face_label.txt"";




"
16052,Feature Request: Setting the shape of a tf.data.Dataset if it cannot be inferred,"Hello, I have really liked the new `tf.data.Dataset` api, and had a feature request. 
I need to often make data transformations that require third-party libraries, and use `Dataset.map` along with a `tf.py_func` command as shown in the Importing Data tutorial. In the process of doing this, Tensorflow is not able to infer the shape of the numpy arrays that are returned by the py_func-based functions, and so the output_shapes attribute of the dataset returns something like `(TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None), TensorShape(None))`

To address this, I have been adding a new map function after that calls set_shape on each tensor to enforce the shape requirement. For example, I have code that looks something like this:

```
dataset = dataset.map(lambda strings, labels: tuple(tf.py_func(_featurize, [strs, labels], [tf.int32, tf.float64, tf.int32, tf.int32, labels.dtype])))
dataset = dataset.map(_set_shapes)
```
where 
```
def _set_shapes(af, pf, split, atp, labels):
    af.set_shape([None, 75])
    pf.set_shape([None, 14])
    split.set_shape([None, ])
    atp.set_shape([None, 2])
    labels.set_shape([None, ])
    return af, pf, split, atp, labels
```

Could this be simplified by adding a new `tf.data.Dataset` member function called ""set_dataset_shape"" which essentially just implements the above _set_shapes method?

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: AWS Deep Learning AMI
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: AWS Deep Learning AMI-based
- **GPU model and memory**: NVIDIA K80
- **Exact command to reproduce**: N/A

"
16050,Eigen assertion when running on GPU with debug enabled ,"I used r1.5 release version to compile in debug mode. The build command is 
```
bazel build -c opt --config cuda -c dbg --strip=never  //tensorflow/tools/pip_package:build_pip_package
```
I tested the `tutorial/mnist/mnist_deep.py` and it got assertion below. I searched the forum and it seems that there is no clear answer for it. Thanks.

===========================
Answer the questions below:
Have I written custom code:No
OS Platform and Distribution:ubuntu 16.04
TensorFlow installed from: official
TensorFlow version: r1.5
Bazel version: 0.8
CUDA/cuDNN version: 9.0 / 7.0
GPU model and memory: P100, 16GB
Exact command to reproduce: as above


===========================
```
Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz
Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz
Saving graph to: /tmp/tmpis6Bjq
2018-01-11 11:45:43.003071: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-11 11:45:43.377683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:04:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2018-01-11 11:45:43.737737: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-11 11:45:43.738343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties:
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.60GiB
2018-01-11 11:45:43.738437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix
2018-01-11 11:45:43.738512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1
2018-01-11 11:45:43.738527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y N
2018-01-11 11:45:43.738535: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   N Y
2018-01-11 11:45:43.738573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:04:00.0, compute capability: 6.0)
2018-01-11 11:45:43.738589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
step 0, training accuracy 0.08
python: external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:262: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, Vectorizable>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_sum_op<float, float>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_product_op<float, float>, const Eigen::TensorBroadcastingOp<const Eigen::array<long int, 1ul>, const Eigen::TensorReshapingOp<const Eigen::Sizes<1l>, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorCwiseNullaryOp<Eigen::internal::scalar_constant_op<const float>, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> >, const Eigen::TensorMap<Eigen::TensorFixedSize<const float, Eigen::Sizes<>, 1, long int>, 16, Eigen::MakePointer> > > >, const Eigen::TensorCwiseBinaryOp<Eigen::internal::scalar_difference_op<const float, const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16, Eigen::MakePointer>, const Eigen::TensorMap<Eigen::Tensor<float, 1, 1, long int>, 16, Eigen::MakePointer> > > > >; bool Vectorizable = true]: Assertion `**cudaGetLastError() == cudaSuccess'** failed.
Aborted (core dumped)
```"
16048,Cannot compile with Visual Studio 15,"If I build a version of the current tensorflow version 1.5.0RC on Windows with CMAKE and Visual Studio 15 following error is occurred:

1>sparse_column_iterable.cc
1>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion)
1>D:\....\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc(54): note: could be 'const __int64 &tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator::operator *(void)'
1>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): note: while trying to match the argument list '(const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator)'
1>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.12.25827\include\algorithm(2439): note: see reference to function template instantiation '_FwdIt std::_Lower_bound_unchecked<_Iter,_Ty,_Fn>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled
1>        with
1>        [
1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,
1>            _Iter=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,
1>            _Ty=tensorflow::int64,
1>            _Fn=std::less<void>,
1>            _Pr=std::less<void>
1>        ]
1>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.12.25827\include\algorithm(2447): note: see reference to function template instantiation '_FwdIt std::lower_bound<_FwdIt,_Ty,std::less<void>>(_FwdIt,_FwdIt,const _Ty &,_Pr)' being compiled
1>        with
1>        [
1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,
1>            _Ty=tensorflow::int64,
1>            _Pr=std::less<void>
1>        ]
1>D:\....\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc(119): note: see reference to function template instantiation '_FwdIt std::lower_bound<tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,tensorflow::int64>(_FwdIt,_FwdIt,const _Ty &)' being compiled
1>        with
1>        [
1>            _FwdIt=tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator,
1>            _Ty=tensorflow::int64
1>        ]
1>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2100: illegal indirection
1>Done building project ""tf_core_kernels.vcxproj"" -- FAILED.

This problem is also discussed in #12000 after closing this issue.

###System information
 tensorflow 1.5.0RC
 Windows 10
 VisualStudio Prof. 2017
 CMake 3.10.1"
16046,Feature Request: clarify supported environments for official binaries.,"As it stands now, binary release of TensorFlow 1.5 is set to drop compatibility with Ubuntu 14.04 ( https://github.com/tensorflow/tensorflow/issues/15777), and compatibility with Debian Linux distros, such as Amazon Linux AMI (`ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found`).

To avoid surprise, TensorFlow should either:
1. Follow other open-source projects like Ray/PyTorch and provide official binaries for these systems
or
2. Document that support is dropped, to encourage other players (ie, AWS) to take over the job of providing these binaries

@martinwicke"
16045,XLA: Won't converge and doesn't respect visible devices.,"------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes (writing a copy I can share)
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Nvidia Optimized Container 17.12
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: IntelPython 3.5
- **Bazel version (if compiling from source)**: 0.5.4
- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**: 9.0.176 with CUBLAS Basic Accelerated Linear Algebra 9.0.234 / 7
- **GPU model and memory**: Pascal Titan 12Gb
- **Exact command to reproduce**: Coming

### Describe the problem
I've seen 2 problems with XLA:
1) XLA doesn't respect visible devices (neither CUDA Visible devices or tf config options)
2) XLA doesn't converge

_1_
XLA doesn't respect visible devices, I have seen this in both Horovod and when running on a single GPU (on a Multi-GPU desktop), `N` XLA instances are created despite only one GPU being used.

_2_ 
When I run my script with and without XLA, the XLA compiled version plateaus and doesn't converge, whilst the none-XLA version converges. Where XLA has a loss of a magnitude higher.
The benchmark is a VGG network (with BatchNorm) and achieves a 3x speed up with XLA, it just doesn't converge.

Is this a known issue? Or is it just a possibility of occurring?

"
16044,Feature Request: tf.multi_one_hot that is one-hot encoding multiple columns of a Tensor,"Hi there,

I just wrote a function that creates multiple one-hot-encodings for a tensor and concatenates them. I was curious whether this might serve some others and contribute this feature.


```
def multiple_one_hot(cat_tensor, depth_list):
    """"""Creates one-hot-encodings for multiple categorical attributes and
    concatenates the resulting encodings

    Args:
        cat_tensor (tf.Tensor): tensor with mutiple columns containing categorical features
        depth_list (list): list of the no. of values (depth) for each categorical

    Returns:
        one_hot_enc_tensor (tf.Tensor): concatenated one-hot-encodings of cat_tensor
    """"""
    one_hot_enc_tensor = tf.one_hot(cat_int_tensor[:,0], depth_list[0], axis=1)
    for col in range(1, len(depth_list)):
        add = tf.one_hot(cat_int_tensor[:,col], depth_list[col], axis=1)
        one_hot_enc_tensor = tf.concat([one_hot_enc_tensor, add], axis=1)

    return one_hot_enc_tensor

```
I am happy for your feedback. Tell me if you think others might profit and I would enjoy to create a pull request ;)"
16042,tf.contrib.cudnn_rnn.CudnnGRU does not work with input_mode='skip_input' in TF1.5,"### System information
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: `pip install tensorflow-gpu==1.5.0rc0`
- **TensorFlow version**: v1.3.0-rc1-6090-g622487f 1.5.0-rc0
- **Python version**:  3.5
- **CUDA/cuDNN version**: CUDA 9.0, CuDNN 7.0.5
- **GPU model and memory**: GTX 1080 8GB

### Describe the problem
We are running tf.contrib.cudnn_rnn.CudnnGRU in our speech recognition setup with input_mode='skip_input' and it crashes the whole process. Here is the assertion error that we are getting:

`Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000`

Looks like it's crashing here: [tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc#L440). It does not crash if input_mode is 'linear_input'.

Here is the minimal example to reproduce:

### Source code / logs
```
import tensorflow as tf

layer = tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=100,
                             input_mode='skip_input', direction='bidirectional')
# (time, batch_size, num_inputs)
x = tf.random_normal((100, 16, 100))
y = layer(x)

with tf.Session() as sess:
	sess.run(tf.global_variables_initializer())
	print(sess.run(y))

```

Logs:

```
2018-01-11 17:10:44.858413: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-11 17:10:45.067854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095
pciBusID: 0000:03:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2018-01-11 17:10:45.274222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.8095
pciBusID: 0000:04:00.0
totalMemory: 7.92GiB freeMemory: 7.80GiB
2018-01-11 17:10:45.274776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Device peer to peer matrix
2018-01-11 17:10:45.274804: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1126] DMA: 0 1 
2018-01-11 17:10:45.274812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 0:   Y Y 
2018-01-11 17:10:45.274817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1136] 1:   Y Y 
2018-01-11 17:10:45.274826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0, compute capability: 6.1)
2018-01-11 17:10:45.274832: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0, compute capability: 6.1)
2018-01-11 17:10:46.240862: F tensorflow/contrib/cudnn_rnn/kernels/cudnn_rnn_ops.cc:440] Check failed: size == params_input[i].NumElements() Params size mismatch. Expected 0, got 10000
Aborted (core dumped)
```

### Thoughts

I think the problem is in python wrapper code located at [contrib/cudnn_rnn/python/layers/cudnn_rnn.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py).

Both [`_canonical_weight_shape(self, layer)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L417) and [`_canonical_bias_shape(self, layer)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py#L443) don't handle the case when `input_mode='skip_input'`. The wrapper code doesn't even check if `input_size == num_units` when `input_mode='skip_input'` as it should! The issue seems to be easy to fix, but I may be mistaken.

Here is an example that shows that both 'skip_input' and 'linear_input' get the same set of canonical parameter shapes:
```
CudnnGRU = lambda **kwargs: tf.contrib.cudnn_rnn.CudnnGRU(num_layers=1, num_units=7, direction='bidirectional', **kwargs)

layer = CudnnGRU(input_mode='skip_input')
layer.build((200, 16, 5))
print(""skip_input weights"", layer.canonical_weight_shapes)
print(""skip_input biases"", layer.canonical_bias_shapes)

layer = CudnnGRU(input_mode='linear_input')
layer.build((200, 16, 5))
print(""linear_input_weights"", layer.canonical_weight_shapes)
print(""linear_input_biases"", layer.canonical_bias_shapes)
```

Example output:
```
skip_input weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]
skip_input biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]
linear_input_weights [(7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7), (7, 5), (7, 5), (7, 5), (7, 7), (7, 7), (7, 7)]
linear_input_biases [[7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7], [7]]
```

### Workaround

In our setup we decided that we will keep allocating opaque_params buffer manually with first querying its size with 
```
# Note: this code may require tf==1.4 to run.
layer = tf.contrib.cudnn_rnn.CudnnGRU(1, num_units=800, input_size=800,
                                      input_mode='linear_input',
                                      direction='bidirectional')
with tf.Session() as sess:
    OPAQUE_BUFFER_SIZE = sess.run(layer.params_size())
    print(OPAQUE_BUFFER_SIZE)
```

But this is really troublesome because it requires running the intermediate graph to get OPAQUE_BUFFER_SIZE before building the main graph."
16041,Code documentation for `confusion_matrix.py` misleading,"### Describe the problem

The documentation for `confusion_matrix.py` says:

```
  Args:
    labels: 1-D `Tensor` of real labels for the classification task.
    predictions: 1-D `Tensor` of predictions for a given classification.
```

, however I found that those two arguments are simply python arrays and not Tensors. The following trial test code demonstrates this. As a TF/Python newbie, I'm wondering if this is actually a real issue, and if so I'll create a PR to correct it to prevent confusion to future programmers.

### Source code / logs

```
import tensorflow as tf

y_ = [0, 2, 2, 2]
y = [2, 1, 2, 2]

with tf.Session() as sess:
    confusion_matrix = tf.confusion_matrix(labels=y_, predictions=y, num_classes=4)
    confusion_matrix_to_Print = sess.run(confusion_matrix)
    print(confusion_matrix_to_Print)

```"
16040,Bug in boolean input tensors for ops ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. 

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 16.04

- **TensorFlow installed from (source or binary)**:
pip3 install --user tensorflow-gpu 

- **TensorFlow version (use command below)**:
1.4.1 

- **Python version**: 
3.5.2 

- **Bazel version (if compiling from source)**:
NA

- **GCC/Compiler version (if compiling from source)**:
gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609

- **CUDA/cuDNN version**:
cuda-8.0.61-cudnn-v6

- **GPU model and memory**:
any

- **Exact command to reproduce**:
see description below

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem

Boolean tensors seem to have a bug. 

In C++ I define an op with a boolean input tensor like so: 
    REGISTER_OP(""SpatialAugmentationPossible"")
        .Input(""input: float32"")
        .Output(""output: bool"")
        .Input(""mirror: bool"")
        .Input(""angle: float32"")
        .Input(""dx: float32"")
        .Input(""dy: float32"")
        .Input(""sx: float32"")
        .Input(""sy: float32"")
        .Attr(""crop_width: int"")
        .Attr(""crop_height: int"")
    ...
Note that mirror is a boolean tensor. The op compiles fine. When calling like this: 

                    op = ops.spatial_augmentation_possible(
                        input=test_input,
                        mirror=tf.cast(mirror, dtype=tf.bool),
                        angle=tf.convert_to_tensor(angle, dtype=tf.float32),
                        dx=tf.convert_to_tensor(dx, dtype=tf.float32),
                        dy=tf.convert_to_tensor(dy, dtype=tf.float32),
                        sx=tf.convert_to_tensor(sx, dtype=tf.float32),
                        sy=tf.convert_to_tensor(sy, dtype=tf.float32),
                        crop_width = 500,
                        crop_height = 300
                    )

                    possible_tensor = sess.run(op)

I get:

2018-01-11 14:57:45.722505: F tensorflow/core/framework/tensor.cc:586] Check failed: dtype() == expected_dtype (10 vs. 1)

I checked the proto, 10 corresponds to boolean and 1 to float32. If I change the line with the mirror argument like this:                     

                    mirror=tf.cast(mirror, dtype=tf.float32),

I get: 

ValueError: Tensor conversion requested dtype bool for Tensor with dtype float32: 'Tensor(""Cast:0"", shape=(1,), dtype=float32, device=/device:GPU:0)'


That means now it is complaining that I don't input a bool. But the message above is saying that I shouldn't input a bool. For this reason there seems to be a bug. 

Best, 


Eddy
                    

"
16039,How TF-Detect draw a rectangular?,"How TF-Detect draw a rectangular?
I can't find the corresponding code?
Is it calling OpenGL to draw a rectangular?"
16038,Link gives 404,https://github.com/tensorflow/models/tree/master/syntaxnet#installation gives   404
16036,"raise PiCameraMMALError(status, prefix) picamera.exc.PiCameraMMALError: Failed to enable connection: Out of resources",
16035,FasterRCNN error,"Hi friends. 
While  i am trying to execute tensor flow based faster RCNN, i got the following error. 
please help me how to solve this.

tensorflow.python.framework.errors_impl.InternalError: WhereOp: Could not launch cub::DeviceReduce::Sum to count number of true indices.  temp_storage_bytes: 2815, status: invalid device function
"
16034,Feature request: tf.nn.dropout noise_shape should support unspecified dimensions,"It would be nice if the noise_shape in [tf.nn.dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) would support unspecified dimensions, and just use the shape of the input tensor, e.g. `-1` or `None`. This way it could be specified as `noise_shape = [-1, 1, 1, -1]` instead of `noise_shape = [k, 1, 1, n]`."
16033,NNAPI/libneuralnetwors.so does not work for other models than default mobilenet_quant_v1_224.tflite,"I have made changes to tensorflowlite to force it use nnapi on my android 8.1 phone, and the nnapi only worked normally for the default default mobilenet_quant_v1_224.tflite.

The nnapi/libneuralnetworks.so did not work for the tensorflow_inception_graph.tflite inside the attached asserts.zip, which was built by toco, from tensorflow_inception_graph.pb of https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android

My steps,
1. generated tensorflow_inception_graph.tflite from tensorflow_inception_graph.pb by toco
2. add 7 extra lines to labels.txt ""a\nb\nc\nd\ne\ng\ng""
3. copy new labels.txt and tensorflow_inception_graph.tflite to my bazel cache
4. apply the attached patch a.txt to tensorflow
5.  bazel build --cxxopt=--std=c++11 //tensorflow/contrib/lite/java/demo/app/src/main:TfLiteCameraDemo --config=android_arm64 --cpu=arm64-v8a --fat_apk_cpu=arm64-v8a
6. adb install -r <apk_file>

If the nnapi is not forced to use, then the apk run normally, though the results are wrong. But if I forced it to use nnapi, then the apk crashed.

I guess the tensorflow_inception_graph.tflite contained some operators which were not supported by nnapi/libneuralnetwors.so. How can I confirm it?

Thank you.

[a.txt](https://github.com/tensorflow/tensorflow/files/1622153/a.txt)
[assets.zip](https://github.com/tensorflow/tensorflow/files/1622067/assets.zip)
"
16032,"When will those operations such as ""BuiltinOperator_L2_NORMALIZATION"" be delegated to NNAPI? ","System information
Have I written custom code: yes
OS Platform and Distribution: Ubuntu 14.04
TensorFlow installed from: pip
TensorFlow version (use command below): 1.4.1
Python version: 2.7.5

My problem:
      My android application wants to load the ""facenet"" model through TF lite with NNAPI enabled to do face recognition. But it always crashed. After I debugged,  I found it was caused by that the ""L2_NORMALIZATION""  was not delegated to NNAPI. I wonder when will those operations be supported by nnapi_delegate, as my application has to use the hardware acceleration. Or is there any workaround to make the application complete normally with ""Use_NNAPI"" enabled ?  "
16031,tf.data.Dataset.padded_batch() doesn't work with dataset.map using tf.py_func,"
------------------------

### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: CentOS Linux release 7.2.1511
- **TensorFlow installed from**: pip
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.5

### Describe the problem
It's quite common for NLP tasks to read variable-length sentences from text files, to map them and to padd them. But Dataset.padded_batch() doesn't work with tf.dataset which uses map (tf.py_func)

### Source code
```
import tensorflow as tf                                                                           
import numpy as np                                                                                
                                                                                                  
def convert(line):                                                                                
    tokens = line.split()                                                                 
    return np.array(tokens, dtype=np.int32)

# Simulating reading variable-length sentences from a file. Using TextLineDataset will have the same problem
dataset = tf.data.Dataset.from_tensor_slices([""1 2 3"", ""4 5""])         
                           
# Tokenize each sentence and convert it to list of int
dataset = dataset.map(lambda line: tf.py_func(convert, [line], [tf.int32]))     
                  
dataset = dataset.padded_batch(1, [None]) # This line doesn't work whatever the batch_size is
# dataset = dataset.batch(1) # This line works well                                              
 
iterator = dataset.make_one_shot_iterator()                                                       
batch_data = iterator.get_next()                                                                  
                                                                                                  
with tf.Session() as sess:                                                                        
    print sess.run(batch_data)                                                                                                 
```

### Log
```
    dataset = dataset.padded_batch(1, [None])
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 695, in padded_batch
    return PaddedBatchDataset(self, batch_size, padded_shapes, padding_values)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/ops/dataset_ops.py"", line 1292, in __init__
    input_dataset.output_shapes, _partial_shape_to_tensor, padded_shapes)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 512, in map_structure_up_to
    assert_shallow_structure(shallow_tree, input_tree)
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/data/util/nest.py"", line 356, in assert_shallow_structure
    ""Input has type: %s."" % type(input_tree))
TypeError: If shallow structure is a sequence, input must also be a sequence. Input has type: <type 'list'>.
```"
16028,Optimzer: Better handling of gradients for min/max ops.,"Please don't kick me too hard for this. If this ticket should not be here, please direct me to the proper place. It's not a help or support request, just a very naive idea/request/improvement.

Image a model has this op in the graph: `y = tf.maximum(a, b)`. If `y` is contributing error to the loss (higher `y` higher is loss), then you are interested in minimizing both `a` and `b`. Otherwise, your optimizer will play whack-a-mole game forever. Especially, if you have something like this in your model: `b = 1 - a` and `y = tf.maximum(a, b)`

I've researching how does optimizer works and looks like each op has a function that defines how to calculate its gradients. 
In the current implementation for maximum/minimum (python's version) is located here: 
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L901
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L883

In the current implementation, it discards gradients for `a`, if `b` is bigger than `a` and other way around. 

I think, that there is some place for improvement of training speed simply by considering gradients for all variables in min/max operations (including reduce_min/reduce_max and all max/min pooling). There are a lot of models with max pooling. It makes sense that they will train faster if they stop playing whack-a-feature each time it has max pooling layer.

If it make sense, I would like to do a small PoC.
"
16026,can't use mpi_allreduce op when tensors run on gpus,"I compiled and installed new mpi_collective feature with openmpi3.0 on tensorflow 1.5.  When I use allreduce function to aggregate loss, I got segmentation fault errors. However, When I force code to run on CPU device, it's fine. 

**## here is my runtime environment.**
== cat /etc/issue ===============================================
Linux user-ubuntu 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""14.04.5 LTS, Trusty Tahr""
VERSION_ID=""14.04""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 4.8.4-2ubuntu1~14.04.3) 4.8.4
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux user-ubuntu 3.19.0-25-generic #26~14.04.1-Ubuntu SMP Fri Jul 24 21:16:20 UTC 2015 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.5.0rc0)
tensorflow-tensorboard (0.4.0rc2)

== tensorflow import ============================================
tf.VERSION = 1.5.0-rc0
tf.GIT_VERSION = unknown
tf.COMPILER_VERSION = unknown
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Wed Jan 10 21:56:08 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Tesla P100-PCIE...  Off  | 0000:06:00.0     Off |                    0 |
| N/A   27C    P0    27W / 250W |      0MiB / 16276MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  Tesla P100-PCIE...  Off  | 0000:84:00.0     Off |                    0 |
| N/A   25C    P0    27W / 250W |      0MiB / 16276MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/lib64/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61

**##here is error.**

tensor@user-ubuntu:~/tensorflow-r1.4/tensorflow/contrib/mpi_collectives$ mpirun -n 2 python mpi_simple_nn.py 

2018-01-10 21:50:44.579981: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 21:50:44.580748: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 21:50:47.127209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:06:00.0
totalMemory: 15.89GiB freeMemory: 15.34GiB
2018-01-10 21:50:47.127952: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 0 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:06:00.0
totalMemory: 15.89GiB freeMemory: 15.34GiB
2018-01-10 21:50:48.027960: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.34GiB
2018-01-10 21:50:48.028762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1206] Found device 1 with properties: 
name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285
pciBusID: 0000:84:00.0
totalMemory: 15.89GiB freeMemory: 15.34GiB
2018-01-10 21:50:48.028835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1221] Device peer to peer matrix
2018-01-10 21:50:48.028932: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1221] Device peer to peer matrix
2018-01-10 21:50:48.028886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] DMA: 0 1 
2018-01-10 21:50:48.028910: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 0:   Y N 
2018-01-10 21:50:48.028926: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 1:   N Y 
2018-01-10 21:50:48.028933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0
2018-01-10 21:50:48.028938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 1
2018-01-10 21:50:48.028966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1227] DMA: 0 1 
2018-01-10 21:50:48.028974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 0:   Y N 
2018-01-10 21:50:48.028979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1237] 1:   N Y 
2018-01-10 21:50:48.028994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 0
2018-01-10 21:50:48.029000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1300] Adding visible gpu device 1
2018-01-10 21:50:48.568865: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14824 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)
2018-01-10 21:50:48.668728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 553 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:06:00.0, compute capability: 6.0)
2018-01-10 21:50:48.676343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 14824 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
2018-01-10 21:50:48.708625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:987] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 553 MB memory) -> physical GPU (device: 1, name: Tesla P100-PCIE-16GB, pci bus id: 0000:84:00.0, compute capability: 6.0)
my_rank  1
my_rank  0
[user-ubuntu:49967] *** Process received signal ***
[user-ubuntu:49967] Signal: Segmentation fault (11)
[user-ubuntu:49967] Signal code: Invalid permissions (2)
[user-ubuntu:49967] Failing at address: 0x1042c005000
[user-ubuntu:49967] [ 0] /lib/x86_64-linux-gnu/libpthread.so.0(+0x10330)[0x7f2122752330]
[user-ubuntu:49967] [ 1] /lib/x86_64-linux-gnu/libc.so.6(+0x9ac36)[0x7f2122413c36]
[user-ubuntu:49967] [ 2] /usr/local/openmpi3/lib/openmpi/mca_btl_vader.so(mca_btl_vader_sendi+0x332)[0x7f1fd5d9b832]
[user-ubuntu:49967] [ 3] /usr/local/openmpi3/lib/openmpi/mca_pml_ob1.so(+0xb6eb)[0x7f1fd63c86eb]
[user-ubuntu:49967] [ 4] /usr/local/openmpi3/lib/openmpi/mca_pml_ob1.so(mca_pml_ob1_send+0x690)[0x7f1fd63ca130]
[user-ubuntu:49967] [ 5] /usr/local/openmpi3/lib/libmpi.so.40(PMPI_Send+0xf2)[0x7f1ffc59d062]
[user-ubuntu:49967] [ 6] /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so(_ZN10tensorflow7contrib15mpi_collectives13RingAllreduceIN5Eigen9GpuDeviceEfEENS_6StatusEPNS_15OpKernelContextEPKNS_6TensorEPS8_SB_+0x14f)[0x7f21082b50bf]
[user-ubuntu:49967] [ 7] /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so(+0x1cab9)[0x7f21082aaab9]
[user-ubuntu:49967] [ 8] /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/mpi_collectives/python/ops/_mpi_ops.so(+0x23760)[0x7f21082b1760]
[user-ubuntu:49967] [ 9] /usr/lib/x86_64-linux-gnu/libstdc++.so.6(+0xb1a60)[0x7f1ff5c48a60]
[user-ubuntu:49967] [10] /lib/x86_64-linux-gnu/libpthread.so.0(+0x8184)[0x7f212274a184]
[user-ubuntu:49967] [11] /lib/x86_64-linux-gnu/libc.so.6(clone+0x6d)[0x7f2122476ffd]
[user-ubuntu:49967] *** End of error message ***
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun noticed that process rank 0 with PID 0 on node user-ubuntu exited on signal 11 (Segmentation fault)"
16025,Checkpoints continue to grow after the first restore,"### System information
- No

- **OS Platform and Distribution**: Mac OS X, v 10.13.2
- **TensorFlow installed from**: binary
- **TensorFlow version**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126
- **Python version**: Python 3.5.0
- **Bazel version**: N/A
- **GCC/Compiler version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: Intel Iris Pro 1536 MB
- **Exact command to reproduce**: N/A
- **Have I written custom code**: N/A

### Describe the problem

The parameter `max_to_keep` of the `Saver` class does not seem to have effect once a model and its training variables are restored. In other words, the first time I train my model, the saver is keeping only `max_to_keep` checkpoints. Then I interrupt the training. Later, when I resume it, the number of checkpoints keeps going without any apparent limit.

### Related issues

- https://github.com/tensorflow/tensorflow/issues/5929
- https://github.com/tensorflow/tensorflow/issues/6326
"
16023, can't feed  the multi-channel to contrib_audio.audio_spectrogram,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.4.0-cp35-cp35m-linux_x86_64.whl
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: CUDA 8.0/cuDNN6.0
- **GPU model and memory**: Geforce 1080 TI
- **Exact command to reproduce**:

### Describe the problem
can't  feed the multi-channel to audio_spectrogram()
the error is 
Spectrogram size calculation failed:Expected height 98 but got 100

if use [16000, 1] at line 29 in my below code, the result is ok. no error. It means only accept one channel.
I read the code at:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spectrogram_op.cc
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/spectrogram.cc

I think the error is :
samples_to_next_step_ and  input_queue_ is modified when calculation for each channel.
But Spectrogram don't initialized these two variable at each channel, 

### Source code / logs
![image](https://user-images.githubusercontent.com/32910309/34800518-fff6eaee-f618-11e7-8563-3cac1b8637c6.png)

![image](https://user-images.githubusercontent.com/32910309/34800232-c10414f2-f617-11e7-94a5-2c0a7ec3738d.png)

"
16019,tf-nightly and master - cannot import tensorflow,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: tf-nightly-gpu-1.6.0.dev20180110
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9.0/7
- **GPU model and memory**: V100 16GB
- **Exact command to reproduce**: ```import tensorflow```

### Describe the problem
On the current tf-nightly-gpu I cannot import tensorflow, the following error is produced. I am also seeing the same behavior on tf-nightly and also a build from source of master (SHA: 82b1e8eee8847730026379e3a5762c0e09d6fd36):
``` python
In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-1-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

/home/ubuntu/tensorflow/tensorflow/__init__.py in <module>()
     22
     23 # pylint: disable=wildcard-import
---> 24 from tensorflow.python import *
     25 # pylint: enable=wildcard-import
     26

/home/ubuntu/tensorflow/tensorflow/python/__init__.py in <module>()
     47 import numpy as np
     48
---> 49 from tensorflow.python import pywrap_tensorflow
     50
     51 # Protocol buffers

/home/ubuntu/tensorflow/tensorflow/python/pywrap_tensorflow.py in <module>()
     23 import traceback
     24
---> 25 from tensorflow.python.platform import self_check
     26
     27

ImportError: No module named platform
```

### Source code / logs
N/A"
16014,Error While Importing Tensorflow 1.5 RC0 with CUDA 9.1,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 64-bit
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.5 RC0
- **Python version**: 3.6.3 (via Anaconda)
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: 9.1/7.0.5
- **GPU model and memory**: GTX 1050 4GB (notebook version)
- **Exact command to reproduce**: import tensorflow (from a Python shell)

### Describe the problem
The installation of the CUDA, cuDNN and Tensorflow went smoothly. When I import tensorflow I get the error pasted below. CUDA and CUDA_PATH_V9_1 were automatically set by the installer to `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.1`.

Installation instructions used:

CUDA: http://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html#windows
(steps 7 to 10 ignored, because it involved downloading nearly 6 GB for a test. Is it required? I haven't installed TF in Windows before.)
cuDNN: http://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html
(step 5 ignored as I didn't have a Visual Studio project - is there a VS project in the context of Tensorflow?)
Tensorflow: https://www.tensorflow.org/install/install_windows
(`pip install --ignore-installed --upgrade tensorflow-gpu==1.5.0rc` inside a conda environment)


### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
(tensorflow) C:\Users\Navneeth>python
Python 3.6.3 |Anaconda, Inc.| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Navneeth\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\self_check.py"", line 75, in preload_check
    ctypes.WinDLL(build_info.cudart_dll_name)
  File ""C:\Users\Navneeth\Miniconda3\envs\tensorflow\lib\ctypes\__init__.py"", line 348, in __init__
    self._handle = _dlopen(self._name, mode)
OSError: [WinError 126] The specified module could not be found

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Navneeth\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Navneeth\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Navneeth\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 30, in <module>
    self_check.preload_check()
  File ""C:\Users\Navneeth\Miniconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\self_check.py"", line 82, in preload_check
    % (build_info.cudart_dll_name, build_info.cuda_version_number))
ImportError: Could not find 'cudart64_90.dll'. TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Download and install CUDA 9.0 from this URL: https://developer.nvidia.com/cuda-toolkit
```"
16011,Tensorboard issue with the official docker image - 1.5.0-rc0-gpu-py3,"Hello everyone,

I have the exact same issue as stated here: https://github.com/tensorflow/tensorflow/issues/14855
And on the official tensorboard repository: https://github.com/tensorflow/tensorboard/issues/812

The fact that I am using the official Docker Image and I didn't build anything from scratch.

`tensorflow/tensorflow   1.5.0-rc0-gpu-py3   9e770d59b136        6 days ago          2.85GB`

What I get when I try to launch Tensorboard as usual:

```
# tensorboard --logdir=log_directory
/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
ImportError: cannot import name 'run_main'
```

**Resolution Idea:**
I noticed that by simply running the command: `pip install tensorboard` inside the container the problem is solved and I am able to normally launch Tensorboard.

Thanks a lot for the help,

All the best,

Jonathan
"
16010,lib_package does not bundle MKL-DNN,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: 0.8.1
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: -
- **GPU model and memory**: -
- **Exact command to reproduce**: 

### Describe the problem
I wan't to build the tensorflow C-API from source with MKL-DNN support in order to use it in another project. The easiest solution (if not the only convenient one) I found for building the C-API is using the lib_package tool:

```bash
bazel build --config=mkl -c opt //tensorflow/tools/lib_package:libtensorflow
```
The build succeeds. However, the packaged library does not contain `libmklml_intel.so` and `libiomp5.so`. 

```bash
$ ldd libtensorflow_framework.so
	linux-vdso.so.1 =>  (0x00007ffec0f8a000)
	libmklml_intel.so => not found
	libiomp5.so => not found
	libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007feb07b2f000)
	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007feb07826000)
	libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007feb07609000)
	libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007feb07287000)
	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007feb07071000)
	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007feb06ca7000)

```
Is there a way to fix the Bazel build such that it outputs all necessary libs?
"
16009,"bazel build ask for ANDROID_NDK_HOME, ANDROID_SDK_HOME -- no way to disable it","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:
source
- **TensorFlow version (use command below)**:
('v1.5.0-rc0-1-g793280a', '1.5.0-rc0')
- **Python version**: 
2.7
- **Bazel version (if compiling from source)**:
```
Build label: 0.9.0
Build target: bazel-out/k8-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar
Build time: Tue Dec 19 09:31:58 2017 (1513675918)
Build timestamp: 1513675918
Build timestamp as int: 1513675918
```
- **GCC/Compiler version (if compiling from source)**:
g++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- **CUDA/cuDNN version**:
toolkit_9.0 and cudnn 7.0.5_for_9.0
- **GPU model and memory**:
different machines (irrelevant)
- **Exact command to reproduce**:
see [this gist](https://gist.github.com/PatWie/aef90e72dbeaf2f79fbcaa031d74baad) which is mainly

```bash
export TF_NEED_GCP=0
export TF_NEED_CUDA=1
export TF_CUDA_VERSION=""$($CUDA_TOOLKIT_PATH/bin/nvcc --version | sed -n 's/^.*release \(.*\),.*/\1/p')""
export TF_CUDA_COMPUTE_CAPABILITIES=6.1,5.2,3.5
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_ENABLE_XLA=0
export TF_NEED_VERBS=0
export TF_CUDA_CLANG=0
export TF_CUDNN_VERSION=7
export TF_NEED_MKL=0
export TF_DOWNLOAD_MKL=0
export TF_NEED_MPI=0
export TF_NEED_GDR=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export GCC_HOST_COMPILER_PATH=$(which gcc)
export CC_OPT_FLAGS=""-march=native""

./configure

bazel build --config=opt --copt=-mfpmath=both --copt=-msse4.2 --copt=-O3 --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=1 ....
```
### Describe the problem
In the past, using exactly this scripted worked. However, there are now a few issues:
The build uses `AVX2` even I haven't specified it as `--copt` (which worked in the past)

### Source code / logs
depending on the machine it gives

```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
2018-01-10 15:06:19.070740: F tensorflow/core/platform/cpu_feature_guard.cc:36] The TensorFlow library was compiled to use AVX2 instructions, but these aren't available on your machine.
zsh: abort      python
```
or
```
Python 2.7.12 (default, Nov 20 2017, 18:23:56) 
[GCC 5.4.0 20160609] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
zsh: illegal hardware instruction  python
```

On machines with AVX2 everything is fine. Further, there is no way to skip to setup ANDROID_NDK_HOME, ANDROID_SDK_HOME (I manually uncommented this in `configure.py`).

*edit*
I am willing to provide a pull-request for `configure.py`, adding something like `TF_NEED_ANDROID`."
16008,"Java/JNI , Object Detection: Not big Difference with GPU or CPU? (Insignificant difference) ~300ms with and without GPU","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: 

**binary** by instructions https://www.tensorflow.org/versions/master/install/install_java
> 
> Install on Linux
> 
> Take the following steps to install TensorFlow for Java on Linux or macOS:
> 
> 1 Download libtensorflow.jar, which is the TensorFlow Java Archive (JAR).
> 2 Decide whether you will run TensorFlow for Java on CPU(s) only or with the help of GPU(s). To help you decide, read the section entitled ""Determine which TensorFlow to install"" in one of the following guides:
>  - Installing TensorFlow on Linux
>
> 3 Download and extract the appropriate Java Native Interface (JNI) file for your operating system and processor support by running the following shell commands:
> 
>  TF_TYPE=""gpu""
>  OS=$(uname -s | tr '[:upper:]' '[:lower:]')
>  mkdir -p ./jni
>  curl -L \
>    ""https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow_jni-${TF_TYPE}-${OS}-x86_64-1.4.0.tar.gz"" |
>    tar -xz -C ./jni

- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: n/a, not used here (Java instead)
- **Bazel version (if compiling from source)**: n/a, not used here
- **GCC/Compiler version (if compiling from source)**: n/a, not used here
- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61, cuDNN 6
- **GPU model and memory**: GeForce 940MX

### Source code / logs
```
Checking to see if TensorFlow native methods are already loaded
TensorFlow native methods not found, attempting to load via tensorflow_inference
Successfully loaded TensorFlow native methods (RunStats error may be ignored)
2018-01-10 15:51:41.115224: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2018-01-10 15:51:41.254497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-10 15:51:41.255183: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce 940MX major: 5 minor: 0 memoryClockRate(GHz): 1.2415
pciBusID: 0000:01:00.0
totalMemory: 1,96GiB freeMemory: 1,51GiB
2018-01-10 15:51:41.255217: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce 940MX, pci bus id: 0000:01:00.0, compute capability: 5.0)
Model load took 313ms, TensorFlow version: 1.4.0
```
"
16004,cudnn,"INFO:tensorflow:Starting Queues.
2018-01-10 18:45:29.789178: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:29.843774: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
INFO:tensorflow:global_step/sec: 0
2018-01-10 18:45:33.552377: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:33.587139: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:33.621270: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:33.660148: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:34.072946: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:34.244066: E tensorflow/stream_executor/cuda/cuda_blas.cc:366] failed to create cublas handle: CUBLAS_STATUS_NOT_INITIALIZED
2018-01-10 18:45:37.678240: E tensorflow/stream_executor/cuda/cuda_dnn.cc:385] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2018-01-10 18:45:37.678276: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM
2018-01-10 18:45:37.678313: F tensorflow/core/kernels/conv_ops.cc:667] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)"
16001,fail to convert resnet_v1_50 to tflite,"I downloaded resnet_v1_50 model from https://github.com/tensorflow/models/tree/master/research/slim

when I tried to convert this model to tflite, I got below error:
2018-01-10 14:54:19.491608: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: Mean, Squeeze.

I use below command to convert:
bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=./resnet_v1_50_frozen.pb \
  --input_format=TENSORFLOW_GRAPHDEF  --output_format=TFLITE \
  --output_file=./resnet_v1_50.tflite --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=resnet_v1_50/predictions/Reshape_1  --input_shapes=1,224,224,3


Does it means that  tflite do not support op Mean and Squeeze  ?

  "
16000,TensorFlow Installation Error on SLES 11 SP Linux ,"I have been trying to install tensorflow (1.3.0/1.4/1.4.1) on SLES 11 Linux, I was getting GLIBC_2.14 not found an exception, currently, we have the **GLIBC_2.11.3 version in SLES 11 SP3T**. 

Please help me to install any of **tensorflow > 0.8** versions on SLES 11 Linux and let me know the tensorflow compatible version for SLES 11. OR SLES 11 SP3 Linux is not compatible with TensorFlow any of the versions.

Thanks in Advance.
  "
15999,[Feature Request] Request for weighted sampling in tf.data.Dataset ,Are there any methods that we can sample with weights without pre-weighting the dataset as the input in Dataset API?
15998,tensorflow input/output tensor reshape c++,"currently , I am working on loading and testing a tensorflow model on android using c++, and the trained model is a full convolutional model, so the input need to be dynamically reshaped according to input image size.

I can make this done easily using python. but when turn to c++ , I can hardly find much examples and experience on this.

the trained model is converted to *.pb file , and the input and output tensor shape has been specified before conversion in python. and now I want to reshape the input and output in c++ before using the model."
15997,Allow tensorflow/tensorflow/workspace.bzl to customize dependencies,"When including tensorflow as a dependency of a Bazel project, it requires you to take all the declared dependencies in  `tensorflow/tensorflow/workspace.bzl` or none of them. Some projects, like the closure_rules allow you to customize the dependencies:
https://github.com/bazelbuild/rules_closure/blob/master/closure/repositories.bzl#L21

This allows you to use whatever versions you want for specific dependencies that may be different from what tensorflow's workspace.bzl declares."
15996,CMake building fail on Linux Centos 7 ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Centos 7
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**:  2.7.5
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: 4.8.5
- **CUDA/cuDNN version**: 8/6
- **GPU model and memory**: K80
- **Exact command to reproduce**:

### Describe the problem
After successful config by CMake, the make failed.

### Source code / logs
[  1%] Performing build step for 'zlib'
Scanning dependencies of target zlib
[  2%] Building C object CMakeFiles/zlib.dir/adler32.o
[  5%] Building C object CMakeFiles/zlib.dir/compress.o
[  7%] Building C object CMakeFiles/zlib.dir/crc32.o
[ 10%] Building C object CMakeFiles/zlib.dir/deflate.o
[ 12%] Building C object CMakeFiles/zlib.dir/gzclose.o
[ 15%] Building C object CMakeFiles/zlib.dir/gzlib.o
[ 17%] Building C object CMakeFiles/zlib.dir/gzread.o
[ 20%] Building C object CMakeFiles/zlib.dir/gzwrite.o
[ 22%] Building C object CMakeFiles/zlib.dir/inflate.o
[ 25%] Building C object CMakeFiles/zlib.dir/infback.o
[ 27%] Building C object CMakeFiles/zlib.dir/inftrees.o
[ 30%] Building C object CMakeFiles/zlib.dir/inffast.o
[ 32%] Building C object CMakeFiles/zlib.dir/trees.o
[ 35%] Building C object CMakeFiles/zlib.dir/uncompr.o
[ 37%] Building C object CMakeFiles/zlib.dir/zutil.o
[ 40%] Linking C shared library libz.so
/usr/bin/ld: CMakeFiles/zlib.dir/compress.o: relocation R_X86_64_32 against `.rodata.str1.1' can not be used when making a shared object; recompile with -fPIC
CMakeFiles/zlib.dir/compress.o: could not read symbols: Bad value
collect2: error: ld returned 1 exit status
make[5]: *** [libz.so.1.2.8] Error 1
make[4]: *** [CMakeFiles/zlib.dir/all] Error 2
make[3]: *** [all] Error 2
make[2]: *** [zlib/src/zlib-stamp/zlib-build] Error 2
make[1]: *** [CMakeFiles/zlib.dir/all] Error 2
make: *** [all] Error 2


Do I need revise some code of CMakeLists.txt?

"
15995,/home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1). ,"

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:clone from git
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**:0.5.4
- **GCC/Compiler version (if compiling from source)**:5.4.0
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:GTX1070ti  8GB
- **Exact command to reproduce**:


### Describe the problem
build error.when I finished ./configure and run bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package, met an error.

### Source code / logsExtracting Bazel installation...
..............
WARNING: /home/hp/Downloads/tensorflow/tensorflow/core/BUILD:1825:1: in includes attribute of cc_library rule //tensorflow/core:framework_headers_lib: '../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/hp/Downloads/tensorflow/tensorflow/tensorflow.bzl:1152:30.
WARNING: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE:1: Workspace name in /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/grpc/WORKSPACE (@com_github_grpc_grpc) does not match the name given in the repository's definition (@grpc); this will cause a build error in future versions.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/hp/Downloads/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
INFO: Found 1 target...
ERROR: /home/hp/.cache/bazel/_bazel_hp/a7dace51e7355e610cd60a2c24b75c6d/external/astor_archive/BUILD:8:1: Converting to Python 3: external/astor_archive/astor/source_repr.py failed (Exit 1).

  "
15994,Feature request: (documentation) operation complexity / performance chart,"* Have I written custom code: NA
* OS Platform and Distribution: Any
* TensorFlow installed from: NA
* TensorFlow version: NA
* Bazel version: NA
* CUDA/cuDNN version: NA
* GPU model and memory: NA
* Exact command to reproduce: NA

It would be interesting to have a complexity/performance chart for different operations. For example, to know that `tf.reshape` is computationally cheaper than `tf.transpose`. 

I did see the [Performance Guide](https://www.tensorflow.org/performance/performance_guide), but that's not what I mean."
15992,"android demo ,How to switch the horizontal screen?","tensorflow 1.4
TF-OD-API model

```
<activity android:name=""org.tensorflow.demo.DetectorActivity""
                  android:screenOrientation=""landscape""
                  android:label=""@string/activity_name_detection"">
            <intent-filter>
                <action android:name=""android.intent.action.MAIN"" />
                <category android:name=""android.intent.category.LAUNCHER"" />
            </intent-filter>
 </activity>
```

I have some problems now, and I want to ask how to do the rotation.
  "
15990,[Tracking bug] Building Tensorflow with Clang on Windows,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 x64
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.0
- **Python version**: Python 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: VS 2017 15.5
- **CUDA/cuDNN version**: n/a
- **GPU model and memory**: n/a
- **Exact command to reproduce**: n/a

This is tracking bug to use Clang on Windows to build Tensorflow.

Benefits:
- Faster build (Clang does not suffer from `__forceinline` issue in pure MSVC #10521).
- Mostly compatible with MSVC (`clang-cl` understands MSVC command flags, macros and even imitates some of the MSVC's bugs).
- Cross compilation.
- Unlock some runtime optimizations such as [crc32c acceleration](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/hash/crc32c_accelerate.cc) that relies on `__builtin_cpu_supports` (compiler-rt is required).
- Better x64 code (according to one Chromium engineer)."
15987,"Documentation for placeholder does not explain when shape is (), [] or [None]","### System information

Not necessary.

### Describe the problem

The documentation for `placeholder` does not explain the case when its shape is `()`, `[]` or `[None]`.

### Possible solution

Add the explanations in [this SO  answer](https://stackoverflow.com/a/46941087/3924118) to the documentation of `placeholder`, including the example !!

  "
15985,Feature Request: Dense to Sparse and Dense to Sparse Tensor Ops,"I think it would be helpful if there is a dense_to_sparse op in Tensorflow for ops like `ctc_loss` that requires sparse labels. I'm not really sure where else it can be used aside from that but in case only `ctc_loss` uses it, I think it would help if dense labels can be passed into `ctc_loss` and do the conversion within."
15983,Feature request: Reduce learning rate on plateau,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes. But applies to stock examples as well.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.
- **TensorFlow installed from (source or binary)**:
Binary.
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.5.4
- **Bazel version (if compiling from source)**:
N/A
- **GCC/Compiler version (if compiling from source)**:
N/A
- **CUDA/cuDNN version**:
CUDA: V8.0.61
cuDNN: 6.0.21
- **GPU model and memory**:
GTX 1080Ti 11GB running driver version 384.98
- **Exact command to reproduce**:
N/A. However, I am using the Experiment, Estimator and Dataset APIs in order to do training.

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.
I would like to reduce the learning rate during training. However, I do not want to treat this as another tunable hyperparameter, so I would like this to be based on performance plateauing. In Keras, it is easy to implement learning rate reduction by monitoring the validation loss using the ReduceLROnPlateau callback function, but in TensorFlow this does not seem to be the case/easy. I certainly haven't found any implementation of this, so I propose this as a feature request.

Feel free to close this if this is not the correct forum.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
15981,tf.Estimator creates loss and loss_1 for eval/train,"- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Ubuntu 14.12
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
2.7

### Describe the problem

When using the tf.Estimator, the summary files save out summaries for the loss variable evaluated every checkpoint.  The summary for the training, is saved as 'loss_1' .  I got this tensorboard by running the ciphar10_estimator code located: https://github.com/tensorflow/models/tree/master/tutorials/image/cifar10_estimator/

This makes it difficult to compare the eval/train loss on the same graph in tensorboard.  What causes this naming issue and what can be done to fix it?

Thanks!

![screen shot 2018-01-09 at 12 02 53 pm](https://user-images.githubusercontent.com/22623388/34740703-311a2fc6-f535-11e7-9f88-ab16f65052ee.png)

  "
15980,While loop randomly doesn't evaluate tensors,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a tensor. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) but didn't get any response there.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```
System information:
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10 (Artful Aardvark)
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**:  1.4.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Execute the code block as a python file a few times

Thanks!
Nikita
  
  
  "
15978,No documentation for ConfigProto,"### System information

Not necessary in this case.

### Describe the problem

No documentation for the `ConfigProto` class in the TF website. Specifically, in neither of the following pages

- https://www.tensorflow.org/api_docs/python/tf/ConfigProto
- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/ConfigProto
- https://www.tensorflow.org/versions/master/api_docs/python/tf/ConfigProto
  
### Possible solutions

The following article https://www.tensorflow.org/tutorials/using_gpu contains info about `ConfigProto`. Either the docs for `ConfigProto` can be written based on that info or, at least, a link to that article should be added to the `ConfigProto` docs."
15977,Improve video input pipeline (using TFRecord files),"I am building a video input pipeline for DeepMind's [Kinetics dataset](https://deepmind.com/research/open-source/open-source-datasets/kinetics/) using TFRecord files. Since the dataset is large (200k videos) my TFRecord files store the frames as compressed JPG images; otherwise it would require too much space on disk. Each `tf.train.Example` has the following structure:

```
Example {
  'num_frames': tf.int64,
  'label': tf.int64,
  'frames/0001': tf.string,
  'frames/0002': tf.string,
  ...
}
```

Where all the frames store a JPG image as compressed bytes. Using `tf.data.TFRecordDataset` and `tf.image.decode_jpg` I am able to load the images and decode from JPG into `tf.uint8` tensors (full code can be found [here](https://github.com/tomrunia/TF_VideoInputPipeline/blob/master/kinetics/input_pipeline.py)):

```
def decode(serialized_example):
  
    # Prepare feature list; read encoded JPG images as bytes
    features = dict()
    features[""class_label""] = tf.FixedLenFeature((), tf.int64)
    for i in range(64):
        features[""frames/{:04d}"".format(i)] = tf.FixedLenFeature((), tf.string)

    # Parse into tensors
    parsed_features = tf.parse_single_example(serialized_example, features)

    # Decode the encoded JPG images
    images = []
    for i in range(64):
        images.append(tf.image.decode_jpeg(parsed_features[""frames/{:04d}"".format(i)]))

    # Pack the frames into one big tensor of shape (N,H,W,3)
    images = tf.stack(images)
    label  = tf.cast(parsed_features['class_label'], tf.int64)

    return images, label
```
Two things currently seem impossible with the current features of TFRecord files:

1. There seems to be no way to take a random sample of frames. The code example now takes the first 64 frames from the TFRecord, but what is often preferred is taking a random sample of consecutive frames. In one of my failed attempts I have tried to accomplish this along the lines of:

```
num_frames = tf.cast(parsed_features['num_frames'], tf.int64)
offset = tf.random_uniform(shape=(), minval=0, maxval=label, dtype=tf.int64)
```

2. The number of frames in the video example seems impossible to access in TensorFlow. It can be obtained using ` tf.train.Example.FromString` as given [here](https://stackoverflow.com/a/42402484/3419427), but that does not help me in this case. If this was possible I could just load all the video frames into a tensor (at increased cost...) and than use `tf.random_crop` to sample a random number of frames from the video. 

My overall question is whether the input pipeline for videos using TFRecord files can be improved? This needs to consider speed of reading data and compression options to limit file size for enormous  datasets. It would be convenient to directly use mp4 streams with TFRecord files, however decoding this is problably much slower than decoding JPG images (**EDIT**: this pull request is related: https://github.com/tensorflow/tensorflow/pull/13242)

Note that there are many ways to setup the data pipeline for videos. I have described some of them in this post on StackOverflow and motivated why I chose for TFRecord files. This post also describes the problem described here, so it may be informative: https://stackoverflow.com/questions/48101576/tensorflow-read-video-frames-from-tfrecords-file

Have I written custom code: N/A
OS Platform and Distribution: N/A
TensorFlow installed from: N/A
TensorFlow version: N/A
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce: N/A
  "
15976,iOS Camera Example not able to fit to iphone 6 screen ,"### System information
- **OS Platform and Distribution macOS High Sierra **:
- **TensorFlow installed from source**:
- **TensorFlow version 1.4**:

### Describe the problem
When I run the tensorflow camera example on xcode it runs fine but only in dimensions that fit an iphone 4. I changed the build settings, have turned on autolayout, and even tried messing around with the constraints. It seems like no matter what I try the viewcontroller won't get any larger.
"
15974,Estimator.predict always loads model checkpoint preventing partially loading checkpoints,"### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Windows 10
- **TensorFlow installed from (source or binary)**:
pip
- **TensorFlow version (use command below)**:
1.4
- **Python version**: 
3.5

### Describe the problem
When using Tensorflow's Estimator to do predictions, the Estimator always loads the checkpoint in the model_dir. As this is done after the model_fn is called, there is no way to partially load a checkpoint for predictions. For training, I partially load the initial checkpoint in the model_fn which works fine.

I also tried not specifying the model_dir for the Estimator. As the [documentation states](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#__init__), this results in the Estimator using a temporary folder. However, as the temporary folder does not contain a checkpoint, I get the error `Could not find trained model in model_dir`.

It looks like there is no way to only partially load a checkpoint for prediction. If so, please provide a way to do this. For me, this is important, because I have a large model with several outputs. For different datasets, some of the outputs have different sizes. However, some of them are the same for all datasets. That's why I want to load them with the same code and only partially, because I don't need to load and run the whole network for this prediction. "
15973,"How to change the model, without any change into android APK file","Hello,

I want to make an android app in this way, like we can change model file anytime in future, and it will not require any change into application code, means no need to generate new APK file of application, on any change into model.
In short I want to know, is there anyway to place model file other then assets folder. So that I can refer updated model file anytime from app.

Thanks,
Sumeet Guha."
15972,Maven Version of tensorflow Java API jar wrongly updated in Documentation,"### System information
Have I written custom code : N/A
OS Platform and Distribution : N/A
TensorFlow installed from : N/A
TensorFlow version : N/A
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A
Exact command to reproduce : N/A

### Describe the problem
https://www.tensorflow.org/install/install_java shows maven version as 1.4.1 

```
<dependency>
  <groupId>org.tensorflow</groupId>
  <artifactId>tensorflow</artifactId>
  <version>1.4.1</version>
</dependency>
```
However, this version is not available in public maven Repositories.
https://mvnrepository.com/artifact/org.tensorflow/tensorflow
Only versions  1.3.0 , 1.4.0, 1.4.0-rc0 and 1.5.0-rc0 are available.
Please correct documentation or release 1.4.1 Versions.

### Source code / logs
N/A
  "
15968,Imperfect implementation of tf.losses.mean_pairwise_squared_error,"### System information
- **TensorFlow version**: 1.4.0, 1.4.1, and 1.5.0-rc0 (checked)
- **Have I written custom code**: N/A
- **OS Platform and Distribution**: N/A
- **TensorFlow installed from**: N/A
- **Bazel version**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
The implementation of `tf.losses.mean_pairwise_squared_error` looks imperfect.
For example, as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error)
> For example, if `labels`=[a, b, c] and `predictions`=[x, y, z], there are three pairs of differences are summed to compute the loss: loss = [ ((a-b) - (x-y)).^2 + ((a-c) - (x-z)).^2 + ((b-c) - (y-z)).^2 ] / 3

let me put the following data as `labels` and `predictions`:
```
labels = tf.constant([[0., 0.5, 1.]])
predictions = tf.constant([[1., 1., 1.]])
tf.losses.mean_pairwise_squared_error(labels, predictions)
```
In this case, the result should be `[(0-0.5)^2+(0-1)^2+(0.5-1)^2]/3=0.5`, but tensorflow returns different value 0.3333333134651184.

### Suggestion to fix the source code
[tensorflow/python/ops/losses/losses_impl.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/losses/losses_impl.py)

If the loss function `mean_pairwise_squared_error` measures the differences between pairs of corresponding elements of `predictions` and `labels` as explained in [the API reference of the function](https://www.tensorflow.org/api_docs/python/tf/losses/mean_pairwise_squared_error), here is a simple patch:
> (lines 520-521 need to be changed as)
> `term1 = 2.0 * _safe_div(sum_squares_diff_per_batch, num_present_per_batch-1)`
and
> (lines 525-526 need to be changed as)
> `term2 = 2.0 * _safe_div(math_ops.square(sum_diff), math_ops.multiply(num_present_per_batch, num_present_per_batch-1))`
  "
15965,unsupported operand type(s) for /: 'Tensor' and 'float,"Tensorflow version is :  tensorflow-gpu (1.4.1)
Python version is:    Python 3.5.4 :: Anaconda custom (64-bit)

"
15964,DownloadfileTask Failed,"try projrct as https://www.tensorflow.org/mobile/android_build#android_sample_apps,but downloadtask failed,  then solve it ,may be you shoule change the 

> download-models.gradle  classpath 'de.undercouch:gradle-download-task:3.2.0' to 3.3.0"
15963,how to disable do_constant_folding  of OptimizerOptions for my custom ops ,"I write two ops named own_send and own_recv in c++ 
own_send write data to an buffer while wait the buffer flag is 0 ,then set the buffer flag to 1
own_recv read the buffer while wait the buffer flag is 1 and set the flag to 0
but when I run my model I got the error own_recv is waiting the buffer flag is 1 but the flag is 0 when own_send is done rightly.
I finally found the GraphOptimizer class would run own_recv  previously ends with my flag is changed before my model really process.
I run the code following get the error own_recv is always  waiting the flag is 1 thile the flag is 0.
`input = [tf.ones([3136,1024])]`
`send = own_send(input)`
`recv = own_recv([tf.ones([1])])`
`sess = tf.Session()`
`sess.run(recv)`

then I try to run with opt_level is L0, it works:

`input = [tf.ones([3136,1024])]`
`send = own_send(input)`
`recv = own_recv([tf.ones([1])])`
`config = tf.ConfigProto(graph_options=tf.GraphOptions(optimizer_options=tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)))`
`sess = tf.Session(config = config)`
`sess.run(recv)`

I want to know how to disable the optimizer options just for my own custiom ops leaving the other ops optimied
Is there some path to disable option for specified ops ？
something like  add some opt in my BUILD ?

### System information
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux  Ubuntu 14.04.4 LTS 
- **TensorFlow installed from (source or binary)**:source
- **TensorFlow version (use command below)**:1.4.0-rc1
- **Python version**: Python 2.7.6
- **Bazel version (if compiling from source)**: 0.5.4"
15962,Model diverges with NaN if the class label exceeds the expected number of classes,"Tried in Tensorflow v1.4

Have I written custom code - Yes
OS Platform and Distribution - CentOS Linux release 7.4.1708
TensorFlow installed from - Not from source
TensorFlow version - v1.4.1
GPU model and memory - Tesla P100 16GB

If the class label exceeds the expected number of classes (dimensions of losses and given label won't match), TensorFlow simply errors out with `loss diverged with a NaN`. Ideally it should warn the user that the class label provided exceeded the expected number of classes.

It is easy to hit this error and spend lot of time in debugging. For Ex: If there are 101 number of classes, TensorFlow expects the labels to be in `[0,100]`, but if the user has labels `[1,101]`, any attempts to train would simply error out with `loss diverged with a NaN`.

Can we have some warning specifically for this case?
  "
15958,Undefined Symbol gen_parsing_ops_py_wrappers_cc when building pip package,"
### System information

greg@salt:~/code/tensorflow$ more tf_env.txt 

== cat /etc/issue ===============================================
Linux salt 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
(configure set to gcc-6)
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux salt 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.0)
protobuf (3.5.1)
s2clientprotocol (1.1)
tensorflow-gpu (1.4.0)
tensorflow-serving-api (1.4.0)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""tensorflow/python/pywrap_tensorflow.py"", line 25, in <module>
    from tensorflow.python.platform import self_check
ImportError: No module named platform

== env ==========================================================
LD_LIBRARY_PATH /usr/local/cuda/lib64:/opt/tensorlibs/lib
DYLD_LIBRARY_PATH /usr/local/cuda/lib64:/opt/tensorlibs/lib

== nvidia-smi ===================================================
Mon Jan  8 13:25:47 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 387.26                 Driver Version: 387.26                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |
| 24%   34C    P8     9W / 151W |    596MiB /  8112MiB |      3%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0       988      G   /usr/lib/xorg/Xorg                           373MiB |
|    0      1384      G   /usr/bin/compiz                              116MiB |
|    0      8602      G   ...-token=09D12A9300046A211380F83BAD0A60C6    46MiB |
|    0     15983      G   ...-token=29A09CA037A6A6610BD563090AA5B5DA    56MiB |
+-----------------------------------------------------------------------------+

== cuda libs  ===================================================
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-9.1/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.1/doc/man/man7/libcudart.7
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.1/targets/x86_64-linux/lib/libcudart.so.9.1.85
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.103


### Describe the problem
Build command below fails w/ undefined symbol.  
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures

ERROR: /home/greg/code/tensorflow/tensorflow/python/BUILD:1398:1: Executing genrule //tensorflow/python:parsing_ops_pygenrule failed (Exit 127): bash failed: error executing command 
  (cd /home/greg/.cache/bazel/_bazel_greg/ca148a14f6c24000015970c3c0a435f7/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda \
    CUDNN_INSTALL_PATH=/usr/local/cuda-9.1 \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc-6 \
    LD_LIBRARY_PATH=/usr/local/cuda/lib64:/opt/tensorlibs/lib \
    PATH=/opt/ant/bin:.:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/usr/lib/jvm/java-9-oracle/bin:/usr/lib/jvm/java-9-oracle/db/bin:/opt/gephi/bin:/opt/eclipse:/opt/gradle/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3/dist-packages \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=9.1 \
    TF_CUDNN_VERSION=7 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
  /bin/bash bazel-out/k8-py3-opt/genfiles/tensorflow/python/parsing_ops_pygenrule.genrule_script.sh)
bazel-out/host/bin/tensorflow/python/gen_parsing_ops_py_wrappers_cc: symbol lookup error: bazel-out/host/bin/tensorflow/python/gen_parsing_ops_py_wrappers_cc: undefined symbol: _ZN10tensorflow17ParseExampleAttrs10FinishInitEv
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 63.574s, Critical Path: 19.07s
FAILED: Build did NOT complete successfully


### Source code / logs
greg@salt:~/code/tensorflow$ dpkg -l | grep cuda
ii  cuda                                       9.1.85-1                                    amd64        CUDA meta-package
ii  cuda-9-1                                   9.1.85-1                                    amd64        CUDA 9.1 meta-package
....
ii  libcudnn7                                  7.0.5.15-1+cuda9.1                          amd64        cuDNN runtime libraries
ii  libcudnn7-dev                              7.0.5.15-1+cuda9.1                          amd64        cuDNN development libraries and headers
ii  libcudnn7-doc                              7.0.5.15-1+cuda9.1                          amd64        cuDNN documents and samples


  "
15957,Switching branch and run ./configure does not regenerate spec.json,"When building from source with TensorFlow and switch to another branch, error returned even if I rerun `./configure`:

```
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
ubuntu@ubuntu:~/tensorflow$ git checkout -b test
ubuntu@ubuntu:~/tensorflow$ ./configure
..........
ubuntu@ubuntu:~/tensorflow$ bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
..........
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (252 packages loaded).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/BUILD:1671:1: Executing genrule //tensorflow/core:version_info_gen failed (Exit 1)
Traceback (most recent call last):
  File ""tensorflow/tools/git/gen_git_source.py"", line 284, in <module>
    generate(args.generate)
  File ""tensorflow/tools/git/gen_git_source.py"", line 229, in generate
    (old_branch, new_branch))
RuntimeError: Run ./configure again, branch was 'refs/heads/master' but is now 'refs/heads/test'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 9.025s, Critical Path: 0.30s
FAILED: Build did NOT complete successfully
```


I think the issue is that `spec.json` is not updated when running `./configure`


```
ubuntu@ubuntu:~/tensorflow$ cat /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/local_config_git/gen/spec.json
{
  ""path"": ""/home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/org_tensorflow/"", 
  ""git"": true, 
  ""branch"": ""refs/heads/master""
}
ubuntu@ubuntu:~/tensorflow$ 
```

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: master
- **Python version**:  2.7.12
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.5) 
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

```sh
git checkout -b test
./configure
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

  "
15956,Tensorflow-lite breaks Android module. ,"Inserting Tensorflow lite inside an android module/library like this:

```
apply plugin: 'com.android.library'
[some code]

dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])

    implementation 'org.tensorflow:tensorflow-lite:+'
}
```

Results in this error when including the module and trying to running for Android API 19:

> Error:Error converting bytecode to dex:
> Cause: com.android.dex.DexException: Multiple dex files define LR;

The module I am trying to include contains no classes, files  or other dependencies. All it has I a dependency of Tensorflow-lite. If I insert tensorflow lite directly inside the app, the problem goes away. 

The problem doesn't happen in API 21+, and it only happens for API <= 19

I am using android gradle plugin 3.0.1, but I also trying with 3.1.0-alpha-04/5/6/7. 

Is there a solution or work around to this problem?
"
15954,TypeError: Input 'split_dim' of 'Split' Op has type float32 ,"Hi,

I use Tensorflow 1.5.0rc0  on windows (Python 3.5.2 )and I have this issue: 
TypeError: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32. I have seen some answers for older versions of tensor flow but is there any fix for the new version?

"
15953,tf.Print() re/direction,"## Feature Request

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: latest (1.5.0-rc0)
- **Python version**:  N/A
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
https://github.com/tensorflow/tensorflow/blob/a77096897f1a8068ca8f57ffb6e3d9e28508cc27/tensorflow/core/platform/default/logging.cc#L89
It would be nice to be able to direct the string to a log file instead of `stderr` (following the **TODO** in the code)

  "
15952,LSTM in eager is 15 slower than in tensorflow on CPU,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.3.0-rc1-6744-gf99275a 1.6.0-dev20180105
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: 

```
git clone https://gist.github.com/idavydov/1060b3fae833af436cdad11913c9e7e1
cd 1060b3fae833af436cdad11913c9e7e1
time python lstm_test_tensorflow.py
time python lstm_test_eager.py
```

### Describe the problem
LSTM in eager seeems to work 15 times slower than LSTM in tensorflow, when running on CPU (`tf.nn.dynamic_rnn`).  Tensorflow time: 9s, eager time: 149s.

### Source code / logs
## lstm_test_tensorflow.py
```
#!/usr/bin/env python
import numpy as np
import tensorflow as tf


# use 1 CPU
conf=tf.ConfigProto(
    intra_op_parallelism_threads=1,
    inter_op_parallelism_threads=1)

n_iter = 100

n_layers = 2

batch_size = 32
seq_len = 1000
input_dim = 7
data = np.random.uniform(size=(batch_size, seq_len, input_dim))

x = tf.placeholder(tf.float32, shape=(batch_size, seq_len, input_dim))

cells = [tf.contrib.rnn.LSTMCell(input_dim) for _ in range(n_layers)]
multicell = tf.contrib.rnn.MultiRNNCell(cells)
rnn_outputs, final_state = tf.nn.dynamic_rnn(multicell, x, dtype=tf.float32)

init = tf.global_variables_initializer()

with tf.Session(config=conf) as sess:
    sess.run(init)
    for _ in range(n_iter):
        sess.run(rnn_outputs, {x: data})
```

## lstm_test_eager.py
```
#!/usr/bin/env python
import tensorflow as tf
import tensorflow.contrib.eager as tfe

# use 1 CPU
conf=tf.ConfigProto(
    intra_op_parallelism_threads=1,
    inter_op_parallelism_threads=1)

tfe.enable_eager_execution(conf)

n_iter = 100

n_layers = 2

batch_size = 32
seq_len = 1000
input_dim = 7
data = tf.random_uniform((batch_size, seq_len, input_dim))

cells = [tf.contrib.rnn.LSTMCell(input_dim) for _ in range(n_layers)]
multicell = tf.contrib.rnn.MultiRNNCell(cells)


for _ in range(n_iter):
    tf.nn.dynamic_rnn(multicell, data, dtype=tf.float32)
```"
15951,[Build] Source build at HEAD generating XLA erros on Mac OS,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS High Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: HEAD@a770968
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 9.0.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: Build procedures on doc optimized for native arch and XLA enabled.

### Describe the problem
Building TensorFlow on Mac OS with XLA enabled and configuration given above optimized for native arch and CPU only yields the following errors:
```
ERROR: /Users/adriano/MachineLearning/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:522:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 1)
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:42:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:65:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
In file included from tensorflow/compiler/xla/service/cpu/runtime_fft.cc:21:
./tensorflow/compiler/xla/service/cpu/runtime_fft_impl.h:106:30: error: implicit instantiation of undefined template 'std::__1::array<long long, 3>'
  const std::array<int64, 3> fft_shape = {
                             ^
/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/include/c++/v1/__tuple:222:64: note: template is declared here
template <class _Tp, size_t _Size> struct _LIBCPP_TEMPLATE_VIS array;
                                                               ^
3 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1997.286s, Critical Path: 88.18s
FAILED: Build did NOT complete successfully

```
"
15949,Building TensorFlow on Windows: patch and rm,"### System information
- **Have I written custom code**: Yes, provided below.
- **OS Platform and Distribution**: Windows 10 1709, Build 16299.192
- **TensorFlow installed from (source or binary)**: Binary, Attempting source build of master
- **TensorFlow version**: 1.4.0
- **Python version**: 3.6
- **Bazel version**:  0.9.0
- **GCC/Compiler version**:  MSYS2 Shell, GCC unknown.
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A, CPU is i7-8550U, 8 GB memory
- **Exact command to reproduce**: Any `bazel build` on Windows. Please see Description.

### Describe the problem
Building TensorFlow on Windows has been a struggle with compatibility due to the fact that for many, MSYS will not run `patch` when installed from the MSYS2 shell. I have found a reliable way to resolve the issue: using Choco to install `patch`, moving patch.exe to a folder FOLDERNAME within its default directory, and then running %FOLDERNAME%/patch.exe with the flag `--binary` (to use CR LF line breaks) with a custom batch script compiled into a executable.

`bazel build` now completes `patch` commands without issue. But as it often is, another hurdle exists to the finish line. Bazel now attempts to recursively force remove a file using `rm -rf`, which obviously does not exist as a package in Choco as a bash command. MSYS will run it, but not from the command line.

Is there any way to get around the use of `rm`, or make a compatible solution for Windows using `del`?

I have ensured that #15829 has been installed. Still fails

If this is better left to the Bazel developers, please close this issue. 

### Source code

#### patch.bat
``` sh
start C:\ProgramData\chocolatey\lib\patch\tools\bin\%FOLDERNAME%\patch.exe --binary
exit
```

### Logs
``` sh
C:\tensorflow>bazel build --config=mkl --config=monolithic -c opt --copt=-march=native --copt=-mmmx --copt=-msse --copt=-msse2 --copt=-msse3 --copt=-mssse3 --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-mavx2 --copt=-maes --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package
#ERROR: C:/tensorflow/tensorflow/python/BUILD:4646:1: no such package '@cython//': Traceback (most recent call last):
        File ""C:/tensorflow/third_party/repo.bzl"", line 86
                _apply_delete(ctx, ctx.attr.delete)
        File ""C:/tensorflow/third_party/repo.bzl"", line 68, in _apply_delete
                _execute_and_check_ret_code(ctx, cmd)
        File ""C:/tensorflow/third_party/repo.bzl"", line 44, in _execute_and_check_ret_code
                fail(""Non-zero return code({1}) when ...))
Non-zero return code(127) when executing 'C:\msys64\usr\bin\bash.exe -c rm -rf C:/users/eric/appdata/local/temp/_bazel_eric/x1e5egqw/external/cython/BUILD.bazel':
Stdout:
Stderr: /usr/bin/bash: rm: command not found
 and referenced by '//tensorflow/python:framework/fast_tensor_util.pyx_cython_translation'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 61.324s
FAILED: Build did NOT complete successfully (92 packages loaded)
```

  "
15948,Error converting to .tflite Using Toco,"### System information
- **Have I written custom code**: Yes. See network definition code [here](https://gist.github.com/OluwoleOyetoke/30f2cac788042c495f1ae34a6b742a1d). For complete project, see [here](https://github.com/OluwoleOyetoke/Computer_Vision_Using_TensorFlowLite)
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Yes
- **TensorFlow version (use command below)**: tensorflow (1.4.0)
- **Python version**:  Python 2.7.12
- **Bazel version (if compiling from source)**: Bazel 0.9.0
- **GCC/Compiler version (if compiling from source)**: gcc 5.4.1 20160904
- **CUDA/cuDNN version**: Null
- **GPU model and memory**: Null
- **Exact command to reproduce**: bazel-bin/tensorflow/contrib/lite/toco/toco --input_format=TENSORFLOW_GRAPHDEF --input_file=$1 --output_format=TFLITE --output_file=$2 --inference_type=$3 --#input_type=$4 --input_arrays=$5 --output_arrays=$6 --inference_input_type=$7 --input_shapes=1,227,227,3

### Problem Description, Source Code and Logs
When I try to use Toco to convert my Custom AlexNet Model from TensorFlow to TensorFlowLite, I repeatedly get a dimensions error as shown below

    F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)

Here is how I call toco:

    bazel-bin/tensorflow/contrib/lite/toco/toco \
    --input_format=TENSORFLOW_GRAPHDEF \
    --input_file=/tmp/output_graph.pb \
    --output_format=TFLITE \
    --output_file=/tmp/my_model.lite \
    --inference_type=FLOAT \
    --inference_input_type=FLOAT \
    --input_arrays=input_layer \
    --output_arrays=classes_tensor\
    --input_shapes=1,227,227,3

Here is my terminal print out during the operation: 

    INFO: Analysed 0 targets (4 packages loaded).
    INFO: Found 0 targets...
    INFO: Elapsed time: 5.267s, Critical Path: 0.03s
    INFO: Build completed successfully, 1 total action
    2018-01-05 10:24:23.011483: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
    2018-01-05 10:24:25.853112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: IsVariableInitialized
    2018-01-05 10:24:25.853197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RefSwitch
    2018-01-05 10:24:25.853241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomShuffleQueueV2
    2018-01-05 10:24:25.853268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QueueDequeueUpToV2
    2018-01-05 10:24:26.207160: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 64 operators, 90 arrays (0 quantized)
    2018-01-05 10:24:27.327055: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 15 operators, 33 arrays (0 quantized)
    2018-01-05 10:24:27.327262: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 15 operators, 34 arrays (0 quantized)
    2018-01-05 10:24:27.327356: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)
    /home/olu/Dev/scratch_train_sign/freeze_graph_tf.sh: line 28:  8881 Aborted 

I went into the propagate_fixed_sizes.cc file, and around line line 982 I found this comment below

    // The current ArgMax implementation only supports 4-dimensional inputs with
    // the last dimension as the axis to perform ArgMax for.

The only place in my training code where I used ArgMax is as below:

     predictions = { ""classes"": tf.argmax(input=logits, axis=1, name=""classes_tensor""), ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"") }

The failure messaged printed out seem not to be sufficient enough for me to understand what exactly the problem is. My trained TensorFow Model is fine and I have been able to run inferences on it, however, converting these saved model to TFLite doesn't work. Could this mean the current TFLite version does not support the conversion of some custom TF models yet?. Please I will be glad to know what exactly I may be doing wrong. 

Thank you 

  "
15945,DataLossError when loading saved model from r1.4 (Unable to read file ... failed to seek to header entry),"I have a few saved models stored, these were built using _version 1.2.0-rc1_ 
When I try to load any of these saved models using _version 1.4.1_, I get the following error:
```
INFO:tensorflow:Restoring parameters from b'../models/export_output/1510150323/variables/variables'
2018-01-08 19:42:35.838751: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
--------quite a few entries of the above log-------------
2018-01-08 19:42:36.134122: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
2018-01-08 19:42:36.134144: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
2018-01-08 19:42:36.134177: W tensorflow/core/framework/op_kernel.cc:1192] Data loss: Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
Traceback (most recent call last):
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1323, in _do_call
    return fn(*args)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1302, in _run_fn
    status, run_metadata)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py"", line 473, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.DataLossError: Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
	 [[Node: save_1/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2_159/tensor_names, save_1/RestoreV2_159/shape_and_slices)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/media/Files/Research/FoodClassification/deployment/deployment.py"", line 308, in _main
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], EXPORT_MODEL_DIR)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 226, in load
    saver.restore(sess, variables_path)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1666, in restore
    {self.saver_def.filename_tensor_name: save_path})
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 889, in run
    run_metadata_ptr)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1120, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1317, in _do_run
    options, run_metadata)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1336, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.DataLossError: Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
	 [[Node: save_1/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2_159/tensor_names, save_1/RestoreV2_159/shape_and_slices)]]

Caused by op 'save_1/RestoreV2_159', defined at:
  File ""<stdin>"", line 1, in <module>
  File ""/media/Files/Research/FoodClassification/deployment/deployment.py"", line 308, in _main
    tf.saved_model.loader.load(sess, [tf.saved_model.tag_constants.SERVING], EXPORT_MODEL_DIR)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py"", line 216, in load
    saver = tf_saver.import_meta_graph(meta_graph_def_to_load, **saver_kwargs)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/training/saver.py"", line 1810, in import_meta_graph
    **kwargs)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py"", line 660, in import_scoped_meta_graph
    producer_op_list=producer_op_list)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 313, in import_graph_def
    op_def=op_def)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/home/amsha/virtualenv/tf14-no-gpu-p3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

DataLossError (see above for traceback): Unable to read file (../models/export_output/1510150323/variables/variables.index). Perhaps the file is corrupt or was produced by a newer version of TensorFlow with format changes (failed to seek to header entry): corrupted compressed block contents
	 [[Node: save_1/RestoreV2_159 = RestoreV2[dtypes=[DT_FLOAT], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2_159/tensor_names, save_1/RestoreV2_159/shape_and_slices)]]
```
I was able to load the same models in _version 1.2.0-rc1_.
I tried building a new saved model using _version 1.4.1_ , that I was able to load in both versions without any problems. 

The versions I tested were both built, without cuda. 
OS: arch linux"
15943,add_n: issue with IndexedSlices,"inside function add_n, line 2117, shouldn't
""if not all(isinstance(x, ops.Tensor) for x in inputs):"" check also whether x is IndexedSlices instead of merely Tensor? i.e. replace the statement with:

if not ( (all(isinstance(x, ops.Tensor) for x in inputs)) | (all(isinstance(x, ops.IndexedSlices) for x in inputs)) ):

Thanks!


"
15942,build_all_ios.sh:  x86_64 compilation failed.,"when i compile build_all_ios.sh:   

****tensorflow 1.1 :**   is OK.... build success....**

when i load ""xxxx.pb"" model,  error:

Invalid argument: No OpKernel was registered to support Op 'Mul' with these attrs.  Registered devices: [CPU], Registered kernels:
  device='CPU'; T in [DT_FLOAT]
	 [[Node: UpSample2D_2/mul = Mul[T=DT_INT32](UpSample2D_2/mul/x, UpSample2D_2/Const)]]

How to solver ?

**tensorflow 1.4 :**   

make: *** [/Users/open/Downloads/tensorflow-1.4/tensorflow/contrib/makefile/gen/host_obj/tensorflow/core/lib/random/distribution_sampler.o] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'i386 compilation failed.'
i386 compilation failed.
+ exit 1
## 
**tensorflow 1.5 :**   

ld: symbol(s) not found for architecture x86_64
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make: *** [/Users/open/Downloads/tensorflow-1.5/tensorflow/contrib/makefile/gen/bin/ios_X86_64/benchmark] Error 1
+ '[' 2 -ne 0 ']'
+ echo 'x86_64 compilation failed.'
x86_64 compilation failed.
+ exit 1


How to solver ?

**python:3.6.3  mac OS:  10.12.6**
"
15941,Import Error: No module named '_pywrap_tensorflow',"On running the following command: import tensorflow I get an error:

`C:\Users\Neerav>python
Python 3.5.0 (v3.5.0:374f501f4567, Sep 13 2015, 02:27:37) [MSC v.1900 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 60, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 18, in swig_import_helper
    fp, pathname, description = imp.find_module('_pywrap_tensorflow', [dirname(__file__)])
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\imp.py"", line 296, in find_module
    raise ImportError(_ERR_MSG.format(name), name=name)
ImportError: No module named '_pywrap_tensorflow'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\__init__.py"", line 54, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""C:\Users\Neerav\AppData\Local\Programs\Python\Python35\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 20, in swig_import_helper
    import _pywrap_tensorflow
ImportError: No module named '_pywrap_tensorflow'


Error importing tensorflow.  Unless you are using bazel,
you should not try to import tensorflow from its source directory;
please exit the tensorflow source tree, and relaunch your python interpreter
from there.`


I have the following system features:

windows 64 bit
python 3.5.0 64 bit
Nvidia computing toolkit/CUDA/v8.0./(the cuDNN version 6.0)
all of them are added to my path location also which is: Python\Python35\Scripts
i have tensorflow in Python\Python35\Lib\site-packages\tensorflow
I even have a _pywrap_tensorflow.so file and pywrap_tensorflow.py"
15939,Slim VGG losses increase gradually with default training configuration,"Hi, when I try to train imagenet with slim vgg network with default configuration,
The loss increases gradually from ~0.1 to over 10000. 
I am not even able to debug this issue because, all tensors losses are encapsulated inside slim.
Is there any way to debug this issue? "
15938,An easy problem about tensorflow tf.reduce_mean op,"I know... there might not be a suitable palce to ask this question, but I really hope someone cloud help me.

i want to use the ""tf.reduce_mean"" to obtain the mean of an array (ignore the zeros element)
eg:
    data = [[1,2,3],[4,5,6],[0,0,0]]   
    i want to obtain mean= [2.5, 3.5, 4.5]  
    but  tf.reduce_mean op gets the mean=[1.6, 2.3, 3]

Thank you very much!
  "
15937,"tensorflow/contrib/lite/toco/dump_graphviz.cc: In function 'void toco::DumpGraphviz(const toco::Model&, std::__cxx11::string*)': tensorflow/contrib/lite/toco/dump_graphviz.cc:329:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]    for (int op_index = 0; op_index < ops_to_dump.size(); op_index++) {                                    ^ Target //tensorflow/contrib/lite/toco:toco failed to build","tensorflow/contrib/lite/toco/dump_graphviz.cc: In function 'void toco::DumpGraphviz(const toco::Model&, std::__cxx11::string*)':
tensorflow/contrib/lite/toco/dump_graphviz.cc:329:35: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int op_index = 0; op_index < ops_to_dump.size(); op_index++) {
                                   ^
Target //tensorflow/contrib/lite/toco:toco failed to build"
15936,bazel ubuntu 16.04 bazel build tensorflow/python/tools:optimize_for_inference,"./tensorflow/core/platform/macros.h:78:30: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (x)
                              ^
./tensorflow/core/util/tensor_format.h:340:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attributes.size())
   ^
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int]':
./tensorflow/core/util/tensor_format.h:381:54:   required from here
./tensorflow/core/util/tensor_format.h:355:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:78:30: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (x)
                              ^
./tensorflow/core/util/tensor_format.h:355:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
ERROR: /home/zyx/Desktop/code/tensorflow-master/tensorflow/contrib/lite/toco/BUILD:169:1: C++ compilation of rule '//tensorflow/contrib/lite/toco:graph_transformations' failed (Exit 1)
In file included from external/gemmlowp/public/../internal/dispatch_gemm_shape.h:20:0,
                 from external/gemmlowp/public/gemmlowp.h:19,
                 from ./tensorflow/contrib/lite/kernels/internal/common.h:48,
                 from ./tensorflow/contrib/lite/toco/runtime/types.h:18,
                 from ./tensorflow/contrib/lite/toco/model.h:25,
                 from ./tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.h:23,
                 from tensorflow/contrib/lite/toco/graph_transformations/resolve_tensorflow_matmul.cc:20:
external/gemmlowp/public/../internal/../internal/kernel_default.h:88:2: error: #error ""SIMD not enabled, you'd be getting a slow software fallback. Consider enabling SIMD extensions (for example usin
g -msse4 if you're on modern x86). If that's not an option, and you would like to continue with the slow fallback, define GEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK.""
 #error \
  ^
Target //tensorflow/python/tools:optimize_for_inference failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 637.451s, Critical Path: 54.99s
FAILED: Build did NOT complete successfully"
15935,android CameraActivity: Exception!,"tensorflow: CameraActivity: Exception!
java.lang.RuntimeException: Error initializing box priors from file:///android_asset/multibox_location_priors.txt
                                                                     at org.tensorflow.demo.TensorFlowMultiBoxDetector.create(TensorFlowMultiBoxDetector.java:121)
                                                                     at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:146)
                                                                     at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:120)
                                                                     at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1189)
                                                                     at android.os.Handler.dispatchMessage(Handler.java:102)
                                                                     at android.os.Looper.loop(Looper.java:135)
                                                                     at android.app.ActivityThread.main(ActivityThread.java:5372)
                                                                     at java.lang.reflect.Method.invoke(Native Method)
                                                                     at java.lang.reflect.Method.invoke(Method.java:372)
                                                                     at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1117)
                                                                     at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:810)
"
15933,Tensorflow 1.4.1 on Linux (CentOS-7.4) and Tensorflow 1.4.1 on MacOSX producing *very* different results in image creation simulation.," 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No.

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  
Linux CentOS-7.4 and MacOSx 10.10.5

- **TensorFlow installed from (source or binary)**: Both; Installed from binary, then, built and installed from source. Same behaviour on each install.

- **TensorFlow version (use command below)**:
Tensorflow 1.4.0 and Tensorflow 1.4.1

- **Python version**: 
2.7.14 (installed from binary, and then built and installed from source

- **Bazel version (if compiling from source)**:
Bazel 0.9.0.  Source built and installed successfully, Python .whl file built & installed successfully.

- **GCC/Compiler version (if compiling from source)**:
Xcode7.2.1 and the Gnu gFortran, 5.2.  (needed gFortran for SciPy install.  All installs OK.)

- **CUDA/cuDNN version**:
N/A - compiled and running CPU versions only for now.

- **GPU model and memory**:

- **Exact command to reproduce**:
(See supplied test program - based on the Laplace PDE (""Raindrops on Pond"") simulation example
from Tensorflow Tutorial)
 
Description of Problem:
 I've run into a curious situation.  I am getting very different behaviour in Tensorflow 1.4.1 on Linux and Tensorflow 1.4.1 on MacOSX, in straightforward image-generation simulation, based on the ""Raindrops on a Pond"" (Laplace PDE) example from the Tensorflow Tutorial.

I must stress that *both* Tensorflow installations seem to be 100% correct, and operate other tests correctly, producing the same numeric results for simple models.

I have also built Tensorflow 1.4.1 completely from source, and the Python 2.7.14 as well, on the MacOSX (MacBook) machine, in order to build the Python using ""--enable-unicode=ucs4"", since that was one difference I was able to find, between the two version.  But even with the Macbook now running exactly the same Python 2.7.14 as the Linux box, I am still getting wildly divergent evoluationary behaviour as when I iterate the simple simulation.   The numbers just zoom off in very different directions on each machine, and the generated images show this.  

On the MacOSX, the simulation evolves very quickly to a pure white canvas (all ""255""s), but on the Linux platform, the image grows more complex, with the generated numbers bifurcating between large negative and large positive - and hence when np.clip-ed, to range 0-255, show a complex moire-style pattern.

I have confirmed all related libraries and packages seem to be the same versions.  The difference seems to be in the operation of Tensorflow.  

This seems pretty serious, as each platform is Intel.  The Linux box (CentOS-7.4) is Core-i3, while the Macbook is Core-i5.  But both are 64-bit, and both Tensorflow installations seem to be correct.  I have tried both the binary version, and then built a complete local version of Tensorflow 1.4.1 for the Macbook from source.  Both seem to be Ok, and operate correctly.  The Linux version of Tensorflow 1.4.0 was installed from binary appears to be operating correctly, albeit differently, but just for this one program.

When the sample program runs, it will display fourteen 400x400 images, as well as the numeric values of the row-20 of the ""a"" array (400 numbers).   The program can be started from an Xterm shell window, with ""python LapTest.py"".  It does not need Jupyter or IPython.  With SciPy loaded, the images are rendered as .PNG files on both platforms, using Preview on the MacOSX MacBook, and ImageMagick on the CentOS-7.4 Linux box.   Program runs fine to completion, and all looks ok on both machines.

But the results - even with the simple initial pseudo-random conditions - evolve completely differently, and consistantly.  The Macbook version of Tensorflow 1.4.1 goes to a pure white screen, while the LInux Tensorflow 1.4.1 configuration evolves to a complex, chaotic, moire-pattern.  

Leaving aside the question of even which machine is ""correct"", the expected result is of course that both machines should at least show clear evidence of similar behaviour.

No change was made to the test program, ""LapTest.py"", from one machine to the other.   The different behaviour is not related to how the images are displayed, which is working fine on both platforms.   A copy of this simple program is provided.   I have removed or commented out the IPython/Jupyter dependent code, so this program can be run on plain vanilla Python 2.7.14, as long the appropriate packages (tensorflow, numpy, scipy, PIL (Pillow version), matplotlib, imageio ...) are available

Example of Source code to demostrate behaviour:     LapTest.py 
``` 
#-------------------------------------------------------------------------------
# Prgm: LapTest.py
#
# --- the Tensorflow LaPlace Image example (Uses PIL(Pillow ver.), and numpy)
# --- updated for TensorFlow 1.4.1 running on CentOS-7.4 & Python 2.7.14
#     compiled (configured, actually) with the ""--enable-unicode=ucs4"" option
#                                             (Python compile default is ucs2)
#                                             (which caused TensorFlow 1.4 to)
#                                             (fail to load. Building Python )
#                                             (with ucs4, => pip can install )
#                                             (TensorFlow 1.4.0 successfully.)
#
# --- This version of program tested on: MacOSX 10.10.5. (Yosemite)
# --- LapTest.py on Linux (CentOS-7.4), and LapTest.py on MacOSX, with Tensorflow-1.4.1 and
#     Python 2.7.14 (with ucs4 enabled on both Python versions), show *very*
#     different behaviour, and produce very different results.
#     Note: CentOS-7.4 is using Linux kernel: 4.14.9-1el7.elrepo.x86_64    
#
# --- Import various libraries for simulation
import tensorflow as tf
import numpy as np
import scipy.misc
import imageio
import os
import sys
import subprocess
import PIL
import time    


# --- Import for visualization and jpeg encoder  
import matplotlib
matplotlib.rcParams[""backend""]= 'TkAgg'
from matplotlib import pyplot as plt
# from PIL import Image, ImageDraw
from io import BytesIO
#  from IPython.display import clear_output, Image, display

#--- we need this to get a sane for-loop...
def jump_range(start, end, step):
    while start <= end:
        yield start
        start += step

# --- function for displaying state of the pond's surface as an image
def DisplayArray(a, fmt='jpeg', rng=[0,1]):
  global proc
  # proc.kill() 
  # """"""Display an array as a picture. """"""
  a = (a - float(rng[0]))/float(rng[1] - rng[0])*37
  amod = np.clip(a, 0, 255)
  a = np.uint8(amod)
#  a = np.clip(a, 0, 255) 
#  a = np.uint8(a) 
#  np.clip(a, 0, 255, out=a )
#  a = a.astype('uint8')
  print "" ""
  print "" ----------- This is a: => row 20  ------------""
  print a[20]
  print "" ----------------------------------------------""
  f = BytesIO()
  # --- this is the cool, realtime thing that runs in Jupyter-IPython Notebook
  PIL.Image.fromarray(a).save(f,fmt)
  # --- clear_output(wait = True)  --- only for IPython
  # display(Image(data=f.getvalue()))
  # --- write the image
  # --- write the simulation images to .jpg files
  scipy.misc.imsave(""tensor.jpg"", a)
  pic = PIL.Image.open(""tensor.jpg"")
  # --- new approach... use subprocess, wait for time(2) then kill it
  # proc = subprocess.Popen([""display"", ""./tensor.jpg""])
  # time.sleep(0.5)
  pic.show()
  # clear_output(wait=True)
  # --- this line below doesn't work outside of the Jupyter environment...
  # display(Image(data=f.getvalue()))
  #
  # pic.close()  <--- does not work to close image.  Just removes the pointer to image in memory
    
def DisplayArrayToFile(a, fmt='jpeg', rng=[0,1]):
  # """"""Display an array as a picture to a file... """"""
  a = (a - rng[0])/float(rng[1] - rng[0])*37
  a = np.uint8(np.clip(a, 0, 255))
  f = BytesIO()
  # --- this is the cool, realtime thing that runs in Jupyter-IPython Notebook
  PIL.Image.fromarray(a).save(f,fmt)
  # clear_output(wait = True)
  # display(Image(data=f.getvalue()))
  # --- write the image
  # --- this is my stuff to write the simulation images to .jpg files
  #scipy.misc.imsave (""tensor_new.jpg"", a)
  imageio.imwrite(""tensor_new.jpg"", a)
  # --- image = PIL.Image.open(""tensor_new.jpg"")
  # --- image.show()
  # clear_output(wait=True)
  # display(Image(data=f.getvalue()))
  #
 
# --- make print stmt print the whole array... (not just part of it...)
np.set_printoptions(threshold=np.nan)
  
# --- make interactive session for testing - can use regular session also
sess = tf.InteractiveSession()
# sess = tf.Session()

# --- computational functions go here... once we get jpeg pic working
def make_kernel(a):
  """"""Transform a 2D array into a convolutional kernel """"""
  a = np.asarray(a)
  a = a.reshape(list(a.shape) + [1,1])
  return tf.constant(a, dtype=1)


def simple_conv(x, k):
  """""" A simplified 2D convolutional operation """"""
  x = tf.expand_dims(tf.expand_dims(x, 0), -1)
  y = tf.nn.depthwise_conv2d(x, k, [1, 1, 1, 1], padding='SAME')
  return y[0, :, :, 0]


def laplace(x):
  """"""Compute the 2D laplacian of an array """"""
  laplace_k = make_kernel([[0.5, 1.0, 0.5],
                           [1.0, -6., 1.0],
                           [0.5, 1.0, 0.5]])  
  return simple_conv(x, laplace_k)



# --- Define the PDE - the pond surface is a perfect 400x400 square
N = 400

# --- list of display points...
dispval = jump_range(0, 12500, 1000)
# --- dispval has to be a list...
dispval = list(dispval)
print ""We will look at these values: "",dispval

# --- now, we create some ""raindrops""
# --- Initial Conditions -- some rain drops hit the pond
# --- set everything to zero
u_init = np.zeros([N, N], dtype=np.float32)
ut_init = np.zeros([N, N], dtype=np.float32)

# Some material accretion occurs (raindrops hit pond) at random points
for n in range(40):
  a,b = np.random.randint(0, N, 2)
  u_init[a,b] = np.random.uniform()

# --- Create and Display the jpeg image...
# proc = subprocess.Popen([""display"", ""./tensor.jpg""])
# DisplayArray(u_init, rng=[-0.1, 0.1])

# Parameters
# eps -- time resolution
# damping -- wave damping
eps = tf.placeholder(tf.float32, shape=())
damping = tf.placeholder(tf.float32, shape=())

# --- Create vaiables for simulation state
U  = tf.Variable(u_init)
Ut = tf.Variable(u_init)

# --- Discretized PDE update rules
U_  = U + eps * Ut
Ut_ = Ut + eps * (laplace(U) - damping * Ut)

# --- Operation to update the state
step = tf.group(
  U.assign(U_),
  Ut.assign(Ut_))

# --- Run the simulation forward with a simple FOR loop.
# --- Initialize state to initial conditions
tf.global_variables_initializer().run(session=sess)

# --- Run 12701 steps of PDE
for i in range(12701):
  # Step simulation  (damping was 0.04, I made it negative .14)
   with sess.as_default(): step.run( {eps: 0.03, damping: -0.14})
# --- to see everything...
#   with sess.as_default(): print ""U.eval()   .... "", U.eval()[20]  # --- ,""   "", Ut.eval()
# ------

   if (i in dispval) :
       with sess.as_default(): DisplayArray(U.eval(), rng=[-0.1, 0.1])
       print ""                                ------ For iteration:  "",i
       sys.stdout.flush()
       print ""U.eval()   ....... ""
       with sess.as_default(): print   U.eval()[20]      # --- ,""   "", Ut.eval()
       print ""                                --- End of iteration:  "",i
       sys.stdout.flush()
       continue
#
# --- to show each iteration...
#  with sess.as_default(): DisplayArray(U.eval(), rng=[-0.1, 0.1])
print ""Done at: "",i

# --- Ok, we are done...
with sess.as_default(): DisplayArray(U.eval(), rng=[-0.1, 0.1])

with sess.as_default(): DisplayArrayToFile(U.eval(), rng=[-0.1, 0.1])
print ""Last Image Written to file: tensor_new.jpg. Done.""   
#--------------- done ------------------
```

If someone could try this program on a supported version of Linux (ie. the Ubuntu version that TensorFlow officially supports), that would be helpful.  I am running a recent version of the Linux kernel on the CentOS-7.4 box  (uname -a reports: kernel version 4.14.9-1.el7.elrepo.x86_64 ).  Really like to nail down what is happening.  I have attached images of results I am seeing on the two machines, first the Linux box, second is the Macbook.   

![laptest_linux_img_20180107_150905_sml](https://user-images.githubusercontent.com/16905336/34654259-45838fc6-f3c7-11e7-93b1-96751153c3a4.jpg)
![laptest_mac_img_20180107_151332_sml](https://user-images.githubusercontent.com/16905336/34654263-4c961b44-f3c7-11e7-8998-f3122f6d9e7c.jpg)
"
15932,General change in batch size Performance Question,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: N/A
- **TensorFlow installed from (source or binary)**: N/A
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem
I have opened a question on StackOverflow. I suspect only a developer can answer the question. The link to the question is here:
https://stackoverflow.com/questions/48099754/does-a-change-in-batch-size-impact-performance

The question is around Reinforcement Learning and if there will be a speed improvement if the batch size of the graph is not changed. RL usually has a training batch and a ""next_action"" type of single sample (batch size =1). The network is intermittently hit with either the training batch size or batch size of 1. Does TF take a performance hit every time the graph sees a different batch size through the feed_dict()? If this is the case, should 2 graphs be created so the batch size doesn't change from call to call?

If you post the answer here I will copy it to StackOverflow.

Thanks

### Source code / logs
N/A

  "
15930,Discontinuity at halfway point in graph output,"- **I have written custom code (as opposed to using a stock example script provided in TensorFlow)**:
to reproduce the error:
1) convert HnH_gate.txt to HnH_gate.py
2) Edit mypath in out() method at end of file for your system.  Save 
3) in python: run HnH_gate.py
4) run out() to create csv files for the good and bad output
            i) out(""101"", new_probka_good)
            ii) out(""102"", new_probka_bad)
5) Plot data from hh_101.csv and hh_102.csv and verify the discontinuity at half way point in hh_102.csv
6) Two additional tests can be run:
          i) Edit parameter timepoints in main() to show error remaps to half way point.
          ii) My temporary correction is to create 2x points and throw half away.  this is done in p_update() setting cut_in_half = True

7) This same error was found running the code in Tensorflow 1.4 on MacOS Sierra.  My system info is:


== cat /etc/issue ===============================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.3 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux PAULP-XPS15 4.4.0-43-Microsoft #1-Microsoft Wed Dec 31 14:42:53 PST 2014 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
numpydoc (0.7.0)
protobuf (3.4.1)
tensorflow (1.3.0)
tensorflow-tensorboard (0.1.5)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.3.0
tf.GIT_VERSION = b'unknown'
tf.COMPILER_VERSION = b'unknown'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


### Describe the problem
I am running a RNN for a Hodgkin and Huxley type gating of an ion channel protein
called HnH_gate.py.
The program takes a placeholder vmem and produces a timeseries output of the size
timepoints.
At the halfway point in the timeseries there is a discontinuity in the results
This only appears with some arrays fed to my tf.placeholder.  Others produce normal
results.  I can correct for the problem by doubling the number of timepoints requested
and throwing half away.

The array:
vmem_list_good = [[-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
                [-100.0, -90.0, -80.0, -70.0, -60.0, -50.0, -41.0, -30.0, -20.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0]]
appears to work perfectly
The array:
vmem_list_bad = [[80.0, 60.0, 40.0, 20.0, 00.0, -20.0, -41.0, -60.0, -80.0, -55.0, 0.0, 10.0, 20.0, 30.0, 40.0, 50.0],
            [70.0, 50.0, 30.0, 10.0, -10.0, -30.0, -50.0, -70.0, -90.0, -30.0, -10.0, 0.0, 10.0, 20.0, 30.0, 40.0]]

shows the error.

To see my temporary correction, edit the parameter in the HH.p_update() method
to: cut_in_half = True

I have written a short output routine to export the simulation to a csv file,
just edit the path and provide a string to make a unique filename:

out(""101"", new_probka_good)
out(""102"", new_probka_bad)

### Source code / logs
program file is: HnH_gate.py (provided as HnH_gate.txt)
HnH_gate.txt  (convert to HnH_gate.py)
[HnH_gate.txt](https://github.com/tensorflow/tensorflow/files/1610057/HnH_gate.txt)

System and Error Description: HnH_gate_bug_report.txt
[HnH_gate_bug_report.txt](https://github.com/tensorflow/tensorflow/files/1610056/HnH_gate_bug_report.txt)

Output example demonstrating problem: Artifact plotting new_probka_bad.py
[Artifact plotting new_probka_bad.pdf](https://github.com/tensorflow/tensorflow/files/1610058/Artifact.plotting.new_probka_bad.pdf)

Thanks for your help.
Paul
  
  "
15929,"C API, SIGABRT abort, Non-OK-status: RegisterAlreadyLocked, Invalid name.","### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Working with public C API.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
MacOS 10.13.2 (17C88)
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
- libtensorflow 1.4.1 (from brew package)
- **Python version**: 
non
- **Bazel version (if compiling from source)**:
non
- **GCC/Compiler version (if compiling from source)**:
Apple Swift version 4.0.3 (swiftlang-900.0.74.1 clang-900.0.39.2), lldb-900.0.64, Swift-4.0
- **CUDA/cuDNN version**:
non
- **GPU model and memory**:
non
- **Exact command to reproduce**:
Using swift code as example (https://github.com/Octadero/Example).

### Describe the problem
Dear TensorFlow community, 
It is really strange issue, from time to time at the same code, I have SIGABRT crash.
```
2018-01-05 21:03:55.627002: F tensorflow/core/framework/op.cc:165] Non-OK-status: RegisterAlreadyLocked(deferred_[i]) status: Invalid argument: Invalid name: {\242	
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT
    frame #0: 0x00007fff528f7e3e libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x0000000108cd41b4 libsystem_pthread.dylib`pthread_kill + 333
    frame #2: 0x00007fff52854312 libsystem_c.dylib`abort + 127
    frame #3: 0x0000000108e600c0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 32
    frame #4: 0x0000000108e600d0 libtensorflow_framework.so`tensorflow::internal::LogMessageFatal::~LogMessageFatal() + 16
    frame #5: 0x0000000108d28676 libtensorflow_framework.so`tensorflow::OpRegistry::MustCallDeferred() const + 406
    frame #6: 0x0000000108d2819d libtensorflow_framework.so`tensorflow::OpRegistry::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 61
    frame #7: 0x0000000108d0c875 libtensorflow_framework.so`tensorflow::FunctionLibraryDefinition::LookUp(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpRegistrationData const**) const + 117
    frame #8: 0x0000000108d27b0a libtensorflow_framework.so`tensorflow::OpRegistryInterface::LookUpOpDef(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tensorflow::OpDef const**) const + 42
    frame #9: 0x0000000108d81b65 libtensorflow_framework.so`tensorflow::Graph::AddNode(tensorflow::NodeDef const&, tensorflow::Status*) + 69
    frame #10: 0x0000000108d8177a libtensorflow_framework.so`tensorflow::Graph::Graph(tensorflow::OpRegistryInterface const*) + 458
    frame #11: 0x00000001004f7d73 libtensorflow.so`TF_NewGraph + 51
  * frame #12: 0x00000001066b739d CAPI`newGraph() at Graph.swift:26
    frame #13: 0x00000001063021b3 TensorFlowKit`Graph.init() at Graph.swift:36
    frame #14: 0x000000010630214a TensorFlowKit`Graph.__allocating_init() at Graph.swift:0
    frame #15: 0x0000000106500f2d TensorFlowKit`static SavedModel.load(exportPath=""/Users/Volodymyr/Projects/Examples/03_Reinforcement/Resources/save/"", tags=1 value, options=0x00000001098e35d0, self=TensorFlowKit.SavedModel) at SavedModel.swift:221
    frame #16: 0x00000001000069f4 03_Reinforcement`static Network.loadGraph(self=_3_Reinforcement.Network) at Network.swift:146
    frame #17: 0x0000000100003428 03_Reinforcement`Network.init() at Network.swift:73
    frame #18: 0x0000000100002e4c 03_Reinforcement`Network.__allocating_init() at Network.swift:0
    frame #19: 0x0000000100008656 03_Reinforcement`main at main.swift:32
    frame #20: 0x00007fff527a8115 libdyld.dylib`start + 1
    frame #21: 0x00007fff527a8115 libdyld.dylib`start + 1
```
Sanitizer options can't help to resolve that issue. List of libs loaded in attached file.
[dyld_log.txt](https://github.com/tensorflow/tensorflow/files/1609977/dyld_log.txt)


### Source code / logs
Using C API I am alloc [new Graph by TF_NewGraph()](https://github.com/Octadero/TensorFlow/blob/34addfc80cb7f220a7d9afa310f8a9845dba0d36/Sources/CAPI/Graph.swift#L26)"
15925,cmake compile error C2678: binary '*': no operator found sparse_column_iterable.cc,"`cmake .. -A x64 -DCMAKE_BUILD_TYPE=Release -DSWIG_EXECUTABLE=C:/ProgramData/chocolatey/bin/swig.exe -DPYTHON_EXECUTABLE=C:/Python36/python.exe -DPYTHON_LIBRARIES=C:/Python36/libs/python36.lib -Dtensorflow_WIN_CPU_SIMD_OPTIONS=/arch:AVX2 `

```
c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2678: binary '*': no operator found which takes a left-hand operand of type 'const tensorflow::boosted_trees::utils::`anonymous-namespace'::IndicesRowIterator' (or there is no acceptable conversion) (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\contrib\cmake\build\tf_core_kernels.vcxproj]
  c:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.12.25827\include\algorithm(2417): error C2100: illegal indirection (compiling source file D:\_working_dir\_ml\tensorflow\tensorflow\contrib\boosted_trees\lib\utils\sparse_column_iterable.cc) [D:\_working_dir\_ml\tensorflow\tensorflow\con
trib\cmake\build\tf_core_kernels.vcxproj]
```
Windows 8.1 x64
cmake 3.10.1
swig 3.0.9
Visual Studio 2017 Community

the same problem asked
https://stackoverflow.com/questions/48058113/compiling-tensorflow-1-4-on-windows-10"
15924,Tensorflow Optimize for Inference KeyError.,"I got the following error when optimizing the graph for inference:
Traceback (most recent call last):
  File ""C:\Python35\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""C:\Python35\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 146, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""C:\Python35\lib\site-packages\tensorflow\python\platform\app.py"", l
ine 48, in run
    _sys.exit(main(_sys.argv[:1] + flags_passthrough))
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference.py"", line 90, in main
    FLAGS.output_names.split("",""), FLAGS.placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\optimize_for_
inference_lib.py"", line 109, in optimize_for_inference
    placeholder_type_enum)
  File ""C:\Python35\lib\site-packages\tensorflow\python\tools\strip_unused_
lib.py"", line 83, in strip_unused
    raise KeyError(""The following input nodes were not found: %s\n"" % not_f
ound)
KeyError: ""The following input nodes were not found: {'input'}\n""

Please help me soon!"
15921,iOS: Op type not registered 'DecodeWav',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: I am trying to run graph model from Simple Audio Recognition example on iOS.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13
- **TensorFlow installed from (source or binary)**: Branch r1.4
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: Build label: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: N/A

### Describe the problem

I am trying to run graph model from Simple Audio Recognition example on iOS. When I am calling `session->Create(tensorflow_graph)` with the graph I get the error: ""Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav'..."".

My initial thought is that because I am using TensorFlow-experimental (1.1.1) from pods, it's possible that this Op type is not registered. So I tried building it myself, which builds without errors with command: `tensorflow/contrib/makefile/build_all_ios.sh`. I then remove TensorFlow-experimental (1.1.1) from the project and link my own build of tensorflow, but I get the same error. 

I also found the following PR - [[iOS] Add optional Selective Registration of Ops #14421](https://github.com/tensorflow/tensorflow/pull/14421)

I tried building from master with the above PR merged like so:
 
For iPhone 5:
`tensorflow/contrib/makefile/build_all_ios.sh -a armv7 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

For iPhone SE:
`tensorflow/contrib/makefile/build_all_ios.sh -a arm64 -g /Users/anton/Development/tensorflow/tensorflow/examples/ios/simple/data/tensorflow_inception_graph_speech.pb`

If I then go and check the file `/tensorflow/tensorflow/core/framework/ops_to_register.h` (auto-generated after above command) I can see that DecodeWav is listed among kernels and operations:

```
// This file was autogenerated by print_selective_registration_header.py
#ifndef OPS_TO_REGISTER
#define OPS_TO_REGISTER

    namespace {
      constexpr const char* skip(const char* x) {
        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;
      }

      constexpr bool isequal(const char* x, const char* y) {
        return (*skip(x) && *skip(y))
                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))
                   : (!*skip(x) && !*skip(y));
      }

      template<int N>
      struct find_in {
        static constexpr bool f(const char* x, const char* const y[N]) {
          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);
        }
      };

      template<>
      struct find_in<0> {
        static constexpr bool f(const char* x, const char* const y[]) {
          return false;
        }
      };
    }  // end namespace
    constexpr const char* kNecessaryOpKernelClasses[] = {
""BinaryOp< CPUDevice, functor::add<float>>"",
""AudioSpectrogramOp"",
""ConstantOp"",
""Conv2DOp<CPUDevice, float>"",
""DecodeWavOp"",
""IdentityOp"",
""MatMulOp<CPUDevice, float, false >"",
""MaxPoolingOp<CPUDevice, float>"",
""MfccOp"",
""NoOp"",
""PlaceholderOp"",
""ReluOp<CPUDevice, float>"",
""ReshapeOp"",
""SoftmaxOp<CPUDevice, float>"",
""RecvOp"",
""SendOp"",
};
#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))

constexpr inline bool ShouldRegisterOp(const char op[]) {
  return false
     || isequal(op, ""Add"")
     || isequal(op, ""AudioSpectrogram"")
     || isequal(op, ""Const"")
     || isequal(op, ""Conv2D"")
     || isequal(op, ""DecodeWav"")
     || isequal(op, ""Identity"")
     || isequal(op, ""MatMul"")
     || isequal(op, ""MaxPool"")
     || isequal(op, ""Mfcc"")
     || isequal(op, ""NoOp"")
     || isequal(op, ""Placeholder"")
     || isequal(op, ""Relu"")
     || isequal(op, ""Reshape"")
     || isequal(op, ""Softmax"")
     || isequal(op, ""_Recv"")
     || isequal(op, ""_Send"")
  ;
}
#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)

#define SHOULD_REGISTER_OP_GRADIENT false
#endif

```

But when I try to run the graph model I still get same error message. I have removed the pod version, and I am 100% sure I am running my own build version of tensorflow on iOS.

I can't tell if this is a bug or I am doing something wrong during the build process.

Has anyone tried running any graph that uses DecodeWav on iOS?

Thanks.

### Source code / logs

Error: Could not create TensorFlow Graph: Not found: Op type not registered 'DecodeWav' in binary running on Antons-iPhone. Make sure the Op and Kernel are registered in the binary running in this process.





"
15920,cmake CUDA include-path whitespaces not supported,"I'm not sure if this is intentional, but I just had some trouble with compiling tf_core_gpu_kernels due to a invalid include-dir path as command line argument given to nvcc (caused by a whitespace in the path).

In windows I was able to solve this by adding double quotes at line 277 in CMakeLists.txt:
`set(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS};--include-path \""${PROJECT_BINARY_DIR}/$\{build_configuration\}\"";--expt-relaxed-constexpr)`

Possible errors when not using double quotes:
nvcc fatal : A single input file is required for a non-link phase when an outputfile is specified

Also note that there will be problems because of CUDA not supporting some versions of msvc.
The current version for example is not yet supported.
CUDA_HOST_COMPILER path is automatically set to $(VCInstallDir)/bin, what will cause problems on some systems. For VS2015 this works fine but not for VS2017 if you are using the recent compiler.

There are also several errors when using the intel compiler, as for example the typename TType<...> declarations, which are not really required. Either remove the typename keyword or use 'auto'.
Other than that the master branch is compilable with ICC 18.
"
15919,A bug of tf.layers.batch_normalization when training is not a constant,"I encountered a bug of tf.layers.batch_normalization when the training argument is not a constant. Usually, when the training argument evaluates to False, the moving mean and variance should not be updated. However, in certain cases, the moving mean and variance may become NaN.

The bug occurs in the following code in python/layers/normalization.py:

```
    training_value = utils.constant_value(training)
    if training_value is None:
      one_minus_decay = utils.smart_cond(training,
                                         lambda: self._one_minus_decay,
                                         lambda: 0.)
    else:
      one_minus_decay = ops.convert_to_tensor(self._one_minus_decay)
    if training_value or training_value is None:
      mean_update = self._assign_moving_average(self.moving_mean, mean,
                                                one_minus_decay)
      variance_update = self._assign_moving_average(self.moving_variance,
                                                    variance, one_minus_decay)
```

When training is not a constant but evaluates to False, one_minus_decay is set to 0, and it is expected that _assign_moving_average does not actually change the moving average. However, mean and variance are outputs of FusedBatchNormOp, which are not actually computed when training is False. So, the content of mean and variance are random, and it can contain NaN values in certain cases. The NaN values in mean and variance then lead to NaN values in the moving mean and moving variance, even if the one_minus_decay is 0 (NaN times 0 is still NaN). Once moving mean and variance contain NaN values, the network produces NaN outputs forever.

I think a way to fix this issue is to modify _assign_moving_average. Just add following one line: 
`update_delta = tf.cond(tf.equal(one_minus_decay, 0), 0, update_delta)`
 Another way to fix is to let mean and variance output be zero if training is False. This way also helps preventing triggering the inf_or_nan_filter of tfdbg.
  "
15916,Object Tracking Support ,"I have a bug after updating to the latest android studio and building the detection app with it.
For previous versions of android studio I didn't have this issue before

when I ran the tf_detect app it showed an error for few seconds that says ""Object Tracking Support Not Found...""
and when I add the line ""dependencies {
    compile 'org.tensorflow:tensorflow-android:+'
}""
to the gradle build file it shows another error
Error:(42, 0) Could not find method compile() for arguments [org.tensorflow:tensorflow-android:+] on object of type org.gradle.api.internal.artifacts.dsl.dependencies.DefaultDependencyHandler.
<a href=""openFile:C:\Users\mohda\Desktop\tensorflow-master_new\tensorflow-master\tensorflow\examples\android\build.gradle"">Open File</a>

any suggestions or fixes to this issue please?
What am I doing:
I have created a custom trained detector and it was working fine till android studio was updated. I have even tried with a fresh copy of the original demo and I have the same error. Yet when I downloaded the nightly build apk it didn't show any error. so it must be the android studio / tensorflow compatibility / dependency issue here.
Thanks 
  "
15915,Failed to synchronise stop event.,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.4
- **Python version**: 3.6.3
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**:  5.4.0 
- **CUDA/cuDNN version**: 9.1/7.0.5
- **GPU model and memory**: GT 750M 2GB
- **Exact command to reproduce**: Run the custom program.

### Describe the problem
Trying to train a simple 5 layer model with 3.7million parameters that are trainable which would occupy around 1.5GB VRAM, the training fails instantly at the very first epoch. People who had similar error claimed that it was fixed in cuDNN 7.0.5 from this thread [#14363](https://github.com/tensorflow/tensorflow/issues/14363) . But this update isn't fixing the crash. I've posted the error log below:

### Source code / logs
```
Total params: 3,714,788
Trainable params: 3,714,788
Non-trainable params: 0
_________________________________________________________________
2018-01-06 21:58:27.567991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-01-06 21:58:27.568491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GT 750M major: 3 minor: 0 memoryClockRate(GHz): 0.967
pciBusID: 0000:01:00.0
totalMemory: 1.95GiB freeMemory: 1.60GiB
2018-01-06 21:58:27.568526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GT 750M, pci bus id: 0000:01:00.0, compute capability: 3.0)
Train on 646 samples, validate on 162 samples
Epoch 1/20
2018-01-06 21:58:30.447458: E tensorflow/stream_executor/cuda/cuda_driver.cc:1080] failed to synchronize the stop event: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018-01-06 21:58:30.447529: E tensorflow/stream_executor/cuda/cuda_timer.cc:54] Internal: error destroying CUDA event in context 0x5650223aede0: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018-01-06 21:58:30.447572: E tensorflow/stream_executor/cuda/cuda_timer.cc:59] Internal: error destroying CUDA event in context 0x5650223aede0: CUDA_ERROR_ILLEGAL_INSTRUCTION
2018-01-06 21:58:30.447629: F tensorflow/stream_executor/cuda/cuda_dnn.cc:2964] failed to set stream for cudnn handle: CUDNN_STATUS_MAPPING_ERROR
Aborted (core dumped)

```

  
  "
15914,How to know my loss function does not have numerical problems?,"I wrote the following loss function that I post below. How do I know that I don't have any kind of numerical issues with it? (because something tells me that I do)

```
def gather_cols(params, indices, name=None):
    """"""Gather columns of a 2D tensor.

    Args:
        params: A 2D tensor.
        indices: A 1D tensor. Must be one of the following types: ``int32``, ``int64``.
        name: A name for the operation (optional).

    Returns:
        A 2D Tensor. Has the same type as ``params``.
    """"""
    with tf.op_scope([params, indices], name, ""gather_cols"") as scope:
        # Check input
        params = tf.convert_to_tensor(params, name=""params"")
        indices = tf.convert_to_tensor(indices, name=""indices"")
        try:
            params.get_shape().assert_has_rank(2)
        except ValueError:
            raise ValueError('\'params\' must be 2D.')
        try:
            indices.get_shape().assert_has_rank(1)
        except ValueError:
            raise ValueError('\'params\' must be 1D.')

        # Define op
        p_shape = tf.shape(params)
        p_flat = tf.reshape(params, [-1])
        i_flat = tf.reshape(tf.reshape(tf.range(0, p_shape[0]) * p_shape[1],
                                       [-1, 1]) + indices, [-1])
        return tf.reshape(tf.gather(p_flat, i_flat),
                          [p_shape[0], -1])


def custom_binary_crossentropy(y_true, y_pred):
    # Assumes y_pred are probabilities and that y_true has actually 2 labels inside
    # Calculate: gain(y1, y2) * log(p) + gain(y2, y1) * log(1 - p)
    # gain(x1, x2) = (2 ^ x1 - 1) / ((2 ^ x1 - 1) + (2 ^ x2 - 1))

    # Gather y1 and y2 first
    y1 = gather_cols(y_true, [0])
    y2 = gather_cols(y_true, [1])

    # Get 2^y - 1
    y1_g = tf.subtract(tf.pow(tf.fill(tf.shape(y1), 2.0), y1), tf.fill(tf.shape(y1), 1.0))
    y2_g = tf.subtract(tf.pow(tf.fill(tf.shape(y2), 2.0), y1), tf.fill(tf.shape(y2), 1.0))

    # Get gains
    gain1 = tf.div(y1_g, tf.add(y1_g, y2_g))
    gain2 = tf.div(y2_g, tf.add(y1_g, y2_g))

    # Get logs
    log1 = tf.log(y_pred)
    log2 = tf.log(tf.subtract(tf.fill(tf.shape(y_pred), 1.0), y_pred))

    return -K.mean(tf.add(tf.multiply(gain1, log1), tf.multiply(gain2, log2)))
```"
15913,How to build contrib module that depends on tensorflow/core/kernels:linalg?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. Making a contrib module.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS Sierra
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**: 3.6
- **Bazel version (if compiling from source)**: 0.9
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)
- **CUDA/cuDNN version**: Building without GPU support
- **GPU model and memory**: Building without GPU support
- **Exact command to reproduce**: bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
I am writing a contrib module that depends on tensorflow/core/kernels:linalg (specifically the `//tensorflow/core/kernels:linalg_ops_common` target). However, adding this dependency

	tf_custom_op_library(
	    name = ""python/ops/_medical_image_ops.so"",
	    srcs = [
	        ""kernels/index_ops.cc"",
	        ""ops/index_ops.cc"",
	    ],
	    deps = [
	        ""//tensorflow/core/kernels:linalg""
	    ],
	)

yields

	ERROR: /Users/kasper/Development/tensorflow3/tensorflow/contrib/medical_image/BUILD:13:1: in check_deps rule //tensorflow/contrib/medical_image:python/ops/_medical_image_ops.so_check_deps:
	Traceback (most recent call last):
		File ""/Users/kasper/Development/tensorflow3/tensorflow/contrib/medical_image/BUILD"", line 13
			check_deps(name = 'python/ops/_medical_image_ops.so_check_deps')
		File ""/Users/kasper/Development/tensorflow3/tensorflow/tensorflow.bzl"", line 1196, in _check_deps_impl
			fail(((_dep_label(input_dep) + "" cann...)))
	tensorflow/core/kernels:linalg cannot depend on tensorflow/core:framework

The `//tensorflow/core/kernels:linalg_ops_common` target is private so I cannot depend on it directly. In an earlier release I believe there was a `//tensorflow/core/kernels:linalg_ops_common_headers_lib` target but this is not there anymore.

How can I add one of the linalg targets as a dep without introducing this circular dependency?

(I have scoured over many of the `cannot depend on tensorflow/core:framework` issues from the past but have not found a solution)

### Source code / logs
This is my full BUILD file:

	# Description:
	#   Contains ops for working natively with medical images in TensorFlow.

	licenses([""notice""])  # Apache 2.0

	exports_files([""LICENSE""])

	package(default_visibility = [""//visibility:public""])

	load(""//tensorflow:tensorflow.bzl"", ""tf_custom_op_library"", ""tf_custom_op_py_library"",
	     ""tf_kernel_library"", ""tf_gen_op_libs"", ""tf_gen_op_wrapper_py"")

	tf_custom_op_library(
	    name = ""python/ops/_medical_image_ops.so"",
	    srcs = [
	        ""kernels/index_ops.cc"",
	        ""ops/index_ops.cc"",
	    ],
	    deps = [
	        ""//tensorflow/core:lib""
	    ],
	)

	tf_gen_op_libs(
	    op_lib_names = [""index_ops""],
	)

	tf_gen_op_wrapper_py(
	    name = ""medical_image_ops"",
	    deps = ["":index_ops_op_lib""],
	)

	tf_custom_op_py_library(
	    name = ""medical_image_py"",
	    srcs = [
	        ""__init__.py"",
	        ""python/medical_image/medical_image.py"",
	        ""python/medical_image/transforms.py"",
	        ""python/ops/index_ops.py"",
	    ],
	    dso = ["":python/ops/_medical_image_ops.so""],
	    srcs_version = ""PY2AND3"",
	    deps = [
	        "":medical_image_ops"",
	    ],
	)

	filegroup(
	    name = ""all_files"",
	    srcs = glob(
	        [""**/*""],
	        exclude = [
	            ""**/METADATA"",
	            ""**/OWNERS"",
	        ],
	    ),
	    visibility = [""//tensorflow:__subpackages__""],
	)

  
  "
15912,Eager: error when restore tfe.Network checkpoint by tfe.restore_network_checkpoint,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.6.0dev20170105
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:pascal
- **Exact command to reproduce**:N/A
### Describe the problem
When I want to restore tfe.Network by using tfe.restore_network_checkpoint from a graph-mode checkpoint(such as pretrained slim resnet ckpt, so need name map of tfe.restore_network_checkpoint), I get an error:
```Python
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-1-cbde22383c9e> in <module>()
     20     images = tf.constant(toy_data)
     21     logits = net(images)
---> 22     tf.contrib.eager.restore_network_checkpoint(net, ckpt)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\contrib\eager\python\network.py in restore_network_checkpoint(network, save_path, map_func)
    946       save_path=save_path,
    947       map_func=map_func,
--> 948       user_map_func=user_map_func)
    949   # Step two is to set a custom getter which restores variables on creation,
    950   # for those variables which have not been added to sub-Layers yet.

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\contrib\eager\python\network.py in _restore_existing_variables(network, save_path, map_func, user_map_func)
    859       sess = ops.get_default_session()
    860     saver_lib.Saver(var_list=existing_variables_by_checkpoint_name).restore(
--> 861         sess=sess, save_path=save_path)
    862   return existing_variables_by_checkpoint_name
    863 

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\training\saver.py in restore(self, sess, save_path)
   1686                {self.saver_def.filename_tensor_name: save_path})
   1687     else:
-> 1688       self._build_eager(save_path, build_save=False, build_restore=True)
   1689 
   1690   @staticmethod

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\training\saver.py in _build_eager(self, checkpoint_path, build_save, build_restore)
   1250   def _build_eager(self, checkpoint_path, build_save, build_restore):
   1251     self._build(
-> 1252         checkpoint_path, build_save=build_save, build_restore=build_restore)
   1253 
   1254   def _build(self, checkpoint_path, build_save, build_restore):

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\training\saver.py in _build(self, checkpoint_path, build_save, build_restore)
   1282           restore_sequentially=self._restore_sequentially,
   1283           filename=checkpoint_path,
-> 1284           build_save=build_save, build_restore=build_restore)
   1285     elif self.saver_def and self._name:
   1286       # Since self._name is used as a name_scope by builder(), we are

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\training\saver.py in _build_internal(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)
    748                         [saveable.op for saveable in saveables]) as name:
    749       # Add the Constant string tensor for the filename.
--> 750       filename_tensor = constant_op.constant(filename or ""model"")
    751 
    752       # Add the save ops.

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name, verify_shape)
    182   ctx = context.context()
    183   if not ctx.in_graph_mode():
--> 184     t = convert_to_eager_tensor(value, ctx, dtype)
    185     if shape is None:
    186       return t

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
    129     return t
    130   else:
--> 131     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)
    132 
    133 

RuntimeError: Error copying tensor to device: /job:localhost/replica:0/task:0/device:GPU:0. Can't copy Tensor with type string to device /job:localhost/replica:0/task:0/device:GPU:0.
```
code to reproduce this error:
first run graph code to save a ckpt:
```Python
import tensorflow as tf
import numpy as np
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
def cnn(x):
    with tf.variable_scope('net'):
        x = tf.layers.dense(x, 10, name='fc')
        return x
ckpt = '/tmp/graph/test.ckpt'
with tf.Graph().as_default():
    images = tf.placeholder(tf.float32, [None, 784])
    logits = cnn(images)
    saver = tf.train.Saver(save_relative_paths=True)
    print(tf.global_variables())
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        saver.save(sess, ckpt)
```
Then run eager code to load ckpt, note that any ckpt path can produce same error, so I think ckpt file has no problem.
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
config = tf.ConfigProto()
config.gpu_options.allow_growth=True
tfe.enable_eager_execution(config=config)
class CNN(tfe.Network):
    def __init__(self, name):
        super(CNN, self).__init__(name)
        self.fc = self.track_layer(tf.layers.Dense(10, name='fc'))
    def call(self, x):
        x = tf.reshape(x, [-1, 784])
        x = self.fc(x)
        return x
toy_data = np.ones((100, 784)).astype(np.float32)
device = ""gpu:0"" if tfe.num_gpus() else ""cpu:0""
net = CNN('net')
ckpt = '/tmp/graph/test.ckpt' # any other paths produce same error
with tf.device(device):
    images = tf.constant(toy_data)
    logits = net(images)
    tfe.restore_network_checkpoint(net, ckpt)
```
  "
15911,"Crash when using CUDA API while using Tensorflow. ""current context was not created by the StreamExecutor cuda_driver API""","### Expected behavior

I expect to work with the CUDA library by using libcudart directly in my Python script before and while Tensorflow is being used - without errors.

### Actual behavior

The script crashes with following error as soon as I've used libcudart through ctypes before tensorflow is imported.

```
2018-01-06 14:45:50.641211: F tensorflow/stream_executor/cuda/cuda_driver.cc:232] current context was not created by the StreamExecutor cuda_driver API: 0x7fe122002400; a CUDA runtime call was likely performed without using a StreamExecutor context
```

### Environment

| Name  | Value |
| ------------- | ------------- |
| OS  | Mac OS 10.12.4 |
| tensorflow-gpu  | pip 1.1.0, v1.1.0-rc0-61-g1ec6ed5  |
| Device  | GPU Titan X pascal  |
| Python  | 2.7.12  |
| Python  | 3.5.2 |
| CUDA  | 8.0  |
| cuDNN  | 5.1.5 |


### Reproduce

```python
from __future__ import print_function

import ctypes
import platform

system = platform.system()

if system == ""Linux"":
    libcudart = ctypes.cdll.LoadLibrary(""libcudart.so"")
elif system == ""Darwin"":
    libcudart = ctypes.cdll.LoadLibrary(""libcudart.dylib"")
elif system == ""Windows"":
    libcudart = ctypes.windll.LoadLibrary(""libcudart.dll"")
else:
    raise Exception(""Cannot identify system."")

version = ctypes.c_int()
rc = libcudart.cudaRuntimeGetVersion(ctypes.byref(version))
if rc != 0:
    raise ValueError(""Could not get version"")
if version.value < 6050:
    raise Exception(""CUDA version must be >= 6.5"")

libcudart.cudaSetDevice(0)

free = ctypes.c_size_t()
total = ctypes.c_size_t()
rc = libcudart.cudaMemGetInfo(ctypes.byref(free), ctypes.byref(total))

print(""Memory "" + str(free.value) +  "" of "" + str(total.value))

del libcudart

import tensorflow as tf

hello = tf.constant('Hello, TensorFlow!')

# Start tf session
sess = tf.Session()

# Run the op
print(sess.run(hello))
````

Execute 

```
$ python3 tf-cuda-crash.py
```

Output will be like

```
$ python3 tf-cuda-crash.py
Memory 9916960768 of 12884574208
2018-01-06 14:45:50.639837: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-06 14:45:50.639863: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-01-06 14:45:50.639870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-01-06 14:45:50.641211: F tensorflow/stream_executor/cuda/cuda_driver.cc:232] current context was not created by the StreamExecutor cuda_driver API: 0x7fe122002400; a CUDA runtime call was likely performed without using a StreamExecutor context
[1]    4240 abort      python3 tf-cuda-crash.py
```

Happens with both Python2 and 3.
  "
15910,Feature Request: Sparse Cholesky decomposition,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
TensorFlow has a Cholesky decomposition [kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/cholesky_op.cc) based on wrappers around the [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) and [cuSOLVER](http://docs.nvidia.com/cuda/cusolver/index.html#cusolver-intro) (for GPUs) libraries.

From experience, a sparse solver can provide huge speedups in the right circumstances. The cuSOLVER library has the feature in their sparse LAPACK library, cuSolverSP, and the Eigen library in the SparseCholesky module.

Alternatively, there is the [CHOLMOD](https://developer.nvidia.com/cholmod) library which is supported by Eigen in the CholmodSupport module. This CHOLMOD library supports both CPU and GPU sparse Cholesky factorisations.

Would a Cholesky decomposition for sparse matrices be a feature of interest? 
  "
15909,Python Configuration Error,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**:  source 
- **TensorFlow version (use command below)**:the latest master branch
- **Python version**: 3.6.3 in anaconda ,python path is :C:/Users/huo_y/Anaconda3/python.exe
- **Bazel version (if compiling from source)**:0.9.0
- **GCC/Compiler version (if compiling from source)**:msvc 14
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
 bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
when I build tensorflow with bazel on windows by msys2 shell. I got this error
Python Configuration Error : --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?

### Source code / logs
ERROR: C:/users/huo_y/tensorflow-master/util/python/BUILD:5:1: no such package '@local_config_python//': Traceback (most recent call last):
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 291
                _create_local_python_repository(repository_ctx)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 251, in _create_local_python_repository
                _check_python_bin(repository_ctx, python_bin)
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 204, in _check_python_bin
                _fail((""--define %s='%s' is not execut...)))
        File ""C:/users/huo_y/tensorflow-master/third_party/py/python_configure.bzl"", line 27, in _fail
                fail((""%sPython Configuration Error:%...)))
Python Configuration Error: --define PYTHON_BIN_PATH='C:/Users/huo_y/Anaconda3/python.exe' is not executable. Is it the python binary?
 and referenced by '//util/python:python_headers'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 16.677s
FAILED: Build did NOT complete successfully (63 packages loaded)
    currently loading: tensorflow/core/kernels
    Fetching http://ufpr.dl.sourceforge.net/project/swig/swig/swig-3.0.8/swig-3.0.8.tar.gz; 32,768b 4s

It may cause by python_configure.bzl I think,But I don't know how to correct it.

here is on function about the error  in python_config.bzl 
```
def _check_python_bin(repository_ctx, python_bin):
  """"""Checks the python bin path.""""""
  cmd =  '[[ -x ""%s"" ]] && [[ ! -d ""%s"" ]]' % (python_bin, python_bin)
  result = repository_ctx.execute([""bash"", ""-c"", cmd])
  if result.return_code == 1:
    _fail(""--define %s='%s' is not executable. Is it the python binary?"" % (
        _PYTHON_BIN_PATH, python_bin))
```
I find that when I run configure  the python path is windows format just like C:/Users/huo_y/Anaconda3/python.exe  but in msys2 the path may show /c/Users/huo_y/Anaconda3/python.exe .  I guess that when using bash -c ,it should need the path just like /c/Users/huo_y/Anaconda3/python.exe can get the correct return code, but it seems that the python_bin parameter is the windows path format .

can anyone help check it because I don't be farmiliar with bazel

"
15908,How to see the implementation of softmax_cross_entropy,"In tf.nn_ops file ，it imports the file : tensorflow.python.ops import gen_nn_ops
but I can't find the file gen_nn_ops in the directory"
15907,nightly installed TF is the new 1.5 TF?,"today I heard that there are a new version 1.5 TF which is with good support dynamic graph.

And I also find there is a new nightly installed method.


So this nightly installing method is install the new Version TF?


```
chg0901@ubuntu:~$ python3
Python 3.5.2 (default, Nov 23 2017, 16:37:01) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2018-01-06 16:49:32.242801: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library
>>> print(tf.__version__)
1.6.0-dev20180105
>>> x = [[2.]]
>>> m = tf.matmul(x,x)
>>> print(m)
Tensor(""MatMul:0"", shape=(1, 1), dtype=float32)
>>> print(tf.Session().run(m))
2018-01-06 16:51:25.418750: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX FMA
[[ 4.]]

```

  "
15905,Documentation does not explain the utility of -1 as value for the axis parameter of the tf.concat method,"### System information

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X, version 10.13.2
- **TensorFlow installed from (source or binary)**: Binary (pip)
- **TensorFlow version (use command below)**: v1.3.0-rc1-5211-gab0fcac 1.5.0-dev20171126
- **Python version**: Python 3.5.0
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**:  N/A
- **CUDA/cuDNN version**:  N/A
- **GPU model and memory**:  N/A
- **Exact command to reproduce**: Just run a Python script with the code I am sharing with you

### Describe the problem

It apparently concatenates along the last axis. See the following example:  

```
import tensorflow as tf

t1 = [[[1, 2], [2, 3]], [[4, 4], [5, 3]]]
t2 = [[[7, 4], [8, 4]], [[2, 10], [15, 11]]]

with tf.Session() as sess:
    result = sess.run(tf.concat([t1, t2], -1))
    print(result)
```

which produces

```
[[[ 1  2  7  4]
  [ 2  3  8  4]]

 [[ 4  4  2 10]
  [ 5  3 15 11]]]
``` 

The following documention does not seem to explain this use case:

- https://www.tensorflow.org/api_docs/python/tf/concat
- https://www.tensorflow.org/versions/r1.5/api_docs/python/tf/concat
  
"
15904,【TensorFlow Servering】 bazel build tensorflow_serving/... ImportError: No module named numpy,"fong@ubuntu:~/serving$ bazel build tensorflow_serving/...
..........
DEBUG: /home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:197:9: @//tensorflow_serving/model_servers:tensorflow_model_server_tar: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.

ERROR: /home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):
	File ""/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl"", line 291
		_create_local_python_repository(repository_ctx)
	File ""/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl"", line 255, in _create_local_python_repository
		_get_numpy_include(repository_ctx, python_bin)
	File ""/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl"", line 239, in _get_numpy_include
		_execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
	File ""/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl"", line 54, in _execute
		_fail(""\n"".join([error_msg.strip() if ... """"]))
	File ""/home/fong/.cache/bazel/_bazel_fong/38e1867f819d663d91548408e483d3bf/external/org_tensorflow/third_party/py/python_configure.bzl"", line 27, in _fail
		fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: No module named numpy
Is numpy installed?
 and referenced by '@org_tensorflow//third_party/py/numpy:headers'
ERROR: Analysis of target '//tensorflow_serving/example:inception_saved_model' failed; build aborted: Loading failed
INFO: Elapsed time: 35.784s
FAILED: Build did NOT complete successfully (149 packages loaded)
    currently loading: @org_tensorflow//tensorflow/contrib/rnn ... (6 packages\
)


This is the error，but  my python paths and python library path is true,python library path have numpy already.


fong@ubuntu:~/serving/tensorflow$ ./configure 
You have bazel 0.9.0 installed.
Please specify the location of python. [Default is /home/fong/anaconda3/bin/python]: 


Found possible Python library paths:
  /home/fong/anaconda3/lib/python3.5/site-packages
Please input the desired Python library path to use.  Default is [/home/fong/anaconda3/lib/python3.5/site-packages]

Do you wish to build TensorFlow with jemalloc as malloc support? [Y/n]: y
jemalloc as malloc support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Google Cloud Platform support? [Y/n]: y
Google Cloud Platform support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Hadoop File System support? [Y/n]: y
Hadoop File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: y
Amazon S3 File System support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [y/N]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with GDR support? [y/N]: y
GDR support will be enabled for TensorFlow.

Do you wish to build TensorFlow with VERBS support? [y/N]: y
VERBS support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Add ""--config=mkl"" to your bazel command to build with MKL support.
Please note that MKL on MacOS or windows is still not supported.
If you would like to use a local MKL instead of downloading, please set the environment variable ""TF_MKL_ROOT"" every time before build.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Configuration finished"
15903,Does Staged Variables only support parameter_server mode？,Does Staged Variables only support parameter_server mode？When I used it in my multi-gpu like replicated mode，it works and trainning faster than no Staged Variables？Can I use it to speedup my trainning？
15902," W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745519: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations. 2018-01-06 09:50:33.745553: W c:\l\tensorflow_1501918863922\work\tensorflow-1.2.1\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
15900,Feature Request: MonitoredTrainingSession should accept checkpoint steps,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: n/a
- **TensorFlow installed from (source or binary)**: n/a
- **TensorFlow version (use command below)**: n/a
- **Python version**: n/a
- **Bazel version (if compiling from source)**:n/a
- **GCC/Compiler version (if compiling from source)**:n/a
- **CUDA/cuDNN version**:n/a
- **GPU model and memory**:n/a
- **Exact command to reproduce**:n/a

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
As of 1.4, MonitoredTrainingSession conveniently offers constructor parameters for checkpoint_secs, summary_secs, and summary_steps. It does not offer a parameter for checkpoint_steps, so if I want to checkpoint by steps and not seconds, I have to register a custom CheckpointSaverHook. It would be nice if MonitoredTrainingSession encapsulated that, as it does for the similar summary_secs, summary_steps, and checkpoint_secs.

### Source code / logs
https://github.com/tensorflow/tensorflow/blob/cddf8d82dec9fff526f5c064add725b7f35f95fa/tensorflow/python/training/monitored_session.py#L272
"
15897,Tensor Core support for NVIDIA Volta architecture,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:
5386775e64aac0bb5020974122645da900bc312a
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:0.8.1
- **GCC/Compiler version (if comp6iling from source)**:5.4.0
- **CUDA/cuDNN version**:9.1 / 7.0.5
- **GPU model and memory**: Titan V
- **Exact command to reproduce**:

### Describe the problem
It is widely reported that using float16 on Nvidia Volta architecture comes only with x2 improvement instead of the expected x4 x8 improvement using Tensor Cores
https://github.com/tensorflow/benchmarks/issues/77
https://devblogs.nvidia.com/parallelforall/programming-tensor-cores-cuda-9/

I checked that Tensorflow master branch used 
cudnnGetConvolutionForwardAlgorithm
to get the best possible algorithm for the given GPU.
However I think either
cudnnGetConvolutionForwardAlgorithm_v7
or 
cudnnFindConvolutionForwardAlgorithmEx
should be used to fully utilize the Volta architecture.
Could you please check this issue with a Volta architecture GPU?
### Source code / logs
"
15892,Change GitHub repo URL from http://www.tensorflow.org to https://www.tensorflow.org,Reasoning: HTTPS all the things
15891,Dependencies of tensors created within a tf.while_loop() might not be executed,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See test case below.
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 'Sierra' Version 10.12.6 (16G1114)
- **TensorFlow installed from (source or binary)**: Both. I have compiled TensorFlow at 136697ecdc64b5171522fb7f89cfe51a02f0f1c1 with my small change in PR #15823. I have also tried using the pip package.
- **TensorFlow version (use command below)**: ('v1.4.0-19-ga52c8d9b01', '1.4.1') (pip package)
- **Python version**: 2.7.10
- **Bazel version (if compiling from source)**: 0.9.0-homebrew
- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)
- **CUDA/cuDNN version**: CUDA 9.0.176_mac, cuDNN 9.0-osx-x64-v7
- **GPU model and memory**: NVIDIA GeForce GT 750M with 2048 MB device memory (CUDA Compute Capability 3.0)
- **Exact command to reproduce**:

`python repro.py`

.. where `repro.py` contains the test case to reproduce, listed below.

### Describe the problem
Here is my test case:

```python
# Part I
from __future__ import division, print_function
import numpy as np
import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

rs = np.random.RandomState(seed = 2)
A = rs.normal(size = (10, 10,))
print('singular values of A: %s' % (np.linalg.svd(A, compute_uv = False),))
B = rs.normal(size = (10, 10,))
print('singular values of B: %s' % (np.linalg.svd(B, compute_uv = False),))



# Part II
A_var = tf.Variable(B)
init_A_var_op = tf.assign(A_var, A)
A_dep = tf.constant(10, tf.int32)

with tf.control_dependencies([init_A_var_op]):
    A_dep = A_dep + 1

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var, compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))



# Part III
A_var = tf.Variable(B)
init_A_var_op = tf.assign(A_var, A)
A_dep = tf.constant(9, tf.int32)

def loop_condition(j, A_dep):
    return j < 1
def loop_body(j, A_dep):
    with tf.control_dependencies([init_A_var_op]):
        A_dep = A_dep + 1
    return j + 1, A_dep

_, A_dep = tf.while_loop(loop_condition,
                         loop_body,
                         loop_vars = [tf.constant(0, tf.int32), A_dep],
                         parallel_iterations = 1,
                         back_prop = False)

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var, compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))



# Part IV
A_var = rr.ResourceVariable(B)
init_A_var_op = A_var.assign(A)
A_dep = tf.constant(8, tf.int32)

def loop_condition(j, A_dep):
    return j < 1
def loop_body(j, A_dep):
    with tf.control_dependencies([init_A_var_op]):
        A_dep = A_dep + 1
    return j + 1, A_dep

_, A_dep = tf.while_loop(loop_condition,
                         loop_body,
                         loop_vars = [tf.constant(0, tf.int32), A_dep],
                         parallel_iterations = 1,
                         back_prop = False)

with tf.control_dependencies([A_dep]):
    var_s = tf.svd(A_var.read_value(), compute_uv = False)
with tf.Session() as session:
    session.run(tf.global_variables_initializer())
    computed_s, computed_A_dep = session.run([var_s, A_dep])
print('computed_s = %s, computed_A_dep = %d' % (computed_s, computed_A_dep,))
```

Part I is basic setup. I create two random 10&times;10 matrices and compute their singular values:
<pre>
singular values of A: [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
  1.73387162  1.16000494  0.58836563  0.39101954]
singular values of B: [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664]
</pre>

Part II shows usage of control_dependencies() to guarantee that `A` has been assigned to `A_var` before the singular values of `A_var` are computed. The output from this part is:
<pre>
computed_s = [ 5.65906715  4.9420261   4.40626739  3.73506125  2.70703249  2.57429488
  1.73387162  1.16000494  0.58836563  0.39101954], computed_A_dep = 11
</pre>

(This is the expected result for Part II.)

In Part III, I have introduced use of a tf.while_loop(). Now, tf.svd() is returning the singular values of `B`:
<pre>
computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 10
</pre>

(This is **not** the expected result for Part III. I expect that the singular values of `A` would be printed.)

In Part IV, based on reading https://github.com/tensorflow/tensorflow/issues/4663#issuecomment-336609536 , I switched to using `ResourceVariable`. However, the output is still the same (the singular values of `B`):
<pre>
computed_s = [ 7.0283055   4.65840063  4.48502098  3.25319445  2.94667168  2.74267484
  1.86004291  1.6626967   0.63884034  0.27131664], computed_A_dep = 9
</pre>

(This is **not** the expected result for Part IV. I expect that the singular values of `A` would be printed.)

It appears the issue is that tf.control_dependencies() on tensors created by tf.while_loop() might not execute the tensors' own dependencies.

This used to work okay (around TensorFlow 1.1, if I recall correctly).

While searching for a previous report of this issue, I found #6087 which appears related, in that the sample code there has a tf.while_loop() that creates tensors with dependencies. When I run the sample code, I consistently get result = 10. This is an unexpected result, in my opinion. What is happening is that `update_x` runs exactly once, so for each of the 5 loop iterations, `x` has the value 2.

I tried rewriting the sample code to use a ResourceVariable, but the output is the same:

```python
from __future__ import division, print_function
import tensorflow as tf
from tensorflow.python.ops import resource_variable_ops as rr

with tf.variable_scope('state'):
    x = rr.ResourceVariable(tf.constant(1, dtype=tf.float32))
    update_x = x.assign(x.read_value() + 1)

def iter_fun(i, y):
    # comment the line below, the program will run without any error
    # but I need control_dependencies, or at least some way to replace it...
    with tf.control_dependencies([update_x]):
        y = y + tf.Print(x.read_value(), ['i = ', i, 'y = ', y, 'x = ', x.read_value()])
    return (i+1, y,)

with tf.variable_scope('iteration'):
    num_iterations = 5
    initial_i = tf.constant(0, dtype=tf.int32)
    initial_y = tf.constant(0, dtype=tf.float32)
    _, result = tf.while_loop(
        cond=lambda i, *_: i < num_iterations,
        body=iter_fun,
        loop_vars=(initial_i, initial_y))

init_op = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init_op)
    print(sess.run(result))
```
  "
15889,error while bazel build,"## System information

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
**no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
**Ubuntu 16.04**
- TensorFlow installed from (source or binary):
 **Source**
- TensorFlow version (use command below):
 **r1.5**
- Python version: 
**3.6.3(Anaconda)**
- GCC/Compiler version (if compiling from source): 
**5.4**
- CUDA/cuDNN version:
 **9.0.176 / 7.0.5**
- GPU model and memory: 
**NVIDIA GTX 1080, Driver 385.111, 8G**
- Bazel version (if compiling from source):
**0.9**
- Exact command to reproduce:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`

##  Describe the problem

Use the commend line:
`bazel build  --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --config=cuda //tensorflow/tools/pip_package:build_pip_package --action_env=""LD_LIBRARY_PATH=${LD_LIBRARY_PATH}""`
And got the error:

```
> ERROR: /home/xxh/tensorflow/tensorflow/contrib/boosted_trees/BUILD:559:1: Linking of rule '//tensorflow/contrib/boosted_trees:gen_gen_stats_accumulator_ops_py_wrap_py_wrappers_cc' failed (Exit 1)
> /usr/bin/ld: warning: libcublas.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcudnn.so.7, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcufft.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> /usr/bin/ld: warning: libcurand.so.9.0, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasGemmEx@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasZhpmv_v2@libcublas.so.9.0'
> bazel-out/host/
> ......
> ......
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cublasDsymm_v2@libcublas.so.9.0'
> bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scontrib_Sboosted_Utrees_Cgen_Ugen_Ustats_Uaccumulator_Uops_Upy_Uwrap_Upy_Uwrappers_Ucc___Utensorflow/libtensorflow_framework.so: undefined reference to `cudnnCreateLRNDescriptor@libcudnn.so.7'
> collect2: error: ld returned 1 exit status
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 894.638s, Critical Path: 24.00s
> FAILED: Build did NOT complete successfully
```
When coppileing,I got a lot of warning like this.
`WARNING: /home/xxh/tensorflow/tensorflow/contrib/learn/BUILD:15:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.`

Cuda 9.0 test pass! And copied the cudnn.h and libcudnn* to cuda file.
It's all fine.
  
  "
15887,"Building Tensorflow 1.4.1 from source successful on MacOS, but pip install to Python 2.7.14 of wheel file fails with ""not supported wheel on this platform"""," 
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
MacOX Yosemite 10.10.5
- **TensorFlow installed from (source or binary)**: 
source (binary loads fine, no problem)

- **TensorFlow version (use command below)**:
Tensorflow 1.4.1

- **Python version**: 
Python 2.7.14  

- **Bazel version (if compiling from source)**:
0.9.0

- **GCC/Compiler version (if compiling from source)**:
Xcode7.2.1

- **CUDA/cuDNN version**: 
N/a 

- **GPU model and memory**:
N/a

- **Exact command to reproduce**:

See my notes for TensorFlow 1.4.1 install from source below...  Note: Bazel build completed successfully, only issue is the failure to ""pip install ..."" the wheel file.


 
### Describe the problem
 < see the tl:dr at bottom of notes...> 
 Can't load generated wheel file of Tensorflow 1.4.1 into Python 2.7.14, despite having built everything local.  Tried with several Python versions, MacOS orginal version, Python 2.7.14 (from binary upgrade), and using a Python 2.7.14 that I built an installed locally.  Note: Tensorflow  1.4.0 binary can be loaded and runs ok (but operates differently than TensorFlow 1.4.0 that I have running on CentOS-7.4 box, hence my interest in building from source, and tracking down why this is the case

### Source code / logs
Bazel Build was ok, built for CPU only , 4044 actions, successful completion.  Also the build_pip_package works fine.  Only issue is the failure to pip install the wheel file

Here are notes of exactly what I did... 

TensorFlow Build Notes using Bazel                    - MCL, Jan. 5, 2018
=========================================================================

Done on Macbook, Yosemite 10.10.5, using Xcode-7.2.1

You need to get Xcode7.2.1 from Apple and install it.

For TensorFlow build using Bazel you need to install:

    - JDK8  (Java Development Kit #8, (not #9...))
    - Bazel (built from source, see: /home/Bazel
    - Xcode7.2.1 (which offers clang 700.xxx version)

First tried with: Xcode6.3.x (needed to put DOSbox on iPad as DOSpad)

Had to update Xcode.  After Xcode7.2.1 install (copy .dmg to /Applications/Xcode7.2.1)
Note the directory had to be manually created first.

Run it all as root on the Macbook...

   cd /home/TensorFlow/tensorflow-1.4.1

Needed to do a ""make clean"" in Bazel.  Do this:

   bazel clean --expunge     (pitche le vache... totemo, eh?)


Run this to build...

   Note: the ./configure is a Q&A.  For initial build, no special features were
   selected (ie. I selected ""n"" to all the ""build with this cool thing? (Y/n): "" ) 

      ./configure   

   Once configure is complete, do this... (note, it is all one line...)

      bazel build --config=opt --incompatible_load_argument_is_label=false 
//tensorflow/tools/pip_package:build_pip_package


   The build runs for 66 minutes, and reports:

********************************************************
      Elapsed time: 3985.215 s
      Critical Path: 132.83 s
      Build completed successfully, 4044 total actions.
******************************************************** 

   This makes a build_pip_package.sh file in the ../tensorflow/tools/pip_package subdir.

   You now need to make the .whl (the compressed package pip uses to install to python)
   from this thing the Bazel build made.  You run the script: ""bazel-bin"" to do this.
   On the Macbook (and Linux), you will need to enter: ""bazel-bin/tensorflow... ""
   for it to work.

   Note: ""pwd"" reports: ""/home/TensorFlow/tensorflow-1.4.1

   Note: The TensorFlow build instructions use top level directory ""/tmp"", but I 
         don't want it destroyed if it works.  (/tmp means ""temporary"", eh?)
         I created top level ""/tftop""
   
   Note: ""bazel-bin"" is a directory.  You are running the ""build_pip_package"" script
         that is down in there, not the build_pip_package.sh script in ./tensorflow tree.

      ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tftop/tensorflow_pkg
   
         I ran this to /tftop, which might be wrong, since I got a bunch of warnings
         about *.dll, *.lib, *.h, * (for Eigen), * (for external), * (for google)...
         and some other, all not being found.  Tried to run it again, using /tmp, and
         I got message:

-------------------------------------------------------------------------------------------
   "" ./bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory""
-------------------------------------------------------------------------------------------

         The ""build_pip_package"" destroys the build-generated files.  (this is no 
         problem, it turns out.  Just re-run the ""bazel build ..."", as bazel remembers
         all the compiles, and the build_pip_package thing completes in a few seconds...

         The file that is created in /tftop/tensorflow_pkg is:

          tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl
       
         This TensorFlow install instructions say to use this:

      sudo pip install /tftop/tensorflow_pkg/tensorflow-1.4.1-py2-none-any.whl

         But it, of course, does not work, as there is no such file...
 
         And when I try, (as root):

      pip install /tftop/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl

      I get the showstopper message:

*************************************************************************************************************************         
 tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl is not a supported wheel on this platform
*************************************************************************************************************************
         and for now, I am completely stopped. 

  What I did:

        Re-ran Bazel (it remembers everything was compiled.  Ran in a few seconds...)

        Ran:
             bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

        < exactly as indicated in the TensorFlow  documentation, here:
               https://www.tensorflow.org/install/install_sources#common_installation_problems
        >

        Got exactly the same warning messages about a bunch of missing files...

        Tried: 
                pip install /tmp/tensorflow_pkg/tensorflow-1.4.0-py2-none-any.whl

               (of course, did not work.  No such filename....  You have to use the wheel file
                that the build process generates, which is in the /tmp/tensorflow_pkg directory... )

            pip install /tmp/tensorflow_pkg/tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl

                (this ""pip install"" is to a local-built Python 2.7.14, which currently has the
                 Tensorflow-1.4.0 binary successfully installed, which successfully operates...)

            Error message: (same)
----------------------------------------------------------------------------------------------------------- 
""tensorflow-1.4.1-cp27-cp27m-macosx_10_4_x86_64.whl is not a supported wheel on this platform""
-----------------------------------------------------------------------------------------------------------
           Tried with different versions of Python (tweaked the PATH in .bash_profile to 
           get my custom built one, and the ..framework system verison. Same problem for both.

Result:  TensorFlow Build successful, ""pip install ..."" to Python 2.7.14 on MacOSX 
         fails with version mis-match.  (? or a bug maybe ?)

Post-Mortem Questions: 
===================  

   I was expecting the documentation to match this install process.  
   I probably need the ""-py2-none-any.whl"" type of wheel to be generated, right?

   What I am getting is some very MacOSX version-specific thing being built, which 
   is failing the ""pip install ..."" because of a version-mismatch, maybe..

 TL;DR:  How can I force the build process to build the ""-py2-none-any.whl"" type of wheel (which
           does not have fascist-style version-checking (:D), and just builds the .whl file as per
           what the TensorFlow build instructions describe?
                   or
           Is there some tweak I can make to the build process (which all seems to be working quite fine),
           to tell the process to really, actually, please code the built version of TensorFlow 1.4.1, to 
           the actual machine (the Yosemite 10.10.5 Macbook), that I have run the build upon??

- Mark Langdon, Jan. 5, 2018


  "
15886,"Successful Local Build of Tensorflow r1.5 GPU for Python 3.6, CUDA Toolkit 9.0, and CUDNN 7.0 on Windows 7 X64 SP1 using CMake in VS 2015 Update 3","I have spent a week trying to compile Tensorflow from source using Bazel on Windows with no success. In 2 days, I was able to compile it using CMake following the command output from a successful build I saw on Jenkins on 02-Jan-2018.

I wanted to provide details to spare others the pain of development in the future. The whole build took 6 hours to compile on my system.

I have an older system, which is why I was doing this. You will need to path variables in the attached scripts to work with the path variables for your system. For the most part, however, the scripts replicate what is mentioned on github here:

https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/cmake

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes

- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7 x64 SP1
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5.0-rc0
- **Python version**: 3.6.4
- **CMake version (if compiling from source)**: CMake 3.10.1
- **GCC/Compiler version (if compiling from source)**: cl.exe (Visual Studio 2015 Update 3)
- **CUDA/cuDNN version**: 9.0.176 / 7.0.4
- **GPU model and memory**: NVIDIA Quadro K4000 P8, Driver 385.54, 3072 MiB
- **SWIG Version**: swigwin-3.0.12
- **Git Version**: Git for Windows 2.15.1 64-bit
- **MSBuild Version**: 14.0.25420.1
- **CPU**: Intel Xeon E5-2620 v2

- **Exact command to reproduce**:

After installing the above, I wrote a batch script to set and clean up system environment variables (please see attached script). Due to the 1024 character limit for PATH on windows, I manually edited the PATH in the registry editor to overcome this limitation.

I then wrote another script that set local variables, cloned tensorflow source and checked out version 1.5, then prepared the source with cmake and compiled with msbuild.

The final output was a python wheel, which I pip installed. I successfully ran the standard hello world script without error, i.e.

```
import tensorflow as tf
hello = tf.constant('Hello')
sess = tf.Session()
sess.run(hello)
```

as well as a small AlexNet network without issues.

I hope this helps future users and further emphasizes that it is possible to build Tensorflow 1.5 for GPU on Windows 7. I have not compiled this with AVX support, but that could be a next improvement (I'm not sure if this is possible. I only know MKL support is limited to Linux at the moment. However, Windows binaries for MKL and MPI can be downloaded from the Intel website).


[Scripts.zip](https://github.com/tensorflow/tensorflow/files/1607554/Scripts.zip)



"
15883,Different convolutional padding per channel,"Hello,

would it be possible to specify different convolutional padding for each channel? My use case is using 3D convolutions on input, where the first dimension is temporal and the next 2 dimensions are 2D images. I'd like to have ""SAME"" padding on temporal dimension and ""VALID"" padding on convolutional dimensions.

The model in question is for estimating speed of a vehicle from on-board camera where a sequence of images of fixed length (e.g. 10 frames) is sent to a 3D convolutional network and it makes sense to avoid unnecessary clipping of temporal dimension where it's perfectly fine for convolutional part.

Would this be possible to add to TensorFlow?

Thank you!

Have I written custom code: No
OS Platform and Distribution: Linux Mint 18.2
TensorFlow installed from: pip
TensorFlow version: 1.3.0
Bazel version: N/A
CUDA/cuDNN version: 9.0
GPU model and memory: GTX970 4GB
Exact command to reproduce: tf.nn.conv2d's ""padding"" parameter uses a single approach for all convolutional dimensions
  "
15882,"tfdbg error ""Dump root directory does not exist"" with empty fetches","### System information
- **Have I written custom code**: yes
- **OS Platform and Distribution**: Linux Ubuntu 16.04
- **TensorFlow installed from**: binary (pip install)
- **TensorFlow version**:
== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)
- **Python version**: 2.7.12
- **CUDA/cuDNN version**: 
== cuda libs  ===================================================
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
- **GPU model and memory**: GeForce GTX 1080, 8114MiB
- **Exact command to reproduce**: see code below

### Describe the problem
`LocalCLIDebugWrapperSession.run()` does not behave like `tf.Session.run()` if there are no fetches. The dump directory will never be created and it crashes with an `IOError`. For me this issue occured in a situation like this:
```
      session.run([var.initializer for var in not_initialized_from_checkpoint])
```
where actually everything was restored from the checkpoint and `not_initialized_from_checkpoint` was empty. This code runs fine with an ordinary tf.Session but crashed with tfdbg. It took me some time to track down the issue. If it's not too hard to fix, it would be nice to keep other users from the same pain (maybe - just speculating - #13604 crashes for the same reason)

### Source code / logs
```
import tensorflow as tf
from tensorflow.python import debug as tf_debug

sess = tf.Session()
dbg_sess = tf_debug.LocalCLIDebugWrapperSession(tf.Session())

print sess.run([tf.constant(1.0)])     # [1.0]
print sess.run([])                     # []
print dbg_sess.run([tf.constant(1.0)]) # [1.0]
print dbg_sess.run([])                 # IOError: Dump root directory /tmp/tfdbg_ai_aWv does not exist
```
"
15880,Allow full deallocation of GPU memory,"When using the TF C++ library inside an application that also uses GPUs for other tasks (not implemented in TF), it would be useful to be able to deallocate all the GPU memory TF has allocated once the session is closed, and no further TF calls are expected for the time being. gpu_options.allow_growth keeps TF's allocated pool small, but it still can grow to several GB. Even after the session is deleted, the pool doesn't shrink. To free it up, the whole application must be restarted, if I'm not mistaken.

Being able to destroy the [ProcessState](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/process_state.cc) singleton seems to solve it without breaking anything. However, its destructor is protected. Alternatively, getting the Allocator for each GPU from ProcessState and manually destroying them does the trick, but renders TF unusable for all future operations because ProcessState still thinks the Allocators exist and doesn't recreate them when they are required again.

I think making the ProcessState destructor public (or adding a public method to invoke similar code) would be the best solution, but maybe I'm missing an obvious solution that already exists?"
15876,tfcompile with --config=monolithic and -fvisibility=hidden results in undefined reference __xla_cpu_runtime_EigenMatMulF32,"Some background first. For DeepSpeech, I have been experimenting simplification of our set of dependencies, trying to do a `--config=monolithic` build. The root cause for that was being able to run a `SYCL`-enabled build on my system (Ubuntu 17.10). Using `OpenCL` on this would trigger dependency load-chain that in the end loads `libmirprotobuf`. This would clash with the `protobuf` symbols already built in our `libtensorflow_framework` / `libtensorflow_cc`. To avoid this, monolithic build and forcing `visibility=hidden` seemed to be the best solution.

This allows us to move from those libraries (non tfcompile build, tfcompile adds `libdeepspeech_model.so` and all the `XLA` dependencies):
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`
 - `libtensorflow_cc.so`
 - `libtensorflow_framework.so`

To just:
 - `libdeepspeech.so`
 - `libdeepspeech_utils.so`

This way, we have all needed TensorFlow bits within `libdeepspeech.so`, and those symbols are not re-exported thus avoiding any unwanted interaction. I could get `SYCL` build nearly working on Intel GPU.

Adding `tfcompile` in the equation, however, lead to linking issues. Symptom would be that build completes, but when one links binary against the model's `.so`, then it fails with:
```
undefined reference __xla_cpu_runtime_EigenMatMulF32
```

Checking with `objdump -t bazel-bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_matmul/tensorflow/compiler/xla/service/cpu/runtime_matmul*.o | grep EigenMatMul` would show that the symbol is properly built into `runtime_matmul`, but that it is hidden.

I would be able to solve that by exposing `__xla_cpu_runtime_EigenMatMulF32` and `__xla_cpu_runtime_EigenMatMulF64` through `TF_EXPORT`."
15874,Weird behaviour of tf.control_dependencies,"

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:  CUDA V8.0.61 CUDNN 6.0.20
- **GPU model and memory**: K40M
- **Exact command to reproduce**:

### Describe the problem

The source code is a minimal one to use dynamic rnn to predict token tag.

I tried to use ` tf.control_dependencies` to ensure `loss` will be evaluated before train_op. However,  I mistakenly evaluated `loss` in `session.run([train, loss])`.   

Then I found that if the input length (`EXAMPLE_LENGTH ` in the example code) is larger than or equal to 32, the program will hang without any notification. If I set `CUDA_VISIBLE_DEVICES=''` to use CPU only, the program will output an error code. However, if the input length is smaller than 32, it will run without any problem. 

I am not sure if it is a bug or an intentional behavior.

### Source code / logs
```python
import tensorflow as tf

from tensorflow.contrib.rnn import stack_bidirectional_dynamic_rnn
from tensorflow.python.ops import rnn_cell

EXAMPLE_LENGTH = 31
with tf.Graph().as_default():
    x = tf.random_uniform(maxval=2000, minval=1, 
                          shape=[1, EXAMPLE_LENGTH, 300], dtype=tf.float32)
    lengths = tf.constant([EXAMPLE_LENGTH])
    y = tf.random_uniform(maxval=5, minval=0, 
                          shape=[1, EXAMPLE_LENGTH], dtype=tf.int32)

    cell = rnn_cell.BasicRNNCell(50)
    output, _ = tf.nn.dynamic_rnn(cell, x, dtype=""float32"")
    
    logits = tf.layers.dense(output ,units=5)
    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))
    with tf.control_dependencies([loss]):
        opt = tf.train.AdamOptimizer()
        train_op = opt.minimize(loss)
        


    sess = tf.InteractiveSession()
    sess.run(tf.global_variables_initializer())

    for i in range(10):
        _, l = sess.run([train_op, loss])
        print(i, l)
```


### Traceback


#### Only output error with CPU
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Retval[0] does not have value

During handling of the above exception, another exception occurred:

InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-43-926b17edef2c> in <module>()
     26 
     27     for i in range(10):
---> 28         _, l = sess.run([train_op, loss])
     29         print(i, l)

/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/var/pyenv/versions/anaconda3-4.1.1/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

InvalidArgumentError: Retval[0] does not have value
```"
15873,Feature request: Want TFSlim to automatically download the pre-trained checkpoint,"I want TFSlim to automatically download the pre-trained checkpoint like Keras:
https://keras.io/applications/

When I use Keras, the code below automatically download the pre-trained VGG 16 weights:

```python
from keras.applications.vgg16 import VGG16
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input
import numpy as np

model = VGG16(weights='imagenet', include_top=False)
```

On the other hands, TFSlim required to download the checkpoint manually.

Is there any plans to add the feature?"
15872,Converting unsupported operation:Unpack Equal TensorArrayV3,"zhangyixindeMBP:tensorflow-1.5.0-rc0 zhangyixin$ bazel run --config=opt \
>   //tensorflow/contrib/lite/toco:toco -- \
>   --input_file=/Users/zhangyixin/Desktop/ssd_mobilenet_201801051717_face/frozen_inference_graph.pb  \
>   --output_file=/Users/zhangyixin/Desktop/ssd_mobilenet_201801051717_face/frozen_inference_graph.pb-pb-lite.lite \
>   --input_format=TENSORFLOW_GRAPHDEF \
>   --output_format=TFLITE \
>   --inference_type=FLOAT \
>   --input_shape=1,300,300,3 \
>   --input_array=image_tensor \
>   --output_arrays=detection_boxes,detection_scores,detection_classes,num_detections
WARNING: Config values are not defined in any .rc file: opt
INFO: Analysed target //tensorflow/contrib/lite/toco:toco (0 packages loaded).
INFO: Found 1 target...
Target //tensorflow/contrib/lite/toco:toco up-to-date:
  bazel-bin/tensorflow/contrib/lite/toco/toco
INFO: Elapsed time: 0.387s, Critical Path: 0.01s
INFO: Build completed successfully, 1 total action

INFO: Running command line: bazel-bin/tensorflow/contrib/lite/toco/toco '--input_file=/Users/zhangyixin/Desktop/ssd_mobilenet_201801051717_face/frozen_inference_graph.pb' '--output_file=/Users/zhangyixin/Desktop/ssd_mobilenet_201801051717_face/frozen_inference_graph.pb-pb-lite.lite' '--input_format=TENSORFLOW_GRAPHDEF' '--output_format=TFLITE' '--inference_type=FLOAT' '--input_shape=1,300,300,3' '--input_array=image_tensor' '--output_arrays=detection_boxes,detection_scores,detection_classes,num_detections'
2018-01-05 16:55:04.603261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.603851: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayV3
2018-01-05 16:55:04.603888: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.603921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayScatterV3
2018-01-05 16:55:04.603938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayV3
2018-01-05 16:55:04.603976: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Enter
2018-01-05 16:55:04.604002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Enter
2018-01-05 16:55:04.604038: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Enter
2018-01-05 16:55:04.604098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: LoopCond
2018-01-05 16:55:04.604176: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Enter
2018-01-05 16:55:04.604219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Enter
2018-01-05 16:55:04.604238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayReadV3
2018-01-05 16:55:04.604379: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Enter
2018-01-05 16:55:04.604437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayWriteV3
2018-01-05 16:55:04.604600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Exit
2018-01-05 16:55:04.604620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArraySizeV3
2018-01-05 16:55:04.604668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayGatherV3
2018-01-05 16:55:04.604778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.604868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.605025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: LogicalAnd
2018-01-05 16:55:04.621652: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.627529: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.629129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.630742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.631634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.631684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.632365: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.632426: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.633228: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.633277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.633915: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.633964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.634590: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.634647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.635357: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.635451: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.636153: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.636180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Equal
2018-01-05 16:55:04.636466: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.636938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.637261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.637554: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.637835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638604: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638691: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.638875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Equal
2018-01-05 16:55:04.638953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.639062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Unpack
2018-01-05 16:55:04.639152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Unpack
2018-01-05 16:55:04.639201: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Exp
2018-01-05 16:55:04.639216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: Exp
2018-01-05 16:55:04.639407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.639464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: StridedSlice
2018-01-05 16:55:04.640078: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayV3
2018-01-05 16:55:04.640109: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayV3
2018-01-05 16:55:04.640130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1057] Converting unsupported operation: TensorArrayV3"
15871,TFlite toco failed to convert the quantized inception protobuf to tflite format ,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:

Linux qiuji01 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.2 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

- **TensorFlow installed from (source or binary)**:
source
 
- **TensorFlow version (use command below)**:
tensorflow (1.5.0rc0)

git commit id:
commit f99275a6a309699c73e1bbebd89ba9aa32e79aa3
Author: Amit Patankar <amitpatankar@google.com>
Date:   Thu Jan 4 17:35:54 2018 -0800

- **Python version**: 
2.7.12
- **Bazel version (if compiling from source)**:
0.5.4.
- **GCC/Compiler version (if compiling from source)**:
gcc version 5.4.0 20160609
- **CUDA/cuDNN version**:
no
- **GPU model and memory**:
no
- **Exact command to reproduce**:

/root/TF/new/out/toco/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/lite/toco/toco --input_file=incept_8wn_gt.pb --output_file=incept_8wn_gt.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shape=1,299,299,3 --input_array=""Mul""  --output_array=""softmax""

### Describe the problem
I am trying to follow the transform_graph (~/tensorflow/tools/graph_transforms/) README.md to get a 8 bit quantized model, then feed it into the Tflite toco (~/tensorflow/contrib/lite/) to  transform it from protobuf into tflite format.  My steps are:
1. download the inception model  from http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz and extract it.

2. build the graph_transforms using the following command:
bazel --output_base=../out/transform_graph/ build -s -c opt tensorflow/tools/graph_transforms:transform_graph

3. quantize and optimize the inception model using t he following command:
/root/TF/new/out/transform_graph/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/tools/graph_transforms/transform_graph --in_graph=classify_image_graph_def.pb --out_graph=incept_8wn_gt.pb --inputs='Mul:0' --outputs='softmax:0' --transforms='add_default_attributes strip_unused_nodes(type=float, shape=""1,299,299,3"") remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'

4. build the toco using the following command:
bazel --output_base=../out/toco build tensorflow/contrib/lite/toco:toco

5. transform the quantized inception model from protobuf format into TFlite format using the following the command:

/root/TF/new/out/toco/execroot/org_tensorflow/bazel-out/local-opt/bin/tensorflow/contrib/lite/toco/toco --input_file=incept_8wn_gt.pb --output_file=incept_8wn_gt.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --inference_type=QUANTIZED_UINT8 --input_shape=1,299,299,3 --input_array=""Mul""  --output_array=""softmax""





### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

The error log is as following:

2018-01-05 08:08:37.682405: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.682950: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683126: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683385: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683546: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.683960: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684051: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684156: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684210: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684364: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684538: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.684738: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685213: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.685883: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.686456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688240: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688313: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688454: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688508: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688537: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.688566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688612: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.688634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.688697: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.688722: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.688781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.688797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.688813: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.688856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.688949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689182: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.689992: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.690290: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.690314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.690338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.690360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.690382: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.690403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.690425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.690489: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.690915: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691122: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691368: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691476: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.691807: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692065: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692081: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692093: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692189: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.692229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.692241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692293: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692302: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692331: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692359: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692584: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692609: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692628: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692664: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692706: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692773: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692815: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.692827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692858: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692868: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.692921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.692932: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.692942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.692965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.692996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693021: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693168: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693255: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693380: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693407: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.693459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.693473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693484: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693522: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693566: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693639: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693723: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693736: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693755: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.693799: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.693826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.693834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.693842: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.693869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.693957: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694349: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694394: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694474: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694483: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694492: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694501: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694510: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694518: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694536: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694544: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694561: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694581: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizeV2
2018-01-05 08:08:37.694651: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694661: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.694724: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694741: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694750: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694783: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694792: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694800: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694825: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694833: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694841: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694857: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694865: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694874: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694906: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694930: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694946: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.694954: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.694963: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.694979: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.694987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.694994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695020: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695052: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695060: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695069: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.695079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695113: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695128: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.695147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695157: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695164: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695206: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695224: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695249: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695259: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695298: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695333: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.695352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695403: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695411: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.695421: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695431: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695439: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695448: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695456: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695472: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695481: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695490: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695541: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695558: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695567: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695575: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695616: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695633: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695641: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695675: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695684: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695692: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695700: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695751: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695794: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695803: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695827: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695844: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695869: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695877: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695884: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695892: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695909: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695926: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695934: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695941: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.695949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.695959: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.695969: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.695977: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.695986: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.695994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.696029: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696048: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696072: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696079: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696096: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696105: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696146: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696180: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696212: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696221: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696237: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696261: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696270: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696287: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696296: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696303: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696319: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696328: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696345: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696354: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696395: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696427: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696436: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696479: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.696506: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696532: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696540: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696548: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696565: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696582: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696591: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696614: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.696635: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.696645: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696663: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696671: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696679: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696703: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696753: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696762: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696771: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696779: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696788: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696797: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696804: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696847: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696879: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696887: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696896: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696912: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696921: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696936: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.696945: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696953: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696962: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.696970: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.696978: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.696985: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.696994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697036: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697045: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697053: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697062: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697070: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697078: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697086: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697094: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697137: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697145: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697153: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697169: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697186: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697211: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697227: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.697238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.697248: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697300: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697309: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697318: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697327: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697344: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697352: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697378: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697386: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697396: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697404: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697428: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697445: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697462: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697470: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697503: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697511: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697534: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697569: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697593: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697618: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697634: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697642: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697650: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697667: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697676: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697685: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697693: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697709: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697727: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697735: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697776: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697785: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697793: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697802: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697810: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697818: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697826: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.697845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697872: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697881: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697889: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697897: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697905: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697914: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697931: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697940: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.697965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.697973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.697981: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.697990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.697998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698006: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698014: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698040: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698047: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698056: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698064: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698071: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698106: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698123: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698130: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698155: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698163: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698171: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698195: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.698204: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.698215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698225: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698233: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698242: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.698250: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698266: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.698275: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.698284: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.698292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.698301: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706033: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706101: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706111: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706121: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706139: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706179: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706200: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706229: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706257: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706276: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706487: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706530: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706543: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706552: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706560: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706570: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706598: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706607: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706677: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706686: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706704: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706712: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706737: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706746: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706756: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706764: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706772: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706791: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.706801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706819: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706829: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706846: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706853: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706863: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.706882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMaxPool
2018-01-05 08:08:37.706902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706917: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.706948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.706964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.706973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.706982: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.706991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707001: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707017: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707025: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707034: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707058: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707083: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707119: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707133: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707148: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707160: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707203: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707217: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707232: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707262: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707277: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707311: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707425: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707438: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707453: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707482: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707512: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707539: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707556: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707585: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707615: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707629: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707643: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707658: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConv2D
2018-01-05 08:08:37.707673: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707687: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707731: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedRelu
2018-01-05 08:08:37.707761: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedConcat
2018-01-05 08:08:37.707780: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedAvgPool
2018-01-05 08:08:37.707798: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedReshape
2018-01-05 08:08:37.707814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedMatMul
2018-01-05 08:08:37.707830: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707860: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QuantizedBiasAdd
2018-01-05 08:08:37.707875: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RequantizationRange
2018-01-05 08:08:37.707890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Requantize
2018-01-05 08:08:37.707904: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Dequantize
2018-01-05 08:08:37.763586: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1080 operators, 3039 arrays (0 quantized)
2018-01-05 08:08:37.849158: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:37.934553: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.022469: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.082050: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 793 operators, 2752 arrays (1 quantized)
2018-01-05 08:08:38.156372: F tensorflow/contrib/lite/toco/tooling_util.cc:1217] Array conv/Conv2D_eightbit/Mul__port__0/min, which is an input to the (Unsupported TensorFlow op: QuantizeV2) operator producing the output array conv/Conv2D_eightbit/Mul__port__0/quantize, is lacking min/max data, which is necessary for quantization. Either target a non-quantized output format, or change the input graph to contain min/max information, or pass --default_ranges_min= and --default_ranges_max= if you do not care about the accuracy of results.

"
15870,the determinant is not finite,"
### Describe the problem
 The items ‘tf.matrix_determinant(sig))‘ and 'tf.matrix_inverse(sig) 'induce error when i try to transform my code from CPU version with r1.2 Tensorflow on windows to GPU r1.4 tensorflow on Ubuntu 16.04. It works fine on windows,while something's wrong on Linux.
I'm not sure whether this problem is triggered by changed version of Tensorflow or the changed platform from CPU to GPU

It seems to me that the data precision has something to do with this, but have little knowlege on this region. Any idea what causes this phenomenon and how to solve this? Thx!

Here are the errors.
InvalidArgumentError (see above for traceback): The determinant is not finite.
         [[Node: MatrixDeterminant_2 = MatrixDeterminant[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](truediv_6)]]
         [[Node: gradients/Exp_1_grad/mul/_85 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_657_gradients/Exp_1_grad/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]


InvalidArgumentError (see above for traceback): Input is not invertible.
         [[Node: gradients/MatrixDeterminant_2_grad/MatrixInverse = MatrixInverse[T=DT_FLOAT, adjoint=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](truediv_6)]]
         [[Node: gradients/Exp_1_grad/mul/_85 = _Recv[client_terminated=false, recv_device=""/job:localhost/replica:0/task:0/device:CPU:0"", send_device=""/job:localhost/replica:0/task:0/device:GPU:0"", send_device_incarnation=1, tensor_name=""edge_657_gradients/Exp_1_grad/mul"", tensor_type=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""]()]]





"
15869,tensorflow/go for Windows?,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
- **TensorFlow installed from (source or binary)**: either
- **TensorFlow version (use command below)**: 1.4+
- **Python version**: n/a
- **Bazel version (if compiling from source)**: n/a
- **GCC/Compiler version (if compiling from source)**: MINGW64
- **CUDA/cuDNN version**: none
- **GPU model and memory**: none
- **Exact command to reproduce**: visit https://www.tensorflow.org/versions/master/install/install_go

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Feature request: https://www.tensorflow.org/versions/master/install/install_go doesn't have Windows 10 instructions.  I am curious if the tensorflow/go will be installable for windows

### Source code / logs
I currently get an error on the `ld.exe` phase of the go get . for github.com/tensorflow/tensorflow/tensorflow/go : `cannot find -ltensorflow`.  I'm not sure how to specify that the `tensorflow.dll` should be used. "
15868,S3 Checkpointing fails with large graphs,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: tensorflow/tensorflow:1.4.0 container running on Amazon Linux
- **TensorFlow installed from (source or binary)**: Used tensorflow/tensorflow:1.4.0 image
- **TensorFlow version (use command below)**: ('v1.4.0-rc1-11-g130a514', '1.4.0')
- **Python version**: 2.7.12
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:

1. Exported AWS credentials to environment variables
2. docker run -it --rm -p 8888:8888 -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_SESSION_TOKEN tensorflow/tensorflow:1.4.0
3. Opened Jupyter UI in browser at localhost:8888
4. Pasted code below in a new notebook, modified checkpoint_s3_dir variable to point to my S3 bucket, and ran it:

```
# Replace with a bucket that you have write access to
checkpoint_s3_dir = 's3://<your_bucket>/checkpoint_testing'

import numpy as np
import tensorflow as tf
from tensorflow.python.keras.layers import Dense, LSTM
from tensorflow.python.estimator.model_fn import ModeKeys
from tensorflow.contrib.learn import RunConfig

# I don't believe the specifics of how my Estimator is configured are important;
# this is a toy example similar to the code where we encountered the problem.
# By varying the embedding dimension used, we can easily generate a graph
# large enough to trigger this problem.

WINDOW_SIZE = 7
BATCH_SIZE = 128
MAX_VOCAB_SIZE = 100000
HIDDEN_DIM = 512
NUM_CLASSES = 2
NUM_PARTITIONS = 10

def partitioned_embeddings(embedding_dim):    
    # Randomly generate embedding
    emb = np.random.rand(MAX_VOCAB_SIZE, embedding_dim).astype(np.float32)
    partitioned_embeddings = []
    for i in range(NUM_PARTITIONS):
        partitioned_embeddings.append(tf.constant(emb[i::NUM_PARTITIONS]))

    return partitioned_embeddings

def model_fn(features, labels, mode, params):
    emb_parts = partitioned_embeddings(params['embedding_dim'])
    embedded_vectors = tf.nn.embedding_lookup(params=emb_parts, ids=features['inputs'],
                                          name='embedding_lookup', partition_strategy='mod')
    l = LSTM(HIDDEN_DIM)(embedded_vectors)
    layers = Dense(NUM_CLASSES, activation='sigmoid')(l)

    global_step = tf.train.get_or_create_global_step()
    
    optimizer = tf.train.AdamOptimizer()
    loss = tf.losses.sigmoid_cross_entropy(multi_class_labels=labels, logits=layers)
    train_op = optimizer.minimize(loss, global_step=global_step)
    
    return tf.estimator.EstimatorSpec(ModeKeys.TRAIN, loss=loss, train_op=train_op)

def train(embedding_dim, model_dir=None):
    # Generate random training data
    word_ids = np.random.randint(0, high=MAX_VOCAB_SIZE, size=(BATCH_SIZE * 10, WINDOW_SIZE),
                             dtype=np.int32)
    # Generate random labels
    labels = np.random.randint(0, high=NUM_CLASSES, size=(BATCH_SIZE * 10, NUM_CLASSES), dtype=np.int32)
    
    input_fn = tf.estimator.inputs.numpy_input_fn(x={'inputs': word_ids}, y=labels, batch_size=BATCH_SIZE,
                                              shuffle=False)
    
    estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},
                                       config=RunConfig(model_dir=model_dir))
    estimator.train(input_fn=input_fn)

# Embedding dimension = 5 succeeds
train(5, model_dir=checkpoint_s3_dir + '/dim_5')
# Embedding dimension = 500 fails during checkpointing to S3
train(500, model_dir=checkpoint_s3_dir + '/dim_500')
```


### Describe the problem
I believe this is a TensorFlow Bug.

My example code, which trains using an Estimator and checkpoints to an S3 bucket I own, succeeds when the graph it produces is small, but fails during checkpointing when I increase the embedding_dimension variable. Everything else should be the same, so I suspect file sizes are the issue.

I've dug into this a bit and my guess is that when the graph size gets too large, the S3 operations time out during checkpointing due to the default client-side timeout on the AWS C++ S3 SDK. That issue is described here: https://stackoverflow.com/questions/38647444/aws-c-s3-sdk-putobjectrequest-unable-to-connect-to-endpoint

The S3 file system currently uses the default timeout: https://github.com/tensorflow/tensorflow/blob/b3d5ec90bc6b7ae7822ea82d82b41e529c5e047a/tensorflow/core/platform/s3/s3_file_system.cc#L513

Which is 3 seconds:
https://sdk.amazonaws.com/cpp/api/0.12.9/df/d19/struct_aws_1_1_client_1_1_client_configuration.html#a68c35ac8d14619e4bfc77d848fd89473

### Source code / logs
Example code above.

Full output is below:
```
INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_5', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ad8212510>, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Saving checkpoints for 1 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.
INFO:tensorflow:loss = 0.733852, step = 1
INFO:tensorflow:Saving checkpoints for 10 into s3://sagemaker-826912895975/checkpoint_testing/dim_5/model.ckpt.
INFO:tensorflow:Loss for final step: 0.693191.
INFO:tensorflow:Using config: {'_model_dir': 's3://sagemaker-826912895975/checkpoint_testing/dim_500', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': None, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6ac1672690>, '_tf_config': gpu_options {
  per_process_gpu_memory_fraction: 1.0
}
, '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}
INFO:tensorflow:Create CheckpointSaverHook.
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-1-1df0ebd892ce> in <module>()
     61 train(5, model_dir=checkpoint_s3_dir + '/dim_5')
     62 # Embedding dimension = 500 fails during checkpointing to S3
---> 63 train(500, model_dir=checkpoint_s3_dir + '/dim_500')

<ipython-input-1-1df0ebd892ce> in train(embedding_dim, model_dir)
     56     estimator = tf.estimator.Estimator(model_fn=model_fn, params={'embedding_dim': embedding_dim},
     57                                        config=RunConfig(model_dir=model_dir))
---> 58     estimator.train(input_fn=input_fn)
     59 
     60 # Embedding dimension = 5 succeeds

/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    300 
    301     saving_listeners = _check_listeners_type(saving_listeners)
--> 302     loss = self._train_model(input_fn, hooks, saving_listeners)
    303     logging.info('Loss for final step: %s.', loss)
    304     return self

/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.pyc in _train_model(self, input_fn, hooks, saving_listeners)
    781         loss = None
    782         while not mon_sess.should_stop():
--> 783           _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
    784       return loss
    785 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    519                           feed_dict=feed_dict,
    520                           options=options,
--> 521                           run_metadata=run_metadata)
    522 
    523   def should_stop(self):

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)
    890                               feed_dict=feed_dict,
    891                               options=options,
--> 892                               run_metadata=run_metadata)
    893       except _PREEMPTION_ERRORS as e:
    894         logging.info('An error was raised. This may be due to a preemption in '

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)
    965         raise six.reraise(*original_exc_info)
    966       else:
--> 967         raise six.reraise(*original_exc_info)
    968 
    969 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, *args, **kwargs)
    950   def run(self, *args, **kwargs):
    951     try:
--> 952       return self._sess.run(*args, **kwargs)
    953     except _PREEMPTION_ERRORS:
    954       raise

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in run(self, fetches, feed_dict, options, run_metadata)
   1014     options = options or config_pb2.RunOptions()
   1015     feed_dict = self._call_hook_before_run(run_context, actual_fetches,
-> 1016                                            feed_dict, options)
   1017 
   1018     # Do session run.

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.pyc in _call_hook_before_run(self, run_context, fetch_dict, user_feed_dict, options)
   1040     hook_feeds = {}
   1041     for hook in self._hooks:
-> 1042       request = hook.before_run(run_context)
   1043       if request is not None:
   1044         if request.fetches is not None:

/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/basic_session_run_hooks.pyc in before_run(self, run_context)
    432           ops.get_default_graph().as_graph_def(add_shapes=True),
    433           self._checkpoint_dir,
--> 434           ""graph.pbtxt"")
    435       saver_def = self._get_saver().saver_def if self._get_saver() else None
    436       graph = ops.get_default_graph()

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_io.pyc in write_graph(graph_or_graph_def, logdir, name, as_text)
     67   if as_text:
     68     file_io.atomic_write_string_to_file(path,
---> 69                                         text_format.MessageToString(graph_def))
     70   else:
     71     file_io.atomic_write_string_to_file(path, graph_def.SerializeToString())

/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in atomic_write_string_to_file(filename, contents, overwrite)
    421   write_string_to_file(temp_pathname, contents)
    422   try:
--> 423     rename(temp_pathname, filename, overwrite)
    424   except errors.OpError:
    425     delete_file(temp_pathname)

/usr/local/lib/python2.7/dist-packages/tensorflow/python/lib/io/file_io.pyc in rename(oldname, newname, overwrite)
    400   with errors.raise_exception_on_not_ok_status() as status:
    401     pywrap_tensorflow.RenameFile(
--> 402         compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)
    403 
    404 

/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/errors_impl.pyc in __exit__(self, type_arg, value_arg, traceback_arg)
    471             None, None,
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive
    475     # as there is a reference to status from this from the traceback due to

InternalError: : Unable to connect to endpoint
```
"
15866,TensorFlow fails to build with MPI,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.4
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.5
- **Python version**: 2.7.5
- **Bazel version (if compiling from source)**: 0.9.0 & 0.7.0
- **GCC/Compiler version (if compiling from source)**: 6.3.0
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel build --copt -mfma --copt -mavx2 --copt -O3 --verbose_failures -s -c opt //tensorflow/tools/pip_package:build_pip_package

### Describe the problem
TensorFlow fails to build with MPI with the following error:
```
ERROR: /disk/public_tf/tensorflow/tensorflow/contrib/mpi_collectives/BUILD:40:1: undeclared inclusion(s) in rule '//tensorflow/contrib/mpi_collectives:python/ops/_mpi_ops.so':
this rule is missing dependency declarations for the following files included by 'tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc':
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/statusor.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/port.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/error.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/status.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/lib/stringpiece.h'
  '/disk/public_tf/tensorflow/tensorflow/stream_executor/platform/logging.h'
tensorflow/contrib/mpi_collectives/kernels/mpi_ops.cc:128:6: warning: 'bool tensorflow::contrib::mpi_collectives::{anonymous}::IsGPUDevice() [with T = Eigen::GpuDevice]' defined but not used [-Wunused-function]
 bool IsGPUDevice<GPUDevice>() {
      ^~~~~~~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 379.332s, Critical Path: 76.80s
FAILED: Build did NOT complete successfully
```
This error persists even when Bazel 0.7.0 is used to build TensorFlow.

  
  "
15859,Eager mode: create_file_writer cannot be called twice,"Have I written custom code: No
OS Platform and Distribution: OS X 10.13.2
TensorFlow installed from: Source
TensorFlow version: cc9bdb70b88c76f293f27e29e91cb7739ba3fdc4
Bazel version: N/A
CUDA/cuDNN version: N/A
GPU model and memory: N/A
Exact command to reproduce:

```python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
tf.contrib.summary.create_file_writer(""/Users/malmaud/tmp/summaries"")
tf.contrib.summary.create_file_writer(""/Users/malmaud/tmp/summaries"")
```

The second call to `create_file_writer` gives:

```
---------------------------------------------------------------------------
AlreadyExistsError                        Traceback (most recent call last)
<ipython-input-5-4583ede32bc2> in <module>()
----> 1 tf.contrib.summary.create_file_writer(""/Users/malmaud/tmp/summaries"")

~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/summary/summary_ops.py in create_file_writer(logdir, max_queue, flush_millis, filename_suffix, name)
    210         max_queue=max_queue,
    211         flush_millis=flush_millis,
--> 212         filename_suffix=filename_suffix)
    213 
    214 

~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/summary/summary_ops.py in _make_summary_writer(name, factory, **kwargs)
    271   #   ops.get_default_session().run(node)
    272   ops.add_to_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME,
--> 273                         factory(resource, **kwargs))
    274   return SummaryWriter(resource)
    275 

~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/summary/gen_summary_ops.py in create_summary_file_writer(writer, logdir, max_queue, flush_millis, filename_suffix, name)
    145     _result = _execute.execute(b""CreateSummaryFileWriter"", 0,
    146                                inputs=_inputs_flat, attrs=_attrs, ctx=_ctx,
--> 147                                name=name)
    148     _result = None
    149   return _result

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   # pylint: enable=protected-access
     68   return tensors

~/anaconda3/lib/python3.6/site-packages/six.py in raise_from(value, from_value)

AlreadyExistsError: Resource localhost//N10tensorflow22SummaryWriterInterfaceE [Op:CreateSummaryFileWriter]
```

Explicitly calling `create_file_writer` with a unique `name` keyword argument solves the issue, but I'd have thought the name should be auto-uniquefied like with other TensorFlow ops.
  "
15852,Eager: crashed when using embedding_lookup in tfe.defun in tfe.GradientTape,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Win10
- **TensorFlow installed from (source or binary)**:binary
- **TensorFlow version (use command below)**:1.5.0dev20171230
- **Python version**: 3.6
- **Bazel version (if compiling from source)**:N/A
- **GCC/Compiler version (if compiling from source)**:N/A
- **CUDA/cuDNN version**:9.0/7.0
- **GPU model and memory**:pascal
- **Exact command to reproduce**:N/A
### Describe the problem
When I train a seq2seq model in eager, backward will raise a error:
```Python
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2162           with errors.raise_exception_on_not_ok_status() as status:
-> 2163             c_api.TF_OperationGetAttrValueProto(self._c_op, name, buf, status)
   2164           data = c_api.TF_GetBuffer(buf)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

InvalidArgumentError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    369     try:
--> 370       xla_compile = op.get_attr(""_XlaCompile"")
    371       xla_separate_compiled_gradients = op.get_attr(

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in get_attr(self, name)
   2166         # Convert to ValueError for backwards compatibility.
-> 2167         raise ValueError(str(e))
   2168       x = attr_value_pb2.AttrValue()

ValueError: Operation 'embedding_lookup' has no attr named '_XlaCompile'.

During handling of the above exception, another exception occurred:

IndexError                                Traceback (most recent call last)
<ipython-input-1-ed6ea9045e3f> in <module>()
     12         embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
     13         toy_data = np.ones((1, 10)).astype(np.int64)
---> 14         embedding_crash(toy_data, embed)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in decorated(*args, **kwds)
    639       arguments_to_functions[cache_key] = _defun_internal(
    640           name, func, args, kwds)
--> 641     return arguments_to_functions[cache_key](*args)
    642 
    643   return decorated

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args)
    461         self._extra_inputs):
    462       if not self._has_backprop:
--> 463         self._compute_backprop()
    464       return self._backprop_call(tensor_inputs)
    465 

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\eager\function.py in _compute_backprop(self)
    359             filtered_outputs,
    360             self._input_placeholders,
--> 361             grad_ys=self._out_grad_placeholders)
    362         shapes = tuple(x.shape for x in in_gradients if x is not None)
    363     captures = list(sorted(c.captured_tensors, key=lambda x: x.name))

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)
    373       xla_scope = op.get_attr(""_XlaScope"").decode()
    374     except ValueError:
--> 375       return grad_fn()  # Exit early
    376 
    377   if not xla_compile:

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py in <lambda>()
    607                 # functions.
    608                 in_grads = _MaybeCompile(
--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))
    610               else:
    611                 # For function call ops, we add a 'SymbolicGradient'

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\ops\resource_variable_ops.py in _GatherGrad(op, grad)
    895     # TODO(apassos): implement this for EAGER mode.
    896     while handle.op.type != ""VarHandleOp"":
--> 897       handle = handle.op.inputs[0]
    898   params_shape = gen_resource_variable_ops.variable_shape(handle)
    899   size = array_ops.expand_dims(array_ops.size(indices), 0)

c:\users\yanyan\anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in __getitem__(self, i)
   1992 
   1993     def __getitem__(self, i):
-> 1994       return self._inputs[i]
   1995 
   1996 # pylint: enable=protected-access

IndexError: list index out of range
```
No problem when remove @tfe.defun, but eager is very slow without defun so I need to compile model with tfe.defun.
code to reproduce error:
```Python
import tensorflow as tf
import tensorflow.contrib.eager as tfe
import numpy as np
tfe.enable_eager_execution()
@tfe.defun
def embedding_crash(x, embedding):
    return tf.nn.embedding_lookup(embedding, x)
with tf.device(""gpu:0""):
    with tfe.GradientTape() as g:
        embed = tfe.Variable(np.ones((10, 100)).astype(np.float32))
        toy_data = np.ones((1, 10)).astype(np.int64)
        embedding_crash(toy_data, embed)
```
  "
15851,Tensorflow: Non-deterministic behaviour with large model using while_loop,"Hello!
I believe to have found a bug in Tensorflow when running the code below. I am currently trying to build a neural transducer, and have stumbled across TF sometimes not returning any values for a function. I have not had the chance yet to test this out on another machine (no GPU, TF 1.4.1, Ubuntu 17.10). I am not sure whether this is indeed a bug or not, so I'm first posting it here. The code is redacted a bit to highlight only the parts that fail. [I've also posted to StackOverflow](https://stackoverflow.com/questions/48081063/tensorflow-non-deterministic-behaviour-with-large-model-using-while-loop) considering it might be an error in my code, but haven't got any response yet.

Notes:

- I believe the bug occurs around line 160, in the body of the while loop in the function run_full_transducer
- The session is returning [encoder_outputs, transducer_outputs]
- I do not use random functions
- As far as I can tell, if I remove the Print OP in line 164, the output is always 0

Example of a correct return value (more or less):
```
array([[[ 0.00811536, -0.00200322, -0.01177037,  0.03676344, -0.01909475,
             -0.03157664,  0.026092  ,  0.02367685, -0.01894805,  0.02832799,
              0.0377345 , -0.02583589, -0.02908566,  0.0299024 ,  0.00518877,
             -0.00064737,  0.01431572, -0.01053502, -0.01783628, -0.00382657,
              0.00076749, -0.02705991,  0.00112415, -0.0193013 ,  0.02346764,
              0.03014467,  0.02663364,  0.02503882,  0.03362656, -0.01877708,
              0.01859642,  0.02460729, -0.01395229, -0.03033791,  0.01177907,
             -0.03049169, -0.00389978,  0.02221515, -0.00073605,  0.01248251,
              0.00424051,  0.01070387,  0.02818898,  0.0321721 , -0.02462685,
              0.03495178, -0.02408989, -0.02742486,  0.00331823, -0.02311424,
             -0.01327039,  0.01095297,  0.02584363,  0.02083527, -0.01588045,
              0.02837921,  0.02100117,  0.00918638,  0.00109535, -0.02965789,
              0.01040822, -0.03240473,  0.00453057, -0.00603903]],
    
           [[ 0.01053647, -0.00457577, -0.01939731,  0.06317309, -0.03113565,
             -0.05525927,  0.04647589,  0.04213476, -0.03498235,  0.04962765,
              0.05989208, -0.04340284, -0.04777668,  0.05346756,  0.00395604,
             -0.0005207 ,  0.02079381, -0.01424338, -0.02584206, -0.00530154,
             -0.00031365, -0.04966826, -0.00091683, -0.03025239,  0.04526306,
              0.0595435 ,  0.0463665 ,  0.04578522,  0.05916505, -0.031725  ,
              0.03164144,  0.04257958, -0.02865831, -0.04795898,  0.01856991,
             -0.05512668, -0.00730711,  0.03953242,  0.00017992,  0.01710426,
              0.00754557,  0.01975578,  0.0469296 ,  0.05237873, -0.04435374,
              0.05924731, -0.04474678, -0.04605344,  0.00947831, -0.04284734,
             -0.01979787,  0.02003288,  0.04196753,  0.03900779, -0.02887472,
              0.05130195,  0.03419674,  0.0105699 ,  0.001114  , -0.0524303 ,
              0.01738651, -0.06084244,  0.01364262, -0.01153531]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```
Incorrect:
```
 [array([[[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],
    
           [[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
              0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]], dtype=float32), array([], shape=(0, 1, 3), dtype=float32)]
```

Code:
``` python
 import tensorflow as tf
    from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple
    from tensorflow.python.layers import core as layers_core
    import numpy as np
    # NOTE: Time major
    
    # Constants
    input_dimensions = 1
    vocab_size = 3
    input_embedding_size = 20
    encoder_hidden_units = 64
    inputs_embedded = True
    transducer_hidden_units = 64
    batch_size = 1
    GO_SYMBOL = vocab_size - 1  # TODO: Make these constants correct
    END_SYMBOL = vocab_size
    input_block_size = 2
    log_prob_init_value = 0
    
    
    # ---------------- Helper classes -----------------------
    
    
    # ----------------- Model -------------------------------
    embeddings = tf.Variable(tf.random_uniform([vocab_size, input_embedding_size], -1.0, 1.0), dtype=tf.float32)
    
    
    class Model(object):
        def __init__(self):
            self.encoder_inputs, self.encoder_inputs_length, self.encoder_hidden_state, \
            self.encoder_outputs, self.encoder_hidden_state_new = self.build_encoder_model()
            self.encoder_raw_outputs, self.trans_hidden_state, self.transducer_amount_outputs, \
            self.transducer_hidden_state_new, self.logits, self.decoder_prediction = self.build_transducer_model()
    
        def build_encoder_model(self):
            encoder_inputs = tf.Variable(tf.zeros(shape=(input_block_size, batch_size, input_dimensions)),
                                         dtype=tf.float32, name='encoder_inputs', trainable=False)
            encoder_inputs_length = tf.Variable([tf.shape(encoder_inputs)[0]], dtype=tf.int32,
                                                name='encoder_inputs_length', trainable=False)
            encoder_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, encoder_hidden_units)), dtype=tf.float32,
                                               name='encoder_hidden_state')  # Save the state as one tensor
    
            if inputs_embedded is True:
                encoder_inputs_embedded = encoder_inputs
            else:
                encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs)
    
            # Build model
            encoder_cell = tf.contrib.rnn.LSTMCell(encoder_hidden_units)
    
            # Build previous state
            encoder_hidden_c, encoder_hidden_h = tf.split(encoder_hidden_state, num_or_size_splits=2, axis=0)
            encoder_hidden_c = tf.reshape(encoder_hidden_c, shape=[-1, encoder_hidden_units])
            encoder_hidden_h = tf.reshape(encoder_hidden_h, shape=[-1, encoder_hidden_units])
            encoder_hidden_state_t = LSTMStateTuple(encoder_hidden_c, encoder_hidden_h)
    
            #   encoder_outputs: [max_time, batch_size, num_units]
            encoder_outputs, encoder_hidden_state_new = tf.nn.dynamic_rnn(
                encoder_cell, encoder_inputs_embedded,
                sequence_length=encoder_inputs_length, time_major=True,
                dtype=tf.float32, initial_state=encoder_hidden_state_t)
    
            # Modify output of encoder_hidden_state_new so that it can be fed back in again without problems.
            encoder_hidden_state_new = tf.concat([encoder_hidden_state_new.c, encoder_hidden_state_new.h], axis=0)
            encoder_hidden_state_new = tf.reshape(encoder_hidden_state_new, shape=[2, -1, encoder_hidden_units])
    
            return encoder_inputs, encoder_inputs_length, encoder_hidden_state, encoder_outputs, encoder_hidden_state_new
    
        def build_transducer_model(self):
            encoder_raw_outputs = tf.Variable(tf.zeros(shape=(input_block_size, 1, encoder_hidden_units)),
                                              dtype=tf.float32,
                                              name='encoder_raw_outputs')
            trans_hidden_state = tf.Variable(tf.zeros(shape=(2, 1, transducer_hidden_units)),
                                             dtype=tf.float32,
                                             name='trans_hidden_state')  # Save the state as one tensor
            transducer_amount_outputs = tf.Variable(0, dtype=tf.int32, name='transducer_amount_outputs',
                                                    trainable=False)
    
            # Model building
            helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(
                embedding=embeddings,
                start_tokens=tf.tile([GO_SYMBOL], [batch_size]),
                end_token=END_SYMBOL)
    
            attention_states = tf.transpose(encoder_raw_outputs,
                                            [1, 0, 2])  # attention_states: [batch_size, max_time, num_units]
    
            attention_mechanism = tf.contrib.seq2seq.LuongAttention(
                encoder_hidden_units, attention_states)
    
            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(
                tf.contrib.rnn.LSTMCell(transducer_hidden_units),
                attention_mechanism,
                attention_layer_size=transducer_hidden_units)
    
            projection_layer = layers_core.Dense(vocab_size, use_bias=False)
    
            # Build previous state
            trans_hidden_c, trans_hidden_h = tf.split(trans_hidden_state, num_or_size_splits=2, axis=0)
            trans_hidden_c = tf.reshape(trans_hidden_c, shape=[-1, transducer_hidden_units])
            trans_hidden_h = tf.reshape(trans_hidden_h, shape=[-1, transducer_hidden_units])
            trans_hidden_state_t = LSTMStateTuple(trans_hidden_c, trans_hidden_h)
    
            decoder = tf.contrib.seq2seq.BasicDecoder(
                decoder_cell, helper,
                decoder_cell.zero_state(1, tf.float32).clone(cell_state=trans_hidden_state_t),
                output_layer=projection_layer)
    
            outputs, transducer_hidden_state_new, _ = tf.contrib.seq2seq.dynamic_decode(decoder,
                                                                                        output_time_major=True,
                                                                                        maximum_iterations=transducer_amount_outputs)
            logits = outputs.rnn_output  # logits of shape [max_time,batch_size,vocab_size]
            decoder_prediction = outputs.sample_id  # For debugging
    
            # Modify output of transducer_hidden_state_new so that it can be fed back in again without problems.
            transducer_hidden_state_new = tf.concat(
                [transducer_hidden_state_new[0].c, transducer_hidden_state_new[0].h],
                axis=0)
            transducer_hidden_state_new = tf.reshape(transducer_hidden_state_new,
                                                     shape=[2, -1, transducer_hidden_units])
    
            return encoder_raw_outputs, trans_hidden_state, transducer_amount_outputs, transducer_hidden_state_new, \
                   logits, decoder_prediction
    
    
    model = Model()
    
    
    # ----------------- Alignment -------------------------
    
    # ----------------- Training --------------------------
    
    def run_full_transducer():
        # Inputs
        max_blocks = tf.placeholder(dtype=tf.int32, name='max_blocks')
        inputs_full_raw = tf.placeholder(shape=(None, batch_size, input_dimensions), dtype=tf.float32,
                                         name='inputs_full_raw')
        transducer_list_outputs = tf.placeholder(shape=(None,), dtype=tf.int32,
                                                 name='transducer_list_outputs')  # amount to output per block
    
        # Turn inputs into tensor which is easily readable
        inputs_full = tf.reshape(inputs_full_raw, shape=[max_blocks, input_block_size, batch_size, input_dimensions])
    
        # Outputs
        outputs_ta = tf.TensorArray(dtype=tf.float32, size=max_blocks)
    
        # Hidden states
        # TODO: make these correct
        encoder_hidden_init = tf.ones(shape=(2, 1, encoder_hidden_units))
        trans_hidden_init = tf.ones(shape=(2, 1, transducer_hidden_units))
    
        init_state = (0, outputs_ta, encoder_hidden_init, trans_hidden_init)
    
        def cond(current_block, outputs_int, encoder_hidden, trans_hidden):
            return current_block < max_blocks
    
        def body(current_block, outputs_int, encoder_hidden, trans_hidden):
            # Process encoder
            model.encoder_inputs = model.encoder_inputs.assign(inputs_full[current_block])
            model.encoder_inputs_length = model.encoder_inputs_length.assign([tf.shape(model.encoder_inputs)[0]])
            model.encoder_hidden_state = model.encoder_hidden_state.assign(encoder_hidden)
    
            # TODO: Error is SOMETIMES gone when using tf.Print
            current_block = tf.Print(current_block, [model.encoder_inputs], message='Enc in: ')
            #current_block = tf.Print(current_block, [model.encoder_outputs], message='Enc out: ')
    
            # Flow data from encoder to transducer
            model.encoder_raw_outputs = model.encoder_raw_outputs.assign(model.encoder_outputs)
            model.trans_hidden_state = model.trans_hidden_state.assign(trans_hidden)
            model.transducer_amount_outputs = model.transducer_amount_outputs.assign(transducer_list_outputs[current_block])
    
            # Note the outputs
            outputs_int = outputs_int.write(current_block, model.logits)
    
            return current_block + 1, outputs_int, model.encoder_hidden_state_new, model.transducer_hidden_state_new
    
        _, outputs_final, _, _ = tf.while_loop(cond, body, init_state)
    
        # Process outputs
        outputs = outputs_final.stack()  # Now the outputs are of shape [block, amount_of_trans_out, batch_size, vocab]
        outputs = tf.reshape(outputs, shape=(-1, 1, vocab_size))  # And now its [amount_outputs, batch_size, vocab]
    
        model.encoder_outputs = tf.Print(model.encoder_outputs, [model.encoder_outputs], message='Current block enc out: ')
    
        return max_blocks, inputs_full_raw, transducer_list_outputs, outputs, model.encoder_outputs
    
    # ---------------------- Testing -----------------------------
    
    
    # ---------------------- Management -----------------------------
    
    init = tf.global_variables_initializer()
    
    with tf.Session() as sess:
        sess.run(init)
    
        inp_max_blocks, inp_inputs_full_raw, inp_trans_list_out, out_outputs, enc_out = run_full_transducer()
    
        print sess.run([enc_out, out_outputs], feed_dict={
            inp_max_blocks: 3,
            inp_inputs_full_raw: np.ones(shape=(3 * input_block_size, 1, input_dimensions)),
            inp_trans_list_out: [1, 3, 2]
        })
```

Info about machine:
```

== cat /etc/issue ===============================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""17.10 (Artful Aardvark)""
VERSION_ID=""17.10""
VERSION_CODENAME=artful

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 7.2.0-8ubuntu3) 7.2.0
Copyright (C) 2017 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux nikita-coolboi 4.13.0-21-generic #24-Ubuntu SMP Mon Dec 18 17:29:16 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.13.3)
protobuf (3.5.1)
tensorflow (1.4.1)
tensorflow-tensorboard (0.4.0rc3)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.4.1
tf.GIT_VERSION = v1.4.0-19-ga52c8d9
tf.COMPILER_VERSION = v1.4.0-19-ga52c8d9
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
tf.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```
Have I written custom code Yes
OS Platform and Distribution Ubuntu 17.10 (Artful Aardvark)
TensorFlow installed from binary
TensorFlow version 1.4.1
Bazel version N/A
CUDA/cuDNN version N/A
GPU model and memory N/A
Exact command to reproduce Execute the code block as a python file a few times.

Thanks!
Nikita
  "
15850,unable to install from source with undefined external dependency target error,"
------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: r1.4.0
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: 0.9.0
- **GCC/Compiler version (if compiling from source)**: 5.4.0
- **CUDA/cuDNN version**: no
- **GPU model and memory**: no
- **Exact command to reproduce**:
`sudo bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package`
### Describe the problem
 I'm trying to install r1.4.0 from source with CPU version and follow the install guidelines on the official website.  But some errors occur. (The errors are shown in ""Source code / logs"" )
(1) It seems that the undefined external dependency target ""@local_config_sycl"" is referred in some files (eg. //tensorflow-1.4.0/ third_party/eigen3/build ).
(2) I am sure the openCL suppurt is disabled when configure.
(3) I tried to install version r1.0.1 from source, but the same errors occur.
(4) I tried to install version r1.4.0 from binary, and success.

Why this happens and how can I fix it? Thank you!

### Source code / logs
`ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:4:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:6:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:30:9: Traceback (most recent call last):
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 27
		cc_library(name = ""syclrt"", srcs = [sycl_libr..."")], <3 more arguments>)
	File ""/home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD"", line 30, in cc_library
		sycl_library_path
name 'sycl_library_path' is not defined
ERROR: /home/tangdehong/.cache/bazel/_bazel_root/4bf03e1269a0e4905c62fa69a980acb4/external/local_config_sycl/sycl/BUILD:39:1: Target '@local_config_sycl//sycl:using_sycl' contains an error and its package is in error and referenced by '@local_config_sycl//sycl:sycl'
ERROR: /home/tangdehong/OpenSourceCode/tensorflow-1.4.0/third_party/eigen3/BUILD:20:1: Target '@local_config_sycl//sycl:sycl' contains an error and its package is in error and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Loading failed
INFO: Elapsed time: 0.745s
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/contrib/losses ... (17 packages)
`
Thank you!"
15849,gcc: error: unrecognized command line option '-fcolor-diagnostics',"The issue is similar to [#1192](https://github.com/tensorflow/tensorflow/issues/1192) 

I am trying build Tensorflow from source on UBUNTU machine (16.04 LTS)
I am using following version of gcc and bazel

- gcc (Ubuntu 5.4.0-6ubuntu1~16.04.5) 5.4.0 20160609
- bazel Build label: 0.9.0

### Problem Description
I get a number of build errors during the **bazel build** phase.
All errors are similar to one below (I used --verbose_failures option for **bazel build**):


`ERROR: /home/murthy/tensorflow/tensorflow/core/BUILD:1656:1: Couldn't build file tensorflow/core/_objs/version_lib/tensorflow/core/util/version_info.pic.o: C++ compilation of rule '//tensorflow/core:version_lib' failed (Exit 1): gcc failed: error executing command 
  (cd /home/murthy/.cache/bazel/_bazel_murthy/72b24eaacfd1d73bef6a8acb540b8c44/execroot/org_tensorflow && \
  exec env - \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/home/murthy/.virtualenvs/dl4cv/bin/python \
    PYTHON_LIB_PATH=/home/murthy/.virtualenvs/dl4cv/lib/python3.5 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fcolor-diagnostics -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-march=native' -mssse3 -mfma -mcx16 -msse4.1 -msse4.2 -mpopcnt -mavx '-std=c++0x' -MD -MF bazel-out/k8-py3-opt/bin/tensorflow/core/_objs/version_lib/tensorflow/core/util/version_info.pic.d '-frandom-seed=bazel-out/k8-py3-opt/bin/tensorflow/core/_objs/version_lib/tensorflow/core/util/version_info.pic.o' -fPIC -iquote . -iquote bazel-out/k8-py3-opt/genfiles -iquote external/bazel_tools -iquote bazel-out/k8-py3-opt/genfiles/external/bazel_tools -isystem external/bazel_tools/tools/cpp/gcc3 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -msse3 -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c bazel-out/k8-py3-opt/genfiles/tensorflow/core/util/version_info.cc -o bazel-out/k8-py3-opt/bin/tensorflow/core/_objs/version_lib/tensorflow/core/util/version_info.pic.o)
gcc: error: unrecognized command line option '-fcolor-diagnostics'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
`
## What did I do?
I manually removed the `-fcolor-diagnostics` flag and gcc did not give an error.
further I looked up the man pages for gcc and was surprised to find

` Language Independent Options
           -fmessage-length=n 
           -fdiagnostics-show-location=[once|every-line] 
           -fdiagnostics-color=[auto|never|always]
           -fno-diagnostics-show-option -fno-diagnostics-show-caret
`

**Note**: the interchange in the words 'diagnostics' and 'color' in the man pages as compared to that in flags for **bazel build**

I am not sure if this is Tensorflow issue or bazel issue but am looking for a way to suppress the `-fcolor-diagnostics' flag or change it to `-fdiagnostics-color`
"
15848,*** Error in `python': double free or corruption (out): 0x00007fc5d674d9b0 ***,"I've built the latest version of TensorFlow from github repository with the following commands.

bazel build -s --config=mkl -c opt --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package

bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg

pip install /tmp/tensorflow_pkg/tensorflow-1.4.0-cp36-cp36m-linux_x86_64.whl

And get the following error after the import tensorflow command in python

Python 3.6.3 |Anaconda custom (64-bit)| (default, Oct 16 2017, 15:28:36) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> import tensorflow as tf
*** Error in `python': double free or corruption (out): 0x00007fc5d674d9b0 ***

Any suggestions on how to fix this issue?"
15847,Reinitializing an iterator throws an OutOfRangeError when using a MonitoredSession with NanTensorHook,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
Windows 10
- **TensorFlow installed from (source or binary)**:
binary
- **TensorFlow version (use command below)**:
1.4.0
- **Python version**: 
3.5.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
edit: none, using CPU only
- **GPU model and memory**:
- **Exact command to reproduce**:
See below under source code

### Describe the problem
When using a Monitored Training Session with a NanTensorHook and an Iterator from a Dataset, reinitializing the Iterator causes an OutOfRangeError. 
This is likely because the NanTensorHook adds the loss value to the SessionRunArgs, but the evaluation of the loss-value then fails since no more data are available from the iterator.

Ideally, there should be a way to reinitialize the iterator without the hooks beeing executed.

### Source code / logs
example code to reproduce the problem
```
import tensorflow as tf

dataset = tf.data.Dataset.range(100)
dataset = dataset.map(lambda x: (0, x))
dataset = dataset.batch(64)
iterator = dataset.make_initializable_iterator()
(label, element) = iterator.get_next()

# pseudo loss for the NanTensorHook
loss = tf.reduce_mean(1 - label)

global_step = tf.train.get_or_create_global_step()
scaffold = tf.train.Scaffold(local_init_op=iterator.initializer)

with tf.train.MonitoredTrainingSession(
        scaffold=scaffold,
        hooks=[tf.train.NanTensorHook(loss_tensor=loss)]) as sess:
    # Compute for 5 epochs.
    for epoch in range(5):
        print('epoch: ' + str(epoch))
        try:
            while not sess.should_stop():
                sess.run(element)
        except tf.errors.OutOfRangeError:
            print('end')

        if sess.should_stop():
            break

        # the following line silently fails, since an OutOfRangeError is thrown
        sess.run(iterator.initializer)
```
Removing the NanTensorHook or placing the `sess.run(iterator.initializer)` call inside an try-except-statement provides a workaround for this problem
```
        try:
            sess.run(iterator.initializer)
        except tf.errors.OutOfRangeError:
            print('Out of range errors occurs')
```

  "
15846,tf.losses.sigmoid_cross_entropy,"Hi,
it seems that there is a mistake in the api doc:
logits: Float [batch_size, num_classes] logits outputs of the network. 
however it should be:
logits: Float [batch_size, num_classes] Unscaled log probabilities.
since it said it is going to ""Creates a cross-entropy loss using tf.nn.sigmoid_cross_entropy_with_logits"""
15845,Support RNNs and LSTMs,"Feature Request for TF Lite - support for the following:
td_split
tf_pad (Padding is something that an RNN needs de-facto unless tensorflow RNNs accept lists of variously shaped tensors.)
tf_gather
tf_slice
LSTM Cells
Dynamic RNNs
Are there alternatives to the above in TF Lite?

Have I written custom code - No
OS Platform and Distribution - Android
TensorFlow installed from - https://github.com/tensorflow
TensorFlow version - 1.4
Bazel version - 0.5.4
CUDA/cuDNN version - NA
GPU model and memory - Na
Exact command to reproduce - NA




  "
15844,MonitoredTrainingSession aborts without error or exeption,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
No 
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
Linux Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**:
binary, pip install
- **TensorFlow version (use command below)**:
v1.4.0-19-ga52c8d9 1.4.1
- **Python version**: 
Python 3.6.3 :: Anaconda, Inc.
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
cuda_8.0.61.2, libcudnn.so.6.0.21
- **GPU model and memory**:
Titan-X, 32GB
- **Exact command to reproduce**:
During some training runs, this script just ends after few epochs with printing 'eval done'. It doesn't print any error nor an exception. In other runs, with the same setup in runs through. How could this happen that the for-loop stops even the epochs is smaller than 100?

Edit: I tried it also without the try-except block around the `MonitoredTrainingSession`, but it was the same: no exception, no error, epoch ~ 15 and printing ""eval done""


```python
    tf.logging.set_verbosity(tf.logging.INFO)
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '0'

    listener_eval = ExampleCheckpointSaverListener(fun_after_save=eval_model)
    listener_generate = ExampleCheckpointSaverListener(fun_after_save=generate)
    hooks_train = [
      tf.train.SummarySaverHook(output_dir=model_path, save_secs=60, scaffold=scaffold_train),
      tf.train.LoggingTensorHook(train_log_tensors, every_n_secs=30),
      tf.train.CheckpointSaverHook(checkpoint_dir=model_path, save_secs=600, scaffold=scaffold_train,
                                   listeners=[listener_eval, listener_generate])
    ]

    try:
      with tf.train.MonitoredTrainingSession(is_chief=True, checkpoint_dir=model_path, save_checkpoint_secs=None,
                                             hooks=hooks_train, save_summaries_secs=None, save_summaries_steps=None,
                                             config=sess_config, scaffold=scaffold_train, log_step_count_steps=1000, stop_grace_period_secs=20) as sess:

        for epochs in range(0, 100):
          print(epochs)
          try:
            while True:
              sess.run([train_op])
          except tf.errors.OutOfRangeError as e:
            print(""Epoch %d end."" % epochs)
            sess.run(train_reader.iterator.initializer)
    except:
      print(""MonitoredTrainingSession"")
      print(sys.exc_info())

    print(""eval done"")
```

[tf_env.txt](https://github.com/tensorflow/tensorflow/files/1603159/tf_env.txt)


  "
15841,MultiRNNCell with attention cause Dimensions error,"when i use MultiRNNCell  and attention for decoding,it goes to 

> ValueError: Dimensions must be equal, but are 2048 and 3072 for 'read/decode/seq_decode/while/BasicDecoderStep/seq_decode/attention_wrapper/attention_wrapper/multi_rnn_cell/cell_0/cell_0/lstm_cell/MatMul_1' (op: 'MatMul') with input shapes: [?,2048], [3072,4096].

I trace the code,and found it maybe a bug:
https://github.com/tensorflow/tensorflow/blob/r1.4/tensorflow/python/ops/rnn_cell_impl.py:
line:1066 calls to LSTMCell(line:600),its _linear1 is initialization from the first LSTMCELL, and will not initialization from the second LSTMCELL. my first LSTMCELL input is with attention.its shape is (?, emb+attention)
but,my second LSTMCELL input is without attention because it called in the while loop, which shape is (?, emb).,When second LSTMCELL use _linear1 is initialization from the first LSTMCELL,it goes to this error



  "
15840,Numerous ::`anonymous namespace':: Linking errors in Tensorflow 1.3.1 Windows build with GPU,"
Building with VS 2015 64bit and CUDA 8 and Cudnn v7 (cudnn64_7.dll).

All other projects in the solution got built. Even the project pywrap_tensorflow_internal_static got built. But, when building the project pywrap_tensorflow_internal for the dll one, it compiles successfully but gives numerous linking errors as below:

I also see that (probably) all of them contain the anonymous namespace - ``class tensorflow::`anonymous namespace'::``.

```
1>pywrap_tensorflow_internal.exp : warning LNK4070: /OUT:_pywrap_tensorflow_internal.pyd directive in .EXP differs from output filename 'D:\rough\tensorflow-1.3.1\tensorflow\contrib\cmake\build\Release\pywrap_tensorflow_internal.dll'; ignoring directive
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: void __cdecl tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16>::Initialize<1>(class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int)'::`2'::$TSS0"" (?$TSS0@?1???$Initialize@$00@?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@QEAAXAEBV?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@H@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: void __cdecl tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16>::Initialize<0>(class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int)'::`2'::$TSS0"" (?$TSS0@?1???$Initialize@$0A@@?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@QEAAXAEBV?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@H@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""TSS0<`template-parameter-2',tensorflow::`anonymous namespace'::M::cale_independent_strtonum,cointerface ?? :: ?? ::HA::tensorflow::Z::AMPEBDPEAPEBD>"" (?$TSS0@?1???$locale_independent_strtonum@M@?A0x77a2fbcf@tensorflow@@YAMPEBDPEAPEBD@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""TSS0<`template-parameter-2',tensorflow::`anonymous namespace'::N::cale_independent_strtonum,cointerface ?? :: ?? ::HA::tensorflow::Z::ANPEBDPEAPEBD>"" (?$TSS0@?1???$locale_independent_strtonum@N@?A0x77a2fbcf@tensorflow@@YANPEBDPEAPEBD@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `private: static void __cdecl tensorflow::SparseMatMul<float,float>::ComputeOutputBlock(class std::vector<struct tensorflow::`anonymous namespace'::SparseSlice<float> *,class std::allocator<struct tensorflow::`anonymous namespace'::SparseSlice<float> *> > const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int,int,int,bool,bool,class Eigen::TensorMap<class Eigen::Tensor<float,2,1,__int64>,16,struct Eigen::MakePointer> *)'::`2'::$TSS0"" (?$TSS0@?1??ComputeOutputBlock@?$SparseMatMul@MM@tensorflow@@CAXAEBV?$vector@PEAU?$SparseSlice@M@?A0x20ed32c2@tensorflow@@V?$allocator@PEAU?$SparseSlice@M@?A0x20ed32c2@tensorflow@@@std@@@std@@AEBV?$TensorMap@V?$Tensor@$$CBM$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@HHH_N2PEAV?$TensorMap@V?$Tensor@M$01$00_J@Eigen@@$0BA@UMakePointer@2@@7@@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `private: static void __cdecl tensorflow::SparseMatMul<float,struct tensorflow::bfloat16>::ComputeOutputBlock(class std::vector<struct tensorflow::`anonymous namespace'::SparseSlice<float> *,class std::allocator<struct tensorflow::`anonymous namespace'::SparseSlice<float> *> > const &,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int,int,int,bool,bool,class Eigen::TensorMap<class Eigen::Tensor<float,2,1,__int64>,16,struct Eigen::MakePointer> *)'::`2'::$TSS0"" (?$TSS0@?1??ComputeOutputBlock@?$SparseMatMul@MUbfloat16@tensorflow@@@tensorflow@@CAXAEBV?$vector@PEAU?$SparseSlice@M@?A0x20ed32c2@tensorflow@@V?$allocator@PEAU?$SparseSlice@M@?A0x20ed32c2@tensorflow@@@std@@@std@@AEBV?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@HHH_N2PEAV?$TensorMap@V?$Tensor@M$01$00_J@Eigen@@$0BA@UMakePointer@2@@7@@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `private: static void __cdecl tensorflow::SparseMatMul<struct tensorflow::bfloat16,float>::ComputeOutputBlock(class std::vector<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *,class std::allocator<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *> > const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int,int,int,bool,bool,class Eigen::TensorMap<class Eigen::Tensor<float,2,1,__int64>,16,struct Eigen::MakePointer> *)'::`2'::$TSS0"" (?$TSS0@?1??ComputeOutputBlock@?$SparseMatMul@Ubfloat16@tensorflow@@M@tensorflow@@CAXAEBV?$vector@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@V?$allocator@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@@std@@@std@@AEBV?$TensorMap@V?$Tensor@$$CBM$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@HHH_N2PEAV?$TensorMap@V?$Tensor@M$01$00_J@Eigen@@$0BA@UMakePointer@2@@7@@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `private: static void __cdecl tensorflow::SparseMatMul<struct tensorflow::bfloat16,struct tensorflow::bfloat16>::ComputeOutputBlock(class std::vector<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *,class std::allocator<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *> > const &,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int,int,int,bool,bool,class Eigen::TensorMap<class Eigen::Tensor<float,2,1,__int64>,16,struct Eigen::MakePointer> *)'::`2'::$TSS0"" (?$TSS0@?1??ComputeOutputBlock@?$SparseMatMul@Ubfloat16@tensorflow@@U12@@tensorflow@@CAXAEBV?$vector@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@V?$allocator@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@@std@@@std@@AEBV?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@HHH_N2PEAV?$TensorMap@V?$Tensor@M$01$00_J@Eigen@@$0BA@UMakePointer@2@@7@@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `bool __cdecl tensorflow::`anonymous namespace'::IsGradientNode(class tensorflow::Graph const *,class tensorflow::Node const *)'::`2'::$TSS0"" (?$TSS0@?1??IsGradientNode@?A0xd76da148@tensorflow@@YA_NPEBVGraph@3@PEBVNode@3@@Z@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<signed char>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@C@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<unsigned char>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@E@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<short>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@F@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<int>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@H@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<float>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@M@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<double>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@N@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<struct Eigen::QInt16>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@UQInt16@Eigen@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<struct Eigen::QInt32>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@UQInt32@Eigen@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<struct Eigen::QInt8>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@UQInt8@Eigen@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<struct Eigen::QUInt16>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@UQUInt16@Eigen@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<struct Eigen::QUInt8>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@UQUInt8@Eigen@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<struct Eigen::half>::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@Uhalf@Eigen@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<class std::basic_string<char,struct std::char_traits<char>,class std::allocator<char> > >::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""int `public: virtual class tensorflow::gtl::InlinedVector<enum tensorflow::DataType,4> const & __cdecl tensorflow::`anonymous namespace'::DenseToSparseBatchDatasetOp::Dataset<class std::complex<float> >::output_dtypes(void)const '::`2'::$TSS0"" (?$TSS0@?1??output_dtypes@?$Dataset@V?$complex@M@std@@@DenseToSparseBatchDatasetOp@?A0x0fde61d7@tensorflow@@UEBAAEBV?$InlinedVector@W4DataType@tensorflow@@$03@gtl@5@XZ@4HA)
...
...
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""class Eigen::array<int,2> const `private: static void __cdecl tensorflow::SparseMatMul<struct tensorflow::bfloat16,float>::ComputeOutputBlock(class std::vector<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *,class std::allocator<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *> > const &,class Eigen::TensorMap<class Eigen::Tensor<float const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int,int,int,bool,bool,class Eigen::TensorMap<class Eigen::Tensor<float,2,1,__int64>,16,struct Eigen::MakePointer> *)'::`26'::zero"" (?zero@?BK@??ComputeOutputBlock@?$SparseMatMul@Ubfloat16@tensorflow@@M@tensorflow@@CAXAEBV?$vector@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@V?$allocator@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@@std@@@std@@AEBV?$TensorMap@V?$Tensor@$$CBM$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@HHH_N2PEAV?$TensorMap@V?$Tensor@M$01$00_J@Eigen@@$0BA@UMakePointer@2@@7@@Z@4V?$array@H$01@7@B)
1>pywrap_tensorflow_internal.exp : error LNK2001: unresolved external symbol ""class Eigen::array<int,2> const `private: static void __cdecl tensorflow::SparseMatMul<struct tensorflow::bfloat16,struct tensorflow::bfloat16>::ComputeOutputBlock(class std::vector<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *,class std::allocator<struct tensorflow::`anonymous namespace'::SparseSlice<struct tensorflow::bfloat16> *> > const &,class Eigen::TensorMap<class Eigen::Tensor<struct tensorflow::bfloat16 const ,2,1,__int64>,16,struct Eigen::MakePointer> const &,int,int,int,bool,bool,class Eigen::TensorMap<class Eigen::Tensor<float,2,1,__int64>,16,struct Eigen::MakePointer> *)'::`26'::zero"" (?zero@?BK@??ComputeOutputBlock@?$SparseMatMul@Ubfloat16@tensorflow@@U12@@tensorflow@@CAXAEBV?$vector@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@V?$allocator@PEAU?$SparseSlice@Ubfloat16@tensorflow@@@?A0x20ed32c2@tensorflow@@@std@@@std@@AEBV?$TensorMap@V?$Tensor@$$CBUbfloat16@tensorflow@@$01$00_J@Eigen@@$0BA@UMakePointer@2@@Eigen@@HHH_N2PEAV?$TensorMap@V?$Tensor@M$01$00_J@Eigen@@$0BA@UMakePointer@2@@7@@Z@4V?$array@H$01@7@B)
1>D:\rough\tensorflow-1.3.1\tensorflow\contrib\cmake\build\Release\pywrap_tensorflow_internal.dll : fatal error LNK1120: 1937 unresolved externals
========== Build: 0 succeeded, 1 failed, 0 up-to-date, 0 skipped ==========

```
By any chance, does anyone know what these errors are hinting towards?

Thanks in advance.


  "
15839,"Problem in ""/tensorflow/examples"" project to build an android app","## As we know, there will build three APP and one TF speech app in the 'tensorflow/examples'. 
## But, I just want to use the  detectorActivity to build an app, aim at to use this app to do a porject named 'Image Recognition'.
##  So I need to delete three other java file like ClassifierActivity、StylizeActivity、SpeechActvity and so on, and download the model that detectorActivity need. like this

![1](https://user-images.githubusercontent.com/33651882/34554975-6605fa0e-f16a-11e7-889a-418d99ac1498.png)

## But, when I choose to build apk, there was a problem that ""Execution failed for task: downloadFile  org.apache.http.conn.ConnectTineoutException:......."" 
## I use the windows and android studio to build this project. 
#  **Is anyone can help me to separate this example just retain the  detectorActivity  and use our's model in android:assets , to help me build an app to do 'Image Recognition' Project. Thx every friends**
  
  
  "
15838,Gif can't be decoded. InvalidArgumentError: Invalid GIF data,"## System information
**Have I written custom code : yes
OS Platform and Distribution : Mac OS 10.12.3
TensorFlow installed from : pip3
TensorFlow version : 1.4
Bazel version : N/A
CUDA/cuDNN version : N/A
GPU model and memory : N/A**

## Describe the problem
![0071qvrrgy1fn3h6v55gag308w0adx6p](https://user-images.githubusercontent.com/8256827/34550343-04b28b42-f14b-11e7-9b8a-729a82ba4742.gif)

The gif can be decoded by PIL,but the error occurred when I used tf.image.decode_gif to decode.

```
def load_gif(image_path, sess):
    image = tf.read_file(image_path)
    image = tf.image.decode_gif(image)
    return sess.run(image)

load_gif('0071Qvrrgy1fn3h6v55gag308w0adx6p',sess))
```

The error is:
```
NotFoundError                             Traceback (most recent call last)
/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1322     try:
-> 1323       return fn(*args)
   1324     except errors.OpError as e:

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run_fn(session, feed_dict, fetch_list, target_list, options, run_metadata)
   1301                                    feed_dict, fetch_list, target_list,
-> 1302                                    status, run_metadata)
   1303 

/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    472             compat.as_text(c_api.TF_Message(self.status.status)),
--> 473             c_api.TF_GetCode(self.status.status))
    474     # Delete the underlying status object from memory otherwise it stays alive

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

During handling of the above exception, another exception occurred:

NotFoundError                             Traceback (most recent call last)
<ipython-input-70-7b1d3fa2f712> in <module>()
      1 print(load_gif('/Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p',
----> 2                sess))

<ipython-input-55-fd8c43263043> in load_gif(image_path, sess)
      3     # image = tf.image.decode_png(image, channels=3)
      4     image = tf.image.decode_gif(image)
----> 5     return sess.run(image)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    887     try:
    888       result = self._run(None, fetches, feed_dict, options_ptr,
--> 889                          run_metadata_ptr)
    890       if run_metadata:
    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1118     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1119       results = self._do_run(handle, final_targets, final_fetches,
-> 1120                              feed_dict_tensor, options, run_metadata)
   1121     else:
   1122       results = []

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1315     if handle is None:
   1316       return self._do_call(_run_fn, self._session, feeds, fetches, targets,
-> 1317                            options, run_metadata)
   1318     else:
   1319       return self._do_call(_prun_fn, self._session, handle, feeds, fetches)

/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1334         except KeyError:
   1335           pass
-> 1336       raise type(e)(node_def, op, message)
   1337 
   1338   def _extend_graph(self):

NotFoundError: /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]

Caused by op 'ReadFile_21', defined at:
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python3/3.6.2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/site-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/ioloop.py"", line 177, in start
    super(ZMQIOLoop, self).start()
  File ""/usr/local/lib/python3.6/site-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 440, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 472, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py"", line 414, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/site-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2698, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2808, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 2862, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-70-7b1d3fa2f712>"", line 2, in <module>
    sess))
  File ""<ipython-input-55-fd8c43263043>"", line 2, in load_gif
    image = tf.read_file(image_path)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py"", line 376, in read_file
    ""ReadFile"", filename=filename, name=name)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 2956, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1470, in __init__
    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access

NotFoundError (see above for traceback): /Users/chunchang/Downloads/0071Qvrrgy1fn3h6v55gag308w0adx6p; No such file or directory
	 [[Node: ReadFile_21 = ReadFile[_device=""/job:localhost/replica:0/task:0/device:CPU:0""](ReadFile_21/filename)]]```
  
  
  
  "
15837,occurs error to convert from pb to tflite file format,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**:  v1.4.0
- **Python version**: 3.4
- **Bazel version (if compiling from source)**:  0.5.4
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**:
bazel build -c opt --copt=-msse4.1 --copt=-msse4.2 tensorflow/contrib/lite/toco:toco
bazel-bin/tensorflow/contrib/lite/toco/toco  \
  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \
  --input_file=/home/yh/tttt/aaa.pb \
  --output_file=/home/yh/tttt/aaa.lite \
  --inference_type=FLOAT \
  --input_type=FLOAT \
  --input_arrays=input \
  --output_arrays=MobilenetV1/Predictions/ArgMax_2 \
  --input_shapes=1,64,64,3

### Describe the problem

current tensorflow lite couldn't support below operation?
If we use below operation, how can I do something?
Here is the warning and error message I got :
### Source code / logs
2018-01-04 16:04:35.910769: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.910856: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.910902: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.910928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.910971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911035: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911066: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911129: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911165: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911188: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911283: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911320: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911355: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911377: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911415: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911450: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911473: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911513: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911571: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911601: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911624: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911654: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911677: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911707: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911759: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911781: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.911907: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.911931: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.911968: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.912003: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912026: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912063: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.912097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912158: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.912192: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912215: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912252: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.912286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912337: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912390: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912412: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912442: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912464: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912496: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912520: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912550: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912572: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912602: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912625: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912655: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912708: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912730: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912760: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912782: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912837: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912867: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912890: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912919: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.912973: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.912996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913026: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913050: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913080: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913102: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913132: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913184: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913218: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913258: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913282: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913312: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913335: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913365: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913418: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913441: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913471: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913525: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913577: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913653: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913734: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913757: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913786: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913808: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913838: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913861: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913913: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913942: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.913965: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.913994: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914016: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914097: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914120: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914183: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914238: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914260: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914289: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914310: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914360: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914408: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914457: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Pow
2018-01-04 16:04:35.914500: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914523: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914557: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914721: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914754: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914778: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914812: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914864: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914885: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.914966: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.914993: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915026: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915078: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915099: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915127: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915149: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915245: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915274: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915297: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915347: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915376: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915397: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915472: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915498: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915528: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915549: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915578: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915627: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915649: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915718: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915745: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915795: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915845: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915895: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.915971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.915998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916027: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916098: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916126: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916147: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916217: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916243: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916271: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916292: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916320: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916342: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916370: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916391: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916495: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916524: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916545: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916594: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916622: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916644: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916713: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916739: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916768: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916817: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916839: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916866: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916888: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.916964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.916991: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917041: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917089: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917117: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917138: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917234: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917263: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917288: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917317: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917338: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917366: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917387: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917463: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomUniform
2018-01-04 16:04:35.917497: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917519: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917547: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.917568: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: SoftmaxCrossEntropyWithLogits
2018-01-04 16:04:35.917801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: SaveV2
2018-01-04 16:04:35.917850: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.917873: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.917922: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917949: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.917971: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.917998: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918019: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918046: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918067: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918094: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918175: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918216: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918239: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918269: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918286: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918308: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918325: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918346: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918384: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918399: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918421: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918438: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918477: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918514: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918535: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918562: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918583: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918630: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918657: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918678: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918705: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918725: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918752: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918774: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918801: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918821: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918848: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918867: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918895: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918916: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918943: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.918963: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.918990: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919010: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919037: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919057: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919084: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919131: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919151: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919178: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919246: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919273: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919321: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919368: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919388: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919415: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919435: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919461: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919480: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919506: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919527: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919574: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919620: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919669: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919696: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919717: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919743: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919789: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919809: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919835: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919855: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919901: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919928: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919948: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.919974: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.919996: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920042: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920088: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920114: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920135: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920161: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920181: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920208: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920228: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920256: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920278: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920305: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920326: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920353: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920373: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920400: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920420: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920447: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920468: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920494: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920516: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920542: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920563: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920589: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920610: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920636: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920656: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920682: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920702: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920729: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920749: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920775: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920796: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920823: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920843: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920870: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920891: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920938: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.920964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.920984: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921011: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921031: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921057: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921077: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921104: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921125: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921152: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921172: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921199: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921219: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921246: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921294: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921314: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921341: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921362: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921389: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921410: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921437: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921459: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921486: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921507: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921533: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921580: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921599: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921626: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921647: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921674: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921694: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921720: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921740: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921767: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921787: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921814: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921834: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921862: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921882: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921908: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921929: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.921956: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.921976: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922002: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922022: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922049: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922068: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922095: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922115: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922154: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922187: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922210: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922226: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922247: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922264: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922285: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922301: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922322: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922339: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922361: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922377: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922398: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922430: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922458: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922478: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922505: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922526: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922553: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922573: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922600: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922621: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922648: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922668: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922695: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922715: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922742: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922763: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922790: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RestoreV2
2018-01-04 16:04:35.922811: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922854: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: VariableV2
2018-01-04 16:04:35.922878: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Assign
2018-01-04 16:04:35.922918: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Equal
2018-01-04 16:04:35.922964: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: ScalarSummary
2018-01-04 16:04:35.922987: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: ScalarSummary
2018-01-04 16:04:35.923009: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: ScalarSummary
2018-01-04 16:04:35.923028: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: MergeSummary
2018-01-04 16:04:35.923082: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Prod
2018-01-04 16:04:35.923107: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: Prod
2018-01-04 16:04:35.923150: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: ZerosLike
2018-01-04 16:04:35.923209: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: BiasAddGrad
2018-01-04 16:04:35.923248: F tensorflow/contrib/lite/toco/import_tensorflow.cc:924] Check failed: GetBoolAttr(node, ""transpose_b"") == false (1 vs. 0)
Aborted (core dumped)

"
15835,"When data become large,parition variables can not initialized successfully","i use tensorflow to distributed trainning models, i use the partition valriables to store an array data, when the data is not so bigger, everything looks ok,but when the array data become larger, when the session initialize, the partition variables can not initialized and the session will wait util time out.

Describe the problem

Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request. the feat_info can initialize successfully, but the adj_info cannot initialized. the adj_info is larger than feat_info

 i use monitored_session or supervisor to do the initialize_variables, when data is not so large, it will initialize successfully, my machine have more than 300GB memory is enough for the larger variable

Have I written custom code
yes
OS Platform and Distribution
linux platform
TensorFlow installed from
N/A
TensorFlow version
r1.4
Bazel version
CUDA/cuDNN version
no gpu
GPU model and memory
no gpu
Exact command to reproduce
N/A

Source code / logs

i use ps_num = 4, worker_num =4 and i also try some other distributed config, like ps_num=1, worker_num=4, the result is the same
source code:
with tf.device(tf.train.replica_device_setter(
worker_device=""/job:worker/task:%d"" % task_id,
cluster=cluster_spec)):

  feat_info = tf.get_variable(""feature_info"", (len(id_map),FLAGS.features_column), tf.float32, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))
  adj_info = tf.get_variable(""adj_info"", (len(id_map),FLAGS.max_degree), tf.int64, trainable=False, partitioner=tf.fixed_size_partitioner(num_workers))
 
  with tf.device('/job:worker/task:%d' %task_id):
      adj_local = tf.Variable(tf.constant(minibatch.adj, dtype=tf.int64), trainable=False, name=""adj_local"", collections=[tf.GraphKeys.LOCAL_VARIABLES])
      feat_local = tf.Variable(tf.constant(features, dtype=tf.float32), trainable=False, name=""feat_local"", collections=[tf.GraphKeys.LOCAL_VARIABLES])
 
  length, begin, end = split_node_by_task(len(id_map), task_id, num_workers)
  adj = tf.nn.embedding_lookup(adj_info, [x for x in range(begin, end)])
  adj = adj_local
  
  feat = tf.nn.embedding_lookup(feat_info, [x for x in range(begin, end)])
  feat = feat_local
log:
2017-12-08 23:54:17.377290: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session c2b3ba9b700261ba with config:
INFO:tensorflow:Waiting for model to be ready. Ready_for_local_init_op: None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15
2017-12-09 00:00:35.637019: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session f35fcf332e3908ec with config:
INFO:tensorflow:Waiting for model to be ready. Ready_for_local_init_op: None, ready: Variables not initialized: adj_info/part_0, adj_info/part_1, adj_info/part_2, adj_info/part_3, adj_info/part_4, adj_info/part_5, adj_info/part_6, adj_info/part_7, adj_info/part_8, adj_info/part_9, adj_info/part_10, adj_info/part_11, adj_info/part_12, adj_info/part_13, adj_info/part_14, adj_info/part_15
and it will alway waiting adj_info to initialize"
15834,I think some bug in  tf.contrib.layers.l2_regularizer,"
### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:win10
- **TensorFlow installed from (source or binary)**:install
- **TensorFlow version (use command below)**:1.5
- **Python version**: 3.5
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
when I call the  tf.contrib.layers.l2_regularizer(0.5)(w), I was toll that ""Expected int64, got 0.5 of type 'float' instead."" But the doc says that it need a float clearly and as a weight it can't be an integer.

### Source code / logs
` l2 = 0
for w in tf.global_variables():
    l2 += tf.contrib.layers.l2_regularizer(0.5)(w)
loss = tf.losses.softmax_cross_entropy(onehot_labels=one_hot_labels,logits=result_vector)+l2`

  "
15833,gradient function missing for odeint,"I am trying to program a optimization problem using TensorFlow. The cost function depends on the outcome of an ODE solved by TensorFlow. However I find that TensorFlow's auto differential does not support the function which contains odeint.

odeint: tensorflow.org/api_docs/python/tf/contrib/integrate/odeint

It would great be of great help if we could have it.

Have I written custom code: N/A
OS Platform and Distribution: Ubuntu 16.04 
TensorFlow installed from: official website pip 
TensorFlow version: 14.01
Bazel version: N/A
CUDA/cuDNN version: 8.0
GPU model and memory: GTX1080Ti, not relevant
Exact command to reproduce: N/A
  "
15832,Feature Request: saver.save mkdir if directory not exist,"### Describe the problem
`saver.save(sess, 'my-model') `
returns error if directory not exist.
It would be nice if saver can automatically create the missing directory.
`Traceback (most recent call last):
  File ""tf_voice_recognition.py"", line 783, in <module>
    saver.save(sess, 'my-model')
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1594, in save
    raise exc
ValueError: Parent directory of my-model doesn't exist, can't save.
`"
15831,Using keras and tf.keras has different result,"### System information
- **TensorFlow)**:
- **Linux Ubuntu 16.04**:
- **TensorFlow installed from Binary**:
- **TensorFlow version (1.4)**:
- **Python version 3.6.3**: 

### Describe the problem
I am using tf.keras and keras but it has a different result. I am using the code below for sentiment analysis classification using imdb dataset.
### Source code / logs
```
model = Sequential()
model.add(Embedding(n_unique_words, n_dim, input_length=max_reviw_length))
model.add(Flatten())
model.add(Dense(n_dense, activation='relu'))
model.add(Dropout(dropout))
model.add(Dense(1, activation='sigmoid'))
model.summary() 
```


```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_1 (Embedding)      (None, 100, 64)           320000    
_________________________________________________________________
flatten_1 (Flatten)          (None, 6400)              0         
_________________________________________________________________
dense_1 (Dense)              (None, 64)                409664    
_________________________________________________________________
dropout_1 (Dropout)          (None, 64)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 65        
=================================================================
Total params: 729,729
Trainable params: 729,729
Non-trainable params: 0
__________________________

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(x_valid, y_valid), callbacks=[modelcheckpoint])
```
preprocessing all exact the same

tf.keras result

```
Train on 25000 samples, validate on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 9s - loss: 0.7412 - acc: 0.4990 - val_loss: 0.6932 - val_acc: 0.50000.4
Epoch 2/4
25000/25000 [==============================] - 7s - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 3/4
25000/25000 [==============================] - 7s - loss: 0.6932 - acc: 0.5001 - val_loss: 0.6931 - val_acc: 0.5000
Epoch 4/4
25000/25000 [==============================] - 7s - loss: 0.6932 - acc: 0.4942 - val_loss: 0.6931 - val_acc: 0.5000

<tensorflow.python.keras._impl.keras.callbacks.History at 0x7fad685e7c18>
```

keras result

```
Train on 25000 samples, validate on 25000 samples
Epoch 1/4
25000/25000 [==============================] - 9s 344us/step - loss: 0.5607 - acc: 0.6900 - val_loss: 0.3622 - val_acc: 0.8404
Epoch 2/4
25000/25000 [==============================] - 8s 308us/step - loss: 0.2849 - acc: 0.8849 - val_loss: 0.3486 - val_acc: 0.8446
Epoch 3/4
25000/25000 [==============================] - 8s 305us/step - loss: 0.1179 - acc: 0.9642 - val_loss: 0.4183 - val_acc: 0.8339
Epoch 4/4
25000/25000 [==============================] - 8s 307us/step - loss: 0.0252 - acc: 0.9960 - val_loss: 0.5202 - val_acc: 0.8345

<keras.callbacks.History at 0x7f560b85d940>


```"
15830,tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1),"###System information
Have I written custom code: Yes, 
OS Platform and Distribution: Ubuntu14.04
TensorFlow installed from: source build w/ Bazel
TensorFlow version: 1.4
Python version: Anaconda 3.5.2
Bazel version: 0.9.0
GCC/Compiler version (if compiling from source): gcc version 4.8.4
CUDA/cuDNN version: Not relevant
GPU model and memory: Not  relevant
Exact command to reproduce: Not relevant


### Describe the problem

I construct a network with only bilinear_resize operation (I use the tf.image.resize_bilinear) and add operation, and  convert it to a tflite model successfully. However,  when I run the tflite mode, it comes to the errors as follows:

java.lang.NullPointerException: Can not allocate memory for the given inputs: tensorflow/contrib/lite/kernels/resize_bilinear.cc:42 NumInputs(node) != 1 (2 != 1)

I find the code line in resize_bilinear.cc:42 as follows:
TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);

Is it right to modify the code line to : 
TF_LITE_ENSURE(context, NumInputs(node) == 1 || NumInputs(node) == 2); 
   


### Source code / logs
def network():
      img = tf.placeholder(name='img', dtype=tf.float32, shape=(1,100,100,3))
      img = tf.layers.conv2d(img, 3, (3,3), padding='same', name='conv1')
      img = tf.image.resize_bilinear(img, [200,200])
      var = tf.get_variable('weights', dtype=tf.float32, shape=(1,200,200,3))
      val = img + var
      out = tf.identity(val, name='out')


  "
15827,ci.tensorflow.org lacks a security certificate,"### The Problem
https://ci.tensorflow.org/ now lacks a security certificate (or it expired).
To repro, visit https://ci.tensorflow.org/. Chrome will note that the connection is not private.

This is breaking TensorBoard's tests that run on travis. The tests `pip install` nightly versions of TensorFlow from these URLs:

- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl 
- https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON3,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp34-cp34m-linux_x86_64.whl

### Error Logs from a Failed Test Run

> Collecting tensorflow==1.head from https://ci.tensorflow.org/view/Nightly/job/nightly-matrix-cpu/TF_BUILD_IS_OPT=OPT,TF_BUILD_IS_PIP=PIP,TF_BUILD_PYTHON_VERSION=PYTHON2,label=cpu-slave/lastSuccessfulBuild/artifact/pip_test/whl/tensorflow-1.head-cp27-none-linux_x86_64.whl
> Exception:
> Traceback (most recent call last):
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/basecommand.py"", line 215, in main
>     status = self.run(options, args)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/commands/install.py"", line 335, in run
>     wb.build(autobuilding=True)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/wheel.py"", line 749, in build
>     self.requirement_set.prepare_files(self.finder)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 380, in prepare_files
>     ignore_dependencies=self.ignore_dependencies))
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/req/req_set.py"", line 620, in _prepare_file
>     session=self.session, hashes=hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 821, in unpack_url
>     hashes=hashes
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 659, in unpack_http_url
>     hashes)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 853, in _download_http_url
>     stream=True,
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 488, in get
>     return self.request('GET', url, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/download.py"", line 386, in request
>     return super(PipSession, self).request(method, url, *args, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 475, in request
>     resp = self.send(prep, **send_kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/sessions.py"", line 596, in send
>     r = adapter.send(request, **kwargs)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/cachecontrol/adapter.py"", line 47, in send
>     resp = super(CacheControlAdapter, self).send(request, **kw)
>   File ""/home/travis/virtualenv/python2.7.14/lib/python2.7/site-packages/pip/_vendor/requests/adapters.py"", line 497, in send
>     raise SSLError(e, request=request)
> SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:661)

  "
15821,TFGAN not compatible with eager execution mode,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colaboratory Google Compute Engine backend (not sure about OS here)
- **TensorFlow installed from (source or binary)**: binary (non-GPU version)
- **TensorFlow version (use command below)**: 1.5.0-dev20180102
- **Python version**: 3

### Describe the problem
When enabling eager execution mode
```python
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()
```
and running 
```python
noise_dims = 64
gan_model = tfgan.gan_model(
    generator_fn,
    discriminator_fn,
    real_data=images,
    generator_inputs=tf.random_normal([batch_size, noise_dims]))
```
from the [TFGAN tutorial](https://github.com/tensorflow/models/blob/master/research/gan/tutorial.ipynb) by @joel-shor, I get the following error:
```bash
ValueError: When Eager Execution is enabled, variable collections are not supported.
```
because of the following lines:
https://github.com/tensorflow/tensorflow/blob/8c2d6fc2b0202304885d5d6c3cba57eb2a1b3262/tensorflow/contrib/gan/python/train.py#L119-L121
.

Are there any plans to make TFGAN compatible with eager in the short term? Is there any help wanted in this regard? I'd be happy to contribute."
15818,Tensorflow Object Detection using Tensorflow Mobile,"I am trying to use a custom model in the [TF-Detect Android Demo](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android).

**model: ssd_mobilenet_v1_coco**

The model is trained on 8 classes. After exporting the model I've optimised it using Tensorflow-Mobile.
```
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
    --in_graph=frozen_inference_graph.pb \
    --out_graph=optimized_inf_graph.pb \
    --inputs='image_tensor' \
    --outputs='detection_boxes detection_scores detection_classes num_detections' \
    --transforms='fold_batch_norms  fold_old_batch_norms  quantize_weights'
```
The optimised graph is giving proper output in my local system, but when it's integrated in the application there is no output shown in the screen. But when I'm using the unoptimised graph (`frozen_inference_graph.pb`) in the application, it's working fine. I'm getting outputs.What am I doing wrong here?


Have I written custom code: No
OS Platform and Distribution: Mac OS Sierra
TensorFlow installed from: Virtualenv installation
TensorFlow version: 1.4
Bazel version: Build label: 0.7.0-homebrew
CUDA/cuDNN version: NA
GPU model and memory: NA
Exact command to reproduce:

1. Trained a `ssd_mobilenet_v1_coco` model using google cloud ml for 8 classes
2. Exported the frozen graph from checkpoints using the below command set:

```
python export_inference_graph.py --input_type image_tensor \
     --pipeline_config_path training/ssd_inception_v2_coco.config \
     --trained_checkpoint_prefix training/model.ckpt-200000 \
     --output_directory frozen_graph/
```

`export_inference_graph.py` is the python script provided in here https://github.com/tensorflow/models/blob/master/research/object_detection/export_inference_graph.py. Tested the `frozen_inference_graph.py` in my local system, it's working fine.

3. Used the below command to convert the `frozen_inference_graph.py` to optimized graph:

```
bazel-bin/tensorflow/tools/graph_transforms/transform_graph \
    --in_graph=<path to frozen_inference_graph.pb> \
    --out_graph=<path where optimized_inf_graph.pb will be stored> \
    --inputs='image_tensor' \
    --outputs='detection_boxes detection_scores detection_classes num_detections' \
    --transforms='fold_batch_norms  fold_old_batch_norms  quantize_weights'
```
Tested the `optimized_inf_graph.pb` graph in my local system. It's working fine.

4. Copied the `optimized_inf_graph.pb` in `tensorflow/tensorflow/examples/android/assets/` folder. Also, copied `my_labels.txt` file in the assets folder.

5. Replaced `ssd_mobilenet_v1_android_export.pb` with `optimized_inf_graph.pb` in DetectorActivity.java [file](https://github.com/tensorflow/tensorflow/blob/15b1cf025da5c6ac2bcf4d4878ee222fca3aec4a/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L65)

Replaced `coco_labels_list.txt` with `my_labels.txt` in DetectorActivity.java [file](https://github.com/tensorflow/tensorflow/blob/15b1cf025da5c6ac2bcf4d4878ee222fca3aec4a/tensorflow/examples/android/src/org/tensorflow/demo/DetectorActivity.java#L67)

Tf Detect:
```
private static final String TF_OD_API_MODEL_FILE =
      ""file:///android_asset/optimized_inf_graph.pb"";
  private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/coco_labels_list.txt"";
```

My Version:
```
private static final String TF_OD_API_MODEL_FILE =
      ""file:///android_asset/ssd_mobilenet_v1_android_export.pb"";
  private static final String TF_OD_API_LABELS_FILE = ""file:///android_asset/my_labels.txt"";
```

6. Installed on my phone: LG G4"
15817,ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory,"I met the same problem as ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory , https://github.com/tensorflow/tensorflow/issues/15604 , but non of their methods work.
  "
15816,ImportError: libcublas.so.9.0: cannot open shared object file:,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug or a feature request.
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
- **TensorFlow installed from (source or binary)**:
- **TensorFlow version (use command below)**:
- **Python version**: 
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with

python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
15815,"When data become large,parition variables can not initialized successfully","#15216 
i have a issue, but nobody help me to solve it "
15810,Calling variable property of DropoutWrapper gives Error: AttributeError: 'DropoutWrapper' object has no attribute 'trainable',"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Using docker container minimaxir/keras-cntk:cpu-compiled (https://hub.docker.com/r/minimaxir/keras-cntk/tags/)
- **TensorFlow installed from (source or binary)**: source
- **TensorFlow version (use command below)**: 1.2.1 also tested on 1.4
- **Python version**: 3.5.2
- **Bazel version (if compiling from source)**: 
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**:
- **GPU model and memory**:
- **Exact command to reproduce**:

### Describe the problem
I was trying to access the weights and biases of an BasicLSTM cell I created. The BasicLSTM cell also has a DropoutWrapper and when trying to access the variables property the below error is thrown.
`AttributeError: 'DropoutWrapper' object has no attribute 'trainable'`

Someone tried to help me with the error on stackoverflow (second half of this [answer](https://stackoverflow.com/a/47643427/1169493)) and noticed that while variables is implemented in Layer. 

I will quote his very helpful response below:

> Although variables is documented for most (all?) RNN classes, it does break for DropoutWrapper. The property has been documented since r1.2, but accessing the property causes an exception in 1.2 and 1.4 (and looks like 1.3, but untested). Specifically,
> 
> from tensorflow.contrib import rnn
> ...
> lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)
> wrapped_cell = rnn.DropoutWrapper(lstm_cell)
> outputs, states = rnn.static_rnn(wrapped_cell, x, dtype=tf.float32)
> print(""LSTM vars!"", lstm_cell.variables)
> print(""Wrapped vars!"", wrapped_cell.variables)
> will throw AttributeError: 'DropoutWrapper' object has no attribute 'trainable'. From the traceback (or a long stare at the DropoutWrapper source), I noticed that variables is implemented in DropoutWrapper's super RNNCell's super Layer. Dizzy yet? Indeed, we find the documented variables property here. It returns the (documented) weights property. The weights property returns the (documented) self.trainable_weights + self.non_trainable_weights properties. And finally the root of the problem:

```
 @property
 def trainable_weights(self):
     return self._trainable_weights if self.trainable else []
 
 @property
 def non_trainable_weights(self):
     if self.trainable:
         return self._non_trainable_weights
     else:
        return self._trainable_weights + self._non_trainable_weights
```

> That is, variables does not work for a DropoutWrapper instance. Neither will trainable_weights or non_trainable_weights sinceself.trainable is not defined.
> 
> One step deeper, Layer.__init__ defaults self.trainable to True. Where that property gets removed, deled, unset, whatever from a DropoutWrapper object, I cannot tell.

  "
15806,Can we install tensorflow with a package manager in Linux distro?,"As we all know, Tensorflow is a major opensource deeplearning framework to the developers. 
BTW, How can we install tensorflow with a package manager such as apt-get (for *.deb), yum/zypper/dnf (for *.rpm)  in Linux distro?

* Reference - https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/ci_build/builds

I could not find related scripts from the above web address. Does tensorflow support 1)  manual compilation with ./tools/ci_build/build/*.sh and 2) pre-built docker-based compilation only?

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: CentOS 7.0 and Ubuntu 16.04.3
- **TensorFlow installed from (source or binary)**: Latest version of Tensorflow (form github)
- **TensorFlow version (use command below)**: Latest version of Tensorflow (form github)
- **Python version**:  2.7
- **Bazel version (if compiling from source)**: with Cmake (w/o Bazel)
- **GCC/Compiler version (if compiling from source)**: GCC 5.0
- **CUDA/cuDNN version**:  None (w/ CPU only)
- **GPU model and memory**: None , DRAM 16GB
- **Exact command to reproduce**:   Nonthing.



### Describe the problem
No support

### Source code / logs
Omission. 

  "
15805,Unable to convert LSTM model to .tflite model,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra 10.13.2
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')
- **Python version**: 2.7.13
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A, using CPU only
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
```
~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco \
  --input_file=""$(pwd)/lstm-model.pb"" \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --output_file=""$(pwd)/lstm-model.tflite"" --inference_type=FLOAT \
  --input_type=FLOAT --input_arrays=input \
  --output_arrays=output --input_shapes=28,28
```
### The Issue
When trying to convert an LSTM from a frozen graph (.pb) file to (.tflite) using the tensorflow toco script, I get unsupported operations error.

### Source code / logs
This is the source code for the mode:
```
'''
Edited code from https://jasdeep06.github.io/posts/Understanding-LSTM-in-Tensorflow-MNIST/
'''

import tensorflow as tf
from tensorflow.contrib import rnn

#import mnist dataset
from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data.read_data_sets(""/tmp/data/"",one_hot=True)

#define constants
#unrolled through 28 time steps
time_steps=28
#hidden LSTM units
num_units=128
#rows of 28 pixels
n_input=28
#learning rate for adam
learning_rate=0.001
#mnist is meant to be classified in 10 classes(0-9).
n_classes=10
#size of batch
batch_size=128

#weights and biases of appropriate shape to accomplish above task
out_weights=tf.Variable(tf.random_normal([num_units,n_classes]))
out_bias=tf.Variable(tf.random_normal([n_classes]))

#defining placeholders
#input image placeholder
x=tf.placeholder(""float"",[None,time_steps,n_input],name=""input"")
#input label placeholder
y=tf.placeholder(""float"",[None,n_classes])

#processing the input tensor from [batch_size,n_steps,n_input] to ""time_steps"" number of [batch_size,n_input] tensors
input=tf.unstack(x ,time_steps,1)

#defining the network
#cell = rnn.BasicLSTMCell(num_units,forget_bias=0)
lstm_layer = tf.nn.rnn_cell.MultiRNNCell([rnn.BasicLSTMCell(num_units) for _ in range(3)])
outputs, _ = rnn.static_rnn(lstm_layer,input,dtype=""float32"")

#converting last output of dimension [batch_size,num_units] to [batch_size,n_classes] by out_weight multiplication
prediction=tf.matmul(outputs[-1],out_weights,name=""output"")+out_bias

#loss_function
loss=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction,labels=y))
#optimization
opt=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

#model evaluation
correct_prediction=tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))

#initialize variables
init=tf.global_variables_initializer()

saver = tf.train.Saver()

with tf.Session() as sess:
    sess.run(init)
    iter=1
    while iter<800:
        batch_x,batch_y=mnist.train.next_batch(batch_size=batch_size)

        batch_x=batch_x.reshape((batch_size,time_steps,n_input))

        sess.run(opt, feed_dict={x: batch_x, y: batch_y})

        if iter %10==0:
            acc=sess.run(accuracy,feed_dict={x:batch_x,y:batch_y})
            los=sess.run(loss,feed_dict={x:batch_x,y:batch_y})
            print(""For iter "",iter)
            print(""Accuracy "",acc)
            print(""Loss "",los)
            print(""__________________"")

        filename = saver.save(sess, ""model/model.ckpt"")

        iter=iter+1

#calculating test accuracy
test_data = mnist.test.images[:128].reshape((-1, time_steps, n_input))
test_label = mnist.test.labels[:128]
print(""Testing Accuracy:"", sess.run(accuracy, feed_dict={x: test_data, y: test_label}))
```
This is code I used for freezing the graph:
```
'''
Code from https://blog.metaflow.fr/tensorflow-how-to-freeze-a-model-and-serve-it-with-a-python-api-d4f3596b3adc
'''

import os, argparse

import tensorflow as tf

# The original freeze_graph function
# from tensorflow.python.tools.freeze_graph import freeze_graph

dir = os.path.dirname(os.path.realpath(__file__))

def freeze_graph(model_dir, output_node_names):
    """"""Extract the sub graph defined by the output nodes and convert
    all its variables into constant
    Args:
        model_dir: the root folder containing the checkpoint state file
        output_node_names: a string, containing all the output node's names,
                            comma separated
    """"""
    if not tf.gfile.Exists(model_dir):
        raise AssertionError(
            ""Export directory doesn't exists. Please specify an export ""
            ""directory: %s"" % model_dir)

    if not output_node_names:
        print(""You need to supply the name of a node to --output_node_names."")
        return -1

    # We retrieve our checkpoint fullpath
    checkpoint = tf.train.get_checkpoint_state(model_dir)
    input_checkpoint = checkpoint.model_checkpoint_path

    # We precise the file fullname of our freezed graph
    absolute_model_dir = ""/"".join(input_checkpoint.split('/')[:-1])
    output_graph = absolute_model_dir + ""/frozen_model.pb""

    # We clear devices to allow TensorFlow to control on which device it will load operations
    clear_devices = True

    # We start a session using a temporary fresh Graph
    with tf.Session(graph=tf.Graph()) as sess:
        # We import the meta graph in the current default Graph
        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)

        # We restore the weights
        saver.restore(sess, input_checkpoint)

        # We use a built-in TF helper to export variables to constants
        output_graph_def = tf.graph_util.convert_variables_to_constants(
            sess, # The session is used to retrieve the weights
            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes
            output_node_names.split("","") # The output node names are used to select the usefull nodes
        )

        # Finally we serialize and dump the output graph to the filesystem
        with tf.gfile.GFile(output_graph, ""wb"") as f:
            f.write(output_graph_def.SerializeToString())
        print(""%d ops in the final graph."" % len(output_graph_def.node))

    return output_graph_def

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(""--model_dir"", type=str, default="""", help=""Model folder to export"")
    parser.add_argument(""--output_node_names"", type=str, default="""", help=""The name of the output nodes, comma separated."")
    args = parser.parse_args()

    freeze_graph(args.model_dir, args.output_node_names)
```

This is the output of the toco command:
```
2018-01-02 20:05:24.912921: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.
2018-01-02 20:05:24.973744: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: Unpack
2018-01-02 20:05:24.974315: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1099] Converting unsupported operation: StridedSlice
2018-01-02 20:05:25.041459: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1209 operators, 1775 arrays (0 quantized)
2018-01-02 20:05:25.118862: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 1114 operators, 1672 arrays (0 quantized)
2018-01-02 20:05:25.176555: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 1114 operators, 1672 arrays (0 quantized)
2018-01-02 20:05:25.208552: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:313] Total transient array allocated size: 0 bytes, theoretical optimal value: 0 bytes.
2018-01-02 20:05:25.234811: F tensorflow/contrib/lite/toco/tflite/export.cc:303] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops. Here is a list of operators for which you will need custom implementations: ExpandDims, Fill, SPLIT, StridedSlice, TensorFlowShape, Unpack.
pbtotflite.sh: line 8:  8277 Abort trap: 6           ~/tensorflow/bazel-bin/tensorflow/contrib/lite/toco/toco --input_file=""$(pwd)/lstm-model.pb"" --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --output_file=""$(pwd)/lstm-model.tflite"" --inference_type=FLOAT --input_type=FLOAT --input_arrays=input --output_arrays=output --input_shapes=28,28
```"
15804,[bug]libtensorflowlite_jni.so crash when creating interpreter with byteBuffer mode,"Dear tensorflow developers：
       A crash is found when using java nio to create interpreter of tensorflowLite, which make us puzzled for a long time. I hope you can help us to solve the issue, thanks & best regard. The issues will be described in detail as follows:

       Naturally, we use the function provided by tensorflow below to create model & interpreter.      
 
       NativeInterpreterWrapper(ByteBuffer modelByteBuffer) {
          errorHandle = createErrorReporter(ERROR_BUFFER_SIZE);
          modelHandle = createModelWithBuffer(modelByteBuffer, errorHandle);
          interpreterHandle = createInterpreter(modelHandle);
       }

       step 1.  We create the Input parameter modelByteBuffer. The ByteBuffer as follows:

          fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);
          
          The following conditions should be satisfied at the same time, 
          1)  startOffset is not zero,  for example startOffset = 200
               If the startOffset is zero, the issue will not occur.
          2)  the byteBuffer size is very large, for example size = 80MB
               if the size is small, the issue will not occur.

       Step 2. We call the funtion NativeInterpreterWrapper with byteBuffer.
        
       Unfortunately, after running  step 1 & 2, the crash is occured as follows:

01-02 18:58:54.544 21135-21135/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
01-02 18:58:54.545 21135-21135/? A/DEBUG: Build fingerprint: 'google/marlin/marlin:8.0.0/OPR3.170623.013/4397526:user/release-keys'
01-02 18:58:54.545 21135-21135/? A/DEBUG: Revision: '0'
01-02 18:58:54.545 21135-21135/? A/DEBUG: ABI: 'arm'
01-02 18:58:54.545 21135-21135/? A/DEBUG: pid: 20837, tid: 20837, name: fish.xxxxxx  >>> com.taobao.idlexxxx.xxxxxx <<<
01-02 18:58:54.545 21135-21135/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0xa733f5a6
01-02 18:58:54.545 21135-21135/? A/DEBUG:     r0 a733f5a6  r1 00000004  r2 00000006  r3 00000000
01-02 18:58:54.545 21135-21135/? A/DEBUG:     r4 e0f59c58  r5 de9734b0  r6 a73f5160  r7 ffc5de20
01-02 18:58:54.545 21135-21135/? A/DEBUG:     r8 00000004  r9 00000000  sl ffc5de90  fp a733f57a
01-02 18:58:54.545 21135-21135/? A/DEBUG:     ip edee063c  sp ffc5dd90  lr edea7741  pc a73aa208  cpsr 200f1830
01-02 18:58:54.546 21135-21135/? A/DEBUG: backtrace:
01-02 18:58:54.546 21135-21135/? A/DEBUG:     #00 pc 00062208  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/lib/arm/libtensorflowlite_jni.so
01-02 18:58:54.546 21135-21135/? A/DEBUG:     #01 pc 00062d53  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/lib/arm/libtensorflowlite_jni.so
01-02 18:58:54.546 21135-21135/? A/DEBUG:     #02 pc 00006d93  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/lib/arm/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_createInterpreter+50)
01-02 18:58:54.546 21135-21135/? A/DEBUG:     #03 pc 000281ef  /data/app/com.taobao.idlexxxx.xxxxxx-qw202S8jC-x2xWFXznSPlw==/oat/arm/base.odex (offset 0x1c000)
01-02 18:58:55.403 742-742/? E//system/bin/tombstoned: Tombstone written to: /data/tombstones//tombstone_03


    While, If we reallocate memory, the issue will also not occur .The relative code is as follows:
    …….
    rf = new RandomAccessFile(modelPath, ""r"");
    declaredLength = rf.length() - startOffset;
    rf.seek(startOffset);
    ByteBuffer byteBuffer = ByteBuffer.allocateDirect((int) declaredLength);
    …….
    byteBuffer.put(tfbyte);
    …….

    Is there a memory allocation bug? And could you tell me how to avoid the crash if not reallocate 
    memory？ Thanks very much.
  "
15802,tf.stack eats memory over time,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: ubuntu 16.04
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.4.1
- **Python version**:  3.5
- **CUDA/cuDNN version**: 8
- **GPU model and memory**: 1060 + 6GB
 

### Describe the problem
I use tf.stack to stack 2 images. But the memory used by this program increase over time. I use `memory_profiler` check it. it is caused by tf.stack, here is the minimal re-produce code:

```
import tensorflow as tf
import glob
import gc
from memory_profiler import profile


@profile
def function_mark():
  pass

@profile
def stack_images():
  image_file_list = glob.glob(""car_images/*.jpg"")
  sess = tf.Session()

  for _ in range(300):
    # read image
    image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
    image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
    # decode image
    image1_decode = tf.image.decode_image(image1, channels=3)
    image2_decode = tf.image.decode_image(image2, channels=3)
    # stack image
    image_stack = tf.stack([image1_decode, image2_decode])
    # run session
    r_image_stack = sess.run(image_stack)
    # mark function. so I can check the memory-usage of every loop.
    function_mark()
    # force garbage collection, so all the un-reference variable will be freed.
    del r_image_stack
    gc.collect()
```

 First, I profile it line by line, here is the result:
**you can see it very clearly that line 26 take a lot of memory. My image is 800*600 and I only stack 2 image each time, so 1.3G memory consumption is not normal.**

> 
> Line #    Mem usage    Increment   Line Contents
> ================================================
>     11    190.0 MiB    190.0 MiB   @profile
>     12                             def stack_images():
>     13    190.0 MiB      0.0 MiB     image_file_list = glob.glob(""car_images/*.jpg"")
>     14    421.6 MiB    231.7 MiB     sess = tf.Session()
>     15
>     16   1998.4 MiB      0.0 MiB     for _ in range(300):
> 
>     17                                 # read image
>     18   1992.5 MiB      1.5 MiB       image1 = tf.gfile.FastGFile(image_file_list[0], 'rb').read()
>     19   1992.5 MiB      0.0 MiB       image2 = tf.gfile.FastGFile(image_file_list[1], 'rb').read()
>     20                                 # decode image
>     21   1992.8 MiB     77.6 MiB       image1_decode = tf.image.decode_image(image1, channels=3)
>     22   1993.3 MiB    108.6 MiB       image2_decode = tf.image.decode_image(image2, channels=3)
>     23                                 # stack image
>     24   1993.3 MiB      0.7 MiB       image_stack = tf.stack([image1_decode, image2_decode])
>     25                                 # run session
>     26   1998.4 MiB   1350.4 MiB       r_image_stack = sess.run(image_stack)
>     27                                 # mark function. so I can check the memory-usage of every loop.
>     28   1998.4 MiB     29.0 MiB       function_mark()
>     29                                 # force garbage collection, so all the un-reference variable will be freed.
>     30   1998.4 MiB      0.0 MiB       del r_image_stack
>     31   1998.4 MiB      8.9 MiB       gc.collect()

Then **I profile it over time.** I use function `function_mark` to distinguish each loop. So we can see it very clearly that the memory usage of this program is increase over time.
![figure_1](https://user-images.githubusercontent.com/5325686/34505710-d25c80b0-f061-11e7-99c6-5db2e990d9aa.png)

My question is: How should I avoid this problem. because it cause a serious performance regression.
"
15800,[bug] tf.estimator.DNNClassifier setting n_classes has no effect,"### Describe the problem
I was following the examples for a [tensorflow estimator](https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html). I am setting n_classes but the label check in (_check_labels tensorflow/python/estimator/canned/head.py"", line 222) keeps kicking back the following error:

**ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.**

### Source code / logs
``` python
import tensorflow as tf
import numpy as np

trainX = np.array([1,0,2,3])
num_classes = 4
feature_names = ['f1']
feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]
classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns, 
                                            n_classes=num_classes, #setting number of classes here
                                            hidden_units=[10])

def input_fn():
    my_int_variable = tf.get_variable(""my_int_variable"", [1], dtype=tf.int32, initializer=tf.zeros_initializer)
    label = tf.one_hot(my_int_variable, num_classes) #using same number of classes
    return {'f1':trainX},label

classifier.train(input_fn=lambda: input_fn())
```

``` bash
Traceback (most recent call last):
  File ""test.py"", line 17, in <module>
    classifier.train(input_fn=lambda: input_fn())
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 302, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 711, in _train_model
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 694, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 334, in _model_fn
    config=config)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/dnn.py"", line 203, in _dnn_model_fn
    logits=logits)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 493, in create_estimator_spec
    features=features, mode=mode, logits=logits, labels=labels)
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 433, in create_loss
    label_ids = self._label_ids(_check_labels(_maybe_expand_dim(labels), 1))
  File ""/home/user/tensorflow/tf/lib/python2.7/site-packages/tensorflow/python/estimator/canned/head.py"", line 222, in _check_labels
    (expected_labels_dimension, dim1))
ValueError: Mismatched label shape. Classifier configured with n_classes=1.  Received 4. Suggested Fix: check your n_classes argument to the estimator and/or the shape of your label.
```

------------------------

### System information
Mac OSX 10.12.6
Python 2.7
Tensorflow ('v1.4.0-19-ga52c8d9b01', '1.4.1')



"
