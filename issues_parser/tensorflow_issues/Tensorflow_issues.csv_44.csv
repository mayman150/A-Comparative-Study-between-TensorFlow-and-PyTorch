Issue Number,Issue Title,Issue Body
26366,"How to plot train, test and validation line to evaluate your model whether overfit or underfit","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

```
# Copy and paste here
```

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26365,Interpreter lifecycle Tensorflow Lite,"**System information**
- TensorFlow version: `org.tensorflow:tensorflow-lite:0.0.0-nightly`
- Doc Link: there is no doc as far as I know

**Describe the documentation issue**
I was not able to find information about what is the proper life cycle for the [Intepreter](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) class. I'm a contributor of this [Android library](https://github.com/the-super-toys/glimpse-android) which makes use of TensorFlow Lite. The problem is that I don't know where I should instantiate the Interpreter. If I do it with a global scope and reuse the instance over and over after a few calls  it throws the next exception:

```
--------- beginning of crash
2019-03-05 01:15:23.444 31140-31169/glimpse.sample A/libc: Fatal signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x6d36b8d51c in tid 31169 (glide-disk-cach), pid 31140 (glimpse.sample)
2019-03-05 01:15:23.476 31140-31159/glimpse.sample E/libc: Access denied finding property ""vendor.debug.egl.swapinterval""
2019-03-05 01:15:23.513 31228-31228/? A/DEBUG: *** *** *** *** *** *** *** *** *** *** *** *** *** *** *** ***
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: Build fingerprint: 'OnePlus/OnePlus6/OnePlus6:9/PKQ1.180716.001/1901231401:user/release-keys'
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: Revision: '0'
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: ABI: 'arm64'
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: pid: 31140, tid: 31169, name: glide-disk-cach  >>> glimpse.sample <<<
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG: signal 11 (SIGSEGV), code 1 (SEGV_MAPERR), fault addr 0x6d36b8d51c
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x0  0000007e068702c0  x1  0000000000000000  x2  0000000000000010  x3  0000000000000004
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x4  0000007e068702d0  x5  0000007e06870300  x6  0000000000000002  x7  0000007e068702c0
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x8  0000000000000002  x9  0000000000000004  x10 0000000000000003  x11 fffff80dc78c320f
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x12 ffffffef3031d1dc  x13 0000000000000004  x14 0000000000000001  x15 0000000000000003
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x16 0000007e077e6280  x17 0000007ea829de70  x18 0000000000000010  x19 0000007e068702c0
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x20 0000000000000004  x21 0000007e091f28c4  x22 0000007e068689c0  x23 0000000000000002
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x24 0000007e06870300  x25 0000007e06870340  x26 0000000000000060  x27 0000000000000060
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     x28 0000007e091e8500  x29 0000007e08912ae0
2019-03-05 01:15:23.514 31228-31228/? A/DEBUG:     sp  0000007e08912aa0  lr  0000007e07703254  pc  0000007e0770335c
2019-03-05 01:15:23.518 31140-31159/glimpse.sample E/libc: Access denied finding property ""vendor.debug.egl.swapinterval""
2019-03-05 01:15:23.533 31140-31159/glimpse.sample E/libc: Access denied finding property ""vendor.debug.egl.swapinterval""
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG: backtrace:
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #00 pc 00000000000d035c  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #01 pc 00000000000cfb4c  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #02 pc 00000000000ccc54  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #03 pc 000000000011a38c  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #04 pc 000000000011cd98  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #05 pc 0000000000010f60  /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/lib/arm64/libtensorflowlite_jni.so (Java_org_tensorflow_lite_NativeInterpreterWrapper_run+32)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #06 pc 000000000055e1e0  /system/lib64/libart.so (art_quick_generic_jni_trampoline+144)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #07 pc 000000000055544c  /system/lib64/libart.so (art_quick_invoke_static_stub+604)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #08 pc 00000000000cf6e8  /system/lib64/libart.so (art::ArtMethod::Invoke(art::Thread*, unsigned int*, unsigned int, art::JValue*, char const*)+232)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #09 pc 000000000027f2b8  /system/lib64/libart.so (art::interpreter::ArtInterpreterToCompiledCodeBridge(art::Thread*, art::ArtMethod*, art::ShadowFrame*, unsigned short, art::JValue*)+344)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #10 pc 00000000002792c0  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+968)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #11 pc 0000000000526498  /system/lib64/libart.so (MterpInvokeStatic+204)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #12 pc 0000000000547a14  /system/lib64/libart.so (ExecuteMterpImpl+14612)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #13 pc 000000000026abe8  /dev/ashmem/dalvik-classes2.dex extracted in memory from /data/app/glimpse.sample-Jd0f0_ct3CDvNgh38yERpA==/base.apk!classes2.dex (deleted) (org.tensorflow.lite.NativeInterpreterWrapper.run+164)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #14 pc 0000000000252fc4  /system/lib64/libart.so (_ZN3art11interpreterL7ExecuteEPNS_6ThreadERKNS_20CodeItemDataAccessorERNS_11ShadowFrameENS_6JValueEb.llvm.99565114+488)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #15 pc 0000000000258ab8  /system/lib64/libart.so (art::interpreter::ArtInterpreterToInterpreterBridge(art::Thread*, art::CodeItemDataAccessor const&, art::ShadowFrame*, art::JValue*)+216)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #16 pc 00000000002792a4  /system/lib64/libart.so (bool art::interpreter::DoCall<false, false>(art::ArtMethod*, art::Thread*, art::ShadowFrame&, art::Instruction const*, unsigned short, art::JValue*)+940)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #17 pc 0000000000524f94  /system/lib64/libart.so (MterpInvokeVirtual+588)
2019-03-05 01:15:23.557 31228-31228/? A/DEBUG:     #18 pc 0000000000547894  /system/lib64/libart.so (ExecuteMterpImpl+14228)...
```

To provide more context, the `interpreter` is used from a background thread, specifically from a [Glide Transformation](https://bumptech.github.io/glide/doc/transformations.html) component. So, at this point, I don't know if the problem is due to a wrong lifecycle handling of the interpreter instance or rather something related to threading. 

But beyond my particular case I wanted to ask for some guidance about the proper lifecycle handling for the Interpreter class.

Thanks."
26364,Win 10 Import Error: Failed to load the native TensorFlow runtime TF-GPU(both1.13 and 2.0) ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tensorflow-gpu-1.13.1
- Python version: 3.6.4
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: CUDA 10.1/cuDNN7.5
- GPU model and memory: 930mx 2GB

**Describe the current behavior**  
Fails to run because of an Import Error whenever I try to import tf-GPU,
**Describe the expected behavior**  
Expected it to import so I could use it.
**Code to reproduce the issue**  
import tensorflow as tf

**Other info / logs**  
---------------------------------------------------------------------------
```
ImportError                               Traceback (most recent call last)
c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

c:\users\ent\appdata\local\programs\python\python36\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

c:\users\ent\appdata\local\programs\python\python36\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-1-602d2050546b> in <module>
----> 1 import tensorflow as tf
      2 import time
      3 import os
      4 from tensorflow.examples.tutorials.mnist import input_data
      5 

c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 from tensorflow._api.v1 import app

c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 # Protocol buffers

c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""c:\users\ent\appdata\local\programs\python\python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""c:\users\ent\appdata\local\programs\python\python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""c:\users\ent\appdata\local\programs\python\python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.  
```
---------------------------------------------------------------------------"
26363,No gen_bigquery_reader_ops,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 1.12.0
- Python version: 3.5
- Installed using: pip
- CUDA/cuDNN version: 9.0
- GPU model and memory: Jetson TX2



I can't run uff conversion.
The traceback is:

```
Traceback (most recent call last):
  File ""/tegra/AIC_tf_to_trt_image_classification/scripts/frozen_graphs_to_uffs.py"", line 9, in <module>
    from model_meta import NETS, FROZEN_GRAPHS_DIR, CHECKPOINT_DIR, UFF_DIR
  File ""/tegra/AIC_tf_to_trt_image_classification/scripts/model_meta.py"", line 10, in <module>
    import tensorflow.contrib.slim as slim
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/__init__.py"", line 38, in <module>
    from tensorflow.contrib import cloud
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cloud/__init__.py"", line 24, in <module>
    from tensorflow.contrib.cloud.python.ops.bigquery_reader_ops import *
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/cloud/python/ops/bigquery_reader_ops.py"", line 21, in <module>
    from tensorflow.contrib.cloud.python.ops import gen_bigquery_reader_ops
ImportError: cannot import name 'gen_bigquery_reader_ops'
```
There's no such file `gen_bigquery_reader_ops.py` at this location."
26362,How can I do distributed training using Parameter Server Strategy with estimators? How to define Cluster Spec without using TF_CONFIG  for Estimators?,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
26360,extend_with_weight_decay function doesn't exist? ,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: 
https://www.tensorflow.org/versions/r1.12/api_docs/python/tf/contrib/opt/AdamWOptimizer
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/opt/python/training/weight_decay_optimizers.py


**Describe the documentation issue**
In `MomentumWOptimizer`, `AdamWOptimizer` and `DecoupledWeightDecayExtension` the documentation refers to `extend_with_weight_decay`:

```python
  extend_with_weight_decay(tf.train.MomentumOptimizer,
                           weight_decay=weight_decay)
  ```
AFAIK there is no ""`extend_with_weight_decay`"", only ""`extend_with_decoupled_weight_decay`"", which seems to serve a similar purpose:

```python
MyAdamW = extend_with_decoupled_weight_decay(tf.train.AdamOptimizer)
  # Create a MyAdamW object
  optimizer = MyAdamW(weight_decay=0.001, learning_rate=0.001)
```
Is my understanding correct?
I could submit a PR if necessary
"
26359,RAM memory issue in Xavier ,"Hi,
I am currently working on an application which runs ssd mobilenet based Object detector in Nvidia AGX Xavier Developer kit, which is flashed with Jetpack 4.1. 

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): LINUX UBUNTU 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): pip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v411 tensorflow-gpu
- TensorFlow version (use command below): 1.13.0
- Python version: 2.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: CUDA 10.0/cuDNN 7.3.1.20
- GPU model and memory:Xavier developer kit

The sofware specifications are as follows, 
Ubuntu 18.04
tf = 1.13.0 from pip install --pre --extra-index-url https://developer.download.nvidia.com/compute/redist/jp/v411 tensorflow-gpu
CUDA 10.0
CUDNN 7.3.1.20

I have noticed that the algorithm is taking around 12GB out of 15.5GB RAM memory available in Xavier.

I have made use of ""config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=log_device)"" and ""config.gpu_options.allow_growth=allow_memory_growth"" in the code to control the memory growth. But still I could not see any improvement in the memory allocation.

I have uninstalled tensorflow and installed it again too. But there is no much change.

The same algorithm allocates only about 2-3GB is GPU based Laptop.

Could you help me to make a conclusion on this issue. Are there any more changes to be done specific to xavier for using the split model algorithm you have created ? Also can the split model code be used for mutilple cameras ? 

Thanks in Advance !!!


Regards,
Niran"
26357,fatal error: tensorflow/cc/ops/array_ops.h,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I use c++ API
 
**Describe the problem**
fatal error: tensorflow/cc/ops/array_ops.h
**Provide the exact sequence of commands / steps that you executed before running into the problem**

cant find array_ops.h
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
tensorflow/tensorflow/cc/ops/standard_ops.h:19:41: fatal error: tensorflow/cc/ops/array_ops.h:"
26356,Low GPU usage for inference when multithreaded vs multiprocess c++,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7 64-bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source): vs2015
- CUDA/cuDNN version: 10.0 / 7.5
- GPU model and memory: RTX 2060 6GB


**Describe the current behavior**
I'm using Python to build a graph (using tf.keras.layers) that contains cudnnlstm, timedistributed, dense layers. It crashes the RTX 2060 (there's a bug report for that already, this is not this issue) if I don't set options allow_growth  or limit ram usage. 
I'm using the model for inference from C++, my batch size is known as soon as the program starts but it can vary each time it's started so the graph doesn't have a fixed input size. I'm creating several threads with one Session on each. Tensorflow GPU usage is low (38%) and it eventually gobbles up all gpu ram available if it's not manually limited, I can't simply increase the batch sizes, because different weights are loaded on each Run().
Now, the issue here is that it doesn't matter much how many parallel sessions I run in multiple threads, GPU utilization is still low, **_but_** if I limit gpu RAM usage so that I can run two separate processes (not threads), then they can both use about half the ram and increase GPU utilization to 80%. Why can't Tensorflow figure out a way to do that in just one process with multiple threads?  It's the same inputs and results, just multi-threaded vs multi-process.


**Describe the expected behavior**
TF should utilize at least 80%+ GPU in my case since I'm just running inference sessions in parallel in one process, instead of having to resort to limiting ram usage and starting several processes.

**Code to reproduce the issue**
Too much code to extract to make a barebones example, but I could do it if absolutely necessary.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26355,running the tflite model with a video input?,"I want to give the  tflite object detection model an recorded video input instead of an live streaming video
is that possible?"
26354,"Wrong outputs of ""tensorflow.python.ops.distributions.util.fill_triangular_inverse""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6


**Describe the current behavior**
tensorflow.python.ops.distributions.util.fill_triangular_inverse didn't give the right outputs.

**Code to reproduce the issue**
import tensorflow as tf
from tensorflow.python.ops.distributions.util import fill_triangular_inverse
import numpy as np


sess = tf.InteractiveSession()
a = np.arange(1, 17).reshape((1, 4, 4))
""""""
a = [[[ 1  2  3  4]
  [ 5  6  7  8]
  [ 9 10 11 12]
  [13 14 15 16]]]
""""""

T = fill_triangular_inverse(a, upper=True)
t = sess.run(T)

""""""
t = [ 1  2  3  4 21 21 21 21 21 21]]
""""""
"
26353,"[TF 2.0] Provide a tool to convert checkpoints for optimizers, from TF 1.x to TF 2.0.","[Checkpoints will break with RNNs](https://github.com/tensorflow/tensorflow/issues/26350) and shared embedding columns, which is unfortunate but acceptable. For optimizers, though, [checkpoint breaking is a bit more extensive](https://github.com/tensorflow/tensorflow/issues/26349). 

Ideally this feature request would result in a function or tool that could **take the old checkpoint and some information about the old + new optimizers**, and **replace the old checkpoint with the new**. This should involve some variable renaming, and some new variables added in. (Hyperparameters are now variables, so we will need to add those in to the checkpoints, and set with reasonable values-- either the defaults, or with user-specified values.)

**Priority**:
P0 - optimizers
P1 - RNNs
P1 - shared embeddings"
26352,win10vc2017build failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (win10):
- TensorFlow installed from (source or binary): source r1.12
- TensorFlow version: 1.12
- Python version: python 3.7 (anaconda)
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): vc 2017
- CUDA/cuDNN version:cuda:10.0/cudnn:7.5
- GPU model and memory: Nvidia Geforce 840M



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

python ./configure.py
...
bazel build -c opt --copt=/arch:AVX2  --config=cuda  --copt=-nvcc_options=disable-warnings //tensorflow/tools/pip_package:build_pip_package  --local_resources 2048,.5,1.0

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

external/com_google_absl\absl/meta/type_traits.h(338): error: static assertion failed with ""Not compliant with std::is_trivially_copy_assignable; Standard: true, Implementation: false""
          detected during:
            instantiation of class ""absl::is_trivially_copy_assignable<T> [with T=Eigen::Index]""
external/com_google_absl\absl/types/optional.h(267): here
            processing of template argument list for ""absl::optional_internal::optional_data"" based on template argument <tensorflow::int64>
external/com_google_absl\absl/types/optional.h(485): here
            instantiation of class ""absl::optional<T> [with T=tensorflow::int64]""
.\tensorflow/core/framework/allocator.h(68): here

1 error detected in the compilation of ""D:/Temp/nvcc_inter_files_tmp_dir/dense_update_functor_gpu.cu.cpp1.ii"".
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 7092.230s, Critical Path: 846.02s
INFO: 1705 processes: 1705 local.
FAILED: Build did NOT complete successfully

but std::is_trivially_copy_assignable is ok .
i build the below source 

#include <iostream>

#include <iostream>
#include <utility>
#include <type_traits>
struct Foo { int n; };

int main()
{
    std::cout << ""Hello World!\n""; 
	std::cout << std::boolalpha
		<< ""Foo is trivially copy-assignable? ""
		<< std::is_trivially_copy_assignable<Foo>::value << '\n'
		<< ""int[2] is copy-assignable? ""
		<< std::is_copy_assignable<int[2]>::value << '\n'
		<< ""int is nothrow copy-assignable? ""
		<< std::is_nothrow_copy_assignable<int>::value << '\n';
}

outpub the below 
Hello World!
Foo is trivially copy-assignable? true
int[2] is copy-assignable? false
int is nothrow copy-assignable? true

"
26350,[TF 2.0] Checkpoint breaking change for RNN.,"**Type of breakage**: Breakage with changing code.

**APIs that are affected**: 

1. [`tf.nn.dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)
2. [`tf.nn.static_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/static_rnn)
3. [`tf.nn.bidirectional_dynamic_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn)
4. [`tf.nn.static_bidirectional_rnn`](https://www.tensorflow.org/api_docs/python/tf/nn/static_bidirectional_rnn)
5. All cells within the [`tf.nn.rnn_cell.*`](https://www.tensorflow.org/api_docs/python/tf/nn/rnn_cell) module.

**Description of change**: The current TensorFlow RNN code is going to be replaced by `tf.keras` RNNs in TensorFlow 2.0. User will need to update their code to use the new Keras API. The weights of the Keras RNN cell are different from the existing TensorFlow RNN cell.

**Variable name change map**: The RNN cell weights are in a different format between existing TensorFlow RNN cells and Keras cells. There is currently no direct mapping for that.

**Target time window**: Undecided since the update requires a non-trivial user side change."
26349,[TF 2.0] Checkpoint breaking change for all optimizers.,"**Type of breakage**: Breakage with changing code.

APIs that are affected: 

1. [`tf.train.GradientDescentOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/GradientDescentOptimizer)
2. [`tf.train.MomentumOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/MomentumOptimizer)
3. [`tf.train.RMSPropOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/RMSPropOptimizer)
4. [`tf.train.AdamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)
5. [`tf.train.AdadeltaOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdadeltaOptimizer)
6. [`tf.train.AdagradOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/AdagradOptimizer)
7. [`tf.train.FtrlOptimizer`](https://www.tensorflow.org/api_docs/python/tf/train/FtrlOptimizer)

**Side Note**: there are two extra `tf.contrib`-owned optimizers that will also have breaking changes: 

1. [`tf.contrib.opt.NadamOptimizer`](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/NadamOptimizer)
2. [`tf.contrib.opt.AdaMaxOptimizer`](https://www.tensorflow.org/api_docs/python/tf/contrib/opt/AdaMaxOptimizer)

**Description of change**: The current endpoint `tf.train.xxxOptimizer()` is being deprecated in favor of `tf.keras.optimizers.xxx()`. Specifically:

1. `tf.train.GradientDescentOptimizer(lr=???)` -> `tf.keras.optimizers.SGD(learning_rate=???)`
2. `tf.train.MomentumOptimizer(lr=???, momentum=???)` -> `tf.keras.optimizers.SGD(learning_rate=???, momentum=???)`
3. `tf.train.RMSPropOptimizer(lr=???)` -> `tf.keras.optimizers.RMSprop(learning_rate=???)`
4. `tf.train.AdamOptimizer(lr=???, beta1=???, beta2=???)` -> `tf.keras.optimizers.Adam(learning_rate=???, beta_1=???, beta_2=???)`
5. `tf.train.AdadeltaOptimizer(lr=???)` -> `tf.keras.optimizers.Adadelta(learning_rate=???)`
6. `tf.train.AdagradOptimizer(lr=???)` -> `tf.keras.optimizers.Adagrad(learning_rate=???)`
7. `tf.train.FtrlOptimizer(lr=???)` -> `tf.keras.Ftrl(learning_rate=???)`

TensorFlow users of `tf.train.xxxOptimizer()` will be updated to `tf.keras.optimizers.xxx()`, and checkpoints from the old `tf.train.xxxOptimizer()` calls will no longer work.

**Variable name change map**:  The `tf.keras.optimizers.xxx` weights are in different format than existing `tf.train.xxxOptimizer`. There isn't direct mapping for that.

**Target time window**: Undecided since the update requires non-trivial user side change."
26348,TF_CPP_MIN_LOG_LEVEL=3 does not work after upgrading to 1.13.1,"After upgrading to 1.13.1, the `TF_CPP_MIN_LOG_LEVEL` does not seem to work for certain warnings.
For example, I get the following warnings even if log level is set to 3:

```
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.

```
"
26347,2019-03-05 12:24:39.106 18565-18583/org.tensorflow.lite.demo W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648,"When i run the tfLite demo app in my mobile I am getting the following error:

2019-03-05 12:24:39.106 18565-18583/org.tensorflow.lite.demo W/System.err: java.lang.ArrayIndexOutOfBoundsException: length=10; index=-2147483648

"
26346,ref type problem merging simple graphs,"Hi guys, the following simple graph is compiled correctly on my computer:


graph_unique = tf.Graph()

with graph_unique.as_default():
    v = tf.get_variable(name='v', shape=[2], dtype=tf.float64)
    x = v[:1]
    y = v[1:]
    c = tf.add(x, y, name='c')
    gra = tf.gradients([c], [x])
    

When I try to write it as a stack of two graphs, though, I get an error.
I guess it is related with this post and I beg for some help.
Here my procedure:



adr_big = ''   # please add a valid addres 
adr_small = '' #       ""          ""


graph_small = tf.Graph()

with graph_small.as_default():
    v = tf.get_variable(name='v', shape=[2], dtype=tf.float64)
    x = tf.identity(v[:1], name='x')
    y = tf.identity(v[1:], name='y')
    s_small = tf.train.Saver()
    
with tf.Session(graph=graph_small) as sess:
    sess.run(tf.global_variables_initializer())
    s_small.save(sess, adr_small)
    
    
graph_big = tf.Graph()

with graph_big.as_default():
    a = tf.get_variable(name='a', shape=[1], dtype=tf.float64)
    b = tf.get_variable(name='b', shape=[1], dtype=tf.float64)
    c = tf.add(a, b, name='c')
    s = tf.train.Saver()
    
with tf.Session(graph=graph_big) as sess:
    sess.run(tf.global_variables_initializer())
    s.save(sess, adr_big)
    
    
    
graph_together = tf.Graph()

with graph_together.as_default():
    tf.train.import_meta_graph(adr_small+'.meta', import_scope='g_small')
    x = graph_together.get_tensor_by_name('g_small/x:0')
    y = graph_together.get_tensor_by_name('g_small/y:0')
    tf.train.import_meta_graph(adr_big+'.meta', import_scope='g_big', input_map={'a:0':x, 'b:0':y})
    c = graph_together.get_tensor_by_name('g_big/c:0')
    gra = tf.gradients([c],[x])
    

Tensorflow says:
[...]
InvalidArgumentError: Input 0 of node gg/a/Assign was passed double from g_small/x:0 incompatible with expected double_ref.
During handling of the above exception, another exception occurred:
[...]
ValueError: Input 0 of node gg/a/Assign was passed double from g_small/x:0 incompatible with expected double_ref.

Please notice: everything works correctly if the definition of graph_small above is replaced with the following one:  

with graph_small.as_default():
    tf.get_variable(name='x', shape=[1], dtype=tf.float64)
    tf.get_variable(name='y', shape=[1], dtype=tf.float64)
    s_small = tf.train.Saver()  
    
Thanx a lot!
"
26344,How to retrain with custom object and get frozen_inference.pb model file with Tensorflow.,"Hi,

I am working in a project for eye-lid detection. My eye-lid detection is based on landmark points in face. it uses tensor-flow library. i was stuck in a procedure how to retrain the algorithm with new data-set and generate frozen_inference.pb file. I have tried with procedure given in tensor-flow page but i cannot able to get frozen_inference.pb file. Please help me in the procedure how to retrain and get the frozen_inference,pb file.

Kindly do the needful..

Regards,
Niranjan B"
26341,A function decorated with tf.function cannot return a variable by reference,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf.version.VERSION: '2.0.0-dev20190304'
tf.version.GIT_VERSION: 'v1.12.0-9475-gc1487a9c93'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
A function decorated with `tf.function` cannot return a variable by reference. This makes it impossible to write accessors, so we end up having to access variables directly, which leads to ugly code & hard to maintain. For example, this is the root cause of #25754.
This might be considered a feature request, rather than an issue: I grant you that it's debatable! :)

**Describe the expected behavior**
I expect `return variable` to return a reference, not a copy of the variable. 

**Code to reproduce the issue**

```python
counter = tf.Variable(0)
    
@tf.function
def increment():
    return counter.assign_add(1) # ugly, direct access, but it works
    
print(increment().numpy()) # prints 1
print(increment().numpy()) # prints 2

@tf.function
def get_counter():
    return counter   # actually returns a Tensor (a copy), not a reference to the variable

@tf.function
def increment():
    return get_counter().assign_add(1)  # access through an accessor

increment() # raises AttributeError: 'Tensor' object has no attribute 'assign_add'
```

**Other info / logs**
Here is the stacktrace:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-1-dfbc861847ab> in <module>
     18     return get_counter().assign_add(1)
     19
---> 20 increment() # raises AttributeError: '[...].EagerTensor' object has no attribute 'assign_add'

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    424     # This is the first call of __call__, so we have to initialize.
    425     initializer_map = {}
--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
    427     if self._created_variables:
    428       try:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    368     self._concrete_stateful_fn = (
    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 370             *args, **kwds))
    371
    372     def invalid_creator_scope(*unused_args, **unused_kwds):

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1311     if self._input_signature:
   1312       args, kwargs = None, None
-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1314     return graph_function
   1315

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1578           or call_context_key not in self._function_cache.missed):
   1579         self._function_cache.missed.add(call_context_key)
-> 1580         graph_function = self._create_graph_function(args, kwargs)
   1581         self._function_cache.primary[cache_key] = graph_function
   1582         return graph_function, args, kwargs

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1510             arg_names=arg_names,
   1511             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1512             capture_by_value=self._capture_by_value),
   1513         self._function_attributes)
   1514

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693
--> 694       func_outputs = python_func(*func_args, **func_kwargs)
    695
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    684                   optional_features=autograph_options,
    685                   force_conversion=True,
--> 686               ), args, kwargs)
    687
    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    390     return _call_unconverted(f, args, kwargs)
    391
--> 392   result = converted_f(*effective_args, **kwargs)
    393
    394   # The converted function's closure is simply inserted into the function's

/var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gn/T/tmpfo1eq6i2.py in tf__increment()
      4   retval_ = None
      5   do_return = True
----> 6   retval_ = ag__.converted_call('assign_add', get_counter(), ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (1,), {})
      7   return retval_
      8

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    234       owner = inspect_utils.SuperWrapperForDynamicAttrs(owner)
    235
--> 236     f = getattr(owner, f)
    237
    238   if inspect_utils.isbuiltin(f):

AttributeError: 'Tensor' object has no attribute 'assign_add'
```

"
26340,Timeline not reporting compute times for some GPU ops,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: Python 2.7.15rc1
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: NVIDIA Corporation GP104GL [Quadro P5000] (2 GPUs)


**Describe the current behavior**
I am running the sample [RNNLM code](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb). I just added timeline logging code in the given tutorial for training ([Modified code here](https://github.com/xilenteyex/RNNLM_ptb_timeline)) to analyze the sequence of low level operations being executed as well as the time each operation is taking. There are a lot of operations which are placed on GPU and they have just kernel queue/launch times recorded in timeline in `/job:localhost/replica:0/task:0/device:GPU:.* Compute` streams. I am unable to find actual time to execute those operations in `/device:GPU:0/.*Compute` streams. To identify these operations, I used [this script](https://github.com/xilenteyex/RNNLM_ptb_timeline/blob/master/get_unexec_ops.py). Is this a bug, or am I missing something in my understanding of Timeline? [List of operations with kernel queue times reported, but no execution times can be found here.](https://github.com/xilenteyex/RNNLM_ptb_timeline/blob/master/un_exec_tmp.txt)
I want to do this because I am trying to do model parallelism for this RNNLM (Reccurrent Neural Network for Language Modeling) code and I want to place ops after running a heuristic graph partitioning algorithm which takes as input the estimated compute time as well as dependencies for each operation.

**Describe the expected behavior**
For every kernel that is queued on a GPU I expect it to be executed at some point and logged in the Timeline

**Code to reproduce the issue**
[tutorial code for RNNLM using ptb_data_set can be found here](https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb)
[modified tutorial code with Timline logging added for training loop can be found here](https://github.com/xilenteyex/RNNLM_ptb_timeline)

execute the following commands to get the list of operations which have kernel queue times in timeline and execution times are not logged. A file named `new_unexecute_ops.txt` will be created and each line contains name of one of these operations.
 
```
git clone https://github.com/xilenteyex/RNNLM_ptb_timeline
python ptb_word_lm_timeline.py --data_path=simple-examples/data/ --model=medium
python get_unexec_ops.py logs/timeline_logs/timeline_rnnlm_med_0_100.ctf.json new_unexecute_ops.txt
```



**Other info / logs**
[Sample timeline file can be found here](https://github.com/xilenteyex/RNNLM_ptb_timeline/tree/master/logs/timeline_logs)
[List of ops with missing execution times in the timeline file can be found here](https://github.com/xilenteyex/RNNLM_ptb_timeline/blob/master/un_exec_tmp.txt)
"
26339,"Build r1.12 from source with CUDA10.0 sucessfully, but still need CUDA9.0 when importing","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.12
- Python version: py3.6
- Installed using virtualenv? pip? conda?: source and pip 
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.9.3
- CUDA/cuDNN version: CUDA10.0/cuDNN7.5.0
- GPU model and memory: TitanXP

**Describe the problem**
Since I want to use CUDA10.0, I tried to build TF1.12 from source with above versions.
The build is successfully done, and my .tf_configure.bazelrc is correct.
After install the pip wheel package, I import the tensorflow but it still look for CUDA9.0 which is not provided in environment path.

ERROR:
```
Python 3.6.4 (default, Mar 23 2018, 22:59:35) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/web_server/dlpy72/py3.6-cuda10.0/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

I tried the pip wheel that built by other people but still get the same error.
Seems somehow CUDA9.0 is needed by some components? 
Any hint will be welcome

Configuration:
```
$ ./configure 
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.15.0 installed.
Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: n
No Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-10.0


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Please specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/web_server/xiaolun/TensorRT-5.0.2.6-10.0


Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 2.3


Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]:


NCCL found at /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/libnccl.so.2
Assuming NCCL header path is /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/../include/nccl.h
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 3.5


Do you want to use clang as CUDA compiler? [y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /home/web_server/gcc-4.9.3/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
Configuration finished

```"
26338,CPU build error on OSX Mojave in hwloc dependency,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX - 10.14
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master branch
- Python version: Python 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): clang 10.0.1
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


**Describe the problem**
I have followed the instructions in the [installation docs](https://www.tensorflow.org/install/source#macos) for Mac, but when I try to build the source using bazel:

```
 bazel build --verbose_failures --config=opt //tensorflow/tools/pip_package:build_pip_package
```

I see the following errors when trying to build `hwloc`:
```
external/hwloc/hwloc/distances.c:32:3: error: use of undeclared identifier 'locale_t'
```

<Details> <summary><em>trace</em></summary>
```

ERROR: /private/var/tmp/_bazel_npradhan/36fac8838dc4eaff04ef63d531bc51a3/external/hwloc/BUILD.bazel:214:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1): wrapped_clang failed: error executing command
  (cd /private/var/tmp/_bazel_npradhan/36fac8838dc4eaff04ef63d531bc51a3/execroot/org_tensorflow && \
  exec env - \
    APPLE_SDK_PLATFORM='' \
    APPLE_SDK_VERSION_OVERRIDE='' \
    PATH='/Users/npradhan/miniconda3/envs/numpyro/bin:/Users/npradhan/miniconda3/bin:/Users/npradhan/.cargo/bin:/usr/local/opt/python/libexec/bin:/Users/npradhan/.nvm/v0.10.32/bin:/bin:/usr/local/sbin:/usr/local/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/opt/X11/bin:/usr/local/munki:/Library/TeX/texbin:/Users/npradhan/bin:/Users/npradhan/gocode/bin:/Users/npradhan/.opus/bin:/Users/npradhan/miniconda2/bin:/Users/npradhan/.opus/bin:~/Library/Python/3.6/bin' \
    XCODE_VERSION_OVERRIDE=10.2.0 \
  external/local_config_cc/wrapped_clang '-D_FORTIFY_SOURCE=1' -fstack-protector -fcolor-diagnostics -Wall -Wthread-safety -Wself-assign -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -iquote external/hwloc -iquote bazel-out/host/genfiles/external/hwloc -iquote bazel-out/host/bin/external/hwloc -isystem external/hwloc/hwloc -isystem bazel-out/host/genfiles/external/hwloc/hwloc -isystem bazel-out/host/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/host/genfiles/external/hwloc/include -isystem bazel-out/host/bin/external/hwloc/include -MD -MF bazel-out/host/bin/external/hwloc/_objs/hwloc/distances.d '-frandom-seed=bazel-out/host/bin/external/hwloc/_objs/hwloc/distances.o' -isysroot __BAZEL_XCODE_SDKROOT__ -g0 '-march=native' -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -no-canonical-prefixes -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/hwloc/hwloc/distances.c -o bazel-out/host/bin/external/hwloc/_objs/hwloc/distances.o)
Execution platform: @bazel_tools//platforms:host_platform
external/hwloc/hwloc/distances.c:32:3: error: use of undeclared identifier 'locale_t'
  hwloc_localeswitch_declare;
  ^
external/hwloc/include/private/misc.h:493:36: note: expanded from macro 'hwloc_localeswitch_declare'
#define hwloc_localeswitch_declare locale_t __old_locale = (locale_t)0, __new_locale
                                   ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__new_locale'
    hwloc_localeswitch_init();
    ^
external/hwloc/include/private/misc.h:495:3: note: expanded from macro 'hwloc_localeswitch_init'
  __new_locale = newlocale(LC_ALL_MASK, ""C"", (locale_t)0); \
  ^
external/hwloc/hwloc/distances.c:52:5: warning: implicit declaration of function 'newlocale' is invalid in C99 [-Wimplicit-function-declaration]
external/hwloc/include/private/misc.h:495:18: note: expanded from macro 'hwloc_localeswitch_init'
  __new_locale = newlocale(LC_ALL_MASK, ""C"", (locale_t)0); \
                 ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier 'locale_t'
external/hwloc/include/private/misc.h:495:47: note: expanded from macro 'hwloc_localeswitch_init'
  __new_locale = newlocale(LC_ALL_MASK, ""C"", (locale_t)0); \
                                              ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier 'LC_ALL_MASK'
external/hwloc/include/private/misc.h:495:28: note: expanded from macro 'hwloc_localeswitch_init'
  __new_locale = newlocale(LC_ALL_MASK, ""C"", (locale_t)0); \
                           ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier 'locale_t'
external/hwloc/include/private/misc.h:496:24: note: expanded from macro 'hwloc_localeswitch_init'
  if (__new_locale != (locale_t)0)                         \
                       ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__new_locale'; did you mean 'newlocale'?
external/hwloc/include/private/misc.h:496:7: note: expanded from macro 'hwloc_localeswitch_init'
  if (__new_locale != (locale_t)0)                         \
      ^
external/hwloc/hwloc/distances.c:52:5: note: 'newlocale' declared here
external/hwloc/include/private/misc.h:495:18: note: expanded from macro 'hwloc_localeswitch_init'
  __new_locale = newlocale(LC_ALL_MASK, ""C"", (locale_t)0); \
                 ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__old_locale'
    hwloc_localeswitch_init();
    ^
external/hwloc/include/private/misc.h:497:5: note: expanded from macro 'hwloc_localeswitch_init'
    __old_locale = uselocale(__new_locale);                \
    ^
external/hwloc/hwloc/distances.c:52:5: warning: implicit declaration of function 'uselocale' is invalid in C99 [-Wimplicit-function-declaration]
external/hwloc/include/private/misc.h:497:20: note: expanded from macro 'hwloc_localeswitch_init'
    __old_locale = uselocale(__new_locale);                \
                   ^
external/hwloc/hwloc/distances.c:52:5: error: use of undeclared identifier '__new_locale'; did you mean 'newlocale'?
external/hwloc/include/private/misc.h:497:30: note: expanded from macro 'hwloc_localeswitch_init'
    __old_locale = uselocale(__new_locale);                \
                             ^
external/hwloc/hwloc/distances.c:52:5: note: 'newlocale' declared here
external/hwloc/include/private/misc.h:495:18: note: expanded from macro 'hwloc_localeswitch_init'
  __new_locale = newlocale(LC_ALL_MASK, ""C"", (locale_t)0); \
                 ^
external/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier 'locale_t'
    hwloc_localeswitch_fini();
    ^
external/hwloc/include/private/misc.h:500:24: note: expanded from macro 'hwloc_localeswitch_fini'
  if (__new_locale != (locale_t)0) {   \
                       ^
external/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier '__new_locale'
external/hwloc/include/private/misc.h:500:7: note: expanded from macro 'hwloc_localeswitch_fini'
  if (__new_locale != (locale_t)0) {   \
      ^
external/hwloc/hwloc/distances.c:62:5: warning: implicit declaration of function 'uselocale' is invalid in C99 [-Wimplicit-function-declaration]
external/hwloc/include/private/misc.h:501:5: note: expanded from macro 'hwloc_localeswitch_fini'
    uselocale(__old_locale);           \
    ^
external/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier '__old_locale'
external/hwloc/include/private/misc.h:501:15: note: expanded from macro 'hwloc_localeswitch_fini'
    uselocale(__old_locale);           \
              ^
external/hwloc/hwloc/distances.c:62:5: warning: implicit declaration of function 'freelocale' is invalid in C99 [-Wimplicit-function-declaration]
external/hwloc/include/private/misc.h:502:5: note: expanded from macro 'hwloc_localeswitch_fini'
    freelocale(__new_locale);          \
    ^
external/hwloc/hwloc/distances.c:62:5: error: use of undeclared identifier '__new_locale'
external/hwloc/include/private/misc.h:502:16: note: expanded from macro 'hwloc_localeswitch_fini'
    freelocale(__new_locale);          \
               ^
4 warnings and 12 errors generated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 14.263s, Critical Path: 6.70s
INFO: 158 processes: 158 local.
FAILED: Build did NOT complete successfully
```

</Details>"
26337,Make weighted_cross_entropy_with_logits consistent with sigmoid_cross_entropy_with_logits in signature,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Currently, the signature of the two functions are:
```python
@tf_export(""nn.sigmoid_cross_entropy_with_logits"")
def sigmoid_cross_entropy_with_logits(  # pylint: disable=invalid-name
    _sentinel=None,
    labels=None,
    logits=None,
    name=None):
```
And 
```python
@tf_export(""nn.weighted_cross_entropy_with_logits"")
def weighted_cross_entropy_with_logits(targets, logits, pos_weight, name=None):
```

TF2.0 is a good chance to make them consistent in the naming.

**Will this change the current api? How?**
Yes.

**Who will benefit with this feature?**
Users of the two functions will be less confused.

**Any Other info.**
A follow up of #7086"
26335,error building on Windows 10: Python Configuration Error: Problem getting numpy include path.,"
- Windows 10
- TensorFlow installed from source following these instructions: https://www.tensorflow.org/install/source_windows
- TensorFlow version:
- Python version: 3.7 installed with Anaconda, in ../Anaconda3/python
- Bazel version (if compiling from source): 0.23.0
- CUDA/cuDNN version: 10.1/7
- GPU model and memory: Geoforce GTX 1080Ti

Running this command: 

bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

c:\Users\Usertytell_admin\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: C:/users/usertytell_admin/_bazel_tytell_admin/oimkit2z/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

ERROR: C:/users/usertytell_admin/tensorflow/third_party/py/numpy/BUILD:11:1: no such package '@local_config_python//': Traceback (most recent call last):
        File ""C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl"", line 344
                _create_local_python_repository(repository_ctx)
        File ""C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl"", line 296, in _create_local_python_repository
                _get_numpy_include(repository_ctx, python_bin)
        File ""C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl"", line 276, in _get_numpy_include
                _execute(repository_ctx, [python_bin, ""-c"",...""], <2 more arguments>)
        File ""C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl"", line 56, in _execute
                _fail(""\n"".join([error_msg.strip() if ... """"]))
        File ""C:/users/usertytell_admin/tensorflow/third_party/py/python_configure.bzl"", line 27, in _fail
                fail((""%sPython Configuration Error:%...)))
Python Configuration Error: Problem getting numpy include path.
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Usertytell_admin\Anaconda3\lib\site-packages\numpy\__init__.py"", line 140, in <module>
    from . import _distributor_init
  File ""C:\Users\Usertytell_admin\Anaconda3\lib\site-packages\numpy\_distributor_init.py"", line 34, in <module>
    from . import _mklinit
ImportError: DLL load failed: The specified module could not be found.
Is numpy installed?
 and referenced by '//third_party/py/numpy:headers'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
INFO: Elapsed time: 14.234s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (235 packages loaded, 3598 targets configured)
    currently loading: tensorflow/lite/kernels
    Fetching @eigen_archive; Restarting. 4s
    Fetching @local_config_python; fetching
    Fetching @highwayhash; fetching



"
26333,GPU Support build instructions suggest old Docker image,"
**System information**
- Linux Ubuntu 18.04.2 LTS:
- TensorFlow installed from Docker image: 1.13.1-gpu-py3-jupyter
- TensorFlow version: Should be 1.13.1
- Python version: From Docker, appears to be 3.5.2
- Installed using pip: NA - don't get that far
- Bazel version (if compiling from source): Missing in docker image?
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda-10 from Docker image
- GPU model and memory:

processor       : 11
vendor_id       : GenuineIntel
cpu family      : 6
model           : 44
model name      : Intel(R) Core(TM) i7 CPU       X 980  @ 3.33GHz
stepping        : 2
microcode       : 0x13
cpu MHz         : 1630.792
cache size      : 12288 KB
physical id     : 0
siblings        : 12
core id         : 10
cpu cores       : 6
apicid          : 21
initial apicid  : 21
fpu             : yes
fpu_exception   : yes
cpuid level     : 11
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 cx16 xtpr pdcm pcid sse4_1 sse4_2 popcnt aes lahf_lm epb pti tpr_shadow vnmi flexpriority ept vpid dtherm ida arat
bugs            : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf
bogomips        : 6675.50
clflush size    : 64
cache_alignment : 64
address sizes   : 36 bits physical, 48 bits virtual
power management:

THE PROBLEM: The Docker image tensorflow/tensorflow:1.13.1-gpu-py3-jupyter doesn't build as per the instructions here:

https://www.tensorflow.org/install/source#gpu_support_2

Using the tensorflow/tensorflow:1.13.1-gpu-py3-jupyter Docker image the build fails at the first step. (./configure) There's nothing in the /tensorflow directory.

Using the tensorflow/tensorflow:nightly-devel-gpu-py3 Docker image everything works fine and after being built tensorflow can be imported in python within the Docker container. In this image there is source code in /tensorflow.

ALSO, in this Docker container /usr/local/lib has only python3.5. In the nightly-devel-gpu-py image there is a bazel directory


"
26331,tf.gradients returns incorrect results if used multiple times on CudnnLSTM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below):1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): unknown
- GCC/Compiler version (if compiling from source): unknown
- CUDA/cuDNN version:  10.0 / 7.4
- GPU model and memory: TI 1080, 12GB RAM

I found some cases where fetching multiple gradient computations for the parameters of a CudnnLSTM seems to result in incorrect results. For example, the following code:

```
import tensorflow as tf
from tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import CudnnLSTM

x = tf.fill((2, 1, 5), 0.5)
layer = CudnnLSTM(1, 5)
layer.build(x.shape)
output = layer.apply(x)[0]

sess = tf.Session()

loss = tf.reduce_sum(output)
vars = tf.trainable_variables()
grad1 = tf.gradients(loss, vars)[0]
grad2 = tf.gradients(loss, vars)[0]

sess.run(tf.global_variables_initializer())
g1, g2 = sess.run([grad1, grad2])
print(g1[:10])
print(g2[:10])
print((g1 - g2)[:10])
```
Shows the two gradient computation are far from equal, even though they are requesting exactly the same value. On my machine I saw:

```
[ 0.0290555   0.0290555   0.0290555   0.0290555   0.0290555  -0.00758932
 -0.00758932 -0.00758932 -0.00758932 -0.00758932]
[1.7652620e-04 1.7652620e-04 1.7652620e-04 1.7652620e-04 1.7652620e-04
 7.4650106e-06 7.4650106e-06 7.4650106e-06 7.4650106e-06 7.4650106e-06]
[ 0.02887898  0.02887898  0.02887898  0.02887898  0.02887898 -0.00759678
 -0.00759678 -0.00759678 -0.00759678 -0.00759678]
```

which seems far too large for the difference to be a rounding issue.

Using `g1, g2 = [sess.run(grad1), sess.run(grad2)]` will fix the issue.

The issue also goes away if we use  `grads = sess.run([grad1, grad1])`, but seems to remain for other, non-trivial cases. For example, if we had done `grad2 = tf.gradients(loss*2, vars)[0]` `g2` should be twice as large as `g1`, but it will be way off."
26325,[TF 2.0] Allow declaring dynamic tf.Variable shapes,"In the current TensorFlow 2.0 beta, `tf.Variables` inherits its shape from the initial value that it's provided:

```python
>>> tf.Variable(tf.zeros([16, 32])).shape
TensorShape([16, 32])
```

But for some use-cases, I'd like to explicitly make the Variable have a dynamic (`None`) shape:

```python
tf.Variable(tf.zeros([16, 32]), shape=[None, 32])
```

In the current beta, you can kind of do this via;
```python
import tensorflow as tf

class HasDynamicVariable(tf.Module):

  def __init__(self):
    super(HasDynamicVariable, self).__init__()
    self.v = None

  @tf.function(input_signature=[tf.TensorSpec([None, 2], tf.float32)])
  def make_dynamic_variable(self, initial_value):
    if self.v is None:
      self.v = tf.Variable(initial_value)
    return self.v
```
(xref https://github.com/tensorflow/community/pull/34#issuecomment-467690616)

But allowing you to explicitly annotate `tf.Variable(..., shape=...)` is IMO a lot cleaner and easier to use than having to declare it via the input signature.

(My particular use-case is to have something like a `tf.keras.layers.LSTM` with a dynamic batch size -- that means I'd like to do something like:

```
lstm = ...
lstm.reset_state(batch_size=5)
lstm(batch1)
lstm.reset_state(batch_size=20)
lstm(batch2)
```
)"
26324,tf.image.grayscale_to_rgb is not taking a grayscale image going into bugs ,"*System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  ubuntu 16.04
- TensorFlow installed from (source or binary): pip installed
- TensorFlow version (use command below): 1.12
- Python version: 2.7

Code- 
import tensorflow as tf
import matplotlib.pyplot
import matplotlib.pyplot as plt

fname = '/media/titan/ACER DATA/Mainak/ytfvideo/resized/Aaron_Eckhartimage_00001.jpg'
img = matplotlib.pyplot.imread(fname)

tf_img = tf.convert_to_tensor(img)
tf_img = tf.image.grayscale_to_rgb(tf_img)
brght_img = tf.image.flip_left_right(tf_img)
plt.imshow(img)
plt.show()
**Aaron_Eckhartimage_00001.jpg is a grayscale image
Error-

Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    tf_img = tf.image.grayscale_to_rgb(tf_img)
  File ""/home/titan/Documents/cpuenv/local/lib/python2.7/site-packages/tensorflow/python/ops/image_ops_impl.py"", line 1530, in grayscale_to_rgb
    rgb.set_shape(images.get_shape()[:-1].concatenate([3]))
  File ""/home/titan/Documents/cpuenv/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 561, in set_shape
    raise ValueError(str(e))
ValueError: Dimension 1 in both shapes must be equal, but are 120 and 3. Shapes are [40,120] and [40,3].

"
26323,"Pruning example, ValueError: Could not find a checkpoint at: .","Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No I have not
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Antergos Linux (Gnome Version 3.30.2)
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: Source
- **TensorFlow version (use command below)**: b'v1.13.1-0-g6612da8951' 1.13.1
- **Python version**: 3.6.0
- **Bazel version (if compiling from source)**: 0.22.1
- **GCC/Compiler version (if compiling from source)**: gcc (GCC) 8.2.1 20181127
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: 
``
$ examples_dir=contrib/model_pruning/examples
$ bazel build -c opt $examples_dir/cifar10:cifar10_{train,eval}
$ bazel-bin/$examples_dir/cifar10/cifar10_train --pruning_hparams=name=cifar10_pruning,begin_pruning_step=10000,end_pruning_step=100000,target_sparsity=0.9,sparsity_function_begin_step=10000,sparsity_function_end_step=100000
$ bazel build -c opt contrib/model_pruning:strip_pruning_vars
$ bazel-bin/contrib/model_pruning/strip_pruning_vars --checkpoint_path=/tmp/cifar10_train --output_node_names=softmax_linear/softmax_linear_2 --filename=cifar_pruned.pb
``

### Describe the problem
When I run the commands above, it gets to train the model and everything, but fails when it tries to strip the pruning vars. Traceback is:
``
$ bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars --checkpoint_path=/tmp/cifar10_train --output_node_names=softmax_linear/softmax_linear_2 --filename=cifar_pruned.pb
Traceback (most recent call last):
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars.py"", line 103, in <module>
    app.run(main=main, argv=[sys.argv[0]] + unparsed)
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/absl_py/absl/app.py"", line 300, in run
    _run_main(main, args)
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/absl_py/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars.py"", line 78, in main
    FLAGS.output_dir, FLAGS.filename)
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars.py"", line 67, in strip_pruning_vars
    checkpoint_dir, output_node_names)
  File ""/home/cenh/tensorflow/bazel-bin/tensorflow/contrib/model_pruning/strip_pruning_vars.runfiles/org_tensorflow/tensorflow/contrib/model_pruning/python/strip_pruning_vars_lib.py"", line 132, in graph_def_from_checkpoint
    .format(checkpoint_dir))
ValueError: Could not find a checkpoint at: .
``

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
26322,[Bug]: tf.keras update_op for computing running mean and variance for BatchNorm causes an error,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 9 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6
- CUDA/cuDNN version: CUDA 9.0
- GPU model and memory: TITAN X (Pascal) 

I am using tf.keras API to build a ConvNet and I train it with the Tensorflow custom training loop, using graph API. The ConvNet contains BatchNorm layers and as I don't use model.fit() for training, I have to manage the updates to the moving mean and variance manually. However this causes an error. Here is a minimal reproducible case:

```
import numpy as np
import tensorflow as tf

from tensorflow.python.keras import layers
from tensorflow.python.keras import initializers
from tensorflow.python.keras import models

tfsum = tf.contrib.summary

HEIGHT = 480
WIDTH = 640

def conv(x, num_out_layers, kernel_size, stride, activation_fn=""relu""):
    kernel_initializer = initializers.VarianceScaling()
    x = layers.Conv2D(num_out_layers,
                      (kernel_size, kernel_size),
                      strides=stride,
                      padding=""SAME"",
                      activation=None,
                      kernel_initializer=kernel_initializer)(x)

    x = layers.BatchNormalization()(x)
    x = layers.Activation(activation_fn)(x)
    return x


def build_cnn(img_shape):
    inputs = tf.keras.layers.Input(img_shape)

    conv1 = conv(inputs, 64, 7, 1)
    outputs = conv(conv1, 3, 7, 1)

    model = models.Model(inputs=[inputs], outputs=[outputs])
    return model


def main_graph():
    learning_rate = 0.001
    batch_size = 1
    num_steps = 100
    train_dir = "".""
    dataset_size = 20

    images = np.random.random_sample([dataset_size, HEIGHT, WIDTH, 3]).astype(np.float32)
    dataset = tf.data.Dataset.from_tensor_slices(images)
    dataset = dataset.repeat() \
                     .batch(batch_size)
    iterator = dataset.make_one_shot_iterator()
    image = iterator.get_next()
    print(image)

    global_step = tf.train.get_or_create_global_step()
    summary_writer = tfsum.create_file_writer(
        train_dir, flush_millis=10000)
    with summary_writer.as_default(), tfsum.record_summaries_every_n_global_steps(10):
        model = build_cnn([HEIGHT, WIDTH, 3])
        prediction = model(image, training=True)
        update_ops = model.updates
        train_var_list = model.trainable_variables

        prediction = tf.ensure_shape(prediction, [batch_size, HEIGHT, WIDTH, 3])

        loss = tf.reduce_mean(tf.abs(prediction-image))

        tf.contrib.summary.scalar(""loss"", loss)

        optimizer = tf.train.AdamOptimizer(learning_rate)
        with tf.control_dependencies(update_ops):
            train_op = optimizer.minimize(loss, global_step=global_step, var_list=train_var_list)

    sess = tf.Session()
    with sess, summary_writer.as_default():
        sess.run(tf.global_variables_initializer())
        sess.run(tf.local_variables_initializer())
        tf.contrib.summary.initialize(graph=tf.get_default_graph())

        for i in range(num_steps+1):
            _, global_step_val, loss_val, _, = sess.run([train_op, global_step, loss,
                                                        tfsum.all_summary_ops()])
            print(f""iter:{global_step_val:06d} loss: {loss_val:04.4f}"")


if __name__ == ""__main__"":
    main_graph()
```

This results in the following error message:

```
Caused by op 'input_1', defined at:
  File ""../../train_bug.py"", line 104, in <module>
    main_graph()
  File ""../../train_bug.py"", line 72, in main_graph
    model = build_cnn([HEIGHT, WIDTH, 3])
  File ""../../train_bug.py"", line 44, in build_cnn
    inputs = tf.keras.layers.Input(img_shape)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py"", line 229, in Input
    input_tensor=tensor)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/keras/engine/input_layer.py"", line 112, in __init__
    name=self.name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 1747, in placeholder
    return gen_array_ops.placeholder(dtype=dtype, shape=shape, name=name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 5206, in placeholder
    ""Placeholder"", dtype=dtype, shape=shape, name=name)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/BS/eldar-3dshape/work/apps/miniconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor 'input_1' with dtype float and shape [?,480,640,3]
         [[node input_1 (defined at ../../train_bug.py:44)  = Placeholder[dtype=DT_FLOAT, shape=[?,480,640,3], _device=""/job:localhost/replica:0/task:0/device:GPU:0""]()]]
```

Removing `with tf.control_dependencies(update_ops)` makes it run, but then the batchnorm statistics are not being updated. How can I use Keras in this setting?"
26321,LookupError: No gradient defined for operation 'MatrixLogarithm',"**System information**
- OS Platform and Distribution :Linux Ununtu 14.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 3.6
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: GeForce GTX 1080/ 8G

**Describe the current behavior**
There is no gradient defined for tf.linalg.logm(), therefore it could not be used in training procedures.
"
26319,Training parameter in Keras models passed as None in 1.13,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary(pip)
- TensorFlow version (use command below): v1.13.1-0-g6612da8951 1.13.1
- Python version: 3.5.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 10, cuDNN 7.5
- GPU model and memory: GTX 1050ti, 4GB

**Describe the current behavior**
As described in the third example in [the documentation for Keras models](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#class_model), a boolean `training` parameter can be used in the `call` method of subclassed models. However, the parameter is passed as `None` when it should be `True`. 

**Describe the expected behavior**
The training parameter should be passed as True when the model is training.

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
tf.enable_eager_execution()

class MyModel(tf.keras.Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.dense = tf.keras.layers.Dense(4)

  def call(self, inputs, training=False):
    print('Training', training)
    return self.dense(inputs)
    
model = MyModel()
model.compile(optimizer=tf.train.AdagradOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])

inp = np.ones((5, 3), dtype=np.float32)
out = np.ones((5, 4), dtype=np.float32)

# training should be False
model(inp)

# training should be True
model.fit(inp, out)
```

**Other info / logs**
This only happens in TF 1.13 and not in 1.12. I also tried 2.0 alpha and the bug is still present"
26318,[FR] A better way instead of  `while True: sess.run(..)`,"I'd like to make tensorflow as the inference backend, and there are quantities of inference inputs to handle.

Currently, tensorflow manages multiple inference inputs as different execution triggers of `sess.run`, e.g:

```python
while True:
    sess.run(infer_op)
```

However, the throughput is very bad because every inference task will run through python runtime and also tensorflow scheduler, so I require something like this **directly**:

```python
    sess.run(tf.loop_forever(infer_op))
```

You can assume the infer_op read inputs from an one_shot_iterator, and I never want this loop to stop just like it is a long-run server.

Anyway to do this at the moment, or the possibility to support this op like `tf.loop_forever` ?

You can also share a simple hack in tensorflow source code to make the `sess.run` (in C++ session) to repeat same computing again without returning data to python session. Thanks!"
26316,TF crashes when tensor size is increased,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): not sure 
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: release 9.0, V9.0.176/ 9.0
- GPU model and memory: GeForce RTX 2080 Ti 11GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am running a custom code using TF. Part of the code includes a 2d convolution using tf.nn.conv2d. The code is running great, but when I increase the size of one of the tensors by a factor of 2 then everything crashes and I get this error:
2019 12:29:56.729188: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2019 12:29:56.988944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce RTX 2080 Ti major: 7 minor: 5 memoryClockRate(GHz): 1.65
pciBusID: 0000:65:00.0
totalMemory: 11.00GiB freeMemory: 8.99GiB
2019 12:29:56.995629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019 12:29:57.467089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019 12:29:57.467875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988] 0 
2019 12:29:57.468098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0: N 
2019 12:29:57.468449: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8665 MB memory) > physical GPU (device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:65:00.0, compute capability: 7.5)
2019 12:30:06.043926: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019 12:30:06.044411: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 000001F8E589DCD0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019 12:30:06.044888: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 000001F8E589DCD0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019 12:30:06.045401: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.

The problem might not be connected to the convolution operation but it fails after I increase the size of this tensor. 
Can you help me solving this issue?
Thank you very much,
Gilad
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26315,error: call of overloaded 'ResizeBilinearOpModel(<brace-enclosed initializer list>)' is ambiguous,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS - docker images tensorflow/tensorflow:latest and tensorflow/devel
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit a2bb5db1bf7931b0dc2cd08e53b8798489568198
- Python version: Python 2.7.12
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
When building tensorflow/lite/kernels:resize_bilinear_test error occurs because there's ambiguity in constructor arguments and as the result which constructor to be used.
```
tensorflow/lite/kernels/resize_bilinear_test.cc:403:58: error: call of overloaded 'ResizeBilinearOpModel(<brace-enclosed initializer list>)' is ambiguous
   ResizeBilinearOpModel m({TensorType_INT8, {1, 2, 2, 2}});
                                                          ^
tensorflow/lite/kernels/resize_bilinear_test.cc:29:12: note: candidate: tflite::{anonymous}::ResizeBilinearOpModel::ResizeBilinearOpModel(const tflite::TensorData&, std::initializer_list<int>)
   explicit ResizeBilinearOpModel(const TensorData& input,
            ^
tensorflow/lite/kernels/resize_bilinear_test.cc:27:7: note: candidate: tflite::{anonymous}::ResizeBilinearOpModel::ResizeBilinearOpModel(const tflite::{anonymous}::ResizeBilinearOpModel&) <deleted>
 class ResizeBilinearOpModel : public SingleOpModel {
       ^
tensorflow/lite/kernels/resize_bilinear_test.cc:27:7: note: candidate: tflite::{anonymous}::ResizeBilinearOpModel::ResizeBilinearOpModel(tflite::{anonymous}::ResizeBilinearOpModel&&) <deleted>
Target //tensorflow/lite/kernels:resize_bilinear_test failed to build
```
It's explained on [stack overflow](https://stackoverflow.com/questions/48011854/deleting-overloaded-function-c11-call-of-overloaded-is-ambiguous/48012069#48012069) and [second point of the document](https://timsong-cpp.github.io/cppwp/n3337/dcl.fct.def.delete)

It can be solved by changing places like:
```
 ResizeBilinearOpModel m({TensorType_FLOAT32, {1, 2, 1, 1}});
```
to 
```
 ResizeBilinearOpModel m(TensorData({TensorType_FLOAT32, {1, 2, 1, 1}}));
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test --config=opt tensorflow/lite/kernels:resize_bilinear_test

**Any other info / logs**
Below is a simplified code showing the same problem as occurring inside the test:
```
struct Pair{
	int i;
	int j;
	Pair(int i, int j = -1): i(i), j(j) {};
};

struct B {
	B(const B&) = delete;
	explicit B(const Pair& p, int j = 0) {
		cout << p.i << "" "" << p.j << "" "" << j << endl;
	}
};

int main() {
	B b({1, 2});
}
```
It also causes:
```
delete.cpp: In function int main():
delete.cpp:32:12: error: call of overloaded B(<brace-enclosed initializer list>) is ambiguous
  B b({1, 2});
            ^
delete.cpp:20:11: note: candidate: B::B(const Pair&, int)
  explicit B(const Pair& p, int j = 0) {
           ^
delete.cpp:19:2: note: candidate: B::B(const B&) <deleted>
  B(const B&) = delete;
  ^
```
It doesn't matter that copy constructor is deleted as this resolution takes place later."
26314,output 'beam_search_ops_gpu.cu.pic.o' was not created,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow -b r1.8
- TensorFlow version: r1.8
- Python version:3.6.8
- Installed using virtualenv? pip? conda?: conda 
- Bazel version (if compiling from source): 0.14.0
- GCC/Compiler version (if compiling from source):Xcode 8.2
- CUDA/cuDNN version: 9.2/ 7.2
- GPU model and memory: 1060 6G



**Describe the problem**
[1,081 / 1,095] 4 actions running
    Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc [for host]; 7s local
    Compiling tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resourERROR: /Users/aiamjay/PycharmProjects/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: output 'tensorflow/contrib/seq2seq/_objs/python/ops/_beam_search_ops_gpu/tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.pic.o' was not created
[1,081 / 1,095] 4 actions running
    Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc [for host]; 7s local
    Compiling tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resourERROR: /Users/aiamjay/PycharmProjects/tensorflow/tensorflow/contrib/seq2seq/BUILD:64:1: not all outputs were created or valid
[1,081 / 1,095] 4 actions running
    Compiling tensorflow/contrib/seq2seq/kernels/beam_search_ops_gpu.cu.cc [for host]; 7s local
    Compiling tensorflow/contrib/tensor_forest/kernels/v4/decision-tree-resourTarget //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 287.490s, Critical Path: 21.75s
INFO: 1046 processes, local.
**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26313,All mirrors are down: [GET returned 404 Not Found],"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): git clone https://github.com/tensorflow/tensorflow -b r1.8
- TensorFlow version: r1.8
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: conda python=3.6.8
- Bazel version (if compiling from source): 0.14.0
- GCC/Compiler version (if compiling from source):Xcode 8.2    
- CUDA/cuDNN version:cud 9.2 / cudnn 7.2.1
- GPU model and memory:   1066 6G



**Describe the problem**
/Users/aiamjay/PycharmProjects/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: error loading package 'tensorflow': 
Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': 
java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, 
https://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] 
to /private/var/tmp/_bazel_aiamjay/e76cd148deca576702165034f4a7f23b/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: 
All mirrors are down: [GET returned 404 Not Found] and referenced by '//tensorflow/tools/pip_package:build_pip_package'



ERROR: /Users/aiamjay/PycharmProjects/tensorflow/tensorflow/tools/pip_package/BUILD:166:1: error loading package 'tensorflow': 
Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': 
java.io.IOException: Error downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, 
https://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] 
to /private/var/tmp/_bazel_aiamjay/e76cd148deca576702165034f4a7f23b/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: 
All mirrors are down: [GET returned 404 Not Found] and referenced by '//tensorflow/tools/pip_package:build_pip_package'




Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: error loading package 'tensorflow': 
Encountered error while reading extension file 'protobuf.bzl': no such package '@protobuf_archive//': 
java.io.IOException: 
Error downloading [https://mirror.bazel.build/github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz, 
https://github.com/dtrebbien/protobuf/archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz] 
to /private/var/tmp/_bazel_aiamjay/e76cd148deca576702165034f4a7f23b/external/protobuf_archive/50f552646ba1de79e07562b41f3999fe036b4fd0.tar.gz: 
All mirrors are down: [GET returned 404 Not Found]
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel clean
bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env PATH --action_env DYLD_LIBRARY_PATH //tensorflow/tools/pip_package:build_pip_package**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26311,tf.function creates a new concrete function multiple times for the same inputs,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below <= needs to be updated to TF2):
tf.version.VERSION: '2.0.0-dev20190303'
tf.version.GIT_VERSION: 'v1.12.0-9460-gbfa3fcead3'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When calling a TF function multiple times with the same arguments, the function keeps getting traced and more and more new concrete functions get generated. This will hinder performance and waste RAM.

**Describe the expected behavior**
I expect a function to be trace just once for a given set of inputs.

**Code to reproduce the issue**

```python
import tensorflow as tf

@tf.function
def f(x):
    print(""  Tracing with shape"", x.shape)
    return x ** 3

for round in range(5):
    print(""Round"", round)
    f(tf.constant(3.))
    f(tf.constant(4.))
    f(tf.constant([[1., 2.]]))
    f(tf.constant([[]]))
    f(tf.constant([[3., 4.], [5., 6.]]))
    f(tf.constant([[7., 8.], [9., 10.], [11., 12.]]))        
    print()
```

When you run this, you see that `f()` gets traced at every round, but it should only be traced during round 0.

Also, not sure if this is related or if I should file another issue, but I noticed this:

```python
cf1 = f.get_concrete_function(tf.TensorSpec(shape=[], dtype=tf.float32))
cf2 = f.get_concrete_function(tf.constant(0.))
assert cf1 is cf2  # AssertionError
```

**Other info / logs**
Here is the output:

```
Round 0
  Tracing with shape ()
  Tracing with shape (1, 2)
  Tracing with shape (1, None)
  Tracing with shape (None, 2)

Round 1
  Tracing with shape (1, None)
  Tracing with shape (None, 2)

Round 2
  Tracing with shape (1, None)
  Tracing with shape (None, 2)

Round 3
  Tracing with shape (1, None)
  Tracing with shape (None, 2)

Round 4
  Tracing with shape (1, None)
  Tracing with shape (None, 2)
```"
26310,Trained model inference on GPU of nvidia TX2 get poor result  even error result,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:nvidia TX2
- TensorFlow installed from (source or binary):binary from https://nvidia.box.com/v/JP33-TF1-11-0-py35-wTRT
- TensorFlow version (use command below):1.110
- Python version:3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA9.0 cudnn 7.15
- GPU model and memory:8G


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I trained the model on the server and deployed the same version of tensorflow on TX2, but when I run the trained model with the GPU on TX2, I get a lot worse than on the server, but running the model on the CPU of TX2 does not cause this problem.
**Describe the expected behavior**
1. The result of running on the GPU of the server should be the same as the result of the GPU running on TX2. There should not be such a big gap.
2. The GPU running result on TX2 should be the same as the CPU running result on TX2.
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26309,tf.Data Pipeline using interleave starts fast then becomes extremely slow ,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: Binary
- **TensorFlow version (use command below)**: 1.13
- **Python version**: 3.7
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**:
- **CUDA/cuDNN version**: 9
- **GPU model and memory**: GTX 980 4GB
- **Exact command to reproduce**:

### Describe the problem

I am building a pipeline to feed a DNNRegressor model and the issue is that the input pipeline becomes extremely slow after the first few 100's steps.

I have thousands of TFRecord files of varying size (from 10MB to 3GB in size). Each input has 1038 columns. 
I am trying to create batches of size 512 with 1 ""row"" from each TFRecord file (each file has correlated data so I need 1 line from each of the 512 files being interleaved to have a ""shuffled"" batch). To do this,
I am using the Data.interleave method which seems perfect for my case.

Problem is the input is super fast at the start but gets painfully slow after the first 100's of steps.
CPU usage goes quickly from 100% at the start to near 0% as the pipeline becomes slower. However the disk read speed remains always at  > 60 MBps. What is the issue ? Are the smaller files straggling the pipeline in someway ?

I have tried using parralel interleave but that was even slower

### Source code / logs

```
def parse_tf(example):
    with tf.device('/cpu:0'):
        parsed_features = tf.parse_example(example, feature)
        feats = parsed_features
        labels = parsed_features.pop('labels')
        return feats, labels


def tf_input_fn(batch_size):
    with tf.device('/cpu:0'):
        files = os.listdir('D:/data/data')
        files = ['D:/data/data/' + file for file in files]

        dataset = (tf.data.Dataset.from_tensor_slices(files)
                   .interleave(lambda x:
                               tf.data.TFRecordDataset(x),
                               cycle_length=batch_size, block_length=1, num_parallel_calls=8)).batch(batch_size).map(
            parse_tf, num_parallel_calls=1)

        dataset = dataset.prefetch(100)
        return dataset


if __name__ == ""__main__"":

    batch_size_ = 512

    data = tf_input_fn(batch_size_)
    data_next = data.make_one_shot_iterator().get_next()
    ctr = 0
    with tf.Session() as data_sess:
        data_sess.run(tf.global_variables_initializer())
        tld_start = dt.datetime.now()
        try:
            while True:
                tld_out = data_sess.run(data_next)
                ctr += 1
                if ctr % 100 == 0:
                    tld_end = dt.datetime.now()
                    print(""Time for 100 steps: "" + str(tld_end - tld_start))
                    tld_start = dt.datetime.now()
                    ctr = 0
        except tf.errors.OutOfRangeError:
            print(""Done"")
```


Logs:

```
Time for 100 steps: 0:00:04.547647
Time for 100 steps: 0:00:03.132636
Time for 100 steps: 0:00:02.839416
Time for 100 steps: 0:00:02.639220
Time for 100 steps: 0:00:02.492228
Time for 100 steps: 0:00:01.991023
Time for 100 steps: 0:00:12.380473
Time for 100 steps: 0:00:29.162056
Time for 100 steps: 0:00:23.747592
Time for 100 steps: 0:00:28.041797
Time for 100 steps: 0:00:28.309055
Time for 100 steps: 0:00:26.240207
Time for 100 steps: 0:00:27.894402
Time for 100 steps: 0:00:25.832465
Time for 100 steps: 0:00:27.795538
Time for 100 steps: 0:00:28.219248
Time for 100 steps: 0:00:26.962029
Time for 100 steps: 0:00:24.568246
Time for 100 steps: 0:00:29.052262
Time for 100 steps: 0:00:27.003928
Time for 100 steps: 0:00:27.839937
Time for 100 steps: 0:00:28.380154
Time for 100 steps: 0:00:27.109850
Time for 100 steps: 0:00:28.234840
```"
26307,Inconsistent encoding leads to AttributeError,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Red Hat 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): compiled from source
- TensorFlow version (use command below): r13.1
- Python version: 3.6.3
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: P5000/16G

DeepMind open-sourced the implementation of IMPALA: https://github.com/deepmind/scalable_agent

For parallelism, they wrap a mechanism which is class-based in doing so, the file is: https://github.com/deepmind/scalable_agent/blob/master/py_process.py

There is a code snippet at the beginning of the file:

```
  class Zeros(object):
    def __init__(self, dim0):
      self._dim0 = dim0
    def compute(self, dim1):
      return np.zeros([self._dim0, dim1], dtype=np.int32)
    @staticmethod
    def _tensor_specs(method_name, kwargs, constructor_kwargs):
      dim0 = constructor_kwargs['dim0']
      dim1 = kwargs['dim1']
      if method_name == 'compute':
        return tf.contrib.framework.TensorSpec([dim0, dim1], tf.int32)
  with tf.Graph().as_default():
    p = py_process.PyProcess(Zeros, 1)
    result = p.proxy.compute(2)
    with tf.train.SingularMonitoredSession(
        hooks=[py_process.PyProcessHook()]) as session:
      print(session.run(result))  # Prints [[0, 0]].

```
however, when I tried to run it, I got the following error:

```
2019-03-01 17:37:16.260732: W tensorflow/core/framework/op_kernel.cc:1389] Unknown: AttributeError: 'Zeros' object has no attribute 'b'compute''
Traceback (most recent call last):

  File ""/home/yuming/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py"", line 207, in __call__
    ret = func(*args)

  File ""/data/yuming/eeg-dpg/py_process.py"", line 89, in py_call
    raise result

AttributeError: 'Zeros' object has no attribute 'b'compute''

```

I suspect maybe there is a mismatch between encoding, but not for sure, since I have no problem in running the code if I use Python 2.7.

Could anyone please take some effort on looking into it?
"
26306,tf.python no longer available in 1.13.1?,"Sorry if this is not exactly a documentation bug, but it is, for us, a big change in 1.13.1 that is not addressed in the release notes.

I realize that tf.python was never part of the public API, but it used to be available, and after upgrading from 1.12, it isn't any longer.  The reason we need it is that it is heavily used internally by tensorflow code, and occasionally we need to ""vendor"" some of this code.  It would be really inconvenient to do this by maintaining our own fork of tensorflow; instead we just copied a python file from the tensorflow source into our repo and made the changes we needed.  Is there some way to keep doing this in 1.13.1 with minimal changes (i.e., not finding all the uses of tf.python and changing them to their public api equivalents)?  (Also, btw, why do the python parts of the Tensorflow source use tf.python so much in the first place?  Couldn't they just use the public api?)"
26305,ValueError: as_list() is not defined on an unknown TensorShape.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10 pro
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.12.0
- Python version:3.6.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:9.0
- GPU model and memory:Nvidida1070/ 8GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
It gives an error when I am calling the unknown shape tensor into the dynamic_rnn function
**Describe the expected behavior**

**Code to reproduce the issue**
`def LSTM_model(embed, lstm_sizes, keep_prob_, batch_size):
     lstms = [tf.contrib.rnn.BasicLSTMCell(size) for size in lstm_sizes]
     drops = [tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob_) for lstm in lstms]
     cell = tf.contrib.rnn.MultiRNNCell(drops)
     initial_state = cell.zero_state(batch_size, tf.float64)
     lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)
     return lstm_outputs, final_state`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`
  File ""<ipython-input-2-fc074b4d9b11>"", line 23, in <module>
    output_data, final_state = LSTM_model(e_tr1, lstm_size, keep_prob_, batch_size)

  File ""C:\Users\rpsworker\Documents\GitHub\KGE-FE-SR_1\NAM_Modified\test_LSTM.py"", line 54, in LSTM_model
    lstm_outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state=initial_state)

  File ""C:\ProgramData\Anaconda3\envs\new_ten\lib\site-packages\tensorflow\python\ops\rnn.py"", line 664, in dynamic_rnn
    dtype=dtype)

  File ""C:\ProgramData\Anaconda3\envs\new_ten\lib\site-packages\tensorflow\python\ops\rnn.py"", line 733, in _dynamic_rnn_loop
    const_time_steps, const_batch_size = inputs_got_shape[0].as_list()[:2]

  File ""C:\ProgramData\Anaconda3\envs\new_ten\lib\site-packages\tensorflow\python\framework\tensor_shape.py"", line 904, in as_list
    raise ValueError(""as_list() is not defined on an unknown TensorShape."")

ValueError: as_list() is not defined on an unknown TensorShape.`
"
26304,Trouble Importing TensorFlow,"
**System information**
- Windows 10 Home

- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version: 3.6.0
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 
- GPU model and memory:



**Describe the problem**
I installed python 3.6.0 to make sure that tensorflow could run, because before i had python 3.7.1. Then I installed tensorflow via ""pip install tensorflow"". The installation was successful, but when trying to import tensorflow I got the error message shown below. I am unsure of what to do next, please help. PS: I am running this on a Lenovo ideapad 100, so there is not a dedicated graphics card or gpu.



**Any other info / logs**
File ""c:\Users\patri_yllzc1c\Desktop\TensorFlow Projects\NumberGuessCNN.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\patri_yllzc1c\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\patri_yllzc1c\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\patri_yllzc1c\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\patri_yllzc1c\AppData\Local\Programs\Python\Python36\lib\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.
"
26303,[TF 2.0] Support for TFGAN.,"[TFGAN](https://www.tensorflow.org/api_docs/python/tf/contrib/gan) is a popular lightweight library for training and evaluating GANs. In addition to providing the infrastructure for easily training and evaluating GANS, this library contains modules for a TFGAN-backed Estimator, evaluation metrics, features (such as virtual batch normalization), and losses.

The purpose of this feature request would be to migrate TFGAN to its own repo, and to ensure it is compatible with TensorFlow 2.0."
26302,[TF 2.0] StridedSlice issue with empty slice.,"Empty arrays cause TypeErrors with `StridedSlice`. A one-liner to reproduce would be:

```python
 tf.constant([1, 2, 3])[tf.constant([], dtype=tf.int32)]
```

Which returns:

```
TypeError: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got <tf.Tensor: id=1, shape=(0,), dtype=int32, numpy=array([], dtype=int32)>
```

The numpy equivalent:

```python
np.array([1, 2, 3])[np.array([], dtype=np.int32)]
```

works as expected."
26300,Random Uniform Not Supported for TFlite Conversion,"**System **information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): tf-nightly 1.14.1

**Describe the Problem**
Trying to convert frozen tensorflow model to .tflite so I can use it on my android phone, but conversion fails due to RandomUniform OP being unsupported.

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). 
Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). 
Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, FLOOR, MAX_POOL_2D, MUL, SHAPE, SPLIT, SUB. 
Here is a list of operators for which you will need custom implementations: RandomUniform.

```

**Any other info / logs**

Toco command used

tflite_convert 
--output_file=segmentleaves.tflite 
--graph_def_file=frozenGraph/frozen_model.pb 
--input_shapes=1,224,224,3 
--input_arrays=image_input 
--output_arrays=layer7_out

"
26299,TF_SessionRun_wrapper: expected all values in input dict to be ndarray,"I am using Tensorflow 1.13 in windows 10 

I am converting image data into tf.records. I am using build_image_data.py in which i am getting below error. 

**TF_SessionRun_wrapper: expected all values in input dict to be ndarray SKIPPED: Unexpected error while decoding** 

Code section 
    def decode_jpeg(self, image_data):
        print(type(image_data))
        **image = self._sess.run([self._decode_jpeg],
                               feed_dict={self._decode_jpeg_data: image_data})**
        assert len(image.shape) == 3
        assert image.shape[2] == 3
        return image



Please find below code snippet 
-----------------------------------------------------------------START-------------------------------------------

""""""Converts image data to TFRecords file format with Example protos.
The image data set is expected to reside in JPEG files located in the
following directory structure.
  data_dir/label_0/image0.jpeg
  data_dir/label_0/image1.jpg
  ...
  data_dir/label_1/weird-image.jpeg
  data_dir/label_1/my-image.jpeg
  ...
where the sub-directory is the unique label associated with these images.
This TensorFlow script converts the training and evaluation data into
a sharded data set consisting of TFRecord files
  train_directory/train-00000-of-01024
  train_directory/train-00001-of-01024
  ...
  train_directory/train-00127-of-01024
and
  test_directory/test-00000-of-00128
  test_directory/test-00001-of-00128
  ...
  test_directory/test-00127-of-00128
where we have selected 1024 and 128 shards for each data set. Each record
within the TFRecord file is a serialized Example proto. The Example proto
contains the following fields:
  image/encoded: string containing JPEG encoded image in RGB colorspace
  image/height: integer, image height in pixels
  image/width: integer, image width in pixels
  image/colorspace: string, specifying the colorspace, always 'RGB'
  image/channels: integer, specifying the number of channels, always 3
  image/format: string, specifying the format, always'JPEG'
  image/filename: string containing the basename of the image file
            e.g. 'n01440764_10026.JPEG' or 'ILSVRC2012_val_00000293.JPEG'
  image/class/label: integer specifying the index in a classification layer.
    The label ranges from [0, num_labels] where 0 is unused and left as
    the background class.
  image/class/text: string specifying the human-readable version of the label
    e.g. 'dog'
If you data set involves bounding boxes, please look at build_imagenet_data.py.
""""""
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from datetime import datetime
import os
import random
import sys
import threading

import numpy as np
import tensorflow as tf

from utils import constants

tf.app.flags.DEFINE_string('train_directory', constants.training_images_dir,
                           'Training data directory')

tf.app.flags.DEFINE_string('test_directory', constants.test_images_dir,
                           'Test data directory')

tf.app.flags.DEFINE_string('output_directory', constants.data_dir,
                           'Output data directory')

tf.app.flags.DEFINE_integer('train_shards', 3,
                            'Number of shards in training TFRecord files.')
tf.app.flags.DEFINE_integer('test_shards', 3,
                            'Number of shards in test TFRecord files.')

tf.app.flags.DEFINE_integer('num_threads', 3,
                            'Number of threads to preprocess the images.')

# The labels file contains a list of valid labels are held in this file.
# Assumes that the file contains entries as such:
#   dog
#   cat
#   flower
# where each line corresponds to a label. We map each label contained in
# the file to an integer corresponding to the line number starting from 0.
tf.app.flags.DEFINE_string('labels_file', constants.labels_file, 'Labels file')

FLAGS = tf.app.flags.FLAGS


def _int64_feature(value):
    """"""Wrapper for inserting int64 features into Example proto.""""""
    if not isinstance(value, list):
        value = [value]
    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))


def _bytes_feature(value):
    """"""Wrapper for inserting bytes features into Example proto.""""""
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))


def _convert_to_example(filename, image_buffer, label, text, height, width):
    """"""Build an Example proto for an example.

    Args:
      filename: string, path to an image file, e.g., '/path/to/example.JPG'
      image_buffer: string, JPEG encoding of RGB image
      label: integer, identifier for the ground truth for the network
      text: string, unique human-readable, e.g. 'dog'
      height: integer, image height in pixels
      width: integer, image width in pixels
    Returns:
      Example proto
    """"""

    colorspace = 'RGB'
    channels = 3
    image_format = 'JPEG'

    example = tf.train.Example(features=tf.train.Features(feature={
        'height': _int64_feature(height),
        'width': _int64_feature(width),
        'label': _int64_feature(label),
        'image_raw': _bytes_feature(tf.compat.as_bytes(image_buffer))}))
    return example


class ImageCoder(object):
    """"""Helper class that provides TensorFlow image coding utilities.""""""

    def __init__(self):
        # Create a single Session to run all image coding calls.
        self._sess = tf.Session()

        # Initializes function that converts PNG to JPEG data.
        self._png_data = tf.placeholder(dtype=tf.string)
        image = tf.image.decode_png(self._png_data, channels=3)
        self._png_to_jpeg = tf.image.encode_jpeg(image, format='rgb', quality=100)

        # Initializes function that decodes RGB JPEG data.
        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)
        self._decode_jpeg = tf.image.decode_jpeg(self._decode_jpeg_data, channels=3)

    def png_to_jpeg(self, image_data):
        print(type(image_data))
        return self._sess.run([self._png_to_jpeg],
                              feed_dict={self._png_data: image_data})

    def decode_jpeg(self, image_data):
        print(type(image_data))
        image = self._sess.run([self._decode_jpeg],
                               feed_dict={self._decode_jpeg_data: image_data})
        assert len(image.shape) == 3
        assert image.shape[2] == 3
        return image


def _is_png(filename):
    """"""Determine if a file contains a PNG format image.

    Args:
      filename: string, path of the image file.

    Returns:
      boolean indicating if the image is a PNG.
    """"""
    return filename.endswith('.png')


def _process_image(filename, coder):
    """"""Process a single image file.

    Args:
      filename: string, path to an image file e.g., '/path/to/example.JPG'.
      coder: instance of ImageCoder to provide TensorFlow image coding utils.
    Returns:
      image_buffer: string, JPEG encoding of RGB image.
      height: integer, image height in pixels.
      width: integer, image width in pixels.
    """"""
    # Read the image file.
    with tf.gfile.FastGFile(filename, 'rb') as f:
        image_data = f.read()

    # Convert any PNG to JPEG's for consistency.
    if _is_png(filename):
        image_data = coder.png_to_jpeg(image_data)

    # Decode the RGB JPEG
    image = coder.decode_jpeg(image_data)
    # Check that image converted to RGB
    assert len(image.shape) == 3
    height = image.shape[0]
    width = image.shape[1]
    assert image.shape[2] == 3

    return image_data, height, width


def _process_image_files_batch(coder, thread_index, ranges, name, filenames,
                               texts, labels, num_shards):
    """"""Processes and saves list of images as TFRecord in 1 thread.

    Args:
      coder: instance of ImageCoder to provide TensorFlow image coding utils.
      thread_index: integer, unique batch to run index is within [0, len(ranges)).
      ranges: list of pairs of integers specifying ranges of each batches to
        analyze in parallel.
      name: string, unique identifier specifying the data set
      filenames: list of strings; each string is a path to an image file
      texts: list of strings; each string is human readable, e.g. 'dog'
      labels: list of integer; each integer identifies the ground truth
      num_shards: integer number of shards for this data set.
    """"""
    # Each thread produces N shards where N = int(num_shards / num_threads).
    # For instance, if num_shards = 128, and the num_threads = 2, then the first
    # thread would produce shards [0, 64).
    num_threads = len(ranges)
    assert not num_shards % num_threads
    num_shards_per_batch = int(num_shards / num_threads)

    shard_ranges = np.linspace(ranges[thread_index][0],
                               ranges[thread_index][1],
                               num_shards_per_batch + 1).astype(int)
    num_files_in_thread = ranges[thread_index][1] - ranges[thread_index][0]

    counter = 0
    for s in range(num_shards_per_batch):
        # Generate a sharded version of the file name, e.g. 'train-00002-of-00010'
        shard = thread_index * num_shards_per_batch + s
        output_filename = '%s-%.5d-of-%.5d' % (name, shard, num_shards)
        output_file = os.path.join(FLAGS.output_directory, output_filename)
        writer = tf.python_io.TFRecordWriter(output_file)

        shard_counter = 0
        files_in_shard = np.arange(shard_ranges[s], shard_ranges[s + 1], dtype=int)
        for i in files_in_shard:
            filename = filenames[i]
            label = labels[i]
            text = texts[i]

            try:
                image_buffer, height, width = _process_image(filename, coder)
            except Exception as e:
                print(e)
                print('SKIPPED: Unexpected error while decoding %s.' % filename)
                continue

            example = _convert_to_example(filename, image_buffer, label,
                                          text, height, width)
            writer.write(example.SerializeToString())
            shard_counter += 1
            counter += 1

            if not counter % 1000:
                print('%s [thread %d]: Processed %d of %d images in thread batch.' %
                      (datetime.now(), thread_index, counter, num_files_in_thread))
                sys.stdout.flush()

        writer.close()
        print('%s [thread %d]: Wrote %d images to %s' %
              (datetime.now(), thread_index, shard_counter, output_file))
        sys.stdout.flush()
        shard_counter = 0
    print('%s [thread %d]: Wrote %d images to %d shards.' %
          (datetime.now(), thread_index, counter, num_files_in_thread))
    sys.stdout.flush()


def _process_image_files(name, filenames, texts, labels, num_shards):
    """"""Process and save list of images as TFRecord of Example protos.

    Args:
      name: string, unique identifier specifying the data set
      filenames: list of strings; each string is a path to an image file
      texts: list of strings; each string is human readable, e.g. 'dog'
      labels: list of integer; each integer identifies the ground truth
      num_shards: integer number of shards for this data set.
    """"""
    assert len(filenames) == len(texts)
    assert len(filenames) == len(labels)

    # Break all images into batches with a [ranges[i][0], ranges[i][1]].
    spacing = np.linspace(0, len(filenames), FLAGS.num_threads + 1).astype(np.int)
    ranges = []
    for i in range(len(spacing) - 1):
        ranges.append([spacing[i], spacing[i + 1]])

    # Launch a thread for each batch.
    print('Launching %d threads for spacings: %s' % (FLAGS.num_threads, ranges))
    sys.stdout.flush()

    # Create a mechanism for monitoring when all threads are finished.
    coord = tf.train.Coordinator()

    # Create a generic TensorFlow-based utility for converting all image codings.
    coder = ImageCoder()

    threads = []
    for thread_index in range(len(ranges)):
        args = (coder, thread_index, ranges, name, filenames,
                texts, labels, num_shards)
        t = threading.Thread(target=_process_image_files_batch, args=args)
        t.start()
        threads.append(t)

    # Wait for all the threads to terminate.
    coord.join(threads)
    print('%s: Finished writing all %d images in data set.' %
          (datetime.now(), len(filenames)))
    sys.stdout.flush()


def _find_image_files(data_dir, labels_file):
    """"""Build a list of all images files and labels in the data set.

    Args:
      data_dir: string, path to the root directory of images.

        Assumes that the image data set resides in JPEG files located in
        the following directory structure.

          data_dir/dog/another-image.JPEG
          data_dir/dog/my-image.jpg

        where 'dog' is the label associated with these images.

      labels_file: string, path to the labels file.

        The list of valid labels are held in this file. Assumes that the file
        contains entries as such:
          dog
          cat
          flower
        where each line corresponds to a label. We map each label contained in
        the file to an integer starting with the integer 0 corresponding to the
        label contained in the first line.

    Returns:
      filenames: list of strings; each string is a path to an image file.
      texts: list of strings; each string is the class, e.g. 'dog'
      labels: list of integer; each integer identifies the ground truth.
    """"""
    print('Determining list of input files and labels from %s.' % data_dir)
    unique_labels = [l.strip() for l in tf.gfile.FastGFile(
        labels_file, 'r').readlines()]

    labels = []
    filenames = []
    texts = []

    # Leave label index 0 empty as a background class.
    label_index = 1

    # Construct the list of JPEG files and labels.
    for text in unique_labels:
        jpeg_file_path = '%s/%s/*' % (data_dir, text)
        matching_files = tf.gfile.Glob(jpeg_file_path)

        labels.extend([label_index] * len(matching_files))
        texts.extend([text] * len(matching_files))
        filenames.extend(matching_files)

        if not label_index % 100:
            print('Finished finding files in %d of %d classes.' % (
                label_index, len(labels)))
        label_index += 1

    # Shuffle the ordering of all image files in order to guarantee
    # random ordering of the images with respect to label in the
    # saved TFRecord files. Make the randomization repeatable.
    shuffled_index = list(range(len(filenames)))
    random.seed(12345)
    random.shuffle(shuffled_index)

    filenames = [filenames[i] for i in shuffled_index]
    texts = [texts[i] for i in shuffled_index]
    labels = [labels[i] for i in shuffled_index]

    print('Found %d JPEG files across %d labels inside %s.' %
          (len(filenames), len(unique_labels), data_dir))
    return filenames, texts, labels


def _process_dataset(name, directory, num_shards, labels_file):
    """"""Process a complete data set and save it as a TFRecord.
    Args:
      name: string, unique identifier specifying the data set.
      directory: string, root path to the data set.
      num_shards: integer number of shards for this data set.
      labels_file: string, path to the labels file.
    """"""
    filenames, texts, labels = _find_image_files(directory, labels_file)
    _process_image_files(name, filenames, texts, labels, num_shards)


def main(unused_argv):
    assert not FLAGS.train_shards % FLAGS.num_threads, (
        'Please make the FLAGS.num_threads commensurate with FLAGS.train_shards')
    assert not FLAGS.test_shards % FLAGS.num_threads, (
        'Please make the FLAGS.num_threads commensurate with '
        'FLAGS.test_shards')
    print('Saving results to %s' % FLAGS.output_directory)

    # Run it!
    _process_dataset('test', FLAGS.test_directory,
                     FLAGS.test_shards, FLAGS.labels_file)
    _process_dataset('train', FLAGS.train_directory,
                     FLAGS.train_shards, FLAGS.labels_file)


if __name__ == '__main__':
    tf.app.run()
-----------------------------------------------------------------END---------------------------------------------
 

"
26298,ImportError: DLL load failed: The specified module could not be found.,"I get the following error message:

> 'ImportError: DLL load failed: The specified module could not be found.'.

when I run the following code:

> python -c 'import tensorflow as tf; print(tf.__version__)'

Please check this screenshoot to see the list of my environment variables and the error message itself.
https://www.screencast.com/t/PsEXiaM0S

I'm running Windows 10 PRO and I'm using Geforce RTX 2070 and TensorFlow-GPU 1.13.1

I have installed the following:

1. Python 3.6.7
2. Microsoft Visual C++ 2015 Redistributable (x64) - 14.0.24215 (could not be installed - legacy now)
2. Microsoft Visual C++ 2017 Redistributable (x64) - 14.16.27027
3. Microsoft Visual C++ Build Tools - 15.9.28307.423
4. msys2-x86_64-20180531.exe
5. Cuda_10.1.105_win10_network
6. Cudnn 7.4.1.5 or 7.5 (I tried both)
7. Nvidia driver 419.17-desktop-win10-64bit-international-whql.exe
"
26297,Tensorflow lite gpu delegate inference using opengl and SSBO in android,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes, modified inference code from tflite gpu delegate android sample with additional code from https://www.tensorflow.org/lite/performance/gpu_advanced#android_2.
- OS Platform and Distribution : Android 8.0.0
- Mobile device: OnePlus 3
- TensorFlow version: 12.0

**Describe the current behavior**
The tensorflow lite gpu delegate documentation has provided a sample code for running the tflite inference efficiently on android, avoiding CPU_GPU memory copying with the help of opengl and SSBO in a egl context. However, this method does not seem to give any performance gains; rather it degraded the inference performance in terms of speed.The documentation mentions a method - 'interpreter.runInference(null, outputArray)' for running the inference in this case.Is this method same as the basic run method i.e interpreter.run(inputTensor, outputTensor). (There seems to be no method in the current api called 'interpreter.runInference').Is the method suggested currently supported in the experimental gpu delegate api (i.e accessing input image from opengl ssbo directly for running the inference)?How can we ensure that the model takes the input from this SSBO in GPU memory?

** Expected behaviour**
The tflite inference using opengl ssbo should be faster than the basic gpu delegate inference, where data is copied every-time from cpu to gpu.

**Other info / logs**
We measured the time for the 'tflite.run' method in android studio.The input was in the recommended ByteBuffer format.

Error: Cannot resolve method runInference(null, ?)"
26296,Excpeption in using Tensorflow 1.12 from java on windows 10,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
jar
- TensorFlow version (use command below):
1.12.0
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

I am trying to use tensorflow from java program on windows 10. I have followed all the instruction from here https://www.tensorflow.org/install/lang_java carefully. I am getting following exception:

Mar 03, 2019 5:45:53 PM org.openqa.selenium.remote.ProtocolHandshake createSession
INFO: Detected dialect: OSS
Exception in thread ""Thread-4"" java.lang.UnsatisfiedLinkError: C:\Users\olelo\AppData\Local\Temp\tensorflow_native_libraries-1551615368173-0\tensorflow_jni.dll: A dynamic link library (DLL) initialization routine failed
	at java.lang.ClassLoader$NativeLibrary.load(Native Method)
	at java.lang.ClassLoader.loadLibrary0(Unknown Source)
	at java.lang.ClassLoader.loadLibrary(Unknown Source)
	at java.lang.Runtime.load0(Unknown Source)
	at java.lang.System.load(Unknown Source)
	at org.tensorflow.NativeLibrary.load(NativeLibrary.java:101)
	at org.tensorflow.TensorFlow.init(TensorFlow.java:66)
	at org.tensorflow.TensorFlow.<clinit>(TensorFlow.java:70)
	at org.tensorflow.Graph.<clinit>(Graph.java:361)

	



**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26295,AttributeError: module 'tensorflow' has no attribute 'uint32',"import tensorflow as tf
tf.unit32

it works fine with the tf.unit16 and tf.unit8 but it gives error while accessing tf.unit32
I tried reinstalling tensorflow but it wont work.
I have installed tensorflow using anaconda . then i run the code and i have this problem:

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-16-0a81b7b43fe1> in <module>
----> 1 tf.uint32

AttributeError: module 'tensorflow' has no attribute 'uint32'
----------------------------------------------------------------------------

i have also tried ' pip install --upgrade --force-reinstall tensorflow ' as per the answer in
https://stackoverflow.com/questions/51654346/attributeerror-module-tensorflow-has-no-attribute-float32
but it still wont work "
26294,[TF 2.0] Conversion script fails to parse IPython functions.,"The [`tf_upgrade_v2`](https://github.com/tensorflow/docs/blob/master/site/en/r2/guide/upgrade.md) tool fails to parse commands with common IPython functions (for example, `!pip install tf-nightly` and `%matplotlib inline`).

```
ERROR: Failed to parse.
Traceback (most recent call last):
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/tools/compatibility/ast_edits.py"", line 510, in update_string_pasta
    t = pasta.parse(text)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/__init__.py"", line 23, in parse
    t = ast_utils.parse(src)
  File ""/usr/local/lib/python3.6/dist-packages/pasta/base/ast_utils.py"", line 56, in parse
    tree = ast.parse(sanitize_source(src))
  File ""/usr/lib/python3.6/ast.py"", line 35, in parse
    return compile(source, filename, mode, PyCF_ONLY_AST)
  File ""<unknown>"", line 37
    !pip install tf-nightly
    ^
SyntaxError: invalid syntax
```"
26293,"In a @tf.function, after `for i in tf.range(10)`, i is still undefined","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf.version.VERSION: 2.0.0-dev20190302
tf.version.GIT_VERSION: v1.12.0-9439-g837e5feed2
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When using a `for` loop in a @tf.function (with autograph), such as `for i in tf.range(10)`, `i` is undefined after the loop ends, unless it is defined in the global scope, in which case it takes this value.

**Describe the expected behavior**
I expect the same behavior as in regular Python: `i` should be the last value that was run in the `for` loop.

**Code to reproduce the issue**

```python
import tensorflow as tf

@tf.function
def loop():
    for i in tf.range(10):
        pass
    return i

loop() # NameError: name 'i' is not defined (see full stacktrace below)
```

If I define `i` outside of the function, then autograph uses that:

```python
import tensorflow as tf

i = 1234

@tf.function
def loop():
    for i in tf.range(10):
        pass
    return i

print(loop().numpy()) # prints 1234, which is *incorrect*
```

The correct behavior should be the same as without decorating with @tf.function:

```python
import tensorflow as tf

i = 1234

def loop():
    for i in tf.range(10):
        pass
    return i

print(loop().numpy()) # prints 9, which is correct.
```

**Other info / logs**
Here is the stacktrace:

```
NameError                                 Traceback (most recent call last)
<ipython-input-1-2ae01b93772c> in <module>
      7     return i
      8
----> 9 loop() # NameError: name 'i' is not defined (see full stacktrace below)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    424     # This is the first call of __call__, so we have to initialize.
    425     initializer_map = {}
--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
    427     if self._created_variables:
    428       try:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    368     self._concrete_stateful_fn = (
    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 370             *args, **kwds))
    371
    372     def invalid_creator_scope(*unused_args, **unused_kwds):

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1311     if self._input_signature:
   1312       args, kwargs = None, None
-> 1313     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1314     return graph_function
   1315

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1578           or call_context_key not in self._function_cache.missed):
   1579         self._function_cache.missed.add(call_context_key)
-> 1580         graph_function = self._create_graph_function(args, kwargs)
   1581         self._function_cache.primary[cache_key] = graph_function
   1582         return graph_function, args, kwargs

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1510             arg_names=arg_names,
   1511             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1512             capture_by_value=self._capture_by_value),
   1513         self._function_attributes)
   1514

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    692                                           converted_func)
    693
--> 694       func_outputs = python_func(*func_args, **func_kwargs)
    695
    696       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    684                   optional_features=autograph_options,
    685                   force_conversion=True,
--> 686               ), args, kwargs)
    687
    688         # Wrapping around a decorator allows checks like tf_inspect.getargspec

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    390     return _call_unconverted(f, args, kwargs)
    391
--> 392   result = converted_f(*effective_args, **kwargs)
    393
    394   # The converted function's closure is simply inserted into the function's

/var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gn/T/tmpk_ywisd3.py in tf__loop()
     10   ag__.for_stmt(ag__.converted_call('range', tf, ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (10,), {}), None, loop_body, ())
     11   do_return = True
---> 12   retval_ = i
     13   return retval_
     14

NameError: name 'i' is not defined
```

"
26292,can not find tensor flow/contrib/lite/java/demo this demo,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


I want to download tensor flow code to install TFLite  in my android phone. But in tensorflow directory, I don not find tensorflow/contirbute/lite this directory. 
code : https://github.com/tensorflow/tensorflow, branch is master
And I hope you can provide install_docs to help me install and run model in Android 
Thanks very much"
26290,Failed to pip install tensorflow-gpu ,"
```
# sudo pip install -U tensorflow-gpu --user
Collecting tensorflow-gpu
  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)
    99% || 345.2MB 53.0MB/s eta 0:00:01Exception:
Traceback (most recent call last):
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/cli/base_command.py"", line 179, in main
    status = self.run(options, args)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/commands/install.py"", line 315, in run
    resolver.resolve(requirement_set)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/resolve.py"", line 131, in resolve
    self._resolve_one(requirement_set, req)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/resolve.py"", line 294, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/resolve.py"", line 242, in _get_abstract_dist_for
    self.require_hashes
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/operations/prepare.py"", line 334, in prepare_linked_requirement
    progress_bar=self.progress_bar
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/download.py"", line 878, in unpack_url
    progress_bar=progress_bar
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/download.py"", line 702, in unpack_http_url
    progress_bar)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/download.py"", line 946, in _download_http_url
    _download_url(resp, link, content_file, hashes, progress_bar)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/download.py"", line 639, in _download_url
    hashes.check_against_chunks(downloaded_chunks)
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/utils/hashes.py"", line 62, in check_against_chunks
    for chunk in chunks:
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/download.py"", line 607, in written_chunks
    for chunk in chunks:
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/utils/ui.py"", line 159, in iter
    for x in it:
  File ""/root/.local/lib/python3.6/site-packages/pip/_internal/download.py"", line 596, in resp_read
    decode_content=False):
  File ""/root/.local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 494, in stream
    data = self.read(amt=amt, decode_content=decode_content)
  File ""/root/.local/lib/python3.6/site-packages/pip/_vendor/urllib3/response.py"", line 442, in read
    data = self._fp.read(amt)
  File ""/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py"", line 65, in read
    self._close()
  File ""/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/filewrapper.py"", line 52, in _close
    self.__callback(self.__buf.getvalue())
  File ""/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/controller.py"", line 300, in cache_response
    cache_url, self.serializer.dumps(request, response, body=body)
  File ""/root/.local/lib/python3.6/site-packages/pip/_vendor/cachecontrol/serialize.py"", line 72, in dumps
    return b"","".join([b""cc=4"", msgpack.dumps(data, use_bin_type=True)])
MemoryError
```"
26289,Compatibility with Cuda 10.1? ,"Just wanna compile tensorflow 1.12 against Cuda 10.1 ?

```
  tensorflow git:(master) bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: Rule 'build_bazel_rules_swift' modified arguments {""commit"": ""001736d056d7eae20f1f4da41bc9e6f036857296"", ""shallow_since"": ""1547844730 -0800""} and dropped [""tag""]
DEBUG: ~/.cache/bazel/_bazel_user/15086820fc7a6f1383d8c38c62220208/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in ....../tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""....../third_party/gpus/cuda_configure.bzl"", line 1501
                _create_local_cuda_repository(repository_ctx)
        File ""....../third_party/gpus/cuda_configure.bzl"", line 1266, in _create_local_cuda_repository
                _find_libs(repository_ctx, cuda_config)
        File ""....../third_party/gpus/cuda_configure.bzl"", line 859, in _find_libs
                _find_cuda_lib(""cublas"", repository_ctx, cpu_value, c..., ...)
        File ""....../third_party/gpus/cuda_configure.bzl"", line 773, in _find_cuda_lib
                find_lib(repository_ctx, [(""%s/%s%s"" % (bas...], ...)))
        File ""....../third_party/gpus/cuda_configure.bzl"", line 750, in find_lib
                auto_configure_fail((""No library found under: "" + "",...)))
        File ""....../third_party/gpus/cuda_configure.bzl"", line 341, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: No library found under: /usr/local/cuda-10.1/lib64/libcublas.so.10.1, /usr/local/cuda-10.1/lib64/stubs/libcublas.so.10.1, /usr/local/cuda-10.1/lib/powerpc64le-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x86_64-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x64/libcublas.so.10.1, /usr/local/cuda-10.1/lib/libcublas.so.10.1, /usr/local/cuda-10.1/libcublas.so.10.1
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': in ....../tensorflow/tensorflow.bzl: Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""....../third_party/gpus/cuda_configure.bzl"", line 1501
                _create_local_cuda_repository(repository_ctx)
        File ""....../third_party/gpus/cuda_configure.bzl"", line 1266, in _create_local_cuda_repository
                _find_libs(repository_ctx, cuda_config)
        File ""....../third_party/gpus/cuda_configure.bzl"", line 859, in _find_libs
                _find_cuda_lib(""cublas"", repository_ctx, cpu_value, c..., ...)
        File ""....../third_party/gpus/cuda_configure.bzl"", line 773, in _find_cuda_lib
                find_lib(repository_ctx, [(""%s/%s%s"" % (bas...], ...)))
        File ""....../third_party/gpus/cuda_configure.bzl"", line 750, in find_lib
                auto_configure_fail((""No library found under: "" + "",...)))
        File ""....../third_party/gpus/cuda_configure.bzl"", line 341, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: No library found under: /usr/local/cuda-10.1/lib64/libcublas.so.10.1, /usr/local/cuda-10.1/lib64/stubs/libcublas.so.10.1, /usr/local/cuda-10.1/lib/powerpc64le-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x86_64-linux-gnu/libcublas.so.10.1, /usr/local/cuda-10.1/lib/x64/libcublas.so.10.1, /usr/local/cuda-10.1/lib/libcublas.so.10.1, /usr/local/cuda-10.1/libcublas.so.10.1
INFO: Elapsed time: 10.828s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package
    Fetching @local_config_cuda; fetching
```


Any suggestions?"
26288,Remove all TF Logging ,"
I wanted to exclude all of the tensorflow logs in output. I searched and found that one or both of these setting should work:

    os.environ['TF_CPP_MIN_LOG_LEVEL']='3'
    tf.logging.set_verbosity(tf.logging.ERROR)

Though, I am still getting network loading logs, e.g.:

    2019-02-28 23:51:13,520:INFO::Restoring parameters from ./pre_model/classic/brain4/network--6009999

I was wondering why the `INFO` logs are still there. I also searched a lot about it and read the previous issues, but neither had a certain solution for this problem. 

- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian 8.7
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: 1.10.1
- **Python version**: 2.7
- **Bazel version (if compiling from source)**: NA
- **GCC/Compiler version (if compiling from source)**: NA
- **CUDA/cuDNN version**: 9.0
- **GPU model and memory**: K80 12GB
- **Exact command to reproduce**: 
```
checkpoint = tf.train.get_checkpoint_state(os.path.join(directory, saved_model_address))
tf.train.Saver.restore(session, checkpoint.model_checkpoint_path)

```
in which `saved_model_address` is the address of the saved model, and session is an instance of  tf.Session() which the model is created in. 
"
26284,Rename tf.nn.batch_normalization,"**System information**
- TensorFlow version: 1.13
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization


**Describe the documentation issue**
tf.nn.batch_normalization can actually be used to implement layer_normalization or group_normalization as well. (https://github.com/tensorflow/addons/pull/14)  Therefore the name of the function is quite confusing since the ""batch normalization"" is more or less a special case of this function (when you feed the right mean and variance tensor). 

I think something like ""normalization()"" could work as well and would be less confusing.
"
26283,Disabling tf.cond name scope,"**System information**
- TensorFlow version: 2.0
- Are you willing to contribute it: No

**Describe the feature and the current behavior/state.**

With all variables name being resolved via `name_scope` in TensorFlow 2.0, mixing tf.cond and variable creation can lead to unexpected or inconvenient results. Please consider this class that implements a conditional logic for inference but not training (here, reading a cached value):

```python
class MyModel(tf.keras.layers.Layer):

    def __init__(self):
        super(MyModel, self).__init__()
        self.linear = tf.keras.layers.Dense(10)

    def call(self, x, cache=None, training=None):
        if training or cache is None:
            return self.linear(x)
        else:
            return tf.cond(
                tf.equal(tf.shape(cache)[0], 0),
                true_fn=lambda: self.linear(x),
                false_fn=lambda: cache)
```

This creates incompatible variable names in inference.

**Training:**

```python
x = tf.zeros([3, 5])
model = MyModel()
_ = model(x, training=True)
print(model.trainable_variables)
```

```text
[<tf.Variable 'my_model/dense/kernel:0' shape=(5, 10) dtype=float32>, <tf.Variable 'my_model/dense/bias:0' shape=(10,) dtype=float32>]
```

**Inference:**

```python
x = tf.zeros([3, 5])
cache = tf.ones([0, 5])
model = MyModel()
_ = model(x, cache=cache, training=False)
print(model.trainable_variables)
```

```text
[<tf.Variable 'my_model/cond/dense/kernel:0' shape=(5, 10) dtype=float32>, <tf.Variable 'my_model/cond/dense/bias:0' shape=(10,) dtype=float32>]
```

The developer should then fiddle with the name or build the layer beforehand but I believe this code should just work. In V1, this could be fixed by setting the `tf.cond` name to:

```python
tf.get_default_graph().get_name_scope() + ""/""
```

but there is no alternative in V2.

**Will this change the current api? How?**

There should be a way (e.g. argument) to disable the name scope introduced by `tf.cond`. Making this the default behavior is maybe unreasonable to ask.

**Who will benefit with this feature?**

People applying conditional logic on layers during inference but not during training.

**Any Other info.**

There are 2 real world examples in the project I maintain, [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf):

* When dynamically decoding from a Transformer model, the decoder can cache the projection of the encoder output: https://github.com/OpenNMT/OpenNMT-tf/blob/v1.21.0/opennmt/layers/transformer.py#L466-L476. To mitigate the scope issue, I manually build the layer before the `tf.cond`.
* When generating from a language model, we should either get the initial decoder state or the context state (if a context exists): https://github.com/OpenNMT/OpenNMT-tf/blob/v1.21.0/opennmt/models/language_model.py#L83-L87. In this case, I know that `self.name` is the top level name scope and use it directly. "
26282,Changing the default initializer globally for tf.keras.layers,"**System information**
- TensorFlow version: 2.0
- Are you willing to contribute it: No

**Describe the feature and the current behavior/state.**

For V1 `tf.layers`, one could set a default initializer to the global variable scope:

```python
with tf.variable_scope(name, initializer=...):
  ...
```

In V2 `tf.keras.layers`, the default initializer is hardcoded to `initializers.glorot_uniform()`. I currently see 2 imperfect ways to change the default initializer globally:

* change the initializer to every layers that are created **but** this is not convenient for large models
* implement a manual assignation loop **but** AFAIK there is no clean way to detect if the variable initializer is the default one or an initializer that was explicitly passed by the developer (typically a constant initialization).

**Will this change the current api? How?**

I think the most user friendly approach to this is to add an endpoint to globally change the default initializer: each layer created after this statement will use this initializer by default.

**Who will benefit with this feature?**

Anyone who wants to easily apply a global initialization strategy while still ensuring that specific initializer can be set locally.

---

Any recommendation to achieve this with the current code is of course appreciated."
26280,Bug in TensorflowLite Android Example,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 5T
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v1.1.0-rc0-61-g1ec6ed5 1.1.0
- Python version: 3.6.5
- Bazel version (if compiling from source): - 
- GCC/Compiler version (if compiling from source): - 
- CUDA/cuDNN version: - 
- GPU model and memory: - 


Currently, the tensorflow lite example uses dp for textsize attribute for android example. As per definition, this is a bug. The dp has to be changed to sp, as per Android standards.
"
26278,tf.image.crop_and_resize() - weird alignment behavior?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):n/a
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

This is a follow up on https://github.com/tensorflow/tensorflow/issues/6720#issuecomment-468830039 about `tf.image.crop_and_resize`. (cc @martinwicke )

Suppose I have an image that looks like this:
```
[[ 0.  1.  2.  3.  4.]
 [ 5.  6.  7.  8.  9.]
 [10. 11. 12. 13. 14.]
 [15. 16. 17. 18. 19.]
 [20. 21. 22. 23. 24.]]
```
I wanted to crop the 2x2 patch that contains `[[6, 7], [11, 12]]`, and upsample it to 4x4. I expect to get the following outputs:
```
[[ 4.5  5.   5.5  6. ]
 [ 7.   7.5  8.   8.5]
 [ 9.5 10.  10.5 11. ]
 [12.  12.5 13.  13.5]]
```
I think this is a reasonable expectation. The above output, is also what I got if I do ""resize_and_crop"" instead of `tf.image.crop_and_resize`, __after the fix__ https://github.com/tensorflow/tensorflow/commit/371c96df55a7b23eb8d8496fb477b473fd137fcc yesterday that addressed the alignment issues in resize op.
```python
import tensorflow as tf
import numpy as np
from tensorflow.python.ops.image_ops_impl import resize_images_v2
arr = np.arange(25).astype('float32').reshape(5, 5)
input4D = tf.reshape(arr, [1, 5, 5, 1])
resize = resize_images_v2(input4D, [10, 10], method='bilinear')[0,:,:,0]   # resize
print(resize[2:6,2:6])  # crop
# print expected output
```
See a Colab proof in https://colab.research.google.com/drive/1ojDErHyG_4v3vwdi3xYwpdtyxShYM9a6#scrollTo=-T1zLhI5uumV

OK, what is the correct ""boxes"" I should provide for `crop_and_resize`, in order to get the above output?
Here is what the document says:

> boxes: A Tensor of type float32. A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values.

It turns out, that the correct ""boxes"" I should use, is: `[3/16, 3/16, 9/16, 9/16]`. If you cannot tell why it is 3/16 and 9/16 from the above documentation, you and I are on the same page:
```python
import tensorflow as tf
import numpy as np
import tensorflow.contrib.eager as tfe
tfe.enable_eager_execution()

# want to crop 2x2 out of a 5x5 image, and resize to 4x4
image = np.arange(25).astype('float32').reshape(5, 5)
target = 4
print(tf.image.crop_and_resize(
    image[None, :, :, None],
    np.asarray([[3/16,3/16,9/16,9/16]]), [0], [target, target])[0][:, :, 0])
# print expected output
```

The crop_and_resize function has weird alignment issues like those fixed in #6720 . It's less of a problem than #6720 because at least we can provide some box coordinates to make it work as expected, and you can say it's just how this function is defined. 
There is actually [a formula](https://github.com/tensorpack/tensorpack/blob/1cdd2e9efb6ffe3066a010e677ef9e76547faaa3/examples/FasterRCNN/modeling/model_box.py#L104-L135) that I use in my code to compute the coordinates in order to use this function. 

But I do hope this function can have a better-defined behavior and fit reasonable expectation. In my experiments this ill-posed behavior actually hurt my models (which I believe also hurt other models like TF's object detection)."
26276,Good first issue tag?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: N/A
- Doc Link: https://github.com/tensorflow/tensorflow/blob/master/CONTRIBUTING.md


**Describe the documentation issue**

Is there a 'good first issue' tag similar to the one used in pandas-dev?
https://github.com/pandas-dev/pandas#contributing-to-pandas-
I see there is a contributions welcome tag 
https://github.com/tensorflow/tensorflow/labels/stat%3Acontributions%20welcome

Perhaps I could add a sentence saying that:
If you want to contribute but you're not sure where to start, take a look at the issues with the ""contributions welcome"" label. These are issues that we believe are particularly well suited for outside contributions *and first time contributors*, often because we probably won't get to them right now. If you decide to start on an issue, leave a comment so that other people know that you're working on it. If you want to help out, but not alone, use the issue comment thread to coordinate.

I'm sure that could be worded better.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
26275,`tensorflow.python.client.device_lib.list_local_devices()`  crushes when none of GPU has enough free memory,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.10

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
Binary

- TensorFlow version (use command below):
1.13.1

- Python version:
3.6.7

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
CUDA10.0/cuDNN _7.5.0.56

- GPU model and memory:
0: GTX 1060 TI 6GB
1: GTX 1050 Ti 4GB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.13.1-0-g6612da8951' 1.13.1

**Describe the current behavior**
If I run list_local_devices() in order to get the number of GPUs available while almost all of memory is already allocated on all the GPUs, the process crashes due to CUDA_ERROR_OUT_OF_MEMORY.
There's no problem at all if at least one GPU has substantial amount of free memory.

Error Message:

> 2019-03-02 12:26:38.852155: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
> 2019-03-02 12:26:38.989613: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:0: failed initializing StreamExecutor for CUDA device ordinal 0: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 6370295808
> 2019-03-02 12:26:38.999115: W tensorflow/compiler/xla/service/platform_util.cc:240] unable to create StreamExecutor for CUDA:1: failed initializing StreamExecutor for CUDA device ordinal 1: Internal: failed call to cuDevicePrimaryCtxRetain: CUDA_ERROR_OUT_OF_MEMORY: out of memory; total memory reported: 4236312576
> 2019-03-02 12:26:38.999399: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Internal: no supported devices found for platform CUDA
> 


**Describe the expected behavior**
If none of GPUs is available, it could just return the empty list, not crushing itself.
I wrote my code for training a model in the way it fully makes use of all the available resources, but I have another code for doing various stuff and I want it to run on CPU when GPU is not available.
I expected get_available_gpus() to return the empty list when none of them is available.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```python
from tensorflow.python.client import device_lib

def get_available_gpus():
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']

get_available_gpus()
```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

the result of `nvidia-smi`

```
Sat Mar  2 12:41:52 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 106...  On   | 00000000:01:00.0  On |                  N/A |
| 40%   55C    P2    41W / 120W |   6033MiB /  6075MiB |     51%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 105...  On   | 00000000:03:00.0 Off |                  N/A |
| 30%   41C    P0    N/A /  75W |   3997MiB /  4040MiB |     59%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1960      G   /usr/lib/xorg/Xorg                            27MiB |
|    0      2557      G   /usr/bin/gnome-shell                          60MiB |
|    0     13385      C   python                                      5933MiB |
|    1     13385      C   python                                      3985MiB |
+-----------------------------------------------------------------------------+

```

Here is my current workaround
```python
def get_available_gpus(workaround=True):
    if workaround:
        ret = os.spawnl(os.P_WAIT, python_path, python_path, 'metax/get_num_gpus.py')
        if ret < 0:
            # detecting aborted process
            return []
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']
```
For now, I'm creating a child process to see if it crushes."
26274,Custom model's `build()` method is not called automatically,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf.version.VERSION: '2.0.0-dev20190301'
tf.version.GIT_VERSION: 'v1.12.0-9345-g4eeb2714f4'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When creating a custom model with a `build()` method (e.g., if one of the model's layers has a size that depends on the input shape, such as a reconstruction layer), the model cannot be trained unless I explicitly call `build()` with a `tf.TensorShape()`. Moreover, I cannot specify an `input_shape`.

**Describe the expected behavior**
I expect the custom model to be built automatically the first time it is called (e.g., by the `fit()` method).

**Code to reproduce the issue**

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras

X_train = np.random.randn(1000, 8)
y_train = np.random.rand(1000, 1)

class ReconstructingRegressor(keras.models.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.hidden = keras.layers.Dense(30, activation=""elu"")
        self.out = keras.layers.Dense(output_dim)

    def build(self, batch_input_shape):
        n_inputs = batch_input_shape[-1]
        self.reconstruct = keras.layers.Dense(n_inputs)
        super().build(batch_input_shape)

    def call(self, inputs):
        Z = self.hidden(inputs)
        reconstruction = self.reconstruct(Z)
        reconstruction_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
        self.add_loss(0.1 * reconstruction_loss)
        return self.out(Z)

model = ReconstructingRegressor(1)
#model.build(tf.TensorShape([None, 8])) # <= works if I add this line
model.compile(loss=""mse"", optimizer=""nadam"")
history = model.fit(X_train, y_train, epochs=2) # <= AttributeError (see below)
```

**Other info / logs**
Here is the stacktrace:

```
AttributeError                            Traceback (most recent call last)
<ipython-input-1-3cb185747474> in <module>
     27 #model.build(tf.TensorShape([None, 8])) # <= works if I add this line
     28 model.compile(loss=""mse"", optimizer=""nadam"")
---> 29 history = model.fit(X_train, y_train, epochs=2) # <= ERROR!

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    804         steps=steps_per_epoch,
    805         validation_split=validation_split,
--> 806         shuffle=shuffle)
    807
    808     # Prepare validation data.

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2501       else:
   2502         cast_inputs = x_input
-> 2503       self._set_inputs(cast_inputs)
   2504     else:
   2505       y_input = y

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)
   2773             outputs = self.call(inputs, training=training)
   2774           else:
-> 2775             outputs = self.call(inputs)
   2776           # Reset to the previously saved value. If `call()` had `add_metric`
   2777           # or `add_loss`, then `_contains_symbolic_tensors` will have been set

<ipython-input-1-3cb185747474> in call(self, inputs)
     19     def call(self, inputs):
     20         Z = self.hidden(inputs)
---> 21         reconstruction = self.reconstruct(Z)
     22         reconstruction_loss = tf.reduce_mean(tf.square(reconstruction - inputs))
     23         self.add_loss(0.1 * reconstruction_loss)

AttributeError: 'ReconstructingRegressor' object has no attribute 'reconstruct'
```

"
26273,window import bug,"python3.7
tensorflow 1.13.1
Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'
ImportError: numpy.core.multiarray failed to import

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""<frozen importlib._bootstrap>"", line 980, in _find_and_load
SystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set
ImportError: numpy.core._multiarray_umath failed to import
ImportError: numpy.core.umath failed to import
2019-03-02 10:16:10.848675: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr
"
26270,[TF 2.0 API Docs] tf.custom_gradient,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.13.1
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/custom_gradient

I've been having some difficulty using `custom_gradient` recently. Before implementing the op I actually have in mind, I'm trying to make a [simple polynomial op](https://gist.github.com/tsbertalan/b6c02bf6e39116d8446faa0159a011af) with `custom_gradient` in an effort to get my head around the requirements.. The documentation is not at all clear on how summing over batch and output indices should be done, and (until recently) wasn't clear on whether we should return the full *Jacobian* of the operation, or just the vector-Jacobian-product (""VJP"") function. (I did notice some explanation of the latter issue made it in in the 1.12 -> 1.13 update, which was helpful).

I think this problem applies both the derivative WRT the op's input, and WRT to the parameters (called `grad_xs` and `grad_vars` in the documentation). However, when I incorporated the VJP tidbit in my sample code, I got correct results for the former, but not the latter, I believe because the implicit sum over batch was already present in the supplied `grad_ys`, and because the VJP also collapses out the index across `grad_ys`.

So, the lack of clarity about sums (accidentally) only hit me when trying to write the `grad_vars` part. In my [gist linked above](https://gist.github.com/tsbertalan/b6c02bf6e39116d8446faa0159a011af), I had to use a `reduce_sum` when computing $dy/dp$, which, for a polynomial, is the corresponding powers of $x$. (Since $dy/dx$ doesn't depend on $x$, this issue didn't appear when writing the `grad_xs` part.)

In addition to simply saying what you mean in this documentation (the sum of the gradient over examples is not ""the gradient""--only the per-example gradient truly deserves that name! But I suppose that particular fight is a lost cause.), it would be good if there were at least some examples that exercised the `grad_vars` part and the `variables=None` keyword argument."
26268,"Even in eager mode, Keras is passing custom models non-eager tensors in 'fit'","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.14
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): '1.14.1-dev20190301'
- Python version: 3.7

**Describe the current behavior**
When calling `keras.model.fit` on a custom model, it seems the model is passed a graph-mode tensor instead of an eager tenser, even when in eager mode.

**Describe the expected behavior**
If in eager mode, the tensors passed to the `call` method of a custom model should be eager tensors. Otherwise, the advantages of eager mode, like the ability to use native control flow, are lost.

**Code to reproduce the issue**

```python
import tensorflow as tf
tf.enable_v2_behavior()
from tensorflow import keras

class MyModel(keras.Model):
    def call(self, x):
        if x > 0:
            return x + 1
        else:
            return x - 1
            
m = MyModel()
m(tf.constant(0))  # This works, returns -1 as expected
m.compile(loss='mse', optimizer='sgd')
m.fit(tf.constant(0), tf.constant(1)) # This fails
```

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-55-2c7beacc3d4f> in <module>
----> 1 m.fit(tf.constant(0), tf.constant(1))

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    804         steps=steps_per_epoch,
    805         validation_split=validation_split,
--> 806         shuffle=shuffle)
    807 
    808     # Prepare validation data.

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)
   2501       else:
   2502         cast_inputs = x_input
-> 2503       self._set_inputs(cast_inputs)
   2504     else:
   2505       y_input = y

/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    454     self._setattr_tracking = False  # pylint: disable=protected-access
    455     try:
--> 456       result = method(self, *args, **kwargs)
    457     finally:
    458       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in _set_inputs(self, inputs, outputs, training)
   2773             outputs = self.call(inputs, training=training)
   2774           else:
-> 2775             outputs = self.call(inputs)
   2776           # Reset to the previously saved value. If `call()` had `add_metric`
   2777           # or `add_loss`, then `_contains_symbolic_tensors` will have been set

<ipython-input-52-94b7a4f52815> in call(self, x)
      1 class MyModel(keras.Model):
      2     def call(self, x):
----> 3         if x > 0:
      4             return x+1
      5         else:

/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in __bool__(self)
    658       `TypeError`.
    659     """"""
--> 660     raise TypeError(""Using a `tf.Tensor` as a Python `bool` is not allowed. ""
    661                     ""Use `if t is not None:` instead of `if t:` to test if a ""
    662                     ""tensor is defined, and use TensorFlow ops such as ""

TypeError: Using a `tf.Tensor` as a Python `bool` is not allowed. Use `if t is not None:` instead of `if t:` to test if a tensor is defined, and use TensorFlow ops such as tf.cond to execute subgraphs conditioned on the value of a tensor.
```"
26266,AttributeError: module 'pandas' has no attribute 'compat',"Hi,

I am trying to use ScipyOptimizerInterface() in the tensorflow, but it gave the following attribute error

AttributeError: module 'pandas' has no attribute 'compat'.

By going through the discussion threads at Tensorflow github page, I have upgarded ""dask"", downgraded pandas, reinstalled tensorflow and scipy packages. Unfortunately, it is still giving me same AttributeError. Does anyone having similar issues and can help me to resolve it? I can use tensorflow normally for other minimizations algorithms (tested ADAMS) but for scipy's BFGS implementation, I am getting this attribute error.

I am running code on Linux Centos system with python 3.6 and tensorflow 1.12.0. Version for pandas is 0.24.0. I tried to downgrade the pandas to 0.19.2 but it broke other parts of my code which use f2py library.

Any thoughts on how to fix this issue?"
26263,Error when using tf.constant to provide input to Keras model.predict,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux (Google Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7/3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: K80


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
This is a followup to the [issue ](https://github.com/aamini/introtodeeplearning_labs/issues/22#issue-416226096) raised in MIT's TensorFlow labs. When I use a TensorFlow tensor created using `tf.constant` as input to Keras `model.predict` it gives an error. However, when I directly feed in the same tensor to the Keras model it gives output as expected. 

**Describe the expected behavior**
According to the documentation provided, model.predict should also be able to take a TensorFlow tensor as input.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
# Import relevant packages
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

# Define the number of inputs and outputs
n_input_nodes = 2
n_output_nodes = 3

# First define the model 
model = Sequential()

'''TODO: Define a dense (fully connected) layer to compute z'''
dense_layer = Dense(n_output_nodes, input_shape=(n_input_nodes,),activation='sigmoid') # TODO 

# Add the dense layer to the model
model.add(dense_layer)
```
Now when I do prediction using:
```
# Test model with example input
x_input = tf.constant([[1.0,2.]], shape=(1,2))
'''TODO: feed input into the model and predict the output!'''
print(model.predict(x_input)) # TODO
```
I get following error:

> InvalidArgumentError: In[0] is not a matrix. Instead it has shape [2]
> [[{{node MatMul_3}}]] [Op:StatefulPartitionedCall]

When I use directly feed the tensor:
```
# Test model with example input
x_input = tf.constant([[1,2.]], shape=(1,2))
'''TODO: feed input into the model and predict the output!'''
print model(x_input)
```
I get the expected behaviour:

> tf.Tensor([[0.31025296 0.48313126 0.7821198 ]], shape=(1, 3), dtype=float32)

@aamini also had the same behavior. Is this expected in TensorFlow 2.0 or a bug?. If it is expected, please update the documentation."
26262,TF Boosted Trees doesn't consume the tf.Dataset as expected (incorrect checksum for freed object - object was probably modified after being freed),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs/CloudML
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.1
- Python version: 2.7
- CPU training


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

The model doesn't consume the dataset object properly.

My input_fn is:

```
def build_training_input_fn(file_pattern, train=False):
    """"""Creates an input function reading from transformed data.
    Args:
      transformed_examples: Base filename of examples.
    Returns:
      The input function for training or valid.
    """"""

    def parse_record(record):
        transformed_metadata = metadata_io.read_metadata(
            os.path.join(
                'gs://path_to_bucket/transform',
                transform_fn_io.TRANSFORMED_METADATA_DIR))
        transformed_feature_spec = transformed_metadata.schema.as_feature_spec()

        transformed_features = tf.parse_single_example(record, transformed_feature_spec)
        cols_to_remove = {'label_key'}
        transformed_labels = transformed_features.pop('label_key')
        transformed_features = {key: value for (key, value) in transformed_features.items() if
                                key not in cols_to_remove}
        return transformed_features, transformed_labels

    def input_fn(train=train):
        """"""Input function for training and valid.""""""
        files = tf.data.Dataset.list_files(file_pattern=file_pattern)
        dataset = files.apply(
            tf.data.experimental.parallel_interleave(
                lambda filename: tf.data.TFRecordDataset(filename),
                cycle_length=32,
                block_length=1,
                sloppy=True,
            ))
        if train:
            dataset = dataset.repeat(None)

        dataset = dataset.apply(tf.data.experimental.map_and_batch(
            map_func=parse_record, batch_size=64, drop_remainder=False,
            num_parallel_batches=16))

        return dataset

    return input_fn
```

I train the model using:
```
    classifier = tf.estimator.BoostedTreesClassifier()
    input_fn_train = build_training_input_fn(file_pattern=train_directory, train=True)
    classifier.train(input_fn=input_fn_train)
```

**Describe the expected behavior**

The model should consume the dataset provided by the `train_input_fn` and build trees. 

**Code to reproduce the issue**
Unfortunately, I'm unable to share this.

**Other info / logs**
When training on CloudML, the error is: ```The replica master 0 exited with a non-zero status of 11(SIGSEGV).```

Locally, I get:

```
WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 0 vs previous value: 0. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.
python(71585,0x700009986000) malloc: *** error for object 0x7fe0d6f99e00: incorrect checksum for freed object - object was probably modified after being freed.
*** set a breakpoint in malloc_error_break to debug
```

As a side note, if I simply yield the data using the snippet below, I get the `OutOfRangeError` error as expected.
```
    dataset = input_fn_eval()
    next_val = dataset.make_one_shot_iterator().get_next()
    with tf.Session() as sess:
        while True:
            try:
                x, y = sess.run(next_val)
                print(x)
                sleep(0)
                # predictions = classifier.predict(input_fn=(x, y))
                # print([i for i in predictions])
            except tf.errors.OutOfRangeError as e:
                print(e)
                print('Expected Behaiviour')
                break
```
"
26260,Creating an unused Mean metric in a custom model's constructor breaks the model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
MacOSX 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
tf.version.VERSION: '2.0.0-dev20190301'
tf.version.GIT_VERSION: 'v1.12.0-9345-g4eeb2714f4'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When creating a custom model, simply creating a `Mean` metric  in the constructor and setting it as one of the attributes leads to an exception when fitting the model.
Moreover, if I use the metric, it seems to burn the batch size into the model, so it behaves like a stateful model.

**Describe the expected behavior**
Creating a `Mean` instance should be harmless, especially if it is unused. 

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

training_set_size = 32 * 10 # <= works only if self.add_metric() is added and this is a multiple of 32
X = np.random.randn(training_set_size, 8) 
y = np.random.randn(training_set_size, 1)

class MyModel(keras.models.Model):
    def __init__(self, output_dim, **kwargs):
        super().__init__(**kwargs)
        self.out = keras.layers.Dense(output_dim)
        self.my_metric = keras.metrics.Mean()

    def call(self, inputs):
        #self.add_metric(self.my_metric(5.)) # <= Works if you add this line, but model is stateful
        return self.out(inputs)

model = MyModel(1)
model.compile(loss=""mse"", optimizer=""nadam"")
history = model.fit(X, y, epochs=2)
```

**Other info / logs**
Here is the stacktrace:

```
KeyError                                  Traceback (most recent call last)
<ipython-input-1-ca089e94cadc> in <module>
     19 model = MyModel(1)
     20 model.compile(loss=""mse"", optimizer=""nadam"")
---> 21 history = model.fit(X, y, epochs=2)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--> 873           steps_name='steps_per_epoch')
    874
    875   def evaluate(self,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    149
    150   # Get step function and loop type.
--> 151   f = _make_execution_function(model, mode)
    152   use_steps = is_dataset or steps_per_epoch is not None
    153   do_validation = val_inputs is not None

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py in _make_execution_function(model, mode)
    519   if model._distribution_strategy:
    520     return distributed_training_utils._make_execution_function(model, mode)
--> 521   return model._make_execution_function(mode)
    522
    523

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _make_execution_function(self, mode)
   2229   def _make_execution_function(self, mode):
   2230     if mode == ModeKeys.TRAIN:
-> 2231       self._make_fit_function()
   2232       return self._fit_function
   2233     if mode == ModeKeys.TEST:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _make_fit_function(self)
   2172   def _make_fit_function(self):
   2173     metrics_tensors = [
-> 2174         self._all_stateful_metrics_tensors[m] for m in self.metrics_names[1:]
   2175     ]
   2176     self._make_train_function_helper(

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in <listcomp>(.0)
   2172   def _make_fit_function(self):
   2173     metrics_tensors = [
-> 2174         self._all_stateful_metrics_tensors[m] for m in self.metrics_names[1:]
   2175     ]
   2176     self._make_train_function_helper(

KeyError: 'mean'
```

"
26259,Any plan to add SoftClipping and SmoothMax,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):
Yes.
**Describe the feature and the current behavior/state.**
SoftClipping for activation. [https://en.wikipedia.org/wiki/Activation_function]
SmootMax for pooling. [https://en.wikipedia.org/wiki/Smooth_maximum]
**Will this change the current api? How?**
No. This is new feature.
**Who will benefit with this feature?**
Everybody. 
SmoothClipping is smoot function and can not suffer from vanishing/exploding gradients. 
So with this we will have faster learning. https://github.com/tiny-dnn/tiny-dnn/pull/1014.  Only 10 layers achieve 99.35% on MNIST set, Comparing to original 12 and 99.0%
**Any Other info.**
"
26258,Support matrix_inverse in XLA,"It seems MatrixInverse op is not supported in the XLA. 

Indeed, trying to `tf.compile` a graph containing a `tf.matrix_inverse` op yields the following error 

    No registered 'MatrixInverse' OpKernel for XLA_CPU_JIT devices compatible with node 

Can you please add that support?

In the meanwhile, can you suggest a workaround to compile a graph containing a matrix_inverse operation? I thought about re-writing the matrix_inverse in terms of LU decomposition, but LU is not supported neither."
26256,Contrib AdaMax implementation producing NaNs on GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): Tested on 1.12 and 1.13
- Python version: 3.6
- CUDA/cuDNN version: 9
- GPU model and memory: verified on 1080ti and titan v

**Describe the current behavior**

On GPU AdaMax from `tf.contrib.opt.AdaMaxOptimizer` appears to apply NaNs to variables. Seems fine on CPU and bizarrely if any ops are put as a control_dependency to the apply_grads call then everything seems fine.

**Describe the expected behavior**
Not to produce NaNs.

**Code to reproduce the issue**
```
import tensorflow as tf

a = tf.get_variable(""a"", shape=[10000])
b = tf.get_variable(""b"", shape=[10000])
loss = tf.minimum(tf.reduce_mean(a), tf.reduce_mean(b))
sess = tf.Session()

print(""===== NORMALLY ======"")

opt_op= tf.contrib.opt.AdaMaxOptimizer().minimize(loss)
sess.run(tf.global_variables_initializer())
for a in range(10):
    print(sess.run([opt_op, loss])[1])

print(""===== WITH NOOP ======"")

opt = tf.contrib.opt.AdaMaxOptimizer()
grads_and_vars = opt.compute_gradients(loss)
noop = tf.no_op()
    
with tf.control_dependencies([noop]):
    opt_op_fixed = opt.apply_gradients(grads_and_vars)

sess.run(tf.global_variables_initializer())
for a in range(10):
    print(sess.run([opt_op, loss])[1])
```

The above code produces on my machine:
```
===== NORMALLY ======
-3.637023e-05
-0.00037572667
nan
nan
nan
nan
nan
nan
nan
nan
===== WITH NOOP ======
-9.1626454e-05
-0.00018933268
-0.00028703889
-0.00038474516
-0.0004824514
-0.0005801577
-0.0006778639
-0.00077557017
-0.0008732763
-0.00097098254
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26255,"Failed to get device properties, error code: 30","Failed to initialize GPU device #0: unknown error

Os: windows 10
Tensorflow version: 1.13.1
Keras Version: 2.1.6
CUDA version: 10(major)
Cudnn version: 7.5
python 3.6


I got following error:

WARNING:tensorflow:From C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Created base network...
(1, 1)
Tensor(""Shape:0"", shape=(2,), dtype=int32)
Training started...
WARNING:tensorflow:From C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/20
2019-03-01 16:54:42.970057: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library cublas64_100.dll locally
Traceback (most recent call last):
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: CUB reduce errorinvalid configuration argument
	 [[{{node dense_1/Sum}}]]
	 [[{{node loss/add_1}}]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py"", line 370, in <module>
    main(args)
  File ""F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py"", line 333, in main
    callbacks=[checkpointer, tbpointer])  # KERAS 2
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py"", line 2230, in fit_generator
    class_weight=class_weight)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py"", line 1883, in train_on_batch
    outputs = self.train_function(ins)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 2482, in __call__
    **self.session_kwargs)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: CUB reduce errorinvalid configuration argument
	 [[node dense_1/Sum (defined at C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py:1269) ]]
	 [[node loss/add_1 (defined at C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py:848) ]]

Caused by op 'dense_1/Sum', defined at:
  File ""F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py"", line 370, in <module>
    main(args)
  File ""F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py"", line 274, in main
    base_network = create_base_network_signet(input_shape)
  File ""F:/Implementation/Rtip2r_Conference18/siamese/signet_distance/SigNet_stat_measures_more.py"", line 139, in create_base_network_signet
    seq.add(Dense(1024, kernel_regularizer=l2(0.0005), activation=""relu"", kernel_initializer=""glorot_uniform""))
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\models.py"", line 522, in add
    output_tensor = layer(self.outputs[0])
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\topology.py"", line 592, in __call__
    self.build(input_shapes[0])
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\layers\core.py"", line 864, in build
    constraint=self.kernel_constraint)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\legacy\interfaces.py"", line 91, in wrapper
    return func(*args, **kwargs)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\topology.py"", line 418, in add_weight
    self.add_loss(regularizer(weight))
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\regularizers.py"", line 42, in __call__
    regularization += K.sum(self.l2 * K.square(x))
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 1269, in sum
    return tf.reduce_sum(x, axis, keepdims)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1286, in reduce_sum_v1
    return reduce_sum(input_tensor, axis, keepdims, name)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\dispatch.py"", line 180, in wrapper
    return target(*args, **kwargs)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 1334, in reduce_sum
    name=name))
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 10209, in _sum
    name=name)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\util\deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\framework\ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

InternalError (see above for traceback): CUB reduce errorinvalid configuration argument
	 [[node dense_1/Sum (defined at C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py:1269) ]]
	 [[node loss/add_1 (defined at C:\Users\SUS\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\engine\training.py:848) ]]


Process finished with exit code 1


I have tried updating tensorflow to 2.0-preview but that leads to other errors like module ConfigProto not found.

Please help to resolve this issue.
"
26254,cuda 9 or 10,"TF installation via pip is pointing to 1.13.1, requiring CUDA 10, while in the documentation it's mentioned CUDA 9

In order to install via pip for cuda 9 one should
> pip3 install tensorflow-gpu==1.12.0

see
https://github.com/tensorflow/tensorflow/issues/26209"
26253,MPI call makes `//tensorflow:tf_python_api_gen_v1` fail when building tf-1.12.0 or 1.13.1 on Cray system,"**System information**
- Cray XC 50 (both login node and compute nodes)
- Bazel-0.18.1 and 0.21.0
- TensorFlow-1.12.0 and 1.13.1
- Python-3.6.5
- GCC-6.2.0
- CUDA-9.1
- cuDNN-7.1.4



**Describe the problem**
We are encountering the following error when building TensorFlow-1.12.0 or 1.13.1:
```
ERROR: /path/to/my/installation/tensorflow-1.13.1/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 255): bash failed: error executing command 
  (echo of the command that failed)
```
This continues with the echo of the command that failed 
```
/bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/k8-opt/bin/tensorflow/python_api_1_tf_python_api_gen_v1 ...
```
an then at the end there is this
```
Execution platform: @bazel_tools//platforms:host_platform
[Fri Mar  1 09:31:29 2019] [unknown] Fatal error in PMPI_Init_thread: Other MPI error, error stack:
MPIR_Init_thread(537): 
MPID_Init(246).......: channel initialization failed
MPID_Init(638).......:  PMI2 init failed: 1 
aborting job:
Fatal error in PMPI_Init_thread: Other MPI error, error stack:
MPIR_Init_thread(537): 
MPID_Init(246).......: channel initialization failed
MPID_Init(638).......:  PMI2 init failed: 1 
```
One gets this when trying to run an MPI program on the login node (which is the desired behavior in our system). The message appears twice as if something is trying to execute an MPI program with 2 ranks during the installation. I don't manage to find where this is coming from. The error also occurs if building on a compute node. Is there any option in Bazel to avoid this?

This doesn't happen for TensorFlow-1.11.0 or other older versions that we have build.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Our choices on the `configure` phase for tf-1.13.1 (the same we use for other versions we have compiled):
```
Do you wish to build TensorFlow with XLA JIT support? No
Do you wish to build TensorFlow with OpenCL SYCL support? No
Do you wish to build TensorFlow with ROCm support? No
Do you wish to build TensorFlow with CUDA support? Yes
Please specify the CUDA SDK version you want to use: 9.1
Please specify the cuDNN version you want to use: 7.1.4
Do you wish to build TensorFlow with TensorRT support? No
Please specify the locally installed NCCL version you want to use: 2.2.13
Please specify a list of comma-separated Cuda compute capabilities you want to build with: 6.0
Do you want to use clang as CUDA compiler? No
Please specify which gcc should be used by nvcc as the host compiler: /path/to/gcc-6.2.0
Do you wish to build TensorFlow with MPI support? No
Please specify optimization flags ""--config=opt"" : -march=native
Android builds? No
```
The bazel command is:
```
bazel build --verbose_failures --distinct_host_configuration=false --action_env=PYTHONPATH=$PYTHONPATH --config=cuda --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-msse4.2 --copt=-msse4.1 --cxxopt='-D_GLIBCXX_USE_CXX11_ABI=0' -c opt //tensorflow/tools/pip_package:build_pip_package && bazel-bin/tensorflow/tools/pip_package/build_pip_package $BUILDDIR
```

We also edit `third_party/gpus/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc.tpl` to replace the paths to the compilers, but this is always the same for every version that we build."
26251,The eigen_spatial_convolutions_test benchmark crashes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): b'v1.8.0-14450-g9c38906' 1.13.1
- Python version: Python 3.5.2
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): g++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

```
$ bazel test --config opt -- //tensorflow/core/kernels:eigen_spatial_convolutions_test

Starting local Bazel server and connecting to it...
DEBUG: /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

INFO: Analysed target //tensorflow/core/kernels:eigen_spatial_convolutions_test (78 packages loaded, 3781 targets configured).
INFO: Found 1 test target...
INFO: From Compiling tensorflow/core/kernels/eigen_spatial_convolutions_test.cc:
tensorflow/core/kernels/eigen_spatial_convolutions_test.cc: In function 'void Eigen::PackRhsHelper(int, int, int, int, int, int, int, int, int, int, Eigen::Index, Eigen::Index)':
tensorflow/core/kernels/eigen_spatial_convolutions_test.cc:1400:69: warning: typedef 'using Traits = struct Eigen::internal::gebp_traits<float, float>' locally defined but not used [-Wunused-local-typedefs]
   using Traits = typename Eigen::internal::gebp_traits<float, float>;
                                                                     ^
tensorflow/core/kernels/eigen_spatial_convolutions_test.cc: In function 'void Eigen::PackLhsHelper(int, int, int, int, int, Eigen::Index, Eigen::Index)':
tensorflow/core/kernels/eigen_spatial_convolutions_test.cc:1631:16: warning: variable 'nocontract_dim' set but not used [-Wunused-but-set-variable]
   nocontract_t nocontract_dim = {0};
                ^
tensorflow/core/kernels/eigen_spatial_convolutions_test.cc:1632:14: warning: variable 'contract_dim' set but not used [-Wunused-but-set-variable]
   contract_t contract_dim = {1};
              ^
Target //tensorflow/core/kernels:eigen_spatial_convolutions_test up-to-date:
  bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test
INFO: Elapsed time: 36.190s, Critical Path: 31.21s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 25 processes: 25 local.
INFO: Build completed successfully, 27 total actions
//tensorflow/core/kernels:eigen_spatial_convolutions_test                PASSED in 0.2s

INFO: Build completed successfully, 27 total actions

$ bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test --benchmarks=all
Running main() from test_main.cc
Benchmark                                                        Time(ns) Iterations
------------------------------------------------------------------------------------
*** Received signal 11 ***
*** BEGIN MANGLED STACK TRACE ***
/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/_U_S_Stensorflow_Score_Skernels_Ceigen_Uspatial_Uconvolutions_Utest___Utensorflow/libtensorflow_framework.so(+0x83085e)[0x7f470a1d785e]
/lib/x86_64-linux-gnu/libpthread.so.0(+0x11390)[0x7f4708fdf390]
bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x40c6cc]
bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x412c58]
bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x41316f]
/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/libtensorflow_Score_Slibtest.so(_ZN10tensorflow7testing9Benchmark3RunEiiPiPd+0x44)[0x7f470ab601e4]
/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/libtensorflow_Score_Slibtest.so(_ZN10tensorflow7testing9Benchmark3RunEPKc+0x409)[0x7f470ab612d9]
/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/core/kernels/../../../_solib_k8/libtensorflow_Score_Slibtest_Umain.so(main+0xb3)[0x7f470ab65b83]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0xf0)[0x7f4708395830]
bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test[0x409649]
*** END MANGLED STACK TRACE ***

*** Begin stack trace ***
	tensorflow::CurrentStackTrace[abi:cxx11]()
	
	
	
	
	
	tensorflow::testing::Benchmark::Run(int, int, int*, double*)
	tensorflow::testing::Benchmark::Run(char const*)
	main
	__libc_start_main
	
*** End stack trace ***
Aborted (core dumped)
```

**Describe the expected behavior**

The benchmark should run correctly and should not crash.

**Code to reproduce the issue**

```
$ bazel test --config opt -- //tensorflow/core/kernels:eigen_spatial_convolutions_test
$ bazel-bin/tensorflow/core/kernels/eigen_spatial_convolutions_test --benchmarks=all
```

The issue is 100% reproducible on my machine

"
26250,Tf.Keras metrics issue ,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): Source ( Pip ) 
- TensorFlow version (use command below): 1.13
- Python version: 3.6.7



You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

> 1.13.1

**Describe the current behavior**
I need to use Keras metric while compiling an LSTM model. it is getting compiled. But when I started to train I am getting error.

my code looks as follows : 

```
model = Sequential()
model.add(LSTM (120,activation = ""tanh"", input_shape=(timesteps,dim), return_sequences=True))
model.add(LSTM(120, activation = ""tanh"", return_sequences=True))
model.add(LSTM(120, activation = ""tanh"", return_sequences=True))
model.add(LSTM(120, activation = ""tanh"", return_sequences=True))
model.add(LSTM(120, activation = ""tanh"", return_sequences=True))
model.add(LSTM(120, activation = ""tanh"", return_sequences=True))
model.add(Dense(dim))
model.compile(optimizer=""adam"", loss=""mse"",  metrics=[tf.keras.metrics.Precision()])

history = model.fit(data,data, 
                    epochs=100,
                    batch_size=10,
                    validation_split=0.2,
                    shuffle=True,
                    callbacks=[ch]).history
```

There error I am getting as follows


> InvalidArgumentError: assertion failed: [predictions must be >= 0] [Condition x >= y did not hold element-wise:x (dense_3/BiasAdd:0) = ] [[[2.72658144e-06 1.17555362e-06 1.96436554e-06...]]...] [y (metrics_3/precision_1/Cast/x:0) = ] [0] [[{{node metrics_3/precision_1/assert_greater_equal/Assert/AssertGuard/Assert}}]]

"
26249,Compiling TF 1.12 failes under CentOS 6: name 'http_archive' is not defined,"**System information**
- _CentOS release 6.9 (Final), Kernel: 4.15.0-45-generic, x86_64_
- TensorFlow installed from (source or binary): _source_
- TensorFlow version: _1.12.0_
- Python version: _3.6.8, 64bit_
- Bazel version (if compiling from source): _Build label: 0.23.0- (@non-git)_ 
   actually Bazel, is build from source too.
- GCC/Compiler version (if compiling from source): _gcc (GCC) 6.3.1 20170216 (Red Hat 6.3.1-3)_
- CUDA/cuDNN version: None
- GPU model and memory: None



**Describe the problem**
I want to use TensorFlow under the system (actually inside a Docker container) specified above (the Linux/GCC/Python versions are fixed).  I get the following error while executing ` bazel build -c opt //tensorflow/tools/pip_package:build_pip_package`

    ERROR: /usr/src/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined
    ERROR: Error evaluating WORKSPACE file
    ERROR: error loading package '': Encountered error while reading extension 
        file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error
        loading package 'external': Could not load //external package
    ERROR: error loading package '': Encountered error while reading extension 
        file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error 
        loading package 'external': Could not load //external package
    INFO: Elapsed time: 1.276s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (0 packages loaded)
    FAILED: Build did NOT complete successfully (0 packages loaded)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

    FROM amd64/centos:6.9
    
    RUN yum -y install gcc openssl-devel bzip2-devel wget centos-release-scl
    
    # Install bazel
    WORKDIR /usr/src
    RUN yum -y install java-1.8.0-openjdk-devel devtoolset-6
    
    RUN wget https://github.com/bazelbuild/bazel/releases/download/0.23.0/bazel-0.23.0-dist.zip \
        && unzip bazel-*.zip -d /bazel
    
    # Set PATH, because ""scl enable"" does not have any effects to ""docker build""
    ENV PATH /opt/rh/devtoolset-6/root/usr/bin:$PATH
    
    RUN cd /bazel \
        && scl enable devtoolset-6 bash \
        && ./compile.sh \
        && mkdir -p /my/bin/ \
        && cp output/bazel /my/bin \
        && exit
    
    # Install Python new build
    RUN wget https://www.python.org/ftp/python/3.6.8/Python-3.6.8.tgz
    RUN tar xzf Python-3.6.8.tgz
    
    WORKDIR /usr/src/Python-3.6.8
    RUN ./configure --enable-optimizations
    RUN make altinstall
    
     # Install pip
    RUN wget https://bootstrap.pypa.io/get-pip.py
    RUN python3.6 get-pip.py
    
    RUN pip3 install -U pip six numpy wheel mock \
        && pip3 install -U keras_applications==1.0.6 --no-deps \
        && pip3 install -U keras_preprocessing==1.0.5 --no-deps
    
    # Build Tensorflow
    WORKDIR /usr/src/
    RUN yum -y install git

    RUN git clone https://github.com/tensorflow/tensorflow.git \
        && cd /usr/src/tensorflow \
        && git checkout v1.12.0 
    
    WORKDIR /usr/src/tensorflow
    RUN scl enable devtoolset-6 bash \
       && export \
          PATH=$PATH:/my/bin \
          PYTHON_BIN_PATH=/usr/local/bin/python3.6 \
          PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/ \
          TF_NEED_JEMALLOC=0 \
          TF_NEED_GCP=0 \
          TF_NEED_HDFS=0 \
          TF_NEED_S3=0 \
          TF_ENABLE_XLA=0 \
          TF_NEED_GDR=0 \
          TF_NEED_VERBS=0 \
          TF_NEED_OPENCL=0 \
          TF_NEED_CUDA=0 \
          TF_NEED_MPI=0 \
       && ./configure \
       && echo ""import /usr/src/tensorflow/tools/bazel.rc"" > /usr/src/tensorflow/.bazelrc \
       && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package \
       && mkdir -p /tmp/tensorflow_pkg \
       && bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg \
       && exit



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

    [Non relevant Docker cache logs above]
    
    Step 21/21 : RUN scl enable devtoolset-6 bash    && export       PATH=$PATH:/my/bin       PYTHON_BIN_PATH=/usr/local/bin/python3.6       PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/       TF_NEED_JEMALLOC=0       TF_NEED_GCP=0       TF_NEED_HDFS=0       TF_NEED_S3=0       TF_ENABLE_XLA=0       TF_NEED_GDR=0       TF_NEED_VERBS=0       TF_NEED_OPENCL=0       TF_NEED_CUDA=0       TF_NEED_MPI=0    && ./configure    && echo ""import /usr/src/tensorflow/tools/bazel.rc"" > /usr/src/tensorflow/.bazelrc    && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package    && exit
     ---> Running in e006e5ffc788
    WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
    /usr/src/tensorflow/tools/bazel.rc
    Extracting Bazel installation...
    WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
    You have bazel 0.23.0- (@non-git) installed.
    Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: Apache Ignite support will be enabled for TensorFlow.
    
    Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: No OpenCL SYCL support will be enabled for TensorFlow.
    
    Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow.
    
    Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded.
    
    Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 
    
    Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds.
    
    Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
    	--config=mkl         	# Build with MKL support.
    	--config=monolithic  	# Config for mostly static monolithic build.
    	--config=gdr         	# Build with GDR support.
    	--config=verbs       	# Build with libverbs support.
    	--config=ngraph      	# Build with Intel nGraph support.
    Configuration finished
    Starting local Bazel server and connecting to it...
    Loading: 
    Loading: 0 packages loaded
    ERROR: /usr/src/tensorflow/WORKSPACE:3:1: name 'http_archive' is not defined
    ERROR: Error evaluating WORKSPACE file
    ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package
    ERROR: error loading package '': Encountered error while reading extension file 'closure/defs.bzl': no such package '@io_bazel_rules_closure//closure': error loading package 'external': Could not load //external package
    INFO: Elapsed time: 1.276s
    INFO: 0 processes.
    FAILED: Build did NOT complete successfully (0 packages loaded)
    FAILED: Build did NOT complete successfully (0 packages loaded)
    ERROR: Service 'mysys_replica' failed to build: The command '/bin/sh -c scl enable devtoolset-6 bash    && export       PATH=$PATH:/my/bin       PYTHON_BIN_PATH=/usr/local/bin/python3.6       PYTHON_LIB_PATH=/usr/local/lib/python3.6/site-packages/       TF_NEED_JEMALLOC=0       TF_NEED_GCP=0       TF_NEED_HDFS=0       TF_NEED_S3=0       TF_ENABLE_XLA=0       TF_NEED_GDR=0       TF_NEED_VERBS=0       TF_NEED_OPENCL=0       TF_NEED_CUDA=0       TF_NEED_MPI=0    && ./configure    && echo ""import /usr/src/tensorflow/tools/bazel.rc"" > /usr/src/tensorflow/.bazelrc    && bazel build -c opt //tensorflow/tools/pip_package:build_pip_package    && exit' returned a non-zero code: 1
"
26247,wav_io needs to accept wav files which has 'JUNK' chunk before 'fmt ' chunk,"**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Not yet decided

**Describe the feature and the current behavior/state.**
Sometimes we encounter wav files which has `JUNK` chunk before `fmt `chunk. CPython's wave module handles this kind of wav files [by skipping those unnecessary chunk](https://github.com/python/cpython/blob/3.6/Lib/wave.py#L125). But tensorflow's wav_io module [expects `fmt ` chunk comes immediately after `WAVE` tag](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/wav/wav_io.cc#L220). As a consequence, those wav files with `JUNK` chunk, which is playable by media file players, generates *Header mismatch* InvalidArgument error.

As a conclusion, we need to implement, I may call it as, unnecessary chunk skipping functionality on wav_io.

**Will this change the current api? How?**
I expect that api won't be affected at all.

**Who will benefit with this feature?**
Those who are handling wide variety of wav files would be benefit with this feature, as they do not have to do some preprocessing(removing `JUNK` chunk manually all the time).

**Any Other info.**
[Kaldi toolkit's handling of wav input](https://github.com/kaldi-asr/kaldi/blob/master/src/feat/wave-reader.cc#L135) would be a good reference."
26245,InvalidArgumentError Incompatible shapes when multi_gpu_model used on LSTM model in TF 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190228
- Python version: 3.7
- CUDA/cuDNN version: 10/7.4
- GPU model and memory:  Tesla M60 on AWS g3.8xlarge

**Describe the current behavior**
`multi_gpu_model` fails to parallelize LSTM model. Things I've tried without success:
- setting  `cpu_relocation=True` parameter
- disabling eager mode

**Describe the expected behavior**
the code below should run instead of giving an error.

**Code to reproduce the issue**

```python
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.utils import multi_gpu_model

vocab_size= 20000
maxlen=80

(X_train, y_train), (X_test, y_test) = \
    imdb.load_data(num_words=vocab_size)

X_train_pad = pad_sequences(X_train, maxlen=maxlen)
X_test_pad = pad_sequences(X_test, maxlen=maxlen)

with tf.device('/cpu:0'):
    model = Sequential([
        Embedding(vocab_size, 100, input_length=maxlen),
        LSTM(64, dropout=0.2, recurrent_dropout=0.2),
        Dense(1, activation='sigmoid')
    ])

model = multi_gpu_model(model, 2)

model.compile(loss='binary_crossentropy',
              optimizer='rmsprop',
              metrics=['accuracy'])

model.fit(X_train_pad, y_train,
          batch_size=2048,
          epochs=2,
          shuffle=True)
```

**Other info / logs**

```python
W0301 07:00:25.055628 139938579490560 tf_logging.py:161] <tensorflow.python.keras.layers.recurrent.UnifiedLSTM object at 0x7f45f84f5470>: Note that this layer is not optimized for performance. Please use tf.keras.layers.CuDNNLSTM for better performance on GPU.

Epoch 1/2

---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-6-827e285c76cb> in <module>
     33           batch_size=2048,
     34           epochs=2,
---> 35           shuffle=True)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--> 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    349 
    350         # Get outputs.
--> 351         batch_outs = f(ins_batch)
    352         if not isinstance(batch_outs, list):
    353           batch_outs = [batch_outs]

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3215         value = math_ops.cast(value, tensor.dtype)
   3216       converted_inputs.append(value)
-> 3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
   3219                                  [x.numpy() for x in outputs])

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    523       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(
    524           list(kwargs.keys()), list(self._arg_keywords)))
--> 525     return self._call_flat(args)
    526 
    527   def _filtered_call(self, args, kwargs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    592     # Only need to override the gradient in graph mode and when we have outputs.
    593     if context.executing_eagerly() or not self.outputs:
--> 594       outputs = self._inference_function.call(ctx, args)
    595     else:
    596       self._register_gradient()

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=(""executor_type"", executor_type,
    381                    ""config_proto"", config),
--> 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Incompatible shapes: [1024,64] vs. [2048,64]
	 [[{{node sequential_5_1/unified_lstm_5/while/body/_1008/mul_6}}]]
	 [[training_3/RMSprop/gradients/loss_5/dense_5_loss/binary_crossentropy/Mean_grad/Prod/_842]] [Op:__inference_keras_scratch_graph_32546]
```"
26244,FailedPreconditionError when running Convolutional Keras model on CPU in TF 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190228
- Python version: 3.7
- CUDA/cuDNN version: 10/7.4
- GPU model and memory:  Tesla M60 on AWS g3.8xlarge


**Describe the current behavior**
Training a model defined on the CPU raises a `FailedPreconditionError` when using a machine with a GPU in TF 2.0 nightly.

**Describe the expected behavior**
No error is raised if I use one of the following fixes:
1) use `tensorflow.compat.v1.disable_eager_execution()`
2) remove the `Conv2D` layer
3) remove the `batch_size` and `epochs` arguments from the `.fit` call

However, the context setter seems to have no effect and training is happening on the GPU anyways (I can tell by how fast it's training)

the behavior seems weird, can anyone explain what's going on?

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Dense
from tensorflow.keras.layers import Conv2D

(X_train, y_train), _ = cifar10.load_data()
X_train = X_train.astype('float32') / 255.0

with tf.device('cpu:0'):
    model = Sequential([
        Conv2D(32, (3, 3), input_shape=(32, 32, 3)),
        Flatten(),
        Dense(10, activation='softmax')
    ])
    
    model.compile(loss='sparse_categorical_crossentropy',
                  optimizer='rmsprop',
                  metrics=['accuracy'])

    
model.fit(X_train, y_train,
          batch_size=1024,
          epochs=2)
```

**Other info / logs**
traceback:
```python
---------------------------------------------------------------------------
FailedPreconditionError                   Traceback (most recent call last)
<ipython-input-1-d00bb2a707a4> in <module>
     24 model.fit(X_train, y_train,
     25           batch_size=1024,
---> 26           epochs=2)

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    871           validation_steps=validation_steps,
    872           validation_freq=validation_freq,
--> 873           steps_name='steps_per_epoch')
    874 
    875   def evaluate(self,

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)
    349 
    350         # Get outputs.
--> 351         batch_outs = f(ins_batch)
    352         if not isinstance(batch_outs, list):
    353           batch_outs = [batch_outs]

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3215         value = math_ops.cast(value, tensor.dtype)
   3216       converted_inputs.append(value)
-> 3217     outputs = self._graph_fn(*converted_inputs)
   3218     return nest.pack_sequence_as(self._outputs_structure,
   3219                                  [x.numpy() for x in outputs])

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    523       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(
    524           list(kwargs.keys()), list(self._arg_keywords)))
--> 525     return self._call_flat(args)
    526 
    527   def _filtered_call(self, args, kwargs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    592     # Only need to override the gradient in graph mode and when we have outputs.
    593     if context.executing_eagerly() or not self.outputs:
--> 594       outputs = self._inference_function.call(ctx, args)
    595     else:
    596       self._register_gradient()

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=(""executor_type"", executor_type,
    381                    ""config_proto"", config),
--> 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

~/miniconda3/envs/tf2/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

~/miniconda3/envs/tf2/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

FailedPreconditionError: Error while reading resource variable _AnonymousVar15 from Container: localhost. This could mean that the variable was uninitialized. Invalid argument: Trying to access resource _AnonymousVar15 located in device /job:localhost/replica:0/task:0/device:GPU:0 from device /job:localhost/replica:0/task:0/device:CPU:0
	 [[{{node training/RMSprop/RMSprop/update_3/mul/ReadVariableOp}}]] [Op:__inference_keras_scratch_graph_646]
```
"
26242,Build failure on s390x on hwloc ,"**System information**

    * OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04

    * Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA

    * TensorFlow installed from (source or binary): source

    * TensorFlow version: master as on today

    * Python version:  2.7.x

    * Bazel version (if compiling from source): 0.19.0 

    * GCC/Compiler version (if compiling from source): gcc 7.3.0, glibc 2.28

    * CUDA/cuDNN version: NA

    * GPU model and memory: NA


**Describe the problem**
Tensorflow build fails with an error:

```
ERROR: /home/jenkins/.cache/bazel/_bazel_jenkins/14d9bef57f8e4d2a0eef0de174c4144b/external/hwloc/BUILD.bazel:212:1: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1)
In file included from external/hwloc/hwloc/topology-x86.c:23:0:
external/hwloc/hwloc/topology-x86.c: In function 'cpuid_or_from_dump':
external/hwloc/include/private/cpuid-x86.h:67:3: error: inconsistent operand constraints in an 'asm'
   __asm__(
   ^~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```"
26241,"F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine. Aborted","Error :

tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.
Aborted

 getting this error after running

**python -m rasa_core.train -d domain.yml -s data/stories.md -o models/current/dialogue -c policies.yml**

Version:

 
My System Config


Architecture:          x86_64
CPU op-mode(s):        32-bit, 64-bit
Byte Order:            Little Endian
CPU(s):                1
On-line CPU(s) list:   0
Thread(s) per core:    1
Core(s) per socket:    1
Socket(s):             1
NUMA node(s):          1
Vendor ID:             AuthenticAMD
CPU family:            16
Model:                 9
Model name:            AMD Opteron(tm) Processor 6128 HE
Stepping:              1
CPU MHz:               2000.000
BogoMIPS:              4000.00
Hypervisor vendor:     VMware
Virtualization type:   full
L1d cache:             64K
L1i cache:             64K
L2 cache:              512K
L3 cache:              10236K
NUMA node0 CPU(s):     0



  
"
26240,cannot import tensorflow,"**System information**
- Win10 X64 :
- pip install tensorflow-gpu==1.12.0
- TensorFlow version:
- Python version: 3.6
- Installed using  pip
- GCC/Compiler version (if compiling from source):
- CUDA: 9 
- cuDNN version: 7
- GPU model and memory: NVIDIA GeForce MX150

**Describe the problem**
cannot import tensorflow,  always this error: tensorflow.python.pywrap_tensorflow_internal import *,
I have checked therre is no related DLL existed, and i know it is an install issue. I have searched many same issue and tried to fix, but it does not work . 

PS: This issue only for tensorflow-gpu install, it's ok for tensorflow install on the same machine. But i want to use tensorflow-gpu.



>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\__init__.py"", line 28, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Program Files\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Program Files\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
26239,"Why is op:BatchMatMul listed in file(.pbtxt), when there is no call to such op?","**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: binary
- TensorFlow version: 1.12
- Python version: 3.6 

**Describe the current behavior**
There is no usage of op named _**BatchMatMul**_ in my code. But the op is listed in the file(.pbtxt) which gives me other issues while converting the graph to tflite.

**Describe the expected behavior**
If there is no explicit usage of the particular op, it should not be listed in file(.pbtxt)"
26234,"DLL Load failed, WIn10","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary - pip install tensorflow-gpu
- TensorFlow version: tensorflow-gpu-1.13.1
- Python version: Python 3.6.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 9 - cudnn-9.0-windows10-x64-v7.5.0.56
- GPU model and memory: 2080 TI SLI - 64 GB DDR4



**Describe the problem**

Reports missing modules:

```
(venv) C:\Users\Ryan\Developer\TF>python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
Traceback (most recent call last):
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ryan\Developer\TF\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ryan\Developer\TF\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Ryan\Developer\TF\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Ryan\Developer\TF\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Ryan\Developer\TF\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**

.\venv\Scripts\activate
python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""

**Any other info / logs**

Here is a photo of the cudnn DLL's placed in the appropriate locations, and also my environment variables:
https://i.imgur.com/T0yuuQa.png

Also, here is the output from deviceQuery.exe, I could build that example just fine:
```
C:\ProgramData\NVIDIA Corporation\CUDA Samples\v9.0\bin\win64\Debug>deviceQuery.exe
deviceQuery.exe Starting...

 CUDA Device Query (Runtime API) version (CUDART static linking)

Detected 2 CUDA Capable device(s)

Device 0: ""GeForce RTX 2080 Ti""
  CUDA Driver Version / Runtime Version          10.1 / 9.0
  CUDA Capability Major/Minor version number:    7.5
  Total amount of global memory:                 11264 MBytes (11811160064 bytes)
MapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM
MapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM
  (68) Multiprocessors, ( 64) CUDA Cores/MP:     4352 CUDA Cores
  GPU Max Clock rate:                            1650 MHz (1.65 GHz)
  Memory Clock rate:                             7000 Mhz
  Memory Bus Width:                              352-bit
  L2 Cache Size:                                 5767168 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1024
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 6 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)
  Device supports Unified Addressing (UVA):      Yes
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 1 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

Device 1: ""GeForce RTX 2080 Ti""
  CUDA Driver Version / Runtime Version          10.1 / 9.0
  CUDA Capability Major/Minor version number:    7.5
  Total amount of global memory:                 11264 MBytes (11811160064 bytes)
MapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM
MapSMtoCores for SM 7.5 is undefined.  Default to use 64 Cores/SM
  (68) Multiprocessors, ( 64) CUDA Cores/MP:     4352 CUDA Cores
  GPU Max Clock rate:                            1650 MHz (1.65 GHz)
  Memory Clock rate:                             7000 Mhz
  Memory Bus Width:                              352-bit
  L2 Cache Size:                                 5767168 bytes
  Maximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384)
  Maximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers
  Maximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  1024
  Maximum number of threads per block:           1024
  Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
  Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 6 copy engine(s)
  Run time limit on kernels:                     Yes
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement for Surfaces:            Yes
  Device has ECC support:                        Disabled
  CUDA Device Driver Mode (TCC or WDDM):         WDDM (Windows Display Driver Model)
  Device supports Unified Addressing (UVA):      Yes
  Supports Cooperative Kernel Launch:            No
  Supports MultiDevice Co-op Kernel Launch:      No
  Device PCI Domain ID / Bus ID / location ID:   0 / 2 / 0
  Compute Mode:
     < Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) >

deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.1, CUDA Runtime Version = 9.0, NumDevs = 2
Result = PASS
```

Any help appreciated I am quite stuck here. "
26233,Any plan to support control flow in TF lite?,"Seems TF lite doesn't support object detection models like rcnn (https://github.com/tensorflow/tensorflow/issues/19293), I guess it was due to the control flow stuff, is there any plan/timeline to add the support? "
26227,[SOLVED] Failed to build 1.13.0-rc2 with CUDA in Windows 7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): WINDOWS 7
- TensorFlow installed from (source or binary): SOURCE
- TensorFlow version: 1.13.0-rc2
- Python version: 3.7
- Installed using virtualenv? pip? conda?: building
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): MSVS 2015 14.0.24215.1
- CUDA/cuDNN version: 10 / 7.4.2.24
- GPU model and memory: NVIDIA QUADRO k5000 4GB (3.0)



**Describe the problem**
Building from source in Windows 7 with CUDA 10 support for 3.0 computing capability, with a clean Python 3.7 installation, fails with the message: `ImportError: DLL load failed: The specified module could not be found.`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
* Installed CUDA 10 and cuDnn 7.4.2.24
* Installed mysys64 in C:/mysys64
* Installed bazel 0.21
* Installed a fresh 3.7 Python in %APPDATA%/Programs/Python
* pip3 install six numpy wheel
* pip3 install keras_applications --no-deps
* pip3 install keras_preprocessing --no-deps
* configured all enviroment variables according to the tutorial
* downloaded and extracted tensorflow 1.13.0-rc2 release
* opened VS 2015 x64 Native Tools Command Prompt as Administrator
* entered the tensorflow source dir
* python ./configure.py
* bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
```cmd
INFO: From Linking tensorflow/contrib/boosted_trees/python/ops/_boosted_trees_ops.so:
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/boosted_trees/python/ops/python/ops/lib_boosted_trees_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/boosted_trees/python/ops/
INFO: From Linking tensorflow/contrib/tpu/python/ops/_tpu_ops.so:
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/python/ops/python/ops/lib_tpu_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/tpu/python/ops/python/ops/lib_tpu_ops.so.exp
ERROR: C:/users/reinert/downloads/tensorflow-1.13.0-rc2/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/reinert/_bazel_reinert/kbgdyy6s/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin
    SET PYTHON_BIN_PATH=C:/Users/reinert/AppData/Local/Programs/Python/Python37/python.exe
    SET PYTHON_LIB_PATH=C:/Users/reinert/AppData/Local/Programs/Python/Python37/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh
Execution platform: @bazel_tools//platforms:host_platform
Traceback (most recent call last):
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\reinert\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\reinert\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\reinert\AppData\Local\Temp\Bazel.runfiles_t5_pd1qh\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\reinert\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\reinert\AppData\Local\Programs\Python\Python37\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2166,410s, Critical Path: 1238,93s
INFO: 4651 processes: 4651 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```
"
26226,Tensorboard 1.12 not working after upgrade from 1.8 to 1.12,"I upgraded tensorflow to 1.12 which also upgraded tensorboard to 1.12 from 1.8 version but I get the error:
Traceback (most recent call last):
  File ""/usr/local/bin/tensorboard"", line 7, in <module>
    from tensorboard.main import run_main
  File ""/usr/local/lib/python3.6/site-packages/tensorboard/main.py"", line 44, in <module>
    from tensorboard import default
  File ""/usr/local/lib/python3.6/site-packages/tensorboard/default.py"", line 44, in <module>
    from tensorboard.plugins.interactive_inference import interactive_inference_plugin
  File ""/usr/local/lib/python3.6/site-packages/tensorboard/plugins/interactive_inference/interactive_inference_plugin.py"", line 27, in <module>
    from grpc.framework.interfaces.face.face import AbortionError
ModuleNotFoundError: No module named 'grpc.framework'

I unsintalled and reinstalled grpcio, I completely removed tensorflow and tensorboard and then re-installed 1.12 version but same error keeps coming back."
26225,record_summaries_every_n_global_steps() should not execute code unless global_step % n == 0,"* Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  No
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Ubuntu 18.04
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  No
* TensorFlow installed from (source or binary):
  Source
* TensorFlow version (use command below):
  b'v1.12.0-5845-g764109a352' 1.12.0
* Python version:
  3.6.7
* Bazel version (if compiling from source):
  Invocation ID: 42251854-036f-415c-8a52-76aac8520ea0
  Build label: 0.21.0
  Build time: Wed Dec 19 12:58:44 2018 (1545224324)
* GCC/Compiler version (if compiling from source):
  gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
* CUDA/cuDNN version:
  Cuda compilation tools, release 10.0, V10.0.130
* GPU model and memory:
  GTX 1060 Max Q, 6gb VRAM

**Describe the current behavior**
`tf.contrib.summary.record_summaries_every_n_global_steps(n, global_step)` may not be recording summaries unless `global_step % n == 0`, HOWEVER, it is running the code within its block anyways. In my opinion this is a huge waste of resources, especially when image logging is being done and transformations are required. Furthermore, there is no information of this behaviour on the documentation. Finally, it defeats the purpose of the function.
https://www.tensorflow.org/api_docs/python/tf/contrib/summary/record_summaries_every_n_global_steps
**Describe the expected behavior**
It should not run the code within its ""with"" block when `global_step % n != 0`, or documentation should be added that describes the current behaviour. It defeats the purpose of it when I can combine an if statement with `tf.contrib.summary.always_record_summaries():` to produce better results.
**Code to reproduce the issue**
```
global_step = tf.train.get_or_create_global_step()
writer = tf.contrib.summary.create_file_writer('./test')
writer.set_as_default()

images = []
for i in range(100):
        images.append(np.random.uniform(0, 255, (32, 32, 3)).astype(np.float32))
images = np.array(images)
data = tf.data.Dataset.from_tensor_slices((images))

for i in range(10000):
        batch = data.batch(len(images))
        for image in batch:
                with tf.contrib.summary.record_summaries_every_n_global_steps(100, global_step):
                        print(global_step.numpy())
                        tf.contrib.summary.image('image', image)
        global_step.assign_add(1)
```
**Other info / logs**
```
0
1
2
3
4
5
6
....
9999
```
The images were still logged every global_step % n == 0 iterations, yet as can be seen here, the code within the block still executed every global_step. I shouldn't need to add an `if global_step % n == 0:` outside of the `with tf.contrib.summary.record_summaries_every_n_global_steps(n, global_step):` function. It literally defeats the purpose of it, as I might as well just use `with tf.contrib.summary.always_record_summaries():` inside of an `if global_step % n == 0:`."
26222,Estimator training hangs in multiple gpu if dataset doesn't have enough element to feed both gpus last batches,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Distributed training (one node, Multiple GPUs)
- TensorFlow installed from (source or binary):
PIP
- TensorFlow version (use command below):
TF 1.12
- Python version:
3.6.8
- CUDA/cuDNN version:
9.0
- GPU model and memory:
2 GTX1080 8Go

**Describe the current behavior**
Basically, if the dataset doesn't have enough elements to feed both gpus last batches the training hangs.
- If you doesn't have enough to feed the first gpu last batch and don't want to drop the last batch then the training hangs.
- If you doesn't have enough to feed the first gpu last batch and want to drop the last batch then you're fine.
- If you have enough to feed the first gpu last batch but not the second gpu last batch and don't want to drop the last batch then the training hangs
- If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then the training hangs


**Describe the expected behavior**
- If you doesn't have enough to feed the first gpu last batch and don't want to drop the last batch then run the first gpu partial batch and do nothing with the second gpu
- If you doesn't have enough to feed the first gpu last batch and want to drop the last batch then drop the last batch for both gpus.
- If you have enough to feed the first gpu last batch but not the second gpu last batch and don't want to drop the last batch then run the first gpu entire batch and run the second gpu partial batch.
- If you have enough to feed the first gpu last batch but not the second gpu last batch and want to drop the last batch then run the first gpu entire batch and do nothing with the second gpu

**Code to reproduce the issue**
```
import tensorflow as tf

# Play with sample count (5, 6, 7) and drop_remainder (True, False) to reproduce the issue
sample_count = 5
drop_remainder = False


def run():
    # Config
    run_config = tf.estimator.RunConfig(
        session_config=tf.ConfigProto(allow_soft_placement=True),
        train_distribute=tf.contrib.distribute.MirroredStrategy(num_gpus=2))

    # Estimator
    estimator = tf.estimator.Estimator(model_fn=model_fn, config=run_config)
    estimator.train(train_input_fn)


# Times two dataset
def train_input_fn():
    return tf.data.Dataset \
        .range(sample_count) \
        .repeat(1) \
        .map(lambda x: (x, x * 2)) \
        .batch(2, drop_remainder)


# Times two model
def model_fn(features, labels, mode):
    input_layer = tf.cast(tf.reshape(features, [-1, 1]), tf.float32)
    expected_output = tf.cast(tf.reshape(labels, [-1, 1]), tf.float32)

    logit = tf.layers.dense(input_layer, 1, None, False)
    loss = tf.losses.mean_squared_error(expected_output, logit)

    logging_hook = tf.train.LoggingTensorHook(tensors={""feature_value"": features.name}, every_n_iter=1)

    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer(0.001)
        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode,
                                          loss=loss,
                                          train_op=train_op,
                                          training_hooks=[logging_hook])


if __name__ == '__main__':
    tf.logging.set_verbosity(tf.logging.DEBUG)
    run()

```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
INFO:tensorflow:Initializing RunConfig with distribution strategies.
INFO:tensorflow:Not using Distribute Coordinator.
WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp0ofz0qx1
INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_device_fn': None, '_experimental_distribute': None, '_task_type': 'worker', '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_distribute_coordinator_mode': None, '_service': None, '_save_summary_steps': 100, '_model_dir': '/tmp/tmp0ofz0qx1', '_master': '', '_keep_checkpoint_max': 5, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7feedf9f5dd8>, '_protocol': None, '_task_id': 0, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true
, '_is_chief': True, '_num_worker_replicas': 1, '_global_id_in_cluster': 0, '_evaluation_master': '', '_log_step_count_steps': 100, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7feedf9f5e48>, '_eval_distribute': None, '_num_ps_replicas': 0}
2019-02-28 11:18:22.645478: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-28 11:18:22.818624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-28 11:18:22.820057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.847
pciBusID: 0000:01:00.0
totalMemory: 7.90GiB freeMemory: 7.11GiB
2019-02-28 11:18:22.954140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-28 11:18:22.955822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.847
pciBusID: 0000:02:00.0
totalMemory: 7.93GiB freeMemory: 7.81GiB
2019-02-28 11:18:22.957142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-28 11:18:23.349095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 11:18:23.349133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-28 11:18:23.349139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-28 11:18:23.349143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-28 11:18:23.349775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 6853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-28 11:18:23.350097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:1 with 7535 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
INFO:tensorflow:Configured nccl all-reduce.
2019-02-28 11:18:23.372783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-28 11:18:23.373002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 11:18:23.373032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-28 11:18:23.373038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-28 11:18:23.373043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-28 11:18:23.373272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-28 11:18:23.373346: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7535 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:batch_all_reduce invoked for batches size = 1 with algorithm = nccl, num_packs = 1, agg_small_grads_max_bytes = 0 and agg_small_grads_max_group = 10
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2019-02-28 11:18:23.707824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-28 11:18:23.707940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 11:18:23.707963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-28 11:18:23.707967: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-28 11:18:23.707988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-28 11:18:23.708250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)
2019-02-28 11:18:23.708475: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7535 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080, pci bus id: 0000:02:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmp0ofz0qx1/model.ckpt.
INFO:tensorflow:loss = 47.902126, step = 0
INFO:tensorflow:feature_value = [2 3]
```
"
26218,tf-gpu==1.13.1  : 35% less batch size before OOM vs tf-gpu==1.11.0,"**System information**
- Windows 7
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.11.0 , 1.13.1
- Python version: 3.6.5
- CUDA/cuDNN version: 9/7.1.4 , 10/7.4.1
- GPU model and memory: GTX 1060 6GB

**Describe the current behavior**

I have standard AE network with pixel shuffler layer.

on `tf.1.11.0-cuda 9` maximum batch size for my GTX 1060 6GB is `132`

but after upgrade to `tf.1.13.1-cuda 10` tf cannot handle same batch size it produces OOM error
and maximum now `90` for my card.

**Describe the expected behavior**

expected not to downgrade performance when upgrading tensorflow

**Code to reproduce the issue**

```
import numpy as np
import tensorflow as tf
keras = tf.keras
KL = keras.layers
K = keras.backend

bgr_shape = (128, 128, 3)
#batch_size = 132 #max -tf.1.11.0-cuda 9
batch_size = 86 #max -tf.1.13.1-cuda 10
 
class PixelShuffler(keras.layers.Layer):
    def __init__(self, size=(2, 2), data_format=None, **kwargs):
        super(PixelShuffler, self).__init__(**kwargs)
        self.size = size

    def call(self, inputs):

        input_shape = K.int_shape(inputs)
        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))


        batch_size, h, w, c = input_shape
        if batch_size is None:
            batch_size = -1
        rh, rw = self.size
        oh, ow = h * rh, w * rw
        oc = c // (rh * rw)

        out = K.reshape(inputs, (batch_size, h, w, rh, rw, oc))
        out = K.permute_dimensions(out, (0, 1, 3, 2, 4, 5))
        out = K.reshape(out, (batch_size, oh, ow, oc))
        return out

    def compute_output_shape(self, input_shape):

        if len(input_shape) != 4:
            raise ValueError('Inputs should have rank ' +
                             str(4) +
                             '; Received input shape:', str(input_shape))


        height = input_shape[1] * self.size[0] if input_shape[1] is not None else None
        width = input_shape[2] * self.size[1] if input_shape[2] is not None else None
        channels = input_shape[3] // self.size[0] // self.size[1]

        if channels * self.size[0] * self.size[1] != input_shape[3]:
            raise ValueError('channels of input and size are incompatible')

        return (input_shape[0],
                height,
                width,
                channels)

    def get_config(self):
        config = {'size': self.size}
        base_config = super(PixelShuffler, self).get_config()

        return dict(list(base_config.items()) + list(config.items()))
        
def upscale (dim):
    def func(x):
        return PixelShuffler()((KL.Conv2D(dim * 4, kernel_size=3, strides=1, padding='same')(x)))
    return func 
            
inp = KL.Input(bgr_shape)
x = inp
x = KL.Conv2D(128, 5, strides=2, padding='same')(x)
x = KL.Conv2D(256, 5, strides=2, padding='same')(x)
x = KL.Conv2D(512, 5, strides=2, padding='same')(x)
x = KL.Conv2D(1024, 5, strides=2, padding='same')(x)
x = KL.Dense(1024)(KL.Flatten()(x))
x = KL.Dense(8 * 8 * 1024)(x)
x = KL.Reshape((8, 8, 1024))(x)
x = upscale(512)(x)
x = upscale(256)(x)
x = upscale(128)(x)
x = upscale(64)(x)
x = KL.Conv2D(3, 5, strides=1, padding='same')(x)

model = keras.models.Model ([inp], [x])
model.compile(optimizer=keras.optimizers.Adam(lr=5e-5, beta_1=0.5, beta_2=0.999), loss='mae')

training_data = np.zeros ( (batch_size,128,128,3) )
loss = model.train_on_batch( [training_data], [training_data] )
print (""FINE"")
```




**Other info / logs**

```
1] 1 Chunks of size 12032 totalling 11.8KiB
2019-02-28 19:45:23.516100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 19200 totalling 75.0KiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 38400 totalling 150.0KiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 262144 totalling 1.00MiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 368640 totalling 360.0KiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 1179648 totalling 4.50MiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 5 Chunks of size 3276800 totalling 15.63MiB
2019-02-28 19:45:23.517100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 4 Chunks of size 4718592 totalling 18.00MiB
2019-02-28 19:45:23.520100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 3 Chunks of size 13107200 totalling 37.50MiB
2019-02-28 19:45:23.520100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 17028352 totalling 16.24MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 17694720 totalling 16.88MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 17694976 totalling 16.88MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 3 Chunks of size 18874368 totalling 54.00MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 23592960 totalling 22.50MiB
2019-02-28 19:45:23.521100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 5 Chunks of size 52428800 totalling 250.00MiB
2019-02-28 19:45:23.529100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 5 Chunks of size 75497472 totalling 360.00MiB
2019-02-28 19:45:23.529100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 94371840 totalling 90.00MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 100362240 totalling 95.71MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 2 Chunks of size 188743680 totalling 360.00MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 194688000 totalling 185.67MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 12 Chunks of size 268435456 totalling 3.00GiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
1] 1 Chunks of size 552317184 totalling 526.73MiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
5] Sum Total of in-use chunks: 5.02GiB
2019-02-28 19:45:23.530100: I tensorflow/core/common_runtime/bfc_allocator.cc:64
7] Stats:
Limit:                  5838622720
InUse:                  5393793792
MaxInUse:               5708028928
NumAllocs:                     434
MaxAllocSize:           1363673088

2019-02-28 19:45:23.531100: W tensorflow/core/common_runtime/bfc_allocator.cc:27
1] *****************************************************__**********_***********
**********************x
2019-02-28 19:45:23.531100: W tensorflow/core/framework/op_kernel.cc:1401] OP_RE
QUIRES failed at conv_grad_input_ops.cc:1054 : Resource exhausted: OOM when allo
cating tensor with shape[90,128,64,64] and type float on /job:localhost/replica:
0/task:0/device:GPU:0 by allocator GPU_0_bfc
Traceback (most recent call last):
  File ""D:\DeepFaceLab\_internal\bin\DeepFaceLab\test.py"", line 87, in <module>
    loss = model.train_on_batch( [training_data], [training_data] )
  File ""D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\keras\e
ngine\training.py"", line 1188, in train_on_batch
    outputs = self.train_function(ins)  # pylint: disable=not-callable
  File ""D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\keras\b
ackend.py"", line 3076, in __call__
    run_metadata=self.run_metadata)
  File ""D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\client\
session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""D:\DeepFaceLab\_internal\bin\lib\site-packages\tensorflow\python\framewo
rk\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocat
ing tensor with shape[90,128,64,64] and type float on /job:localhost/replica:0/t
ask:0/device:GPU:0 by allocator GPU_0_bfc
         [[{{node training/Adam/gradients/conv2d_1/Conv2D_grad/Conv2DBackpropInp
ut}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add repor
t_tensor_allocations_upon_oom to RunOptions for current allocation info.
```

"
26215,option for returning *state* sequences in tf.keras.layers.rnn,"**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No

Currently tf.keras.layers.RNN has an option for returning the sequnces of outputs `return_sequences`, and an option for returning the final state `return_state`.  Sometimes it is useful to analyze the trajectory of c-states for a given sequence of inputs. Therefore I'd like to recommend including an option to `return_state_sequences` where just like `return_sequences` returns a sequence of outputs, `return_state_sequences` returns the sequence of internal states. 
"
26214,No registered 'Placeholder' OpKernel for XLA_TPU_JIT devices compatible with node {{node tpu_139872799811176/input_2}},"Please have a look at my original post here: https://github.com/tensorflow/tensorflow/issues/26081
 It is no longer a tf.keras problem - now I get 

> RuntimeError: Compilation failed: Compilation failure: Detected unsupported operations when trying to compile graph cluster_15366487156777984482[] on XLA_TPU_JIT: Placeholder (No registered 'Placeholder' OpKernel for XLA_TPU_JIT devices compatible with node {{node tpu_139872799811176/input_2}}
> 	.  Registered:  device='TPU'
>   device='CPU'
>   device='GPU'
>   device='XLA_CPU'
> ){{node tpu_139872799811176/input_2}}"
26211,[TF 2.0 API Docs] tf.keras.activations.relu ,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/relu


**Describe the documentation issue**

* The description is minimal, not written with complete sentences, and lacks recommendations of when and when not to use the symbol.

* There is no usage example.

* The parameters are described only briefly and with inconsistent capitalization.

* The returned object description could be more useful.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**

Yes."
26210,app_loader failure at tensorflow.org makes website unusable,"**System information**
- TensorFlow version: N/A
- Doc Link: https://www.tensorflow.org/

My locale is set to uk_UA. When I visit https://www.tensorflow.org/, the app_loader.js tries to load https://www.gstatic.com/devrel-devsite/v138ceba75b8052825cb18f1c6025ea06485e9d36137e0fd82d2d54619f803209/tensorflow/js/devsite_app__uk.js (notice the added '__uk' suffix). The file does not exist. Since devsite_app.js provides the main functionality, none of the links, buttons, and selectors that use JavaScript are operational. This includes the top nav bar, the side nav bar, and the search. It also makes it very hard to navigate to APIs and to browse the API pages. It is not possible to change the language, because the language drop-down is also using JavaScript.

The problem persists across different browsers (Firefox, Safari, Chrome, Vivaldi) and I assume it also affects other locales for which a localised devsite_app.js is not available (for instance, Belarusian be, Croatian hr, etc.)

Ideally, the app_loader should check whether the localised version exists. If not, it should load the unlocalised version instead.

I'm sorry if this is not the right place for the bug. However, it is directly related to the documentation and I could not find any other places to submit.
"
26209,TensorFlow 1.13.1: ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.6 LTS
- TensorFlow installed from (source or binary): Binary (pip3 install -U tensorflow-gpu
- TensorFlow version: current version from pip (maybe version 1.13.1)
- Python version: Python 3.5.2
- Installed using virtualenv? pip? conda?: virtualenv environment with pip
- CUDA/cuDNN version: Cuda 9.0 
- GPU model and memory: Titan Xp



**Describe the problem**
I tried to install tensorflow-gpu follow the instruction at [link](https://www.tensorflow.org/install/gpu) and pip install -U tensorflow gpu. But when I check it `import tensorflow as tf` appear a bug as follow: 

> ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory

Meanwhile, I also tried installing pytorch and succeeded. (Figure below)
![screenshot from 2019-02-28 20-31-09](https://user-images.githubusercontent.com/28798474/53569943-e0417f00-3b97-11e9-8a31-7c23a0f2b481.png)
"
26208,TensorFlow device GPU:0 was not registered,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Red Hat Enterprise Linux Server release 7.4
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10.0
- Python version: 3.6.6
- CUDA/cuDNN version: V9.2.88
- GPU model and memory: Intel(R) Xeon(R) CPU E5-2690 0 @ 2.90GHz

**Describe the current behavior**
Currently I am using distributed Tensorflow to connect two machines one with only a CPU(ps) and one with a CPU and GPU(worker), the worker machine runs only the cluster and server commands followed by a join command and then remains idle, 
the training algorithm is run on the ps machine after defining the cluster and the server and then the training is run but I get ""E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered"" between each epoch of training and it is not run on the gpu (the delay is very big)
**Describe the expected behavior**
the training should be able to run on the GPU of the worker machine

**Code to reproduce the issue**
Code run on the worker machine :
import tensorflow as tf

cluster = tf.train.ClusterSpec({
    ""worker"": [
        ""172.24.145.121:2222"",
    ],
    ""ps"": [
        ""172.24.145.14:2222""
    ]})
server = tf.train.Server(cluster, job_name=""worker"", task_index=0)
server.join()

and on the ps machine:
import tensorflow as tf
import time
from tensorflow.examples.tutorials.mnist import input_data

cluster = tf.train.ClusterSpec({
    ""worker"": [
        ""172.24.145.121:2222"",
    ],
    ""ps"": [
        ""172.24.145.14:2222""
    ]})
server = tf.train.Server(cluster, job_name=""ps"", task_index=0)

tf.logging.set_verbosity(tf.logging.ERROR)
mnist = input_data.read_data_sets(""/tmp/data"",one_hot=True)

n_nodes_hl1=500
n_nodes_hl2=500
n_nodes_hl3=500

n_classes = 10
batch_size = 100
x = tf.placeholder('float',[None, 784],name=""x"")
y = tf.placeholder('float',name=""y_pred"")

def neural_network_model(data):
    with tf.device(""/job:worker/task:0/gpu:0""):
    	hidden_1_layer= {'weights':tf.Variable(tf.random_normal([784, n_nodes_hl1]),name=""layer1_w""),
                    	'biases':tf.Variable(tf.random_normal([n_nodes_hl1]),name=""layer1_b"")}

    	hidden_2_layer= {'weights':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2]),name=""layer2_w""),
                	    'biases':tf.Variable(tf.random_normal([n_nodes_hl2]),name=""layer2_b"")}

    	hidden_3_layer= {'weights':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3]),name=""layer3_w""),
            	        'biases':tf.Variable(tf.random_normal([n_nodes_hl3]),name=""layer3_b"")}

    	output_layer= {'weights':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes]),name=""output_w""),
        	            'biases':tf.Variable(tf.random_normal([n_classes]),name=""output_b"")}

    l1= tf.add(tf.matmul(data,hidden_1_layer['weights']),hidden_1_layer['biases'],name=""l1"")
    l1= tf.nn.relu(l1)

    l2= tf.add(tf.matmul(l1,hidden_2_layer['weights']),hidden_2_layer['biases'],name=""l2"")
    l2= tf.nn.relu(l2)

    l3= tf.add(tf.matmul(l2,hidden_3_layer['weights']),hidden_3_layer['biases'],name=""l3"")
    l3= tf.nn.relu(l3)

    output= tf.add(tf.matmul(l3,output_layer['weights']),output_layer['biases'],name=""output"")

    return output

def train_neural_network(x):
    prediction=neural_network_model(x)
    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction,labels=y))

    optimizer = tf.train.AdamOptimizer().minimize(cost)

    hm_epoch = 10
   with tf.Session(server.target) as sess:
        sess.run(tf.global_variables_initializer())
        for epoch in range(hm_epoch):
            epoch_loss =0 
            start_time = time.time()
            for _ in range(int(mnist.train.num_examples/batch_size)): #(X_train.size/(28*28))
                epoch_x,epoch_y=mnist.train.next_batch(batch_size)  
                _,c = sess.run([optimizer, cost], feed_dict = {x:epoch_x, y:epoch_y})
                epoch_loss += c
            duration = time.time()-start_time
            print('Epoch', epoch+1,'completed out of ',hm_epoch,'loss: ',epoch_loss)
            correct = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))
            accuracy = tf.reduce_mean(tf.cast(correct,'float'))
            print('Accuracy:' ,accuracy.eval({x:mnist.test.images,y:mnist.test.labels}),' Duration: ' ,duration)
train_neural_network(x)

**Other info / logs**
 E tensorflow/core/grappler/clusters/utils.cc:128] Not found: TensorFlow device GPU:0 was not registered"
26207,MirroredStrategy returns AssertionError when run with custom Estimator,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
1.12
- Python version:
3.6
- Bazel version (if compiling from source):
0.17.2
- GCC/Compiler version (if compiling from source):
7.3
- CUDA/cuDNN version:
CUDA 10.0
cuDNN 7.4
- GPU model and memory:
nVidia Geforce GTX 1060 Ti

**Describe the current behavior**

Estimator model fails if passed a tf.contrib.distribute.MirroredStrategy as a parameter to the RunConfig object. 

**Describe the expected behavior**

I am trying to parallelize an Estimator model built from keras model via tf.keras.estimator.model_to_estimator, which works fine unless I pass the train_distribute parameter (of RunConfig) a MirroredStrategy when creating the model. See below, the working RunConfig is commented out, the non-working one not so. My goal is to run the model on the google cloud platform on a couple of GPUs at once, but first I need to get the code running locally. I want to use the mirrored strategy to parallelize the training by splitting the batches across the GPUs.
The input is a tfrecord file with greyscale images of 40x40x1 and 3-class labels. I am intending to use a CNN and RNN on this data but distributed doesn't work with this simple example of a MLP. Any help?

**Code to reproduce the issue**

`

    import tensorflow as tf
    from tensorflow import keras as K
    import argparse
    import random
    
    
    def _parse_function(proto):
    
        # define constants for images
        imsize = 40
        num_channels = 1
    
        # define your tfrecord feature keys
        keys_to_features = {'X': tf.FixedLenFeature([imsize, imsize, num_channels], tf.float32),  # image height, image width, num_channels
                            'Y': tf.FixedLenFeature([], tf.int64)}
        # Load one example
        parsed_features = tf.parse_single_example(proto, keys_to_features)
    
        # extract image and label
        image = parsed_features['X']
        label = tf.cast( parsed_features['Y'], tf.int32 )
        label = tf.add( label, tf.constant( 1, tf.int32 ) )   # add 1 to transform interval [-1,1] to categorical interval [0,2]
        label = tf.one_hot( label, depth=3 )                  # one hot encoding
    
        return image, label
    
    
    def create_dataset( filepath, shuffle_buffer, batch_size, n_epochs, random_seed, num_parall_calls ):
    
        dataset = tf.data.TFRecordDataset( filepath )
        # Maps the parser on every filepath in the array. You can set the number of parallel loaders here
        dataset = dataset.map( _parse_function, num_parallel_calls=num_parall_calls )
        # Set the number of datapoints you want to load and shuffle
        dataset = dataset.shuffle( shuffle_buffer, random_seed ).repeat( n_epochs )
        # Set the batchsize
        dataset = dataset.batch( batch_size )
        # prefetch
        dataset.prefetch( batch_size )
    
        return dataset
    
    
    def SimpleModel( in_shape=(40, 40, 1), n_out=3, dropout_rate=0.3 ):
    
        model = K.models.Sequential()
    
        # fully connected layer
        model.add( K.layers.Flatten( input_shape=in_shape ) )
    
        model.add( K.layers.Dense( 1024, activation='tanh', kernel_regularizer=K.regularizers.l2( 0.01 ) ) )
        model.add( K.layers.Dropout( dropout_rate ) )
    
        model.add( K.layers.Dense( 128, activation='tanh', kernel_regularizer=K.regularizers.l2( 0.01 ) ) )
        model.add( K.layers.Dropout( dropout_rate ) )
    
        # in the end add another dense layer and an output layer
        model.add( K.layers.Dense( 32, activation='tanh', kernel_regularizer=K.regularizers.l2( 0.01 ) ) )
        model.add( K.layers.Dropout( dropout_rate ) )
        model.add( K.layers.Dense( n_out, activation='softmax' ) )
    
        return model
    
    
    if __name__ == ""__main__"":
    
        # get parameters
        parser = argparse.ArgumentParser( description='Keras GC example NN test.' )
        parser.add_argument( '--job-dir', type=str, help='GCS location to write checkpoints and export models' )
        parser.add_argument( '--train_file', type=str, help='GCS location of train data .tfrecord file location.' )
        parser.add_argument( '--vali_file', type=str, help='GCS location validation data .tfrecord file location.' )
        parser.add_argument( '--n_CPU', type=int, help='Number of processes used for reading and parsing the data for model input.' )
        parser.add_argument( '--n_GPU', type=int, help='Number of GPUs used for training the model.' )
        args = parser.parse_args()
    
        # arguments
        n_gpus = args.n_GPU
        num_parallel_processes = args.n_CPU
    
        train_file = args.train_file
        vali_file = args.vali_file
    
        # parameters
        dropout = 0.5
        num_classes = 3  # -1, 0, 1
        image_size = 40
        num_channel = 1
        learning_rate = 0.0001
        epochs = 3
        shuffle_buffer = 50000      # number of samples from which it will sample
        batch_size = 100
        input_size = (image_size, image_size, num_channel)
        checkpoint_steps = 2000
        rseed = int( random.random() * (2 ** 16) )
    
        # number of samples
        num_sets = 6310
        num_sets_vali = 550
        set_length = 1000
        num_samples = num_sets * set_length
        steps_per_epoch = num_samples // batch_size
        num_samples_vali = num_sets_vali * set_length
        steps_per_epoch_vali = num_samples_vali // batch_size
    
        # assemble the model
        train_model = SimpleModel( in_shape=input_size, n_out=num_classes, dropout_rate=dropout )
        optim = tf.train.AdamOptimizer( learning_rate=learning_rate )
        train_model.compile( optimizer=optim,
                             loss='categorical_crossentropy',
                             metrics=['accuracy'] )
    
        tf.logging.set_verbosity( tf.logging.INFO )
        strategy = tf.contrib.distribute.MirroredStrategy( num_gpus=n_gpus )
        runconfig = tf.estimator.RunConfig( model_dir=args.job_dir, save_checkpoints_steps=checkpoint_steps, train_distribute=strategy )
        # runconfig = tf.estimator.RunConfig( model_dir=args.job_dir, save_checkpoints_steps=checkpoint_steps )
    
        # transform model to estimator
        est_train_model = tf.keras.estimator.model_to_estimator( keras_model=train_model, config=runconfig )
    
        train_spec = tf.estimator.TrainSpec( input_fn=lambda: create_dataset( train_file,
                                                                              shuffle_buffer,
                                                                              batch_size,
                                                                              epochs,
                                                                              rseed,
                                                                              num_parallel_processes ),
                                             max_steps=epochs * steps_per_epoch )
    
        eval_spec = tf.estimator.EvalSpec( input_fn=lambda: create_dataset( vali_file,
                                                                            shuffle_buffer,
                                                                            batch_size,
                                                                            epochs,
                                                                            rseed,
                                                                            num_parallel_processes ),
                                           steps=steps_per_epoch_vali,
                                           throttle_secs=10 )
    
        tf.estimator.train_and_evaluate( est_train_model, train_spec, eval_spec )
    

`

**Other info / logs**

INFO:tensorflow:Initializing RunConfig with distribution strategies.
  INFO:tensorflow:Not using Distribute Coordinator.
  INFO:tensorflow:Using the Keras model provided.
  2019-02-28 13:39:21.467595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
  2019-02-28 13:39:21.467913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
  name: GeForce GTX 1060 6GB major: 6 minor: 1 memoryClockRate(GHz): 1.7715
  pciBusID: 0000:01:00.0
  totalMemory: 5.93GiB freeMemory: 5.37GiB
  2019-02-28 13:39:21.467921: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
  2019-02-28 13:39:21.621140: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
  2019-02-28 13:39:21.621157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
  2019-02-28 13:39:21.621160: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
  2019-02-28 13:39:21.621270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5139 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
  INFO:tensorflow:Using config: {'_model_dir': 'output/try1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 2000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true
  graph_options {
    rewrite_options {
      meta_optimizer_iterations: ONE
    }
  }
  , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': <tensorflow.contrib.distribute.python.mirrored_strategy.MirroredStrategy object at 0x7f970c25bba8>, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f970c25bcc0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_distribute_coordinator_mode': None}
  INFO:tensorflow:Not using Distribute Coordinator.
  INFO:tensorflow:Running training and evaluation locally (non-distributed).
  INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 2000 or save_checkpoints_secs None.
  2019-02-28 13:39:21.625686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
  2019-02-28 13:39:21.625718: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
  2019-02-28 13:39:21.625721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
  2019-02-28 13:39:21.625724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
  2019-02-28 13:39:21.625814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/device:GPU:0 with 5139 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
  INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
  INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
  INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
  INFO:tensorflow:Configured nccl all-reduce.
  2019-02-28 13:39:21.651311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
  2019-02-28 13:39:21.651333: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
  2019-02-28 13:39:21.651337: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
  2019-02-28 13:39:21.651339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
  2019-02-28 13:39:21.651435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5139 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
  INFO:tensorflow:Calling model_fn.
  INFO:tensorflow:Error reported to Coordinator: 
  Traceback (most recent call last):
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
      yield
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
      self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
      model_fn_results = self._model_fn(features=features, **kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py"", line 278, in model_fn
      labels)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py"", line 201, in _clone_and_build_model
      optimizer_iterations=global_step)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/models.py"", line 476, in clone_and_build_model
      target_tensors=target_tensors)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
      method(self, *args, **kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 634, in compile
      for loss_tensor in self.losses:
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 667, in losses
      losses = self._unfiltered_losses
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 571, in _unfiltered_losses
      losses += layer.losses
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 377, in losses
      loss_tensor = regularizer()
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 434, in _tag_unconditional
      loss = loss()
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 629, in _loss_for_variable
      with ops.colocate_with(v):
    File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
      return next(self.gen)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4094, in _colocate_with_for_gradient
      with self.colocate_with(op, ignore_existing):
    File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
      return next(self.gen)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4146, in colocate_with
      op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices
      value, dtype=dtype, name=name, as_ref=as_ref)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
      assert not as_ref
  AssertionError
  Traceback (most recent call last):
    File ""/home/aibox/.local/lib/python3.6/site-packages/IPython/core/interactiveshell.py"", line 3267, in run_code
      exec(code_obj, self.user_global_ns, self.user_ns)
    File ""<ipython-input-2-da26aa895fe6>"", line 1, in <module>
      runfile('/home/aibox/Documents/ast-research-dev/R5/R5_nn_emotional_valence_liquid_data/gcloud_trainer/mirroredTest.py', args='--job-dir=output/try1 --train_file=/mnt/DATA/R5_DATA_AI/R5_liquid_tfrecord/R5_6310x1000x40x40_train_data.tfrecords --vali_file=/mnt/DATA/R5_DATA_AI/R5_liquid_tfrecord/R5_550x1000x40x40_vali_data.tfrecords --n_CPU=6 --n_GPU=1', wdir='/home/aibox/Documents/ast-research-dev/R5/R5_nn_emotional_valence_liquid_data/gcloud_trainer')
    File ""/opt/pycharm-community-2018.3.1/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 198, in runfile
      pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
    File ""/opt/pycharm-community-2018.3.1/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
      exec(compile(contents+""\n"", file, 'exec'), glob, loc)
    File ""/home/aibox/Documents/ast-research-dev/R5/R5_nn_emotional_valence_liquid_data/gcloud_trainer/mirroredTest.py"", line 138, in <module>
      tf.estimator.train_and_evaluate( est_train_model, train_spec, eval_spec )
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 471, in train_and_evaluate
      return executor.run()
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 610, in run
      return self.run_local()
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py"", line 711, in run_local
      saving_listeners=saving_listeners)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
      loss = self._train_model(input_fn, hooks, saving_listeners)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1205, in _train_model
      return self._train_model_distributed(input_fn, hooks, saving_listeners)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1316, in _train_model_distributed
      self.config)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/distribute.py"", line 721, in call_for_each_tower
      return self._call_for_each_tower(fn, *args, **kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 556, in _call_for_each_tower
      return _call_for_each_tower(self, fn, *args, **kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 183, in _call_for_each_tower
      coord.join(threads)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 389, in join
      six.reraise(*self._exc_info_to_raise)
    File ""/home/aibox/.local/lib/python3.6/site-packages/six.py"", line 693, in reraise
      raise value
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py"", line 297, in stop_on_exception
      yield
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/mirrored_strategy.py"", line 795, in run
      self.main_result = self.main_fn(*self.main_args, **self.main_kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
      model_fn_results = self._model_fn(features=features, **kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py"", line 278, in model_fn
      labels)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/estimator/keras.py"", line 201, in _clone_and_build_model
      optimizer_iterations=global_step)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/models.py"", line 476, in clone_and_build_model
      target_tensors=target_tensors)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/base.py"", line 474, in _method_wrapper
      method(self, *args, **kwargs)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 634, in compile
      for loss_tensor in self.losses:
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 667, in losses
      losses = self._unfiltered_losses
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py"", line 571, in _unfiltered_losses
      losses += layer.losses
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 377, in losses
      loss_tensor = regularizer()
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 434, in _tag_unconditional
      loss = loss()
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 629, in _loss_for_variable
      with ops.colocate_with(v):
    File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
      return next(self.gen)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4094, in _colocate_with_for_gradient
      with self.colocate_with(op, ignore_existing):
    File ""/usr/lib/python3.6/contextlib.py"", line 81, in __enter__
      return next(self.gen)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 4146, in colocate_with
      op = internal_convert_to_tensor_or_indexed_slices(op, as_ref=True).op
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1307, in internal_convert_to_tensor_or_indexed_slices
      value, dtype=dtype, name=name, as_ref=as_ref)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
      ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    File ""/home/aibox/.local/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/values.py"", line 439, in _tensor_conversion_mirrored
      assert not as_ref
  AssertionError
"
26206,Converting a saved_model to tflite using TensorFlow(Built from source) gives error.,"**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from : source
- TensorFlow version : 1.12
- Python version: 3.6
- Bazel version : 0.19.1
- GCC/Compiler version : 5.4.0

**Describe the current behavior**
Converting a saved_model to tflite using tflite_convert.py gives me the following error. But when I convert the same model using Tensorflow binary I do not find the issue.

**Other info / logs**
` tflite_convert --output_file=./model/lite.tflite --saved_model_dir=./model/saved_model --input_shapes=1 --input_arrays=X_pred --output_arrays=Y_pred

Traceback (most recent call last):
  File ""/home/vinay/venv/tensorflow_src_1.12/bin/tflite_convert"", line 6, in <module>
    from tensorflow.contrib.lite.python.tflite_convert import main
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/__init__.py"", line 48, in <module>
    from tensorflow.contrib import distribute
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/distribute/__init__.py"", line 34, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import TPUStrategy
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/tpu/__init__.py"", line 69, in <module>
    from tensorflow.contrib.tpu.python.ops.tpu_ops import *
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/ops/tpu_ops.py"", line 39, in <module>
    resource_loader.get_path_to_datafile(""_tpu_ops.so""))
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/contrib/util/loader.py"", line 56, in load_op_library
    ret = load_library.load_op_library(path)
  File ""/home/vinay/venv/tensorflow_src_1.12/lib/python3.6/site-packages/tensorflow/python/framework/load_library.py"", line 60, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
**tensorflow.python.framework.errors_impl.InvalidArgumentError: Unrecognized type '**
An op that loads optimization parameters into HBM for embedding. Must be
preceded by a ConfigureTPUEmbeddingHost op that sets up the correct
embedding table configuration. For example, this op is used to install
parameters that are loaded from a checkpoint before a training loop is
executed.

parameters: A tensor containing the initial embedding table parameters to use in embedding
lookups using the Adagrad optimization algorithm.
accumulators: A tensor containing the initial embedding table accumulators to use in embedding
lookups using the Adagrad optimization algorithm.
table_name: Name of this table; must match a name in the
  TPUEmbeddingConfiguration proto (overrides table_id).
num_shards: Number of shards into which the embedding tables are divided.
shard_id: Identifier of shard for this operation.
table_id: Index of this table in the EmbeddingLayerConfiguration proto
  (deprecated).
' in attr '
An op that loads optimization parameters into HBM for embedding. Must be
preceded by a ConfigureTPUEmbeddingHost op that sets up the correct
embedding table configuration. For example, this op is used to install
parameters that are loaded from a checkpoint before a training loop is
executed.

parameters: A tensor containing the initial embedding table parameters to use in embedding
lookups using the Adagrad optimization algorithm.
accumulators: A tensor containing the initial embedding table accumulators to use in embedding
lookups using the Adagrad optimization algorithm.
table_name: Name of this table; must match a name in the
  TPUEmbeddingConfiguration proto (overrides table_id).
num_shards: Number of shards into which the embedding tables are divided.
shard_id: Identifier of shard for this operation.
table_id: Index of this table in the EmbeddingLayerConfiguration proto
  (deprecated).
'; in OpDef: name: ""LoadTPUEmbeddingAdagradParameters"" input_arg { name: ""parameters"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: DT_FLOAT type_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" number_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type_list_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } input_arg { name: ""accumulators"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: DT_FLOAT type_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" number_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type_list_attr: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" default_value { i: -1 } description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" has_minimum: true minimum: -1 } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" default_value { s: """" } description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } attr { name: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" type: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" } summary: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" description: ""\nAn op that loads optimization parameters into HBM for embedding. Must be\npreceded by a ConfigureTPUEmbeddingHost op that sets up the correct\nembedding table configuration. For example, this op is used to install\nparameters that are loaded from a checkpoint before a training loop is\nexecuted.\n\nparameters: A tensor containing the initial embedding table parameters to use in embedding\nlookups using the Adagrad optimization algorithm.\naccumulators: A tensor containing the initial embedding table accumulators to use in embedding\nlookups using the Adagrad optimization algorithm.\ntable_name: Name of this table; must match a name in the\n  TPUEmbeddingConfiguration proto (overrides table_id).\nnum_shards: Number of shards into which the embedding tables are divided.\nshard_id: Identifier of shard for this operation.\ntable_id: Index of this table in the EmbeddingLayerConfiguration proto\n  (deprecated).\n"" is_stateful: true`



Thanks in advance.!"
26205,How to run the examples/android in Windows Environment?,"**System information**
Windows 8 cpu
- TensorFlow installed from (source or binary):
- TensorFlow version:1.12.0
- Python version:3.6
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source):no


**Describe the problem**
I have searched a huge amounts of information about how to use this example but the problem is that most of people are using it on the linux OS. I don't know how to run it on my Windows OS with android studio. Need I install .jar & .so file?And how to deploy the android studio?

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26204,[TF2.0] Broadcasting,"Hello everyone,

I would like resuscitate very old issue. Actually, it is so old that even github's autocompletion doesn't offer it after typing ""#"" - https://github.com/tensorflow/tensorflow/issues/216. This request was raised several times, but still it hasn't been resolved.

In short, *broadcasting* interface is not ""good enough"" in TensorFlow :)

Lets first check how broadcasting works in `numpy`:

```python
In [1]: import numpy as np
In [2]: a = np.random.rand(2, 3, 4)
In [3]: b = np.random.rand(4, 5)
In [4]: a @ b
Out[4]:
array([[[1.42709275, 1.40630067, 0.46525725, 0.68734581, 0.65227036],
        [2.01336504, 1.59980866, 0.93739699, 0.63190484, 0.92472892],
        [1.82979902, 1.46193243, 0.85498406, 0.5994646 , 0.77767957]],

       [[1.83010035, 1.49088728, 0.76694665, 0.65568003, 0.89110954],
        [2.12214864, 1.41728107, 1.04566743, 0.60652825, 0.97115822],
        [2.32478779, 2.06297214, 1.02016205, 0.81821249, 1.02604722]]])
```

Now, let's check what the TF is offering:

```python
In [25]: a = tf.random.normal((2, 3, 4))
In [26]: b = tf.random.normal((4, 5))
In [27]: a @ b
... InvalidArgumentError: In[0] is not a matrix. Instead it has shape [2,3,4] [Op:MatMul] name: matmul/
```

Ouch! The ""correct"" way of doing it in the TF (of course there are other) is:

```python
In [26]: a = tf.random.normal((2, 3, 4))
In [27]: b = tf.random.normal((4, 5))
In [28]: a @ tf.broadcast_to(b, tf.concat([a.shape[:-2], b.shape], axis=0))
<tf.Tensor: id=87, shape=(2, 3, 5), dtype=float32, numpy=
array([[[ 1.1977772 , -1.363074  ,  1.8021748 ,  0.1448586 , -0.6269997 ],
        [ 1.2322128 , -2.1586194 ,  0.09486479,  0.02937585, 0.9694344 ],
        [ 0.5580032 ,  6.11664   , -0.24535722,  0.16691092, -2.2263217 ]],
       [[-0.7386743 ,  1.2142425 ,  1.1371945 , -1.2736351 , -2.971829  ],
        [-1.9222848 , -0.7198772 , -0.9807504 ,  0.02805561, 1.0210879 ],
        [ 1.8334148 ,  0.80895233,  1.2308785 , -0.23910654, -1.5128168 ]]], dtype=float32)>
```

You can see how much effort it requires to make operations broadcastable for two distinct tensors: extract leading shape from the left tensor, extract shape from the right tensors, concatenate these shapes with correct axis, call `tf.broadcast_to`...

The same applies to cholesky, triangular solve and other operations. That is very upsetting that such a crucial feature isn't available out of the box.

Another concern is the performance of these ""solutions"". E.g. memory consumption for tiling and `broadcast_to` operations, as they simply copy the tensor to match leading dimensions. Of course, native TensorFlow broadcasting implementation would be preferable in this case.

Kind regards,
Artem Artemev


"
26203,tf.graph_util.convert_variables_to_constants converts a different  pb parameters with orginal ckpt,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
**The result using 'tf.graph_util.convert_variables_to_constants' api is different with 'freeze_graph.freeze_graph'**
when i use 'freeze_graph.freeze_graph',  the result is same as ckpt but different with the output when i use use 'tf.graph_util.convert_variables_to_constants' api. Why???

```python
# using 'tf.graph_util.convert_variables_to_constants' api
    with tf.get_default_graph().as_default():
        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name = 'input_images')
        

        input_images = readdata.mean_image_subtraction(input_images)

        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=1e-5)):
            outputs, _ = resnet_v1.resnet_v1_50(input_images,  is_training=False, scope='resnet_v1_50', num_classes=FLAGS.num_classes)
            saver = tf.train.Saver()

            with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
                tf.train.write_graph(sess.graph.as_graph_def(), '.', 'resnet50.pbtxt', as_text=False)
                saver.restore(sess, '../model.ckpt-5723')

                pb_file_path = 'model_5723.pb'
                constant_graph = tf.graph_util.convert_variables_to_constants(sess, sess.graph_def,[""resnet_v1_50/predictions/Softmax""])

                with tf.gfile.FastGFile(pb_file_path, mode='wb') as f:
                    f.write(constant_graph.SerializeToString())
```
```python
#using 'freeze_graph' api
    with tf.get_default_graph().as_default():
        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name = 'input_images')
        

        input_images = readdata.mean_image_subtraction(input_images)

        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=1e-5)):
            outputs, _ = resnet_v1.resnet_v1_50(input_images,  is_training=False, scope='resnet_v1_50', num_classes=FLAGS.num_classes)

            saver = tf.train.Saver()

            with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:
                tf.train.write_graph(sess.graph.as_graph_def(), '.', 'resnet50.pbtxt', as_text=False)

                freeze_graph.freeze_graph('./resnet50.pbtxt', """", True, '../model.ckpt-5723', ""resnet_v1_50/predictions/Softmax"", ""save/restore_all"", ""save/Const:0"",""model_freeze.pb"", True, """")
                
```
**Describe the expected behavior**
using 'tf.graph_util.convert_variables_to_constants' api , the output parameters should be same as ckpt.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26202,Cannot run tensorflow against tensorrt,"I'm trying to run tensorflow against tensorrt but looks this always runs with TF.
""TensorRT node TRTEngineOp_0 added for segment 0 consisting of 446 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...""

**System information**
I'm trying to run the one example file: models/research/tensorrt/tensorrt.py

- VERSION=""18.04.2 LTS (Bionic Beaver)""
- one x86-based target
- TensorFlow installed from (source or binary):
source
- TensorFlow version (use command below):
r1.13 

- Python version:
Python 2.7.15rc1

- Bazel version (if compiling from source):
Build label: 0.21.0

- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:

root@96c10be1cfd1:/# dpkg -l | grep TensorRT
ii  graphsurgeon-tf                                             5.0.2-1+cuda10.0                  amd64        GraphSurgeon for TensorRT package
ii  libnvinfer-dev                                              5.0.2-1+cuda10.0                  amd64        TensorRT development libraries and headers
ii  libnvinfer-samples                                          5.0.2-1+cuda10.0                  all          TensorRT samples and documentation
ii  libnvinfer5                                                 5.0.2-1+cuda10.0                  amd64        TensorRT runtime libraries
ii  tensorrt                                                    5.0.2.6-1+cuda10.0                amd64        Meta package of TensorRT
ii  uff-converter-tf                                            5.0.2-1+cuda10.0                  amd64        UFF converter for TensorRT package
- GPU model and memory:
root@96c10be1cfd1:/# nvidia-smi     
Thu Feb 28 09:59:21 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 418.43       Driver Version: 418.43       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GT 710      Off  | 00000000:03:00.0 N/A |                  N/A |
| 40%   44C    P0    N/A /  N/A |      0MiB /  2002MiB |     N/A      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0                    Not Supported                                       |
+-----------------------------------------------------------------------------+

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
root@f31df1cbe0b4:/home/workspace/models/research/tensorrt# python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb --image_file=image.jpg --int8 --output_dir=./output/               
2019-02-28 09:52:50.839058: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 1915900000 Hz
2019-02-28 09:52:50.849051: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bed0da2f90 executing computations on platform Host. Devices:
2019-02-28 09:52:50.849128: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-28 09:52:51.274853: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-28 09:52:51.276663: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55bece26afb0 executing computations on platform CUDA. Devices:
2019-02-28 09:52:51.276744: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GT 710, Compute Capability 3.5
2019-02-28 09:52:51.277347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce GT 710 major: 3 minor: 5 memoryClockRate(GHz): 0.954
pciBusID: 0000:03:00.0
totalMemory: 1.96GiB freeMemory: 1.93GiB
2019-02-28 09:52:51.277411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-28 09:52:51.896007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 09:52:51.896085: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-28 09:52:51.896118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-28 09:52:51.897500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1001 MB memory) -> physical GPU (device: 0, name: GeForce GT 710, pci bus id: 0000:03:00.0, compute capability: 3.5)
WARNING:tensorflow:From tensorrt.py:210: __init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.gfile.GFile.
Running INT8 graph
INFO:tensorflow:Running against TensorRT version 5.0.2
2019-02-28 09:52:56.511405: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 0
2019-02-28 09:52:56.519624: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-02-28 09:52:56.520961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-28 09:52:56.521047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-28 09:52:56.521081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-28 09:52:56.521113: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-28 09:52:56.521594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1001 MB memory) -> physical GPU (device: 0, name: GeForce GT 710, pci bus id: 0000:03:00.0, compute capability: 3.5)
2019-02-28 09:53:00.177209: I tensorflow/contrib/tensorrt/segment/segment.cc:443] There are 4 ops of 3 different types in the graph that are not converted to TensorRT: Softmax, NoOp, Placeholder, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).
2019-02-28 09:53:00.312249: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:913] Number of TensorRT candidate segments: 1
2019-02-28 09:53:16.314935: W tensorflow/core/common_runtime/bfc_allocator.cc:267] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.00GiB.  Current allocation summary follows.
2019-02-28 09:53:16.315102: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (256):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315139: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (512):   Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315302: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1024):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315345: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2048):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315386: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4096):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315421: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8192):  Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315461: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16384):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315496: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (32768):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315531: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (65536):         Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315572: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (131072):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315647: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (262144):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315809: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (524288):        Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.315962: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (1048576):       Total Chunks: 1, Chunks in use: 0. 1.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316172: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (2097152):       Total Chunks: 1, Chunks in use: 0. 2.00MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316368: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (4194304):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316528: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (8388608):       Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316677: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (16777216):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316758: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (33554432):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316794: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (67108864):      Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.316883: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (134217728):     Total Chunks: 2, Chunks in use: 2. 256.00MiB allocated for chunks. 256.00MiB in use in bin. 147.00MiB client-requested in use in bin.
2019-02-28 09:53:16.317046: I tensorflow/core/common_runtime/bfc_allocator.cc:597] Bin (268435456):     Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2019-02-28 09:53:16.317088: I tensorflow/core/common_runtime/bfc_allocator.cc:613] Bin for 2.00GiB was 256.00MiB, Chunk State: 
2019-02-28 09:53:16.317155: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x400de0000 of size 1048576
2019-02-28 09:53:16.317186: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Free  at 0x400fe0000 of size 2097152
2019-02-28 09:53:16.317217: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x401600000 of size 134217728
2019-02-28 09:53:16.317248: I tensorflow/core/common_runtime/bfc_allocator.cc:632] Chunk at 0x409600000 of size 134217728
2019-02-28 09:53:16.317282: I tensorflow/core/common_runtime/bfc_allocator.cc:638]      Summary of in-use Chunks by size: 
2019-02-28 09:53:16.317339: I tensorflow/core/common_runtime/bfc_allocator.cc:641] 2 Chunks of size 134217728 totalling 256.00MiB
2019-02-28 09:53:16.317379: I tensorflow/core/common_runtime/bfc_allocator.cc:645] Sum Total of in-use chunks: 256.00MiB
2019-02-28 09:53:16.317418: I tensorflow/core/common_runtime/bfc_allocator.cc:647] Stats: 
Limit:                  1049657344
InUse:                   268435456
MaxInUse:                268435456
NumAllocs:                      15
MaxAllocSize:            134217728

2019-02-28 09:53:16.317461: W tensorflow/core/common_runtime/bfc_allocator.cc:271] _*****************************xxxxxxxxxxxxxxxxxxxx*****************************xxxxxxxxxxxxxxxxxxxxx
2019-02-28 09:53:16.317514: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (129) - OutOfMemory Error in GpuMemory: 0
2019-02-28 09:53:16.330037: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger GPU memory allocation failed during tactic selection for layer: (Unnamed Layer* 0) [Shuffle]
2019-02-28 09:53:16.334382: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger ./resources.h (129) - OutOfMemory Error in GpuMemory: 0
2019-02-28 09:53:16.335317: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:1021] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 446 nodes failed: Internal: Failed to build TensorRT engine. Fallback to TF...
2019-02-28 09:53:16.503925: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] Optimization results for grappler item: tf_graph
2019-02-28 09:53:16.504025: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 451 nodes (-253), 466 edges (-253), time = 1914.823ms.
2019-02-28 09:53:16.504057: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   layout: Graph size after: 451 nodes (0), 466 edges (0), time = 210.214ms.
2019-02-28 09:53:16.504087: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 451 nodes (0), 466 edges (0), time = 304.044ms.
....

**Describe the expected behavior**
Hope this can work.

**Code to reproduce the issue**
https://github.com/tensorflow/models/tree/master/research/tensorrt and then I just run that example file:
python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb \
  --image_file=image.jpg --native --fp32 --fp16 --int8 --output_dir=./output

Even I only run that with --int8
python tensorrt.py --frozen_graph=resnetv2_imagenet_frozen_graph.pb \
  --image_file=image.jpg --int8 --output_dir=./output

"
26201,The procedure of building the TF r1.13 is not clear. (CUDA10.0 + cuDNN7.5.0+TensorRT5.0.2.6),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version: r1.13
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip3
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): 4.9.3
- CUDA/cuDNN version: 10.0/7.5.0
- GPU model and memory: TitanXP

**Describe the problem**
The Tensorflow r1.13 source code is get from [releases](https://github.com/tensorflow/tensorflow/releases).
Then I tried to build tf 1.13 with cuda10.0 and cudnn7.5.0 and TensorRT5.0.2.6.
After the configuration, I tried command 
`bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package`
But get below error:
```
$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=176
ERROR: Config value opt is not defined in any .rc file

```
1) Is bazel 0.19.1 not compatible with TF1.13?
2) Is my configuration incorrect?
3) Is the build command incorrect?



Below shows my configure:
```
$ ./configure
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.1 installed.
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: n
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /home/web_server/xiaolun/cuda-10.0


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: y
TensorRT support will be enabled for TensorFlow.

Please specify the location where TensorRT is installed. [Default is /usr/lib/x86_64-linux-gnu]:/home/web_server/xiaolun/TensorRT-5.0.2.6-10.0


Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 2.3


Please specify the location where NCCL 2 library is installed. Refer to README.md for more details. [Default is /home/web_server/xiaolun/cuda-10.0]:


NCCL found at /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/libnccl.so.2
Assuming NCCL header path is /media/disk1/fordata/web_server/project/xiaolun/cuda-10.0/lib64/../include/nccl.h
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 7.0


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /home/web_server/gcc-4.9.3/bin/gcc]: 


Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apacha Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished

```
"
26200,tf.nn.ctc_beam_search_decoder is very slow on tf >=1.4,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): tried 1.4, 1.5, 1.8, 1.9, 1.12
- Python version: 3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA8 for 1.4, CUDA9 for >=1.5
- GPU model and memory:P100

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Our text recognizer is based on CRNN model  and CTC. it was developed based on tf 1.3 and performs well.
recently we tried to upgrade the tensorflow to tf 1.9 and find the recognizer inference time increases a lot.
we identified the slow down is caused by tf.nn.ctc_beam_search_decoder op using this way:

for this line of code:
decoded, prob = tf.nn.ctc_beam_search_decoder(logits, seq_length)

running time for session.run(logits) is fine, however session.run(decoded) is very slow

we verified the problem occurrs on tf versions 1.12, 1.9, 1.8, 1.5, 1.4
we also verified the model ckpt trained with tf 1.9 is running inference fast with tf 1.3. similarly, model ckpt trained with tf 1.4/1.5 also running fast with tf 1.3

it looks there is no big change for beam search decoder op between 1.3 and 1.4. googling the problem got no luck either.

can you please take a look?
we could provide the pb file and a small inference script if needed. since pb file size is big, we do not upload it at this point
thanks.

**Describe the expected behavior**


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26199,Tensorflow lite model requests a buffer bigger than the neccesary,"Hi, I created a custom model using keras in tensorflow. The version that I used was tensorflow nightly 1.13.1. I used the official tool to build the tensorflow lite model (the method tf.lite.TFLiteConverter.from_keras_model_file ).

After I created the model I reviewed the input shape and nothing seems is bad.

The input and output shapes in tensorflow lite model are:

[{'name': 'input_1', 'index': 59, 'shape': array([  1, 240, 240,   3], dtype=int32), 'dtype': , 'quantization': (0.0, 0)}]

[{'name': 'dense/Softmax', 'index': 57, 'shape': array([1, 6], dtype=int32), 'dtype': , 'quantization': (0.0, 0)}]

you can note that input shape is 1 * 240 * 240 * 3 so I expected that the buffer would have a size of 172800 units.

However, when I try to run the model in an android device I received the next error:

E/AndroidRuntime: FATAL EXCEPTION: main
    Process: com.megacode, PID: 15067
    java.lang.RuntimeException: Unable to create application com.megacode.base.ApplicationBase: java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 691200 bytes and a ByteBuffer with 172800 bytes.
        at android.app.ActivityThread.handleBindApplication(ActivityThread.java:5771)
        at android.app.ActivityThread.-wrap2(ActivityThread.java)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1648)
I don't understand the reason why the model request an input shape of 691200 units.

If someone has a suggestion I would appreciate it"
26198,Errors while building Tensorflow 1.12,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Void Linux glibc x86_64
- TensorFlow installed from (source or binary): Source
- TensorFlow version: r1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip, no virtualenv
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: No
- GPU model and memory: None



**Describe the problem**

Hello guys, I'm trying to build TF in a non Debian based distro. But what I'm getting is this error, as referrenced by [this windows build from source](https://github.com/tensorflow/tensorflow/issues/24721), especially related to eigen.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
bazel build --incompatible_remove_native_http_archive=false --config=opt --copt=-mavx --copt=-mavx2 --config=monolithic //tensorflow/tools/pip_package:build_pip_package
INFO: Invocation ID: 851c2110-08b2-4ed8-a7b0-4c995b8388e9
DEBUG: /home/neel/.cache/bazel/_bazel_neel/e655abc18ce719a580533b95739e1032/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

INFO: Build options --compiler and --cpu have changed, discarding analysis cache.
ERROR: /home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/eigen3/BUILD:34:1: no such package '@eigen_archive//': Traceback (most recent call last):
	File ""/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl"", line 106
		_apply_patch(ctx, ctx.attr.patch_file)
	File ""/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl"", line 68, in _apply_patch
		fail(""patch command is not found, ple..."")
patch command is not found, please install it and referenced by '//third_party/eigen3:eigen3'
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: no such package '@eigen_archive//': Traceback (most recent call last):
	File ""/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl"", line 106
		_apply_patch(ctx, ctx.attr.patch_file)
	File ""/home/neel/Programming_Workspace/Not-My-Repos/tensorflow/third_party/repo.bzl"", line 68, in _apply_patch
		fail(""patch command is not found, ple..."")
patch command is not found, please install it
INFO: Elapsed time: 3.142s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (42 packages loaded, 470 targets c\
onfigured)
    currently loading: tensorflow/core/kernels
    Fetching @eigen_archive; fetching
    Fetching @double_conversion; fetching
```

**Any other info / logs**

I just cloned the latest repo from git and checked out TF r1.12 last commit. 
After that I passed only CPU options to ""configure"" and also had to install pip3.
I also installed eigen3 package from my distro's repos, but no help.
I don't know what to do, eigen does seem to be in the TF/tools/ folder but bazel doesn't import it. Please help !!

Here's the folder structure of ""third_parties"" folder:
```
ls ./third_party/
android                     libxsmm.BUILD
arm_neon_2_x86_sse.BUILD    linenoise.BUILD
astor.BUILD                 llvm
aws                         lmdb.BUILD
backports_weakref.BUILD     mkl
boringssl                   mkl_dnn
BUILD                       mpi
clang_toolchain             mpi_collectives
codegen.BUILD               nanopb.BUILD
com_google_absl.BUILD       nasm
common.bzl                  nccl
cub.BUILD                   ngraph
curl.BUILD                  ortools
cython.BUILD                pasta
double_conversion.BUILD     pcre.BUILD
eigen3                      png.BUILD
eigen.BUILD                 png_fix_rpi.patch
enum34.BUILD                pprof.BUILD
examples                    protobuf
farmhash.BUILD              py
fft2d                       python_runtime
flatbuffers                 repo.bzl
gast.BUILD                  six.BUILD
gif.BUILD                   snappy.BUILD
git                         sqlite.BUILD
googleapis.BUILD            swig.BUILD
gpus                        sycl
grpc                        systemlibs
hadoop                      tensorrt
highwayhash                 termcolor.BUILD
hwloc                       tflite_mobilenet.BUILD
icu                         tflite_mobilenet_float.BUILD
jpeg                        tflite_mobilenet_quant.BUILD
jsoncpp.BUILD               tflite_ovic_testdata.BUILD
kafka                       tflite_smartreply.BUILD
keras_applications_archive  toolchains
kissfft                     zlib.BUILD
```
Heres my git branches:
```
git branch
  master
* r1.12

git show --oneline r1.12

a6d8ffae09 (HEAD -> r1.12, tag: v1.12.0) Fix a bug in tpu.py and xla.py that while creating an identity node for control input edges under rewrite context, the parent control flow context is lost. (#23446)
diff --git a/tensorflow/contrib/compiler/xla.py b/tensorflow/contrib/compiler/xla.py
index 873b03580d..83d9d8c54a 100644
--- a/tensorflow/contrib/compiler/xla.py
+++ b/tensorflow/contrib/compiler/xla.py
@@ -179,14 +179,11 @@ class XLACompileContext(control_flow_ops.XLAControlFlowContext):
     if external_control_inputs:
       # Use an identity to pull control inputs as data inputs. Note that we
       # ignore ops which don't have outputs. TODO(phawkins): fix that.
-      with ops.control_dependencies(None):
-        self.Enter()
-        external_control_inputs = [
-            array_ops.identity(x.outputs[0]).op
-            for x in external_control_inputs
-            if x.outputs
-        ]
-        self.Exit()
+      external_control_inputs = [
+          array_ops.identity(x.outputs[0]).op
"
26197,[TF 2.0 API Docs] tf.keras,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>
**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras

**Describe the documentation issue**

**Links** 
""Defined in python/keras/api/_v2/keras/__init__.py"" is pointing to a broken link:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/api/_v2/keras/__init__.py

**Description**
Currently it has ""Detailed documentation and user guides are available at keras.io.""
It would be better to point users to https://www.tensorflow.org/guide/keras, otherwise the experience feels broken.

It would be great to list some key differences between tf.keras vs Keras as an independent project (keras.io). Or at least point out there is no 1:1 mapping and what each one has and the other one doesn't have. This has been discussed in blog posts and forums but the official documentation should at least have a high level overview with a few sentences.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes
"
26196,TF 1.13.1 on Raspberry Pi 3B+ __init__.py gives an error: No module named 'tensorflow.python'. How to use static library file?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian 9.8
- Mobile device if the issue happens on mobile device: Raspberry Pi Model 3 B+
- TensorFlow installed from (source or binary): Source I think
- TensorFlow version: 1.13.1
- Python version: 3.5
- Installed using virtualenv? pip? conda?: Native Compile
- Bazel version (if compiling from source): Not Applicable
- GCC/Compiler version (if compiling from source): 6.3.0 20170516
- CUDA/cuDNN version: Not Applicable
- GPU model and memory: Not Applicable



**Describe the problem**
I followed [this guide](https://www.tensorflow.org/lite/rpi) on how to compile TensorFlow Lite Natively on the Raspberry Pi. I cloned the tensorflow repo on the home folder, and then executed the scripts accordingly. I had to adjust the swapsize [as descrbied here](https://github.com/tensorflow/tensorflow/issues/26158#issuecomment-468089490), for the Compile to successfully run.

Back in the home folder, when I try to run:

` ~$ python3`
`>>> import tensorflow as tf`

I don't get any errors.

My goal is to run Object Detection on RaspberryPi using TF-Lite, [and I found this sample code](https://github.com/freedomtan/tensorflow/blob/deeplab_tflite_python/tensorflow/contrib/lite/examples/python/object_detection.py)

But when I tried to do his tensorflow import:

`>>> from tensorflow.contrib.lite.python import interpreter as interpreter_wrapper`

It didn't work because of course, in TF 1.13.1, the `lite` folder is no longer under `contrib`

But when I do:

`>>> from tensorflow.lite.python import interpreter as interpreter_wrapper`

I get: `ImportError: No module named 'tensorflow.lite'`

But when I do:

`>>> from tensorflow.tensorflow.lite.python import interpreter as interpreter_wrapper`

I get:

> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/pi/tensorflow/tensorflow/__init__.py"", line 24, in <module>
>     from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
> ImportError: No module named 'tensorflow.python'

When going to the `__init__.py` file (located in ~/tensorflow/tensorflow), the line in question is:
> from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

The error then becomes:

> Traceback (most recent call last):
>   File ""<stdin>"", line 1, in <module>
>   File ""/home/pi/tensorflow/tensorflow/__init__.py"", line 25, in <module>
>     from tensorflow.tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
>   File ""/home/pi/tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
>    from tensorflow.python import pywrap_tensorflow
> ImportError: No module named 'tensorflow.python'

It's clear that in python code, I have to import Tensorflow as if I'm navigating through the file system.

Having downloaded the tensorflow repo to the home folder, and executing the python3 command from there, importing the tensorflow code had me going through the file structure.

And now that the __init__.py file is encountering problems in importing, I'm not sure how to edit the import paths, and I don't think I should. 

I have a feeling that I did not compile tensorflow properly.

How are we even supposed to use the libtensorflow-lite.a static library file?

**Provide the exact sequence of commands / steps that you executed before running into the problem**

1. Go get yourself an RPi, install Raspbian (not through NOOBS).
2. Download the Tensorflow repo in the home folder. Adjust your swapsize [by doing this](https://github.com/tensorflow/tensorflow/issues/26158#issuecomment-468089490). Then follow [the instructions in this link](https://www.tensorflow.org/lite/rpi)
3. After generating the static library, go back to the home folder and run python3 and the commands I tried above.


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The complete logs are above. 
"
26191,warnings (please do not import '@grpc//third_party/nanopb:pb_common.c' directly ; depends on deprecated target ; ...) during build from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: 9.0 / 7.5.0
- GPU model and memory: GeForce GT 650M / 2 GB



**Describe the problem**
I try to compile TF from source to get compute capability 3.0 support.
Therefore I followed more or less the guide on [this site](https://medium.com/@mccann.matt/compiling-tensorflow-with-cuda-3-0-support-42d8fe0bf3b5).
But I get warnings and the compilation failed.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./configure

bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package

(see details below)

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Not sure if important but I added `import /home/jonathan/tensorflow/tools/bazel.rc` on top line of(hide file) ""/home/jonathan/tensorflow/.bazelrc "" as stated [here](https://github.com/tensorflow/tensorflow/issues/23401#issuecomment-435827786).



```
$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.2 installed.
Please specify the location of python. [Default is /home/jonathan/anaconda2/envs/tf_cu90/bin/python]: 


Found possible Python library paths:
  /home/jonathan/anaconda2/envs/tf_cu90/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/jonathan/anaconda2/envs/tf_cu90/lib/python3.6/site-packages]

Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: 
Apache Ignite support will be enabled for TensorFlow.

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 


Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/local/cuda-9.0


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda-9.0]: /usr/lib/x86_64-linux-gnu


Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, then you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. [Default is 2.2]: 1.3


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.0]: 


Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: /usr/bin/gcc-6


Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See tools/bazel.rc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
Configuration finished

```

```
$ bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package

Loading: 
Loading: 0 packages loaded
Analyzing: target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured)
WARNING: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/jonathan/.cache/bazel/_bazel_jonathan/e7c09fc463511989ded3d56396c466d4/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jonathan/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
[0 / 4] [-----] ProtoCompile tensorflow/core/example/example_pb2.py
[5 / 21] Compiling tensorflow/core/ops/nn_ops.cc [for host]; 2s local ... (8 actions running)
[6 / 22] Compiling tensorflow/core/ops/nn_ops.cc [for host]; 6s local ... (7 actions running)
[13 / 35] Compiling tensorflow/core/ops/nn_ops.cc [for host]; 9s local ... (8 actions running)
[17 / 37] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/routing_gradient_op.cc; 6s local ... (8 actions running)
[24 / 46] Compiling tensorflow/contrib/tensor_forest/hybrid/core/ops/k_feature_gradient_op.cc [for host]; 6s local ... (8 actions running)
[38 / 73] Compiling tensorflow/python/framework/python_op_gen_internal.cc [for host]; 7s local ... (8 actions, 7 running)
INFO: From Compiling tensorflow/python/framework/python_op_gen_internal.cc [for host]:
tensorflow/python/framework/python_op_gen_internal.cc: In member function 'virtual std::__cxx11::string tensorflow::python_op_gen_internal::GenPythonOp::Code()':
tensorflow/python/framework/python_op_gen_internal.cc:542:44: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = op_def_.input_arg_size(); i < params_no_default.size(); ++i) {
                                          ~~^~~~~~~~~~~~~~~~~~~~~~~~~~
tensorflow/python/framework/python_op_gen_internal.cc:545:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < params_with_default.size(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~
INFO: From Compiling tensorflow/python/framework/python_op_gen.cc [for host]:
tensorflow/python/framework/python_op_gen.cc: In function 'std::__cxx11::string tensorflow::{anonymous}::VectorToTuple(const std::vector<std::__cxx11::basic_string<char> >&)':
tensorflow/python/framework/python_op_gen.cc:65:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < l.size(); ++i) {
                   ~~^~~~~~~~~~
tensorflow/python/framework/python_op_gen.cc: In function 'void tensorflow::{anonymous}::Unflatten(const string&, const std::vector<std::__cxx11::basic_string<char> >&, const string&, std::__cxx11::string*)':
tensorflow/python/framework/python_op_gen.cc:77:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   for (int i = 0; i < output_sizes.size(); ++i) {
                   ~~^~~~~~~~~~~~~~~~~~~~~

[...]


ERROR: /home/jonathan/tensorflow/tensorflow/core/kernels/BUILD:2951:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_bitwise_and.cu.pic.o' was not created
INFO: From Compiling tensorflow/core/kernels/cwise_op_gpu_ceil.cu.cc [for host]:
./tensorflow/core/kernels/cwise_ops.h(190): warning: __host__ annotation on a defaulted function(""scalar_left"") is ignored

./tensorflow/core/kernels/cwise_ops.h(190): warning: __device__ annotation on a defaulted function(""scalar_left"") is ignored

./tensorflow/core/kernels/cwise_ops.h(220): warning: __host__ annotation on a defaulted function(""scalar_right"") is ignored

./tensorflow/core/kernels/cwise_ops.h(220): warning: __device__ annotation on a defaulted function(""scalar_right"") is ignored

./tensorflow/core/kernels/cwise_ops.h(190): warning: __host__ annotation on a defaulted function(""scalar_left"") is ignored

./tensorflow/core/kernels/cwise_ops.h(190): warning: __device__ annotation on a defaulted function(""scalar_left"") is ignored

./tensorflow/core/kernels/cwise_ops.h(220): warning: __host__ annotation on a defaulted function(""scalar_right"") is ignored

./tensorflow/core/kernels/cwise_ops.h(220): warning: __device__ annotation on a defaulted function(""scalar_right"") is ignored

ERROR: /home/jonathan/tensorflow/tensorflow/core/kernels/BUILD:2951:1: output 'tensorflow/core/kernels/_objs/cwise_op_gpu/cwise_op_gpu_mul.cu.pic.o' was not created
ERROR: /home/jonathan/tensorflow/tensorflow/core/kernels/BUILD:2951:1: not all outputs were created or valid
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 10303.024s, Critical Path: 9226.48s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 1813 processes: 1813 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

for full log-file see 
[log.txt](https://github.com/tensorflow/tensorflow/files/2913016/log.txt)"
26189,Side effects induced by attribute and slice operators must be limited in Python control flow.,"With `autograph=True`, wrapping ConvND in a `tf.function` works as expected with `use_bias=True`; but `use_bias=False` would fail. Code works as expected with `autograph=True`.

Error message received:

```
AttributeError: 'ConvND' object has no attribute 'b'.
```"
26183,bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
$ uname -a
Linux e1.carbonate.uits.iu.edu 3.10.0-862.6.3.el7.x86_64 #1 SMP Fri Jun 15 17:57:37 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux

- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): 0.22.0
- git version: 2.13.0
- GCC/Compiler version (if compiling from source): gcc (GCC) 6.3.0
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: NVIDIA V100 16GB and NVIDIA P100 16GB

**Describe the problem**
Attempting to build from source, build does not complete.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (383 packages loaded, 19566 targets configured). INFO: Found 1 target... ERROR: /gpfs/home/s/t/steige/Carbonate/.cache/bazel/_bazel_steige/7e49ffaa2aeff17b57c88f7f5097a06c/external/gif_archive/BUILD.bazel:8:1: undeclared inclusion(s) in rule '@gif_archive//:gif': this rule is missing dependency declarations for the following files included by 'external/gif_archive/lib/openbsd-reallocarray.c': '/N/soft/rhel7/gcc/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/stddef.h' '/N/soft/rhel7/gcc/6.3.0/lib/gcc/x86_64-pc-linux-gnu/6.3.0/include/stdint.h' Target //tensorflow/tools/pip_package:build_pip_package failed to build Use --verbose_failures to see the command lines of failed build steps. INFO: Elapsed time: 811.872s, Critical Path: 46.61s INFO: 20 processes: 20 local. FAILED: Build did NOT complete successfully"
26182,ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Linux Mint
-Anaconda - pip install tensorflow-gpu
- 9.0/7.5:
- 1080 ti

I was using tensorflow gpu last year. I wanted to set it up again. I got it running on my Windows 10 partition. Now I have tried to set it up again on my Mint partition. I always get the following error. 
ImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory.
I thought TF needs cuda 9.0 and not 10.0?

The error occurs if I execute the following code.


import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))"
26179,Datasets not reshuffling between epochs in eager mode,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 10.14
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0-dev20190227
- Python version: 3.7

**Describe the current behavior**
Iterating a shuffled dataset returns elements in the same order each time it is iterated over, and the order is unaffected by the random seed.

**Describe the expected behavior**
The order should be deterministic given the random seed, and there should be a way to reshuffle between epochs.

**Code to reproduce the issue**

```python
import tensorflow as tf

tf.enable_eager_execution()

d = tf.data.Dataset.range(5).shuffle(5)
tf.set_random_seed(0)
elems = [item.numpy() for item in d]
print(""First epoch: "", elems)
elems = [item.numpy() for item in d]
print(""Second epoch: "", elems)
```

Running this twice produces:

First run:
```
First epoch:  [4, 2, 1, 3, 0]
Second epoch:  [4, 2, 1, 3, 0]
```

Second run:
```
First epoch:  [3, 1, 0, 2, 4]
Second epoch:  [3, 1, 0, 2, 4]
```

So even though the same random seed is used in both runs, the order of elements differs between them. And additionally, the order is the same between epochs, which you wouldn't necessarily want in your training loop. "
26178,"Output_dir inconsitancy between ""model_to_estimator"" and ""export_savedmodel""","**Problem explanation**
Since I want to train my neural networks into the Google ML Cloud, I am trying to convert a Keras model to a TensorFlow (TF) estimator. 

A tutorial explaining how to do this can be found on [Training Keras with GPUs & Serving Predictions with Cloud ML Engine (Google Cloud AI Huddle)](https://www.youtube.com/watch?v=4pC97HRhK9E). The Jupyter notebook that accompanies this tutorial can be found on [kaggle.com](https://www.kaggle.com/yufengg/emnist-gpu-keras-to-tf). 

Unfortunately, while following this tutorial, I ran into some problems. 

When trying to convert a Keras model into a TF estimator (using the _model_to_estimator_ function while supplying the _output_dir = <USER_DIR>_ argument) and following saving this estimator (using the _export_savedmodel_ module_ function), I received the following error:

`ValueError: Couldn't find trained model at ./estimator_model.`

A solution to overcome this problem has been given on the following [stackoverflow post](https://stackoverflow.com/questions/54615708/exporting-a-keras-model-as-a-tf-estimator-trained-model-cannot-be-found). However I am reporting the issue here in case it was not yet solved.

**System information**
- Tested on Kaggle kernel and Windows 10 Pro
- TF installed from source: v1.12.0
- Python version: 3.6.8
- CUDA/cuDNN version: 9.0
- GPU model and memory:  NVidia K80 GPUs, 13 GB RAM

**Describe the current behaviour**
Currently because _model_to_estimator_ module saves the trained model in a ""Keras"" subfolder under the user specified _output_dir = '<USERDIR>'_ while the _export_savedmodel_ module uses the _model.output_dir_ parameter and thus the user specified parent folder to look for the trained model I get the following error:

`ValueError: Couldn't find trained model at ./estimator_model.`

**Describe the expected behaviour**
I expected the _export_savedmodel_ module to successfully find the trained model in the _<USERDIR>_ instead of the _<USERDIR>/keras_ folder and save the TF model.

**Current workaround**
Currently, I need to move the model files from the _<USERDIR>/keras_ folder to the _<USERDIR>_ to successfully save the model.

**Code to reproduce the issue**
The issue can be reproduced by using the code provided by 

Create a Keras model and save it:

```
import keras
model = keras.Sequential()
model.add(keras.layers.Dense(units=1,
                                activation='sigmoid',
                                input_shape=(10, )))
model.compile(loss='binary_crossentropy', optimizer='sgd')
model.save('./model.h5')
```

Next, convert the model to an estimator with tf.keras.estimator.model_to_estimator, add an input receiver function and export it in the Savedmodel format with estimator.export_savedmodel:

```
# Convert keras model to TF estimator
tf_files_path = './tf'
estimator =\
    tf.keras.estimator.model_to_estimator(keras_model=model,
                                          model_dir=tf_files_path)
def serving_input_receiver_fn():
    return tf.estimator.export.build_raw_serving_input_receiver_fn(
        {model.input_names[0]: tf.placeholder(tf.float32, shape=[None, 10])})

# Export the estimator
export_path = './export'
estimator.export_savedmodel(
    export_path,
    serving_input_receiver_fn=serving_input_receiver_fn())

```

**Error Log**
```

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-25-fa76b1124e44> in <module>()
----> 1 export_path = estimator_model.export_savedmodel('./export', serving_input_receiver_fn=serving_input_receiver_fn)
      2 export_path

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in export_savedmodel(self, export_dir_base, serving_input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs)
    661         checkpoint_path=checkpoint_path,
    662         strip_default_attrs=strip_default_attrs,
--> 663         mode=model_fn_lib.ModeKeys.PREDICT)
    664 
    665   def export_saved_model(

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _export_saved_model_for_mode(self, export_dir_base, input_receiver_fn, assets_extra, as_text, checkpoint_path, strip_default_attrs, mode)
    787         as_text=as_text,
    788         checkpoint_path=checkpoint_path,
--> 789         strip_default_attrs=strip_default_attrs)
    790 
    791   def _export_all_saved_models(

/opt/conda/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _export_all_saved_models(self, export_dir_base, input_receiver_fn_map, assets_extra, as_text, checkpoint_path, strip_default_attrs)
    876             self._model_dir)
    877       if not checkpoint_path:
--> 878         raise ValueError(""Couldn't find trained model at %s."" % self._model_dir)
    879 
    880       export_dir = export_helpers.get_timestamped_export_dir(export_dir_base)

ValueError: Couldn't find trained model at ./estimator_model.
```"
26177,TensorFlow master build failing on Ubuntu18.04 x86,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): source
- TensorFlow version: master as on today 
- Python version:  2.7.x
- Bazel version (if compiling from source): 0.19.0 ( installed using bazel-0.19.0-installer-linux-x86_64.sh)
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
Tensorflow build fails with an error:

In file included from external/hwloc/include/private/private.h:29:0,
                 from external/hwloc/hwloc/traversal.c:11:
external/hwloc/include/private/misc.h:491:10: fatal error: xlocale.h: No such file or directory
 #include ""xlocale.h""
          ^~~~~~~~~~~
compilation terminated.

**Steps to reproduce:**
 git clone https://github.com/tensorflow/tensorflow/
cd tensorflow
./configure

./configure
Extracting Bazel installation...
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
You have bazel 0.19.0 installed.
Please specify the location of python. [Default is /usr/bin/python]:


Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is [/usr/local/lib/python2.7/dist-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: N
No XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: N
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: N
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: N
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: N
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [y/N]: N
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: N
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

 bazel --host_jvm_args=""-Xms512m"" --host_jvm_args=""-Xmx1024m""  build  -c opt  //tensorflow/toockage:build_pip_package 

"
26176,[Tensorflow 2]: Bring back tf.layers?,"In the proposed TensorFlow 2.0 API, `tf.layers` has been removed in favor of `tf.keras.layers`. Would it be possible to bring it back as a simple alias for `tf.keras.layers`? 

For reference, we already have:
- `tf.losses` == `tf.keras.losses`
- `tf.optimizers` == `tf.keras.optimizers`

as aliases. I personally think that `tf.keras.layers` are as-used as optimizers and loss functions, and that `tf.layers` would be a small ergonomic improvement over `tf.keras.layers`."
26175,MultiboxTracker making detections in groups of 9 - 13,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04.1 LTS
- TensorFlow version: 1.12.0
- Python version: 2.7.15

**Describe the current behavior**
It seems that the drawDebug method in the MultiBoxTracker.java file is making more detentions than it should be. 

I modified the drawDebug method such that it returns an ArayList of confidence values for each detection as well as booleans which are a check as to whether the detection is correct. Here is the code for that: 
```

public synchronized ArrayList<Tuple> drawDebug(final Canvas canvas, final String groundTruthClass) {
    Log.d(""myInfoTag"",""IN DRAW DEBUG"");
    final Paint boxPaint = new Paint();
    ArrayList<Tuple> conVals = new ArrayList<Tuple>();
    boxPaint.setAlpha(10);
    boxPaint.setStyle(Style.STROKE);
    boxPaint.setStrokeWidth(DETECTION_BOX_LINE_THICKNESS);
    boxPaint.setPathEffect(new DashPathEffect(new float[] {DASH_LENGTH, DASH_GAP}, 0));

    for (final Triplet<String, Float, RectF> detection : screenRects) {
        final RectF rect = detection.getThird();
        final String className = detection.getFirst();
        final Float confidence = detection.getSecond();
        boolean correct = false;

        if (confidence >= 0.5) {
            if (className.equals(groundTruthClass)) { // put parameter
                correct = true;
            }
            Tuple conPair = new Tuple<Float, Boolean>(confidence, correct);
            if (!(conVals.contains(conPair))) {
                conVals.add(conPair);
            }
        }

        final String labelString =
              !TextUtils.isEmpty(className)
                      ? String.format(""%s %.0f%%"", className, confidence*100)
                      : String.format(""%.0f%%"", confidence);
        if (className.equals(CLASS_1_NAME)) {
            DISPLAY_THRESHOLD = CLASS_1_THRESHOLD;
            boxPaint.setColor(CLASS_1_COLOR);
        } else if (className.equals(CLASS_2_NAME)) {
            DISPLAY_THRESHOLD = CLASS_2_THRESHOLD;
            boxPaint.setColor(CLASS_2_COLOR);
        } else if (className.equals(CLASS_3_NAME)) {
            DISPLAY_THRESHOLD = CLASS_3_THRESHOLD;
            boxPaint.setColor(CLASS_3_COLOR);
        } else if (className.equals(CLASS_4_NAME)) {
            DISPLAY_THRESHOLD = CLASS_4_THRESHOLD;
            boxPaint.setColor(CLASS_4_COLOR);
        } else if (className.equals(CLASS_5_NAME)) {
            DISPLAY_THRESHOLD = CLASS_5_THRESHOLD;
            boxPaint.setColor(CLASS_5_COLOR);
        } else if (className.equals(CLASS_6_NAME)) {
            DISPLAY_THRESHOLD = CLASS_6_THRESHOLD;
            boxPaint.setColor(CLASS_6_COLOR);
        } else if (className.equals(CLASS_7_NAME)) {
            DISPLAY_THRESHOLD = CLASS_7_THRESHOLD;
            boxPaint.setColor(CLASS_7_COLOR);
        }
        if (confidence>= DETECTION_THRESHOLD && confidence < DISPLAY_THRESHOLD) {
            canvas.drawRect(rect, boxPaint);
            borderedText.drawText(canvas, rect.left, rect.top, labelString);
        }

    }

    if (objectTracker == null) {
      return conVals;
    }

```
In DetectorActivity.java, I have all of the values of conVals printed to the screen. However, the same detection prints to the screen anywhere between 9 and 13 times. It seems that there might be an issue with threading, or the drawDebug method is making far too many detections.

Here's the DetectorActivity.java code that takes all the confidences in the arrayList returned by drawDebug, groups them into confidence intervals, and prints them to the screen. I don't believe this is where the issue is, but I've included it just in case. You'll notice that I'm simply just making a count of how many predictions fall in certain confidence ranges and printing those to the screen. There is also a for loop that's denoted by the comment ""look here"" which is where the detections are individually printed to the screen as the app makes detections. This is how I noticed that the detections were being made in groups of 9 - 13, as opposed to being just single detections. 

    trackingOverlay.addCallback(
        new DrawCallback() {
          @Override
          public void drawCallback(final Canvas canvas) {
            tracker.draw(canvas);
            if (isDebug()){
              ArrayList<Tuple> conPairs = tracker.drawDebug(canvas, groundTruthClass); // need to add ground truth label to parameter list
              if (record) {
                for (Tuple<Float, Boolean> conPair : conPairs) {
                  Float conVal = conPair.getFirst();
                  Boolean conCorrect = conPair.getSecond();
                  if (conVal != 0) {
                    if (calArray.size() < 2) {
                      calArray.add(conVal);
                      if ((conVal > 0.0) && (conVal <= 0.1)) {
                        interval0 += 1;
                        cinterval0 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.1) && (conVal <= 0.2)) {
                        interval1 += 1;
                        cinterval1 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.2) && (conVal <= 0.3)) {
                        interval2 += 1;
                        cinterval2 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.3) && (conVal <= 0.4)) {
                        interval3 += 1;
                        cinterval3 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.4) && (conVal <= 0.5)) {
                        interval4 += 1;
                        cinterval4 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.5) && (conVal <= 0.6)) {
                        interval5 += 1;
                        cinterval5 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.6) && (conVal <= 0.7)) {
                        interval6 += 1;
                        cinterval6 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.7) && (conVal <= 0.8)) {
                        interval7 += 1;
                        cinterval7 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.8) && (conVal <= 0.9)) {
                        interval8 += 1;
                        cinterval8 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.9) && (conVal <= 1.0)) {
                        interval9 += 1;
                        cinterval9 += (conCorrect == true) ? 1 : 0;
                      }
                      // } else if (!(((calArray.get(calArray.size() - 1)).equals(conVal))) && !((calArray.get(calArray.size() - 2)).equals(conVal))) {
                    } else {
                      calArray.add(conVal);
                      if ((conVal > 0.0) && (conVal <= 0.1)) {
                        interval0 += 1;
                        cinterval0 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.1) && (conVal <= 0.2)) {
                        interval1 += 1;
                        cinterval1 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.2) && (conVal <= 0.3)) {
                        interval2 += 1;
                        cinterval2 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.3) && (conVal <= 0.4)) {
                        interval3 += 1;
                        cinterval3 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.4) && (conVal <= 0.5)) {
                        interval4 += 1;
                        cinterval4 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.5) && (conVal <= 0.6)) {
                        interval5 += 1;
                        cinterval5 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.6) && (conVal <= 0.7)) {
                        interval6 += 1;
                        cinterval6 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.7) && (conVal <= 0.8)) {
                        interval7 += 1;
                        cinterval7 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.8) && (conVal <= 0.9)) {
                        interval8 += 1;
                        cinterval8 += (conCorrect == true) ? 1 : 0;
                      } else if ((conVal > 0.9) && (conVal <= 1.0)) {
                        interval9 += 1;
                        cinterval9 += (conCorrect == true) ? 1 : 0;
                      }
                    }

                    per0 = (float) cinterval0/interval0;
                    per1 = (float) cinterval1/interval1;
                    per2 = (float) cinterval2/interval2;
                    per3 = (float) cinterval3/interval3;
                    per4 = (float) cinterval4/interval4;
                    per5 = (float) cinterval5/interval5;
                    per6 = (float) cinterval6/interval6;
                    per7 = (float) cinterval7/interval7;
                    per8 = (float) cinterval8/interval8;
                    per9 = (float) cinterval9/interval9;

                  }
                }
              }
            }
          }
        });

    addCallback(
        new DrawCallback() {
          @Override
          public void drawCallback(final Canvas canvas) {
            if (!isDebug()) {
              return;
            }
            final Bitmap copy = cropCopyBitmap;
            if (copy == null) {
              return;
            }
            // Color 'tint' for volume down stats overlay
            final int backgroundColor = Color.argb(0, 0, 0, 0);
            canvas.drawColor(backgroundColor);

            //final Matrix matrix = new Matrix();
            //final float scaleFactor = 2;
            //matrix.postScale(scaleFactor, scaleFactor);
            //matrix.postTranslate(
            //    canvas.getWidth() - copy.getWidth() * scaleFactor,
            //    canvas.getHeight() - copy.getHeight() * scaleFactor);
            //canvas.drawBitmap(copy, matrix, new Paint());

            final Vector<String> lines = new Vector<String>();
            lines.add("""");

            if (record) {
              lines.add(""RECORDING ON"");
            } else {
              lines.add(""RECORDING OFF"");
            }

            for (Float confidence: calArray) {
              lines.add(String.valueOf(confidence));
            }

            lines.add(""0 to 10: "" + String.valueOf(interval0) + "" "" + String.valueOf(cinterval0) + "" "" + Float.toString(per0));
            lines.add(""10 to 20: "" + String.valueOf(interval1) + "" "" + String.valueOf(cinterval1) + "" "" + Float.toString(per1));
            lines.add(""20 to 30: "" + String.valueOf(interval2) + "" "" + String.valueOf(cinterval2) + "" "" + Float.toString(per2));
            lines.add(""30 to 40: "" + String.valueOf(interval3) + "" "" + String.valueOf(cinterval3) + "" "" + Float.toString(per3));
            lines.add(""40 to 50: "" + String.valueOf(interval4) + "" "" + String.valueOf(cinterval4) + "" "" + Float.toString(per4));
            lines.add(""50 to 60: "" + String.valueOf(interval5) + "" "" + String.valueOf(cinterval5) + "" "" + Float.toString(per5));
            lines.add(""60 to 70: "" + String.valueOf(interval6) + "" "" + String.valueOf(cinterval6) + "" "" + Float.toString(per6));
            lines.add(""70 to 80: "" + String.valueOf(interval7) + "" "" + String.valueOf(cinterval7) + "" "" + Float.toString(per7));
            lines.add(""80 to 90: "" + String.valueOf(interval8) + "" "" + String.valueOf(cinterval8) + "" "" + Float.toString(per8));
            lines.add(""90 to 100: "" + String.valueOf(interval9) + "" "" + String.valueOf(cinterval9) + "" "" + Float.toString(per9));


            borderedText.drawLines(canvas, 10, canvas.getHeight() - 10, lines);
          }
        });"
26174,TFLite Mfcc op has inconsistent requirements with standard Mfcc op,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, here's the code to reproduce: https://gist.github.com/reuben/57bee91669a5bd2717c32cf406ca951d
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.14
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: v1.13.0-rc2-5-g6612da8951 1.13.1
- **Python version**: 3.6.6
- **Bazel version (if compiling from source)**: N/A
- **GCC/Compiler version (if compiling from source)**: N/A
- **CUDA/cuDNN version**: N/A
- **GPU model and memory**: N/A
- **Exact command to reproduce**: curl https://gist.githubusercontent.com/reuben/57bee91669a5bd2717c32cf406ca951d/raw/6b81d28d00ff3ec73ab1bcc6a698366bbe3dcb51/test_tflite_mfcc.py | python

### Describe the problem
The Mfcc op in tensorflow.contrib.framework.python.ops.audio_ops (a.k.a. contrib_audio) enforces the shape of the sample rate parameter to be rank 0. The TFLite Mfcc op enforces the sample rate parameter to be rank 1. Converting a model with an Mfcc op in it results in a TFLite model that fails in the preparation step.

### Source code / logs
If you try to pass a sample rate of rank 1 to the contrib_audio op:

```
python3 test_tflite.py
2019-02-27 11:50:41.627337: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Traceback (most recent call last):
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1659, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 0 but is rank 1 for 'Mfcc' (op: 'Mfcc') with input shapes: [1,1,257], [1].

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test_tflite.py"", line 13, in <module>
    mfccs = contrib_audio.mfcc(spectrogram, [16000], dct_coefficient_count=13)
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/ops/gen_audio_ops.py"", line 454, in mfcc
    dct_coefficient_count=dct_coefficient_count, name=name)
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1823, in __init__
    control_input_ops)
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1662, in _create_c_op
    raise ValueError(str(e))
ValueError: Shape must be rank 0 but is rank 1 for 'Mfcc' (op: 'Mfcc') with input shapes: [1,1,257], [1].
```

If you pass a sample rate of rank 0, and then try to convert and use the TFLite model:

```
python3 test_tflite.py
2019-02-27 11:50:27.082848: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
session run works
toco_from_protos /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmp_vtx0s5o /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmp7v9bbo3t /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmpilazyufw /var/folders/k1/7sbxsmm52n59_fpj0v65fmk40000gn/T/tmp9e1513n8
Traceback (most recent call last):
  File ""test_tflite.py"", line 30, in <module>
    interpreter.allocate_tensors()
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/lite/python/interpreter.py"", line 73, in allocate_tensors
    return self._interpreter.AllocateTensors()
  File ""/Users/rmorais/.local/share/virtualenvs/DeepSpeech-HmF6CP0D/lib/python3.6/site-packages/tensorflow/lite/python/interpreter_wrapper/tensorflow_wrap_interpreter_wrapper.py"", line 106, in AllocateTensors
    return _tensorflow_wrap_interpreter_wrapper.InterpreterWrapper_AllocateTensors(self)
RuntimeError: tensorflow/lite/kernels/mfcc.cc:75 NumDimensions(inputRate) != 1 (0 != 1)Node number 2 (Mfcc) failed to prepare.
```

And here's the source of the testing script just in case:

```
import numpy as np
import tensorflow as tf
import sys
import tempfile

from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio


with tf.Session() as sess:
    input_ph = tf.placeholder(tf.float32, [512])
    samples = tf.reshape(input_ph, [512, 1])
    spectrogram = contrib_audio.audio_spectrogram(samples, window_size=512, stride=320, magnitude_squared=True)
    mfccs = contrib_audio.mfcc(spectrogram, 16000, dct_coefficient_count=13)

    sess.run([mfccs], feed_dict={input_ph: np.random.random([512])})
    print('session run works')

    converter = tf.lite.TFLiteConverter(sess.graph_def, input_tensors=[input_ph], output_tensors=[mfccs])
    converter.allow_custom_ops = True
    tflite_model = converter.convert()

with tempfile.NamedTemporaryFile(delete=False) as fout:
    temp_name = fout.name
    fout.write(tflite_model)
    fout.flush()

try:
    # Load TFLite model and allocate tensors.
    interpreter = tf.lite.Interpreter(model_path=temp_name)
    interpreter.allocate_tensors()
    print('tflite model prepare works')

    # Get input and output tensors.
    input_details = interpreter.get_input_details()
    output_details = interpreter.get_output_details()

    # Test model on random input data.
    for inp in input_details:
        input_data = np.array(np.random.random_sample(inp['shape']), dtype=np.float32)
        interpreter.set_tensor(inp['index'], input_data)

    interpreter.invoke()
    output_data = interpreter.get_tensor(output_details[0]['index'])
    print(output_data)
finally:
    try:
        os.remove(temp_name)
    except:
        pass
```"
26172,"[Feature Request, INQ] Add functionality for INQ implementation","**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

My suggestion is related with the [article](https://arxiv.org/pdf/1702.03044.pdf) about Incremental Network Quantization (INQ) method of weights quantization.

Firstly for INQ implementation it is necessary to create integer convolution operation and matrix multiplication operation with bit shift instead of multiplication. I also want to create an API for weights partitions with certain choices of elements (for example, the absolute value maximum).

Suggested API will be low-level. High-level API will be a next step of my contributing. It is also interesting to add this feature to TFLite.

**Will this change the current api? How?**
Yes, There will be added new functions for convolution and matmul with bit shift.

**Who will benefit with this feature?**
The INQ method reduces the weight of the model, and also performs a bit shift instead of floating point multiplication.

**Any Other info.**
"
26171,RuntimeError: Graph is finalized and cannot be modified.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):n/a
- TensorFlow version (use command below):1.12.0
- Python version:2.7.x
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):(Ubuntu 5.4.0-6ubuntu1~16.04.11)5.4.0
- CUDA/cuDNN version:n/a
- GPU model and memory:n/a

**Describe the problem**
I use tfrecords file to train a stacked autoencoder. I want to train the encoding layer for 1000 steps. I tried to create batches from features and labels and use them to train my network.  However, when I run my code I get an error (RuntimeError: Graph is finalized and cannot be modified.) indicating that the problem comes from the following function: 

     def train_layer(output_layer, layer_loss,optimizer):
     """"""Train each encoding layer for 1000 steps""""""
     layer_name = output_layer.name.split('/')[0]
     print('Pretraining {}'.format(layer_name))
     num_steps = 1000
      step=1
     features, labels=train_input_fn()
     input_l = tf.reshape(features, [-1, FLAGS.image_rows, FLAGS.image_cols, 1])
      while step <= num_steps:
     instance_batch, label_batch = tf.train.shuffle_batch([input_l], batch_size=5, capacity=200, min_after_dequeue=100)
    _out_layer, _layer_loss,_ = sess.run([output_layer, layer_loss, optimizer],
     feed_dict ={features:instance_batch,labels:label_batch})
     #print(_layer_loss)
     step += 1
     print('layer finished')`

"
26168,Tensorflow not working with vGPUs (when the machine is not in pass-through mode),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version: **1.12.0**
- Python version: **3.6.8**
- Installed using virtualenv? pip? conda?: **Conda**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: **CUDA 9.2; CuDNN 7.3**
- GPU model and memory: **NVIDIA Tesla V100 (16 GB)**



**Describe the problem**

I was using TF for experimenting with seq2seq models on a VM that had access to a Tesla V100 GPU (in pass-through mode). Recently, that VM was restricted to use a vGPU (with 50% of the memory - Q8 variant of V100), and hence it was not is pass-through mode anymore. With this change, Tensorflow stopped working, and gave `UNKNOWN_CUDA_ERROR`.

I would like to know if Tensorflow could be used with Virtual GPUs (not in pass-through mode).
If yes, any helpful resource for achieving the same would be much appreciated.

Thanks.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```python
import tensorflow as tf
print(tf.test.is_gpu_available())
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
Attempting to fetch value instead of handling error
Internal: failed initializing StreamExecutor for CUDA Device Ordinal 0
```

*(Currently, I do not have the full traceback unfortunately).*"
26167,CRF functions in TensorFlow 2.0,"Hello,

It seems like CRF (tensorflow.contrib.crf) is moving to tensorflow/probability in TensorFlow 2.0. 
(https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md)

but, I couldn't find them on https://github.com/tensorflow/probability.
 
Where can I find CRF related features in TensorFlow 2.0?


 "
26166,Tensorflow run with C++ API cause XAVIER crash,"Tensoflow version:  '1.12.0-rc2'
Device:  Xavier
NV POWER Mode: MAXN
CUDA: 10.0.117
CUDNN : 7.3.1

##############KERNEL CODE#####################
void semanticSegPro::getImgData(uchar* data)
{

    auto input_tensor_mapped = input_tensor.tensor<uchar, 4>();

    for (int y = 0; y < height; ++y)
    {
        const uchar* source_row = data + (y * width * 3);
        for (int x = 0; x < width; ++x)
        {
            const uchar* source_pixel = source_row + (x * 3);
            for (int c = 0; c < 3; ++c)
            {
                const uchar* source_value = source_pixel + c;
                input_tensor_mapped(0, y, x, c) = *source_value;
            }
        }
    }

    QString s=QDateTime::currentDateTime().toString(""yyyy-MM-dd hh:mm:ss.zzz"");
    qDebug()<<""BF = ""<<s;

    **Status status_run =
            session->Run({{inputTensorName.toStdString(), input_tensor}},
            {outputTensorName.toStdString()}, {}, &outputs);**

    if (!status_run.ok()) {
        qDebug()<<""ERROR: RUN failed...""<<QString::fromStdString(status_run.ToString());
    }

    s=QDateTime::currentDateTime().toString(""yyyy-MM-dd hh:mm:ss.zzz"");
    qDebug()<<""AFT = ""<<s;

    output_tensor = outputs[0];
    auto tmap = output_tensor.tensor<int64, 3>();

    for(int i = 0; i < height; i++)
    {
        for(int j =0; j< width; j++)
        {
            outData[i*width + j] = tmap(0,i,j);
        }
    }

    emit segData((uchar*)outData);
}
##############KERNEL CODE#####################

RUN HERE : 
    Status status_run =
            session->Run({{inputTensorName.toStdString(), input_tensor}},
            {outputTensorName.toStdString()}, {}, &outputs);
The program abnormal termination or crash.

I used the same program with PC(GTX 1070), it runs well. 
I also used python API with Xavier platform, it runs well also.
Why the program crash with Xavier and C++ API???
"
26165,Where to place 'tf.contrib.quantize.create_training_graph'  during multi-gpu Quantization-aware training?,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):**By pip**
- TensorFlow version (use command below):**1.11.0**
- Python version:**2.7**
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I follow ""quantization aware training"" tutorial in tensorflow to retrain my already trained floating point  model. And I wrote a muli-gpu version to do the stuff. Actually, I have written two snippets of training code, the first code snippet is :
```python
with tf.variable_scope(tf.get_variable_scope()):
    for i in xrange(len(GPU_NUM_ID)):
       with tf.device('/gpu:%d' % GPU_NUM_ID[i]):
              with tf.name_scope('%s_%d' % ('cnn_mg', i)) as scope:
                     images, labels = ut._load_batch_filename()  
                            logits, out_data = inference(images, reuse=tf.AUTO_REUSE,  num_classes=LABEL_NUM)
                            loss ,accuracy_sep = _tower_loss_depth(scope, logits, labels)
                            tf.get_variable_scope().reuse_variables()
                            grads = optimizer.compute_gradients(loss_total_sep)
                            tower_grads.append(grads)   

tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP)
```
The second snippet of the training code is:
```python
with tf.variable_scope(tf.get_variable_scope()):

    for i in xrange(len(GPU_NUM_ID)):
       with tf.device('/gpu:%d' % GPU_NUM_ID[i]):
              with tf.name_scope('%s_%d' % ('cnn_mg', i)) as scope:
                     images, labels = ut._load_batch_filename()  
                            logits, out_data = inference(images, reuse=tf.AUTO_REUSE,  num_classes=LABEL_NUM)
                            with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE): 
                                   tf.contrib.quantize.create_training_graph(quant_delay=DELAY_STEP)
                            loss ,accuracy_sep = _tower_loss_depth(scope, logits, labels)
                            tf.get_variable_scope().reuse_variables()
                            grads = optimizer.compute_gradients(loss_total_sep)
                            tower_grads.append(grads)
```

**Describe the expected behavior**
What I have found is: The second code snippet if 1.5 times faster than the first  code snippet  with the same training setting.I want to know why this happens and  I want know which code snippet is the tensorflow recommended code snippet. 
However, When I update my tensorflow version to 1.12.0, the second version(faster) could cause some error(""tensorflow Graph is invalid, contains a cycle""). Dose this mean I have to slow down my multi-gpu Quantization-aware training in the future version of tensorflow?

"
26164,Memory Fault (Coredumped) java,"Hi!

When i try to run this Hello World TF example, when the program is ending, appears this message ""Memory Fault(Coredumped)""

Machine:
-java 1.7.64
-SO: Red Hat 7.3
Cores: 4
RAM: 8 gb

Example Code:

`
import org.tensorflow.Graph;
import org.tensorflow.Session;
import org.tensorflow.Tensor;
import org.tensorflow.TensorFlow;

public class HelloTF {
	
  public static void main(String[] args) throws Exception {
    try (Graph g = new Graph()) {
      final String value = ""Hello from "" + TensorFlow.version();

      // Construct the computation graph with a single operation, a constant
      // named ""MyConst"" with a value ""value"".
      try (Tensor t = Tensor.create(value.getBytes(""UTF-8""))) {
        // The Java API doesn't yet include convenience functions for adding operations.
        g.opBuilder(""Const"", ""MyConst"").setAttr(""dtype"", t.dataType()).setAttr(""value"", t).build();
      }
      catch(Exception e) {
    	  System.out.println(""Try 1"");
    	  e.printStackTrace();
      }

      // Execute the ""MyConst"" operation in a Session.
      try (Session s = new Session(g);
           Tensor output = s.runner().fetch(""MyConst"").run().get(0)) {
        System.out.println(new String(output.bytesValue(), ""UTF-8""));
        s.close();
      }
      catch(Exception e) {
    	  System.out.println(""Try 2"");
    	  e.printStackTrace();

      }
    }
    catch(Exception e) {
  	  System.out.println(""Try 3"");
  	  e.printStackTrace();
    }
    System.out.println(""END"");
  }

}`



POM:
`<project>
       
     <modelVersion>4.0.0</modelVersion>
     <groupId>org.myorg</groupId>
     <artifactId>hellotf</artifactId>
     <version>1.0.0</version>
    
     <properties>
       <exec.mainClass>HelloTF</exec.mainClass>
       <maven.compiler.source>1.7</maven.compiler.source>
       <maven.compiler.target>1.7</maven.compiler.target>  
     </properties>
    
     <dependencies>
       
       <dependency>
         <groupId>org.tensorflow</groupId>
         <artifactId>tensorflow</artifactId>
         <version>1.7.0</version>
       </dependency>

    </dependencies>

</project>`

Any Help?

thanks!"
26162,Add speech recognition while true with webcam,is there any way to add speech recognition so that it will respond to our emotions if we speak .please help me or else is there any alternative way to record the audio file and check the input then it will speak with us
26161,Tensorflow Lite on GPU failed on Android Pi9.0,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Android Pi9.0 with NXP i.MX8QM
- TensorFlow installed from (source or binary):
Build from source based on https://www.tensorflow.org/lite/performance/gpu_advanced
- TensorFlow version (or github SHA if from source):
commit ad78d08bd1d03238d434d3d214a1de56f22d7601 (HEAD -> imx_r1.98, origin/r1.98)
Author: A. Unique TensorFlower <gardener@tensorflow.org>
Date:   Wed Feb 13 12:35:06 2019 -0800

    [TF:XLA] Mark a test as not running on XLA due to different flows.

    The test is not run because it is to test a part of grappler not used with XLA:GPU.

    This is change is a no-op. It makes explicit a default value and adds comments.

    PiperOrigin-RevId: 233807945

**Any other info / logs**
02-27 14:37:26.750  4532  4618 E AndroidRuntime: FATAL EXCEPTION: CameraBackground
02-27 14:37:26.750  4532  4618 E AndroidRuntime: Process: android.example.com.tflitecamerademo, PID: 4532
02-27 14:37:26.750  4532  4618 E AndroidRuntime: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #43 cannot be handled by this delegate.  Onl.
02-27 14:37:26.750  4532  4618 E AndroidRuntime:
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.init(NativeInterpreterWrapper.java:83)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:60)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:224)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:   at com.example.android.tflitecamerademo.ImageClassifier.recreateInterpreter(ImageClassifier.java:168)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at com.example.android.tflitecamerademo.ImageClassifier.useGpu(ImageClassifier.java:176)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at com.example.android.tflitecamerademo.Camera2BasicFragment.lambda$updateActiveModel$0$Camera2BasicFragment(Camera2BasicFragment.ja)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at com.example.android.tflitecamerademo.Camera2BasicFragment$$Lambda$0.run(Unknown Source:24)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at android.os.Handler.handleCallback(Handler.java:790)
02-27 14:37:26.750  4532  4618 E AndroidRuntime:        at android.os.Handler.dispatchMessage(Handler.java:99)"
26160,DataLossError: file is too short to be an sstable,"<em>Hi, I am trying to build a custom object detector using Tensorflow. I tried to do it in my local computer but it took a long time as I have an Intel i3 processor and no GPU. But I was able to start the training after running the following code:
`python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config`
The training continued for few epochs for some hours along with losses like it typically does after which I decided to stop and try it using Google Colab since there is GPU support there.
But when I copy all the tensorflow directory containing all the XMLs,Images,tf-records, pretrained models and config files and run in colab, the training starts but ends with an error.
**Output line of Colab:**
WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.
For more information, please see:
  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md
  * https://github.com/tensorflow/addons
If you depend on functionality not listed there, please file an issue.

WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py:125: main (from __main__) is deprecated and will be removed in a future version.
Instructions for updating:
Use object_detection/model_main.py.
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/legacy/trainer.py:266: create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.
Instructions for updating:
Please switch to tf.train.create_global_step
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/builders/dataset_builder.py:80: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.experimental.parallel_interleave(...)`.
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/legacy/trainer.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/core/preprocessor.py:188: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/core/preprocessor.py:1218: calling squeeze (from tensorflow.python.ops.array_ops) with squeeze_dims is deprecated and will be removed in a future version.
Instructions for updating:
Use the `axis` argument instead
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/core/batcher.py:96: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.
Instructions for updating:
Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/input.py:753: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
INFO:tensorflow:depth of additional conv before box predictor: 0
WARNING:tensorflow:From /content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/core/losses.py:664: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Traceback (most recent call last):
  File ""train.py"", line 184, in <module>
    tf.app.run()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""train.py"", line 180, in main
    graph_hook_fn=graph_rewriter_fn)
  File ""/content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/legacy/trainer.py"", line 397, in train
    include_global_step=False))
  File ""/content/drive/My Drive/New Project/TensorFlow/models/research/object_detection/utils/variables_helper.py"", line 126, in get_variables_available_in_checkpoint
    ckpt_reader = tf.train.NewCheckpointReader(checkpoint_path)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 326, in NewCheckpointReader
    return CheckpointReader(compat.as_bytes(filepattern), status)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.DataLossError: file is too short to be an sstable

**The code i used is following:**
`import os

os.chdir('/content/drive/My Drive/New Project/TensorFlow/workspace/training_demo')

os.environ['PYTHONPATH'] += ':/content/drive/My Drive/New Project/TensorFlow/models/research/:/content/drive/My Drive/New Project/TensorFlow/models/research/slim/'

!python train.py --logtostderr --train_dir=training/ --pipeline_config_path=training/ssd_inception_v2_coco.config
`

Please note that all the preprocessing related to generation of tf-records were done and the entire directory is in the Tensorflow folder.
Kindly help. Thank you."
26159,"Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h error: invalid operands to binary expression ('const double2' and 'const double2') pxor(const Packet& a, const Packet& b) { return a ^ b; }","**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: source 
- TensorFlow version: r1.99 and latest
- Python version: 3.5
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0/7.0 and 8.0/7.0
- GPU model and memory: Geforce 1050 Ti



**Describe the problem**
**external/eigen_archive/unsupported/Eigen/CXX11/../../../Eigen/src/Core/GenericPacketMath.h:211:51: error: invalid operands to binary expression ('const double2' and 'const double2')
pxor(const Packet& a, const Packet& b) { return a ^ b; }
**

"
26158,Building 1.13.1 on Raspberry Pi freezes the board,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Raspbian 9.8 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RaspberryPi 3 Model B+
- TensorFlow installed from (source or binary): NA
- TensorFlow version: 1.13.1
- Python version: 3.5
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: None
- GPU model and memory: None

**Describe the problem**
I had a freshly installed and updated Raspberry Pi 3 Model B+, and I followed [the native compiling guide](https://www.tensorflow.org/lite/rpi#native_compiling) in tensorflows website. However, after executing:

`./tensorflow/lite/tools/make/build_rpi_lib.sh`

The RPi Display Freezes up and I'm not sure if it's still proceeding


**Provide the exact sequence of commands / steps that you executed before running into the problem**

Get your RPi with a Raspbian OS, update everything. 
Then open a Terminal or SSH to it, go to the home directory, and download the latest tensorflow repository available.

Run the following commands:
`cd tensorflow`
`./tensorflow/lite/tools/make/download_dependencies.sh`
`./tensorflow/lite/tools/make/build_rpi_lib.sh`

Your RaspberryPi will freeze at the line:

`arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/arg_min_max.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o`

And after a few hours, this comes up:

`arm-linux-gnueabihf-g++: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.
tensorflow/lite/tools/make/Makefile:197: recipe for target '/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o' failed
make: *** [/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o] Error 4
make: *** Waiting for unfinished jobs....`



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here is the stack trace from when I issued the `./tensorflow/lite/tools/make/build_rpi_lib.sh` command.

`pi@raspberrypi:~/tensorflow $ ./tensorflow/lite/tools/make/build_rpi_lib.sh
+ set -e
+++ dirname ./tensorflow/lite/tools/make/build_rpi_lib.sh
++ cd ./tensorflow/lite/tools/make
++ pwd
+ SCRIPT_DIR=/home/pi/tensorflow/tensorflow/lite/tools/make
+ cd /home/pi/tensorflow/tensorflow/lite/tools/make/../../../..
+ CC_PREFIX=arm-linux-gnueabihf-
+ make -j 3 -f tensorflow/lite/tools/make/Makefile TARGET=rpi TARGET_ARCH=armv7l
/bin/sh: 1: [[: not found
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/allocation.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/allocation.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/arena_planner.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/arena_planner.o
arm-linux-gnueabihf-gcc -O3 -DNDEBUG -fPIC  -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/c/c_api_internal.c -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/c/c_api_internal.o
In file included from tensorflow/lite/c/c_api_internal.c:16:0:
./tensorflow/lite/c/c_api_internal.h:60:34: warning: struct TfLiteContext declared inside parameter list will not be visible outside of this definition or declaration
   TfLiteStatus (*Refresh)(struct TfLiteContext* context);
                                  ^~~~~~~~~~~~~
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/core/api/error_reporter.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/error_reporter.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/core/api/flatbuffer_conversions.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/flatbuffer_conversions.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/core/api/op_resolver.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/api/op_resolver.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/core/subgraph.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/core/subgraph.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/c/c_api.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/c/c_api.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/experimental/c/c_api_experimental.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/experimental/c/c_api_experimental.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/graph_info.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/graph_info.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/interpreter.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/interpreter.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/activations.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/add.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/add_n.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/add_n.o
arm-linux-gnueabihf-g++ -O3 -DNDEBUG -fPIC  --std=c++11 -march=armv7-a -mfpu=neon-vfpv4 -funsafe-math-optimizations -ftree-vectorize -fPIC -I. -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/../../../../../../ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/ -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/eigen -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/absl -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/gemmlowp -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/neon_2_sse -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/farmhash/src -I/home/pi/tensorflow/tensorflow/lite/tools/make/downloads/flatbuffers/include -I -I/usr/local/include -c tensorflow/lite/kernels/arg_min_max.cc -o /home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/arg_min_max.o


arm-linux-gnueabihf-g++: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-6/README.Bugs> for instructions.
tensorflow/lite/tools/make/Makefile:197: recipe for target '/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o' failed
make: *** [/home/pi/tensorflow/tensorflow/lite/tools/make/gen/rpi_armv7l/obj/tensorflow/lite/kernels/activations.o] Error 4
make: *** Waiting for unfinished jobs....
`"
26157,Build error: Undeclared inclusions of various numpy header files,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.13
- Python version: 3.6
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.5
- GPU model and memory: GeForce GTX 1070, Compute Capability 6.1, totalMemory: 7.90GiB



**Describe the problem**

I'm trying to build tensorflow from source to have both GPU and CPU instruction set support but I'm facing a perplexing build error about undeclared inclusions.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
./configure
bazel build --verbose_failures -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both --copt=-msse4.1 --copt=-msse4.2 //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" 
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

The error obtained:
```
ERROR: /home/nagreme/tensorflow/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/utils.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ufuncobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_math.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
  'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
```
My bazel build configuration:
```
./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 78f3e435-9b41-440d-a973-946b3e60fa25
You have bazel 0.21.0 installed.
Please specify the location of python. [Default is /usr/bin/python]: /usr/bin/python3
Found possible Python library paths:
  /usr/lib/python3/dist-packages
  /usr/local/lib/python3.6/dist-packages
Please input the desired Python library path to use.  Default is [/usr/lib/python3/dist-packages]
Do you wish to build TensorFlow with XLA JIT support? [Y/n]: 
XLA JIT support will be enabled for TensorFlow.
Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: 
No OpenCL SYCL support will be enabled for TensorFlow.
Do you wish to build TensorFlow with ROCm support? [y/N]: 
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.
Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0
Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 
Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.5
Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: /usr/lib/x86_64-linux-gnu/
Do you wish to build TensorFlow with TensorRT support? [y/N]: 
No TensorRT support will be enabled for TensorFlow.
Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 
Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 6.1]: 6.1
Do you want to use clang as CUDA compiler? [y/N]: 
nvcc will be used as CUDA compiler.
Please specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: 
Do you wish to build TensorFlow with MPI support? [y/N]: 
No MPI support will be enabled for TensorFlow.
Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: 
Not configuring the WORKSPACE for Android builds.
Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apacha Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
```"
26155,protobuf error from build,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04.2 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.1 / 7.5
- GPU model and memory: RTX2080Ti GDDR6 11GB



**Describe the problem**

error during build

**Provide the exact sequence of commands / steps that you executed before running into the problem**

see log below


**Any other info / logs**

./tensorflow/core/framework/node_def_util.h:167:177:   required from here
external/protobuf_archive/src/google/protobuf/map.h:422:29: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsNonEmptyList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
           if (m_->TableEntryIsNonEmptyList(bucket_index_)) {
           ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h:425:30: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsTree(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
           } else if (m_->TableEntryIsTree(bucket_index_)) {
              ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h: In instantiation of 'bool google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::revalidate_if_necessary(google::protobuf::Map<Key, T>::InnerMap::TreeIterator*) [with KeyValueType = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::KeyValuePair; Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::InnerMap::TreeIterator = std::_Rb_tree_const_iterator<std::__cxx11::basic_string<char>*>]':
external/protobuf_archive/src/google/protobuf/map.h:603:12:   required from 'void google::protobuf::Map<Key, T>::InnerMap::erase(google::protobuf::Map<Key, T>::InnerMap::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::InnerMap::iterator_base<google::protobuf::Map<std::__cxx11::basic_string<char>, tensorflow::AttrValue>::KeyValuePair>]'
external/protobuf_archive/src/google/protobuf/map.h:1132:1:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'
external/protobuf_archive/src/google/protobuf/map.h:1137:14:   required from 'void google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator, google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'
external/protobuf_archive/src/google/protobuf/map.h:1140:21:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue]'
external/protobuf_archive/src/google/protobuf/map_field_inl.h:178:1:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::NameAttrList_AttrEntry_DoNotUse; Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)11; int default_enum_value = 0]'
bazel-out/k8-opt/genfiles/tensorflow/core/framework/attr_value.pb.h:1785:15:   required from here
external/protobuf_archive/src/google/protobuf/map.h:484:29: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsNonEmptyList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
         if (m_->TableEntryIsNonEmptyList(bucket_index_)) {
         ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h:498:24: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = tensorflow::AttrValue; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
         return m_->TableEntryIsList(bucket_index_);
         ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h: In instantiation of 'void google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::SearchFrom(google::protobuf::Map<Key, T>::size_type) [with KeyValueType = google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::KeyValuePair; Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]':
external/protobuf_archive/src/google/protobuf/map.h:394:11:   required from 'google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::iterator_base(const google::protobuf::Map<Key, T>::InnerMap*) [with KeyValueType = google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::KeyValuePair; Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map.h:510:28:   required from 'google::protobuf::Map<Key, T>::InnerMap::iterator google::protobuf::Map<Key, T>::InnerMap::begin() [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::InnerMap::iterator_base<google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::KeyValuePair>]'
external/protobuf_archive/src/google/protobuf/map.h:1030:27:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::begin() [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map.h:1140:27:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map_field_inl.h:178:1:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::FunctionDef_RetEntry_DoNotUse; Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; int default_enum_value = 0]'
bazel-out/k8-opt/genfiles/tensorflow/core/framework/function.pb.h:776:14:   required from here
external/protobuf_archive/src/google/protobuf/map.h:422:29: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsNonEmptyList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
           if (m_->TableEntryIsNonEmptyList(bucket_index_)) {
           ~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h:425:30: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsTree(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
           } else if (m_->TableEntryIsTree(bucket_index_)) {
              ~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h: In instantiation of 'bool google::protobuf::Map<Key, T>::InnerMap::iterator_base<KeyValueType>::revalidate_if_necessary(google::protobuf::Map<Key, T>::InnerMap::TreeIterator*) [with KeyValueType = google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::KeyValuePair; Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::InnerMap::TreeIterator = std::_Rb_tree_const_iterator<std::__cxx11::basic_string<char>*>]':
external/protobuf_archive/src/google/protobuf/map.h:603:12:   required from 'void google::protobuf::Map<Key, T>::InnerMap::erase(google::protobuf::Map<Key, T>::InnerMap::iterator) [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::InnerMap::iterator = google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::InnerMap::iterator_base<google::protobuf::Map<std::__cxx11::basic_string<char>, std::__cxx11::basic_string<char> >::KeyValuePair>]'
external/protobuf_archive/src/google/protobuf/map.h:1132:1:   required from 'google::protobuf::Map<Key, T>::iterator google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map.h:1137:14:   required from 'void google::protobuf::Map<Key, T>::erase(google::protobuf::Map<Key, T>::iterator, google::protobuf::Map<Key, T>::iterator) [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map.h:1140:21:   required from 'void google::protobuf::Map<Key, T>::clear() [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>]'
external/protobuf_archive/src/google/protobuf/map_field_inl.h:178:1:   required from 'void google::protobuf::internal::MapField<Derived, Key, T, key_wire_type, value_wire_type, default_enum_value>::Clear() [with Derived = tensorflow::FunctionDef_RetEntry_DoNotUse; Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::internal::WireFormatLite::FieldType kKeyFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; google::protobuf::internal::WireFormatLite::FieldType kValueFieldType = (google::protobuf::internal::WireFormatLite::FieldType)9; int default_enum_value = 0]'
bazel-out/k8-opt/genfiles/tensorflow/core/framework/function.pb.h:776:14:   required from here
external/protobuf_archive/src/google/protobuf/map.h:484:29: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsNonEmptyList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
         if (m_->TableEntryIsNonEmptyList(bucket_index_)) {
         ~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
external/protobuf_archive/src/google/protobuf/map.h:498:24: error: cannot call member function 'bool google::protobuf::Map<Key, T>::InnerMap::TableEntryIsList(google::protobuf::Map<Key, T>::size_type) const [with Key = std::__cxx11::basic_string<char>; T = std::__cxx11::basic_string<char>; google::protobuf::Map<Key, T>::size_type = long unsigned int]' without object
         return m_->TableEntryIsList(bucket_index_);
         ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~
ERROR: /home/wproject/repo/tensorflow/tensorflow/core/kernels/BUILD:3795:1: output 'tensorflow/core/kernels/_objs/topk_op_gpu/topk_op_gpu.cu.pic.o' was not created
ERROR: /home/wproject/repo/tensorflow/tensorflow/core/kernels/BUILD:3795:1: not all outputs were created or valid

"
26153,ERROR: /home/luwei/ML/tensorflow/tensorflow-master/tensorflow/cc/BUILD:460:1: undeclared inclusion(s) in rule '//tensorflow/cc:math_ops_internal':,"@jmhodges @ry @eggie5 @bmabey @djones 
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no
- TensorFlow installed from (source or binary):source
- TensorFlow version:source code
- Python version:3.5.2
- Installed using virtualenv? pip? conda?:bazel build  //tensorflow:libtensorflow_cc.so
- Bazel version (if compiling from source):0.19.0
- GCC/Compiler version (if compiling from source):5.4.0
- CUDA/cuDNN version:no
- GPU model and memory:no



**Describe the problem**
 i tried all kind of suggestion,the error which as below all the time! 
**Provide the exact sequence of commands / steps that you executed before running into the problem**

  bazel build  //tensorflow:libtensorflow_cc.so

**Any other info / logs**

ERROR: /home/luwei/ML/tensorflow/tensorflow-master/tensorflow/cc/BUILD:460:1: undeclared inclusion(s) in rule '//tensorflow/cc:math_ops_internal':
this rule is missing dependency declarations for the following files included by 'tensorflow/cc/ops/math_ops_internal.cc':
  'tensorflow/cc/ops/math_ops_internal.h'
Target //tensorflow:libtensorflow_cc.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 1442.787s, Critical Path: 205.71s
INFO: 3595 processes: 3595 local.
FAILED: Build did NOT complete successfully

"
26151,Tensorflow 1.13: please do not deprecate important functions!!!,"According to Tensorflow 1.13, many important functions will be deprecated, such as:
tf.layers.dense()
tf.layers.dropout()
tf.layers.flatten()
tf.layers.batch_normalization()
...
Since these functions are very widely used, and also very helpful for building complicted models, would you please keep them in the future versions?"
26150,need guide to build with CUDA 10.1,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu18.04.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.13.1
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): >=0.21.0
- GCC/Compiler version (if compiling from source):  7.3.0
- CUDA/cuDNN version: 10.1/7.5
- GPU model and memory:  RTX2080Ti GDDR6 11GB



**Describe the problem**

with CUDA 10.1 came out, build process is messing

there are soft links like . ***.so.10 which points to ****.so.10.1.105 . but tensorflow builder looks for ****.10.0 or ****.10.1

they should point to ***.10 . not ***.10.1 or ****.10.0

and libcublas and libnvblas loacation has been changed to /usr/lib/x86_64-linux-gnu"
26149,Why deprecate tf.layers.dropout?,"In the tensorflow 1.13 document of tf.layers.dropout, there is a warning: 
Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use keras.layers.dropout instead.

It is ridiculous to deprecate tf.layers.dropout, because it is widely used, and very easy to use.
Do you really care about the requirements of TF developers?
Do you really care about the requirements of TF developers?
Do you really care about the requirements of TF developers?"
26148,v1.13.1 CUDA Compilation fails on Windows 10 ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Source
- TensorFlow version:.1.13.1
- Python version:3.6
- Installed using virtualenv? pip? conda?:pip
- Bazel version (if compiling from source):0.21
- GCC/Compiler version (if compiling from source):VS 2017 SDK 10.0.17763.132
- CUDA/cuDNN version:7.4.2
- GPU model and memory: 1060M, 6 GB

**Describe the problem**
Compilation on Windows 10 with CUDA 10 fails. 
**Provide the exact sequence of commands / steps that you executed before running into the problem**

- set PATH=C:\Users\Rishi\AppData\Local\Programs\Python\Python36;%PATH%
- SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin;%PATH%
- SET PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64;%PATH%
- set PATH=c:\tools\cuda\bin;%PATH%
- set BAZEL_SH=C:\tools\msys64\usr\bin\bash.exe
- git clone --recursive https://github.com/tensorflow/tensorflow.git
- cd tensorflow
- git checkout v1.13.1
- pip3 install six numpy wheel
- pip3 install keras_applications==1.0.6 --no-deps
- pip3 install keras_preprocessing==1.0.5 --no-deps
- set PATH=c:\tools\bazel;%PATH%
- set PATH=C:\tools\msys64\usr\bin;%PATH%
- set JAVA_HOME=C:\Program Files\Java\jdk1.8.0_201
- python ./configure.py
- bazel build --config=opt --config=cuda --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xtr1common(87): note: see reference to class template instantiation 'std::integral_constant<bool,false>' being compiled
C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\include\xtr1common(88): note: see reference to class template instantiation 'std::is_same<_Ty1,_Ty2>' being compiled
INFO: From Linking tensorflow/lite/toco/python/_tensorflow_wrap_toco.so:
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
LINK : warning LNK4044: unrecognized option '/ldl'; ignored
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/python/lib_tensorflow_wrap_toco.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/lite/toco/python/lib_tensorflow_wrap_toco.so.exp
libcuda_platform.lo(cuda_dnn.o) : warning LNK4217: locally defined symbol ?ThenBlasGemm@Stream@stream_executor@@QEAAAEAV12@W4Transpose@blas@2@0_K11MAEBV?$DeviceMemory@M@2@H2HMPEAV52@H@Z (public: class stream_executor::Stream & __cdecl stream_executor::Stream::ThenBlasGemm(enum stream_executor::blas::Transpose,enum stream_executor::blas::Transpose,unsigned __int64,unsigned __int64,unsigned __int64,float,class stream_executor::DeviceMemory<float> const &,int,class stream_executor::DeviceMemory<float> const &,int,float,class stream_executor::DeviceMemory<float> *,int)) imported in function ""public: virtual bool __cdecl stream_executor::cuda::CudnnSupport::DoMatMul(class stream_executor::Stream *,class stream_executor::DeviceMemory<float> const &,class stream_executor::DeviceMemory<float> const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::dnn::BatchDescriptor const &,class stream_executor::DeviceMemory<float> *)"" (?DoMatMul@CudnnSupport@cuda@stream_executor@@UEAA_NPEAVStream@3@AEBV?$DeviceMemory@M@3@1AEBVBatchDescriptor@dnn@3@2PEAV53@@Z)
libutils.a(utils.o) : warning LNK4217: locally defined symbol ?DEVICE_CPU@tensorflow@@3QEBDEB (char const * const tensorflow::DEVICE_CPU) imported in function ""bool __cdecl tensorflow::grappler::NodeIsOnCpu(class tensorflow::NodeDef const *)"" (?NodeIsOnCpu@grappler@tensorflow@@YA_NPEBVNodeDef@2@@Z)
ERROR: C:/sw/temp/tensorflow/tensorflow/tensorflow/BUILD:573:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/rishi/_bazel_rishi/t6earjat/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=c:/tools/cuda
    SET PATH=C:\tools\msys64\usr\bin;C:\tools\msys64\bin
    SET PYTHON_BIN_PATH=C:/Users/Rishi/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Rishi/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=3.5,7.0
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/tools/msys64/usr/bin/bash.exe bazel-out/x64_windows-opt/genfiles/tensorflow/tf_python_api_gen_v1.genrule_script.sh
Execution platform: @bazel_tools//platforms:host_platform
Traceback (most recent call last):
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Rishi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Rishi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""\\?\C:\Users\Rishi\AppData\Local\Temp\Bazel.runfiles_md1fa9q2\runfiles\org_tensorflow\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Rishi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Rishi\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 8639.073s, Critical Path: 1339.78s
INFO: 2615 processes: 2615 local.
FAILED: Build did NOT complete successfully
`
"
26147,[docs] (non) determinism in TF,"**Describe the feature and the current behavior/state.**
A documentation explaining determinism and sources of non determinism in TF applications. It should cover things from having multiple readers, seeds in the specific ops, seed in numpy, PYTHONHASHSEED, non-deterministic ops in CUDA (and why TF does not support passing deterministic flag even though it would be slower), `TF_CUDNN_USE_AUTOTUNE`, and probably some others I do not know about.

**Who will benefit with this feature?**
I take part in many Kaggle competitions and very often people there have problems with non-determinism in NN. 
"
26146,Tensorflow 1.13 - can't deprecate tf.layers.batch_normalization.  No existing replacement,"I'm updating to Tensorflow 1.13 from 1.12.

tf.layers.batch_normalization is being deprecated.

The note states:

```
batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.batch_normalization instead.
```
First, there is no 'keras.layers.batch_normalization'. This is a 'keras.layers. BatchNormalization', but that is a different function and a direct replacement for 'tf.layers. BatchNormalization'.

Therefore, either make a 'keras.layers. BatchNormalization' function or remove the deprecation warning for 'tf.layers.batch_normalization.

Here's the link to the batch_normalization fuction.

https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/layers/normalization.py

Thanks,"
26145,Tensorflow 1.13 - can't deprecate tf.layers.conv2d - no existing replacement,"I'm updating to Tensorflow 1.13 from 1.12.

tf.layers.conv2d is being deprecated.  

The note states:
```
conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.conv2d instead.
``` 

First, there is no 'keras.layers.conv2d'.  This is a 'keras.layers.Conv2D', but that is a different function and a direct replacement for 'tf.layers.Conv2D'.  

Therefore, either make a 'keras.layers.conv2d' function or remove the deprecation warning for 'tf.layers.conv2d'.  

Here's the link to the conv2d fuction.

https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/python/layers/convolutional.py

Thanks,
"
26144,Why deprecate tf.layers.dense?,"In the tensorflow 1.13 document of tf.layers.dense, there is a warning:
Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Use keras.layers.dense instead.

I am very confused, because this function is very easy to use, and I used it very frequently. Why do you deprecate it? 

By the way, I did not find the alternative keras.layers.dense, there is only ""keras.layers.Dense"" "
26143,[TF2.0] Error Logging for GradientTape,"Hello everyone,

I was wondering if there is an option for error logging / or could we have tf output error message for gradient calculation. In the example below, the below will output `None` values in the current setting but will output correct gradients when the `tf.Variable`s are a `float` type. My question is, could we please add an error message stating something like `gradient calculation supports only float types`?

Best Regards,
Seung-jae Bang

```
def forward(a, b):
    """"""f = a * b""""""
    return a * b

params = [tf.Variable(1), tf.Variable(2)]

with tf.GradientTape() as tape:
    result = forward(*params)

tape.gradient(result, params)
```
**System information**

* Linux
* TensorFlow installed from `pip install -U tf-nightly-2.0-preview` - ""2.0.0-dev20190226""
* Python version: 3.6

ccing: @random-forests "
26141,Python 3.6.8 and Tensorflow 1.12.0 not working on windows 2008 R2,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 2008 R2 64-bit
- TensorFlow installed from (source or binary): source (pip install tensorflow)
- TensorFlow version: 1.12.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- GPU model and memory: 4GB RAM



**Describe the problem**

I'm trying to import the tensorflow 1.12.0 library in python 3.6.8 but at the initial stage itself in getting following error.
Please help me out with the same.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


Successfully installed tensorflow-1.12.0

C:\Tensorflow\models\research>python
Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load
_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load
_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\_
_init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\p
ython\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load
_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Administrator\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load
_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26140,Tensorflow 1.13.1 not available on pypi,"Hello! I see that 1.13.1 has been released yesterday, but it seems like it's not yet on pypi. When will it be available?
"
26139,"bazel-bin transform_graph err, loading graph ..pb failed with can't parse pb file ","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
26138,TensorFlow v1.12.0 Test failures on s390x,"**System information**
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, 18.04 s390x
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
TensorFlow installed from (source or binary): source
TensorFlow version: v1.12.0
Python version: 2.7.x
Installed using virtualenv? pip? conda?: Building from source
Bazel version (if compiling from source): v0.15.0
GCC/Compiler version (if compiling from source): 7.3.0 (Ubuntu 18.04), 5.4.0 (Ubuntu 16.04)
CUDA/cuDNN version: NA
GPU model and memory: NA

**Describe the problem**
We have build TensorFlow v1.12.0 from source on Ubuntu 16.04, 18.04 on s390x platform. Observed test failures from contrib and python module. 


```
/tensorflow/contrib/metrics:metric_ops_test
//tensorflow/contrib/distributions:independent_test
//tensorflow/contrib/slim/python/slim/data:tfexample_decoder_test
//tensorflow/core/util/tensor_bundle:tensor_bundle_test
//tensorflow/python/kernel_tests:concat_op_test
//tensorflow/python/kernel_tests:fft_ops_test
//tensorflow/python/kernel_tests:segment_reduction_ops_test
//tensorflow/python:cluster_test
//tensorflow/python:cost_analyzer_test
//tensorflow/contrib/layers:normalization_test
//tensorflow/python:nn_test
```

Most of the tests are failing with an error: `AssertionError: Not equal to tolerance rtol, atol.
`
Could you please let us know the severity of these failures? Are these covering/affecting any critical functionality of TensorFlow?
Accordingly, we can start debugging the test failures.


"
26137,RandomUniform,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version (or github SHA if from source):


**Provide the text output from tflite_convert**

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONV_2D, DEPTHWISE_CONV_2D, DIV, FLOOR, FULLY_CONNECTED, MAX_POOL_2D, MUL, RESHAPE, SHAPE, SUB. Here is a list of operators for which you will need custom implementations: RandomUniform"
26136,Estimator.train doesn't respect the RunConfig keep_checkpoint_max limit,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
UBUNTU 16.04.5 LTS (Xenial Xerus)
- TensorFlow installed from (source or binary):
pip version 1.12 tensorflow-gpu
- TensorFlow version (use command below):
1.12.0
- Python version:
3.5

- CUDA/cuDNN version:
9.0
- GPU model and memory:
Tesla V100 16G

**Describe the current behavior**
Running Estimator training as follows, should only keep 1 checkpoint file version (latest)
instead all checkpoints are kept.

session_config = tf.ConfigProto()
session_config.allow_soft_placement = True

run_conf = RunConfig(
            model_dir=out_dir,
            save_summary_steps=400,
            save_checkpoints_steps=5000,
            keep_checkpoint_max=1,
            session_config=session_config,
            tf_random_seed=None, 
            keep_checkpoint_every_n_hours=None,
            log_step_count_steps=10000, 
            train_distribute=None, 
            device_fn=None,
            protocol=None,
            eval_distribute=None,
            experimental_distribute=None
        )


estimator = tf.estimator.Estimator(
            model_fn=model.build_model_fn,
            config=run_config,
            params=params
        )



**Describe the expected behavior**
Only keep_checkpoint_max checkpoints should be saved




"
26135, //tensorflow/contrib/layers:normalization_test fails with Assertion error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):     Linux Ubuntu 16.04 s390x
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): gcc version 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
python tensorflow/contrib/layers/python/layers/normalization_test.py fails on s390x with following parameters
input_shape = (1, 100, 100, 1)
data_format =  'NCHW' and 
tolerance = 1e-3
mu = 1e2
sigma = 0.1

**Describe the expected behavior**
It should pass on s390x like on Intel.

**Code to reproduce the issue**
python tensorflow/contrib/layers/python/layers/normalization_test.py Or
bazel test -c dbg  //tensorflow/contrib/layers:normalization_test

root@1da9d9e529b7:/home/test/tensorflow# python tensorflow/contrib/layers/python/layers/normalization_test.py
..name:  GroupNorm/Reshape_2
......................name:  InstanceNorm/instancenorm/add_1
....F.........
FAIL: testOutputBigInput4DNCHW (__main__.InstanceNormTest)

Traceback (most recent call last):
  File ""tensorflow/contrib/layers/python/layers/normalization_test.py"", line 172, in testOutputBigInput4DNCHW
    self.doOutputTest((1, 100, 100, 1), 'NCHW', tol=1e-3)
  File ""tensorflow/contrib/layers/python/layers/normalization_test.py"", line 155, in doOutputTest
    self.assertAllClose(expected_mean, mean, rtol=tol, atol=tol)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1591, in assertAllClose
    self._assertAllCloseRecursive(a, b, rtol=rtol, atol=atol, msg=msg)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1561, in _assertAllCloseRecursive
    (path_str, path_str, msg)))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/test_util.py"", line 1496, in _assertArrayLikeAllClose
    a, b, rtol=rtol, atol=atol, err_msg=""\n"".join(msgs), equal_nan=True)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 1395, in assert_allclose
    verbose=verbose, header=header, equal_nan=equal_nan)
  File ""/usr/local/lib/python2.7/dist-packages/numpy/testing/utils.py"", line 778, in assert_array_compare
    raise AssertionError(msg)
AssertionError:
Not equal to tolerance rtol=0.001, atol=0.001
Mismatched value: a is different from b.
not close where = (array([0, 0, 0, 0, 0, 0]), array([ 8, 34, 44, 78, 87, 89]))
not close lhs = [ 0.  0.  0.  0.  0.  0.]
not close rhs = [-0.00105469 -0.00128662  0.00124512  0.00106445 -0.00124512 -0.00104004]
not close dif = [ 0.00105469  0.00128662  0.00124512  0.00106445  0.00124512  0.00104004]
not close tol = [ 0.00100105  0.00100129  0.00100125  0.00100106  0.00100125  0.00100104]
dtype = float64, shape = (1, 100)
(mismatch 6.0%)
 x: array([[ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,
         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,...
 y: array([[  2.612305e-04,   1.684570e-04,  -1.342773e-04,  -1.684570e-04,
         -7.495117e-04,  -3.417969e-05,  -5.371094e-05,   6.567383e-04,
         -1.054688e-03,   2.319336e-04,  -1.318359e-04,  -3.222656e-04,...

----------------------------------------------------------------------
Ran 38 tests in 16.724s

FAILED (failures=1)
"
26133,Java Tensorflow save & restore without using filesystem,"**System information**
- TensorFlow version (you are using): Up to date
- Are you willing to contribute it (Yes/No): 


**Describe the feature and the current behaviour/state.**
I previously opened a feature ticket and asking for a **python** approach.
https://github.com/tensorflow/tensorflow/issues/25839

I wasn't aware there is **java tensorflow**.

I would like to retrieve(instead of Save) training information and restore it to a Tensorflow session when necessary (without using a file system). I want to bypass filesystem.

### I want to know a line of code or method that I can use to extract training data(as an object) and import it to (tf)session when necessary without me handling file path etc

to elaborate =>
I found that there is a way to retrieve updated weight values.
https://www.quora.com/How-do-I-print-weights-of-a-fully-connected-neural-network-in-tensorflow
//train your network here
sess.run(optimizer, feed_dict={x:data, y:labels}
print(""layer 1 weights:"", sess.run(W1))
print(""layer 2 weights:"", sess.run(W2))

**Rather than handling each individual tensor information, I want to extract the whole training information (as an object) and import it to (tf)session when necessary.**

I have looked at the followings and searched but was not successful.
https://www.tensorflow.org/guide/saved_model
https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model

Why does this feature not appear already present?

**Will this change the current API? How?**
no, I just need(or to know) an additional API.

**Who will benefit with this feature?**
It will make the use of Tensorflow lib much more flexible.
**Any Other info.**"
26132,Tensorflow hang when specify 'nccl/xring' as all reduce alg ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:No
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.13.0-rc0
- Python version:2.7.12
- Bazel version (if compiling from source):0.20.0
- GCC/Compiler version (if compiling from source):5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)
- CUDA/cuDNN version:CUDA version:9.0.176 and cuDNN version:7.2.1.38
- GPU model and memory:Tesla V100 , 32480MiB

**Describe the current behavior**
I train NeuMF model in distribute environment, and I find that TensorFlow hang all the time when training finish.   
Below is some detail about distributed training:
Model : `NeuMF`
Dataset : `ml-1m`
Num_worker : `2`
Num_gpu_each_worker: `4`
Strategy : `tf.contrib.distribute.MirrorStrategy` 
Reduce_alg : replace `pscpu/pscpu` with `nccl/xring`
Nccl version : `2.3.7-1+cuda9.0`

The distributed training run 120 steps then tensorflow quit when use default all reduce alg `pscpu/pscpu` of `MirrorStrategy`. And when replace default all reduce alg with `nccl/xring` , distributed training run 120 steps, but TensorFlow hang all the time. Below is screen output of two conditions:  
log when normal exit:
```
I0226 10:04:21.481378 140388122244864 basic_session_run_hooks.py:247] loss = 0.3570031, step = 120 (1.436 sec)
I0226 10:04:21.483242 140388122244864 basic_session_run_hooks.py:680] global_step/sec: 6.96639
I0226 10:04:22.065148 140388122244864 util.py:168] Finalize strategy.
I0226 10:04:22.066411 140388122244864 basic_session_run_hooks.py:594] Saving checkpoints for 125 into /tmp/ncf_model/model.ckpt.
I0226 10:04:22.903732 140388122244864 estimator.py:359] Loss for final step: 0.35619634.
I0226 10:04:22.906255 140388122244864 data_preprocessing.py:391] Shutting down train data creation subprocess.
```
log when hang:
```
I0226 10:13:19.793405 140644590810880 basic_session_run_hooks.py:247] loss = 0.360583, step = 100 (1.755 sec)
I0226 10:13:19.795331 140644590810880 basic_session_run_hooks.py:680] global_step/sec: 5.70165
I0226 10:13:21.573970 140644590810880 basic_session_run_hooks.py:247] loss = 0.35865137, step = 110 (1.781 sec)
I0226 10:13:21.576019 140644590810880 basic_session_run_hooks.py:680] global_step/sec: 5.616
I0226 10:13:23.344115 140644590810880 basic_session_run_hooks.py:247] loss = 0.35676146, step = 120 (1.770 sec)
I0226 10:13:23.347656 140644590810880 basic_session_run_hooks.py:680] global_step/sec: 5.64505
``` 
Compare two log,  TensorFlow do not continue to save checkpoints and output final loss when use `nccl/xring` all reduce alg. How to solve this problem, thanks.

**Code to reproduce the issue**
I get NeuMF model source code from official models repo, and original code only support local training with estimator, and I add below code to support distributed training:  
```
distribution = tf.contrib.distribute.MirroredStrategy(
    num_gpus_per_worker=num_gpus)
    
run_config = tf.estimator.RunConfig(
    model_dir=model_dir,
    log_step_count_steps=10,
    train_distribute=distribution,               
    eval_distribute=distribution)
    
 estimator = tf.estimator.Estimator(
    model_fn=model_fn,
    model_dir=model_dir,
    config=run_config, 
    params=params)
    
 train_spec = tf.estimator.TrainSpec(
    train_input_fn,max_steps=5000,hooks=train_hooks)
 eval_spec = tf.estimator.EvalSpec(eval_input_fn,steps=100)
 tf.estimator.train_and_evaluate(estimator,train_spec,eval_spec)
```

The `cross device ops` implement of `MirrorStrategy`  is `MultiWorkerAllReduce`  which is defined in `tensorflow/python/distribute/cross_device_ops.py`, I change value of `all_reduce_alg`  parameter of `__init__`  of class `MultiworkerAllReduce` from `pscpu/pscpu` to `nccl/xring`.
  
**Other info / logs**
When hang, stack of two worker is:
```
#0  0x00007f990858ba13 in epoll_wait () at ../sysdeps/unix/syscall-template.S:84
#1  0x00007f988f250d98 in pollset_work () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007f988f270dcf in cq_pluck(grpc_completion_queue*, void*, gpr_timespec, void*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007f988f27119b in grpc_completion_queue_pluck () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f988f1dc898 in grpc::CoreCodegen::grpc_completion_queue_pluck(grpc_completion_queue*, void*, gpr_timespec, void*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f988882ad02 in grpc::CompletionQueue::Pluck(grpc::internal::CompletionQueueTag*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007f988882f056 in grpc::internal::BlockingUnaryCallImpl<tensorflow::RunStepRequest, tensorflow::RunStepResponse>::BlockingUnaryCallImpl(grpc::ChannelInterface*, grpc::internal::RpcMethod const&, grpc::ClientContext*, tensorflow::RunStepRequest const&, tensorflow::RunStepResponse*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007f988882f26d in tensorflow::grpc::MasterService::Stub::RunStep(grpc::ClientContext*, tensorflow::RunStepRequest const&, tensorflow::RunStepResponse*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007f98886b3f6d in tensorflow::GrpcRemoteMaster::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007f98886adfc9 in tensorflow::GrpcSession::RunProto(tensorflow::CallOptions*, tensorflow::MutableRunStepRequestWrapper*, tensorflow::MutableRunStepResponseWrapper*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007f98886aec52 in tensorflow::GrpcSession::RunHelper(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#11 0x00007f98886af3e0 in tensorflow::GrpcSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#12 0x00007f9888693f42 in tensorflow::SessionRef::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#13 0x00007f98888997a1 in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor>, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, tensorflow::Tensor> > > const&, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Tensor**, std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&, TF_Buffer*, TF_Status*) [clone .constprop.664] ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#14 0x00007f9888899f9e in TF_SessionRun () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#15 0x00007f988868fbc9 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#16 0x00007f988868fc62 in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#17 0x00007f988864a70a in _wrap_TF_SessionRun_wrapper () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#18 0x00000000004bc4aa in PyEval_EvalFrameEx ()
#19 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#20 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#21 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#22 0x00000000004d5669 in ?? ()
#23 0x00000000004a587e in PyObject_Call ()
#24 0x00000000004be51e in PyEval_EvalFrameEx ()
---Type <return> to continue, or q <return> to quit---
```
```
90      pthread_join.c: No such file or directory.
bt
#0  0x00007f65d1baf98d in pthread_join (threadid=140057709098752, thread_return=0x0) at pthread_join.c:90
#1  0x00007f6540377b97 in std::thread::join() () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#2  0x00007f654ef477a0 in tensorflow::(anonymous namespace)::StdThread::~StdThread() () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#3  0x00007f6551b6c032 in tensorflow::GrpcServer::Join() () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007f6551be4a8c in TF_ServerJoin () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007f655198d756 in _wrap_TF_ServerJoin () from /home/zxy/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00000000004bc4aa in PyEval_EvalFrameEx ()
#7  0x00000000004b9b66 in PyEval_EvalCodeEx ()
#8  0x00000000004c1f56 in PyEval_EvalFrameEx ()
#9  0x00000000004b9b66 in PyEval_EvalCodeEx ()
#10 0x00000000004c17c6 in PyEval_EvalFrameEx ()
#11 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#12 0x00000000004c17c6 in PyEval_EvalFrameEx ()
#13 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#14 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#15 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#16 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#17 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#18 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#19 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#20 0x00000000004c1f56 in PyEval_EvalFrameEx ()
#21 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#22 0x00000000004c17c6 in PyEval_EvalFrameEx ()
#23 0x00000000004b9b66 in PyEval_EvalCodeEx ()
#24 0x00000000004eb69f in ?? ()
#25 0x00000000004e58f2 in PyRun_FileExFlags ()
#26 0x00000000004e41a6 in PyRun_SimpleFileExFlags ()
#27 0x00000000004938ce in Py_Main ()
#28 0x00007f65d17fd830 in __libc_start_main (main=0x493370 <main>, argc=30, argv=0x7ffdb8177748, init=<optimized out>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7ffdb8177738)
    at ../csu/libc-start.c:291
#29 0x0000000000493299 in _start ()
```
"
26131,Modify Partitioning and Scheduling,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.1
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Hi,
I'm trying to modify this combinatorial aspect in order to minimize the execution time of the global schedule. I'm looking for the files that contain the partitioning function that assigns tensors to devices and the scheduling function that assigns tensors to time slot in which the tensors are executed.
Does anyone have any ideas?

**Will this change the current api? How?**
Not sure

**Who will benefit with this feature?**
Anyone who wants to optimize this combinatorial problem.
"
26130,tf.contrib.rnn.LayerNormBasicLSTMCell does not work with keras RNN,"**System information**

- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13
- Python version: 3.6


**Describe the current behavior**

Normal RNN cells work with `keras.layers.RNN` which allows to use arbitrary rnn cell class (replacing `tf.nn.dynamic_rnn` in 2.0 ?):

```python
x = tf.zeros([1, 10, 7])   # batch_size, time_steps, channel
cell = tf.nn.rnn_cell.BasicLSTMCell(7, dtype=tf.float32)
tf.keras.layers.RNN(cell, return_sequences=True)(x)

# Result: <tf.Tensor 'rnn_1/transpose_1:0' shape=(1, 10, 7) dtype=float32>
```

But LayerNormBasicLSTMCell does not work:


```python
x = tf.zeros([1, 10, 7])   # batch_size, time_steps, channel
cell = tf.contrib.rnn.LayerNormBasicLSTMCell(256)
tf.keras.layers.RNN(cell, return_sequences=True)(x)
```

An error happens:

```python
..../lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py in
_get_single_variable(self, name, shape, dtype, initializer, regularizer,
 partition_info, reuse, trainable, collections, caching_device, 
 validate_shape, use_resource, constraint, synchronization, aggregation)
    846         tb = [x for x in tb if ""tensorflow/python"" not in x[0]][:3]
    847         raise ValueError(""%s Originally defined at:\n\n%s"" % (err_msg, """".join(
--> 848             traceback.format_list(tb))))
    849       found_var = self._vars[name]
    850       if not shape.is_compatible_with(found_var.get_shape()):

ValueError: Variable kernel already exists, disallowed.
Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:


  File ""..../lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py"", line 1430, in _linear
    weights = vs.get_variable(""kernel"", [proj_size, out_size], dtype=dtype)
  File ""..../lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py"", line 1441, in call
    concat = self._linear(args)
  File ""<ipython-input-25-fd6ef19bdfdb>"", line 1, in <module>
    tf.keras.layers.RNN(cell, return_sequences=True)(x)
```
"
26129,"""The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.""","**I have same issue.
  I have doing real time face recognition using tensorflow. I'm follow this link ""https://github.com/btwardow/tf-face-recognition"" When I'm download clone repo it work in one system but try to run another one this show me error. Please help me out.**

`Traceback (most recent call last):
File ""server.py"", line 49, in detect
faces = recognize(detection.get_faces(image, threshold))
File ""/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/recognition.py"", line 19, in recognize
X[i, :] = embedding(img_to_np(img))
File ""/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/embedding.py"", line 43, in embedding
images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3666, in get_tensor_by_name
return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3490, in as_graph_element
return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3532, in _as_graph_element_locked
""graph."" % (repr(name), repr(op_name)))
**KeyError: ""The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.""**
[2019-02-26 11:28:06,532] ERROR in app: Exception on /detect [POST]
Traceback (most recent call last):
File ""server.py"", line 49, in detect
faces = recognize(detection.get_faces(image, threshold))
File ""/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/recognition.py"", line 19, in recognize
X[i, :] = embedding(img_to_np(img))
File ""/home/bigblue/Music/Test/tf-face-recognition-master/tensorface/embedding.py"", line 43, in embedding
images_placeholder = tf.get_default_graph().get_tensor_by_name(""input:0"")
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3666, in get_tensor_by_name
return self.as_graph_element(name, allow_tensor=True, allow_operation=False)
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3490, in as_graph_element
return self._as_graph_element_locked(obj, allow_tensor, allow_operation)
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3532, in _as_graph_element_locked
""graph."" % (repr(name), repr(op_name)))
KeyError: ""The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.""

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py"", line 2292, in wsgi_app
response = self.full_dispatch_request()
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py"", line 1815, in full_dispatch_request
rv = self.handle_user_exception(e)
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py"", line 1718, in handle_user_exception
reraise(exc_type, exc_value, tb)
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/_compat.py"", line 35, in reraise
raise value
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py"", line 1813, in full_dispatch_request
rv = self.dispatch_request()
File ""/home/bigblue/anaconda3/envs/my_env/lib/python3.6/site-packages/flask/app.py"", line 1799, in dispatch_request
return self.view_functions[rule.endpoint](**req.view_args)
File ""server.py"", line 69, in detect
print('POST /detect error: %e' % e)
TypeError: must be real number, not KeyError`"
26128,"KeyError: ""The name 'input:0' refers to a Tensor which does not exist. The operation, 'input', does not exist in the graph.""","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
26123,"For an SSD mobilenetv2 model, tflite quantized_uint8 inference is slower on some android devices than float inference","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, using the code in the tflite example apps.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac 10.14.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy S8 [Snapdragon 835] (API 26), HTC Bolt [Snapdragon 810] (API 24)
- TensorFlow installed from (source or binary): Binary (using implementation 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on android)
- TensorFlow version (use command below): ('v1.12.0-rc0-17-g7b08198113', '1.12.0-rc1')
- Python version: 2.7.15
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
On the HTC Bolt, the performance of the SSD mobilenetv2 model is about the same for a quantized_uint8 model as well as a float model. However, on the Samsung Galaxy S8, the quantized_uint8 model is about twice as fast.

**Describe the expected behavior**
I would expect the quantized_uint8 model to be faster in every scenario (this is what I observed, for example on iOS)."
26114,ImportError: DLL load failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version: 1.12.0
- Python version: Python 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.2.148.1
- GPU model and memory: GTX 1060 6GB



**Describe the problem**

work well with tensorflow, but get error when change to tensorflow-gpu

I ran ```pip install tensorflow``` and all things goes OK, but then I ran ```pip install tensorflow-gpu```, get errors below

```
Traceback (most recent call last):
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: 


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


"
26108,2.0 Compile is leaking memory.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (2 different machines)
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): gpu 2.0.0-dev20190214, gpu 2.0.0-dev20190224
- Python version: 3.6.6
- CUDA/cuDNN version: 10
- GPU model and memory: 7gb  K5200, 4gb GTX 970

**Describe the current behavior**
Compile leaks memory.
**Describe the expected behavior**
Compile clears the memory when overwritten
**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])

for i in range(100000):
    model.compile(optimizer='adam',
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])
```

Memory slowly increases about 20mb at a time.
I discovered this whilst working on a genetic algorithm which generates models which are then compiled and tested, but it fills my memory and I have narrowed it down to this simple code.
"
26107,Big memory consumption conv2d vs conv3d,"### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
Yes
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
VERSION=""16.04.5 LTS (Xenial Xerus)""
- **TensorFlow installed from**: 
Binary
- **TensorFlow version**:
1.12.0
- **Python version**:
Python 3.5.2
- **CUDA/cuDNN version**:
CUDA: 9.0, V9.0.176
cuDNN: 7.4.2
- **GPU model and memory**:
NVIDIA Tesla V100 SXM2, 32GB
Driver Version: 384.145

### Describe the problem
Conv2D layer consumes a lot of memory comparing to the same operation performed by Conv3D.
I have 2 independent graphs:
1)
Input (shape=[16, 224, 224, 4])
conv2d (padding = SAME, filter=[3, 3, 4, 16])
bias_add (shape=[16])
2)
Input (shape=[1,16, 224, 224, 4])
conv3d (padding = SAME, filter=[1, 3, 3, 4, 16])
bias_add (shape=[16])

TF profiler reports that conv2d operation in first graph consumes 2900MB whereas conv3d that I presume should perform the same operation (cause leading dimensions of filter is 1) consumes 269.2MB which is an order of magnitude less.

Also for the graph with conv2d I see 2 additional layers are injected:
Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer consumes 16.78MB
Conv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer consumes 67.11MB
these layers are disappeared if I remove bias_add operation but memory consumption still stays the same.

Am i missing something obvious or my expectations regarding conv2d vs conv3d doing the same in my case are wrong?

### Source code / logs
```
import os, time
os.environ[""CUDA_VISIBLE_DEVICES""] = os.environ.get(""CUDA_VISIBLE_DEVICES"", ""1"")

import tensorflow as tf 
import numpy as np

def test_conv2d():
    input_ = tf.placeholder(tf.float32, shape=[16, 224, 224, 4], name=""input_2d"")
    filter_ = tf.get_variable(dtype=tf.float32, shape=[3, 3, 4, 16], name=""filter_2d"")
    conv = tf.nn.conv2d(input_, filter_, strides=(1,1,1,1), padding='SAME', dilations=(1,1,1,1))

    vBias1 = tf.get_variable(name='bias_2d', shape=[16], dtype=tf.float32)
    lBias1 = tf.nn.bias_add(conv, vBias1)
    return lBias1

def test_conv3d():
    input_ = tf.placeholder(tf.float32, shape=[1, 16, 224, 224, 4], name=""input_3d"")
    filter_ = tf.get_variable(dtype=tf.float32, shape=[1, 3, 3, 4, 16], name=""filter_3d"")
    conv = tf.nn.conv3d(input_, filter_, strides=(1,1,1,1,1), padding='SAME', dilations=(1,1,1,1,1))

    vBias1 = tf.get_variable(name='bias_3d', shape=[16], dtype=tf.float32)
    lBias1 = tf.nn.bias_add(conv, vBias1)
    return lBias1

gpu_options = tf.GPUOptions(allow_growth=True, visible_device_list=str(0))
config = tf.ConfigProto(gpu_options=gpu_options)

run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()

inputs = [
    {
        ""l"":test_conv2d(), 
        ""i"":{""input_2d:0"": np.random.random([16, 224, 224, 4])}
    },
    {
        ""l"":test_conv3d(), 
        ""i"":{""input_3d:0"": np.random.random([1, 16, 224, 224, 4])}
    }
]
outputs = []
for input in inputs:
    with tf.Session(config=config) as sess:
        sess.run(tf.global_variables_initializer())
        sess.run(
            input[""l""], 
            input[""i""],
            options = run_options,
            run_metadata = run_metadata)
        
        tf.profiler.profile(tf.get_default_graph(),
                                run_meta=run_metadata,
                                cmd='op',
                                options=tf.profiler.ProfileOptionBuilder.time_and_memory())
```
### Output
1)
Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv2D                      2900.02MB (100.00%, 97.19%),      2.70sec (100.00%, 99.83%),      14.11ms (100.00%, 97.40%),      2.68sec (100.00%, 99.84%)
Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer        16.78MB (2.81%, 0.56%),          4.23ms (0.17%, 0.16%),            48us (2.60%, 0.33%),          4.18ms (0.16%, 0.16%)
BiasAdd                               0B (0.00%, 0.00%),           207us (0.02%, 0.01%),           168us (2.27%, 1.16%),            39us (0.00%, 0.00%)
Conv2D-0-0-TransposeNCHWToNHWC-LayoutOptimizer        67.11MB (2.25%, 2.25%),           202us (0.01%, 0.01%),           160us (1.10%, 1.10%),            42us (0.00%, 0.00%)
VariableV2                        2.56KB (0.00%, 0.00%),            20us (0.00%, 0.00%),             0us (0.00%, 0.00%),            20us (0.00%, 0.00%)

2)
Profile:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
Conv3D                      269.20MB (100.00%, 100.00%),     137.06ms (100.00%, 99.85%),      64.03ms (100.00%, 99.74%),      73.03ms (100.00%, 99.94%)
BiasAdd                               0B (0.00%, 0.00%),           197us (0.15%, 0.14%),           168us (0.26%, 0.26%),            29us (0.06%, 0.04%)
VariableV2                        2.56KB (0.00%, 0.00%),            12us (0.01%, 0.01%),             0us (0.00%, 0.00%),            12us (0.02%, 0.02%)
"
26105,vocab_size = len(tokenizer.word_index) ?,"Update it to ""vocab_size = top_k""? "
26104,[TF2.0] Variable with dynamic shape,"Hello everyone,

Why variable's `assign` doesn't work for different shapes? It doesn't look like correct behaviour.

```python
In [5]: v = tf.Variable([1.0])  # tf.Variable([1.0], validate_shape=False) doesn't work as well

In [6]: v.assign(tf.random.normal((1, 1)))
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-6-27ac642ab52e> in <module>
----> 1 v.assign(tf.random.normal((1, 1)))

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py in assign(self, value, use_locking, name, read_value)
   1052     with _handle_graph(self.handle):
   1053       value_tensor = ops.convert_to_tensor(value, dtype=self.dtype)
-> 1054       self._shape.assert_is_compatible_with(value_tensor.shape)
   1055       assign_op = gen_resource_variable_ops.assign_variable_op(
   1056           self.handle, value_tensor, name=name)

~/anaconda3/envs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/tensor_shape.py in assert_is_compatible_with(self, other)
   1070     """"""
   1071     if not self.is_compatible_with(other):
-> 1072       raise ValueError(""Shapes %s and %s are incompatible"" % (self, other))
   1073
   1074   def most_specific_compatible_shape(self, other):

ValueError: Shapes (1,) and (1, 1) are incompatible
```


**System information**
- macOS Mojave
- TensorFlow installed from `pip install -U tf-nightly-2.0-preview` - ""2.0.0.dev20190225""
- Python version: 3.6

Kind regards,
Artem Artemev"
26102,ResizeNearestNeighbor op not supported in TFlite conversion,"<em>
**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.13.0-dev20190219
- Python version: 3

**Describe the current behavior**
ToCo converter is throwing an error when trying to convert ResizeNearestNeighbour op.

**Describe the expected behavior**
Should convert fine

**Code to reproduce the issue**
https://github.com/FCOS/MastersWork/tree/master/error
Download files and edit model location in run.py

**Other info / logs**
ConverterError: TOCO failed. See console for info.
2019-02-25 19:02:10.960423: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: ResizeNearestNeighbor
2019-02-25 19:02:10.960608: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1055] Converting unsupported operation: ResizeNearestNeighbor
2019-02-25 19:02:10.961120: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 88 operators, 126 arrays (0 quantized)
2019-02-25 19:02:10.961641: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 88 operators, 126 arrays (0 quantized)
2019-02-25 19:02:10.962586: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 20 operators, 53 arrays (0 quantized)
2019-02-25 19:02:10.962719: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 20 operators, 53 arrays (0 quantized)
2019-02-25 19:02:10.962839: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 24305664 bytes, theoretical optimal value: 12247040 bytes.
2019-02-25 19:02:10.962964: F tensorflow/contrib/lite/toco/tflite/export.cc:374] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.TocoConverter(). Here is a list of operators for which  you will need custom implementations: ResizeNearestNeighbor.
Aborted (core dumped)
"
26100,Parallel quasi-newton optimizers for tensorflow-gpu,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
Is there any interest in implementing quasi-Newton methods for tensorflow-gpu? There has been some work on a parallel L-BFGS-B method (https://www.sciencedirect.com/science/article/pii/S0097849314000119) for GPus.

**Will this change the current api? How?**
New implementation.

**Who will benefit with this feature?**
I have had some recent success with using the BFGS optimizer for feedforward neural network. This approach, however, is limited to small networks and to tensorflow-cpu.

**Any Other info.**

"
26099,tf.one_hot crashes when indices is tf.uint8,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 / Windows 7
- TensorFlow installed from (source or binary): Official pip source (tensorflow-gpu)
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- CUDA/cuDNN version: 9.0 / 7.5
- GPU model and memory: 1080Ti / 12GB

**Describe the current behavior**
tf.one_hot crashes when the indices tensor has dtype=tf.uint8
The error message shows `Check failed: new_num_elements == NumElements()`

**Code to reproduce the issue**
https://gist.github.com/elmirador/4fc5148e5044478d668237209d265eac

**Other info / logs**
I've also tested under TF 1.4.1 and TF 1.10.0 (both on GPU) on different machines, both have the same problem."
26098,[TF 2.0] optimizer_v2.get_updates() eager mode problem,"**TF Version: 2.0.0-dev20190214
Windows 10
Anaconda Python 3.6.5
GPU: GeForce GTX 1070 Max-Q Design
[Tensorflow 2.0 (gpu) nightly](https://pypi.org/project/tf-nightly-gpu-2.0-preview/) installed via pip.**

**Describe the current behavior**
Calling [`<AnyOptimizerV2>.get_updates(params=my_model.trainable_weights, loss=my_loss)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L447) fails with any `my_loss` (due to eager mode being on):

```
    actor_updates = self.optimizer_actor.get_updates(params=self.actor.trainable_weights, loss=None)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py"", line 448, in get_updates
    grads = self.get_gradients(loss, params)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\optimizer_v2\optimizer_v2.py"", line 361, in get_gradients
    grads = gradients.gradients(loss, params)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_impl.py"", line 158, in gradients
    unconnected_gradients)
  File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\ops\gradients_util.py"", line 547, in _GradientsHelper
    raise RuntimeError(""tf.gradients is not supported when eager execution ""
RuntimeError: tf.gradients is not supported when eager execution is enabled. Use tf.GradientTape instead.
```
Globally disabling eager execution via `tf.compat.v1.disable_eager_execution()` fixes this particular issue but I don't want to globally disable eager mode! I'd like to know how the 2.0 API is intended to be used in this case.

**Describe the expected behavior**
Since the gradient computation is happening internally I expected behavior similar to TF1 i.e. computation of gradients in graph mode in this case. Also I don't want to use `tf.GradientTape` every time I want to compute the gradient instances - instead I want to obtain the update ops so they can be passed to a [`tf.keras.backend.function`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/backend/function) which can then be executed on demand to compute and apply the gradients. For more context please see [here](https://stackoverflow.com/questions/54856829/tensorflow-2-0-tf-keras-api-eager-mode-vs-graph-mode).

**Code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Dense

my_model = Sequential([Dense(1, activation='relu', input_shape=(5,))])
my_loss = -tf.reduce_mean(my_model.output)
my_optim = Adam()
updates = my_optim.get_updates(params=my_model.trainable_weights, loss=my_loss)
```
Edit: this has also been noticed here: https://github.com/tensorflow/tensorflow/issues/25472"
26097,"sorry, unimplemented: non-trivial designated initializers not supported","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.5 LTS - docker images tensorflow/tensorflow:latest and tensorflow/devel
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit a2bb5db1bf7931b0dc2cd08e53b8798489568198
- Python version: Python 2.7.12
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
Some of the tests don't compile because of the incorrect order of parameters and cause the following error: 
`sorry, unimplemented: non-trivial designated initializers not supported`

Problem is caused by the order in designated initializer which is different from the order in struct.

How should be tensorflow tested so that this test passes? It even doesn't pass on official Google tensorflow images.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test --config=opt --test_size_filters=small,medium -- tensorflow/lite/toco/tflite:operator_test

**Any other info / logs**
"
26096,Kenlm in tf.nn.ctc_beam_search_decoder,Please integrate Kenlm feature in tf.nn.ctc decoder. Because it is very hard to build tensorflow bindings from source. There are always issue with bazel version not matching with tf version.If every thing goes fine still there are issues such as missing libraries.So there should be some easy way to add a language model scoring with ctc_decoder.
26095,Warnings raised for deprecated collections.abc usage.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): b'v1.13.0-rc1-0-g63c13ff' 1.13.0-rc1
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: irrelevant

**Describe the current behavior**
TensorFlows raises warnings like:
```
jumping/simulation/test/test_simulation.py::test_gpu_inference[Device.tensorflow]
  /home/neil/.pyenv/versions/3.7.0/lib/python3.7/site-packages/tensorflow/python/util/nest.py:823: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    _pywrap_tensorflow.RegisterType(""Mapping"", _collections.Mapping)

jumping/simulation/test/test_simulation.py::test_gpu_inference[Device.tensorflow]
  /home/neil/.pyenv/versions/3.7.0/lib/python3.7/site-packages/tensorflow/python/util/nest.py:824: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
    _pywrap_tensorflow.RegisterType(""Sequence"", _collections.Sequence)

jumping/simulation/test/test_simulation.py::test_gpu_inference[Device.tensorflow]
  /home/neil/.pyenv/versions/3.7.0/lib/python3.7/site-packages/tensorflow/python/training/checkpointable/util.py:448: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working

```
**Describe the expected behavior**
These warnings should not be raised.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
```
import warnings
warnings.simplefilter(""error"")
import tensorflow
```

**Other info / logs**
I can do this if the patch will be accepted."
26094,Cross-compilation from Windows 10 to Ubuntu 16.04,"**System information**
- TensorFlow version: TF r1.12.0 if possible
- Doc Link: N/A


**Describe the documentation issue**
I can't find any official tutorial on how to cross-compile Tensorflow on the main platforms other than the less-than-descriptive official tutorial to cross-compile for the Pi. I'm not sure if this is more of a Doc issue than a Feature request but I was looking for any help compiling for Ubuntu 16.04 from a Windows 10 machine. Since there are quite a bunch of error-prone steps to cross-compiling, as there are many options to configure and files to write in a specific way, I would really benefit from some guidance, even if minimal. There are a few tutorials on Medium, but those dont't target the same platforms and/or are old.

Is this something that could be added to Tensorflow's website ?

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
No, unless I manage to do it myself."
26093,"sorry, unimplemented: non-trivial designated initializers not supported","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit a2bb5db1bf7931b0dc2cd08e53b8798489568198
- Python version: 2.7.15rc1
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
During compilation I get an error: sorry, unimplemented: non-trivial designated initializers not supported.
It can be solved by setting all function pointers to null and applying initialization in order as in the struct defintion.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test --verbose_failures tensorflow/lite/delegates/nnapi:nnapi_delegate

**Any other info / logs**
I attach diff which fixes this error for me.
[diff.txt](https://github.com/tensorflow/tensorflow/files/2901455/diff.txt)


"
26092,tensorflow lite tests on x86,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: commit a2bb5db1bf7931b0dc2cd08e53b8798489568198
- Python version: 2.7.15rc1
- Installed using virtualenv? pip? conda?: sources
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -



**Describe the problem**
Tensorflow lite tests seem to require android sdk even though we wanted to test them on x86.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel test --local_resources 40000,23,2 -- //tensorflow/lite/...
**Any other info / logs**
ERROR: /home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/external/bazel_tools/tools/android/BUILD:391:1: Executing genrule @bazel_tools//tools/android:no_android_sdk_repository_error faile
d (Exit 1)                                                                                      
This build requires an Android SDK. Please add the android_sdk_repository rule to your WORKSPACE.
"
26091,Tensorflow 2.0 Preview - tf.function error when wrapping a function that works ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf-nightly-2.0-preview==2.0.0.dev20190225
- Python version: 3.6.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
There seems to be some issue with `tf.function`. The decorator transformation of the function for some reason leads to a wrong behaviour in one of the reshape operations, but it is unclear why that happens. 
[Here is a colab demonstrating the issue.](https://drive.google.com/open?id=1Iwrke8yainVV8NHPICOkZSzzAM7BTGCu)

**Describe the expected behavior**
The function should work in the same way as without the decorator, but it doesn't.

**Code to reproduce the issue**
The colab linked cotains the same code as below:
```
def expand_bias_to(bias, shape):
    return tf.reshape(bias, [1] * (len(shape) - len(bias.shape)) + bias.shape.as_list())


def initialize(x_shape):
    w1 = tf.random_normal_initializer()([x_shape[1], 1000])
    b1 = tf.zeros_initializer()([1000])
    w2 = tf.random_normal_initializer()([1000, x_shape[1]])
    b2 = tf.random_normal_initializer()([x_shape[1]])
    return (w1, b1), (), (w2, b2)


def autoencoder(params, x):
    (w1, b1), (), (w2, b2) = params
    b1 = expand_bias_to(b1, x.shape)
    h1 = tf.matmul(x, w1) + b1
    h2 = tf.tanh(h1)
    b2 = expand_bias_to(b2, h2.shape)
    h3 = tf.matmul(h2, w2) + b2
    return h3


def prepare_mnist_features_and_labels(x, y):
    x = tf.cast(x, tf.float32) / 255.0
    y = tf.cast(y, tf.int64)
    return tf.reshape(x, [-1]), y


def mnist_dataset():
    (x, y), _ = tf.keras.datasets.mnist.load_data()
    ds = tf.data.Dataset.from_tensor_slices((x, y))
    ds = ds.map(prepare_mnist_features_and_labels)
    ds = ds.take(20000).shuffle(20000).batch(100)
    return ds


def train_one_step(model, variables, batch, step_size):
    with tf.GradientTape() as tape:
        tape.watch(variables)
        f_eval = model(variables, batch)
        loss = tf.reduce_mean((f_eval - batch) ** 2)
    grads = tape.gradient(loss, variables)
    return tuple(
        tuple(vi - step_size * gi for vi, gi in zip(ps, gs))
        for ps, gs in zip(variables, grads)
    )


def main(batch_size=100,
         num_epochs=100000,
         step_size=1e-4):
    params = initialize([batch_size, 784])
    train_dataset = mnist_dataset()
    partial_func = partial(train_one_step, model=autoencoder)
    train_step = tf.function(partial_func)
#     train_step = partial_func
    
    epoch = 0
    for images, _ in train_dataset:
        start_time = time.time()
        for _ in range(100):
            train_step(variables=params, batch=images, step_size=step_size)
        epoch_time = time.time() - start_time
        epoch += 1

        print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))

main()
```
The error is:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-3-533661780a33> in <module>()
     66         print(""Epoch {} in {:0.2f} sec"".format(epoch, epoch_time))
     67 
---> 68 main()

<ipython-input-3-533661780a33> in main(batch_size, num_epochs, step_size)
     60         start_time = time.time()
     61         for _ in range(100):
---> 62             train_step(variables=params, batch=images, step_size=step_size)
     63         epoch_time = time.time() - start_time
     64         epoch += 1

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    440       canon_args, canon_kwds = self._canonicalize_function_inputs(args, kwds)
    441       # If we did not create any variables the trace we have is good enough.
--> 442       return self._concrete_stateful_fn._filtered_call(canon_args, canon_kwds)  # pylint: disable=protected-access
    443 
    444     def fn_with_cond(*inner_args, **inner_kwds):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
    537     """"""
    538     return self._call_flat(
--> 539         (t for t in nest.flatten((args, kwargs))
    540          if isinstance(t, (ops.Tensor,
    541                            resource_variable_ops.ResourceVariable))))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    590     # Only need to override the gradient in graph mode and when we have outputs.
    591     if context.executing_eagerly() or not self.outputs:
--> 592       outputs = self._inference_function.call(ctx, args)
    593     else:
    594       self._register_gradient()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=(""executor_type"", executor_type,
    381                    ""config_proto"", config),
--> 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Input to reshape is a tensor with 78400 values, but the requested shape has 784
	 [[{{node Reshape_1}}]] [Op:__inference_function_19842]
```

Note that if we comment out the line with `train_step = partial_func` everything works correctly.
"
26087,Changes within the base SSD mobilenet v1 structure,"I have to import the ssd_mobilenetv1_coco_2018 pretrained model inside PyTorch so I have to manage the specific chosen architecture in the code. I noticed that the first layers of the classification/box prediction have less output than the others (12 instead of 24). 
Isn't it a standard implementation?
Does this involve directly the number of anchors that will be built?

Thanks
"
26085,Ambiguous behaviour of tf.train.global_step,"**System information**
- OS Platform and Distribution: *Darwin - Mac OS X 10.14*
- TensorFlow installed from (source or binary): *Source*
- TensorFlow version (use command below): *v1.12.0-rc2-3-ga6d8ffae09 1.12.0*
- Python version: *3.6.5*

**Describe the current behavior**

Let me illustrate the ambiguous behaviour of global step when using iterators and custom gradient operations. For the example, I will consider a toy problem, Input is a list of 10 digits filled with 1. Output is a list of 10 digits filled with 1-10. A variable is added to the input, so during the course of training, Variable mimics the operation and finally predicts a list of 10 digits filled with 1-10.

**Assumption: (Pls correct if wrong)**
1. Global step is created using `tf.train.get_or_create_global_step()` and is fed to global_step variable in optimizer
2. Global step increments for every run of optimizer.
3. Global step is fetched using `tf.train.get_global_step()` for display purpose.

#### 1. Basic Optimization using Initializable iterators as Data Feed. (Expected)
```python
import tensorflow as tf

def gen():
    for i in range(10):
        yield [1,1,1,1,1,1,1,1,1,1]

itr = tf.data.Dataset.from_generator(generator=gen, output_types=tf.float32)
ini = itr.make_initializable_iterator()

pred = ini.get_next() +  tf.Variable(initial_value=tf.random_normal(shape=(1,10)), dtype=tf.float32)
label = tf.constant([1,2,3,4,5,6,7,8,9,10], shape=(1,10), dtype=tf.float32)
loss = tf.reduce_mean(tf.square(pred - label))
training_op = tf.train.AdamOptimizer(learning_rate=0.1).minimize(loss=loss, global_step=tf.train.get_or_create_global_step())

with tf.Session() as sess:
    sess.run(tf.group([tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()]))
    for i in range(10):
        sess.run(ini.initializer)
        print('Global Step: ', sess.run(tf.train.get_global_step()))
        try:
            while True:
                _,l, g = sess.run([training_op, loss, tf.train.get_or_create_global_step()])
        except tf.errors.OutOfRangeError:
            pass
""""""
Output - Expected:
Global Step:  0
Global Step:  10
Global Step:  20
Global Step:  30
Global Step:  40
Global Step:  50
Global Step:  60
Global Step:  70
Global Step:  80
Global Step:  90
""""""
``` 


#### 2. Replace `Optimizer.minimize` with `Optimizer.compute_gradients` and `Optimizer.apply_gradients` with basic gradient manipulation. (Expected)
```python
import tensorflow as tf

def gen():
    for i in range(10):
        yield [1,1,1,1,1,1,1,1,1,1]

itr = tf.data.Dataset.from_generator(generator=gen, output_types=tf.float32)
ini = itr.make_initializable_iterator()

pred = ini.get_next() +  tf.Variable(initial_value=tf.random_normal(shape=(1,10)), dtype=tf.float32)
label = tf.constant([1,2,3,4,5,6,7,8,9,10], shape=(1,10), dtype=tf.float32)
loss = tf.reduce_mean(tf.square(pred - label))
opt = tf.train.AdamOptimizer(learning_rate=0.1)
compute_op = opt.compute_gradients(loss=loss)
# Manipulate existing gradients
xyz = [(tf.nn.sigmoid(i[0]), i[1]) for i in compute_op]
training_op = opt.apply_gradients(grads_and_vars=xyz, global_step=tf.train.get_or_create_global_step())

with tf.Session() as sess:
    sess.run(tf.group([tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()]))
    for i in range(10):
        sess.run(ini.initializer)
        print('Global Step: ', sess.run(tf.train.get_global_step()))
        try:
            while True:
                _,l, g = sess.run([training_op, loss, tf.train.get_or_create_global_step()])
        except tf.errors.OutOfRangeError:
            pass
""""""
Output - Expected:
Global Step:  0
Global Step:  10
Global Step:  20
Global Step:  30
Global Step:  40
Global Step:  50
Global Step:  60
Global Step:  70
Global Step:  80
Global Step:  90
""""""
```

#### 3. Similar to Step 2, but instead of operating on the gradient tensor, Gradients are replaced with `tf.zeros()` (Ambiguous)
```python
import tensorflow as tf

def gen():
    for i in range(10):
        yield [1,1,1,1,1,1,1,1,1,1]

itr = tf.data.Dataset.from_generator(generator=gen, output_types=tf.float32)
ini = itr.make_initializable_iterator()

pred = ini.get_next() +  tf.Variable(initial_value=tf.random_normal(shape=(1,10)), dtype=tf.float32)
label = tf.constant([1,2,3,4,5,6,7,8,9,10], shape=(1,10), dtype=tf.float32)
loss = tf.reduce_mean(tf.square(pred - label))
opt = tf.train.AdamOptimizer(learning_rate=0.1)
compute_op = opt.compute_gradients(loss=loss)
# Replace Gradients
xyz = [(tf.zeros(shape=i[0].shape.as_list()), i[1]) for i in compute_op]
training_op = opt.apply_gradients(grads_and_vars=xyz, global_step=tf.train.get_or_create_global_step())

with tf.Session() as sess:
    sess.run(tf.group([tf.global_variables_initializer(), tf.local_variables_initializer(), tf.tables_initializer()]))
    for i in range(10):
        sess.run(ini.initializer)
        print('Global Step: ', sess.run(tf.train.get_global_step()))
        try:
            while True:
                _,l, g = sess.run([training_op, loss, tf.train.get_or_create_global_step()])
        except tf.errors.OutOfRangeError:
            pass
""""""
Output - Ambiguous:
Global Step:  0
Global Step:  11
Global Step:  22
Global Step:  33
Global Step:  44
Global Step:  55
Global Step:  66
Global Step:  77
Global Step:  88
Global Step:  99
""""""
```

**Describe the expected behavior**
#### Expected behaviour for step 3
```python
""""""
Output - Expected:
Global Step:  0
Global Step:  10
Global Step:  20
Global Step:  30
Global Step:  40
Global Step:  50
Global Step:  60
Global Step:  70
Global Step:  80
Global Step:  90
""""""
```

1. Why / How does global_step gets an extra increment in Step 3?
2. Is it a bad idea to replace gradient tensors with custom tensors (Probably custom tensors do not obey tf.GraphKeys.UpdateOps)?
3. What is the right method to replace gradients and proposed validation?"
26084,libtensorflow.dll leaks icu symbols on Windows,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): Self-compiled with bazel & from https://storage.googleapis.com/tensorflow/libtensorflow/libtensorflow-cpu-windows-x86_64-1.12.0.zip
- TensorFlow version: 1.12.0 & 1.13.0-rc2

**Describe the problem**
`libtensorflow.dll` exports a ton of symbols from icu. They should be private to the dll.

**Any other info / logs**
```
C:\Users\mfischer\Desktop>dumpbin /exports tensorflow-1.12.0.dll
Microsoft (R) COFF/PE Dumper Version 14.16.27027.1
Copyright (C) Microsoft Corporation.  All rights reserved.


Dump of file tensorflow-1.12.0.dll

File Type: DLL

  Section contains the following exports for libtensorflow.so

    00000000 characteristics
    5BDBAEF0 time date stamp Fri Nov  2 02:57:04 2018
        0.00 version
           1 ordinal base
        3078 number of functions
        3078 number of names

    ordinal hint RVA      name

          1    0 01841AA0 ??0?$MaybeStackArray@D$0CI@@icu_62@@AEAA@AEBV01@@Z
          2    1 023BC120 ??0?$MaybeStackArray@D$0CI@@icu_62@@QEAA@$$QEAV01@@Z
          3    2 023BC180 ??0?$MaybeStackArray@D$0CI@@icu_62@@QEAA@H@Z
          4    3 023BC200 ??0?$MaybeStackArray@D$0CI@@icu_62@@QEAA@XZ
          5    4 023C5F90 ??0Appendable@icu_62@@QEAA@AEBV01@@Z
          6    5 023C5F90 ??0Appendable@icu_62@@QEAA@XZ
          7    6 0245DC70 ??0BreakIterator@icu_62@@IEAA@AEBV01@@Z
          8    7 0245DCD0 ??0BreakIterator@icu_62@@IEAA@AEBVLocale@1@0@Z
          9    8 0245DD20 ??0BreakIterator@icu_62@@IEAA@XZ
         10    9 02399020 ??0ByteSink@icu_62@@QEAA@XZ
         11    A 02454B40 ??0BytesDictionaryMatcher@icu_62@@QEAA@AEBV01@@Z
         12    B 02454B70 ??0BytesDictionaryMatcher@icu_62@@QEAA@PEBDHPEAUUDataMemory@@@Z
         13    C 023CC750 ??0BytesTrie@icu_62@@AEAA@PEAXPEBX@Z
         14    D 023CC770 ??0BytesTrie@icu_62@@QEAA@AEBV01@@Z
         15    E 023CC7A0 ??0BytesTrie@icu_62@@QEAA@PEBX@Z
         16    F 0245BA10 ??0BytesTrieBuilder@icu_62@@QEAA@AEAW4UErrorCode@@@Z
         17   10 02458DB0 ??0CStr@icu_62@@QEAA@AEBVUnicodeString@1@@Z
         18   11 023D3F10 ??0CacheKeyBase@icu_62@@QEAA@AEBV01@@Z
         19   12 023D3F30 ??0CacheKeyBase@icu_62@@QEAA@XZ
         20   13 02459A80 ??0CanonicalIterator@icu_62@@QEAA@AEBVUnicodeString@1@AEAW4UErrorCode@@@Z
         21   14 02399030 ??0Char16Ptr@icu_62@@QEAA@$$T@Z
         22   15 02399030 ??0Char16Ptr@icu_62@@QEAA@PEAG@Z
         23   16 02399030 ??0Char16Ptr@icu_62@@QEAA@PEA_S@Z
         24   17 02399030 ??0Char16Ptr@icu_62@@QEAA@PEA_W@Z
         25   18 02459090 ??0CharString@icu_62@@QEAA@$$QEAV01@@Z
         26   19 023BC220 ??0CharString@icu_62@@QEAA@AEBV01@AEAW4UErrorCode@@@Z
         27   1A 023BC270 ??0CharString@icu_62@@QEAA@PEBDHAEAW4UErrorCode@@@Z
         28   1B 023BC2C0 ??0CharString@icu_62@@QEAA@VStringPiece@1@AEAW4UErrorCode@@@Z
         29   1C 023BC320 ??0CharString@icu_62@@QEAA@XZ
         30   1D 02459730 ??0CharacterIterator@icu_62@@IEAA@AEBV01@@Z
         31   1E 02459760 ??0CharacterIterator@icu_62@@IEAA@H@Z
         32   1F 02459790 ??0CharacterIterator@icu_62@@IEAA@HH@Z
...
```
"
26083,tensorflow lite : Convertion from keras works fine but execution on device fails : ByteBuffer is not a valid flatbuffer model,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory / Android 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Oneplus 6, Samsung Galaxy s8, ...
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'v1.13.0-rc1-2-gb6141b06f5' 1.13.0-rc1 (but also present in 1.12.0)
- Firebase ML-Interpreter Version : 16.2.4
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: -
- GPU model and memory: - 

**Describe the current behavior**
We want to convert a keras model to a tflite model. Some architectures won't convert (ex: Reshape Layer followed by a BatchNorm layer), but some seem to be able to be converted, but when used on device the error ""Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model"" happens. Here is a simple model description demonstrating this issue :

**Describe the expected behavior**
If no errors are encountered when converting to tflite model, we should be able to run them with firebase model interpreter

**Code to reproduce the issue**

On google Colaboratory create the model : 

```

from tensorflow.keras import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import mse, categorical_crossentropy

state = Input((884,))
mask = Input((260,))

bn = BatchNormalization()(state)
rs1 = Reshape((52, 17))(bn)
lstm1= LSTM((256, unroll=True))(rs1)

policy_raw = Dense(260, activation=sigmoid)(lstm1)
policy_masked = Multiply()([mask, policy_raw])
output_policy = Activation(activation=softmax)(policy_masked)

output_value = Dense(1)(state)

model = Model([state, mask], [output_policy, output_value])
model.compile(optimizer=Adam(), loss=[categorical_crossentropy, mse])

model.save(""keras_model.keras"")

```

Convert the model : 


```
from tensorflow.lite.python.lite import TFLiteConverter

path = ""keras_model.keras""

converter = TFLiteConverter.from_keras_model_file(path)

tflite_model = converter.convert()

open(path + "".tflite"", ""wb"").write(tflite_model)

```

Then download the tflite model : 

```
from google.colab import files

files.download(path +'.tflite')

```

Then try to use the model on Android with tflite through Firebase ML Interpreter : 


```
FirebaseModelInputOutputOptions dataOptions = new FirebaseModelInputOutputOptions.Builder()
                .setInputFormat(0, FLOAT32, new int[]{1, 884})
                .setInputFormat(1, FLOAT32, new int[]{1, 260})
                .setOutputFormat(0, FLOAT32, new int[]{1, 260})
                .setOutputFormat(1, FLOAT32, new int[]{1, 1})
                .build();

final FirebaseLocalModelSource modelSource = new FirebaseLocalModelSource.Builder(""asset"")
                .setAssetFilePath(""keras_model.tflite"").build();

FirebaseModelManager.getInstance().registerLocalModelSource(modelSource);

        final FirebaseModelOptions options = new FirebaseModelOptions.Builder()
                .setLocalModelName(""asset"")
                .build();

FirebaseModelInterpreter interpreter = FirebaseModelInterpreter.getInstance(options);

final FirebaseModelInputs inputs = new FirebaseModelInputs.Builder()
                .add(new float[][]{new float[884]})
                .add(new float[][]{new float[260]})
                .build();

interpreter.run(inputs, dataOptions)
                .continueWith(new Continuation<FirebaseModelOutputs, Object>() {
                    @Override
                    public Object then(@NonNull final Task<FirebaseModelOutputs> task) {
                        try {
                            task.getResult();
                        } catch (final Throwable e) {
                            Log.e(""TFLITE"", e.getMessage(), e);
                        } finally {
                        }

                        return null;
                    }
                });
```

You can find a converted model here : 
[keras_model.zip](https://github.com/tensorflow/tensorflow/files/2899987/keras_model.zip)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Error log : 

```
com.google.firebase.ml.common.FirebaseMLException: The load task failed
    com.google.android.gms.tasks.RuntimeExecutionException: com.google.firebase.ml.common.FirebaseMLException: The load task failed
        at com.google.android.gms.tasks.zzu.getResult(Unknown Source:15)
        at com.iscoolentertainment.unity.tflite.UnityTFLiteRunner$1.then(UnityTFLiteRunner.java:92)
        at com.google.android.gms.tasks.zzd.run(Unknown Source:5)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:193)
        at android.app.ActivityThread.main(ActivityThread.java:6863)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:537)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:858)
     Caused by: com.google.firebase.ml.common.FirebaseMLException: The load task failed
        at com.google.android.gms.internal.firebase_ml.zzio.zzf(Unknown Source:65)
        at com.google.android.gms.internal.firebase_ml.zzij.call(Unknown Source:2)
        at com.google.android.gms.internal.firebase_ml.zzie.zza(Unknown Source:29)
        at com.google.android.gms.internal.firebase_ml.zzif.run(Unknown Source:2)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at com.google.android.gms.internal.firebase_ml.zze.dispatchMessage(Unknown Source:5)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
     Caused by: com.google.firebase.ml.common.FirebaseMLException: Local model load failed: 
        at com.google.android.gms.internal.firebase_ml.zzje.zza(Unknown Source:127)
        at com.google.android.gms.internal.firebase_ml.zzje.zzgd(Unknown Source:102)
        at com.google.android.gms.internal.firebase_ml.zziq.zzgg(Unknown Source:7)
        at com.google.android.gms.internal.firebase_ml.zziq.call(Unknown Source:23)
        at com.google.android.gms.internal.firebase_ml.zzie.zza(Unknown Source:29)
        at com.google.android.gms.internal.firebase_ml.zzif.run(Unknown Source:2)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at com.google.android.gms.internal.firebase_ml.zze.dispatchMessage(Unknown Source:5)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:74)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:54)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:114)
        at com.google.android.gms.internal.firebase_ml.zzje.zzb(Unknown Source:222)
        at com.google.android.gms.internal.firebase_ml.zzjf.zzc(Unknown Source:0)
        at com.google.android.gms.internal.firebase_ml.zzje.zzb(Unknown Source:148)
        at com.google.android.gms.internal.firebase_ml.zzje.zza(Unknown Source:116)
        at com.google.android.gms.internal.firebase_ml.zzje.zzgd(Unknown Source:102)
        at com.google.android.gms.internal.firebase_ml.zziq.zzgg(Unknown Source:7)
        at com.google.android.gms.internal.firebase_ml.zziq.call(Unknown Source:23)
        at com.google.android.gms.internal.firebase_ml.zzie.zza(Unknown Source:29)
        at com.google.android.gms.internal.firebase_ml.zzif.run(Unknown Source:2)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at com.google.android.gms.internal.firebase_ml.zze.dispatchMessage(Unknown Source:5)
        at android.os.Looper.loop(Looper.java:193)
        at android.os.HandlerThread.run(HandlerThread.java:65)
```

For information, if we use the model on Google collaboratory, it works fine : 

```

import numpy as np
import tensorflow as tf

# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""keras_model.tflite"")
interpreter.allocate_tensors()

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Test model on random input data.
input_shape = input_details[0]['shape']
input_shape_2 = input_details[1]['shape']
input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)
interpreter.set_tensor(input_details[0]['index'], input_data)
input_data = np.array(np.random.random_sample(input_shape_2), dtype=np.float32)
interpreter.set_tensor(input_details[1]['index'], input_data)

interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
print(output_data)
output_data = interpreter.get_tensor(output_details[1]['index'])
print(output_data)

```

"
26082,TensorForestEstimator throws ValueError due to bad feature column handling,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Bug was discovered when writing a custom script that used `TensorForestEstimator`.  Code is provided below.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
```
Linux sandbox 4.15.0-45-generic #48-Ubuntu SMP Tue Jan 29 16:28:13 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""18.04.2 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic
```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

not applicable

- TensorFlow installed from (source or binary):

TensorFlow binary installed from PyPi via `pip`.

- TensorFlow version (use command below):

```
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
Sanity check: array([1], dtype=int32)
```

- Python version:

Python 3.6.8

- Bazel version (if compiling from source):

not applicable

- GCC/Compiler version (if compiling from source):

Since Python was compiled from source via `pyenv install`, including compiler version:

```
c++ (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
```

- CUDA/cuDNN version:

not applicable

- GPU model and memory:

not applicable

- Other information

```
== are we in docker =============================================
No
```

Python environment is managed with `pyenv` version. 1.2.9, with the following packages installed:

```
absl-py==0.7.0
asn1crypto==0.24.0
astor==0.7.1
awscli==1.16.109
azure-common==1.1.18
azure-storage-blob==1.5.0
azure-storage-common==1.4.0
backcall==0.1.0
bleach==3.1.0
boto3==1.9.99
botocore==1.12.99
certifi==2018.11.29
cffi==1.12.1
chardet==3.0.4
colorama==0.3.9
cryptography==2.5
cycler==0.10.0
decorator==4.3.2
defusedxml==0.5.0
docutils==0.14
entrypoints==0.3
future==0.17.1
gast==0.2.2
grpcio==1.18.0
h5py==2.9.0
idna==2.8
ijson==2.3
ipykernel==5.1.0
ipython==7.3.0
ipython-genutils==0.2.0
jedi==0.13.2
Jinja2==2.10
jmespath==0.9.3
jsonschema==2.6.0
jupyter-client==5.2.4
jupyter-core==4.4.0
jupyterlab==0.35.4
jupyterlab-server==0.2.0
Keras-Applications==1.0.7
Keras-Preprocessing==1.0.9
kiwisolver==1.0.1
Markdown==3.0.1
MarkupSafe==1.1.0
matplotlib==3.0.2
mistune==0.8.4
mpmath==1.1.0
nbconvert==5.4.1
nbformat==4.4.0
notebook==5.7.4
numpy==1.16.1
pandas==0.24.1
pandocfilters==1.4.2
parso==0.3.4
pexpect==4.6.0
pickleshare==0.7.5
prometheus-client==0.6.0
prompt-toolkit==2.0.9
protobuf==3.6.1
ptyprocess==0.6.0
pyasn1==0.4.5
pycparser==2.19
pycryptodomex==3.7.3
Pygments==2.3.1
PyJWT==1.7.1
pyOpenSSL==19.0.0
pyparsing==2.3.1
python-dateutil==2.8.0
pytz==2018.9
PyYAML==3.13
pyzmq==18.0.0
requests==2.21.0
rsa==3.4.2
s3transfer==0.2.0
scikit-learn==0.20.2
scipy==1.2.1
Send2Trash==1.5.0
six==1.12.0
snowflake-connector-python==1.7.6
sympy==1.3
tensorboard==1.12.2
tensorflow==1.12.0
termcolor==1.1.0
terminado==0.8.1
testpath==0.4.2
tornado==5.1.1
traitlets==4.3.2
urllib3==1.24.1
wcwidth==0.1.7
webencodings==0.5.1
Werkzeug==0.14.1
```

**Describe the current behavior**

When `feature_columns` argument is specified, `TensorForestEstimator` will throw the following error during a `fit()`:

```
TypeError: '<' not supported between instances of '_RealValuedColumn' and 'str'
```

**Describe the expected behavior**

Expected behavior was that model would be built from training operation.


**Code to reproduce the issue**

Note that the CSV file 'Immunotherapy - ImmunoDataset.csv' can be obtained from the UC Irvine Machine Learning Repository at this URL:

https://archive.ics.uci.edu/ml/datasets/Immunotherapy+Dataset
# Sample Code for `TensorForestEstimator` with `pandas_input_fn`

## Python Standard Library Imports


```python
import csv
import random
```

## Non TensorFlow Imports


```python
import numpy as np
import pandas as pd
```

## TensorFlow Library Imports


```python
import tensorflow as tf
import tensorflow.contrib.layers as layers
import tensorflow.contrib.tensor_forest as tforest
```

## Aliased TensorFlow Library Imports


```python
from tensorflow.estimator.inputs import pandas_input_fn
from tensorflow.python.platform import tf_logging as logging
```


```python
tf.logging.set_verbosity(tf.logging.DEBUG)
```

## Metadata for CSV Columns


```python
COLUMN_PROPS = {
    'sex' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype' : tf.int32,
        'default' : -1,
        'feature_column' : layers.real_valued_column(
            'sex',
            dtype=tf.int32
        )
    },
    'age' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype' : tf.int32,
        'default' : -1,
        'feature_column' : layers.real_valued_column(
            'age',
            dtype=tf.int32
        )  
    },
    'Time' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype' : tf.float32,
        'default' : -1.0,
        'feature_column' : layers.real_valued_column(
            'Time',
            dtype=tf.float32
        )
    },
    'Number_of_Warts' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype' : tf.int32,
        'default' : -1,
        'feature_column' : layers.real_valued_column(
            'Number_of_Warts',
            dtype=tf.int32
        ),
    },
    'Type' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype' : tf.int32,
        'default' : -1,
        'feature_column' : layers.real_valued_column(
            'Type',
            dtype=tf.int32
        )
    },
    'Area' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype' : tf.int32,
        'default' : -1,
        'feature_column' : layers.real_valued_column(
            'Area',
            dtype=tf.int32
        )
    },
    'induration_diameter' : {
        'is_feature' : True,
        'is_label' : False,
        'dtype': tf.int32,
        'default': -1,
        'feature_column' : layers.real_valued_column(
            'induration_diameter',
            dtype=tf.int32
        )
    },
    'Result_of_Treatment': {
        'is_feature' : False,
        'is_label' : True,
        'dtype': tf.int32,
        'default': -1,
        'feature_column' : None
    }
}
```

## Ordering of CSV Columns


```python
CSV_COLUMNS = [
    'sex',
    'age',
    'Time',
    'Number_of_Warts',
    'Type',
    'Area',
    'induration_diameter',
    'Result_of_Treatment'
]
```

## Generate Lists of Features and Labels from Metadata


```python
FEATURE_COLUMNS = []
LABEL_COLUMN = None

for k in CSV_COLUMNS:
    if COLUMN_PROPS[k]['is_feature']:
        FEATURE_COLUMNS.append(k)
    elif COLUMN_PROPS[k]['is_label']:
        LABEL_COLUMN = k
```

## Helper Function for Shuffling and Exporting Subsets

This function is used to export training, evaluation, and test data sets as CSVs, shuffling the rows.


```python
def generate_sets(datasets):
    for k, v in datasets.items():
        random.shuffle(v)
        with open(k + '.csv', 'w') as fobj:
            wrtr = csv.writer(fobj)
            wrtr.writerow(header)
            for rec in v:
                wrtr.writerow(rec)
```

## Split Datasets for Traning, Evaluation, and Testing


```python
trn = []
evl = []
tst = []

with open('Immunotherapy - ImmunoDataset.csv', 'r') as fobj:
    rdr = csv.reader(fobj)
    header = next(rdr)
    label_key = header[-1]
    feature_keys = header[:-1]

    for rec in rdr:
        # Output of random number generator determines
        # which set the record will be placed.
        rn =  random.random()
        if rn < 0.6:
            trn.append(rec)
        elif rn < 0.8:
            evl.append(rec)
        else:
            tst.append(rec)

datasets = {
    'train' : trn,
    'eval' : evl,
    'test' : tst
}

generate_sets(datasets)
```

## Set up `TensorForest` Hyperparameters


```python
fhp = tforest.tensor_forest.ForestHParams(
    num_classes=2,
    num_features=7,
    regression=False
)
```

## Pluck Feature Columns from Metadata Dictionary


```python
fcs = [COLUMN_PROPS[k]['feature_column'] for k in FEATURE_COLUMNS]
```

## Instatntiate `TensorForestEstimator` Object


```python
tfe = tforest.random_forest.TensorForestEstimator(
    fhp,
    feature_columns=fcs,
    report_feature_importances=True
)
```

## Define a Wrapper for Training and Evaluation Using `pandas_input_fn`


```python
def get_input_fn(csv_file, num_epochs=1):
    
    df = pd.read_csv(csv_file)

    # Workaround for this issue:
    #
    # https://stackoverflow.com/questions/48577372/tensorflowusing-pandas-input-fn-with-tensorforestestimator
    # https://github.com/tensorflow/tensorflow/issues/16692        

    def internal_input_fn():

        features = df.loc[:,'sex':'induration_diameter']
        labels =  df.loc[:,'Result_of_Treatment']
        
        features, labels = pandas_input_fn(
            x=features,
            y=labels,
            shuffle=True,
            num_epochs=num_epochs
        )()

        features = {
            k : tf.expand_dims(features[k], axis=1) 
            for k in features
        }
        
        labels = tf.expand_dims(labels, axis=1)
      
        return features, labels
    
    return internal_input_fn
```

## Train on Data


```python
tfe.fit(
    input_fn=get_input_fn('train.csv', num_epochs=None)
)
```

## Evaluate the Model


```python
tfe.evaluate(input_fn=get_input_fn('eval.csv'))
```

## Define a Wrapper for Prediction


```python
def get_predict_input_fn(csv_file):
    
    df = pd.read_csv(csv_file)
    
    print('actual values: {0}'.format(df.loc[:, 'Result_of_Treatment'].values))

    # Workaround for this issue:
    #
    # https://stackoverflow.com/questions/48577372/tensorflowusing-pandas-input-fn-with-tensorforestestimator
    # https://github.com/tensorflow/tensorflow/issues/16692        

    def internal_input_fn():

        features = df.loc[:,'sex':'induration_diameter']
              
        features = pandas_input_fn(
            x=features,
            shuffle=False
        )()

        features = {
            k : tf.expand_dims(features[k], axis=1) 
            for k in features
        }
      
        return features
    
    return internal_input_fn
```

## Make Predictions on Test Data


```python
# The documentation for tf.contrib.learn.Estimator.predict() lies.
# 
# https://stackoverflow.com/questions/40705710/cannot-get-predictions-of-tensorflow-dnnclassifier

np.array(
    [
        res['classes']
        for res in list(
            tfe.predict(input_fn=get_predict_input_fn('test.csv'))
        )
    ],
    dtype=np.int32
)
```

**Other info / logs**

See my comment on the possible source of the problem.

https://github.com/tensorflow/tensorflow/commit/0b182c3940a3d15d574da327ac705c2efa18a985#r32454203"
26081,Problem with keras implementation of Tensorflow,"Hi,
TF Version: b'v1.13.0-rc1-2-gb6141b06f5' 1.13.0-rc1

I have switched a keras implementation to tensorflow.keras. My intention is to give the TPUs a try. The original model trains on GPU. The tensorfow version cancels and shows a different model output: 

Original output is:

Model summary...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                    
==================================================================================================
input_1 (InputLayer)            (None, 224, 224, 3)  0                                           
__________________________________________________________________________________________________
mobilenet_1.00_224 (Model)      (None, 7, 7, 1024)   3228864     input_1[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d_1 (Glo (None, 1024)         0           mobilenet_1.00_224[1][0]        
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 1024)         0           global_average_pooling2d_1[0][0]
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 1024)         1049600     dropout_1[0][0]                 
__________________________________________________________________________________________________
gender (Dense)                  (None, 2)            2050        dense_1[0][0]                   
__________________________________________________________________________________________________
age (Dense)                     (None, 21)           21525       dense_1[0][0]                   
==================================================================================================
Total params: 4,302,039
Trainable params: 4,280,151
Non-trainable params: 21,888


**Tensorflow Keras Version is:**

Model summary...
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                    
==================================================================================================
input_1 (InputLayer)            (None, 224, 224, 3)  0                                           
__________________________________________________________________________________________________
mobilenet_1.00_224 (Model)      (None, 7, 7, 1024)   3228864     input_1[0][0]                   
__________________________________________________________________________________________________
global_average_pooling2d (Globa (None, 1024)         0           mobilenet_1.00_224[1][0]        
__________________________________________________________________________________________________
dropout (Dropout)               (None, 1024)         0           global_average_pooling2d[0][0]  
__________________________________________________________________________________________________
dense (Dense)                   (None, 1024)         1049600     dropout[0][0]                   
__________________________________________________________________________________________________
gender (Dense)                  (None, 2)            2050        dense[0][0]                     
__________________________________________________________________________________________________
age (Dense)                     (None, 21)           21525       dense[0][0]                     
==================================================================================================
Total params: 4,302,039
Trainable params: 4,280,151
Non-trainable params: 21,888
_____________________________________________________________
 

What I see is the missing _1 in dropout, dense and so on. 

This results in the following error when trying to train:
ValueError: Output of generator should be a tuple `(x, y, sample_weight)` or `(x, y)`. Found: [{'input_1':...

Kind regards,

Dirk
"
26077,1.13.0-rc2 gpu doesnt work,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0-rc2
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 / 7.3.1
- GPU model and memory: NVIDIA GeForce GTX 1070 Ti
[trainingres.txt](https://github.com/tensorflow/tensorflow/files/2899440/trainingres.txt)

Image recognition error:

WARNING:tensorflow:From C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\tensorflow\python\ops\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
Traceback (most recent call last):
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\itemtestprediction.py"", line 16, in <module>
    predictions, probabilities = predictionPet.predictImage(""C:\\Users\\navickg\\AppData\\Local\\Programs\\Python\\Python36\\testimages\\lawnmower.jpg"", result_count=5)
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\imageai\Prediction\Custom\__init__.py"", line 588, in predictImage
    prediction = model.predict(x=image_to_predict, steps=1)
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\tensorflow\python\keras\engine\training.py"", line 1113, in predict
    self, x, batch_size=batch_size, verbose=verbose, steps=steps)
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\tensorflow\python\keras\engine\training_arrays.py"", line 266, in model_iteration
    batch_outs = f(actual_inputs)
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\tensorflow\python\keras\backend.py"", line 3076, in __call__
    run_metadata=self.run_metadata)
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\tensorflow\python\client\session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""C:\Users\navickg\AppData\Local\Programs\Python\Python36\Image_NEW_GPU\tensorflow\python\framework\errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d/Conv2D}}]]
	 [[{{node dense/Softmax}}]]

While 1.12 built from source Tensorflow version works without any issues
"
26076,"Why is op:BatchMatMul listed in file(.pbtxt), when there is no call to such op?","There is no usage of op named ***BatchMatMul*** in my code. If the op is not used then why is it listed in the file (.pbtxt)?

Can anyone please help me with this...!

Thanks in advance."
26075,TFLite model converted from pb file yields different output values,"System information:

Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
    `macOS Mojave version 10.14.3`
Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: 
   `IPhone XS (iOS 12.1.4)`
TensorFlow installed from (source or binary):
    bazel tools: build from source
    python imports: install with pip `tensorflow==1.12.0`
TensorFlow version (use command below):
   `bazel tools: github branch r1.13 commit bade323390591fff6fc82b7eeb4a6cc30f807389 Fri Feb 22 `11:00:40 2019 -0800
    python imports: `('v1.12.0-rc2-3-ga6d8ffae09', '1.12.0')`
Python version:
    `Python 2.7.10`
Bazel version (if compiling from source):
    `Build label: 0.22.0`
GCC/Compiler version (if compiling from source):
```
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 10.0.0 (clang-1000.11.45.5)
Target: x86_64-apple-darwin18.2.0
Thread model: posix
InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin
```    
CUDA/cuDNN version:
    N/A
GPU model and memory:
    N/A
Exact command to reproduce:

I trained a custom model based on MobileNetV2 with a fews convolution layers and fully connected layer on top to output ranking score of input images. I tried to convert the model to tflite for running on iOS, but found that the output values of tflite model are different from origin model even with same inputs. Same behaviour is observed for tflite interpreter for iOS and python and also bazel-tools. Models trained with different input size of MobileNetV2 (224, 160, 96) also produce similar behaviour.

I suppose the output values for tflite and original tensorflow model should output the same value?

the following link is a zip of the models files and output uploaded onto google drive
https://drive.google.com/file/d/1tZVQk7kk5fCEnvnvOjZs1oF0y6YDLfOc/view?usp=sharing

```
# path variables

CKPT_META_PATH=checkpoints/mvfn_096.ckpt-15000.meta
CKPT_WEIGHT_PATH=checkpoints/mvfn_096.ckpt-15000
FROZEN_PB_PATH=mvfn_096.pb
TFLITE_PATH=mvfn_096.tflite
TB_PATH=tb_log
TFLITE_VIS_HTML_PATH=mvfn_096_tflite.html
INPUT_IMAGE_SIZE=96
TF_PATH="""" # path to tensorflow repo
```
```
# freeze graph

python ${TF_PATH}/tensorflow/python/tools/freeze_graph.py \
    --input_binary=true \
    --input_meta_graph=${CKPT_META_PATH} \
    --input_checkpoint=${CKPT_WEIGHT_PATH} \
    --output_graph=${FROZEN_PB_PATH} \
    --output_node_names=""ranker/score_func""
```
```
# convert pb to tflite

bazel run tensorflow/lite/toco:toco -- \
    --input_file=${FROZEN_PB_PATH} \
    --input_format=${TENSORFLOW_GRAPHDEF} \
    --output_file=TFLITE_PATH \
    --output_format=TFLITE \
    --inference_type=FLOAT \
    --inference_input_type=FLOAT \
    --input_arrays=input_image \
    --output_arrays=""ranker/score_func"" \
    --input_shapes=1,${INPUT_IMAGE_SIZE},${INPUT_IMAGE_SIZE},3                # fix input batch size to 1
```

```
# pb to tensorboard for visualization

python ${TF_PATH}/tensorflow/python/tools/import_pb_to_tensorboard.py \
    --model_dir=${FROZEN_PB_PATH} \
    --log_dir=${TB_PATH}
```
```
# visualize tflite

bazel run tensorflow/lite/tools:visualize -- ${TFLITE_PATH} ${TFLITE_VIS_HTML_PATH}
```

```
# diff tflite & pb

bazel run tensorflow/lite/testing:tflite_diff_example_test -- \
    --tensorflow_model=${FROZEN_PB_PATH} \
    --tflite_model=${TFLITE_PATH} \
    --input_layer=""input_image"" \
    --input_layer_type=float \
    --input_layer_shape=1,${INPUT_IMAGE_SIZE},${INPUT_IMAGE_SIZE},3 \
    --output_layer=""ranker/score_func""
```
```
# output of tflite_diff_example_test
# pb model and tflite model have different output values

2019-02-25 05:34:28.723184: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
There were errors in invocation '', output tensor '170':
  index 0: got 1.76843, but expected -4.20755
There were errors in invocation '', output tensor '170':
  index 0: got 0.762267, but expected -3.9918
There were errors in invocation '', output tensor '170':
  index 0: got 0.110205, but expected -4.65119
There were errors in invocation '', output tensor '170':
  index 0: got 1.10238, but expected -4.45592
There were errors in invocation '', output tensor '170':
  index 0: got 1.5811, but expected -4.54539
There were errors in invocation '', output tensor '170':
  index 0: got 1.34377, but expected -4.36198
There were errors in invocation '', output tensor '170':
  index 0: got 1.87406, but expected -4.3289
There were errors in invocation '', output tensor '170':
  index 0: got 2.33834, but expected -4.49968
There were errors in invocation '', output tensor '170':
  index 0: got 1.16959, but expected -4.78387
There were errors in invocation '', output tensor '170':
  index 0: got 1.22668, but expected -4.53048
There were errors in invocation '', output tensor '170':
  index 0: got 0.570301, but expected -4.27984
There were errors in invocation '', output tensor '170':
  index 0: got -0.875985, but expected -4.32995
```

"
26074,tf.spectral.dct length argument not implemented,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
1.12.0
- Are you willing to contribute it (Yes/No):
Yes

**Describe the feature and the current behavior/state.**
tf.spectral.dct has parameter ""n"" which is length of the transform. In https://github.com/tensorflow/tensorflow/blob/a68fec0c77a4c99d4306cd3e987b4f6f548a8112/tensorflow/python/ops/signal/dct_ops.py#L33~L34, It seems it is not implemented when it is not None. Is there any detailed plan to implement this?

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**

**Any Other info.**
"
26073, no such attribute 'downloaded_file_path' in 'http_file' rule,"
**System information**
- OS Platform and Distribution :Linux Ubuntu 16.04
- TensorFlow installed from :source code
- TensorFlow version:https://github.com/tensorflow/tensorflow
- Installed using bazel(bazel build tensorflow/cc:cc_ops):
- Bazel version (0.15.0):
- GCC/Compiler version (5.4.0):
**Describe the problem**
when i use bazel(bazel build tensorflow/cc:cc_ops) to buid the tensorflow source code from ""https://github.com/tensorflow/tensorflow"",there is some error as below:
WARNING: while reading option defaults file '/home/luwei/ML/tensorflow/tensorflow-master/.bazelrc':
  invalid command name 'try-import'.
WARNING: while reading option defaults file '/home/luwei/ML/tensorflow/tensorflow-master/.bazelrc':
  invalid command name 'try-import'.
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:101:9: //external:bazel_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:110:9: //external:docker_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:119:9: //external:gcloud_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:128:9: //external:launchpad_openjdk_gpg: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:138:9: //external:golang_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:147:9: //external:debian8_clang_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:155:9: //external:ubuntu16_04_clang_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:164:9: //external:debian8_libcxx_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:172:9: //external:ubuntu16_04_libcxx_release: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0141_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0150_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0152_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0161_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0171_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0172_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0180_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0181_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0190_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0192_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0200_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:184:13: //external:bazel_0210_installer: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:196:9: //external:azul_open_jdk: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: /home/luwei/.cache/bazel/_bazel_luwei/496db56b93dce7ae9ba21590959da509/external/bazel_toolchains/repositories/repositories.bzl:204:9: //external:azul_open_jdk_src: no such attribute 'downloaded_file_path' in 'http_file' rule
ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': error loading package 'external': Could not load //external package
ERROR: error loading package '': Encountered error while reading extension file 'repositories/repositories.bzl': no such package '@io_bazel_rules_docker//repositories': error loading package 'external': Could not load //external package
INFO: Elapsed time: 0.051s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)


**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel build tensorflow/cc:cc_ops

**Any other info / logs**
  no!"
26072,docker tensorflow-tensorflow/latest-gpu slow initialisation of GPU,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: Ubuntu 16.04
Using Docker (latest-gpu image)
- Mobile Device: No
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: 1.13.0-rc1
- Python version:  Python 3.5.2
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130
- GPU model and memory: Quadro M1200 

```
==============NVSMI LOG==============

Timestamp                           : Mon Feb 25 00:50:20 2019
Driver Version                      : 410.78
CUDA Version                        : 10.0

Attached GPUs                       : 1
GPU 00000000:01:00.0
    Product Name                    : Quadro M1200
    Product Brand                   : Quadro
    Display Mode                    : Disabled
    Display Active                  : Disabled
    Persistence Mode                : Enabled
    Accounting Mode                 : Disabled
    Accounting Mode Buffer Size     : 4000
    Driver Model
        Current                     : N/A
        Pending                     : N/A
    Serial Number                   : N/A
    GPU UUID                        : GPU-d9093d17-7927-a053-9104-426e68b1d4ac
    Minor Number                    : 0
    VBIOS Version                   : 82.07.BB.00.13
    MultiGPU Board                  : No
    Board ID                        : 0x100
    GPU Part Number                 : N/A
    Inforom Version
        Image Version               : N/A
        OEM Object                  : N/A
        ECC Object                  : N/A
        Power Management Object     : N/A
    GPU Operation Mode
        Current                     : N/A
        Pending                     : N/A
    GPU Virtualization Mode
        Virtualization mode         : None
    IBMNPU
        Relaxed Ordering Mode       : N/A
    PCI
        Bus                         : 0x01
        Device                      : 0x00
        Domain                      : 0x0000
        Device Id                   : 0x13B610DE
        Bus Id                      : 00000000:01:00.0
        Sub System Id               : 0x224D17AA
        GPU Link Info
            PCIe Generation
                Max                 : 3
                Current             : 3
            Link Width
                Max                 : 16x
                Current             : 16x
        Bridge Chip
            Type                    : N/A
            Firmware                : N/A
        Replays since reset         : 0
        Tx Throughput               : 0 KB/s
        Rx Throughput               : 0 KB/s
    Fan Speed                       : N/A
    Performance State               : P0
    Clocks Throttle Reasons
        Idle                        : Not Active
        Applications Clocks Setting : Active
        SW Power Cap                : Not Active
        HW Slowdown                 : Not Active
            HW Thermal Slowdown     : N/A
            HW Power Brake Slowdown : N/A
        Sync Boost                  : Not Active
        SW Thermal Slowdown         : Not Active
        Display Clock Setting       : Not Active
    FB Memory Usage
        Total                       : 4043 MiB
        Used                        : 3813 MiB
        Free                        : 230 MiB
    BAR1 Memory Usage
        Total                       : 256 MiB
        Used                        : 3 MiB
        Free                        : 253 MiB
    Compute Mode                    : Default
    Utilization
        Gpu                         : 0 %
        Memory                      : 0 %
        Encoder                     : 0 %
        Decoder                     : 0 %
    Encoder Stats
        Active Sessions             : 0
        Average FPS                 : 0
        Average Latency             : 0
    FBC Stats
        Active Sessions             : 0
        Average FPS                 : 0
        Average Latency             : 0
    Ecc Mode
        Current                     : N/A
        Pending                     : N/A
    ECC Errors
        Volatile
            Single Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
            Double Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
        Aggregate
            Single Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
            Double Bit            
                Device Memory       : N/A
                Register File       : N/A
                L1 Cache            : N/A
                L2 Cache            : N/A
                Texture Memory      : N/A
                Texture Shared      : N/A
                CBU                 : N/A
                Total               : N/A
    Retired Pages
        Single Bit ECC              : N/A
        Double Bit ECC              : N/A
        Pending                     : N/A
    Temperature
        GPU Current Temp            : 37 C
        GPU Shutdown Temp           : N/A
        GPU Slowdown Temp           : 96 C
        GPU Max Operating Temp      : 92 C
        Memory Current Temp         : N/A
        Memory Max Operating Temp   : N/A
    Power Readings
        Power Management            : N/A
        Power Draw                  : N/A
        Power Limit                 : N/A
        Default Power Limit         : N/A
        Enforced Power Limit        : N/A
        Min Power Limit             : N/A
        Max Power Limit             : N/A
    Clocks
        Graphics                    : 993 MHz
        SM                          : 993 MHz
        Memory                      : 2505 MHz
        Video                       : 893 MHz
    Applications Clocks
        Graphics                    : N/A
        Memory                      : N/A
    Default Applications Clocks
        Graphics                    : N/A
        Memory                      : N/A
    Max Clocks
        Graphics                    : 1150 MHz
        SM                          : 1150 MHz
        Memory                      : 2505 MHz
        Video                       : 1035 MHz
    Max Customer Boost Clocks
        Graphics                    : N/A
    Clock Policy
        Auto Boost                  : N/A
        Auto Boost Default          : N/A
    Processes
        Process ID                  : 1123
            Type                    : G
            Name                    : /usr/lib/xorg/Xorg
            Used GPU Memory         : 8 MiB
        Process ID                  : 31763
            Type                    : C
            Name                    : python
            Used GPU Memory         : 3791 MiB
```

**Describe the problem**

When using my GPU, it takes several minutes (just over 4 minutes) to initialise to do anything. Issue does not exist when using CPU

**Provide the exact sequence of commands / steps that you executed before running into the problem**

`docker run -it -u $(id -u):$(id -g) --runtime=nvidia -v $(realpath ~/tensorflow):/tf/tensorflow tensorflow/tensorflow:latest-gpu bash`

`python test.py`

contents of test.py:

```
import tensorflow as tf
mnist = tf.keras.datasets.mnist

(x_train, y_train),(x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(512, activation=tf.nn.relu),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(x_train, y_train, epochs=5)
model.evaluate(x_test, y_test)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

logs while running test script
```
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
11501568/11490434 [==============================] - 0s 0us/step
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
2019-02-25 05:46:52.561440: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-25 05:46:52.628689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-25 05:46:52.629997: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x50be7d0 executing computations on platform CUDA. Devices:
2019-02-25 05:46:52.630035: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0
2019-02-25 05:46:52.664820: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2019-02-25 05:46:52.666234: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5128500 executing computations on platform Host. Devices:
2019-02-25 05:46:52.666318: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-25 05:46:52.666979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.90GiB
2019-02-25 05:46:52.667052: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-25 05:46:52.669065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-25 05:46:52.669122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-25 05:46:52.669152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-25 05:46:52.669563: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3696 MB memory) -> physical GPU (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)
Epoch 1/5
2019-02-25 05:51:01.254939: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
60000/60000 [==============================] - 5s 84us/sample - loss: 0.2207 - acc: 0.9348
Epoch 2/5
60000/60000 [==============================] - 5s 79us/sample - loss: 0.0960 - acc: 0.9714
Epoch 3/5
60000/60000 [==============================] - 5s 78us/sample - loss: 0.0697 - acc: 0.9774
Epoch 4/5
60000/60000 [==============================] - 5s 79us/sample - loss: 0.0536 - acc: 0.9826
Epoch 5/5
60000/60000 [==============================] - 5s 76us/sample - loss: 0.0430 - acc: 0.9857
10000/10000 [==============================] - 0s 29us/sample - loss: 0.0606 - acc: 0.9813
```"
26070,Model Memory requirements,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Want to print memory requirement of the model given a particular input size, batch size and a Structure of a model. It would cause Optimal GPU usage and also give a fair idea on what batch size should be.
**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Everyone who doesn't have a lot of compute and needs to know more specifics of the model
**Any Other info.**
"
26069,Many operations don't support uint64,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.14.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'unknown' 1.13.0-rc1
- Python version: Python 3.7.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

I've encountered several operations that support int64 but not uint64, without any clear reasoning. `tf.equal`, `tf.fill`, `tf.where`, and `tf.stack`, for example, give errors like:

    InvalidArgumentError (see above for traceback): No OpKernel was registered to support Op 'Pack' used by node stack (defined at <stdin>:4) with these attrs: [T=DT_UINT64, axis=0, N=2]


**Describe the expected behavior**

Functions that work for int64s should also work for uint64s when the behavior would be the same.

**Code to reproduce the issue**

https://gist.github.com/hjfreyer/31ab2dd2d85d1a509272af1c5e011dde
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26068,Incorrect predictions while exporting keras model to android,"I trained the Keras model on State Farm Distracted Driver Dataset and exported it to **.pb** graph file with the code given below:

```
def export_model(saver, model, input_node_names, output_node_name):
    tf.train.write_graph(K.get_session().graph_def, 'out', \
        MODEL_NAME + '_graph.pbtxt')

    saver.save(K.get_session(), 'out/' + MODEL_NAME + '.chkp')

    freeze_graph.freeze_graph('out/' + MODEL_NAME + '_graph.pbtxt', None, \
        False, 'out/' + MODEL_NAME + '.chkp', output_node_name, \
        ""save/restore_all"", ""save/Const:0"", \
        'out/frozen_' + MODEL_NAME + '.pb', True, """")

    input_graph_def = tf.GraphDef()
    with tf.gfile.Open('out/frozen_' + MODEL_NAME + '.pb', ""rb"") as f:
        input_graph_def.ParseFromString(f.read())

    output_graph_def = optimize_for_inference_lib.optimize_for_inference(
            input_graph_def, input_node_names, [output_node_name],
            tf.float32.as_datatype_enum)

    with tf.gfile.FastGFile('out/opt_' + MODEL_NAME + '.pb', ""wb"") as f:
        f.write(output_graph_def.SerializeToString())

    print(""graph saved!"")
```

 The problem is when I run that exported model on android, it always predicts wrong classes. The same code below predicts the correct labels when given .pb file of model trained on MNIST Dataset from [source.](https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/blob/master/mnistandroid/app/src/main/assets/opt_mnist_convnet-keras.pb).

```
public class MainActivity extends AppCompatActivity {
    ImageView imageView;
    Button button1;
    TextView label;
    private static final int PixelWidth = 100;
    //private List<Classifier> mClassifiers = new ArrayList<>();
    public static final int cam_req = 999; // used in OpenCamera Function
    public Classifier c11;
    private static final int PICK_IMAGE = 1;
    Uri imageUri;
    private boolean isLoaded= true;
    private float[] imageNormalizedPixels;
    private int [] imageBitmapPixels;
    private byte[] bytearray;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        c11 = new ModelClassifier();
        loadModel();

        super.onCreate(savedInstanceState);
        setContentView(R.layout.activity_main);
        imageView = (ImageView) findViewById(R.id.imageView);
        label = (TextView)findViewById(R.id.label_output);
        button1 = (Button) findViewById(R.id.button);
        button1.setOnClickListener(new View.OnClickListener() {
                                       @Override
                                       public void onClick(View v) {
                                           openGallery();
                                       }
                                   }

        );


    }

    private void loadModel() {
        //The Runnable interface is another way in which you can implement multi-threading other than extending the
        // //Thread class due to the fact that Java allows you to extend only one class. Runnable is just an interface,
        // //which provides the method run.
        // //Threads are implementations and use Runnable to call the method run().
        new Thread(new Runnable() {
            @Override
            public void run() {
                try {
                    c11=ModelClassifier.create(getAssets(), ""Keras"", ""opt_keras-10grey.pb"", ""labels.txt"", PixelWidth,
                          ""input_3"", ""softmax_1/Softmax"", false);

                } catch (final Exception e) {
                    //if they aren't found, throw an error!
                    label.setText(""Model not loaded"");
                    isLoaded = false;
                    throw new RuntimeException(""Error initializing classifiers!"", e);
                }
            }
        }).start();
    }



    private void openGallery() {
        if (isLoaded) {
            Intent gallery = new Intent(Intent.ACTION_PICK, MediaStore.Images.Media.INTERNAL_CONTENT_URI);
            startActivityForResult(gallery, PICK_IMAGE);
        }

    }

    public void OpenCamera(View view) {

        if (isLoaded) {

            Intent intnt = new Intent(MediaStore.ACTION_IMAGE_CAPTURE);
            startActivityForResult(intnt, cam_req);
        }
    }


    @Override
    protected void onActivityResult(int requestCode, int resultCode, Intent Data) {
        super.onActivityResult(requestCode, resultCode, Data);
        if (resultCode == RESULT_OK && requestCode == PICK_IMAGE) {

            try {
                final Uri imageUri = Data.getData();
                final InputStream imageStream = getContentResolver().openInputStream(imageUri);
                final Bitmap selectedImage = BitmapFactory.decodeStream(imageStream);
                ImageView imageView = (ImageView) findViewById(R.id.imageView);
                imageView.setImageBitmap(selectedImage);
                imageView.invalidate();
                BitmapDrawable drawable = (BitmapDrawable) imageView.getDrawable();

                Bitmap image = Bitmap.createScaledBitmap(selectedImage, PixelWidth, PixelWidth, false);

                float [] pixels = getPixels(image);

                if (pixels!=null) {
                    System.out.println(""It is gallery image"");

                    final Classification returned = c11.recognize(pixels);
                    System.out.println(""this is the predicted class: ""+returned.getLabel());
                    label.setText(returned.getLabel());


                }

            } catch (FileNotFoundException e) {
                e.printStackTrace();
            }

   }


    public float[] getPixels(Bitmap bitmap)
    {

        // Get 100x100 pixel data from bitmap
        int[] pixels = new int[PixelWidth * PixelWidth];
        bitmap.getPixels(pixels, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());

        float[] retPixels = new float[pixels.length];
        for (int i = 0; i < pixels.length; ++i) {
            // Set 0 for white and 255 for black pixel
            int pix = pixels[i];
            retPixels[i] = pix /255f;
        }
        return retPixels;
    }

    }

```
**ModelClassifier.java** class is given below:

```


public class ModelClassifier implements Classifier {

    private static final float THRESHOLD = 0.1f;
    private TensorFlowInferenceInterface tfHelper;

    private String name;
    private String InputName;
    private String OutputName;

    private int inputsize;

    private boolean feedKeepProb;

    private List<String> Labels;
    private float[] Output;

    private String OutputNames[];


   /*public  ModelClassier(Context a){
       Intent a1=a;
    }*/

    private static List<String> readLabels(AssetManager am, String fileName) throws IOException {
        BufferedReader br = new BufferedReader(new InputStreamReader(am.open(fileName)));

        String line;
        List<String> labels = new ArrayList<>();
        while ((line = br.readLine()) != null) {
            labels.add(line);
        }

        br.close();
        return labels;
    }

    public static ModelClassifier create(AssetManager assetManager, String name,
                                         String modelPath, String labelFile, int inputSize, String inputName, String outputName,
                                         boolean feedKeepProb) throws IOException {

        ModelClassifier c = new ModelClassifier();


        c.name = name;

        c.InputName = inputName;
        c.OutputName = outputName;


        c.Labels = readLabels(assetManager, labelFile);
        System.out.println(""These are the labels""+c.Labels.toString());

        c.tfHelper = new TensorFlowInferenceInterface(assetManager, modelPath);
        System.out.println(""ModelPath is++++++++++"" + modelPath);
        int numClasses = 10;


        c.inputsize = inputSize;


        c.OutputNames = new String[]{outputName};

        c.OutputName = outputName;
        c.Output = new float[numClasses];

        c.feedKeepProb = feedKeepProb;
        System.out.println(""Model is finally loaded++++++++++"");
        return c;
    }


    //@Override
    public String name() {
        return name;
    }


    @Override
    public Classification recognize(float[] pixels) {

        //using the interface
        //give it the input name, raw pixels from the drawing,
        //input size
        tfHelper.feed(InputName, pixels, new long[]{1, inputsize, inputsize, 1});

        //probabilities
        if (feedKeepProb) {
            tfHelper.feed(""keep_prob"", new float[]{1});
            System.out.println(""Infeedprob++++++++++"");
        }
        //get the possible outputs
        tfHelper.run(OutputNames);


        tfHelper.fetch(OutputName, Output);
        System.out.println(""afterfetch++++++++++"");
        // Find the best classification
        //for each output prediction
        //if its above the threshold for accuracy we predefined
        //write it out to the view
        Classification ans = new Classification();
        for (int i = 0; i < Output.length; ++i) {
            if (Output[i] > THRESHOLD && Output[i] > ans.getConf()) {
                ans.update(Output[i], Labels.get(i));
            }

        }

        return ans;
    }

```
**Note that model input shape is [100,100,1]**

Please tell me where I am making mistake. I have tried training on different number of classes but the outputs are completely different. 




"
26061,Cannot load a Keras model with a custom initializer/regularizer/constraint function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION == '2.0.0-dev20190222'
tf.version.GIT_VERSION == 'v1.12.0-8615-g74016a0d51'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I cannot load a model containing a custom initializer, or a custom regularizer, or a custom constraint, if they are defined as regular functions (rather than by subclassing the appropriate classes).

**Describe the expected behavior**
I expect it to work, since the model otherwise works fine, and is saved correctly.

**Code to reproduce the issue**
The following model uses a custom initializer, and a custom regularizer, and a custom constraint, it works fine, saves fine, but cannot be loaded. You can try using only one at a time, they all fail.

```python
import tensorflow as tf
from tensorflow import keras
import numpy as np

def my_glorot_initializer(shape, dtype=tf.float32):
    stddev = tf.sqrt(2. / (shape[0] + shape[1]))
    return tf.random.normal(shape, stddev=stddev, dtype=dtype)

def my_l1_regularizer(weights):
    return tf.reduce_sum(tf.abs(0.01 * weights))

def my_positive_weights(weights):
    return tf.nn.relu(weights)

X_train = np.random.randn(100, 2)
y_train = np.random.randn(100, 1)

model = keras.models.Sequential([
    keras.layers.Dense(1,
                       kernel_regularizer=my_l1_regularizer,
                       kernel_constraint=my_positive_weights,
                       kernel_initializer=my_glorot_initializer),
])

model.compile(loss=""mse"", optimizer=""nadam"")
model.fit(X_train, y_train, epochs=2)
model.save(""my_model.h5"")
model = keras.models.load_model(
    ""my_model.h5"",
    custom_objects={
       ""my_l1_regularizer"": my_l1_regularizer(0.01),
       ""my_positive_weights"": my_positive_weights,
       ""my_glorot_initializer"": my_glorot_initializer,
    })
```

**Other info / logs**
Here's the stacktrace:

```
TypeError                                 Traceback (most recent call last)
<ipython-input-1-e9ce6b82aa4f> in <module>
     31        ""my_l1_regularizer"": my_l1_regularizer(0.01),
     32        ""my_positive_weights"": my_positive_weights,
---> 33        ""my_glorot_initializer"": my_glorot_initializer,
     34     })

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py in load_model(filepath, custom_objects, compile)
    214     model_config = json.loads(model_config.decode('utf-8'))
    215     model = model_config_lib.model_from_config(model_config,
--> 216                                                custom_objects=custom_objects)
    217
    218     # set weights

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/model_config.py in model_from_config(config, custom_objects)
     53                     '`Sequential.from_config(config)`?')
     54   from tensorflow.python.keras.layers import deserialize  # pylint: disable=g-import-not-at-top
---> 55   return deserialize(config, custom_objects=custom_objects)
     56
     57

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
     77       module_objects=globs,
     78       custom_objects=custom_objects,
---> 79       printable_module_name='layer')

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    190             custom_objects=dict(
    191                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +
--> 192                 list(custom_objects.items())))
    193       with CustomObjectScope(custom_objects):
    194         return cls.from_config(cls_config)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/sequential.py in from_config(cls, config, custom_objects)
    349     for layer_config in layer_configs:
    350       layer = layer_module.deserialize(layer_config,
--> 351                                        custom_objects=custom_objects)
    352       model.add(layer)
    353     if not model.inputs and build_input_shape:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/serialization.py in deserialize(config, custom_objects)
     77       module_objects=globs,
     78       custom_objects=custom_objects,
---> 79       printable_module_name='layer')

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    192                 list(custom_objects.items())))
    193       with CustomObjectScope(custom_objects):
--> 194         return cls.from_config(cls_config)
    195     else:
    196       # Then `cls` may be a function returning a class.

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in from_config(cls, config)
    414         A layer instance.
    415     """"""
--> 416     return cls(**config)
    417
    418   def compute_output_shape(self, input_shape):

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py in __init__(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)
    930     self.activation = activations.get(activation)
    931     self.use_bias = use_bias
--> 932     self.kernel_initializer = initializers.get(kernel_initializer)
    933     self.bias_initializer = initializers.get(bias_initializer)
    934     self.kernel_regularizer = regularizers.get(kernel_regularizer)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py in get(identifier)
    176   elif isinstance(identifier, six.string_types):
    177     config = {'class_name': str(identifier), 'config': {}}
--> 178     return deserialize(config)
    179   elif callable(identifier):
    180     return identifier

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py in deserialize(config, custom_objects)
    165       module_objects=globals(),
    166       custom_objects=custom_objects,
--> 167       printable_module_name='initializer')
    168
    169

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/generic_utils.py in deserialize_keras_object(identifier, module_objects, custom_objects, printable_module_name)
    199       custom_objects = custom_objects or {}
    200       with CustomObjectScope(custom_objects):
--> 201         return cls(**cls_config)
    202   elif isinstance(identifier, six.string_types):
    203     function_name = identifier

TypeError: my_glorot_initializer() missing 1 required positional argument: 'shape'
```"
26060,tf-lite: allocateTensors never returns,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- I Have written custom code inspired from the tf-lite demo https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo. The code and model is here: https://github.com/BorisMansencal/tflite_test0
- OS Platform and Distribution: I build with up-to-date Android Studio 3.3.1 on macOS 10.13.6
- Mobile device: tested both on Nokia 7 Plus (TA-1046) with Android 9/API 28, and Motorola Moto G5S (XT1794) with Android 8.1.0/API 27.
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): I use tensorflow-lite:0.0.0-nightly

**Describe the current behavior**

My code uses tf-lite for image segmentation.
It works correctly on Nokia 7 Plus (TA-1046) with Android 9/API 28.
It does not work on Motorola Moto G5S (XT1794) with Android 8.1.0/API 27. 

**Describe the expected behavior**

My code is doing image segmentation using a U-Net DNN architecture.  
It is inspired from the the tf-lite demo example. It returns an image, instead of a classification.

The code is available here: https://github.com/BorisMansencal/tflite_test0
When run on on mobile device, you should see an image and a ""run"" button. You click on the ""run"" button and when the inference is finished, you should see an image : with white where the object was detected, black elsewhere.

It works correctly on Nokia 7 Plus (TA-1046) with Android 9/API 28, but not on Motorola Moto G5S (XT1794) with Android 8.1.0/API 27.

It should work on any device with API 26 Android version.

**Code to reproduce the issue**
https://github.com/BorisMansencal/tflite_test0

**Other info / logs**

When I run the application in the debugger, from Android Studio, on the Motorola Moto G5S , I can see that the call to the Interpreter constructor (called at ImageClassifier.java:59) never returns.
If I step into the Interpreter constructor, the problem seems to come from void NativeInterpreterWrapper::init(long errorHandle, long modelHandle, Options options): the call to allocateTensors(this.interpreterHandle, errorHandle) never returns.


"
26059,Tensorflow-gpu import error (solved),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Win 10

- TensorFlow installed from (source or binary):Source? Not to sure just using pip3
- TensorFlow version: tensorflow-gpu-1.12.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip3 inside a virtualenv environment.
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:10.0
- GPU model and memory: GTX 1080 8192 MB



**Describe the problem**
When attempting to do anything involving tensorflow it errors.  Errors while running ""import tensorflow as tf""  I looked at some older issues that were similar but trying to do what they said did not solve my problem.  First attempt was them saying use an older version of tensorflow but to do so I would need to downgrade CUDA to 9.0 not sure if that's what I should do.  I have tried to uninstall reinstall to no avail.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
C:\>pip install --upgrade pip
C:\>pip install virtualenv
C:\>virtualenv --system-site-packages -p ""C:\Users\Kathy\AppData\Local\Programs\Python\Python36\python.exe"" ./venv
C:\>.\venv\Scripts\activate
(venv) C:\>pip3 install tensorflow-gpu
(venv) C:\>python
>>>import tensorflow as tf
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```

Traceback (most recent call last):
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Kathy\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Kathy\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Kathy\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Kathy\venv\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Kathy\venv\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

```"
26050,Failed to convert object of type <class 'dict'> to Tensor.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code for Model Regression
- I used ANACONDA platform with Tensorflow to Develop DNNRegressor Estimator Mode

During i run code, i found some error during training model as per below 

```
WARNING:tensorflow:From C:\Users\thana\Anaconda3\lib\site-packages\tensorflow\python\estimator\inputs\queues\feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From C:\Users\thana\Anaconda3\lib\site-packages\tensorflow\python\estimator\inputs\queues\feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:Calling model_fn.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    526     try:
--> 527       str_values = [compat.as_bytes(x) for x in proto_values]
    528     except TypeError:

~\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py in <listcomp>(.0)
    526     try:
--> 527       str_values = [compat.as_bytes(x) for x in proto_values]
    528     except TypeError:

~\Anaconda3\lib\site-packages\tensorflow\python\util\compat.py in as_bytes(bytes_or_text, encoding)
     60     raise TypeError('Expected binary or unicode string, got %r' %
---> 61                     (bytes_or_text,))
     62 

TypeError: Expected binary or unicode string, got {'TOL': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:36' shape=(?,) dtype=float64>}

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
<ipython-input-19-b26ed31a0e2e> in <module>
----> 1 model.train(input_fn=input_func,steps=1000)

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py in train(self, input_fn, hooks, steps, max_steps, saving_listeners)
    352 
    353       saving_listeners = _check_listeners_type(saving_listeners)
--> 354       loss = self._train_model(input_fn, hooks, saving_listeners)
    355       logging.info('Loss for final step: %s.', loss)
    356       return self

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py in _train_model(self, input_fn, hooks, saving_listeners)
   1205       return self._train_model_distributed(input_fn, hooks, saving_listeners)
   1206     else:
-> 1207       return self._train_model_default(input_fn, hooks, saving_listeners)
   1208 
   1209   def _train_model_default(self, input_fn, hooks, saving_listeners):

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py in _train_model_default(self, input_fn, hooks, saving_listeners)
   1235       worker_hooks.extend(input_hooks)
   1236       estimator_spec = self._call_model_fn(
-> 1237           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
   1238       global_step_tensor = training_util.get_global_step(g)
   1239       return self._train_with_estimator_spec(estimator_spec, worker_hooks,

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\estimator.py in _call_model_fn(self, features, labels, mode, config)
   1193 
   1194     logging.info('Calling model_fn.')
-> 1195     model_fn_results = self._model_fn(features=features, **kwargs)
   1196     logging.info('Done calling model_fn.')
   1197 

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\canned\dnn.py in _model_fn(features, labels, mode, config)
    654           config=config,
    655           batch_norm=batch_norm,
--> 656           shared_state_manager=shared_state_manager)
    657 
    658     super(DNNRegressor, self).__init__(

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\canned\dnn.py in _dnn_model_fn(features, labels, mode, head, hidden_units, feature_columns, optimizer, activation_fn, dropout, input_layer_partitioner, config, use_tpu, batch_norm, shared_state_manager)
    313           labels=labels,
    314           optimizer=optimizer,
--> 315           logits=logits)
    316 
    317 

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\canned\head.py in create_estimator_spec(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)
    237           self._create_tpu_estimator_spec(
    238               features, mode, logits, labels, optimizer, train_op_fn,
--> 239               regularization_losses))
    240       return tpu_estimator_spec.as_estimator_spec()
    241     except NotImplementedError:

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\canned\head.py in _create_tpu_estimator_spec(self, features, mode, logits, labels, optimizer, train_op_fn, regularization_losses)
   1480 
   1481       training_loss, unreduced_loss, weights, _ = self.create_loss(
-> 1482           features=features, mode=mode, logits=logits, labels=labels)
   1483       if regularization_losses:
   1484         regularization_loss = math_ops.add_n(regularization_losses)

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\canned\head.py in create_loss(***failed resolving arguments***)
   1379     labels = _check_dense_labels_match_logits_and_reshape(
   1380         labels=labels, logits=logits,
-> 1381         expected_labels_dimension=self._logits_dimension)
   1382     labels = math_ops.to_float(labels)
   1383     if self._loss_fn:

~\Anaconda3\lib\site-packages\tensorflow\python\estimator\canned\head.py in _check_dense_labels_match_logits_and_reshape(labels, logits, expected_labels_dimension)
    303         'returns labels.')
    304   with ops.name_scope(None, 'labels', (labels, logits)) as scope:
--> 305     labels = sparse_tensor.convert_to_tensor_or_sparse_tensor(labels)
    306     if isinstance(labels, sparse_tensor.SparseTensor):
    307       raise ValueError(

~\Anaconda3\lib\site-packages\tensorflow\python\framework\sparse_tensor.py in convert_to_tensor_or_sparse_tensor(value, dtype, name)
    277     return value
    278   return ops.internal_convert_to_tensor(
--> 279       value, dtype=dtype, name=name)
    280 
    281 

~\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)
   1144 
   1145     if ret is None:
-> 1146       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1147 
   1148     if ret is NotImplemented:

~\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)
    227                                          as_ref=False):
    228   _ = as_ref
--> 229   return constant(v, dtype=dtype, name=name)
    230 
    231 

~\Anaconda3\lib\site-packages\tensorflow\python\framework\constant_op.py in constant(value, dtype, shape, name, verify_shape)
    206   tensor_value.tensor.CopyFrom(
    207       tensor_util.make_tensor_proto(
--> 208           value, dtype=dtype, shape=shape, verify_shape=verify_shape))
    209   dtype_value = attr_value_pb2.AttrValue(type=tensor_value.tensor.dtype)
    210   const_tensor = g.create_op(

~\Anaconda3\lib\site-packages\tensorflow\python\framework\tensor_util.py in make_tensor_proto(values, dtype, shape, verify_shape)
    529       raise TypeError(""Failed to convert object of type %s to Tensor. ""
    530                       ""Contents: %s. Consider casting elements to a ""
--> 531                       ""supported type."" % (type(values), values))
    532     tensor_proto.string_val.extend(str_values)
    533     return tensor_proto

TypeError: Failed to convert object of type <class 'dict'> to Tensor. Contents: {'TOL': <tf.Tensor 'random_shuffle_queue_DequeueUpTo:36' shape=(?,) dtype=float64>}. Consider casting elements to a supported type.

```
I don't sure that what kind of error that i m facing. 
What should i do to solve this problem ?

Thank you in advance for some suggestion and great advise

Best regards,
Nes





"
26049,official/utils/misc/distribution_utils.py uses the wrong API for OneDeviceStrategy resulting in runtime python error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'unknown' 1.13.0-rc2
- Python version: Python 3.7.2
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Receive an attribute not found error on tf.distribute.OneDeviceStrategy(""device:CPU:0"")
**Describe the expected behavior**
Not see this error.
**Code to reproduce the issue**
Run cifar10_main.py on a macbook without an NVIDIA GPU, I imagine this would also happen on linux if no GPU was detected.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26048,Check failure and silent failures with incorrect usage of tf.custom_gradient (in eager mode).,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.13.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-8779-g2ae06ca491 1.13.0-dev20190223 (as well as 1.12.0)
- Python version: Python 3.6.4 :: Anaconda, Inc.

When `tf.custom_gradient` is used incorrectly (in this case, the returned `grad` function returns an empty list, the script segfaults.

```python
import tensorflow as tf

tf.enable_eager_execution()

@tf.custom_gradient
def identity(x):
    def grad(dy):
        return []  # This return value is wrong!
    return x, grad

x = tf.Variable(1.0)
with tf.GradientTape() as t:
    y = identity(x)
t.gradient(y, [x])
```

The `t.gradient` call fails with 

```
2019-02-23 18:09:14.621207: F ./tensorflow/c/eager/tape.h:642] Check failed: state.op_tape.empty() 
Abort trap: 6
```

I think it'd be preferable to raise an exception instead of crashing.

If I instead return too many values from `grad`, then the script runs, but this is most likely a bug and should probably raise an exception.

```python
import tensorflow as tf

tf.enable_eager_execution()

@tf.custom_gradient
def identity(x):
    def grad(dy):
        return 1.0, 2.0  # Too many return values!
    return x, grad

x = tf.Variable(1.0)
with tf.GradientTape() as t:
    y = identity(x)
t.gradient(y, [x])
```

FYI @alextp "
26047,Switching between TF 1.x and TF 2.0 in the API browser loses the context,"**System information**
- TensorFlow version: from 
- Doc Link:


**Describe the documentation issue**
When I'm browsing the documentation, I often need to switch from TF 1.x to TF 2.0 or vice versa. Unfortunately, the context is lost when I do that.
For example, visit the `tf.constant` documentation for TF 2.0:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/constant
Then from the drop down menu at the top (labeled ""API r2.0""), select any TF 1.x version. Notice that you do not land on the `tf.constant` page.
The same is true in the reverse direction.

This also makes it hard to find TF 2.0 documentation by searching: I generally land on a TF 1.x page, when I switch to TF 2.0 the context is lost.

Of course, many functions have been removed or moved, but this should not be a problem, since most of them are still available in `tf.compat.v1`.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Why not, but I would need someone to point me to the code that handles this."
26046,Tensorflow 1.13.0 reports wrong version,"**System information**

- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from: source
- TensorFlow version: 1.13.0
- Python version: 3.5.2
- Bazel version : 0.22.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: cuda10.0 cudnn7.4.2.24
- GPU model and memory: GTX1070

Tensorflow compiled and built without issue. But when created wheel with command

```
 ./bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOME/opt/cuda_test/cuda10/tensorflow_pkg

```
The wheel created was called tensorflow-**1.12.0**-cp35-cp35m-linux_x86_64.whl

I changed 1.12.0 to 1.13.0 manually and install it with pip
```
cd $HOME/tensorflow_pkg
pip3 install --no-cache-dir ./tensorflow-1.13.0-cp35-cp35m-linux_x86_64.whl --user
Processing ./tensorflow-1.13.0-cp35-cp35m-linux_x86_64.whl
Requirement already satisfied: absl-py>=0.1.6 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.7.0)
Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (1.0.5)
Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (1.11.0)
Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (1.0.6)
Requirement already satisfied: google-pasta>=0.1.2 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.1.4)
Requirement already satisfied: gast>=0.2.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.2.2)
Requirement already satisfied: protobuf>=3.6.1 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (3.6.1)
Requirement already satisfied: numpy<2.0,>=1.14.5 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.16.1)
Requirement already satisfied: grpcio>=1.8.6 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.18.0)
Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.5/dist-packages (from tensorflow==1.13.0) (0.31.1)
Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.13.0rc0)
Requirement already satisfied: termcolor>=1.1.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.1.0)
Requirement already satisfied: astor>=0.6.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (0.7.1)
Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorflow==1.13.0) (1.12.2)
Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.0) (2.9.0)
Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.0) (40.8.0)
Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0) (2.0.0)
Requirement already satisfied: werkzeug>=0.11.10 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0) (0.14.1)
Requirement already satisfied: markdown>=2.6.8 in /home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow==1.13.0) (3.0.1)
Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.5/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow==1.13.0) (4.0.4)
Installing collected packages: tensorflow
**Successfully installed tensorflow-1.12.0**

```
From last sentence of ouput above python still sees tensorflow-1.12.0

Now
```
$ python3
Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
```
>>> import tensorflow as tf
Limited tf.compat.v2.summary API due to missing TensorBoard installation

```
```
"
26045,tensorflow 1.12.0 install package requires protobuf>=3.6.1 but comes with protobuf==3.6.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

('v1.12.0-0-ga6d8ffae09', '1.12.0')

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

If you download the Python 2.7 CPU-only official whl file (following https://www.tensorflow.org/install/pip) https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.12.0-cp27-none-linux_x86_64.whl, you will see that it requires protobuf>=3.6.1.

However, after you extract it and see the include files, they are protobuf==3.6.0.

**Describe the expected behavior**

The protobuf version should be consistent.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

wget https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.12.0-cp27-none-linux_x86_64.whl
unzip tensorflow-1.12.0-cp27-none-linux_x86_64.whl
less tensorflow-1.12.0.dist-info/metadata.jsoin 
less tensorflow-1.12.0.data/purelib/tensorflow/include/google/protobuf/stubs/common.h

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26044,Needs upgrade Russian's translation of page: /tutorials/keras/basic_classification,"**System information**
- TensorFlow version: not important
- Doc Link: https://www.tensorflow.org/tutorials/keras/basic_classification

**Describe the documentation issue**
When I was reading the Russian translation of this documentation page, I found a few places with not a natural way of using the Russian language(I'm a native Russian speaker), also I noticed a few small typo mistakes. I carefully read the whole page and compared with the original article on English and made some updates. The PR with these changes will be sent in the next couple of minutes after opening this issue. PR with updates: https://github.com/tensorflow/docs/pull/345"
26042,Tensorflow r1.12 CPU-only build fails on Ubuntu 18.04,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Not yet installed, trying to build from source
- TensorFlow version: r1.12
- Python version: 2.7.15rc1
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

While bulding tensorflow r1.12 CPU only for Python 2.7, the build fails with the following error:

`ERROR: /home/davide/Downloads/tensorflow/tensorflow/python/estimator/api/BUILD:12:1: Executing genrule //tensorflow/python/estimator/api:estimator_python_api_gen failed (Exit 1)`

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
git clone https://github.com/tensorflow/tensorflow.git 
cd tensorflow
git checkout r1.12
./configure
```

This is the configuration file (`.tf_configure.bazelrc`):

```
build --action_env PYTHON_BIN_PATH=""/usr/bin/python""
build --action_env PYTHON_LIB_PATH=""/usr/local/lib/python2.7/dist-packages""
build --python_path=""/usr/bin/python""
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build:opt --copt=-march=native
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
```

Because I'm using bazel 0.19.2, I opened the file .bazelrc and added at the beginning the command `import /home/davide/Downloads/tensorflow/tools/bazel.rc`

`bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package`



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
ERROR: /home/davide/Downloads/tensorflow/tensorflow/python/estimator/api/BUILD:12:1: Executing genrule //tensorflow/python/estimator/api:estimator_python_api_gen failed (Exit 1)
Traceback (most recent call last):
  File ""/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 510, in <module>
    main()
  File ""/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 503, in main
    importlib.import_module(args.package)
  File ""/usr/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow.python.estimator.estimator_lib
  File ""/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/estimator/estimator_lib.py"", line 22, in <module>
    from tensorflow.python.estimator.canned.baseline import BaselineClassifier
  File ""/home/davide/.cache/bazel/_bazel_davide/621bc5cc8d05a8843bd67c8ac5065abd/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/python/estimator/api/create_tensorflow.python.estimator_api_2.runfiles/org_tensorflow/tensorflow/python/estimator/canned/baseline.py"", line 50, in <module>
    from tensorflow.python.estimator import estimator
ImportError: cannot import name estimator
tf.estimator package not installed.
tf.estimator package not installed.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 8147.753s, Critical Path: 232.48s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 6805 processes: 6805 local.
FAILED: Build did NOT complete successfully
```"
26041,tf-nightly-gpu-2.0-preview dataset and tf.function error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): tf-nightly-gpu-2.0-preview
- Python version: Python 3 in Google Colab
- CUDA/cuDNN version: 10
- GPU model and memory: Tesla K80

**Describe the current behavior**

When using GPU runtime in Google Colab with tf-nightly-gpu-2.0-preview tensorflow version, creating a dataset with tensorflow datasets inside a tf.function decorated function results in an exception regarding incompatible device types. 

**Describe the expected behavior**

 Expected behavior is not receiving an exception. Removing tf.function from the train method results in correct behavior. The code also works well without GPU support (tf-nightly-2.0-preview).

**Code to reproduce the issue**

```python

import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np

class MyModel(tf.keras.Model):

    def __init__(self):
        super(MyModel, self).__init__()
        self._layer1 = tf.keras.layers.Dense(20, activation='relu')
        self._layer2 = tf.keras.layers.Dense(10)

    def call(self, x, training):
        x = self._layer1(x)
        x = self._layer2(x)
        return x


def create_dataset():
    def process(features):
        image, label = features['image'], features['label']
        return tf.reshape(image, [-1]) / np.float32(255.0), label

    data_builder = tfds.builder('mnist')
    dataset = data_builder.as_dataset(split=tfds.Split.TRAIN)
    dataset = (
        dataset.map(process)
        .batch(32)
        .repeat(1)
    )

    return dataset

  
avg_loss = tf.metrics.Mean()

  
@tf.function
def train(model, optimizer):
    dataset = create_dataset()
    step = 0
    for images, labels in dataset:
        step += 1
        with tf.GradientTape() as tape:
            logits = model(images, True)
            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(
                logits=logits, labels=labels)
            loss = tf.math.reduce_mean(loss)
            
        avg_loss.update_state(loss)
        
        grads = tape.gradient(
            loss, model.trainable_variables)
        optimizer.apply_gradients(
            zip(grads, model.trainable_variables))
        
        if tf.equal(step % 20, 0):
            tf.print(avg_loss.result())
            avg_loss.reset_states()
            

NUM_EPOCHS = 2
model = MyModel()
optimizer = tf.keras.optimizers.Adam()
for _ in range(NUM_EPOCHS):
    train(model, optimizer)

```


**Other info / logs**

Link to the original Google Colab [file](https://gist.github.com/Mrpatekful/92f274756dffd6aab2993e401b7fb7af)

The encountered exception:

```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-43-03d2bfd1799c> in <module>()
     59 optimizer = tf.keras.optimizers.Adam()
     60 for _ in range(NUM_EPOCHS):
---> 61     train(model, optimizer)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    436         # Lifting succeeded, so variables are initialized and we can run the
    437         # stateless function.
--> 438         return self._stateless_fn(*args, **kwds)
    439     else:
    440       canon_args, canon_kwds = self._canonicalize_function_inputs(args, kwds)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1251     """"""Calls a graph function specialized to the inputs.""""""
   1252     graph_function, args, kwargs = self._maybe_define_function(args, kwargs)
-> 1253     return graph_function._filtered_call(args, kwargs)  # pylint: disable=protected-access
   1254 
   1255   @property

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _filtered_call(self, args, kwargs)
    537     """"""
    538     return self._call_flat(
--> 539         (t for t in nest.flatten((args, kwargs))
    540          if isinstance(t, (ops.Tensor,
    541                            resource_variable_ops.ResourceVariable))))

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    590     # Only need to override the gradient in graph mode and when we have outputs.
    591     if context.executing_eagerly() or not self.outputs:
--> 592       outputs = self._inference_function.call(ctx, args)
    593     else:
    594       self._register_gradient()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    380             attrs=(""executor_type"", executor_type,
    381                    ""config_proto"", config),
--> 382             ctx=ctx)
    383       # Replace empty list with None
    384       outputs = outputs or None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot place the graph because a reference or resource edge connects colocation groups with incompatible assigned devices: /job:localhost/replica:0/task:0/device:CPU:0 vs /job:localhost/replica:0/task:0/device:GPU:0 [Op:__inference_train_925768]
```"
26038,Warn or throw error when calling tf.train.TFRecordDataset([]),"### Issue

I am just here because it took me way to long to get behind this issue as somebody who tried to write `.tfrecord` files and use them in a `TFRecordDataset` for the first time.

I am not sure if this is intended behavior but it turns out that `dataset = tf.data.TFRecordDataset([])` won't bother you until you run the graph in a session but will only give you 

```
OutOfRangeError (see above for traceback): End of sequence
```
As an error. I was thinking about everything else like wrong data types, not enough samples inside the files etc. until I turned out to be a typo in `file_paths` and I realized I was just passing in an empty array.

### Reproduce

```python
import tensorflow as tf


def main():

    dataset = tf.data.TFRecordDataset([])
    iterator = dataset.make_one_shot_iterator()
    get_next = iterator.get_next()

    with tf.Session() as session:
        session.run(tf.global_variables_initializer())
        batch = session.run(get_next)
        print(batch)


if __name__ == '__main__':
    main()
```


### Environment

```
$ pip freeze | grep tensor
tensorboard==1.12.0
tensorflow-gpu==1.12.0

$ uname -svomp
Linux #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 GNU/Linux
```"
26037,winsock2.h,"tensorflow\core\platform\cloud\gcs_dns_cache.cc i think it should like below 
#include <winsock2.h>
#include <Windows.h>"
26036,event.pb.h error,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (win10):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: r1.12
- Python version:3.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): vc 2017
- CUDA/cuDNN version: No
- GPU model and memory: No 
I want to build the only cpu version first .



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**

python ./configure.py ...
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

: :   bazel-out/x64_windows-opt/genfiles\tensorflow/core/util/event.
pb.h
: :    bazel-out/x64_windows-opt/genfiles\tensorflow/core/framework/
summary.pb.h
bazel-out/x64_windows-opt/genfiles\tensorflow/core/util/event.pb.h(584): error C2059: :
bazel-out/x64_windows-opt/genfiles\tensorflow/core/util/event.pb.h(585): error C2238: ;
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 5943.765s, Critical Path: 303.77s
INFO: 644 processes: 644 local.
FAILED: Build did NOT complete successfully
event.pb.h:
584: static const Level ERROR =
585:   LogMessage_Level_ERROR;
"
26035,"I having an issue that tensorflow is throwing ""Failed to load the native Tensorflow runtime"".","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: *Windows 10 Home*
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.2, cuDNN 7.4.1.5
- GPU model and memory: gtx 1050, 4gb DDR5
-Driver version: 398.35
-Laptop RAM: 8GB
-Platform : Anaconda3 64-bit for python 3.6


Below is the error that it keep on getting. I also looked for the solution that asked me to install Microsoft c++ redistributable 2015. After installing that as well the error persists.
*****
runfile('C:/Users/vishw/.spyder-py3/temp.py', wdir='C:/Users/vishw/.spyder-py3')
Traceback (most recent call last):

  File ""<ipython-input-1-5780f59638b8>"", line 1, in <module>
    runfile('C:/Users/vishw/.spyder-py3/temp.py', wdir='C:/Users/vishw/.spyder-py3')

  File ""C:\Users\vishw\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 705, in runfile
    execfile(filename, namespace)

  File ""C:\Users\vishw\Anaconda3\lib\site-packages\spyder\utils\site\sitecustomize.py"", line 102, in execfile
    exec(compile(f.read(), filename, 'exec'), namespace)

  File ""C:/Users/vishw/.spyder-py3/temp.py"", line 1, in <module>
    import tensorflow as tf

  File ""C:\Users\vishw\Anaconda3\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import

  File ""C:\Users\vishw\Anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow

  File ""C:\Users\vishw\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)

ImportError: Traceback (most recent call last):
  File ""C:\Users\vishw\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\vishw\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\vishw\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\vishw\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\vishw\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors
*******
for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I need a bit of help here. If you need more information about hardware so i am using MSI GF638RC with gtx1050 nvidia graphic card.

I ran a basic tensorflow code to print hello tensorflow to test. 

Thank you !

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26033,[TF2.0] Change default types globally,"Hello everyone,

I made the same request a while ago at [tensorflow/community](https://github.com/tensorflow/community/issues/23). The similar question was raised before at [tensorflow/tensorflow#9781](https://github.com/tensorflow/tensorflow/issues/9781), where maintainers argued that GPU is much faster on float32, default type cannot (should not) be changed because of backwards compatibility reasons and cetera.

The thing is that the precision is very important for algorithms where operations like cholesky, solvers and etc. are used. This becomes very tedious to specify type everywhere, it gets even worse when you start using other frameworks or small libraries which follow the standard type settings and sometimes they become useless, just because type incompatibilities. The policy of **""changing types locally solves your problems""** becomes cumbersome.

It would be great to have methods `tf.set_default_float` and `tf.set_default_int` in TensorFlow 2.0 and I believe that such a small change will make TensorFlow more user friendly.

Kind regards,
Artem Artemev"
26032,[TF2.0] default type ,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
26031,Cannot create stateful metrics based on symbolic tensors using the functional API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION == '2.0.0-dev20190222'
tf.version.GIT_VERSION == 'v1.12.0-8615-g74016a0d51'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
I get an exception when I try to create a metric based on a symbolic tensor when using the functional API: `keras.metrics.Mean()(hidden1)`

**Describe the expected behavior**
I expect to be able to define metrics based on any layer's output, when using the functional API.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow import keras

inputs = keras.layers.Input(shape=[2])
hidden1 = keras.layers.Dense(30)(inputs)
hidden1_mean = keras.metrics.Mean()(hidden1) # TypeError: see stacktrace below
```

**Other info / logs**
I know I could use instead:

```python
model.add_metric(hidden1, name=""hidden1_mean"", aggregation=""mean"")
```

But using stateful metrics should be possible.

Here is the stacktrace:

```pycon
TypeError                                 Traceback (most recent call last)
<ipython-input-1-8b9d3421cd04> in <module>
      4 inputs = keras.layers.Input(shape=[2])
      5 hidden1 = keras.layers.Dense(30)(inputs)
----> 6 mean = keras.metrics.Mean()(hidden1)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py in __call__(self, *args, **kwargs)
    172       The metric value tensor.
    173     """"""
--> 174     update_op = self.update_state(*args, **kwargs)
    175     with ops.control_dependencies([update_op]):
    176       result_t = self.result()

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
     70     """"""Decorated function with `add_update()`.""""""
     71
---> 72     update_op = update_state_fn(*args, **kwargs)
     73     if update_op is not None:  # update_op will be None in eager execution.
     74       metric_obj.add_update(update_op, inputs=True)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/metrics.py in update_state(self, values, sample_weight)
    324     elif self.reduction == metrics_utils.Reduction.WEIGHTED_MEAN:
    325       if sample_weight is None:
--> 326         num_values = math_ops.cast(array_ops.size(values), self._dtype)
    327       else:
    328         num_values = math_ops.reduce_sum(sample_weight)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    178     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    179     try:
--> 180       return target(*args, **kwargs)
    181     except (TypeError, ValueError):
    182       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in size(input, name, out_type)
    397   @end_compatibility
    398   """"""
--> 399   return size_internal(input, name, optimize=True, out_type=out_type)
    400
    401

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in size_internal(input, name, optimize, out_type)
    418     input = ops.convert_to_tensor(input)
    419     np_out_type = out_type.as_numpy_dtype
--> 420     num_elements = np.prod(input._shape_tuple(), dtype=np_out_type)  # pylint: disable=protected-access
    421     return ops.convert_to_tensor(num_elements, dtype=out_type)
    422   with ops.name_scope(name, ""Size"", [input]) as name:

~/.virtualenvs/tf2/lib/python3.6/site-packages/numpy/core/fromnumeric.py in prod(a, axis, dtype, out, keepdims, initial)
   2770     """"""
   2771     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out, keepdims=keepdims,
-> 2772                           initial=initial)
   2773
   2774

~/.virtualenvs/tf2/lib/python3.6/site-packages/numpy/core/fromnumeric.py in _wrapreduction(obj, ufunc, method, axis, dtype, out, **kwargs)
     84                 return reduction(axis=axis, out=out, **passkwargs)
     85
---> 86     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
     87
     88

TypeError: int() argument must be a string, a bytes-like object or a number, not 'NoneType'
```"
26029,super() does not work within a tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION == '2.0.0-dev20190222'
tf.version.GIT_VERSION == 'v1.12.0-8615-g74016a0d51'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When I call a method decorated by `@tf.function`, I get an error if it uses `super()`: `RuntimeError: super(): __class__ cell not found`.

**Describe the expected behavior**
I expect no error, `@tf.function` should ensure that `super()` works normally.

**Code to reproduce the issue**
```python
import tensorflow as tf

class A:
    def foo(self, x):
        return x + 1

class B(A):
    @tf.function
    def bar(self, x):
        return super().foo(x)

b = B()
b.bar(5) # raises RuntimeException
```

**Other info / logs**
I can work around this issue in multiple ways:

The easiest is to replace `super()` with `super(B, self)`. But it's 2019, who still uses Python 2 style? ;-)

Or else, I can work around the issue by using `autograph=False`. This shows that the issue is linked to autograph not recognizing `super()`, only `super(B, self)`:

```python
import tensorflow as tf

class A:
    def foo(self, x):
        return x + 1

class B(A):
    @tf.function(autograph=False)
    def bar(self, x):
        return super().foo(x)

b = B()
b.bar(5) # okay, returns 6
```

I can also work around this issue by calling `super()` outside of the method, e.g., in the constructor:

```python
import tensorflow as tf

class A:
    def foo(self, x):
        return x + 1

class B(A):
    def __init__(self):
        self._super = super()
    @tf.function
    def bar(self, x):
        return self._super.foo(x)

b = B()
b.bar(5) # okay, returns 6
```

I tried to work around it using a `tf.init_scope()`, but I could not get it to work, not sure why.

Here is the full stacktrace for the first example code:

```pycon
RuntimeError                              Traceback (most recent call last)
<ipython-input-2-db8198c32b2e> in <module>
      9
     10 b = B()
---> 11 b.bar(5)

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    424     # This is the first call of __call__, so we have to initialize.
    425     initializer_map = {}
--> 426     self._initialize(args, kwds, add_initializers_to=initializer_map)
    427     if self._created_variables:
    428       try:

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    368     self._concrete_stateful_fn = (
    369         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 370             *args, **kwds))
    371
    372     def invalid_creator_scope(*unused_args, **unused_kwds):

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   1278     if self._input_signature:
   1279       args, kwargs = None, None
-> 1280     graph_function, _, _ = self._maybe_define_function(args, kwargs)
   1281     return graph_function
   1282

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   1545           or call_context_key not in self._function_cache.missed):
   1546         self._function_cache.missed.add(call_context_key)
-> 1547         graph_function = self._create_graph_function(args, kwargs)
   1548         self._function_cache.primary[cache_key] = graph_function
   1549         return graph_function, args, kwargs

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   1477             arg_names=arg_names,
   1478             override_flat_arg_shapes=override_flat_arg_shapes,
-> 1479             capture_by_value=self._capture_by_value),
   1480         self._function_attributes)
   1481

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    636         tf_decorator.rewrap(python_func, original_func, converted_func)
    637
--> 638       func_outputs = python_func(*func_args, **func_kwargs)
    639
    640       # invariant: `func_outputs` contains only Tensors, IndexedSlices,

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    315         # __wrapped__ allows AutoGraph to swap in a converted function. We give
    316         # the function a weak reference to itself to avoid a reference cycle.
--> 317         return weak_wrapped_fn().__wrapped__(*args, **kwds)
    318     weak_wrapped_fn = weakref.ref(wrapped_fn)
    319

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/eager/function.py in bound_method_wrapper(*args, **kwargs)
   2060     # If __wrapped__ was replaced, then it is always an unbound function
   2061     # that takes self as first argument.
-> 2062     return wrapped_fn(weak_instance(), *args, **kwargs)
   2063   weak_bound_method_wrapper = weakref.ref(bound_method_wrapper)
   2064

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    629                   optional_features=autograph_options,
    630                   force_conversion=True,
--> 631               ), args, kwargs)
    632
    633         # Wrapping around a decorator allows checks like tf_inspect.getargspec

~/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, owner, options, args, kwargs)
    358     return f(*args, **kwargs)
    359
--> 360   result = converted_f(*effective_args, **kwargs)
    361
    362   # The converted function's closure is simply inserted into the function's

/var/folders/wy/h39t6kb11pnbb0pzhksd_fqh0000gn/T/tmpd7lvn4li.py in tf__bar(self, x)
      4   retval_ = None
      5   do_return = True
----> 6   retval_ = ag__.converted_call('foo', super(), ag__.ConversionOptions(recursive=True, verbose=0, strip_decorators=(tf.function, defun, ag__.convert, ag__.do_not_convert, ag__.converted_call), force_conversion=False, optional_features=(), internal_convert_user_code=True), (x,), {})
      7   return retval_
      8

RuntimeError: super(): __class__ cell not found
```"
26028,TensorFlow library was compiled to use AVX512F instructions not importing after install,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): Source (master a2bb5db1bf7931b0dc2cd08e53b8798489568198)
- TensorFlow version: 
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: Pip from Intel Distribution for Python on Conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0 / 7.4.2
- GPU model and memory: 2080 Ti - 11 GB
- Intel MKL Version: 2019.2.057
- Intel CPU: i9 9900K

**Describe the problem**

After building and installing with `pip` and then testing via importing with python, `cpu_feature_guard.cc` reporting the TF library was compiled to use AVX512F but they aren't available on the device. I have checked this and the instruction sets are available on my Intel CPU. Can anyone help? I've been having trouble using this bazel build configuration lately but it was working fine previously before without Intel MKL. 

```
Python 3.6.5 |Intel Corporation| (default, Aug  3 2018, 14:28:11) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> import tensorflow as tf
2019-02-23 17:55:40.392830: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX512F instructions
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/2896601/tf_env.txt)
, but these aren't available on your machine.
Aborted (core dumped)
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
(intel_dl4cv) $ bazel build --config=opt --config=cuda --config=mkl -c opt --copt=-mavx --copt=-mavx2 --copt=-mfma --copt=-mavx512f --copt=-mavx512pf --copt=-mavx512cd --copt=-mavx512er //tensorflow/tools/pip_package:build_pip_package
...
Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed time: 3895.082s, Critical Path: 245.69s
INFO: 15722 processes: 15722 local.
INFO: Build completed successfully, 16924 total actions

(intel_dl4cv) $ bazel-bin/tensorflow/tools/pip_package/build_pip_package tensorflow_pkg

(intel_dl4cv) $ python -V
Python 3.6.5 :: Intel Corporation

(intel_dl4cv) $ which pip
/home/codeninja/anaconda3/envs/intel_dl4cv/bin/pip

(intel_dl4cv) $ pip install tensorflow_pkg/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl 
Processing ./tensorflow_pkg/tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl
...
Installing collected packages: tensorflow-estimator, tensorboard, google-pasta, tensorflow
Successfully installed google-pasta-0.1.4 tensorboard-1.12.2 tensorflow-1.12.0 tensorflow-estimator-1.13.0

(intel_dl4cv) $ python
Python 3.6.5 |Intel Corporation| (default, Aug  3 2018, 14:28:11) 
[GCC 4.8.2 20140120 (Red Hat 4.8.2-15)] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Intel(R) Distribution for Python is brought to you by Intel Corporation.
Please check out: https://software.intel.com/en-us/python-distribution
>>> import tensorflow as tf
2019-02-23 17:55:40.392830: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX512F instructions, but these aren't available on your machine.
Aborted (core dumped)
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Checking the instruction sets I have available on my machine.
```
$ gcc -march=knl -dM -E - < /dev/null | egrep ""SSE|AVX"" | sort
#define __AVX__ 1
#define __AVX2__ 1
#define __AVX512CD__ 1
#define __AVX512ER__ 1
#define __AVX512F__ 1
#define __AVX512PF__ 1
#define __SSE__ 1
#define __SSE2__ 1
#define __SSE2_MATH__ 1
#define __SSE3__ 1
#define __SSE4_1__ 1
#define __SSE4_2__ 1
#define __SSE_MATH__ 1
#define __SSSE3__ 1
```
"
26026,"TypeError: can only concatenate list (not ""tuple"") to list","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
* TensorFlow installed from (source or binary): binary
* TensorFlow version (use command below): 2.0.0-dev20190217
* Python version: 3.5.2
* Bazel version (if compiling from source): n/a
* GCC/Compiler version (if compiling from source): n/a
* CUDA/cuDNN version: 10
* GPU model and memory: GTX 1080 8G


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

```
Traceback (most recent call last):
  File ""t.py"", line 16, in <module>
    model(data)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 620, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/sequential.py"", line 248, in call
    outputs = layer(inputs, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py"", line 713, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 620, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py"", line 2132, in call
    return [output] + states
TypeError: can only concatenate list (not ""tuple"") to list
```

**Describe the expected behavior**

Should not throw exception

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```py
import tensorflow as tf

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking())
model.add(tf.keras.layers.GRU(
    units=1,
    # Set `return_state` to `True` will cause the TypeError:
    #   ""can only concatenate list (not ""tuple"") to list""
    return_state=True,
))

data = [1.]
data = tf.convert_to_tensor(data)
data = tf.reshape(data, (1, 1, 1))

model(data)
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

1. If we do not use `tf.keras.layers.Masking`, then it will not throw the exception
1. If we do not use `return_state`, then it will not throw the exception

The problem is at:

https://github.com/tensorflow/tensorflow/blob/2ae06ca49145bfee2e5e98c64ae5cbe064a58a33/tensorflow/python/keras/layers/recurrent.py#L2134

Which the `state` returned from `K.rnn` is a `tuple` instead of a `list`, so that ` [output] + states` will fail and throw exception.

https://github.com/tensorflow/tensorflow/blob/2ae06ca49145bfee2e5e98c64ae5cbe064a58a33/tensorflow/python/keras/layers/recurrent.py#L2107
"
26025,unable to import tensor flow and keras,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip installed using the tensorflow guide
- TensorFlow version: tried with pip install tensorflow, pip install tensorflow==1.5, pip install tensorflow==1.4
- Python version: 3.6.0
- Installed using virtualenv? pip? conda?: in command line using pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: intel core i5



**Describe the problem**


**Provide the exact sequence of commands / steps that you executed before running into the problem**
python 3.6 is installed and I have pip installed tensorflow==1.4 still receiving that error


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
C:\Users\kashish\myFlask>python
Python 3.6.0 (v3.6.0:41df79263a11, Dec 23 2016, 08:06:12) [MSC v.1900 64 bit (AM
D64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>>
>>>
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\kashish\AppData\Local\Programs\Python\Python36\lib\site-package
s\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""C:\Users\kashish\AppData\Local\Programs\Python\Python36\lib\site-package
s\tensorflow\python\__init__.py"", line 52, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\kashish\AppData\Local\Programs\Python\Python36\lib\site-package
s\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\kashish\AppData\Local\Programs\Python\Python36\lib\site-package
s\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found.
>>>
"
26024,[TF Java] Support for AddControlInput in generated factory methods for building operations ,"**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**
There are automatically generated wrapper classes for Graph Operations that provide a neat interface and consistency in practice. We can easily create graph operations with the `create` method. 

However, there is no option to add control inputs to a newly built operation using these wrapper classes. 

For example, in the generated `Add.java` file:
```
    public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y) {
        OperationBuilder opBuilder = scope.graph().opBuilder(""Add"", scope.makeOpName(""Add""));
        opBuilder.addInput(x.asOutput());
        opBuilder.addInput(y.asOutput());
        return new Add(opBuilder.build());
    }
```

**Will this change the current api? How?**

One possibility is to overload the `create` method, such that the new overloaded method would have an additional parameter that is a list of control inputs. This way, current usages of the original `create` method would not break. The proposed generated `create` method would look like:
```
    public static <T> Add<T> create(Scope scope, Operand<T> x, Operand<T> y, Iterable<Operand<T>> controlInputs) {
        OperationBuilder opBuilder = scope.graph().opBuilder(""Add"", scope.makeOpName(""Add""));
        opBuilder.addInput(x.asOutput());
        opBuilder.addInput(y.asOutput());
       for (Operand<T> ctrl : controlInputs) { opBuilder = operBuilder.addControlInput(ctrl.asOutput().op()) }
        return new Add(opBuilder.build());
    }
```
This would require modifying `tensorflow/tensorflow/java/src/gen/cc/op_generator.cc` to add this to the code gen. 

**Who will benefit with this feature?**

Anyone who wants to add control inputs to their graph. 

**Any Other info.**
It is possible to add control inputs to operations by directly using OperationBuilder like so:
```
  public <T> Operation addOp(String name, Output<T> x, Output<T> y, Operation z) {
    return graph.opBuilder(""Add"", name)
        .addInput(x).addInput(y).addControlInput(z).build();
  }
```
But definitely would be best to have addControlInput as an option to use with the neat generated classes :) 
 
cc: @samdow @melissagrueter "
26021,SplitPath should split on both forward slashes and backslashes on Windows,"Currently, on Unix, SplitPath splits only on forward slashes; on
Windows, it splits on forward slashes _unless_ there are no forward
slashes in the string, in which case it splits on backslashes. This is
confusing, and inconsistent with platform APIs like [`_wmkdir`], which
interpret both `\` and `/` as valid path delimiters.

For instance, `CreateRecursiveDirectory(r""foo\bar/baz\quux"")` fails on
Windows. This _should_ be broken into `[""foo"", ""bar"", ""baz"", ""quux""]`,
but actually becomes `[""foo"", ""bar"", ""baz\quux""]`. When `CreateDir` is
called on each of those components in sequence, it creates `foo`, then
creates `foo/bar`, then tries to create `foo/bar/baz\quux`, which fails
because the directory `foo/bar/baz` does not exist.

(Googlers, see <http://b/125762969> for an example of this error.)

The whole situation is a bit less clean than one might hope, because the
TF filesystem API prefers to use `/`-separators everywhere while people
in Python-land prefer to use `os.path.join` to insert the proper
platform-specific path separator. But it seems clear that the current
behavior, where the delimiter used depends on what characters are in the
string, is undesirable, and that being consistent with native APIs is a
better choice.

[`_wmkdir`]: https://docs.microsoft.com/en-us/cpp/c-runtime-library/reference/mkdir-wmkdir?view=vs-2017

cc @nfelt
"
26020,MirroredStrategy doesn't use GPUs,"
**System information**

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.12
- TensorFlow version (use command below): 1.12
- Python version: 3.5.2
- CUDA/cuDNN version: CUDA 9 / cuDNN 7.3
- GPU model and memory: 2 x Nvidia Titan X

**Describe the current behavior**


I was working on rewriting a script from the queue/threading approach to tf.data.Dataset approach of providing data. I got really nice throughput of data with over >90% util of GPUs.  Now that I have rewritten it, when starting the training with MirroredStrategy the GPUs are not used at all and I get the following output:

```
INFO:tensorflow:Device is available but not used by distribute strategy: /device:CPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_GPU:0
INFO:tensorflow:Device is available but not used by distribute strategy: /device:XLA_CPU:0
INFO:tensorflow:Configured nccl all-reduce.
INFO:tensorflow:Calling model_fn.
...
```

At this point I am thinking there is some issue with TF1.12.

**Code to reproduce the issue**

Here is basically the structure of my code. I tried out different things like train directly with tf.keras.fit with multi_gpu_model() but it didn't work out. Basically I am trying to reproduce the functionality of my RandomShuffleQueue() I had before with multiple threads filling up the queue. 



```
model = models.Model(inputs=input, outputs=y_output)
optimizer = tf.train.AdamOptimizer(LEARNING_RATE)
# model = utils.multi_gpu_model(model, gpus=NUM_GPUS, cpu_relocation=True)
model.compile(loss=lossFunc, optimizer=optimizer)

def generator(n):
    while True:
        try:
            imgBatch = []
            
            ...
            
            yield imgBatch
                
        except ValueError:
            pass
        
def get_generator(n):
    return partial(generator, n)

def dataset(n):
    return tf.data.Dataset.from_generator(get_generator(n), output_types=[(tf.float32, tf.float32)], output_shapes=[(tf.TensorShape([None,None,1]),tf.TensorShape([None,None,1]))

def input_fn():
    ds = tf.data.Dataset.range(len(dataSets)).apply(tf.data.experimental.parallel_interleave(dataset, cycle_length=len(dataSets), sloppy=True))
    ds = ds.map(map_func=lambda imgBatch: processImage(img,lbl), num_parallel_calls=12)
    ds = ds.shuffle(SHUFFLE_SIZE)
    ds = ds.batch(BATCH_SIZE)
    ds = ds.prefetch(1)
    return ds
                                                                                                                    
strategy = tf.contrib.distribute.MirroredStrategy(num_gpus=NUM_GPUS)
config = tf.estimator.RunConfig(train_distribute=strategy)
estimator = tf.keras.estimator.model_to_estimator(model,
                                                  config=config)
                                                                                                                    
estimator.train(lambda:input_fn())
```

Any help would be greatly appreciated since I'm stuck on it since a while now.

"
26015,RaggedTile op results in XlaCompile error when calculating gradients,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 10.13.6
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): Both 1.13.0rc2 and tf-nightly
- Python version: 3.6.8
- CUDA/cuDNN version: no GPU
- GPU model and memory: no GPU

**Describe the current behavior**
Gradient operations on ragged arrays requiring an implicit RaggedTile op result in
tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'product/RaggedTile/Tile' has no attr named '_XlaCompile'. 

**Describe the expected behavior**
Gradients can be determined without error.

**Code to reproduce the issue**

```
import tensorflow as tf
import numpy as np

np_arr = np.array([[1.,2.,3.], [4.,5.,6.], [7.,8.,9.]])
myvar = tf.get_variable('myvar', initializer=np_arr)
ragged = tf.ragged.constant([[0,1,2],[1,2],[1]])
with tf.name_scope('gather'):
    ragged_gather = tf.gather(myvar, ragged)
with tf.name_scope('product'):
    product = tf.math.multiply(ragged_gather, ragged_gather, name='prod')
with tf.name_scope('output'):
    output = tf.reduce_sum(product**2, name='output')

sess = tf.Session()
sess.run(tf.global_variables_initializer())

print(sess.run(ragged_gather))
# <tf.RaggedTensorValue [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], [[4.0, 5.0, 6.0], [7.0, 8.0, 9.0]], [[4.0, 5.0, 6.0]]]>
print(sess.run(output))
# 32745.0

grads = tf.gradients(output, myvar)
# tensorflow.python.framework.errors_impl.InvalidArgumentError: Operation 'product/RaggedTile/Tile' has no attr named '_XlaCompile'.

```
[full_traceback.txt](https://github.com/tensorflow/tensorflow/files/2895038/full_traceback.txt)"
26014,TensorFlow devel containers are still built on Ubuntu:16.04 as opposed to Ubuntu:18.04,"I just downloaded the most recent `devel` containers from `DockerHub` and they seem to have been updated just a few hours ago and when I check the OS versions they read: `Ubuntu:16.04`.
I was under impression that those are build from `18.04` and using this [Dockerfiles](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/docker/Dockerfile.devel) however I have:
```
$ docker run -it tensorflow/tensorflow:devel bash
Unable to find image 'tensorflow/tensorflow:devel' locally
devel: Pulling from tensorflow/tensorflow
...
Digest: sha256:05acb8910fcbdb7b8d7bc6c51c25d767ca5418c9cd9718fe8e6a9975af00c31e
Status: Downloaded newer image for tensorflow/tensorflow:devel
...
root@9222294a20e2:/# cat /etc/*release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.5 LTS""
NAME=""Ubuntu""
VERSION=""16.04.5 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.5 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
and
```
$ docker run -it tensorflow/tensorflow:devel-py3 bash
...
root@21572c4dc3fe:/# cat /etc/release
cat: /etc/release: No such file or directory
root@21572c4dc3fe:/# cat /etc/*release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=16.04
DISTRIB_CODENAME=xenial
DISTRIB_DESCRIPTION=""Ubuntu 16.04.5 LTS""
NAME=""Ubuntu""
VERSION=""16.04.5 LTS (Xenial Xerus)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 16.04.5 LTS""
VERSION_ID=""16.04""
HOME_URL=""http://www.ubuntu.com/""
SUPPORT_URL=""http://help.ubuntu.com/""
BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""
VERSION_CODENAME=xenial
UBUNTU_CODENAME=xenial
```
"
26013,tensorflow c++ crash,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): win 10 64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version:1.12.0
- Python version: N/A
- Installed using virtualenv? pip? conda?:N/A
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version:N/A
- GPU model and memory:N/A

I build tensorflow manually , and write an simple test using c++ api , the crash dump is as follow ,
please help !!!

```
0:000> kn
 # Child-SP          RetAddr           Call Site
00 00000056`4cf0f880 00007ff7`2111d70d tfdemo!_invoke_watson+0x18 [minkernel\crts\ucrt\src\appcrt\misc\invalid_parameter.cpp @ 224] 
01 00000056`4cf0f8b0 00007ff7`2111d77d tfdemo!_invalid_parameter+0xad [minkernel\crts\ucrt\src\appcrt\misc\invalid_parameter.cpp @ 112] 
02 00000056`4cf0f8f0 00007ff7`20d53c19 tfdemo!_invalid_parameter_noinfo_noreturn+0x19 [minkernel\crts\ucrt\src\appcrt\misc\invalid_parameter.cpp @ 126] 
03 00000056`4cf0f930 00007ff7`20d4cf58 tfdemo!std::_Adjust_manually_vector_aligned+0x79
04 00000056`4cf0f980 00007ff7`20d58222 tfdemo!std::_Deallocate<16,0>+0x28
05 00000056`4cf0f9b0 00007ff7`20d57487 tfdemo!std::allocator<char>::deallocate+0x22
06 00000056`4cf0f9e0 00007ff7`20d506bc tfdemo!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::_Tidy_deallocate+0x87
07 00000056`4cf0fa30 00007ff7`20d5c39c tfdemo!std::basic_string<char,std::char_traits<char>,std::allocator<char> >::~basic_string<char,std::char_traits<char>,std::allocator<char> >+0x1c
08 00000056`4cf0fa70 00007ff7`20d5bc8f tfdemo!tensorflow::NodeBuilder::~NodeBuilder+0x1c
09 00000056`4cf0faa0 00007ff7`20d5ad37 tfdemo!tensorflow::ops::NoOp::NoOp+0x23f
0a 00000056`4cf0fd30 00007ff7`20d5ad89 tfdemo!demo2+0x27
0b 00000056`4cf0fd90 00007ff7`210d4d4c tfdemo!main+0x9
0c (Inline Function) --------`-------- tfdemo!invoke_main+0x22 [f:\dd\vctools\crt\vcstartup\src\startup\exe_common.inl @ 78] 
0d 00000056`4cf0fdc0 00007ffa`94543034 tfdemo!__scrt_common_main_seh+0x10c [f:\dd\vctools\crt\vcstartup\src\startup\exe_common.inl @ 283] 
0e 00000056`4cf0fe00 00007ffa`948f3691 KERNEL32!BaseThreadInitThunk+0x14
0f 00000056`4cf0fe30 00000000`00000000 ntdll!RtlUserThreadStart+0x21
```

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
26012,[TF 2.0] tf.keras.losses duplication?,"**Windows 10
TF Version: b'v1.11.0-rc2-4-gc19e29306c' 1.11.0
Anaconda Python 3.6.5
GPU: GeForce GTX 1070 Max-Q Design
Tensorflow 2.0 (gpu) preview installed via pip.**

I'm building a [reinforcement learning framework](https://github.com/danaugrs/huskarl) on top of TensorFlow 2.0 using the `tf.keras` API and I've come across the following issue.

The 2.0 API docs for [`tf.keras.losses`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses) shows many objects that are not actually available in the preview package. For example the loss classes such as Huber. Hinge, etc... are not accessible.

1. Why are those classes not included in the preview package?
2. Why are there both classes and functions for many of the same loss types? That seems like unnecessary duplication.
2a. Why is there a `Huber` class but no `huber` function?
3. I'd love to contribute PRs and help fix these issues. Would that be desired?

Edit: This has also been noticed here: https://github.com/tensorflow/tensorflow/issues/26007"
26011,tf.data.experimental.bucket_by_sequence_length,"I wanted to read speech data from tfrecord and use tf.data.experimental.bucket_by_sequence_length to bucketing data by seq_len, but i have same error when use it. here is my code:
`


    fea_conf = dict([
    ('SOS_ID', 2),
    ('EOS_ID', 3),
    ('BATCH_SIZE', 2),
    ('FRAME_SIZE', 80),
    ('GRAPHEME_TARGET_SEQUENCE_LENGTH', 620),])

     def read_TFRecord(file_pattern, symbol):
         """"""read tfrecord.""""""
          file_pattern = os.path.join(file_pattern, '%s.tfrecords-*'%symbol)
          def ParseAndProcess(record):
             """"""Parses a serialized tf.Example record.""""""
             features = [
                 ('uttid', tf.VarLenFeature(tf.string)),
                 ('label', tf.VarLenFeature(tf.int64)),
                 ('frames', tf.VarLenFeature(tf.float32)),
              ]
              example = tf.parse_single_example(record, dict(features))
              fval = {k: v.values for k, v in six.iteritems(example)}
              fval['frames'] = tf.reshape(
                  fval['frames'], shape=[-1, fea_conf['FRAME_SIZE']])
              src_paddings = tf.ones([tf.shape(fval['frames'])[0]], dtype=tf.float32)

              fval['label'] = tf.reshape(fval['label'], shape=[1, -1])
              tgt_labels = tf.concat(
                  [tf.cast(fval['label'], tf.int32),
              tf.fill([1,fea_conf['GRAPHEME_TARGET_SEQUENCE_LENGTH']-tf.shape(fval['label'])[-1]], 
                  fea_conf['EOS_ID'])],
                  axis=1
              )
              tgt_ids = tf.concat(
                  [tf.fill([1,1], fea_conf['SOS_ID']),
              tf.slice(tgt_labels, [0,0], [1, fea_conf['GRAPHEME_TARGET_SEQUENCE_LENGTH']-1])],
                  axis=1
              )
              tgt_paddings = tf.concat(
                  [tf.zeros([1, tf.shape(fval['label'])[-1]+1], dtype=tf.float32),
              tf.ones([1, fea_conf['GRAPHEME_TARGET_SEQUENCE_LENGTH']-tf.shape(fval['label'])[-1]-1], 
                  dtype=tf.float32)],
                  axis=1
              )
              return (fval['uttid'], tgt_ids, tgt_labels, tgt_paddings, fval['frames'], src_paddings)
          def element_length_fn(example): #(uttids, tgt_ids, tgt_labels, tgt_paddings, frames, src_paddings):
              print(type(example))
              batch_size = tf.shape(example)[0]
              return feature_length

           dataset_factory = tf.data.TFRecordDataset
           dataset = (
                  tf.data.Dataset.list_files(
                        file_pattern.lstrip('tfrecord:'), shuffle=True).apply(
                             tf.data.experimental.parallel_interleave(
                                  dataset_factory,
                                  cycle_length=1,
                                  sloppy=True)))
            dataset = dataset.apply(
                  tf.data.experimental.bucket_by_sequence_length(
                        element_length_func=element_length_fn,
                               bucket_batch_sizes= [256, 128, 64],
                               bucket_boundaries=[100, 300],
                               #padded_shapes=([None], [1, None], [1, None], [1, None], [None, 80], [None]),
           ))
           #dataset = dataset.shuffle(500)
           dataset = dataset.map(
                  ParseAndProcess, num_parallel_calls=1)
           #dataset = dataset.padded_batch(
           #   fea_conf['BATCH_SIZE'], padded_shapes=([None], [1, None], [1, None], [1, None], [None, 80], [None]))
           dataset = dataset.repeat()
           iterator = dataset.make_one_shot_iterator()
           input_batch = iterator.get_next()

           return input_batch[0], input_batch[1], input_batch[2], input_batch[3], input_batch[4], input_batch[5]
`
when i run my code, the error is occur:   ValueError: slice index 0 of dimension 0 out of bounds. for 'strided_slice' (op: 'StridedSlice') with input shapes: [0], 1, 1, 1 and with computed input tensors: input1 = <0>, input[2] = <1>, input[3] = <1>, that i don't konw how to use tf.data.experimental.bucket_by_sequence_length with tfrecordes to buckets.  i just know that code[https://stackoverflow.com/questions/50606178/tensorflow-tf-data-dataset-and-bucketing](url)
can anybody tell me how to use it or give me a sample to handle variable length data. 
"
26007,tensorflow.python.keras.api._v2.keras.losses' has no attribute 'SparseCategoricalCrossentropy'   ,"it throw the error when I run autograph.ipynb, error message is following:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-36-2c50f534479f> in <module>
----> 1 compute_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
      2 
      3 compute_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()
      4 
      5 

AttributeError: module 'tensorflow.python.keras.api._v2.keras.losses' has no attribute 'SparseCategoricalCrossentropy'"
26006,Issue in model for parts of speech tagger.,"**System information**
- TensorFlow version (you are using):1.12
- Are you willing to contribute it (Yes/No):No



**Describe the feature and the current behavior/state.**
I built a model which can predict parts of speech of words when given separately. Now I am building a model which can predict parts of speech of words in sentences. For this i have created two arrays. 1st Array - which has sentences tokenized as arrays[14,546,789,4556](words as sequences, There are many arrays like this representing sentences), 2nd Array- Arrays of parts of speech of each word in[14,7,3,21](respective parts of speech as sequences), .Both words and parts and speech are converted to sequences

**CODE STARTS**
model  = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(input_dim=11040,output_dim=64, input_length = 30, batch_input_shape=[128,None]))
model.add(tf.keras.layers.LSTM(30, activation='tanh', recurrent_activation='hard_sigmoid', kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',stateful = True))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(30))
model.summary()
**CODE ENDS**

**PROBLEM**
It is returning a single array where each element in the predicted array represents a whole sentences where it should give array of parts of speech for each sentence in the input. Please tell me how to change the model and why. I am a beginner trying to learn machine learning.

**Will this change the current api? How?**
This wont change the API.

**Who will benefit with this feature?**
Beginners who are starting on textual data
"
26005, Model transfer Tensorflow to Keras,"
I want to transfer the model from Tensorflow to Keras. But I do not see this function"
26003,Issue in running the models of tensorflow,"Hi,I want to know how to run various models like mobile net, image net, resnet  .Is there any particular set of steps to run these models or any official tensorflow tutorial to follow.Kindly guide me for the same.
Right Now i am following these steps:
1. sudo apt install python3-dev python3-pip
2. sudo pip3 install -U virtualenv
3. virtualenv --system-site-packages -p python3 ./venv
4. source ./venv/bin/activate
5. pip install --upgrade pip
6. pip list
7. pip install --upgrade tensorflow
8. pip install -U --user pip six numpy wheel mock
9. pip install -U --user keras_applications==1.0.6 --no-deps
10. pip install -U --user keras_preprocessing==1.0.5 --no-deps
11. git clone https://github.com/tensorflow/tensorflow.git
12. git clone https://github.com/tensorflow/models.git
12. cd tensorflow

(Python version-3.5.2
virtual env version-16.04
pip version-19.0.3)

Please Help!

****"
26002,Install Tensorflow GPU with CUDA 10.0 on windows,"Hi guys. I am new to the tensorflow. I would like to install Tensorflow-Gpu with my RTX2080ti graphic card. I followed this instruction 
https://www.pytorials.com/how-to-install-tensorflow-gpu-with-cuda-10-0-for-python-on-windows/

However, when I do the Step 12. The error shows up.

bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package

Because RTX 2080 ti only support Cuda 10.0, it is very difficult for me to install tensorflow. Anyone can help me with this error or provide another useful instruction? Thanks a lot!!

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from: source
- TensorFlow version: 1.12
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 0.17
- CUDA/cuDNN version: cuda 10.0 cuDNN 7.4.2
- GPU model and memory: Nvidia RTX 2080ti



**The Error**
p_packageuild --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
WARNING: Option 'experimental_shortened_obj_file_path' is deprecated
Loading:
Loading: 0 packages loaded
Loading: 0 packages loaded
    currently loading: tensorflow/tools/pip_package
DEBUG: C:/users/shica/_bazel_shica/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: 'BAZEL_VC' is not set, start looking for the latest Visual C++ installed.
DEBUG: C:/users/shica/_bazel_shica/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Looking for VS%VERSION%COMNTOOLS environment variables, eg. VS140COMNTOOLS
DEBUG: C:/users/shica/_bazel_shica/wvk7snnt/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5:
Auto-Configuration Warning: Visual C++ build tools found at C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1447
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1303, in _create_local_cuda_repository
                find_cc(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 217, in find_cc
                _get_msvc_compiler(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 153, in _get_msvc_compiler
                find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")
type 'NoneType' has no method replace(string, string)
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package '@local_config_cuda//cuda': Traceback (most recent call last):
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1447
                _create_local_cuda_repository(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1303, in _create_local_cuda_repository
                find_cc(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 217, in find_cc
                _get_msvc_compiler(repository_ctx)
        File ""C:/tensorflow/tensorflow/third_party/gpus/cuda_configure.bzl"", line 153, in _get_msvc_compiler
                find_msvc_tool(repository_ctx, vc_path, ""cl.exe"").replace(""\\"", ""/"")
type 'NoneType' has no method replace(string, string)
INFO: Elapsed time: 1.561s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
FAILED: Build did NOT complete successfully (0 packages loaded)
"
26001,MKL build failing for mkl_cpu_allocator.h header in file threadpool_device.cc,"**Problem**
TensorFlow with MKL isn't building successfully.

**Error Log**
```bash
[0m[91mERROR: /opt/tensorflow/tensorflow/core/BUILD:3034:1: C++ compilation of rule '//tensorflow/core:core_cpu_impl' failed (Exit 1)
In file included from tensorflow/core/common_runtime/threadpool_device.cc:38:
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:85:8: error: 'void tensorflow::MklSmallSizeAllocator::GetStats(tensorflow::AllocatorStats*)' marked 'override', but does not override
   void GetStats(AllocatorStats* stats) override {
        ^~~~~~~~
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'virtual void tensorflow::MklSmallSizeAllocator::ClearStats()':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:92:12: error: 'struct tensorflow::AllocatorStats' has no member named 'Clear'
     stats_.Clear();
            ^~~~~
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'void tensorflow::MklSmallSizeAllocator::IncrementStats(size_t)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:101:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?
     stats_.max_bytes_in_use =
            ^~~~~~~~~~~~~~~~
            peak_bytes_in_use
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:102:25: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?
         std::max(stats_.max_bytes_in_use, stats_.bytes_in_use);
                         ^~~~~~~~~~~~~~~~
                         peak_bytes_in_use
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:103:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?
     stats_.max_alloc_size =
            ^~~~~~~~~~~~~~
            largest_alloc_size
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:104:57: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?
         std::max(alloc_size, static_cast<size_t>(stats_.max_alloc_size));
                                                         ^~~~~~~~~~~~~~
                                                         largest_alloc_size
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: At global scope:
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:247:8: error: 'void tensorflow::MklCPUAllocator::GetStats(tensorflow::AllocatorStats*)' marked 'override', but does not override
   void GetStats(AllocatorStats* stats) override {
        ^~~~~~~~
./tensorflow/core/common_runtime/mkl_cpu_allocator.h: In member function 'void tensorflow::MklCPUAllocator::GetStats(tensorflow::AllocatorStats*)':
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:250:45: error: no matching function for call to 'tensorflow::Allocator::GetStats(tensorflow::AllocatorStats*)'
     large_size_allocator_->GetStats(&l_stats);
                                             ^
In file included from ./tensorflow/core/common_runtime/device.h:35,
                 from ./tensorflow/core/common_runtime/local_device.h:19,
                 from ./tensorflow/core/common_runtime/threadpool_device.h:20,
                 from tensorflow/core/common_runtime/threadpool_device.cc:16:
./tensorflow/core/framework/allocator.h:207:42: note: candidate: 'virtual absl::optional<tensorflow::AllocatorStats> tensorflow::Allocator::GetStats()'
   virtual absl::optional<AllocatorStats> GetStats() { return absl::nullopt; }
                                          ^~~~~~~~
./tensorflow/core/framework/allocator.h:207:42: note:   candidate expects 0 arguments, 1 provided
In file included from tensorflow/core/common_runtime/threadpool_device.cc:38:
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:255:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?
     stats->max_bytes_in_use =
            ^~~~~~~~~~~~~~~~
            peak_bytes_in_use
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:256:17: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?
         l_stats.max_bytes_in_use + s_stats.max_bytes_in_use;
                 ^~~~~~~~~~~~~~~~
                 peak_bytes_in_use
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:256:44: error: 'struct tensorflow::AllocatorStats' has no member named 'max_bytes_in_use'; did you mean 'peak_bytes_in_use'?
         l_stats.max_bytes_in_use + s_stats.max_bytes_in_use;
                                            ^~~~~~~~~~~~~~~~
                                            peak_bytes_in_use
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:261:12: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?
     stats->max_alloc_size = l_stats.max_alloc_size;
            ^~~~~~~~~~~~~~
            largest_alloc_size
./tensorflow/core/common_runtime/mkl_cpu_allocator.h:261:37: error: 'struct tensorflow::AllocatorStats' has no member named 'max_alloc_size'; did you mean 'largest_alloc_size'?
     stats->max_alloc_size = l_stats.max_alloc_size;
                                     ^~~~~~~~~~~~~~
                                     largest_alloc_size
[0m[91mTarget //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
```

**Build call**
```bash
   wget https://github.com/bazelbuild/bazel/releases/download/0.19.0/bazel_0.19.0-linux-x86_64.deb && \
    sudo dpkg -i bazel_0.19.0-linux-x86_64.deb && \
    sudo apt-get install -f && \
    rm bazel_0.19.0-linux-x86_64.deb && \
    pip uninstall -y tensorflow-tensorboard tfp-nightly tensorflow_estimator tb-nightly tf-nightly tensorflow && \
    cd /opt && \
    git clone --recursive https://github.com/tensorflow/tensorflow.git && \
    cd /opt/tensorflow && \
    # git checkout 0f22d6431e0d97eb264fa35253de7bf333ced14c && \
    # sed -i 's/2018\.0\.3\.20180406/2019\.0\.1\.20180928/g' tensorflow/contrib/cmake/external/mkl.cmake && \
    # sed -i 's/v0\.14/v0\.17.1/g' tensorflow/contrib/cmake/external/mkl.cmake && \
    # sed -i 's/3063b2e4c943983f6bf5f2fb9a490d4a998cd291/1687299a987d1217fe80253a3b3bddc6fcf4a487/g' tensorflow/contrib/cmake/external/mkldnn.cmake && \
    ln -s /usr/lib/libmkldnn.so /usr/lib/libiomp5.so /usr/lib/libmklml_gnu.so  /usr/lib/libmklml_intel.so $MKLROOT/lib/intel64_lin && \
    ln -s /usr/include/mkldnn* $MKLROOT/include && \
    ln -s $MKLROOT/lib/intel64_lin/* $MKLROOT/lib && \
    ln -s /usr/lib /usr/lib/lib && \
    echo """" > /usr/local/lib/license.txt && \
    echo """" > /usr/local/include/license.txt && \
    echo ""from tensorflow.contrib import cloud"" >> tensorflow/contrib/__init__.py && \
    echo ""from tensorflow.contrib import *"" >> tensorflow/contrib/__init__.py && \
    TF_MKL_ROOT=/usr/lib  \
    TF_MKL_DOWNLOAD=0 \
    USE_DEFAULT_PYTHON_LIB_PATH=1 \
    TF_NEED_MKL=1 \
    TF_NEED_JEMALLOC=1 \
    TF_NEED_GCP=0 \
    TF_NEED_HDFS=0 \
    TF_ENABLE_XLA=1 \
    TF_NEED_MPI=0 \
    TF_NEED_GDR=0 \
    TF_NEED_S3=1 \
    TF_NEED_KAFKA=0 \
    TF_SET_ANDROID_WORKSPACE=0 \
    TF_NEED_CUDA=0 \
    TF_MKL_ENABLED=""true"" \
    CI_BUILD_PYTHON=/opt/conda/bin/python \
    PYTHON_BIN_PATH=/opt/conda/bin/python \
    PYTHON_LIB_PATH=/opt/conda/lib/python3.6/site-packages \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    CC_OPT_FLAGS="" -mavx -msse2 -msse3 -msse4.2 -msse4.1 -mfpmath=sse -lmkl_gf_lp64 -Wl,--start-group -lmkldnn -lmklml_intel -lmkl_gnu_thread -lmkl_core -Wl,--end-group -dl -lpthread -lm "" \
    /bin/bash ./configure && \
    bazel build \
    --config=mkl --config=opt \
    --config=verbs \
    --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
    --copt=-msse4.2 --copt=-msse4.1 --copt=-mavx --copt=-msse2 --copt=-msse3  \
    --copt=-O3 --copt=-mfpmath=both \
    --copt=""-DMKL_LP64"" \
    --copt=""-fPIC"" \
    --linkopt=""-lmkl_gf_lp64"" \
    --linkopt=""-lmkl_gnu_thread"" \
    --linkopt=""-dl"" \
    --linkopt=""-ldl"" \
    --linkopt=""-lpthread"" \
    --linkopt=""-lmkl_core"" \
    --linkopt=""-lm"" \
    --linkopt=""-lmkl_rt"" \
    --linkopt=""-lmkldnn"" \
    tensorflow/tools/pip_package:build_pip_package && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/pip && \
    pip install --no-deps /tmp/pip/tensorflow-*.whl && \
    cd /opt && rm -rf /opt/tensorflow /tmp/* && \
    python -c ""import tensorflow as tf; hello = tf.constant('Hello, TensorFlow!'); sess = tf.Session(); print(sess.run(hello))"" && \
    python -c ""import tensorboard""
```

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian: Stretch
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Source
- TensorFlow version: Master branch
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: Source build
- Bazel version (if compiling from source): 0.19.0
- GCC/Compiler version (if compiling from source): 8.2.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



"
25998,The difference and error in tensorflow-1.8.0,"
When I use tensorflow-1.8.0, the code is different and wrong. It is shape[axis] = -1."
25997,Tflite convert with Bazel 0.22.0 error: Unable to load package for '//tensorflow/tools/def_file_filter:def_file_filter_configure.bzl',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): None
- CUDA/cuDNN version: None
- GPU model and memory: None
**Describe the problem**
Please help me with this error. I searched on the Internet but no one ever met. I run bazel to convert tf model to tflite model according to this tutorial [Running on mobile with TensorFlow Lite](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/running_on_mobile_tensorflowlite.md)
ERROR: error loading package '': in F:/card-details-recognition-android-version2/tensorflow/tensorflow/workspace.bzl: Unable to load package for '//tensorflow/tools/def_file_filter:def_file_filter_configure.bzl': BUILD file not found on package path
**Provide the exact sequence of commands / steps that you executed before running into the problem**
bazel run -c opt tensorflow/lite/toco:toco -- --input_file=F:/ssd_mobilenet_v1_coco_2018_01_28/lite/tflite_graph.pb --output_file=F:/ssd_mobilenet_v1_coco_2018_01_28/lite/detect.tflite --input_shapes=1,300,300,3 --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' --inference_type=QUANTIZED_UINT8 --mean_values=128 --std_values=128 --change_concat_input_ranges=false --allow_custom_ops
"
25996,Excess amount of deprecated warning in tf1.13,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.6.1810
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.13.0rc2
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 10/7.3.1
- GPU model and memory: Titan Xp

**Describe the current behavior**
 Even a script as simple as below have thrown 3 deprecated warnings
```python
import tensorflow as tf
import numpy as np

input = tf.keras.layers.Input(shape=(20,))
output = tf.keras.layers.Dense(2)(input)
model = tf.keras.models.Model(inputs=input, outputs=output)
model.compile(loss='mse', optimizer='sgd')

model.fit(np.random.normal(0, 1, (200, 20)), np.random.normal(0, 1, (200, 2)))
```

```
WARNING:tensorflow:From /home/henrysky/.local/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
```

```
WARNING:tensorflow:From /home/henrysky/.local/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
```

```
WARNING:tensorflow:From /home/henrysky/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
```

**Describe the expected behavior**
Little to no deprecated warning. I mean it is deprecated warning should be shown to user when user write their own code using low level tensorflow function. But the code as simple as above using high level function thrown 3 warnings??

**Code to reproduce the issue**
see above

**Other info / logs**
see above
"
25994,ImportError: cannot import name cloud,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No, using stock tensorflow 
- Python version: 2.7
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): gcc version 8.2.0 (Debian 8.2.0-20)
- CUDA/cuDNN version: No
- GPU model and memory: No

== cat /etc/issue ===============================================
Linux rockpro64 4.4.132-1075-rockchip-ayufan-ga83beded8524 #1 SMP Thu Jul 26 08:22:22 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux
VERSION=""18.04.1 LTS (Bionic Beaver)""
VERSION_ID=""18.04""
VERSION_CODENAME=bionic

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 8.2.0-1ubuntu2~18.04) 8.2.0
Copyright (C) 2018 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

== uname -a ImportError: cannot import name cloud=====================================================
Linux rockpro64 4.4.132-1075-rockchip-ayufan-ga83beded8524 #1 SMP Thu Jul 26 08:22:22 UTC 2018 aarch64 aarch64 aarch64 GNU/Linux

== check pips ===================================================
numpy                         1.16.1   
protobuf                      3.6.1    
tensorflow                    1.12.0   
tensorflow-aarch64            1.2      
tensorflow-estimator          1.13.0rc0

== check for virtualenv =========================================
False

== tensorflow import ============================================
Limited tf.compat.v2.summary API due to missing TensorBoard installation
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-8122-g7328add9da
tf.COMPILER_VERSION = v1.12.0-8122-g7328add9da
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================


rock64@rockpro64:~/Documents$ python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
**('v1.12.0-8122-g7328add9da', '1.12.0')**


following is the error. NO module name cloud.

  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/lazy_loader.py"", line 44, in _load
    module = importlib.import_module(self.__name__)
  File ""/usr/lib/python2.7/importlib/__init__.py"", line 37, in import_module
    __import__(name)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/__init__.py"", line 31, in <module>
    from tensorflow.contrib import cloud
**ImportError: cannot import name cloud**
"
25990,run_metadata should be preserved when session run fails,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: Titan Xp 12GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
run_metadata is empty when a session run fails.

**Describe the expected behavior**
run_metadata should be saved even (especially?) when session run fails.

**Code to reproduce the issue**
```
with tf.device(""/GPU:0""):
    input = tf.random_uniform(dtype=tf.float32, shape=[5, 100, 100, 3], name=""input"")
    resize = tf.image.resize_bilinear(input, [99999, 99999], align_corners=True)  # OOM
run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)
run_metadata = tf.RunMetadata()
with tf.Session() as sess:
    try:
        sess.run(resize, options=run_options, run_metadata=run_metadata)
    finally:
        run_metadata_str = str(run_metadata)
        if run_metadata_str:
            print(run_metadata_str)
        else:
            print(""No run_metadata"")

```
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25988,[TF 2.0 API Docs] tf.keras.applications.MobileNetV2,"**Existing URLs containing the issue:**
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications/MobileNetV2

**Description of issue (what needs changing):**
- __Correct Links__:
https://github.com/tensorflow/tensorflow/blob/master/python/keras/applications/__init__.py is incorrect
The correct link should be:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/applications/__init__.py

* __Clear Description__:
The description does not describe what this symbol does or when it should be 

* __Usage Example__:
No usage example is provided.

* __Parameters Defined__:
Parameters are not defined.

* __Returns Defined__:
Returns are not defined.

* __Raises Listed and Defined__:
Errors are not listed or defined.

* __Visuals, if Applicable__:
No visuals are included.
"
25985,reset_states() failure in a stateful network with initial_states set and training in batch - TypeError: 'NoneType' object is not subscriptable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.0.0-dev20190217
- Python version: 3.5.2
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 10
- GPU model and memory: GTX 1080 8G

**Describe the current behavior**

As @manojrege said from https://github.com/keras-team/keras/issues/11148, when we use `initial_states` with RNN in some case, we will get an exception:

```
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/local/Cellar/python/3.6.5_1/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/Users/manoj/Desktop/repos/yane/yane/LSTM/manytomanyLSTM.py"", line 137, in <module>
    incremental_train(space)
  File ""/Users/manoj/Desktop/repos/yane/yane/LSTM/manytomanyLSTM.py"", line 128, in incremental_train
    model.reset_states()
  File ""/usr/local/lib/python3.6/site-packages/keras/engine/topology.py"", line 1968, in reset_states
    layer.reset_states()
  File ""/usr/local/lib/python3.6/site-packages/keras/layers/recurrent.py"", line 681, in reset_states
    batch_size = self.input_spec[0].shape[0]        
TypeError: 'NoneType' object is not subscriptable
```

There's another issue talking about this problem at #25852

**Describe the expected behavior**

Should not throw exception.

**Code to reproduce the issue**

```py
import tensorflow as tf

# import pdb; pdb.set_trace()
inputs = tf.keras.layers.Input(batch_shape=(1, 1, 1))

state_h = tf.keras.layers.Input(batch_shape=(1, 1))
state_c = tf.keras.layers.Input(batch_shape=(1, 1))

states = [state_h, state_c]

decoder_out = tf.keras.layers.LSTM(1, stateful=True)(
    inputs,
    initial_state=states
)

model = tf.keras.Model([inputs, state_h, state_c], decoder_out)
model.reset_states()
```

**Other info / logs**

I can confirm that the Pull Request https://github.com/keras-team/keras/pull/11149/files can fix this problem.
"
25984,Tensorflow GPU installation issues,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Through PyCharm virtualenv
- TensorFlow version: 1.12.0
- Python version: 3.6.7
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 10 / cuDNN 7.4.2
- GPU model and memory: GeForce GTX 1050 Ti

Trying to install and run tensorflow gpu version. I have installed CUDA and cuDNN and run the deviceQuery sample with what seems like good results (picture attached). I have installed tensorflow and Keras through PyCharm and when I try to run the following two lines to see if TF is installed correctly I get the following error message:

import tensorflow as tf
sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))

Traceback (most recent call last):
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mr.Jones\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mr.Jones\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Mr.Jones/Documents/Master Thesis/Offline_models/define_models_Jonas.py"", line 1, in <module>
    import tensorflow as tf
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Mr.Jones\Documents\Master Thesis\Offline_models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Mr.Jones\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Mr.Jones\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.

![image](https://user-images.githubusercontent.com/17160539/53198308-626ff780-361c-11e9-80bb-5490b771776c.png)

"
25983,Compiling model with tf.keras.optimizers.SGD optimiser in eager execution mode throws an exception,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): 
- TensorFlow version: 1.12.0
- Python version: 3.6.8

**Describe the current behavior**
When trying to build a simple model in eager execution mode using SGD as an optimiser the following exception is thrown:
```ValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.SGD'>```

**Describe the expected behavior**
I'd expect the SGD optimiser to be usable in eager execution mode.

**Code to reproduce the issue**
```
import tensorflow as tf
tf.enable_eager_execution()

model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(50, input_shape=(10, )))
model.add(tf.keras.layers.Activation('relu'))
model.add(tf.keras.layers.Dense(3))
model.add(tf.keras.layers.Activation('sigmoid'))

sgd = tf.keras.optimizers.SGD(lr=0.1, decay=0.000225, momentum=0.5)

model.compile(optimizer=sgd,
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```

**Other info / logs**---------------------------------------------------------------------------
```
ValueError                                Traceback (most recent call last)
<ipython-input-1-3ec239d3547d> in <module>
     12 model.compile(optimizer=sgd,
     13               loss='categorical_crossentropy',
---> 14               metrics=['accuracy'])

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\training\checkpointable\base.py in _method_wrapper(self, *args, **kwargs)
    472     self._setattr_tracking = False  # pylint: disable=protected-access
    473     try:
--> 474       method(self, *args, **kwargs)
    475     finally:
    476       self._setattr_tracking = previous_value  # pylint: disable=protected-access

C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py in compile(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)
    408       raise ValueError(
    409           'optimizer must be an instance of tf.train.Optimizer, not '
--> 410           'a %s' % type(optimizer))
    411 
    412     self.optimizer = optimizers.get(optimizer)

ValueError: optimizer must be an instance of tf.train.Optimizer, not a <class 'tensorflow.python.keras.optimizers.SGD'>
```
"
25981,Stateful version of contrib.summary.always_record_summaries,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.0-dev20190219'
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
When using summaries in the V2 system, summaries are meant to be written in a context manager created by `summary.always_record_summaries()` or `record_summaries_every_n_global_steps`:

```python
from tensorflow.contrib import summary
...
with summary.always_record_summaries():
    summary.scalar(...)
```

I think it would be useful to have a non-context version of these functions as well. Something like

```python
summary.do_always_record_summaries(True)
summary.scalar(...)  # This will write to the event file
```

This is analogous to how summary writers have both a `as_default()` method for creating a context manager and a `set_as_default` for changing global state.

**Will this change the current api? How?**

The`contrib.summary` module (which I believe is slated to become the main `summary` module in 2.0) will have new methods added.

**Who will benefit with this feature?**

People who are interactively using TensorFlow (especially in eager mode) from a REPL or notebook for small-scale experiments or learning TensorFlow. It's (IMO) useful to write summaries in a quick one-off fashion for immediate visualization in tensorboard. It's tedious on the REPL to make sure all those  calls are wrapped in a wordy and easy-to-forget `with summary.always_record_summaries(): ...`



**Any Other info.**
"
25980,Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor,"Hi,
I am using python 3.6.8 and the last keras and tensorflow (1.12.0)version.
When I build something like that, it works:

```
x=keras.layers.Input(shape=(100,),sparse=False)
x1=keras.layers.Dropout(0.2)(x)
```

But the following code would give an error:
```
x=keras.layers.Input(shape=(100,),sparse=True)
x1=keras.layers.Dropout(0.2)(x)
```


The detailed error is 
_TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""input_30/indices:0"", shape=(?, 2), dtype=int64), values=Tensor(""dropout_28/cond/mul:0"", shape=(?,), dtype=float32), dense_shape=Tensor(""input_30/shape:0"", shape=(2,), dtype=int64)). Consider casting elements to a supported type._

I think it is a bug but I am not sure.

Thanks a lot.

Regards.
"
25978,tf.keras.layers.conv3Dtranspose error when input size is None,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04 / Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0 / 1.10.0  /  1.9.0
- Python version: Python 3.6.7 
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 8.0
- GPU model and memory: Tesla M40 24GB

**Describe the current behavior**

Conv3DTranspose layer is producing an error message when the input layer dimension size is given None.

**Describe the expected behavior**

Like other layers, Conv3DTranspose layer should automatically infer the dimension size as well.

**Code to reproduce the issue**

Colab link to produce an error.
[https://colab.research.google.com/drive/1ee0LncVZIr8U4mQvmTW9R14Fpv31syO4](https://colab.research.google.com/drive/1ee0LncVZIr8U4mQvmTW9R14Fpv31syO4)

```

import tensorflow as tf

# Won't produce an error if we give explicitly the dimension size instead of None.
x2 = tf.keras.layers.Input(shape=(None,None,None,3), name='image_input')

x=tf.keras.layers.Conv3D(filters=64,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv1')(x2)
x=tf.keras.layers.Conv3D(filters=64,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv2')(x)

x=tf.keras.layers.Conv3DTranspose(filters=128,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv6')(x)
x=tf.keras.layers.Conv3DTranspose(filters=64,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv8')(x)

mod=tf.keras.models.Model(inputs=x2,outputs=x)
mod.summary()

```
**Output**
```

---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-7-3ec84051b623> in <module>()
      6 x=tf.keras.layers.Conv3D(filters=4,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv2')(x)
      7 
----> 8 x=tf.keras.layers.Conv3DTranspose(filters=4,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv6')(x)
      9 x=tf.keras.layers.Conv3DTranspose(filters=4,kernel_size=(3,3,3),padding=""same"",activation='relu',name='Conv8')(x)
     10 

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    701 
    702       if not in_deferred_mode:
--> 703         outputs = self.call(inputs, *args, **kwargs)
    704         if outputs is None:
    705           raise ValueError('A layer\'s `call` method should return a Tensor '

~/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py in call(self, inputs)
   1060       else:
   1061         outputs_4d = array_ops.reshape(outputs, [
-> 1062             outputs_shape[0], outputs_shape[1] * outputs_shape[2],
   1063             outputs_shape[3], outputs_shape[4]
   1064         ])

TypeError: unsupported operand type(s) for *: 'NoneType' and 'NoneType'


```
"
25977,CUDA_ERROR_ILLEGAL_ADDRESS in SoftmaxCrossEntropyWithLogits when run in FP16,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary (dockerhub: `tensorflow/tensorflow:1.13.0rc2-gpu-py3`)
- TensorFlow version (use command below): `b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2`
- Python version: 3.5.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0 / 7.4.1
- GPU model and memory: Quadro GV100, 32GB

**Describe the current behavior**
The `SoftmaxCrossEntropyWithLogits` op will hit a `CUDA_ERROR_ILLEGAL_ADDRESS` error when executed in fp16 with certain size inputs. 

**Describe the expected behavior**
No segfault -- though it's fine if the results are numerically bad.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf
from tensorflow.python.ops import gen_nn_ops

mb = 4005
vocab = 8193

logits = tf.placeholder(tf.float16, shape=(mb, vocab))
labels = tf.placeholder(tf.int32, shape=(mb,))                        
targets = tf.one_hot(labels, depth=vocab, dtype=tf.float16)

xentropy, _ = gen_nn_ops.softmax_cross_entropy_with_logits(
    logits, targets)

logits_vals = np.random.randn(*logits.shape).astype(np.float16)
labels_vals = np.random.randint(0, vocab, size=labels.shape)

with tf.Session() as sess:
    ret = sess.run(xentropy, {logits: logits_vals,
                              labels: labels_vals})
```

**Other info / logs**
I've attached a log from cuda-memcheck. There is an illegal write in the relevant eigen kernel.
[memcheck-xentropy.log](https://github.com/tensorflow/tensorflow/files/2890344/memcheck-xentropy.log)

Other comments:
- This appears to be a recent regression -- I tried the same code on an older TF build (The nvidia 18.12-py3 container) and there was no error.
- This is a hard bug to hit, since `tf.nn.softmax_cross_entropy_with_logits_v2` will coerce its inputs to fp32.
- Many input sizes run fine; I haven't done thorough testing to establish which ones are OK and which aren't.
- As mentioned above, I don't expect numerically good answers in fp16 -- this came about as a result of perf measurements when coercing fp16 everywhere."
25976,"summary v2 create_file_writer() bug ""SummaryWriterInterface does not exist""","Keeping the description brief as already discussed with @nfelt 

I am getting these errors when logging to tensorboard:
```
NotFoundError: Resource [....] N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: loss/
```
This is a similar outcome to #25707, but it has a completely different cause (see below).

When using this ""set_as_default"" style:
```
# ... setup code
tf.contrib.summary.create_file_writer(""/logdir/path/"", flush_millis=2500).set_as_default()
# ... more setup code
for ibatch in range(iterations):
   # ... training code
   with tf.contrib.summary.record_summaries_every_n_global_steps(FLAGS.tb_every):
      tf.contrib.summary.scalar('loss', loss)
```

The problem does not take place when using this style:
```
# ... setup code
writer = tf.contrib.summary.create_file_writer(""/logdir/path/"", flush_millis=2500)
writer.set_as_default()
# ... more setup code
for ibatch in range(iterations):
   # ... training code
   with tf.contrib.summary.record_summaries_every_n_global_steps(FLAGS.tb_every):
      tf.contrib.summary.scalar('loss', loss)
```

This is very counter-intuitive as the writer was never used after, and therefore collapsing the two lines as I did should have not caused any problem!
 

"
25974,Able to access nodes/namespace from a graph for downloaded models,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
For example, I download a pre-trained model from zoo such as mobilenet. After loading the graph into the session, is there a way to access the higher-level nodes which pertain to layer only. When we access nodes in the graph right now it just gives all the nodes including the ones connected to training and everything. Is there are way to access say only the convolution input/output nodes. 
Currently when you find out the conv weights node and use `node.input` it gives a `mul` node before it. However, i would be looking for the `pooling` layer node or the `activation` node. So is there a way to have a super-view level without the help of tensorboard always. Helps while working on server. Kind of a simple mapping summarizing the model
**Will this change the current api? How?**
yes. the current nodes in the graph_def have attribute `input`. It can be tailored to another attribute say `node.main_input` which gives the next important layer input instead of a low-level op like `mul` or `add`
**Who will benefit with this feature?**
working on remote servers and not able to pull up tensorboard always. Its painful to port the browser always. 
**Any Other info.**
Similar to model.summary() in Keras which gives the overview of the model
"
25973,[TF 2.0 API Docs] tf.keras.losses.poisson,"### Existing URLs containing the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/poisson

### Description of issue (what needs changing):
* **Correct Links**
  All links are correct.
* **Clear Description**
  Lacks a description.
* **Usage Example**
  No usage example is provided.
* **Parameters Defined**
  Parameters are not defined.
* **Returns Defined**
  Returns are not defined.
* **Raises Listed and Defined**
  Errors are not defined.
* **Visuals, if Applicable**
  No visuals are included, but may not be applicable anyway.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I'd like to help, yes.
"
25971,Requirements of SparseTensorDenseMatMul on GPU,"Hi, I'm using SparseTensorDenseMatMul op on GPU. And I noticed that there are some requirements for the sparse matrix.

https://github.com/tensorflow/tensorflow/blob/9d508106b32eb6518912501d29a80ff9967dfe05/tensorflow/core/kernels/sparse_tensor_dense_matmul_op.cc#L96-L114


Is this code snippet also unnecessary? Or I just can't use this op on big sparse matrix?
"
25970,tf.keras computes incorrect loss values with 3+D data,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes. For a minimal example, run 

```
from tensorflow import keras

layer = keras.layers.Input(shape=(1, 1, 1))
model = keras.models.Model(inputs=layer, outputs=layer)
model.compile(optimizer='adam', loss='poisson', metrics=['poisson'])
data = [[[[[1]]], [[[2]]], [[[3]]]]]
model.fit(x=data, y=data, batch_size=2, verbose=1, epochs=10)
```
and observe that loss and poisson values are different, and loss values vary:
```
Epoch 1/10
3/3 [==============================] - 1s 236ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 2/10
3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 3/10
3/3 [==============================] - 0s 40ms/sample - loss: 0.5452 - poisson: 0.4393
Epoch 4/10
3/3 [==============================] - 0s 96ms/sample - loss: 0.5452 - poisson: 0.4393
Epoch 5/10
3/3 [==============================] - 0s 1ms/sample - loss: 0.9772 - poisson: 0.4393
Epoch 6/10
3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 7/10
3/3 [==============================] - 1s 201ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 8/10
3/3 [==============================] - 0s 2ms/sample - loss: 0.6740 - poisson: 0.4393
Epoch 9/10
3/3 [==============================] - 0s 999us/sample - loss: 0.9772 - poisson: 0.4393
Epoch 10/10
3/3 [==============================] - 1s 327ms/sample - loss: 0.9772 - poisson: 0.4393
```


- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10

- TensorFlow installed from (source or binary):
`pip install tensorflow`
- TensorFlow version (use command below):
v1.13.0-rc1-19-gc865ec5621, 1.13.0-rc2
- Python version:
3.7.2 x64
- CUDA/cuDNN version:
n/a
- GPU model and memory:
n/a

**Describe the current behavior**
When fitting a model with `loss=""poisson""`, I would expect reported `loss` and `poisson` values to be identical.

**Describe the expected behavior**
`loss` values are incorrect. They vary from epoch to epoch.

**Code to reproduce the issue**
See above.

**Other info / logs**

More code examples and investigations at https://stackoverflow.com/q/54802328/880783
"
25969,Is it mandatory to use same version of Tensorflow for TFLite conversion,"**System information**
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version:3.6.5
- Installed using virtualenv
- Bazel version (if compiling from source):0.21.0
- GCC/Compiler version (if compiling from source):5.4.0


**Problem description**
I have trained a custom model using Tensorflow 1.9 binary. Now I want to convert graph to tflite. Is it mandatory to use same  Tensorflow 1.9 for this conversion or can I use the latest version of TF ?
"
25966,Compilation error when building tfcompile on Windows 10 (TF 1.13-rc2),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): source
- TensorFlow version: checkout tag v1.13-rc2
- Python version: 3.5.6
- Installed using virtualenv? pip? conda?: -
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): VS 2015 14.0.25431.01 Update 3
- CUDA/cuDNN version: 10.0/7.3.1
- GPU model and memory: GTX 1080, 8GBs

**Describe the problem**
This is a direct clone of issue #24218 as per request of @jvishnuvardhan. Except the code and CUDA version, everything else stated there applies here as well. Below the same information provided in the referenced issue, updated to the current configuration:

Checked out the release 1.13-rc2 branch of the TF repo, configured bazel (configuration is below) and ran 
`bazel build -c opt --config=opt //tensorflow/compiler/aot:tfcompile`

Compilation fails at `.\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338`. Full error message:

```
ERROR: C:/tf/tensorflow/compiler/tf2xla/BUILD:98:1: C++ compilation of rule '//tensorflow/compiler/tf2xla:cpu_function_runtime' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/filippo/_bazel_filippo/rbrahckg/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.17763.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.17763.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe
    SET PYTHON_LIB_PATH=C:/Users/Filippo/Anaconda3/envs/tf_build/lib/site-packages
    SET TEMP=C:\Users\Filippo\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0,6.1,7.5
    SET TF_CUDA_VERSION=10.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\Filippo\AppData\Local\Temp
  C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Ibazel-out/x64_windows-opt/bin /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Ibazel-out/x64_windows-opt/bin/external/local_config_sycl /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /DEIGEN_HAS_TYPE_TRAITS=0 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX2 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/tf2xla/_objs/cpu_function_runtime/cpu_function_runtime.o /c tensorflow/compiler/tf2xla/cpu_function_runtime.cc
Execution platform: @bazel_tools//platforms:host_platform
.\tensorflow/compiler/tf2xla/cpu_function_runtime.h(71): error C2338:
Target //tensorflow/compiler/aot:tfcompile failed to build
```

It's still the same static assert failing: 
```
std::pair<uint64, uint64> Encode() const {
    static_assert(sizeof(*this) == 16, """"); //<<<<<<<<<<<<<<<<< 
    uint64 upper = Pack(kind(), size_);
    uint64 lower = entry_param_number_;
    return {upper, lower};
  }
```
**Other info / logs**

Bazel config:

```
build --action_env PYTHON_BIN_PATH=""C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Users/Filippo/Anaconda3/envs/tf_build/lib/site-packages""
build --python_path=""C:/Users/Filippo/Anaconda3/envs/tf_build/python.exe""
build:xla --define with_xla_support=true
build --config=xla
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0""
build --action_env TF_CUDA_VERSION=""10.0""
build --action_env CUDNN_INSTALL_PATH=""C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v10.0""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""5.0,6.1,7.5""
build --action_env TF_CUDA_CLANG=""0""
build --config=cuda
test --config=cuda
build:opt --copt=/arch:AVX2
build:opt --define with_default_optimizations=true
build --config monolithic
build --copt=-w --host_copt=-w
build --verbose_failures
build --distinct_host_configuration=false
build --experimental_shortened_obj_file_path=true
build --define=override_eigen_strong_inline=true
build:v2 --define=tf_api_version=2
```"
25965,build failed on v1.13.0-rc1 with Geforce RTX 2080 card on Ubuntu 18.04,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution Linux Ubuntu 18.04:
- TensorFlow installed from source:
- TensorFlow version: v1.13.0-rc1
- Python version: python3.6
- Build from source:
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source):7.3.0
- CUDA/cuDNN version: 10.0/7.3
- GPU model and memory: Geforce RTX 2080/8G



**Describe the problem**
After configure the build on NVidia Geforce Card
It failed when I run
` build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --verbose_failures`

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
build failed.
Last message is
`ERROR: /home/windy/work/opensource/tensorflow/tensorflow/core/kernels/BUILD:762:1: C++ compilation of rule '//tensorflow/core/kernels:broadcast_to_op' failed (Exit 4): crosstool_wrapper_dri
ver_is_not_gcc failed: error executing command
  (cd /home/windy/.cache/bazel/_bazel_windy/7f7b18d654824f59399fafbf60350ed2/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/usr/local/lib:/usr/local/cuda/extras/CUPTI/lib64: \
    PATH=/home/windy/install/bin:/home/windy/bin::/home/windy/.local/bin:/usr/local/cuda/bin:/usr/local/bin:/home/windy/install/bin:/home/windy/bin::/usr/local/cuda/bin:/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.o' -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' '-DEIGEN_HAS_TYPE_TRAITS=0' -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTF_USE_SNAPPY -DCURL_STATICLIB -DPLATFORM_LINUX -DENABLE_CURL_CLIENT -DENABLE_NO_ENCRYPTION -iquote . -iquote bazel-out/host/genfiles -iquote bazel-out/host/bin -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote bazel-out/host/bin/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote bazel-out/host/bin/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote bazel-out/host/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote bazel-out/host/bin/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote bazel-out/host/bin/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote bazel-out/host/bin/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote bazel-out/host/bin/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote bazel-out/host/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote bazel-out/host/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote bazel-out/host/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote bazel-out/host/bin/external/highwayhash -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -iquote bazel-out/host/bin/external/zlib_archive -iquote external/local_config_cuda -iquote bazel-out/host/genfiles/external/local_config_cuda -iquote bazel-out/host/bin/external/local_config_cuda -iquote external/double_conversion -iquote bazel-out/host/genfiles/external/double_conversion -iquote bazel-out/host/bin/external/double_conversion -iquote external/curl -iquote bazel-out/host/genfiles/external/curl -iquote bazel-out/host/bin/external/curl -iquote external/boringssl -iquote bazel-out/host/genfiles/external/boringssl -iquote bazel-out/host/bin/external/boringssl -iquote external/jsoncpp_git -iquote bazel-out/host/genfiles/external/jsoncpp_git -iquote bazel-out/host/bin/external/jsoncpp_git -iquote external/aws -iquote bazel-out/host/genfiles/external/aws -iquote bazel-out/host/bin/external/aws -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem bazel-out/host/bin/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem bazel-out/host/bin/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem bazel-out/host/bin/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem bazel-out/host/bin/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem bazel-out/host/bin/external/farmhash_archive/src -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -isystem bazel-out/host/bin/external/zlib_archive -isystem external/local_config_cuda/cuda -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda -isystem bazel-out/host/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/genfiles/external/local_config_cuda/cuda/cuda/include/crt -isystem bazel-out/host/bin/external/local_config_cuda/cuda/cuda/include/crt -isystem external/double_conversion -isystem bazel-out/host/genfiles/external/double_conversion -isystem bazel-out/host/bin/external/double_conversion -isystem external/curl/include -isystem bazel-out/host/genfiles/external/curl/include -isystem bazel-out/host/bin/external/curl/include -isystem external/boringssl/src/include -isystem bazel-out/host/genfiles/external/boringssl/src/include -isystem bazel-out/host/bin/external/boringssl/src/include -isystem external/jsoncpp_git/include -isystem bazel-out/host/genfiles/external/jsoncpp_git/include -isystem bazel-out/host/bin/external/jsoncpp_git/include -isystem external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-core/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-core/include -isystem external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-kinesis/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-kinesis/include -isystem external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/genfiles/external/aws/aws-cpp-sdk-s3/include -isystem bazel-out/host/bin/external/aws/aws-cpp-sdk-s3/include '-std=c++11' -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -g0 '-march=native' -g0 -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -msse3 -pthread '-DGOOGLE_CUDA=1' '-DGOOGLE_TENSORRT=1' -c tensorflow/core/kernels/broadcast_to_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/broadcast_to_op/broadcast_to_op.pic.o)
Execution platform: @bazel_tools//platforms:host_platform
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 203.325s, Critical Path: 132.09s
INFO: 644 processes: 644 local.
FAILED: Build did NOT complete successfully
`"
25961,Warning with custom categorical column with custom weights,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: MacOS 10.12
- TensorFlow installed from source
- TensorFlow version: v1.12
- Python version: 3.6.6

**Describe the current behavior**

A `_CategoricalColumn` is represented with 2 `SparseTensor`s: one for indexes and one for weights. It seems all implementation of Tensorflow feature columns use the default `None` weight `SparseTensor`. However, in my project I need to assign custom weights to a feature column of type `_CategoricalColumn` and it seems we then systematically obtain the following warning:
```python
tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
```
I implemented a toy example of such behaviour:

```python
import collections

from tensorflow.python.ops import math_ops
from tensorflow.python.feature_column.feature_column import _CategoricalColumn


class _DebuggingCategoricalColumn(
    _CategoricalColumn,
    collections.namedtuple('_DebuggingCategoricalColumn', (
        'dense_col',
    ))):

    @property
    def name(self):
        return 'debugging_weighted'

    @property
    def _parse_example_spec(self):
        config = self.dense_col._parse_example_spec  # pylint: disable=protected-access
        return config

    @property
    def _num_buckets(self):
        return 4

    def _transform_feature(self, inputs):
        v = inputs.get(self.dense_col)
        batch_size = tf.to_int64(tf.shape(v)[0])
        buckets = tf.to_int64(tf.squeeze(math_ops.bucketize(input=v, boundaries=(0, 2, 4))))
        indices = tf.stack([
            tf.range(batch_size, dtype=tf.int64),
            tf.zeros(batch_size, dtype=tf.int64)], axis=1)
        id_tensor = tf.SparseTensor(
            indices=indices,
            values=tf.to_int64(tf.squeeze(buckets)),
            dense_shape=[batch_size, 1]
        )
        id_weights = tf.SparseTensor(
            indices=indices,
            values=tf.ones(batch_size, dtype=tf.float32),
            dense_shape=[batch_size, 1]
        )
        return id_tensor, id_weights

    def _get_sparse_tensors(self, inputs, weight_collections=None, trainable=None):
        tensors = inputs.get(self)
        return _CategoricalColumn.IdWeightPair(tensors[0], tensors[1])


def debugging_column(input_column):
    return _DebuggingCategoricalColumn(dense_col=input_column)


df = pd.DataFrame({'x': [-1., 1, 3, 5], 'y': [1., 1, 2, 3]})
feat = tf.feature_column.numeric_column('x')
feature_columns = [debugging_column(feat)]
input_fn = tf.estimator.inputs.pandas_input_fn(df, df['y'], shuffle=True, num_epochs=None)
estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)
estimator.train(input_fn=input_fn, steps=100)
```

Running this code leads to the following warning:
```python
tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.
  ""Converting sparse IndexedSlices to a dense Tensor of unknown shape. ""
```

Importantly, replacing line `return id_tensor, id_weights` by line `return id_tensor, None` (default value for uniform weights) removes the warning.

Am I doing something wrong with the custom `_CategoricalColumn` or is there an issue in TF internals? Thanks for your help
"
25960,Cannot find cudnn.h under ~ but I really have it there !,"
**System information**
- OS Platform and Distribution : Redhat 7.5 ppc64le
- TensorFlow installed from source
- TensorFlow version: v1.12.0
- Python version: 3.6.5  (anaconda3 v5.2)
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source):  4.8.5
- CUDA/cuDNN version: 9.2/7.4.2
- GPU model and memory: V100 16GB

**Describe the problem**

I am trying to build tensorflow r1.12 using bazel 0.15 on Redhat 7.5 ppc64le.

I am stuck with the following error.

    [u0017649@sys-97184 tensorflow]$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package 
    ...
    ERROR: error loading package 'tensorflow/tools/pip_package': Encountered error while reading extension file 'cuda/build_defs.bzl': no such package 
    '@local_config_cuda//cuda': Traceback (most recent call last):
        File 
    ""/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1447
                _create_local_cuda_repository(repository_ctx)
        File 
    ""/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1187, in 
    _create_local_cuda_repository
                _get_cuda_config(repository_ctx)
        File 
    ""/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl"", line 911, in _get_cuda_config
                _cudnn_version(repository_ctx, cudnn_install_base..., ...)
        File 
    ""/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl"", line 582, in _cudnn_version
                _find_cudnn_header_dir(repository_ctx, cudnn_install_base...)
       File 
    ""/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl"", line 869, in 
    _find_cudnn_header_dir
                auto_configure_fail((""Cannot find cudnn.h under %s"" ...))
        File 
    ""/home/u0017649/files/tensorflow/third_party/gpus/cuda_configure.bzl"", line 317, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

    Cuda Configuration Error: Cannot find cudnn.h under /usr/local/cuda-9.2/targets/ppc64le-linux/lib


I do have a soft link for cudnn.h under /usr/local/cuda-9.2/targets/ppc64le-linux/lib as below.

    [u0017649@sys-97184 tensorflow]$ ls -l /usr/local/cuda-9.2/targets/ppc64le-linux/lib/cudnn.h
    lrwxrwxrwx. 1 root root 57 Feb 20 10:15 /usr/local/cuda-9.2/targets/ppc64le-linux/lib/cudnn.h -> /usr/local/cuda-9.2/targets/ppc64le-linux/include/cudnn.h


Any comments, pls ?
"
25959,[TF 2.0 API Docs] tf.image.non_max_suppression,"### Existing URLs containing the issue:

https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image/non_max_suppression#aliases

### Description of issue (what needs changing):

* **Correct Links**
  https://github.com/tensorflow/tensorflow/blob/master/python/ops/image_ops_impl.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/image_ops_impl.py.

* **Clear Description**
  The description is not opinionated about when to use this symbol and unclear how arguments effect the functionality.

* **Usage Example**
  Usage example is mixed into the description, not formatted and not self-contained

* **Parameters Defined**
  Parameters are defined correctly with expected formats.

* **Returns Defined**
  Returns are defined correctly with expected formats.

* **Raises Listed and Defined**
  Errors are not defined.

* **Visuals, if Applicable**
  No visuals are included."
25958,The performance of SparseSoftmaxCrossEntropyWithLogits in inference is very low,"SparseSoftmaxCrossEntropyWithLogits computes softmax cross entropy cost and gradients to backpropagate. 
When SparseSoftmaxCrossEntropyWithLogits used in inference, for example, in language models, we only need the cost. 
SparseSoftmaxCrossEntropyWithLogits is very time-consuming in language models, it is better to add a flag for inference and training, or to split this OP to two.

**Describe the current behavior**
Profile data of language model:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
**SparseSoftmaxCrossEntropyWithLogits     245.76KB (100.00%, 0.00%),      1.58sec (100.00%, 51.51%),             0us (0.00%, 0.00%),      1.58sec (100.00%, 51.51%)**
MatMul                      8905.28MB (100.00%, 88.05%),       1.10sec (48.49%, 36.00%),             0us (0.00%, 0.00%),       1.10sec (48.49%, 36.00%)
Add                                 84B (11.95%, 0.00%),       201.16ms (12.49%, 6.57%),             0us (0.00%, 0.00%),       201.16ms (12.49%, 6.57%)
Split                          671.09MB (11.95%, 6.64%),        105.97ms (5.92%, 3.46%),             0us (0.00%, 0.00%),        105.97ms (5.92%, 3.46%)

**Describe the expected behavior**
Profile data after optimization:
node name | requested bytes | total execution time | accelerator execution time | cpu execution time
MatMul                      8905.28MB (100.00%, 88.05%),      1.11sec (100.00%, 46.86%),             0us (0.00%, 0.00%),      1.11sec (100.00%, 46.86%)
**SparseSoftmaxCrossEntropyWithLogits      245.76KB (11.95%, 0.00%),      871.49ms (53.14%, 36.93%),             0us (0.00%, 0.00%),      871.49ms (53.14%, 36.93%)**
Add                                 84B (11.95%, 0.00%),       200.13ms (16.21%, 8.48%),             0us (0.00%, 0.00%),       200.13ms (16.21%, 8.48%)
Split                          671.09MB (11.95%, 6.64%),        106.76ms (7.73%, 4.52%),             0us (0.00%, 0.00%),        106.76ms (7.73%, 4.52%)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): el7
- TensorFlow installed from (source or binary):source
- TensorFlow version (use command below):1.12
- Python version:3.7.2
- Bazel version (if compiling from source):0.19.2
- GCC/Compiler version (if compiling from source):gcc6.3

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/mlperf/inference/tree/master/cloud/language_modeling
./benchmark.py

"
25957,keras model.compile: allow user-specific label shapes/dtypes for loss/metrics,"## System information
- TensorFlow version: 1.12 (also present in 1.13, 2.0)
## Are you willing to contribute it (Yes/No):
Yes

## Describe the feature and the current behavior/state.
`tf.keras.models.Model.compile` currently has no way of knowing the shape/size of targets that will be passed to it during `Model.fit` (and other main looping methods). Instead it creates placeholders on a 1-to-1 basis corresponding to the same dtype and number of dimensions as each output. This is a safe assumption for most regression tasks, but leads to redundant casting/reshaping for even simple classification problems. More importantly, it leads to very confusing behaviour, loss/metric functions which need to cater for a wider variety of inputs than necessary and an inability to do proper error checking.

For example, in 2.0 `tf.keras.backend.sparse_categorical_crossentropy`'s documentation claims the first argument is `target: An integer tensor.` However, there's absolutely nothing stopping you from providing float values. This is because the implementation needs to be able to accept arbitrarily sized float values when passed placeholders during `compile`, then simply flattens and casts the values (which has no affect on inputs already of the correct shape/type).

This is particularly confusing for users trying to implement their own loss function. Mimicing the advertised interface will result in errors.

This isn't just a documentation issue however: if inappropriate float values are passed in (i.e. values that aren't the result of casting integers to floats) this will be very difficult to pick up.

If users are asked to provide `loss` and `metrics` functions, they will generally know the shape and types of the labels. Allowing them to specify this so their loss/metric functions only need to cater for these removes unecessary reshaping and casting.

## Will this change the current api? How?
I don't understand why loss/metric functions are called during compile with placeholders rather than being lazily evaluated during `fit`. From my perspective that would be the thing to change, resulting in no changes to the API. However, it may result in too much of a breaking change in other niche cases.

Lacking that, adding extra kwargs to `Model.compile - maybe `target_shapes`, `target_sizes` (`targets` is already taken) would be a non-breaking change. Default values can be created using the current assumptions.

## Who will benefit with this feature?
- People writing custom losses/metrics with the expectation that `y_true` shape/dtype will match that of the values in their labels.
- Anyone training with models using unecessary casts/reshapes (probably not a major performance issue).
"
25955,keras `Model.compile` with loss/metrics dict and multiple outputs from same layer,"## System information
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- Python version: ('v1.12.0-7354-gedbd8a15b9', '1.13.0-dev20190203') - also present in 2.0 preview
- CUDA/cuDNN version: 10.0 / 7
- GPU model and memory: quadro k620

## Current Behaviour
Keras' `model.compile` with dict losses matches provided loss functions with outputs passed into the constructor via each output's _layer name_. This is not unique in the case where multiple model outputs come from the same layer.

## Expected Behaviour
I would expect usages of lists/dicts to be consistent between losses, metrics, labels and outputs - i.e. if any are lists, they must all be lists of the same length, and if any are dicts they must all be dicts with the same set of keys. Requiring output layers to be named consistently with the key in the corresponding labels dictionary seems completely bamboozling to me, e.g. if building a hybrid model with a classification output and 'class_index' key in the labels one would have to name the final layer 'class_index' even if it outputs logits/softmax activations.

This is likely a breaking change. At the very least I would expect an error to be raised if `output_names` are not unique and users pass in dictionaries for either `loss` or `metrics` in `Model.fit`.

## Code to Reproduce
```python
import tensorflow as tf
# network that maps 1 input to 2 separate outputs
x = tf.keras.layers.Input(shape=(1,), dtype=tf.float32)
x2 = tf.keras.layers.Dense(2)(x)
y, z = tf.keras.layers.Lambda(
    lambda x2: tf.unstack(x2, axis=-1), name='x2_unstack')(x2)
# y = tf.keras.layers.Lambda(tf.identity, name='y')(y)
# z = tf.keras.layers.Lambda(tf.identity, name='z')(z)  # current work-around
model = tf.keras.models.Model(inputs=x, outputs=dict(y=y, z=z))

print(model.outputs)
# [
#  <tf.Tensor 'x2_unstack/unstack:0' shape=(None,) dtype=float32>,
# <tf.Tensor 'x2_unstack/unstack:1' shape=(None,) dtype=float32>
# ]
# somewhat unexpected as not the same as the value passed to constructor, but ok...
print(model.output_names)
# ['x2_unstack', 'x2_unstack']
# surely that's going to cause issues later...

model.compile('SGD', loss={'x': 'mse', 'y': 'logcosh'})
# ValueError: Unknown entry in loss dictionary: y.
# Only expected following keys: ['x2_unstack', 'x2_unstack']

model.fit(x=x_data, y=dict(y=y_data, z=z_data))  # expected way of using fit
```

"
25954,"Issue with training using gpu on docker (slow to start and in general as well I believe, same speed as CPU)","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
NO, stock example script: tensorflow-tutorials/basic_classification.ipynb (in docker)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): not installed (docker)
- TensorFlow version (use command below): b'v1.13.0-rc1-0-g54c5616308' 1.13.0-rc1 (in docker)
- Python version: N/A I believe
- Bazel version (if compiling from source): N/a
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version:

== cuda libs  ===================================================
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart.so.10.0.130
/usr/local/cuda-10.0/targets/x86_64-linux/lib/libcudart_static.a

- GPU model and memory:
== nvidia-smi ===================================================
Thu Feb 21 00:47:44 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  Quadro M1200        Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   34C    P0    N/A /  N/A |    518MiB /  4043MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

In the introductory script model.fit(train_images, train_labels, epochs=5) begins training 4 minutes after it is run with this log ```tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally```
It also only trained at a rate of 60 us/sample which was equal to the speed of training using my CPU (different docker image)

**Describe the expected behavior**

model.fit(train_images, train_labels, epochs=5) begins training when it is run and trains at a faster speed

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
sudo docker run --name tensor-flow -it -u $(id -u):$(id -g) --runtime=nvidia -v $(realpath ~/notebooks):/tf/notebooks -p 8888:8888 tensorflow/tensorflow:latest-py3-jupyter bash
```
in docker bash - 'jupyter notebook --ip 0.0.0.0'
Then, run the notebook in tensorflow-tutorials/basic_classification.ipynb

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

latest-gpu-py3-jupyter
Started running

```
[I 05:40:40.213 NotebookApp] Adapting to protocol v5.1 for kernel b722a15a-97d0-4d5e-8a53-553c8fb58065
2019-02-20 05:41:18.670612: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 05:41:18.800083: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-20 05:41:18.800916: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5b93e80 executing computations on platform CUDA. Devices:
2019-02-20 05:41:18.800955: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0
2019-02-20 05:41:18.819111: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2019-02-20 05:41:18.819805: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5bfdc50 executing computations on platform Host. Devices:
2019-02-20 05:41:18.819834: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-20 05:41:18.820045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.27GiB
2019-02-20 05:41:18.820069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-20 05:41:18.820972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-20 05:41:18.820986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-20 05:41:18.820993: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-20 05:41:18.821119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3053 MB memory) -> physical GPU (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-02-20 05:45:22.499611: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
```

Took this long to start model.fit, but cpu was on high loads for those 4 minutes (gpu memory was being used by python but utilization was low until it started training when utilization went to 50%), kernel then freezes on shut down (need to ctrl-c twice, this happens for all kernels including cpu only)

latest-gpu-jupyter does the same thing

```
Adapting to protocol v5.1 for kernel 0d53ac42-e696-4671-8595-c884aa8088d3
2019-02-20 05:53:49.471477: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 05:53:49.535506: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-20 05:53:49.536440: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x64416f0 executing computations on platform CUDA. Devices:
2019-02-20 05:53:49.536467: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Quadro M1200, Compute Capability 5.0
2019-02-20 05:53:49.554965: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2019-02-20 05:53:49.555436: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x64ab4a0 executing computations on platform Host. Devices:
2019-02-20 05:53:49.555454: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-20 05:53:49.555645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: Quadro M1200 major: 5 minor: 0 memoryClockRate(GHz): 1.148
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 3.25GiB
2019-02-20 05:53:49.555663: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-20 05:53:49.556601: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-20 05:53:49.556620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-20 05:53:49.556629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-20 05:53:49.556768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3031 MB memory) -> physical GPU (device: 0, name: Quadro M1200, pci bus id: 0000:01:00.0, compute capability: 5.0)
2019-02-20 05:57:51.253642: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
```


latest-py3-jupyter is just as fast as gpu versions (60 us/sample) Quadro M1200 vs intel 7700HQ, it is possible that the GPU is not working in the above examples (there is some other reason for the increase in GPU utilisation) or maybe the speed really is just equivalent

```
[I 05:59:23.763 NotebookApp] Adapting to protocol v5.1 for kernel 02073f3e-f863-42c8-9454-4596c4f0e382
2019-02-20 06:00:03.976951: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 06:00:03.998969: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2808000000 Hz
2019-02-20 06:00:03.999527: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5391b50 executing computations on platform Host. Devices:
2019-02-20 06:00:03.999549: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
```
"
25952,MKL Link error libiomp5md.lib library is corrupt,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.13.0rc1
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.22
- GCC/Compiler version (if compiling from source): visual studio 17
- CUDA/cuDNN version: 7.4.2
- GPU model and memory: titan xp, 12GB ram

When I run ```bazel build --config=opt --config=mkl --linkopt=""/FORCE:MULTIPLE"" -k //tensorflow:libtensorflow.so```

I get this error at link time:
```
C:/Program Files (x86)/Microsoft Visual Studio/2017/Community/VC/Tools/MSVC/14.16.27023/bin/HostX64/x64/link.exe /nologo /DLL /SUBSYSTEM:CONSOLE -defaultlib:advapi32.lib -DEFAULTLIB:advapi32.lib -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 /MACHINE:X64 /FORCE:MULTIPLE @bazel-out/x64_windows-opt/bin/tensorflow/libtensorflow.so-2.params
Execution platform: @bazel_tools//platforms:host_platform
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64'; ignored
LINK : warning LNK4044: unrecognized option '/lpthread'; ignored
external\mkl_windows\lib\libiomp5md.lib : fatal error LNK1127: library is corrupt
```

Is there any workaround?

**Any other info / logs**
I have mkl 2019-update 2 installed and set in my PATH env."
25950,TFLite GPU crashes when not all ops are supported by delegate?,"**System information**
- I have custom code, however GPU Delegate part I have used as described in here provided sample
- Ubuntu 18.04
- Samsung Galaxy J5 2015 (crashes), Galaxy S7 and LG G6 (works fine), 
- TF build from 'org.tensorflow:tensorflow-lite:0.0.0-gpu-experimental'

**Describe the current behavior**
Using TF Lite with GPU developer preview, when I run inference on my mobilenetV2 retrained model (with two outputs) on most devices it works nice. However on Galaxy J5 (odler device), it crashes with:
    
        E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: bazinac.aplikacenahouby, PID: 12367
    java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: WARNING: op code #40 cannot be handled by this delegate.  Only the first 67 ops will run on the GPU, and the remaining 10 on the CPU.GpuDelegate Prepare: No EGL error, but eglCreatePbufferSurface failedNode number 77 (GpuDelegate) failed to prepare.
    
        at org.tensorflow.lite.NativeInterpreterWrapper.applyDelegate(Native Method)_

**Describe the expected behavior**
I would expect that when _isGpuDelegateAvailable()_ method provides only such a delegate, that is capable preparing and running all the ops. It should return false otherwise, I guess. Is there some way how to tell this in advance, or should I manually check for these kind of internal errors and try to fall back by recreating Interpret without GPU support? 

**Code to reproduce the issue**
`    protected void runInference() {

        Log.d(TAG, ""Inference starts "");
        Object[] inputArray = {imgData};

        Map<Integer, Object> outputMap = new HashMap<>();
        outputMap.put(0, outp1);
        outputMap.put(1, outp2);
        tflite.runForMultipleInputsOutputs(inputArray, outputMap);
        Log.d(TAG, ""Inference ends "");
    }`

**Other info / logs**
Thank you guys for your effort to make this available to us, you are doing very good job!
"
25948,Raspberry pi - Go Tensorflow - OOM allocating tensor issue,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
1. Code in Golang
2. Used GO Tensorflow
3. MobileNet model 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Raspberry Pi (1.4GHz, 1024 RAM, 4 Cores)
- TensorFlow version (use command below):
v1.12.0

**Describe the current behavior**
I have written a Face Classification code which uses MobileNet (depth 80) model. But When I ran the model after 8-10 min, tensorflow is throwing OOM allocation error.
**Describe the expected behavior**
Because raspberry pi has limited memory. But as the model is very small it has to run perfectly.
**Code to reproduce the issue**
1. Run MobileNet model classification in Golang Env
2. Input tensor shape [5,224,224,3], tried other batch sizes as well 1 and 3 but no luck.
**Other info / logs**
```
2019-02-20 10:52:26.403200: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at fused_batch_norm_op.cc:573 : Resource exhausted: OOM when allocating tensor with shape[5,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
2019-02-20 10:52:26.478760: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at pad_op.cc:137 : Resource exhausted: OOM when allocating tensor with shape[5,113,113,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0220 10:52:26.483588    3282 classifier.go:203] Failed to classify face: OOM when allocating tensor with shape[5,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1_bn/FusedBatchNorm}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc

```
"
25947,tf.python.keras.utils.Sequence hangs fit_generator,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):  v1.12.0-8400-g7f0bf2ef4f 1.13.0-dev20190220
- Python version: 3.6
- CUDA/cuDNN version: 10/ 7.4.2
- GPU model and memory: 2x 980Ti

**Describe the current behavior**
- I've made a data generator that inherits from Sequence
-  calling fit_generator on it hangs if and only if use_multiprocessing=True.  It hangs in training_generator.py in`_get_next_batch `on the line `generator_output = next(generator)`
**Describe the expected behavior**
- Doesn't hang when using use_multiprocessing=True.

I haven't narrowed it down to the bare minimum code required to reproduce - if this issue isn't explainable with the information above I'll try to cut down the code to the minimum that reproduces so that I can submit it here.
"
25946,BUG: symbolic layer triggers device creation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):b'v1.13.0-rc2-0-gc865ec5621' 1.13.0-rc2
- Python version:3.7
- Bazel version (if compiling from source):n/a
- GCC/Compiler version (if compiling from source):n/a
- CUDA/cuDNN version:10.0 / 7.4.2
- GPU model and memory:gtx960M

**Describe the current behavior**
The following code:
```python
import tensorflow as tf
a = tf.placeholder(tf.float32, [100, 100, 100, 100])
b = tf.layers.Conv2DTranspose(3, 3, data_format='channels_first')
output = b.apply(a)
```
prints:
```
2019-02-20 10:20:05.505595: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-20 10:20:05.578782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-20 10:20:05.579477: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fd579f65d0 executing computations on platform CUDA. Devices:
2019-02-20 10:20:05.579513: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce GTX 960M, Compute Capability 5.0
2019-02-20 10:20:05.606095: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2592000000 Hz                                
2019-02-20 10:20:05.606746: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x55fd57b39b00 executing computations on platform Host. Devices:
2019-02-20 10:20:05.606785: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>               
2019-02-20 10:20:05.607093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties:                              
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.0975
pciBusID: 0000:01:00.0
totalMemory: 1.96GiB freeMemory: 1.92GiB
2019-02-20 10:20:05.607118: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0                                
2019-02-20 10:20:05.608205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-20 10:20:05.608229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0                                                        
2019-02-20 10:20:05.608240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N                                                       
2019-02-20 10:20:05.608504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1742 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)        
```
It can be seen that it initializes the GPU devices. However this should not happen in symbolic functions.

Initializing the GPU devices has many side effects. 
It can lead to different types of failures, such as https://github.com/tensorflow/tensorflow/issues/8136#issuecomment-361727732. The largest side effect is that: any GPU-related flags given to a `tf.Session` created after device initialization will not take effect.
It will also make it much harder to use horovod because horovod requires initializing the GPU in specific ways (with `visible_device_list`). If a graph with `Conv2DTranspose` was created before creating the session (which is the standard way of using TF 1.0), horovod will fail to initialize the session. (cc @alsrgv ).

This bug exists for Conv2DTranspose, but not for Conv2D.
This bug exists in 1.13.0rc0. It does not exist in 1.12.0"
25945,Measuring the prediction accuracy for the tensorflow-lite android version,"I am using the tensorflow-lite version (https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo). Can anyone please tell me how to compute the object detection accuracy or prediction accuracy while using this?

"
25944,Is tensorflow-mkl version same as tensorflow?,"OS: windows 10
Intel said that if I want to install the mkl version of tensorflow on windows 10, I have to use the command as following:
_conda install tensorflow-mkl_

However, Anaconda said that if I want to install the mkl version of tensorflow, I just need to execute the following command no matter my OS is windows or linux:
_conda install tensorflow_

I just want to know if tensorflow is the same as tensorflow-mkl when I install tensorflow using the _conda install_ command ?"
25942,Tensorflow Import Error (numpy.core.multiarray failed to import),"Trying to import Tensorflow in iPython while in an conda environment. An issue with Numpy seems to be breaking the import. I have uninstalled and reinstalled numpy and tensorflow (CPU) using Pip commands within the environment. Thanks everyone

*System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 1.13
- Python version: 3.7.2
- Installed using virtualenv? pip? conda?: pip install --upgrade tensorflow


In [1]: import tensorflow
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
ImportError: DLL load failed: The specified module could not be found.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
ImportError: numpy.core.multiarray failed to import

The above exception was the direct cause of the following exception:

SystemError                               Traceback (most recent call last)
~\AppData\Local\conda\conda\envs\objectdetection\lib\importlib\_bootstrap.py in _find_and_load(name, import_)

SystemError: <class '_frozen_importlib._ModuleLockManager'> returned a result with an error set
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
ImportError: numpy.core._multiarray_umath failed to import
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
ImportError: numpy.core.umath failed to import
2019-02-20 07:42:30.475025: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr"
25939,Docker containers with Python 3.7,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version 1.12:
- Are you willing to contribute it (Yes/No): yes



**Describe the feature and the current behavior/state.** TensorFlow docker image is not available for Python 3.6 or Python 3.7. It would be good if TensorFlow is supported in Python 3.6 or 3.7 to take advantage of the latest Python version capabilities.

**Will this change the current api? How?** I don't believe so. 

**Who will benefit with this feature?** All users who want to use the Python 3.6 and 3.7 capabilities.

**Any Other info.** 
"
25938,keras.models.load_model() fails when the model uses a keras.losses.Loss subclass,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
tf.version.VERSION: '2.0.0-dev20190220'
tf.version.GIT_VERSION: 'v1.12.0-8385-gaaef4e8e43'
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
`keras.models.load_model()` raises a `ValueError` when the model to be loaded uses a `keras.losses.Loss` subclass, such as `keras.losses.Huber`.

**Describe the expected behavior**
Should load normally, and I should be able to continue training where it left off using the loss.

**Code to reproduce the issue**
```python
import tensorflow as tf
from tensorflow import keras

X_train = np.random.randn(100, 2)
y_train = np.random.randn(100, 1)

model = keras.models.Sequential([keras.layers.Dense(1, input_dim=2)])
model.compile(loss=keras.losses.Huber(2.0), optimizer=""sgd"")
model.fit(X_train, y_train, epochs=2)
model.save(""my_model.h5"")
model = keras.models.load_model(""my_model.h5"") # Raises a ValueErro
```

**Other info / logs**
Here is the stacktrace:

```pycon
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 248, in load_model
    sample_weight_mode=sample_weight_mode)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/training/tracking/base.py"", line 456, in _method_wrapper
    method(self, *args, **kwargs)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 281, in compile
    loss, self.output_names)
  File ""/Users/ageron/.virtualenvs/tf2/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py"", line 1142, in prepare_loss_functions
    'following keys: {}'.format(name, output_names))
ValueError: Unknown entry in loss dictionary: class_name. Only expected following keys: ['dense']
```

I did some debugging, and I think I found the origin of the problem.  In `hdf5_format.py`, around line 233, the following lines use `convert_custom_objects()`, but they should be using `losses.deserialize()` and `metrics.deserialize()`.  I'll send a PR.

```python
      # Recover loss functions and metrics.
      loss = convert_custom_objects(training_config['loss'])
      metrics = convert_custom_objects(training_config['metrics'])
      weighted_metrics = convert_custom_objects(
          training_config.get('weighted_metrics', None))
```"
25937,error import tensorflow,"System information
OS Platform: Windows 10 x64
TensorFlow installed from: pip install tensorflow-gpu
Python version: 3.5.3
CUDA/cuDNN version: CUDA10.0 / cuDNN v7.4.2
GPU model and memory: GTX 1060
Exact command to reproduce: import tensorflow
Describe the problem
It is impossible to import tensorflow in my present environment.I am getting the following error.
---------------------------------------------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda2\envs\python3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda2\envs\python3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed:        . .

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-2-566d4aa4fae9> in <module>
      1 
----> 2 import tensorflow

~\Anaconda2\envs\python3\lib\site-packages\tensorflow\__init__.py in <module>
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\__init__.py in <module>
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\quartermaine\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\quartermaine\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\quartermaine\Anaconda2\envs\python3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\quartermaine\Anaconda2\envs\python3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\quartermaine\Anaconda2\envs\python3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed:        . .
"
25936,Adding labels to images while training,"Hi @nsthorat @tensorflower-gardener,  we were about to implement image recognization

I'm able to convert the image to tensor, now I want to add the label and give training to that image. If this gets succeeded then I want to do the same for multiple images with their respective labels. Any suggestions would be helpful.

Thanks"
25935,Bazel incompatible changes,"Tensorflow doesn't build with Bazel incompatible flags.
Based on our CI (https://buildkite.com/bazel/bazel-at-release-plus-incompatible-flags/builds/95), the following flags will fail:

* `--incompatible_new_actions_api`
* `--incompatible_no_support_tools_in_action_inputs`
* `--incompatible_no_transitive_loads`
* `--incompatible_disable_deprecated_attr_params`
* `--incompatible_disallow_load_labels_to_cross_package_boundaries`


From what I've seen, updating the dependency on rules_closure will fix some of the problems.

For `--incompatible_no_transitive_loads`, the simplest is to reexport some symbols explicitly (see https://github.com/bazelbuild/bazel/issues/5636).

If you run `buildifier --lint=fix tensorflow/tensorflow.bzl`, it will also fix some issues.

Let me know if you need help
(bug also filed inside Google: b/124051098)"
25934,"numpy.dtype size changed, may indicate binary incompatibility","## Cleanest reproduction:

Run in a GPU Google Colaboratory session:

    !pip3 install tensorflow-gpu==1.13.0rc2 numpy==1.15.0
    !python3.6 -c 'import tensorflow'

However, how I came across the issue:

**System information**
- Ubuntu 18.04
- Dell XPS 15 2016 laptop
- installed a binary using pip3 install tensorflow-gpu==1.13.0rc2
- TensorFlow version: 1.13.0rc2
- Python 3.6.5
- virtualenv with python3.6 and pip3
- N/A did not installed from source
- N/A did not compiled from source
- cuda 10.0 / libcudnn7_7.4.2.24-1+cuda10.0_amd64.deb
- GPU model and memory:

    NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0
    GeForce GTX 960M
    91MiB /  2004MiB

## Problem:

Warning when importing tensorflow, only when numpy version is 1.15.0

    /home/herbert/.virtualenvs/actigraph/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
      return f(*args, **kwds)
    /home/herbert/.virtualenvs/actigraph/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88
      return f(*args, **kwds)

## Commands to reproduce:

    pip3 install tensorflow-gpu==1.13.0rc2 numpy==1.15.0
    python3.6 -c 'import tensorflow'

## Commands to fix:

    pip3 install tensorflow-gpu==1.13.0rc2 numpy==1.16.1
    python3.6 -c 'import tensorflow'

Or probably, leave the version of numpy out (`==1.16.1`) and include `-U` to upgrade to the newest numpy.

    pip3 install -U tensorflow-gpu==1.13.0rc2 numpy
    python3.6 -c 'import tensorflow'"
25933,tensorflow Module object has no attribute keras.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Pip install...
- TensorFlow version: latest
- Python version: python 2.12.15
- Installed using virtualen? pip? conda?:pip/virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: Cuda 9.2
- GPU model and memory: NVIDIA MX150



**Describe the problem**
I have tried to install tensorflow with gpu support both with pip and virualenv several times, I have got following erros. anyone has idea?

> `python
Python 2.7.15rc1 (default, Nov 12 2018, 14:31:15) 
[GCC 7.3.0] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""tensorflow.py"", line 2, in <module>
    mnist = tf.keras.datasets.mnist
AttributeError: 'module' object has no attribute 'keras'`

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25932,tf.data.experimental.CsvDataset iterates forever,"I have a csv file that has number of rows equal to 1 batch but when I run my code it loops that batch forever and never throws an exception. How can I make the iterator stop after it reaches the end of the dataset? Is this a bug?

```
train_dataset_1 = tf.data.experimental.make_csv_dataset([os.path.join(FLAGS.data_path, ""small_interpolated_2_4_6_ONE.csv"")],
                                                       batch_size=BATCH_SIZE*SEQ_LEN,
                                                       select_columns=[5,6],
                                                       label_name='angle',
                                                       shuffle=False,
                                                       column_defaults=[tf.string, tf.float32])

iterator_1 = Iterator.from_structure(train_dataset_1.output_types, train_dataset_1.output_shapes)

train_iter_init_op_1 = iterator_1.make_initializer(train_dataset_1)
next_batch_op_1 = iterator_1.get_next()
 
 
 
config = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth = True), 
                        allow_soft_placement=True, 
                        log_device_placement=False)                      
sess = tf.Session(config=config)
sess.run(tf.global_variables_initializer())
sess.run(train_iter_init_op_1)


while True:
    try:      
        img_batch_dict, label_batch = sess.run(next_batch_op_1)
        

        for k in img_batch_dict[""filename""]:
            print(k)
            
        print()           
    except tf.errors.OutOfRangeError:
        print(""Epoch ended 11111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111"")
        break
```"
25931,Model with mirror pad can run using Desktop tflite but can't run using Android tflite,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu16.04
- TensorFlow installed from (source or binary):binary
- TensorFlow version (or github SHA if from source):'1.13.0-dev20190215'

I notice that since 11/20/2018, [mirror pad](https://github.com/tensorflow/tensorflow/issues/23962#issuecomment-442147097) was supported in TFlite, and I can successfully convert and interpret the converted model with newest tensorflow on computer.

However, when I try to use that model on Android phone , it can't be interpreted. Here is message from Android studio:
```
E/AndroidRuntime: FATAL EXCEPTION: main
    Process: android.example.com.tflitecamerademo, PID: 22917
    java.lang.RuntimeException: Unable to start activity ComponentInfo{android.example.com.tflitecamerademo/com.example.android.tflitecamerademo.CameraActivity}: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2856)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2931)
        at android.app.ActivityThread.-wrap11(Unknown Source:0)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1620)
        at android.os.Handler.dispatchMessage(Handler.java:105)
        at android.os.Looper.loop(Looper.java:176)
        at android.app.ActivityThread.main(ActivityThread.java:6701)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:246)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)
     Caused by: java.lang.IllegalArgumentException: ByteBuffer is not a valid flatbuffer model
        at org.tensorflow.lite.NativeInterpreterWrapper.createModelWithBuffer(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:59)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:188)
        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:176)
        at com.example.android.tflitecamerademo.ImageClassifier.<init>(ImageClassifier.java:106)
        at com.example.android.tflitecamerademo.Camera2BasicFragment.onActivityCreated(Camera2BasicFragment.java:303)
        at android.app.Fragment.performActivityCreated(Fragment.java:2620)
        at android.app.FragmentManagerImpl.moveToState(FragmentManager.java:1296)
        at android.app.FragmentManagerImpl.addAddedFragments(FragmentManager.java:2420)
        at android.app.FragmentManagerImpl.executeOpsTogether(FragmentManager.java:2199)
        at android.app.FragmentManagerImpl.removeRedundantOperationsAndExecute(FragmentManager.java:2153)
        at android.app.FragmentManagerImpl.execPendingActions(FragmentManager.java:2054)
        at android.app.FragmentManagerImpl.dispatchMoveToState(FragmentManager.java:3049)
        at android.app.FragmentManagerImpl.dispatchActivityCreated(FragmentManager.java:2996)
        at android.app.FragmentController.dispatchActivityCreated(FragmentController.java:178)
        at android.app.Activity.performCreateCommon(Activity.java:7044)
        at android.app.Activity.performCreate(Activity.java:7052)
        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1214)
        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2809)
        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2931)
        at android.app.ActivityThread.-wrap11(Unknown Source:0)
        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:1620)
        at android.os.Handler.dispatchMessage(Handler.java:105)
        at android.os.Looper.loop(Looper.java:176)
        at android.app.ActivityThread.main(ActivityThread.java:6701)
        at java.lang.reflect.Method.invoke(Native Method)
        at com.android.internal.os.Zygote$MethodAndArgsCaller.run(Zygote.java:246)
        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:783)
```
and this is part of  build.gradle:
```
repositories {
    maven {
        url 'https://google.bintray.com/tensorflow'
    }
    google()
}

dependencies {
    compile fileTree(dir: 'libs', include: ['*.jar'])
    androidTestCompile('com.android.support.test.espresso:espresso-core:2.2.2', {
        exclude group: 'com.android.support', module: 'support-annotations'
    })
    compile 'com.android.support:appcompat-v7:25.2.0'
    compile 'com.android.support.constraint:constraint-layout:1.1.0'
    compile 'com.android.support:design:25.2.0'
    compile 'com.android.support:support-annotations:25.3.1'
    compile 'com.android.support:support-v13:25.2.0'

    compile 'org.tensorflow:tensorflow-lite:0.0.0-nightly'

    testCompile 'junit:junit:4.12'
}

```
Based on what I have experienced, `ByteBuffer is not a valid flatbuffer model` may caused by unsupported ops in model.

So I delete all the mirror pad(tf.pad), and use 'SAME' padding in conv, this time the model can run on both desktop and Android phone.

Because the tflite I use on android phone is `org.tensorflow:tensorflow-lite:0.0.0-nightly`, it should be the newest and should support mirror pad, then why it can't be interpreted?

Any suggestion would be appreciated !"
25923,[performance] CPU is idle even when there are operations ready to be executed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.19.2- (@non-git)
- GCC/Compiler version (if compiling from source): 4.8.5
- CUDA/cuDNN version: No
- GPU model and memory: No


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I'm trying to feed tensors produces by a ParseExample OP when inference an exported graph to avoid 
serialization/deserialization tf record of input examples.
I have got the correct prediction results. But the performance is not consistent with my expectation.
There is several idle time in the timeline. And the feed tensors are not processing in parallel.

**Describe the expected behavior**
No CPU idle time in prediction. No ParseExample OP in inference should be faster.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
`saved_bundle.session->Run(run_options, input_tensor_pairs, {output_tensor_name}, {}, &output, nullptr);`
**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
chrome tracing results:
with ParseExample, no cpu idle during inference
![image](https://user-images.githubusercontent.com/32092715/53081857-a7216300-3536-11e9-807c-791c1f47f4c2.png)
[with_parse_example.json.txt](https://github.com/tensorflow/tensorflow/files/2883952/with_parse_example.json.txt)
without ParseExample, cpu idle during inference with red marks
![image](https://user-images.githubusercontent.com/32092715/53082111-231bab00-3537-11e9-922f-3ab8c2f5f743.png)
[without_parse_example.json.txt](https://github.com/tensorflow/tensorflow/files/2883961/without_parse_example.json.txt)

Any advices will be appreciated, Thanks"
25922,tflite_convert - ValueError: Invalid tensors 'input' were found.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Window10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 1.12
- Python version: 3.6.7
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No

**Describe the current behavior**
I want to convert mobilenet_v1_0.50_128 frozen graph using tflite_convert or toco to a TensorFlow Lite model, but I get the following error:
```
2019-02-20 16:09:37.352780: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
Traceback (most recent call last):
  File ""c:\users\alice\anaconda3\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)

  File ""c:\users\alice\anaconda3\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)

  File ""C:\Users\Alice\Anaconda3\Scripts\tflite_convert.exe\__main__.py"", line 9, in <module>

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)

  File ""c:\users\alice\anaconda3\lib\site-packages\absl\app.py"", line 300, in run
    _run_main(main, args)

  File ""c:\users\alice\anaconda3\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 122, in _convert_model
    converter = _get_toco_converter(flags)

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\lite\python\tflite_convert.py"", line 109, in _get_toco_converter
    return converter_fn(**converter_kwargs)

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\lite\python\lite.py"", line 387, in from_frozen_graph
    sess.graph, input_arrays)

  File ""c:\users\alice\anaconda3\lib\site-packages\tensorflow\lite\python\convert_saved_model.py"", line 189, in get_tensors_from_tensor_names
    "","".join(invalid_tensors)))

ValueError: Invalid tensors 'input' were found.
```

**Code to reproduce the issue**
I run the following command:
`tflite_convert --output_file=tflite_convert --output_file=F:/mobilenet_v1_0.50_128/frozen_graph.tflite --graph_def_file=F:/mobilenet_v1_0.50_128/frozen_graph.pb --input_arrays=input --output_arrays=MobilenetV1/Predictions/Reshape_1`
"
25920, undefined reference to `tensorflow::Tensor::ComputeFlatOuterDims,"ubuntu 18.04

create libtensorflow_cc.so and libtensorflow_framework.so in tensorflow1.12

Code:

`tensorflow::TTypes<float, 3>::Tensor boxes = outputs[0].flat_outer_dims<float,3>(); `

Error:

`In function `tensorflow::TTypes<float, 3ul, long>::Tensor tensorflow::Tensor::flat_outer_dims<float, 3ul>()':(.text._ZN10tensorflow6Tensor15flat_outer_dimsIfLm3EEENS_6TTypesIT_XT0_ElE6TensorEv[_ZN10tensorflow6Tensor15flat_outer_dimsIfLm3EEENS_6TTypesIT_XT0_ElE6TensorEv]+0x70): undefined reference to `tensorflow::Tensor::ComputeFlatOuterDims(absl::lts_2018_12_18::Span<long long const>, long long)'`

check libtensorflow_cc.so

link:

` nm /usr/local/lib/tensorflow/libtensorflow_cc.so| grep ComputeFlatOuterDims

0000000004348af0 T _ZN10tensorflow6Tensor20ComputeFlatOuterDimsEN4absl4SpanIKxEEx
`

thanks a lot!
"
25913,"glog macros are redefined by platform/logging.h, included by refcount.h","<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): `42c4f4ab6b53bce8639c203d7839d27eac11bd2f`
- Python version: 3.7
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 5.4.0-6ubuntu1~16.04.10
- CUDA/cuDNN version: 9.0 / 7
- GPU model and memory: 8x GTX 1080 Ti 12GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

```
In file included from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/platform/logging.h:25:0,
                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/lib/core/refcount.h:22,
                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/platform/tensor_coding.h:21,
                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/framework/resource_handle.h:19,
                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/framework/allocator.h:24,
                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/framework/tensor.h:22,
                 from /tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/public/session.h:24,
                 from bug.cc:2:
/tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include/tensorflow/core/platform/default/logging.h:95:0: warning: ""LOG"" redefined
 #define LOG(severity) _TF_LOG_##severity
 ^
In file included from bug.cc:1:0:
/usr/local/include/glog/logging.h:506:0: note: this is the location of the previous definition
 #define LOG(severity) COMPACT_GOOGLE_LOG_ ## severity.stream()
 ^
```

**Describe the expected behavior**

No leaking `platform/logging.h` and no those warnings about redefining `glog` macros.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```c++
#include <glog/logging.h>
#include ""tensorflow/core/public/session.h""

int main() {
    tensorflow::SessionOptions gpu_option;
}
```

```bash
export TF_INC_DIR=/tmp/tensorflow/execroot/org_tensorflow/bazel-out/k8-opt/genfiles/tensorflow/include
g++ bug.cc -std=c++11 -I$TF_INC_DIR -I$TF_INC_DIR/external/com_google_absl
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


I know TensorFlow has tried to avoid leaking the logging macros (#7480), but [`core/lib/core/refcount.h`](https://github.com/tensorflow/tensorflow/blob/42c4f4ab6b53bce8639c203d7839d27eac11bd2f/tensorflow/core/lib/core/refcount.h) is leaking them again. It seems that one way to fix it is to remove `#include ""tensorflow/core/platform/logging.h""` from the headers and add them back in the `.cc` files. However, as said in `core/lib/core/refcount.h`, those are ""Inlined routines, since these are performance critical"" thus cannot be moved to a `.cc` file. But I'm sure there must be a way to stop leaking the logging macros.

"
25912,Freezing a model with Exponential Moving Average gives different probabilities,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.9.0
- Python version: 2.7.12
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 9.0.176
- GPU model and memory:

I'm trying to freeze a model using Exponential Moving Average (EMA) variables for training and inference.

The freezing and inference code work normally, but the inferred probabilities are slightly different (about 0.05) with the ones from the original code. These results become identical (differnce < 1e-6) if I turn off the EMA in both models.

Am I doing the freezing incorrectly, or the freeze_graph tool is just unable to treat EMA?


Code I am using for freezing graph:

    from __future__ import print_function
    import tensorflow as tf
    from nets.inception_v3 import inception_v3, inception_v3_arg_scope
    from tensorflow.python.framework import graph_util
    import sys 
    slim = tf.contrib.slim

    checkpoint_file = '/my/model'

    with tf.Graph().as_default() as graph:

        images = tf.placeholder(shape=[None, 100, 221, 6], dtype=tf.float32, name = 'input')

        with slim.arg_scope(inception_v3_arg_scope()):
            logits, end_points = inception_v3(images, num_classes = 3, create_aux_logits = False, is_training = False)

        variables_to_restore = slim.get_variables_to_restore()

        MOVING_AVERAGE_DECAY = 0.9999
        variable_averages = tf.train.ExponentialMovingAverage(
            MOVING_AVERAGE_DECAY)
        for var in variables_to_restore:
            tf.add_to_collection(tf.GraphKeys.MOVING_AVERAGE_VARIABLES, var)
        variables_to_restore = variable_averages.variables_to_restore()        #This line is commented if EMA is turned off

        saver = tf.train.Saver(variables_to_restore)

        #Setup graph def
        input_graph_def = graph.as_graph_def()
        output_node_names = ""InceptionV3/Predictions/Reshape_1""
        output_graph_name = ""./frozen_inception_v3_new_100_221_ema.pb""

        with tf.Session() as sess:
            saver.restore(sess, checkpoint_file)

            #Exporting the graph
            print (""Exporting graph..."")
            output_graph_def = graph_util.convert_variables_to_constants(
                    sess,
                    input_graph_def,
                    output_node_names.split("",""))

            with tf.gfile.GFile(output_graph_name, ""wb"") as f:
                f.write(output_graph_def.SerializeToString())

This code is modified from https://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b . The EMA part is the same as the original code.

"
25904,benchmark_model from r1.13.0rc0 perform slower that r1.11.0 ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.01
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N.A.
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): r1.13.0rc0 and r1.11.0
- Python version: 2.7
- Bazel version (if compiling from source): 0.21.0 and 0.17.2
- GCC/Compiler version (if compiling from source): N.A.
- CUDA/cuDNN version: N.A.
- GPU model and memory: N.A.


I'm testing the performance of my mobilenetv2_ssdlite model using benchmark_model on my android device( android8.1. aarch64). I built benchmark_model with the bazel command in your readme files.

benchmark command:
./benchmark_model \
--graph=./mobilenetv2_ssdlite.tflite \
--use_nnapi=true \
--num_threads=1

**Result( the same device, the same model):
built from r1.13.0rc0**
Initialized session in 31.021ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=1 curr=4385333
Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=105224 curr=151561 min=87415 max=197387 avg=**116375** std=28964
Average inference timings in us: Warmup: 4.38533e+06, Init: 31021, no stats: 116375

**built from r1.**
Initialized session in 29.833ms
Running benchmark for 1 iterations
count=1 curr=4344146
Running benchmark for 50 iterations
count=50 first=88549 curr=87301 min=87271 max=104938 avg=**91616.2** std=4073
Average inference timings in us: Warmup: 4.34415e+06, Init: 29833, no stats: 91616.2

We can observe that the r1.13.0rc0 version is about 20% slower that the r1.11.0.
Similar result can be obtained while running other public ssdlite models.
Does anyone know why and how fix the problem?"
25903,Please release the Raspberry Pi Camera Follower Demo,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: https://youtu.be/FAMfy7izB6A?t=930


**Describe the documentation issue**
Andrew Selle showed off a cool demo, and said, ""I'm not an electrical engineer or mechanical engineer, so you can do this too."" It's been 10.5 months. Maybe he forgot to release the code so that we can do that too? He also mentioned in the comment section that it would be released as a demo.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I don't have access to the code in question
"
25901,Batch Normalization renorm with tensorflow_hub,"Hi,

I have observed an issue in Tensorflow hub and batch normalization. It seems that I cannot have a batch normalization layer with renorm in a module. Consider the following short script:
```
import tensorflow as tf
import tensorflow_hub as hub



def myfunc():
    input = tf.placeholder(tf.float32, [5, 10])
    output = tf.layers.batch_normalization(input, renorm = True)

    outputs = dict(default=output)
    inputs = dict(input_txt=input)
    hub.add_signature(inputs=inputs, outputs=outputs)

g_spec = hub.create_module_spec(myfunc)
```
which gives me the following error:

> Traceback (most recent call last):
  File ""..../codes/5/test.py"", line 16, in <module>
    g_spec = hub.create_module_spec(myfunc)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow_hub/native_module.py"", line 189, in create_module_spec
    if err: raise ValueError(err)
ValueError: A state-holding node x of a module's graph (e.g., a Variable op) must not be subject to a tf.colocate_with(y) constraint unless y is also a state-holding node.
Details: node 'batch_normalization/renorm_mean_weight' has op 'VarHandleOp', which counts as state-holding, but Operation.colocation_groups() == [b'loc:@batch_normalization/Read/ReadVariableOp']. 

The problem does NOT exist with ```renorm=False```.

Is there anything that I am missing? I have not found similar problem on Stackoverflow.

"
25899,BestExporter exporting multiple models.,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
https://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list#1.10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
1.10
- Python version:
3.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
Nvidia Tesla K80

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Multiple (8) models are exported in `export/best_exporter` directory.

**Describe the expected behavior**
BestExporter exports single best model in `export/best_exporter` directory.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
https://github.com/conversationai/conversationai-models/pull/246
https://github.com/conversationai/conversationai-models/blob/6b383cfa6d8f3c42fc09d7c9496069a13fcad48d/experiments/tf_trainer/common/model_trainer.py#L226

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[ml_job_job_id_tf_trainer_tf_cnn_civil_comments_glove_msushkov_20190215_163340__logs__2019-02-15T08-04.log](https://github.com/tensorflow/tensorflow/files/2881829/ml_job_job_id_tf_trainer_tf_cnn_civil_comments_glove_msushkov_20190215_163340__logs__2019-02-15T08-04.log)
"
25896,Issues while compiling tensorflow v1.12.0 with intel 19 compilers,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux release 7.6.1810 (Core)
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: compiling from source (will use pip for .whl)
- Bazel version (if compiling from source): 0.19.1
- GCC/Compiler version (if compiling from source): gcc 4.8.5 +  intel/19.0.0.117
- CUDA/cuDNN version: NA
- GPU model and memory: NA (CPU build)

while compiling tensorflow using 
` CC=icc bazel build -s --config=mkl --define=grpc_no_ares=true --copt=-xHOST //tensorflow/tools/pip_package:build_pip_package`

the build fails with intel compilers. Compilation error messages are attached - TF12_intel_log.txt


Compilation of tensorflow v12 was succesful with pure gcc compilers.
I am trying to build a TF version compiled with intel + mkl .

Please let me know if any further information is required.
[TF12_intel_log.txt](https://github.com/tensorflow/tensorflow/files/2880703/TF12_intel_log.txt)

As the issue is related to intel compilers, i have posted the issue at intel forum - https://software.intel.com/en-us/forums/intel-c-compiler/topic/805280
"
25895,keras ConvLSTM2d not working with eager execution,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0-dev20190130
- Python version: 3.6
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 10/7.4.2.24
- GPU model and memory: GeForce GTX 1050 Ti, 4096 MB

**Describe the current behavior**
While using Eager Execution keras ConvLSTM2D seems like not initialized.
After that we tried implement our model using gradient tape and it seemed fine but after loading checkpoint results was different than expected.
The second issue probably has something in common with the first, so i'm posting code to reproduce first one and logs. 

**Describe the expected behavior**
ConvLSTM2D works with Eager Execution

**Code to reproduce the issue**
```python
DATA_PATH = 'path/to/directory/with/grayscale/jpg/images""

import os
import tensorflow as tf
from tensorflow.keras import layers, Sequential

tf.enable_eager_execution()
filenames = [os.path.join(DATA_PATH, x) for x in sorted(os.listdir(DATA_PATH))]
tf_dataset = tf.data.Dataset.from_tensor_slices(filenames).map(lambda x: tf.image.decode_jpeg(tf.read_file(x), channels=0))
tf_dataset = tf_dataset.batch(10).batch(10)
tf_dataset = tf.data.Dataset.zip((tf_dataset, tf_dataset)).prefetch(tf.contrib.data.AUTOTUNE)

model =  Sequential([
    layers.ConvLSTM2D(filters=3, kernel_size=3, padding='same')
])
model.compile(optimizer=tf.train.AdamOptimizer(), loss='categorical_crossentropy', metrics=['accuracy'])
model.fit(tf_dataset)
```

**Other info / logs**
```
WARNING: Logging before flag parsing goes to stderr.
W0219 14:04:10.508448 140324682260608 training_utils.py:1249] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.

2019-02-19 14:04:13.426855: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.
         [[{{node convlstm1/convolution/ReadVariableOp}}]]
         [[training/TFOptimizer/gradients/convlstm1/while_grad/convlstm1/while_grad/LoopCond/_705/_52]]

2019-02-19 14:04:13.426894: W tensorflow/core/common_runtime/base_collective_executor.cc:203] BaseCollectiveExecutor::StartAbort Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.
         [[{{node convlstm1/convolution/ReadVariableOp}}]]

2019-02-19 14:04:13.426968: E tensorflow/core/common_runtime/process_function_library_runtime.cc:735] Component function execution failed: Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.
         [[{{node convlstm1/convolution/ReadVariableOp}}]]
         [[training/TFOptimizer/gradients/convlstm1/while_grad/convlstm1/while_grad/LoopCond/_705/_52]]

2019-02-19 14:04:13.427323: E tensorflow/core/common_runtime/process_function_library_runtime.cc:735] Component function execution failed: Failed precondition: Error while reading resource variable _AnonymousVar4 from Container: localhost. This could mean that the variable was uninitialized. Not found: Resource localhost/_AnonymousVar4/N10tensorflow3VarE does not exist.
         [[{{node convlstm1/convolution/ReadVariableOp}}]]
```"
25889,tensorflow r1.6 use Eigen or intel mkl for matrix operation?,"tensorflow r1.6 use Eigen or intel mkl for matrix operation?
and we user java api of tensorFlow 1.6 and try to optimize the time cost "
25886,[tf c++ api] Does `GraphExecutionState::OptimizeGraph` take up too much cpu during repeated `session->Run()`?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  3.18.6-2.el7.centos.x86_64
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.9.0
- Python version: 3.6.5
- Bazel version (if compiling from source): bazel-0.10.0
- GCC/Compiler version (if compiling from source): gcc-4.8.5

**Describe the current behavior**
I load tf model(protobuf) into c++ to do inference. The code roughly is:
```cpp
int TensorflowInfer::LoadModel() {
  tensorflow::Status session_status = tensorflow::NewSession(tensorflow::SessionOptions(), &session_pointer);
  infer_sess_.reset(session_pointer);

  //Read the pb file into the grapgdef
  tensorflow::GraphDef graphdef;
  tensorflow::Status status_load =
      tensorflow::ReadBinaryProto(tensorflow::Env::Default(), model_path, &graphdef);

  tensorflow::graph::SetDefaultDevice(device, &graphdef);

  // Add the graph to the session
  tensorflow::Status status_create = infer_sess_->Create(graphdef);
  return 0;
}

int TensorflowInfer::Infer(
    const tensorflow::Tensor &input_tensor,
    tensorflow::Tensor &output_tensor) {
  std::vector<tensorflow::Tensor> outputs;

  tensorflow::Status status = infer_sess_->Run({""input"", input_tensor}, {""output""}, {}, &outputs);

  output_tensor = outputs[0];

  return 0;
}
```
Then i run `TensorflowInfer::Infer()` with many different `input_tensor`, and make a profiling using `valgrind`, the result call graph is:
![image](https://user-images.githubusercontent.com/705582/53008720-f4d59700-3474-11e9-90d1-724002973865.png)

As this figure shown, while the intensive eigen computaion(left part in this figure) takes 22.64% cpu, the `GraphExecutionState::OptimizeGraph` takes as much as 9.96% cpu. 

The problem is:
1. Is graph optimization need take that much cpu? 
2. Is it possible that the session reuse one `OptimizeGraph` during many infer() runs?"
25885,Ability to hook Iterator initialisation while using estimators,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
I find it hard to track the epochs number while training with Estimators.
One possibility would be to hook the Dataset Iterator initialisation between epoch
combining the use of dataset.repeat(num_epochs) - (Assuming the Estimator is initialising the iterator between epochs) 
 
**Will this change the current api? How?**
No - just add another training hook.

**Who will benefit with this feature?**
Every developer / researcher that would like to track the number of training epochs in addition to
the training steps

**Any Other info.**
Other ways to track the epoch number should be fine as well"
25883,When the TF 2.0 stable edition releases ,I really want to know Anyone could tell me Thank You
25882,tf.image.random_jpeg_quality only products images of single jpeg quality,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Debian 9
- TensorFlow installed from (source or binary): pip install
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09
- Python version: 2.7.13
- CUDA/cuDNN version: 9.0 / 7.0.3
- GPU model and memory: GeForce GTX 1080 Ti, 10405 MB

**Describe the current behavior**
tf.image.random_jpeg_quality generates random jpeg quality on graph creation, which is then fixed.

**Describe the expected behavior**
tf.image.random_jpeg_quality generates random jpeg quality for each image/batch of images passed through it.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

img = np.random.randint(0, 256, (100, 200, 3), dtype=np.uint8)

tf_img = tf.placeholder(tf.uint8)
jpeg_augment = tf.image.random_jpeg_quality(tf_img,
                                            min_jpeg_quality=20,
                                            max_jpeg_quality=90)

sess_config = tf.ConfigProto()
sess_config.gpu_options.allow_growth = True

with tf.Session(config=sess_config) as sess:
    sess.run(tf.global_variables_initializer())
    results = []
    for i in range(25):
        augmented = sess.run([jpeg_augment], feed_dict={tf_img: img})[0]
        results.append(augmented)

results = np.array(results)
same_as_first = results == results[0, ...]
all_equal = np.all(same_as_first)
print('all_equal: {}'.format(all_equal))
assert not all_equal
```

The code causing this is located at:
https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/python/ops/image_ops_impl.py#L1634
"
25881,can't split error,"Traceback (most recent call last):
  File ""train.py"", line 82, in <module>
    tf.app.run(main)
  File ""/home/madono/.local/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 52, in main
    datasets = mnist.load_data()[:10000]
TypeError: 'ConditionalDataset' object is not subscriptable
"
25880,Migration Guide from Tensorflow Android to Tensorflow Lite,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>

**System information**
- TensorFlow version (you are using): Tensorflow Android 1.12.0
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
I'm currently using a custom frozen model in .pb format for real-time face detection with Tensorflow Android. Everything is working fine except that the model is quite heavy and my app stops working from time to time. Logcat shows that the ActivityManager force closes my app, which I think is because it is using too many resources despite running in a different thread. Tensorflow Lite seems to be a good solution with continuous support and better performance. I want to migrate my existing solution to TFLite but I can't find any clear documentation on how to do so. The TensorFlowInferenceInterface methods and parameters are very different from Interpreter in TFLite. 

I followed the TFLiteObjectDetectionAPIModel samples for both Tensorflow Android and TFLite but I can't follow on the differences as someone who is not familiar with Tensorflow. Please provide documentation on how to migrate especially when you are deprecating Tensorflow Android already.

**Will this change the current api? How?**
I think no if there is proper documentation.

**Who will benefit with this feature?**
Developers currently using Tensorflow Android with performance issues

**Any Other info.**
None"
25878,Invalid argument: No OpKernel was registered to support Op 'ListDiff' with these attrs.,
25877,AttributeError: 'NoneType' object has no attribute 'get_file',"<em>Please make sure that this is a bug. As per our [GitHub Policy]
when i run densenet, i got stuck into this bug.
`  File ""D:/Documents/GitHub/DenseNet-master/imagenet_inference.py"", line 26, in <module>
    print('Predicted:', decode_predictions(preds))
  File ""D:\Applications\Anaconda3\envs\keras\lib\site-packages\keras_applications\imagenet_utils.py"", line 224, in decode_predictions
    fpath = keras_utils.get_file(
AttributeError: 'NoneType' object has no attribute 'get_file'
`
**System information**
- Win10
- Python version:3.6"
25876,Does ConditionalAccumulator have any other reduction_type?,"![image](https://user-images.githubusercontent.com/29396595/52999122-ee88f000-345f-11e9-94bf-72550d222b1d.png)

When I use SyncReplicasOptimizer, I want to sum the gradients but not to mean them. The motivation to do this is, I want to train a big model with big batch_size other than mini batch_size. So SyncReplicasOptimizer has any other reduction_types? (For example sum, max...) "
25874,Passing variable length sentences to Tensorflow LSTM,"
I have a tensorflow LSTM model for predicting the sentiment. I build the model with the maximum sequence length 150. (Maximum number of words) While making predictions, i have written the code as below:

    batchSize = 32
    maxSeqLength = 150

    def getSentenceMatrix(sentence):
        arr = np.zeros([batchSize, maxSeqLength])
        sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')
        cleanedSentence = cleanSentences(sentence)
        cleanedSentence = ' '.join(cleanedSentence.split()[:150])
        split = cleanedSentence.split()
        for indexCounter,word in enumerate(split):
            try:
                sentenceMatrix[0,indexCounter] = wordsList.index(word)
            except ValueError:
                sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words
        return sentenceMatrix

    input_text = ""example data""
    inputMatrix = getSentenceMatrix(input_text)\

In the code i'm truncating my input text to 150 words and ignoring remaining data.Due to this my predictions are wrong.

    cleanedSentence = ' '.join(cleanedSentence.split()[:150])
I know that if we have lesser length than sequence length we can pad with zero's. What we need to do if we have more length.
Can anyone suggest me the best way to do this. Thanks in advance.
"
25870,Error installing/running Tensorflow-GPU version,"Unable to install/run tensorflow-gpu in my system.  Below error am getting.  I tried with Python 3.7, then downgraded it to py 3.6.8 and then to 3.6.5, but the end result is the same.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): installed using pip3
- TensorFlow version: GPU version 1.12.0
- Python version: 3.7.x, 3.6.8, 3.6.5
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: Quadro M1200
- GPU model and memory: NVIDIA 4 GB



**Describe the problem**
Traceback (most recent call last):
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""chatbot.py"", line 4, in 
import tensorflow as tf
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow__init__.py"", line 24, in 
from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python__init__.py"", line 49, in 
from tensorflow.python import pywrap_tensorflow
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in 
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in 
from tensorflow.python.pywrap_tensorflow_internal import *
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in 
_pywrap_tensorflow_internal = swig_import_helper()
File ""C:\ProgramData\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
_mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 242, in load_module
return load_dynamic(name, filename, file)
File ""C:\ProgramData\Anaconda3\lib\imp.py"", line 342, in load_dynamic
return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25866,CUDA-related crash when training simple network,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10, also tested on Ubuntu 18.04 on same machine
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0 and also 1.13.0rc2
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0 and also 10.0, cudnn 7.2 (appropriate cudnn versions for 9.0 and 10.0 respectively)
- GPU model and memory: GTX 1080 TI 11gb

**Describe the current behavior**

Depending on the batch size, during training there is eventually (After 1000 or so steps, sometimes a lot more steps with smaller batches) a CUDA-related crash, with one of a number of error messages. When the crash happens, sometimes the entire display on my computer goes black and resets.

This happens despite:
- No OOM errors on startup
- Using a bare-bones example CNN
- Trying with and without limiting the available memory with the option `tf.GPUOptions(per_process_gpu_memory_fraction=0.1, allow_growth=False)`
- Restarting the machine
- Reinstalling latest NVIDIA drivers
- Reinstalling CUDA
- Trying CUDA 9 (TF 1.12.0, conda env) and CUDA 10 (TF 1.13.0rc2, manual installation)

I'm beginning to suspect that this is a hardware issue but the GPU has never had any problems and can still run high intensity games for hours with no problems. There are also no abnormal temperature or other sensor readings in the diagnostics when this happens.

Is there anything I can do to narrow down whether this is a hardware problem?

**Describe the expected behavior**

No crash

**Code to reproduce the issue**
This code is taken from the [CNN tutorial](https://www.tensorflow.org/tutorials/estimators/cnn) with the exception of a line added to try limiting the GPU memory usage. The crash happens with or without this line.

For this particular example, a batch size of 50 was enough to consistently reproduce the error. The crash happens earlier with larger batch sizes however.

```
import tensorflow as tf
import numpy as np

tf.logging.set_verbosity(tf.logging.INFO)

def cnn_model_fn(features, labels, mode):
	""""""Model function for CNN.""""""
	# Input Layer
	input_layer = tf.reshape(features[""x""], [-1, 28, 28, 1])

	# Convolutional Layer #1
	conv1 = tf.layers.conv2d(
	  inputs=input_layer,
	  filters=32,
	  kernel_size=[5, 5],
	  padding=""same"",
	  activation=tf.nn.relu)

	# Pooling Layer #1
	pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)

	# Convolutional Layer #2 and Pooling Layer #2
	conv2 = tf.layers.conv2d(
	  inputs=pool1,
	  filters=64,
	  kernel_size=[5, 5],
	  padding=""same"",
	  activation=tf.nn.relu)
	pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)

	# Dense Layer
	pool2_flat = tf.reshape(pool2, [-1, 7 * 7 * 64])
	dense = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)
	dropout = tf.layers.dropout(
	  inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)

	# Logits Layer
	logits = tf.layers.dense(inputs=dropout, units=10)

	predictions = {
	  # Generate predictions (for PREDICT and EVAL mode)
	  ""classes"": tf.argmax(input=logits, axis=1),
	  # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
	  # `logging_hook`.
	  ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
	}

	if mode == tf.estimator.ModeKeys.PREDICT:
		return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)

	# Calculate Loss (for both TRAIN and EVAL modes)
	loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

	# Configure the Training Op (for TRAIN mode)
	if mode == tf.estimator.ModeKeys.TRAIN:
		optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)
		train_op = optimizer.minimize(
			loss=loss,
			global_step=tf.train.get_global_step())
		return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

	# Add evaluation metrics (for EVAL mode)
	eval_metric_ops = {
	  ""accuracy"": tf.metrics.accuracy(
		  labels=labels, predictions=predictions[""classes""])
	}
	return tf.estimator.EstimatorSpec(
	  mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)
	  
# Load training and eval data
((train_data, train_labels),
 (eval_data, eval_labels)) = tf.keras.datasets.mnist.load_data()

train_data = train_data/np.float32(255)
train_labels = train_labels.astype(np.int32)  # not required

eval_data = eval_data/np.float32(255)
eval_labels = eval_labels.astype(np.int32)  # not required

gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.1, allow_growth=False)
session_config = tf.ConfigProto(gpu_options=gpu_options)
run_config = tf.estimator.RunConfig(session_config=session_config)	

# Create the Estimator
mnist_classifier = tf.estimator.Estimator(
    model_fn=cnn_model_fn, model_dir=""./run"", config=run_config)
	
# Set up logging for predictions
tensors_to_log = {""probabilities"": ""softmax_tensor""}

# Train the model
train_input_fn = tf.estimator.inputs.numpy_input_fn(
    x={""x"": train_data},
    y=train_labels,
    batch_size=50,
    num_epochs=None,
    shuffle=True)

# train one step and display the probabilties
mnist_classifier.train(
    input_fn=train_input_fn,
    steps=10000000)

```

**Other info / logs**


*Example error message 1*
```
2019-02-21 16:43:26.973428: E tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered ::
2019-02-21 16:43:26.973442: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-02-21 16:43:26.984939: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
```

*Example error message 2*
```
2019-02-19 12:32:46.833634: E tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure ::
2019-02-19 12:32:46.833656: E tensorflow/stream_executor/cuda/cuda_event.cc:48] Error polling for event status: failed to query event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 12:32:46.849082: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:274] Unexpected Event status: 1
```

*Example error message 3*
```
2019-02-19 13:19:11.936586: E tensorflow/stream_executor/cuda/cuda_driver.cc:1011] could not synchronize on CUDA context: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure ::
2019-02-19 13:19:12.200064: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.204540: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.208523: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.213922: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.218206: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.223482: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.228027: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.233217: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
2019-02-19 13:19:12.237655: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 000001A6735E4ED0: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure
Traceback (most recent call last):
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1334, in _do_call
    return fn(*args)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""cnn.py"", line 102, in <module>
    steps=10000000)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 358, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1124, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1158, in _train_model_default
    saving_listeners)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow_estimator\python\estimator\estimator.py"", line 1407, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 676, in run
    run_metadata=run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1171, in run
    run_metadata=run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1270, in run
    raise six.reraise(*original_exc_info)
  File ""C:\Program Files\Python36\lib\site-packages\six.py"", line 693, in reraise
    raise value
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1255, in run
    return self._sess.run(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1327, in run
    run_metadata=run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\training\monitored_session.py"", line 1091, in run
    return self._sess.run(*args, **kwargs)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1328, in _do_run
    run_metadata)
  File ""C:\Program Files\Python36\lib\site-packages\tensorflow\python\client\session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: GPU sync failed
```

*Example Output and Error Message when running under cuda-memcheck on Ubuntu*
```
========= CUDA-MEMCHECK
INFO:tensorflow:Using config: {'_model_dir': './run', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff649578710>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}
WARNING:tensorflow:From /home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
WARNING:tensorflow:From /home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
INFO:tensorflow:Graph was finalized.
2019-02-20 21:57:36.622307: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-02-20 21:57:36.717892: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-20 21:57:36.719031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.6705
pciBusID: 0000:06:00.0
totalMemory: 10.91GiB freeMemory: 10.27GiB
2019-02-20 21:57:36.719054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0
2019-02-20 21:57:38.167083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-20 21:57:38.167111: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 
2019-02-20 21:57:38.167117: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N 
2019-02-20 21:57:38.167550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9930 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:06:00.0, compute capability: 6.1)
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:From /home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:804: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.
Instructions for updating:
To construct input pipelines, use the `tf.data` module.
INFO:tensorflow:Saving checkpoints for 0 into ./run/model.ckpt.
INFO:tensorflow:loss = 2.3112628, step = 0
INFO:tensorflow:global_step/sec: 1.52266
INFO:tensorflow:loss = 2.2921445, step = 100 (65.674 sec)
INFO:tensorflow:global_step/sec: 1.52365
INFO:tensorflow:loss = 2.2698283, step = 200 (65.633 sec)
INFO:tensorflow:global_step/sec: 1.52262
INFO:tensorflow:loss = 2.2548892, step = 300 (65.676 sec)
INFO:tensorflow:global_step/sec: 1.51757
INFO:tensorflow:loss = 2.2342505, step = 400 (65.895 sec)
2019-02-20 22:02:30.594297: E tensorflow/stream_executor/cuda/cuda_driver.cc:684] failed to enqueue async memset operation: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-02-20 22:02:30.618259: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x55f0692849d0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-02-20 22:02:30.618293: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x55f0692849d0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2019-02-20 22:02:30.618303: E tensorflow/stream_executor/event.cc:34] error destroying CUDA event in context 0x55f0692849d0: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
Traceback (most recent call last):
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1334, in _do_call
    return fn(*args)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1319, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1407, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.InternalError: cuDNN Backward Filter function launch failure : input shape([500,1,28,28]) filter shape([5,5,1,32])
	 [[{{node gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter}} = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/gradients/conv2d/Conv2D_grad/ShapeN-matshapes-1, gradients/conv2d/Relu_grad/ReluGrad, ^gradients/conv2d/BiasAdd_grad/BiasAddGrad)]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""cnn.py"", line 102, in <module>
    steps=10000000)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1241, in _train_model_default
    saving_listeners)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1471, in _train_with_estimator_spec
    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 671, in run
    run_metadata=run_metadata)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1156, in run
    run_metadata=run_metadata)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1255, in run
    raise six.reraise(*original_exc_info)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/six.py"", line 693, in reraise
    raise value
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1240, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1312, in run
    run_metadata=run_metadata)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py"", line 1076, in run
    return self._sess.run(*args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 929, in run
    run_metadata_ptr)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1152, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1328, in _do_run
    run_metadata)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1348, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InternalError: cuDNN Backward Filter function launch failure : input shape([500,1,28,28]) filter shape([5,5,1,32])
	 [[node gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter (defined at cnn.py:59)  = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/gradients/conv2d/Conv2D_grad/ShapeN-matshapes-1, gradients/conv2d/Relu_grad/ReluGrad, ^gradients/conv2d/BiasAdd_grad/BiasAddGrad)]]

Caused by op 'gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter', defined at:
  File ""cnn.py"", line 102, in <module>
    steps=10000000)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 354, in train
    loss = self._train_model(input_fn, hooks, saving_listeners)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1207, in _train_model
    return self._train_model_default(input_fn, hooks, saving_listeners)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1237, in _train_model_default
    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""cnn.py"", line 59, in cnn_model_fn
    global_step=tf.train.get_global_step())
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 400, in minimize
    grad_loss=grad_loss)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py"", line 519, in compute_gradients
    colocate_gradients_with_ops=colocate_gradients_with_ops)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 630, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 814, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 408, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 814, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_grad.py"", line 526, in _Conv2DGrad
    data_format=data_format)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1092, in conv2d_backprop_filter
    dilations=dilations, name=name)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1770, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'conv2d/Conv2D', defined at:
  File ""cnn.py"", line 102, in <module>
    steps=10000000)
[elided 3 identical lines from previous traceback]
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 1195, in _call_model_fn
    model_fn_results = self._model_fn(features=features, **kwargs)
  File ""cnn.py"", line 17, in cnn_model_fn
    activation=tf.nn.relu)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py"", line 417, in conv2d
    return layer.apply(inputs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 817, in apply
    return self.__call__(inputs, *args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 194, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 868, in __call__
    return self.conv_op(inp, filter)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 520, in __call__
    return self.call(inp, filter)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py"", line 204, in __call__
    name=self.name)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 957, in conv2d
    data_format=data_format, dilations=dilations, name=name)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/daniel/miniconda3/envs/sorter/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3274, in create_op
    op_def=op_def)

InternalError (see above for traceback): cuDNN Backward Filter function launch failure : input shape([500,1,28,28]) filter shape([5,5,1,32])
	 [[node gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter (defined at cnn.py:59)  = Conv2DBackpropFilter[T=DT_FLOAT, data_format=""NCHW"", dilations=[1, 1, 1, 1], padding=""SAME"", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=""/job:localhost/replica:0/task:0/device:GPU:0""](gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter-0-TransposeNHWCToNCHW-LayoutOptimizer, ConstantFolding/gradients/conv2d/Conv2D_grad/ShapeN-matshapes-1, gradients/conv2d/Relu_grad/ReluGrad, ^gradients/conv2d/BiasAdd_grad/BiasAddGrad)]]

========= ERROR SUMMARY: 0 errors
```"
25865,linking libcuda.so.1 in a Docker image at build time,"**System information**
- Host system Ubuntu 18.04
- TensorFlow r1.13 branch
- Installing from `nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04` base docker image
- Bazel 0.21.0
- GCC/Compiler version (if compiling from source):
- Cuda 10.0, cuDNN 7.4
- RTX 2080 Ti

**Describe the problem**
When building `//tensorflow/libtensorflow.so` in a RUN step of my Dockerfile, I get this error:

```
ERROR: /tensorflow/tensorflow/cc/BUILD:477:1: Linking of rule '//tensorflow/cc:ops/data_flow_ops_gen_cc' failed (Exit 1)
/usr/bin/ld: warning: libcuda.so.1, needed by bazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Sdata_Uflow_Uops_Ugen_Ucc___Utensorflow/libtensorflow_framework.so, not found (try using -rpath or -rpath-link)
<snip>
Target //tensorflow:libtensorflow.so failed to build
ERROR: /tensorflow/tensorflow/cc/BUILD:477:1 Linking of rule '//tensorflow/cc:ops/nn_ops_gen_cc' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command
  (cd /root/.cache/bazel/_bazel_root/68a62076e91007a7908bc42a32e4cff9/execroot/org_tensorflow && \
  exec env - \
    PATH=/bin:/usr/bin \
    PWD=/proc/self/cwd \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -o bazel-out/host/bin/tensorflow/cc/ops/nn_ops_gen_cc '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U_S_Stensorflow_Scc_Cops_Snn_Uops_Ugen_Ucc___Utensorflow' '-Wl,-rpath,$ORIGIN/../../../_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib' -Lbazel-out/host/bin/_solib_local/_U_S_Stensorflow_Scc_Cops_Snn_Uops_Ugen_Ucc___Utensorflow -Lbazel-out/host/bin/_solib_local/_U@local_Uconfig_Ucuda_S_Scuda_Ccudart___Uexternal_Slocal_Uconfig_Ucuda_Scuda_Scuda_Slib -Wl,-ldl '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..,-rpath,$ORIGIN/../..' -pthread -Wl,-rpath,../local_config_cuda/cuda/lib64 -Wl,-rpath,../local_config_cuda/cuda/extras/CUPTI/lib64 -Wl,-S -Wl,-no-as-needed -pie -Wl,-z,relro,-z,now '-Wl,--build-id=md5' '-Wl,--hash-style=gnu' -no-canonical-prefixes -fno-canonical-system-headers -B/usr/bin -Wl,--gc-sections -Wl,@bazel-out/host/bin/tensorflow/cc/ops/nn_ops_gen_cc-2.params)
Execution platform: @bazel_tools//platforms:host_platform
INFO: Elapsed time: 229.527s, Critical Path: 139.75s
INFO: 2250 processes: 2250 local.
```

This only occurs when trying to build **in a RUN step**. If skip buliding and shell into the container, it builds to completion. There are several similar issues on github, two of the most similar are:

https://github.com/tensorflow/tensorflow/issues/16184
https://github.com/tensorflow/tensorflow/issues/14573
Particularly https://github.com/tensorflow/tensorflow/issues/14573#issuecomment-362424509

I have tried every variation of the advice given in that thread, as well as cribbed lines from this Dockerfile that is doing a similar thing:
https://github.com/gunan/tensorflow-docker/blob/master/gpu-devel/Dockerfile.ubuntu#L64

Yet, I'm still getting this message. 

I don't know what exactly nvidia is doing with its cuda libraries, but I think they are not in all the same places at buildtime and runtime. I discovered this discrepancy accidentally with `RUN find / | grep libcuda.so.1` in my Dockerfile:

```
/usr/local/cuda-10.0/compat/libcuda.so.1
```
This is the only place that libcuda.so.1 exists at build time. However, even if I symlink from `/usr/local/cuda-10.0/compat/libcuda.so.1
` to  a place that is definitely in `LD_LIBRARY_PATH`, _and_ make sure to pass `action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH}`, I _still_ get this error. 

I'm completely stumped at this point.

To reproduce this you can use the dockerfile attached at the bottom of this post.

Build in the container - works:

```
docker build -f test.dockerfile -t masontest . && docker run --vm=nvidia --rm masontest 
cd /tensorflow
RUN bazel build \
 --verbose_failures \
 --spawn_strategy=standalone \
    --genrule_strategy=standalone \
    --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
    //tensorflow:libtensorflow.so
```
Uncomment the bazel command - the image fails to build with the linker error above.

```
FROM nvidia/cuda:10.0-cudnn7-devel-ubuntu16.04
# TF docker (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/dockerfiles/dockerfiles/devel-gpu.Dockerfile)
# uses cuda:10.0-base-ubuntu16.04, but we need full sources 
# because later in this docker we are going to compile Tensorflow from scratch
# using bazel.

# so we aren't prompted to set up the keyboard and other nonsense
ENV DEBIAN_FRONTEND noninteractive 

RUN set -eux; \
    apt-get update && apt-get install -y --no-install-recommends \
    autoconf \
    automake \
    build-essential \
    ca-certificates \
    g++ \
    gcc \
    git \
    # these were needed by the mnistCUDNN demo
    libfreeimage3 \
    libfreeimage-dev \
    make \
    # bazel needs python2 https://github.com/tensorflow/tensorflow/issues/15618
    python \
    # tf needs swig
    swig \
    unzip \
    wget \
    zlib1g-dev; \
    apt-get clean; \
    rm -rf /var/lib/apt/lists/*

# Tensorflow pukes when you try this with 0.22.0
ENV BAZEL_VERSION 0.21.0
WORKDIR /
RUN mkdir /bazel && \
    cd /bazel && \
    wget https://github.com/bazelbuild/bazel/releases/download/$BAZEL_VERSION/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \
    # curl -fSsL -o /bazel/LICENSE.txt https://raw.githubusercontent.com/bazelbuild/bazel/master/LICENSE.txt && \
    chmod +x bazel-*.sh && \
    ./bazel-$BAZEL_VERSION-installer-linux-x86_64.sh && \
    cd / && \
    rm -f /bazel/bazel-$BAZEL_VERSION-installer-linux-x86_64.sh

RUN echo $(g++ --version)
RUN echo $(bazel version)

### TENSORFLOW 
ENV TENSORFLOW_VERSION r1.13
RUN git clone https://github.com/tensorflow/tensorflow.git && \
    cd tensorflow && \
    git checkout ${TENSORFLOW_VERSION}

ENV PYTHON_LIB_PATH /usr/local/lib/python2.7/dist-packages
ENV PYTHONPATH /tensorflow/lib
ENV PYTHON_ARG /tensorflow/lib

ENV GCC_HOST_COMPILER_PATH /usr/bin/gcc
ENV CC_OPT_FLAGS=""-march=native""
ENV CUDA_TOOLKIT_PATH /usr/local/cuda
ENV CUDNN_INSTALL_PATH /usr/lib/x86_64-linux-gnu
ENV TF_CUDA_COMPUTE_CAPABILITIES 7.0 
ENV TF_CUDNN_VERSION 7
ENV TF_CUDA_VERSION 10.0
ENV TF_NEED_GCP 0
ENV TF_ENABLE_XLA 0
ENV TF_NEED_OPENCL 0
ENV TF_NEED_CUDA 1
ENV TF_CUDA_VERSION 10.0.0
ENV TF_NEED_HDFS 0
ENV TF_NEED_TENSORRT 0
ENV TF_NCCL_VERSION 2
ENV TF_BUILD_BRANCH r1.13
ENV TF_NEED_ROCM 0
ENV TF_NEED_OPENCL_SYCL 0
ENV TMP /tmp/tensorflow
RUN echo ""startup --batch"" >>/etc/bazel.bazelrc

ENV LD_LIBRARY_PATH=""/usr/local/lib:/usr/local/cuda/lib64:/usr/local/cuda/lib64/stubs:${LD_LIBRARY_PATH}""

RUN find / | grep  libcuda.so.1
WORKDIR /tensorflow
RUN yes '' | ./configure
# https://github.com/gunan/tensorflow-docker/blob/master/gpu-devel/Dockerfile.ubuntu#L64
# Is this ln -s is necessary when building? Doesn't seem to help, and the actual libcuda ... https://github.com/tensorflow/tensorflow/issues/14573#issuecomment-362424509
# Similarly, we need to workaround sandboxing issues: https://github.com/bazelbuild/bazel/issues/418
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1
# RUN bazel build \
#  --verbose_failures \
#  --spawn_strategy=standalone \
#     --genrule_strategy=standalone \
#     --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
#     //tensorflow:libtensorflow.so
```"
25862,Can't detected GPU using pycharm CUDA_ERROR_NO_DEVICE,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 1.12.0
- Python version:3.6.8
- Installed using virtualenv? pip? conda?:pip
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 9.0   7.4.2
- GPU model and memory: GeForce GTX 1070 8gb

When running on command line, it seems my graphics card can be detected, but when running in pycharm, i'm getting failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected

![image](https://user-images.githubusercontent.com/35086049/52987994-242bcb80-33c3-11e9-9f5d-6c791f7525f4.png)

![image](https://user-images.githubusercontent.com/35086049/52988006-3443ab00-33c3-11e9-8327-aa1db2145a85.png)


![image](https://user-images.githubusercontent.com/35086049/52988031-56d5c400-33c3-11e9-87b8-31df048c4bcd.png)

I've searched the internet and can't find a solution. Thanks for helping!"
25858,Tensorflow Parameter Server Hangs when doing distributed training with Estimator,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:1.5, 1.8, 1.12 all the same results
- Doc Link:  https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig , https://www.tensorflow.org/guide/distribute_strategy#example_with_estimator_api


**Describe the documentation issue**
Is parameter server expected to be killed after the training is done when using tf.estimator.Estimator for distributed training ? or it is expected behavior that `ps` hangs forever ?

I am trying simple mnist example on localhost to try to get distributed training with estimator work but not able to do so.

Here is the complete code (code I downloaded and modified from https://github.com/yu-iskw/tensorflow-serving-example/blob/master/python/train/mnist_custom_estimator.py). 
```python

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import simplejson 
import numpy as np
import tensorflow as tf

tf.logging.set_verbosity(tf.logging.INFO)

tf.app.flags.DEFINE_integer('steps', 100, 'The number of steps to train a model')
tf.app.flags.DEFINE_string('job_name', 'master', 'job_name')
tf.app.flags.DEFINE_string('task_index', '0', 'task_index')
tf.app.flags.DEFINE_string('model_dir', './models/ckpt/', 'Dir to save a model and checkpoints')
FLAGS = tf.app.flags.FLAGS

INPUT_FEATURE = 'image'
NUM_CLASSES = 10


def model_fn(features, labels, mode):
    # Input Layer
    input_layer = features[INPUT_FEATURE]
    # Logits layer
    logits = tf.layers.dense(inputs=input_layer, units=NUM_CLASSES)

    predictions = {
        # Generate predictions (for PREDICT and EVAL mode)
        ""classes"": tf.argmax(input=logits, axis=1),
        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the
        # `logging_hook`.
        ""probabilities"": tf.nn.softmax(logits, name=""softmax_tensor"")
    }

    # PREDICT mode
    if mode == tf.estimator.ModeKeys.PREDICT:
        return tf.estimator.EstimatorSpec(
            mode=mode,
            predictions=predictions,
            export_outputs={
                'predict': tf.estimator.export.PredictOutput(predictions)
            })

    # Calculate Loss (for both TRAIN and EVAL modes)
    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)

    # Configure the Training Op (for TRAIN mode)
    if mode == tf.estimator.ModeKeys.TRAIN:
        optimizer = tf.train.AdamOptimizer()
        train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)

    # Add evaluation metrics (for EVAL mode)
    eval_metric_ops = {
        ""accuracy"": tf.metrics.accuracy(labels=labels, predictions=predictions[""classes""])
    }
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)


def main(_):
    # Load training and eval data
    mnist = tf.contrib.learn.datasets.load_dataset(""mnist"")
    train_data = mnist.train.images  # Returns np.array
    train_labels = np.asarray(mnist.train.labels, dtype=np.int32)
    eval_data = mnist.test.images  # Returns np.array
    eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)

    # reshape images
    # To have input as an image, we reshape images beforehand.
    train_data = train_data.reshape(train_data.shape[0], 28 * 28)
    eval_data = eval_data.reshape(eval_data.shape[0], 28 * 28)

    # Create the Estimator
    training_config = tf.estimator.RunConfig(
        model_dir=FLAGS.model_dir,
        save_summary_steps=20,
        save_checkpoints_steps=20)
    classifier = tf.estimator.Estimator(
        model_fn=model_fn,
        model_dir=FLAGS.model_dir,
        config=training_config)

    # Set up logging for predictions
    # Log the values in the ""Softmax"" tensor with label ""probabilities""
    tensors_to_log = {""probabilities"": ""softmax_tensor""}
    logging_hook = tf.train.LoggingTensorHook(tensors=tensors_to_log, every_n_iter=50)

    # Train the model
    train_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={INPUT_FEATURE: train_data},
        y=train_labels,
        batch_size=FLAGS.steps,
        num_epochs=1,
        shuffle=True)

    # Evaluate the model and print results
    eval_input_fn = tf.estimator.inputs.numpy_input_fn(
        x={INPUT_FEATURE: eval_data},
        y=eval_labels,
        num_epochs=1,
        shuffle=False)
    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn)
    # setup eval spec evaluating ever n seconds
    eval_spec = tf.estimator.EvalSpec(input_fn = eval_input_fn)
    tf.estimator.train_and_evaluate(classifier, train_spec, eval_spec)
def make_tf_training_config(args):
    """"""
    Returns TF_CONFIG that can be used to set the environment variable necessary for distributed training
    See https://github.com/clusterone/clusterone-tutorials/blob/master/tf-estimator/mnist.py
    """"""
    worker_hosts = ['localhost:2222']
    ps_hosts = ['localhost:2224']
    tf_config = {
        'task': {
            'type': FLAGS.job_name,
            'index': FLAGS.task_index
        },
        'cluster': {
            'master': [worker_hosts[0]],
            'ps': ps_hosts
        },
        'environment': 'cloud'
    }

    return tf_config

import os
if __name__ == ""__main__"":
    print(""@@@ Version: {}"".format(tf.__version__))
    tf_config = make_tf_training_config(None)
    os.environ['TF_CONFIG'] = simplejson.dumps(tf_config)
    tf.app.run()
```

Here is how I launch one master job and one ps job:

```bash
# launch master worker job
python test.py 
```

```bash
# launch ps job
python test.py --job_name ps
```

Here is the job log for master worker job (only last few lines as this job succeeded and exit):
```bash
INFO:tensorflow:Finished evaluation at 2019-02-19-01:40:18
INFO:tensorflow:Saving dict for global step 3301: accuracy = 0.9918, global_step = 3301, loss = 0.025573652
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 3301: ./models/ckpt/model.ckpt-3301
INFO:tensorflow:Loss for final step: 0.02294134.
```
Here is the complete job log for ps job:

```bash
@@@ Version: 1.12.0
WARNING:tensorflow:From test.py:118: load_dataset (from tensorflow.contrib.learn.python.learn.datasets) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/__init__.py:80: load_mnist (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:300: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.
Instructions for updating:
Please write your own downloading logic.
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST-data/train-images-idx3-ubyte.gz
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use tf.data to implement this functionality.
Extracting MNIST-data/train-labels-idx1-ubyte.gz
Extracting MNIST-data/t10k-images-idx3-ubyte.gz
Extracting MNIST-data/t10k-labels-idx1-ubyte.gz
WARNING:tensorflow:From /Users/hjing/miniconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.
Instructions for updating:
Please use alternatives such as official/mnist/dataset.py from tensorflow/models.
INFO:tensorflow:TF_CONFIG environment variable: {u'environment': u'cloud', u'cluster': {u'ps': [u'localhost:2224'], u'master': [u'localhost:2222']}, u'task': {u'index': u'0', u'type': u'ps'}}
INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': device_filters: ""/job:ps""
device_filters: ""/job:worker""
device_filters: ""/job:master""
allow_soft_placement: true
graph_options {
  rewrite_options {
    meta_optimizer_iterations: ONE
  }
}
, '_keep_checkpoint_max': 5, '_task_type': u'ps', '_train_distribute': None, '_is_chief': False, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x127a47550>, '_model_dir': './models/ckpt/', '_protocol': None, '_save_checkpoints_steps': 20, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 1, '_tf_random_seed': None, '_save_summary_steps': 20, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 1, '_master': u'grpc://localhost:2224'}
INFO:tensorflow:Not using Distribute Coordinator.
INFO:tensorflow:Start Tensorflow server.
2019-02-18 17:38:15.844761: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-18 17:38:15.846148: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}
2019-02-18 17:38:15.846171: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2224}
2019-02-18 17:38:15.846704: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:2224

```
And the ps hangs forever. 

I would expect estimator, being such a high-level API should handle closing the parameter server when training is done. If that is not the case, it should be clear in the documentation. 

Thanks !

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25857,Add lower() to string operations,"**System information**
- TensorFlow version (you are using):  1.10 and 1.12
- Are you willing to contribute it (Yes/No): No

**Describe the feature and the current behavior/state.**
Add the string `lower()` function to the tf.string class.
Current state has no way to lower case input strings. This requires it to be done at dataset build time which reduces dataset flexibility

**Will this change the current api? How?**
Would add extra functionality

**Who will benefit with this feature?**
All users doing natural language processing

**Any Other info.**
"
25856,freeze_graph unable to initialize local_variables,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):

Yes, I discovered this bug when running custom code.  However, I am attaching an example that illustrates the issue.

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Version 10.14.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Tensorflow-cpu installed via PIP
- TensorFlow version (use command below): 1.12
- Python version: 1.12
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

When freezing a graph with a local variable, freeze_graph has an error stating ""Attempting to use uninitialized value..."".  The local variable in question was initialized via:

```
	with tf.variable_scope(tf.get_variable_scope(),reuse=tf.AUTO_REUSE):
		b_init = tf.constant(10.0, shape=[2, 1], dtype=""float32"",name = 'bi')
		b = tf.get_variable('b',initializer=b_init,collections=[tf.GraphKeys.LOCAL_VARIABLES])
```

I'm able to create a saved model and run the saved model.  However, I'm trying to freeze another graph for optimization.  This error will go away if I remove the 'LOCAL_VARIABLES' flag.  However, this variable then becomes global, which causes an issue with reloading my checkpoint (Tensorflow is unable to find the variable in the checkpoint).

**Describe the expected behavior**

I'd expect freeze_graph to initialize 'b' using 'b_init'.

**Code to reproduce the issue**

```
import os, sys, json
import tensorflow as tf
from tensorflow.python.lib.io import file_io

from tensorflow.core.framework import variable_pb2
from tensorflow.python.framework import ops
from tensorflow.python.ops import variables
from tensorflow.python.framework.ops import register_proto_function

from tensorflow.python.saved_model import tag_constants
from tensorflow.python.tools import freeze_graph
from tensorflow.python import ops
from tensorflow.tools.graph_transforms import TransformGraph

#flags
tf.app.flags.DEFINE_integer('model_version',1,'Models version number.')
tf.app.flags.DEFINE_string('export_model_dir','../model_batch/versions', 'Directory where model will be exported to')

FLAGS = tf.app.flags.FLAGS

def main(_):
	''' main function'''
	a = tf.placeholder(dtype = tf.float32, shape = [2,1])

	with tf.variable_scope(tf.get_variable_scope(),reuse=tf.AUTO_REUSE):
		b_init = tf.constant(10.0, shape=[2, 1], dtype=""float32"",name = 'bi')
		b = tf.get_variable('b',initializer=b_init,collections=[tf.GraphKeys.LOCAL_VARIABLES])

	b = tf.assign(b,a)
	c = []

	for d in range(5):
		b = b * 1.1

	c.append(b)
	c = tf.identity(c,name = 'c')

	init = tf.group(tf.global_variables_initializer(),
                   tf.local_variables_initializer())

	with tf.Session() as sess:

		#init
		sess.run(init)
		print(tf.get_default_graph().get_collection(tf.GraphKeys.LOCAL_VARIABLES))

		#create saved model builder class
		export_path_base = FLAGS.export_model_dir
		export_path = os.path.join(
			tf.compat.as_bytes(export_path_base),
			tf.compat.as_bytes(str(FLAGS.model_version)))

		if tf.gfile.Exists(export_path):
			print ('Removing previous artifacts')
			tf.gfile.DeleteRecursively(export_path)

		#inputs
		tensor_info_a  = tf.saved_model.utils.build_tensor_info(a)

		#outputs
		tensor_info_c = tf.saved_model.utils.build_tensor_info(c)

		print('Exporting trained model to', export_path)
		builder = tf.saved_model.builder.SavedModelBuilder(export_path)

		#define signatures
		prediction_signature = (
			tf.saved_model.signature_def_utils.build_signature_def(
				inputs={'cameras': tensor_info_a},
				outputs = {'depthmap' : tensor_info_c},
				method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))

		builder.add_meta_graph_and_variables(
			sess, [tf.saved_model.tag_constants.SERVING],
			signature_def_map = {'predict_batch': prediction_signature})

		#export model
		builder.save(as_text=True)

		writer = tf.summary.FileWriter(""output_batch"", sess.graph)
		writer.close()

	#load graph from saved model
	print ('Freezing graph')
	initializer_nodes = ''
	output_node_names = 'c'
	saved_model_dir = os.path.join(FLAGS.export_model_dir,str(FLAGS.model_version))
	output_graph_filename = os.path.join(saved_model_dir,'frozen_graph.pb')

	freeze_graph.freeze_graph(
		input_saved_model_dir=saved_model_dir,
		output_graph=output_graph_filename,
		saved_model_tags = tag_constants.SERVING,
		output_node_names=output_node_names,
		initializer_nodes=initializer_nodes,
		input_graph=None,
		input_saver=False,
		input_binary=False,
		input_checkpoint=None,
		restore_op_name=None,
		filename_tensor_name=None,
		clear_devices=False)

if __name__ == '__main__':
	tf.app.run()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached."
25854,cuda10+tensorflow1.12 on linux build failed,"

**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04

- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Python version: 3.5.2
- Installed using virtualenv? pip? conda?: python3 from system
- Bazel version (if compiling from source): 0.18, 0.17
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: cuda10.0, cuDNN 7.4.2
- GPU model and memory: Nvidia GTX1070
Build failed with
```
 bazel build --config=opt --config=cuda --cxxopt=-D_GLIBCXX_USE_CXX11_ABI=0 --copt=-mavx --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mfma --copt=-mfpmath=both //tensorflow/tools/pip_package:build_pip_package --verbose_failures

WARNING: Processed legacy workspace file /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tools/bazel.rc. This file will not be processed in the next release of Bazel. Please read https://github.com/bazelbuild/bazel/issues/6319 for further information, including how to upgrade.
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
DEBUG: /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
WARNING: /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/grpc/BUILD:1992:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/external/grpc/bazel/grpc_build_system.bzl:172:12
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immediately.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorflow/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/seq2seq/BUILD:23:1: in py_library rule //tensorflow/contrib/seq2seq:seq2seq_py: target '//tensorflow/contrib/seq2seq:seq2seq_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:230:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:filtering_postprocessor' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/timeseries/python/timeseries/state_space_models/BUILD:73:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter: target '//tensorflow/contrib/timeseries/python/timeseries/state_space_models:kalman_filter' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/timeseries/python/timeseries/BUILD:354:1: in py_library rule //tensorflow/contrib/timeseries/python/timeseries:ar_model: target '//tensorflow/contrib/timeseries/python/timeseries:ar_model' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: target '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/contrib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded).
INFO: Found 1 target...
ERROR: /home/bernard/opt/cuda_test/cuda10/tensorflow-1.12.0/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Aborted): bash failed: error executing command 
  (cd /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/home/bernard/opt/cuda_test/cuda10/cuda \
    CUDNN_INSTALL_PATH=/home/bernard/opt/cuda_test/cuda10/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/home/bernard/opt/cuda_test/cuda10/opencv/lib:/home/bernard/opt/cuda_test/cuda10/openmpi/lib:/home/bernard/opt/cuda_test/cuda10/lib:/home/bernard/opt/cuda_test/cuda10/cuda/lib64:/home/bernard/opt/cuda_test/cuda10/cuda/extras/CUPTI/lib64:/home/bernard/opt/cuda_test/cuda10/lib::/home/bernard/opt/cuda_test/cuda10/TensorRT/lib \
    NCCL_HDR_PATH=/usr/local/cuda-9.2/targets/x86_64-linux/lib/../include \
    NCCL_INSTALL_PATH=/usr/local/cuda-9.2/targets/x86_64-linux/lib \
    PATH=/home/bernard/opt/cuda_test/cuda10/opencv/bin:/home/bernard/opt/cuda_test/cuda10/openmpi/bin:/home/bernard/opt/cuda_test/cuda10/cuda/bin:/home/bernard/opt/cuda_test/cuda10/bin:/opt/openmpi-cuda/bin:/usr/local/cuda/bin:/home/bernard/bin:/home/bernard/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/bernard/opt/qt5/bin:/home/bernard/opt/opencv/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages \
    TENSORRT_INSTALL_PATH=/home/bernard/opt/cuda_test/cuda10/TensorRT/targets/x86_64-linux-gnu/lib \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
    TF_TENSORRT_VERSION=5.0.2 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/k8-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py'): bash failed: error executing command 
  (cd /home/bernard/.cache/bazel/_bazel_bernard/8b714e31638c9283b6c7060cf778d169/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/home/bernard/opt/cuda_test/cuda10/cuda \
    CUDNN_INSTALL_PATH=/home/bernard/opt/cuda_test/cuda10/cuda \
    GCC_HOST_COMPILER_PATH=/usr/bin/gcc \
    LD_LIBRARY_PATH=/home/bernard/opt/cuda_test/cuda10/opencv/lib:/home/bernard/opt/cuda_test/cuda10/openmpi/lib:/home/bernard/opt/cuda_test/cuda10/lib:/home/bernard/opt/cuda_test/cuda10/cuda/lib64:/home/bernard/opt/cuda_test/cuda10/cuda/extras/CUPTI/lib64:/home/bernard/opt/cuda_test/cuda10/lib::/home/bernard/opt/cuda_test/cuda10/TensorRT/lib \
    NCCL_HDR_PATH=/usr/local/cuda-9.2/targets/x86_64-linux/lib/../include \
    NCCL_INSTALL_PATH=/usr/local/cuda-9.2/targets/x86_64-linux/lib \
    PATH=/home/bernard/opt/cuda_test/cuda10/opencv/bin:/home/bernard/opt/cuda_test/cuda10/openmpi/bin:/home/bernard/opt/cuda_test/cuda10/cuda/bin:/home/bernard/opt/cuda_test/cuda10/bin:/opt/openmpi-cuda/bin:/usr/local/cuda/bin:/home/bernard/bin:/home/bernard/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin:/home/bernard/opt/qt5/bin:/home/bernard/opt/opencv/bin \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/home/bernard/opt/cuda_test/cuda10/lib/python3.5/site-packages \
    TENSORRT_INSTALL_PATH=/home/bernard/opt/cuda_test/cuda10/TensorRT/targets/x86_64-linux-gnu/lib \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.1 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_NEED_CUDA=1 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
    TF_TENSORRT_VERSION=5.0.2 \
  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/k8-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py')
2019-02-18 10:36:50.619656: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr 
/bin/bash: line 1: 27745 Aborted                 bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1 --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/k8-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/k8-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/k8-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2.103s, Critical Path: 0.66s
INFO: 0 processes.
FAILED: Build did NOT complete successfully

```
note: Build for tensorflow 1.13 rc2 with the same build options and with Bazel-0.19.2 was successful
But if build  tensorflow 1.12 with bazel 0.19 or above it complains that cuda doesn't have a config file or something, it was related to the first warning above."
25852,reset_states raising 'NoneType' object is not subscriptable,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X 10.14.3
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.13.0-dev20190215
- Python version: 3.7


**Describe the current behavior**
```python
import tensorflow as tf

import tensorflow.keras as keras 

rnn=keras.layers.SimpleRNN(1, stateful=True)

rnn.reset_states()
```

causes

```
TypeError                                 Traceback (most recent call last)
<ipython-input-8-271e0ab7f330> in <module>
      5 rnn=keras.layers.SimpleRNN(1, stateful=True)
      6 
----> 7 rnn.reset_states()

/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py in reset_states(self, states)
    889       batch_size = self.input_spec[0].shape[1]
    890     else:
--> 891       batch_size = self.input_spec[0].shape[0]
    892     if not batch_size:
    893       raise ValueError('If a RNN is stateful, it needs to know '

TypeError: 'NoneType' object is not subscriptable
```

**Describe the expected behavior**

A meaningful error message is generated with instructions on why this doesn't work."
25848,tf.test.gpu_device_name() allocated all GPU memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
yes

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 16.04

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
no 

- TensorFlow installed from (source or binary):
binary

- TensorFlow version (use command below):
1.4.1

- Python version:
3.6

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):

- CUDA/cuDNN version:
8.0

- GPU model and memory:
1050Ti / 4Gb


**Describe the current behavior**
My code was 'checking for GPU' by calling tf.test.gpu_device_name()
Then it would go to set GPUoptions to allow_growth.
It caused tf to allocate full GPU memory for the process.
If I remove tf.test.gpu_device_name() then allow_growth works as expected.

**Describe the expected behavior**
tf.test.gpu_device_name() to NOT allocate GPU memory. Only starting a session should.

**Code to reproduce the issue**
-

**Other info / logs**
-
"
25847,Error after installing tensorflow dll,"hello 
I am facing this error just after installing tensorflow and try to use it 
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Abeer\AppData\Roaming\Python\Python36\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Abeer\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\__init__.py"", line 59, in <module>
    from tensorflow.core.framework.graph_pb2 import *
  File ""C:\Users\Abeer\AppData\Roaming\Python\Python36\site-packages\tensorflow\core\framework\graph_pb2.py"", line 6, in <module>
    from google.protobuf import descriptor as _descriptor
  File ""C:\Users\Abeer\AppData\Roaming\Python\Python36\site-packages\google\protobuf\descriptor.py"", line 47, in <module>
    from google.protobuf.pyext import _message
ImportError: DLL load failed: The specified procedure could not be found

however I followed all the steps in Tensorflow website 
what would be the reason and what to do urgently 

Thanks "
25846,[TF 2.0 API Docs] tf.math.add,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/add


**Describe the documentation issue**

Much like #25802, documentation for `tf.math.add` is created from a generated file `python/ops/gen_math_ops.py`; a link to the file that generates `python/ops/gen_math_ops.py` would be handy for users.

`tf.math.add` could use a usage example, and a list of Errors raised. 

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes"
25845,Error while retrain inception-v3,"**System information**
- Have I written custom code: No, it is the sample code to retrain inception-v3 in https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py
- OS Platform and Distribution: Windows 10 build 17134.590
- Mobile device if the issue happens on mobile device: N/A
- TensorFlow installed from: binary
- TensorFlow version: 1.8.0
- Python version:3.6.4
- Bazel version: N/A
- GCC/Compiler version: N/A
- CUDA/cuDNN version: 9.2 / 7.4.2
- GPU model and memory: GTX1070 8GB

**Describe the current behavior**
Start retrain.py as https://tensorflow.google.cn/hub/tutorials/image_retraining.
And throw a error [could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED].

**Describe the expected behavior**
It should initialize fine and start retrain.

**Code to reproduce the issue**
I have not change the sample code in https://github.com/tensorflow/hub/raw/master/examples/image_retraining/retrain.py.

**Other info / logs**
INFO:tensorflow:Looking for images in 'daisy'
INFO:tensorflow:Looking for images in 'dandelion'
INFO:tensorflow:Looking for images in 'roses'
INFO:tensorflow:Looking for images in 'sunflowers'
INFO:tensorflow:Looking for images in 'tulips'
INFO:tensorflow:Using C:\Users\username\AppData\Local\Temp\tfhub_modules to cache modules.

'''
lots of modle variable initialize info
'''

2019-02-18 23:55:23.138894: I T:\src\github\tensorflow\tensorflow\core\platform\cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2
2019-02-18 23:55:23.370513: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1356] Found device 0 with properties:
name: GeForce GTX 1070 major: 6 minor: 1 memoryClockRate(GHz): 1.645
pciBusID: 0000:01:00.0
totalMemory: 8.00GiB freeMemory: 6.62GiB
2019-02-18 23:55:23.376527: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1435] Adding visible gpu devices: 0
2019-02-18 23:55:24.022863: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-18 23:55:24.026942: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:929]      0
2019-02-18 23:55:24.028585: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:942] 0:   N
2019-02-18 23:55:24.030644: I T:\src\github\tensorflow\tensorflow\core\common_runtime\gpu\gpu_device.cc:1053] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6400 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1)
INFO:tensorflow:Creating bottleneck at /tmp/bottleneck\daisy\10140303196_b88d3d6cec.jpg_https~tfhub.dev~google~imagenet~inception_v3~feature_vector~1.txt
2019-02-18 23:55:27.258220: E T:\src\github\tensorflow\tensorflow\stream_executor\cuda\cuda_dnn.cc:455] could not create cudnn handle: CUDNN_STATUS_ALLOC_FAILED
2019-02-18 23:55:27.260973: F T:\src\github\tensorflow\tensorflow\core\kernels\conv_ops.cc:713] Check failed: stream->parent()->GetConvolveAlgorithms( conv_parameters.ShouldIncludeWinogradNonfusedAlgo<T>(), &algorithms)
"
25844,[TF 2.0 API Docs] tf.lite.TFLiteConverter,"**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/lite/TFLiteConverter

**Describe the documentation issue**

- **Links** (Fixed)

Incorrect - https://github.com/tensorflow/tensorflow/blob/master/lite/python/lite.py
Correct - https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/python/lite.py

- **Description** (In PR #26067) 

This is used to convert from a TensorFlow ""GraphDef or SavedModel"" into either a TFLite FlatBuffer or graph visualization.

Should be ""GraphDef, Saved Model or tf.keras model""

- **Usage example** (In PR #26067) 

This line of code should be repeated for each of the methods
open(""converted_model.tflite"", ""wb"").write(tflite_model)

- **Parameters Defined** (Fixed)

Missing 2 parameters
- optimizations
- representative_dataset

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** 

Yes"
25843,TF 2.0: Can't use tf.keras.layers.LSTM on GPU.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution: Google Colab
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `tf-nightly-gpu-2.0-preview==2.0.0.dev20190218`
- Python version: 3.6.7
- CUDA/cuDNN version: CUDA 10.0 CuDNN 7.4.2
- GPU model and memory: ?


**Describe the current behavior**
Getting the following error when trying to fit a model using `tf.keras.layers.LSTM` with `tf.keras`:
```
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-30-145a5e1acc45> in <module>()
     18 model.fit(x=dataset,
     19           epochs=1,
---> 20           verbose=1)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)
    762           workers=0,
    763           shuffle=shuffle,
--> 764           initial_epoch=initial_epoch)
    765 
    766     # Case 3: Symbolic tensors or Numpy array-like.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1482         shuffle=shuffle,
   1483         initial_epoch=initial_epoch,
-> 1484         steps_name='steps_per_epoch')
   1485 
   1486   def evaluate_generator(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)
    244 
    245       is_deferred = not model._is_compiled
--> 246       batch_outs = batch_function(*batch_data)
    247       if not isinstance(batch_outs, list):
    248         batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1226       else:
   1227         self._make_fit_function()
-> 1228         outputs = self._fit_function(ins)  # pylint: disable=not-callable
   1229 
   1230     if reset_metrics:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3207         value = math_ops.cast(value, tensor.dtype)
   3208       converted_inputs.append(value)
-> 3209     outputs = self._graph_fn(*converted_inputs)
   3210     return nest.pack_sequence_as(self._outputs_structure,
   3211                                  [x.numpy() for x in outputs])

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
    438       raise TypeError(""Keyword arguments {} unknown. Expected {}."".format(
    439           list(kwargs.keys()), list(self._arg_keywords)))
--> 440     return self._call_flat(args)
    441 
    442   def _filtered_call(self, args, kwargs):

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in _call_flat(self, args)
    507     # Only need to override the gradient in graph mode and when we have outputs.
    508     if context.executing_eagerly() or not self.outputs:
--> 509       outputs = self._inference_function.call(ctx, args)
    510     else:
    511       self._register_gradient()

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py in call(self, ctx, args)
    295             attrs=(""executor_type"", executor_type,
    296                    ""config_proto"", config),
--> 297             ctx=ctx)
    298       # Replace empty list with None
    299       outputs = outputs or None

/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     64     else:
     65       message = e.message
---> 66     six.raise_from(core._status_to_exception(e.code, message), None)
     67   except TypeError as e:
     68     if any(ops._is_keras_symbolic_tensor(x) for x in inputs):

/usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)

UnknownError: Fail to find the dnn implementation.
	 [[{{node unified_lstm_8/CudnnRNN}}]]
	 [[loss_8/dense_12_loss/binary_crossentropy/weighted_loss/broadcast_weights/assert_broadcastable/is_valid_shape/else/_1/has_valid_nonscalar_shape/then/_47/has_invalid_dims/ExpandDims/_62]] [Op:__inference_keras_scratch_graph_9512]

```

**Describe the expected behavior**
It should train the model correctly. It is running on CPU.

**Code to reproduce the issue**
```
def create_model(vocab_size=10):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, 80),
        tf.keras.layers.LSTM(64),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])
    return model

model = create_model(vocab_size=vocab_size)
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])


dataset1 = tf.data.Dataset.from_tensor_slices([[1, 2, 3], [4, 5,6]])
dataset2 = tf.data.Dataset.from_tensor_slices([1, 2])
dataset = tf.data.Dataset.zip((dataset1, dataset2))
model.fit(x=dataset,
          epochs=1,
          verbose=1)
```
"
25841,[tflite][java] how to deal with result of runForMultipleInputsOutputs(),"I am running posenet on android with tflite.
The model has multiple output arrays with the following dimensions:
`1x14x14x17, 1x14x14x34, 1x14x14x32, 1x14x14x32`

Therefore running the java tfliter interpreter with
```
import org.tensorflow.lite.Interpreter;
Interpreter tflite;
tflite.runForMultipleInputsOutputs(inputs,outputs)
```

i can access the four output tensors with `tflite.getOutputTensor(i)` or with `outputs.get(i)` (with i el. [0,3]) as `outputs` is a `HashMap` filled with `java.nio.HeapByteBuffer` objects.

How can I convert these outputs or tflite tensors to java multi-dimensional arrays (something like `float[][][][];`) to be able to perform mathematical computations on them?"
25839,Tensorflow save & restore without using filesystem,"**System information**
- TensorFlow version (you are using): Up to date
- Are you willing to contribute it (Yes/No): Python is not my daily driver language



**Describe the feature and the current behavior/state.**
I would like to retrieve(instead of save) training information and restore it to a tensorflow session when necessary (without using file system). I want to bypass filesystem.

I have looked the followings and searched but was not successful.

https://www.tensorflow.org/guide/saved_model
https://stackoverflow.com/questions/33759623/tensorflow-how-to-save-restore-a-model

Why does this feature not appear already present?




**Will this change the current api? How?**

**Who will benefit with this feature?**
Distributed system, would be an easier way to persist and handle training data

**Any Other info.**
(as a side note, not important, this Tensorflow approach using filesystem might be plausible for Python. Nonetheless, I am using Scala Tensorflow and I would like to bypass the filesystem entirely and use Cassandra DB for a distributed environment. If I know how to do it in Python. It might be a similar approach in the Scala version, if not using the main APIs, by using byte codes, etc..)"
25838,"How to send a file path tensor(dtype=string, like: './dataset/coef.txt') to numpy.loadtxt() Function? ","This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
25835,keras model's saving format is inconsistent (tf and h5),"hi, I found the behavior is inconsistent when the keras model's saving format changed.
the keras model have two saving format(h5 and tf)

**Describe the current behavior**
the reproduce code as below:
```python
#!/usr/bin/env python
import tensorflow as tf
from tensorflow import keras

print(tf.__version__)

(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()

train_labels = train_labels[:1000]
test_labels = test_labels[:1000]

train_images = train_images[:1000].reshape(-1, 28 * 28) / 255.0
test_images = test_images[:1000].reshape(-1, 28 * 28) / 255.0


def create_model():
  model = tf.keras.models.Sequential([
    keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(10, activation=tf.nn.softmax)
  ])

  model.compile(optimizer=tf.keras.optimizers.Adam(),
                loss=tf.keras.losses.sparse_categorical_crossentropy,
                metrics=['accuracy'])

  return model


model = create_model()

checkpoint_path = ""log_file/cp.bin""


model.fit(train_images, train_labels,  epochs=10, verbose=0,
          validation_data=(test_images, test_labels))  # pass callback to training
# model.save_weights(checkpoint_path, save_format='h5')
model.save_weights(checkpoint_path, save_format='tf')

model = create_model()
loss, acc = model.evaluate(test_images, test_labels)
print(""Untrained model, accuracy: {:5.2f}%"".format(100 * acc))

model.load_weights(checkpoint_path)
loss, acc = model.evaluate(test_images, test_labels)
print(""trained model, accuracy: {:5.2f}%"".format(100 * acc))
```
the photo below is the behavior in my computer.
![image](https://user-images.githubusercontent.com/13925796/52948264-f2f7cf00-33b3-11e9-9ad1-fff9716911c8.png)

**Describe the expected behavior**
when we change save_format from `tf` to `h5`, the expection(OSError) should not happen. "
25834,YOLOv3 TFLite GPU and CPU perform at same speed,"**System information**
- Have I written custom code: No
- OS Platform and Distribution: Android
- Mobile device: Google Pixel, Google Pixel 2 and Xiaomi 6
- TensorFlow installed from: Binary 
  `implementation 'org.tensorflow:tensorflow-lite:0.0.0-gpu-experimental'`
- TensorFlow version: 1.13.0
- Python version: 3.6
- CUDA/cuDNN version: -
- GPU model and memory: -

**Describe the current behavior**
Model inference time is the same for GPU and CPU (~5500 ms)

**Describe the expected behavior**
Expecting it to be faster on GPU

**Code to reproduce the issue**
```
Interpreter.Options options = new Interpreter.Options();
GpuDelegate delegate = new GpuDelegate();
options.addDelegate(delegate);
// loadModelFile is the same with that function in the sample
Interpreter tflite = new Interpreter(loadModelFile(this), options);

Map<Integer, Object> output = new HashMap<>();
float[][][][] output1 = new float[1][19][19][255];
float[][][][] output2 = new float[1][38][38][255];
output.put(0, output1);
output.put(1, output2);

tflite.runForMultipleInputsOutputs(getInput(), output);

private Object[] getInput() {
  Bitmap bitmap = BitmapFactory.decodeResource(getResources(), R.drawable.dog_cycle_car);
  // Mat and Improc are from OpenCV
  Mat mat = new Mat();
  Mat convertedMat = new Mat();
  Mat resizedMat = new Mat();
  Utils.bitmapToMat(bitmap, mat);
  mat.convertTo(convertedMat, CvType.CV_8UC1);
  Size size = new Size(608.0, 608.0);
  Imgproc.resize(convertedMat, resizedMat, size, 0.0, 0.0, Imgproc.INTER_AREA);
  Bitmap resizedBitmap = Bitmap.createBitmap(resizedMat.cols(), resizedMat.rows(), Bitmap.Config.ARGB_8888);
  Utils.matToBitmap(resizedMat, resizedBitmap);
  ByteBuffer imgData = convertBitmapToByteBuffer(resizedBitmap);
  Object[] inputArray = new Object[1];
  inputArray[0] = imgData;
  return inputArray;
}
```

The model I used https://drive.google.com/open?id=1XqNO1Qo5CJwzeP1d0w-mOyyk7DWJKfkk
"
25831,The unit test //tensorflow/python/client/session_clusterspec_prop_test is failing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.8.0-17632-g14ecf71 1.12.0
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**

```
$ bazel test --config=opt -- //tensorflow/python:session_clusterspec_prop_test
DEBUG: /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

WARNING: /home/user/src/tensorflow/tensorflow/python/BUILD:3178:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/python:session_clusterspec_prop_test (0 packages loaded, 0 targets configured).
INFO: Found 1 test target...
FAIL: //tensorflow/python:session_clusterspec_prop_test (see /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/session_clusterspec_prop_test/test.log)
Target //tensorflow/python:session_clusterspec_prop_test up-to-date:
  bazel-bin/tensorflow/python/session_clusterspec_prop_test
INFO: Elapsed time: 4.492s, Critical Path: 4.03s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 1 process: 1 local.
INFO: Build completed, 1 test FAILED, 2 total actions
//tensorflow/python:session_clusterspec_prop_test                        FAILED in 4.0s
  /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/session_clusterspec_prop_test/test.log

INFO: Build completed, 1 test FAILED, 2 total actions
```

**Describe the expected behavior**

```
$ bazel test --config=opt -- //tensorflow/python:session_clusterspec_prop_test
DEBUG: /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

WARNING: /home/user/src/tensorflow/tensorflow/python/BUILD:3178:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/python:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/python:session_clusterspec_prop_test (1 packages loaded, 1518 targets configured).
INFO: Found 1 test target...
Target //tensorflow/python:session_clusterspec_prop_test up-to-date:
  bazel-bin/tensorflow/python/session_clusterspec_prop_test
INFO: Elapsed time: 2.186s, Critical Path: 0.54s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 2 processes: 2 local.
INFO: Build completed successfully, 3 total actions
//tensorflow/python:session_clusterspec_prop_test               (cached) PASSED in 4.4s

INFO: Build completed successfully, 3 total actions
```

**Code to reproduce the issue**

$ bazel test --config=opt -- //tensorflow/python:session_clusterspec_prop_test

**Other info / logs**

```
======================================================================
ERROR: testClusterSpecPropagationWorker1Placement (__main__.SessionClusterSpecPropagationTest)
testClusterSpecPropagationWorker1Placement (__main__.SessionClusterSpecPropagationTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/client/session_clusterspec_prop_test.py"", line 109, in testClusterSpecPropagationWorker1Placement
    output = self.evaluate(const)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1692, in evaluate
    return sess.run(tensors)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1811, in test_session
    yield cached
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5438, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5245, in get_controller
    yield default
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5438, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 480, in _mode
    yield
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5438, in get_controller
    yield g
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5245, in get_controller
    yield default
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 4405, in device
    yield
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1811, in test_session
    yield cached
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1692, in evaluate
    return sess.run(tensors)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1419, in run
    return super(ErrorLoggingSession, self).run(*args, **kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 930, in run
    run_metadata_ptr)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1078, in _run
    raise RuntimeError('The Session graph is empty.  Add operations to the '
RuntimeError: The Session graph is empty.  Add operations to the graph before calling run().

======================================================================
ERROR: testMultipleLocalDevices (__main__.SessionClusterSpecPropagationTest)
testMultipleLocalDevices (__main__.SessionClusterSpecPropagationTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/client/session_clusterspec_prop_test.py"", line 211, in testMultipleLocalDevices
    output = self.evaluate(sum3)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1692, in evaluate
    return sess.run(tensors)
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1811, in test_session
    yield cached
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5438, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5245, in get_controller
    yield default
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5438, in get_controller
    yield g
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/eager/context.py"", line 480, in _mode
    yield
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5438, in get_controller
    yield g
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 5245, in get_controller
    yield default
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py"", line 4405, in device
    yield
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2523, in _constrain_devices_and_set_default
    yield sess
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1811, in test_session
    yield cached
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1692, in evaluate
    return sess.run(tensors)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1419, in run
    return super(ErrorLoggingSession, self).run(*args, **kwargs)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 930, in run
    run_metadata_ptr)
  File ""/home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/session_clusterspec_prop_test.runfiles/org_tensorflow/tensorflow/python/client/session.py"", line 1078, in _run
    raise RuntimeError('The Session graph is empty.  Add operations to the '
RuntimeError: The Session graph is empty.  Add operations to the graph before calling run().

----------------------------------------------------------------------
```
"
25830,CUDNN_STATUS_INTERNAL_ERROR while using tensorflow.keras.applications.VGG16,"I am trying to use a pretrained network from tensorflow.keras.applications with 1.13rc2 and it crashes with a cudnn error.

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):
- TensorFlow installed from source: 1.13.0-rc2
- TensorFlow version (use command below):
- Python version: 3.6.5
- Bazel version: 0.22
- GCC/Compiler version: 7.3
- CUDA/cuDNN version: 10.0 / 7.4.2
- GPU model and memory: GeForce RTX 2070 8GB

**Code to reproduce the issue**

Minimal working example:

```python
import numpy as np
from keras.applications import VGG16
if __name__ == ""__main__"":
    model = VGG16(weights='imagenet', include_top=True, input_shape=(224,224,3))
    model.compile(optimizer=""rmsprop"", loss=""categorical_crossentropy"")
    X_train = np.ones((2000, 224, 224, 3))
    y_train = np.ones((2000, 1000))
    model.fit(x=X_train, y=y_train, epochs=50, batch_size=16)
```
**Other info / logs**

Using TensorFlow backend.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-02-18 09:11:29.560595: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3399705000 Hz
2019-02-18 09:11:29.561392: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2116980 executing computations on platform Host. Devices:
2019-02-18 09:11:29.561431: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-18 09:11:30.251516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-02-18 09:11:30.252153: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ddff10 executing computations on platform CUDA. Devices:
2019-02-18 09:11:30.252175: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): GeForce RTX 2070, Compute Capability 7.5
2019-02-18 09:11:30.252593: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: 
name: GeForce RTX 2070 major: 7 minor: 5 memoryClockRate(GHz): 1.725
pciBusID: 0000:26:00.0
totalMemory: 7.76GiB freeMemory: 7.64GiB
2019-02-18 09:11:30.252614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0
2019-02-18 09:11:30.253693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-18 09:11:30.253709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 
2019-02-18 09:11:30.253716: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N 
2019-02-18 09:11:30.254007: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7436 MB memory) -> physical GPU (device: 0, name: GeForce RTX 2070, pci bus id: 0000:26:00.0, compute capability: 7.5)
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
Epoch 1/50
2019-02-18 09:11:34.331098: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally
2019-02-18 09:11:35.167778: E tensorflow/stream_executor/cuda/cuda_dnn.cc:334] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-02-18 09:11:35.167818: W ./tensorflow/stream_executor/stream.h:2099] attempting to perform DNN operation using StreamExecutor without DNN support
Traceback (most recent call last):
  File ""test.py"", line 9, in <module>
    model.fit(x=X_train, y=y_train, epochs=50, batch_size=16)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/engine/training.py"", line 1039, in fit
    validation_steps=validation_steps)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/engine/training_arrays.py"", line 199, in fit_loop
    outs = f(ins_batch)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2715, in __call__
    return self._call(inputs)
  File ""/home/mate/dev/backend/env/backend/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py"", line 2675, in _call
    fetched = self._callable_fn(*array_vals)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1439, in __call__
    run_metadata_ptr)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py"", line 528, in __exit__
    c_api.TF_GetCode(self.status.status))
tensorflow.python.framework.errors_impl.InternalError: cuDNN launch failure : input shape([16,3,224,224]) filter shape([3,3,3,64])
	 [[{{node block1_conv1/convolution}}]]
	 [[{{node loss/mul}}]]
"
25829,how to specify session target when using Estimator API,"Hi, I am trying to run my training graph in a remote server with remote session. I am able to do it by starting two tasks and specify the target of tf.Session as the first server. But seems there is noway to specify the target for tf.Session. Is there any workaround for this?"
25828,[TF 2.0 API Docs] tf.keras.activations.serialize,"**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r1.13/api_docs/python/tf/keras/activations/serialize
* **Correct Links** https://github.com/tensorflow/tensorflow/blob/master/python/keras/activations.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py.
* **Clear Description**
  The description is not opinionated about when to use this symbol, or what happens when arguments are passed to the symbol.
* **Usage Example**
  No usage example is provided.
* **Parameters Defined**
  Parameters are poorly defined, and not formatted appropriately.
* **Returns Defined**
  Returns are not defined.
* **Raises Listed and Defined**
  Errors are not defined."
25827,[TF 2.0 API Docs] tf.keras.activations.tanh,"**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations/tanh
* **Correct Links** https://github.com/tensorflow/tensorflow/blob/master/python/keras/activations.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/activations.py.
* **Clear Description**
  No description of what the symbol does or what happens when arguments are passed to the symbol.
* **Usage Example**
  No usage example is provided.
* **Parameters Defined**
  Parameters are not defined.
* **Returns Defined**
  Returns are not defined.
* **Raises Listed and Defined**
  Errors are not defined."
25826,[TF 2.0 API Docs] tf.compat.path_to_str,"
**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/compat/path_to_str

### Existing URLs containing the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/compat/path_to_str

### Description of issue (what needs changing):
* **Correct Links**
  https://github.com/tensorflow/tensorflow/blob/master/python/util/compat.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/util/compat.py.
* **Clear Description**
  There could be a description of use cases where using this symbol would be appropriate. 
* **Raises Listed and Defined**
  Errors are not defined.
* **Visuals, if Applicable**
I do not think visuals are necessary for the symbol.

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25825,tf.data.experimental.make_csv_dataset unable to handle missing data,"I believe this is a bug. I've been unable to get `tf.data.experimental.make_csv_dataset` to work with a CSV that contains missing data. As a toy example, suppose I have a single CSV file `example.csv` with the following contents:

```
date, value
D1,1017.1
D2, NA
```

When I run the following code, I receive the following error: `tensorflow.python.framework.errors_impl.InvalidArgumentError: Field 1 in record is not a valid float:  NA [Op:IteratorGetNextSync]`
```
import tensorflow as tf

tf.enable_eager_execution()
dataset = tf.data.experimental.make_csv_dataset(
            file_pattern=filenames,
            column_names=['date', 'value'],
            column_defaults=[tf.string, tf.float32],
            header=True,
            field_delim=',',
            na_value='NA'
        )
iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()
```

Am I misusing the function? How am I supposed to specify that `NA` should be interpreted as a missing value? Replacing `NA` with a literal empty string results in the same problem:

`tensorflow.python.framework.errors_impl.InvalidArgumentError: Field 1 is required but missing in record! [Op:IteratorGetNextSync]`

My system's details:

- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6"
25824,Bug: ImportError: DLL load failed,"
ImportError: DLL load failed
doesnt show the name of the DLL which failed"
25823,Please delete this,incredibly easy fix slightly unrelated to tensorflow
25822, Illegal instruction: 4 when importing tensorflow,"I've installed tensorflow (Version: 1.12.0) using pip3 on MacOS High Sierra 10.13.6, and it crashes on import with Illegal Instruction: 4. I get the same error using any version of python 3.7 and 3.6.8. Same error even after downgrading to tensorflow 1.5 and numpy 1.13. Identical issue whether I install through homebrew or pip3. Would appreciate any help to get it working.
Thanks.
```
$ python3
Python 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) 
[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Illegal instruction: 4
```
 I'm also attaching the full error log, with all the details of my system configuration. 
[ill4.txt](https://github.com/tensorflow/tensorflow/files/2873724/ill4.txt)



"
25820,[TF 2.0 API Docs] tf.math.add_n,"**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/add_n


**Describe the documentation issue**

* **Links**
https://github.com/tensorflow/tensorflow/blob/master/python/ops/math_ops.py
should be
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_ops.py

* **Clear Description**
The description is not clear enough.  In the description, it should probably link to [tf.math.accumulate_n](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/accumulate_n) and explain when to use `tf.math.add_n`.

* **Usage example**
No usage example is provided.

* **Visuals, if Applicable**
  No visuals are included.


**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
Yes.
"
25819,logical issue tf.less and tf.logical.end,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Anaconda
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
None
- TensorFlow installed from (source or binary):
1.12
- TensorFlow version (use command below):
1.12
- Python version:
Matching, no issue
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
Matching, no issue
- GPU model and memory:
Titan 1080 11GB x2


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have in my code from a 3rd party library:
```
            is_up = tf.less(phi, (r_on_broadcast * 0.5))
            is_down = tf.logical_and(tf.less(phi, r_on_broadcast), tf.logical_not(is_up))
```
The code runs in training without error. In serving it gives an error when `phi = r_on_broadcast`
Is there a difference between training and serving?
**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25817,ERROR: /home/max/tensorflow/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':,"****System information**

* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
* TensorFlow installed from (source or binary): source
* TensorFlow version: 1.13.0rc2
* Python version: 3.7.2
* Installed using virtualenv? pip? conda?: pip
* Bazel version (if compiling from source): 0.22
* GCC/Compiler version (if compiling from source): 6.5.0
* CUDA/cuDNN version: 10.0/7.3
* GPU model and memory: RTX 2070 and 8GB

**Describe the problem**

ERROR: /home/max/tensorflow/tensorflow/python/eager/BUILD:10:1: undeclared inclusion(s) in rule '//tensorflow/python/eager:pywrap_tfe_lib':
this rule is missing dependency declarations for the following files included by 'tensorflow/python/eager/pywrap_tensor.cc':
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarrayobject.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ndarraytypes.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/numpyconfig.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_numpyconfig.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_endian.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_cpu.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/utils.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/_neighborhood_iterator_imp.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__multiarray_api.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_interrupt.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/ufuncobject.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_math.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/npy_common.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/arrayobject.h'
'bazel-out/host/genfiles/external/local_config_python/python_include/numpy/__ufunc_api.h'
Target //tensorflow/tools/pip_package:build_pip_package failed to build**"
25814,Batchnorm does not work in Eager mode in TF 1.12: InternalError, Could not find valid device for node 
25812,Ingesting Tensorflow in larger (Bazel) project,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux 16.04
- TensorFlow version: 1.10
- Python version: 3.5
- Installed using virtualenv? pip? conda?: Custom
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.2/7
- GPU model and memory: compute 6.1

**Describe the problem**
I am trying to use Tensorflow in a repository which also uses Bazel as build system.
Currently, Tensorflow C++ API library is pre-compiled and ingested as cc_library; however, I am trying to ingest it and build it from source directly since I need to support a number of configurations (and pre-compiling them is a pain)

I have been successful with a CPU only version. I did this by following the method used by Tensorflow Serving. I included the following in my WORKSPACE file

```
load(""//repo.bzl"", ""tensorflow_http_archive"")

tensorflow_http_archive(
    name = ""org_tensorflow"",
    git_commit = ""355cc566efd2d86fe71fa9d755ceabe546d577a7"",
    sha256 = ""a19b64d5c7aecb8487302843b27c8fde916dc08df8d9c2fdded20d94e0947c34"",
)

load(""@bazel_tools//tools/build_defs/repo:http.bzl"", ""http_archive"")

# TensorFlow depends on ""io_bazel_rules_closure"" so we need this here.
# Needs to be kept in sync with the same target in TensorFlow's WORKSPACE file.
http_archive(
    name = ""io_bazel_rules_closure"",
    sha256 = ""a38539c5b5c358548e75b44141b4ab637bba7c4dc02b46b1f62a96d6433f56ae"",
    strip_prefix = ""rules_closure-dbb96841cc0a5fb2664c37822803b06dab20c7d1"",
    urls = [
        ""https://mirror.bazel.build/github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",
        ""https://github.com/bazelbuild/rules_closure/archive/dbb96841cc0a5fb2664c37822803b06dab20c7d1.tar.gz"",  # 2018-04-13
    ],
)

# Specify the minimum required bazel version.
load(""@org_tensorflow//tensorflow:version_check.bzl"", ""check_bazel_version_at_least"")

check_bazel_version_at_least(""0.20.0"")

load(""@org_tensorflow//tensorflow:workspace.bzl"", ""tf_workspace"")

tf_workspace(
    path_prefix = """",
    tf_repo_name = ""org_tensorflow"",
)
```

where I defined repo.bzl as 

```
"""""" TensorFlow Http Archive
Modified http_archive that allows us to override the TensorFlow commit that is
downloaded by setting an environment variable. This override is to be used for
testing purposes.
Add the following to your Bazel build command in order to override the
TensorFlow revision.
build: --action_env TF_REVISION=""<git commit hash>""
  * `TF_REVISION`: tensorflow revision override (git commit hash)
""""""

_TF_REVISION = ""TF_REVISION""

def _tensorflow_http_archive(ctx):
    git_commit = ctx.attr.git_commit
    sha256 = ctx.attr.sha256

    override_git_commit = ctx.os.environ.get(_TF_REVISION)
    if override_git_commit:
        sha256 = """"
        git_commit = override_git_commit

    strip_prefix = ""tensorflow-%s"" % git_commit
    urls = [
        ""https://mirror.bazel.build/github.com/tensorflow/tensorflow/archive/%s.tar.gz"" % git_commit,
        ""https://github.com/tensorflow/tensorflow/archive/%s.tar.gz"" % git_commit,
    ]
    ctx.download_and_extract(
        urls,
        """",
        sha256,
        """",
        strip_prefix,
    )

tensorflow_http_archive = repository_rule(
    implementation = _tensorflow_http_archive,
    attrs = {
        ""git_commit"": attr.string(mandatory = True),
        ""sha256"": attr.string(mandatory = True),
    },
)
```

However, once I try using the GPU version, I run into problems... I have tried this by adding the following to my .bazelrc file, which are the same as the onces generated by a manual ./configure step in a standalone Tensorflow build

```
# Options for CUDA enabled Tensorflow
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_CUDA=""1""
build --action_env CUDA_TOOLKIT_PATH=""/usr/local/cuda""
build --action_env TF_CUDA_VERSION=""9.2""
build --action_env CUDNN_INSTALL_PATH=""/usr/local/cuda-9.2""
build --action_env TF_CUDNN_VERSION=""7""
build --action_env TF_NCCL_VERSION=""1""
build --action_env TF_CUDA_COMPUTE_CAPABILITIES=""6.1""
build --action_env LD_LIBRARY_PATH=""/usr/local/cuda-9.2/lib64:""
build --action_env TF_CUDA_CLANG=""0""
build --action_env GCC_HOST_COMPILER_PATH=""/usr/bin/gcc""
build --define=using_cuda=true --define=using_cuda_nvcc=true
```

I get the following error:
```
/cuda_configure.bzl"", line 423, in cuda_toolkit_path
                auto_configure_fail(""Cannot find cuda toolkit path."")
        File ""/home/ferronfr/.cache/bazel/_bazel_ferronfr/fb0eee7f7437f761aece7d670e37cf54/external/org_tensorflow/third_party/gpus/cuda_configure.bzl"", line 342, in auto_configure_fail
                fail((""\n%sCuda Configuration Error:%...)))

Cuda Configuration Error: Cannot find cuda toolkit path.
```

Does anybody know how to solve this?

Cheers!"
25811,toco: error: argument --output_file is required,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
`IMAGE_SIZE=224
toco \
  --input_file=tf_files/retrained_graph.pb \
  --output_file=tf_files/optimized_graph.tflite \
  --input_format=TENSORFLOW_GRAPHDEF \
  --output_format=TFLITE \
  --input_shape=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \
  --input_array=input \
  --output_array=final_result \
  --inference_type=FLOAT \
  --input_data_type=FLOAT`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Im using windows subsytems for linux
- TensorFlow version (use command below):
tensorflow 1.12
- Python version:
2.7

**Describe the current behavior**
i want to convert my custom model to tflite as stated in [this](https://codelabs.developers.google.com/codelabs/tensorflow-for-poets-2-tflite/#2) but it gives a toco error saying toco: error: argument --output_file is required


**Other info / logs**

this was the whole log:
`usage: toco [-h] --output_file OUTPUT_FILE
            (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR | --keras_model_file KERAS_MODEL_FILE)
            [--output_format {TFLITE,GRAPHVIZ_DOT}]
            [--inference_type {FLOAT,QUANTIZED_UINT8}]
            [--inference_input_type {FLOAT,QUANTIZED_UINT8}]
            [--input_arrays INPUT_ARRAYS] [--input_shapes INPUT_SHAPES]
            [--output_arrays OUTPUT_ARRAYS]
            [--saved_model_tag_set SAVED_MODEL_TAG_SET]
            [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]
            [--std_dev_values STD_DEV_VALUES] [--mean_values MEAN_VALUES]
            [--default_ranges_min DEFAULT_RANGES_MIN]
            [--default_ranges_max DEFAULT_RANGES_MAX]
            [--post_training_quantize] [--drop_control_dependency]
            [--reorder_across_fake_quant]
            [--change_concat_input_ranges {TRUE,FALSE}] [--allow_custom_ops]
            [--converter_mode {DEFAULT,TOCO_FLEX,TOCO_FLEX_ALL}]
            [--dump_graphviz_dir DUMP_GRAPHVIZ_DIR] [--dump_graphviz_video]
toco: error: argument --output_file is required`"
25810,report_tensor_allocations_upon_oom and embedding_lookup together lead to memory leak,"
**Describe the current behavior**
The RunOptions report_tensor_allocations_upon_oom and embedding_lookup will lead to CPU memory leak.
This will only happen in tf 1.12, tf 1.10 didn't have this problem.

I thought `tf.gather` have almost same functionality, so I test it and it seems no problem at all.

I'm not sure if #21348 is related to this problem. My problem will only occur with runoptions.

**Code to reproduce the issue**
```
import tensorflow as tf

embeddings = tf.reshape(tf.range(1000), shape=(100,10))
ids = tf.reshape(tf.range(100), shape=(20,5))

embedding_lookup = tf.nn.embedding_lookup(embeddings, ids)
ga = tf.gather(embeddings, ids)

with tf.Session() as sess:
  sess.graph.finalize()
  run_opts = tf.RunOptions(report_tensor_allocations_upon_oom=True)

  for i in range(10000000):
    sess.run(embedding_lookup, options=run_opts)
    #sess.run(ga, options=run_opts)
```
The code above leaks cpu memory quite rapidly ~3 Mb/s.

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6.7
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 9.0/ 7.4.2
- GPU model and memory: GTX 1070 with 8GB memory"
25808,'tensorflow' has no attribute 'enable_eager_execution',"**System information**
-Google Colab 
-GPU
-Python 3
-Tensorflow tf-nightly-2.0-preview

When I write the code:
  `from __future__ import absolute_import, division, print_function`
  `!pip install tf-nightly-2.0-preview`
  `import tensorflow as tf`
  `tf.enable_eager_execution()`
It gives me the error **AttributeError: 'module' object has no attribute 'enable_eager_execution'**

whereas when I simply run the code as:
  `from __future__ import absolute_import, division, print_function`
  `!pip install tf-nightly-2.0-preview`
  `import tensorflow as tf`
  `tf.executing_eagerly()` 
It returns **'True'**, which means that eager execution is working by default without even enabling it before.
If I proceed without using tf.enable_eager_execution() in this collab tutorial [https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/generative/image_captioning.ipynb#scrollTo=U8l4RJ0XRPEm](url)
In the cell Caching the features extracted from InceptionV3, 
the last line with ""np.save(path_of_feature, bf.numpy())"" gives me an error saying **""Tensor has no attribute called numpy()""**
If I see the type() of batch_features in the line:
`batch_features = image_features_extract_model(img)` line is returning a graph tensor, instead of an eager tensor. It's not supposed to do that. Does this means that eager execution is not yet enabled or is this a bug."
25807,tensorflow/stream_executor/dso_loader.cc couldn't import cupti64_100.dll which exists in path,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): gpu 1.13.0rc2
- Python version: 3.7.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: RTX2080Ti GDDR6 11GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
2019-02-17 16:49:22.874964: I tensorflow/stream_executor/dso_loader.cc:142] Couldn't open CUDA library cupti64_100.dll
2019-02-17 16:49:22.876813: F tensorflow/stream_executor/lib/statusor.cc:34] Attempting to fetch value instead of handling error Failed precondition: could not dlopen DSO: cupti64_100.dll; dlerror: cupti64_100.dll not found

**Describe the expected behavior**
should import properly because it exists in path

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
python .\mnist_softmax_xla.py  in mnist tutorial

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

it works until few days ago
only i do recently is update nvidia driver to 418.91"
25806,About gradient reduction in DistributionStrategy,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): master
- Python version: 2.3.7
- Bazel version (if compiling from source): 0.2.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda 9.0, cudnn 7
- GPU model and memory:

**Describe the current behavior**
1. Currently the gradient reduction in `distributed_apply` is `SUM` not `MEAN`. This makes learning rate adjustment more confusing. In general, learning rate should be adjusted according to total batch size. If it is `SUM` case, the learning rate adjustment will be different between single mode and distribution mode given the same total batch size.  

2. The gradient reduction is implemented in `_distributed_apply` function. That means the gradient clipping will be done before gradient reduction. This is not identical with single mode because the gradient clipping should be done after reduction along the `batch size` dimenstion. Another, in `clip_by_global_norm` case, it blocks the overlapping between computation and communication. The performance is even worse then using `CollectiveAllReduceStrategy`. I have tried to put the gradient reduction in `def gradients` instead of `_distributed_apply` locally. If you think this is necessary, I will refine my code and submit a pull request here.
 
**Describe the expected behavior**
1. Change the gradient reduction from `SUM` to `MEAN`
2. Move gradient reduction from `_distributed_apply` to `def gradients`

**Other info / logs**
I will contribute for it if you think the above are necessary. Thanks. @yuefengz 
"
25805,About model size in ckpt format or numpy format,"Hello, I got a strange question when I tried to convert model from ckpt format to numpy format.
I have two files for the source model in ckpt format
model.ckpt.index - 53.7KB
model.ckpt.data-00000-of-0001 - 330MB

But when I tried to use get_variable_to_shape_map() to get all variables from the src model and save them into a file with numpy format, I got a npy file with 494MB !!!

I wonder if I used the wrong way to convert ckpt to npy? Or if the ckpt model has been saved in some code formats so it looks small. 

Thank you !"
25804,tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR,"<em>I have installed TensorFlow from scratch and I ht following problem while running CuDNN</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Uubntu 18.10
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12
- Python version: 3.67 & 2.7.15
- Installed using virtualenv? pip? conda?: pip 
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 6.5.0
- CUDA/cuDNN version: 7.4.1
- GPU model and memory: Nvidia RTX 2080 / 7949MiB

**Logs**
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-02-17 05:00:46.661914: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcublas.so.10.0
2019-02-17 05:00:46.811147: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudnn.so.7
2019-02-17 05:00:47.318462: E tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
2019-02-17 05:00:47.324283: E tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File ""/home/suhahil/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1335, in _do_call
    return fn(*args)
  File ""/home/suhahil/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1320, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/suhahil/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1408, in _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv/Conv2D}}]]
	 [[Relu/_47]]

During handling of the above exception, another exception occurred:

**CuDNN Test**
suhahil@ml:/usr/src/cudnn_samples_v7/mnistCUDNN$ ./mnistCUDNN 
cudnnGetVersion() : 7401 , CUDNN_VERSION from cudnn.h : 7401 (7.4.1)
Host compiler version : GCC 6.5.0
There are 1 CUDA capable devices on your machine :
device 0 : sms 46  Capabilities 7.5, SmClock 1800.0 Mhz, MemSize (Mb) 7949, MemClock 7000.0 Mhz, Ecc=0, boardGroupID=0
Using device 0

Testing single precision
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm ...
Fastest algorithm is Algo 0
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS for Algo 0: 0.012224 time requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 2: 0.041856 time requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 1: 0.045056 time requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 7: 0.059808 time requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS for Algo 4: 0.071168 time requiring 207360 memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 

Result of classification: 1 3 5

Test passed!


"
25802,[TF 2.0 API Docs] tf.math.acos,"**System information**
- TensorFlow version: 2.0
- Doc Link: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/math/acos

**Describe the documentation issue**

Documentation for `tf.math.acos` is created from a generated file, `python/ops/gen_math_ops.py`. It would be excellent to have a link to the files that are used to generate `gen_math_ops.py` - so a user could make modifications quickly, without having to search through `tensorflow/tensorflow`.

`tf.math.acos` could use a clear description, usage examples, and example visuals. A great template to model this on could be [`numpy.arccos`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.arccos.html).

Users also experience obfuscated errors from unexpected arguments (ex: strings, Booleans, and even just `int`s). Some examples are shared below. The file used to generate this error is located [here](https://github.com/tensorflow/tensorflow/blob/e1d20b3b4f25047679cf34107f8d87329cc9070f/tensorflow/core/common_runtime/eager/execute.cc#L201). All `tf.math.*` operations and the operations they influence (e.g., `tf.linspace`) would experience these same obfuscated XLA errors.

```
InvalidArgumentError: Invalid cast from floating point type to S32 in ConstantR0WithType.
	 [[{{node Acos}}]] [Op:Acos]
```

```
InternalError: Could not find valid device for node.
Node: {{node Acos}}
All kernels registered for op Acos :
  device='XLA_CPU'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_COMPLEX128, DT_HALF]
  device='XLA_CPU_JIT'; T in [DT_FLOAT, DT_DOUBLE, DT_INT32, DT_COMPLEX64, DT_INT64, DT_COMPLEX128, DT_HALF]
  device='CPU'; T in [DT_DOUBLE]
  device='CPU'; T in [DT_FLOAT]
 [Op:Acos]
```

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
I shall certainly try! :) 
"
25801,TensorBoard logging batch level metrics with tf.keras,"This is something that has been added quite recently in the Keras library, see discussion [there](https://github.com/keras-team/keras/issues/6692):
""It'd be useful if there was some batch-level logging in TensorBoard when using the TensorBoard callback (as defined in keras/callbacks.py). I think this'd be generally useful when trying to keep track of stats between epochs.""

Currently using the `TensorBoard` callback like so will only log metrics at the end of each epoch, which can quickly become a problem for big datasets:
```
tensorboard = TensorBoard(log_dir=""logs/{}"".format(time()))
model.fit(x=training_set, epochs=epochs, validation_data=validation_set, callbacks=[tensorboard])
```

This is already merged in the Keras library, see [here](https://github.com/keras-team/keras/pull/11152).
"
25800,Cannot install TensorFlowLiteSwift with CocoaPods,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.12
- Python version: 3.6.5
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

I'm excited to see that TensorFlow Lite for Swift is available!
[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/experimental/swift](url)

I tried installing it with CocoaPods using the instructions at the link above, but I get the following error. Is the CocoaPod available yet? Thanks!
`[!] Unable to find a specification for TensorFlowLiteSwift`

"
25799,Error importing TensorFlow nightly gpu (docker),"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: Docker
- TensorFlow version: nightly-gpu
- Python version: 2
- CUDA version: Driver Version: 410.79 CUDA Version: 10.0
- GPU model and memory: Nvidia Tesla P100 16280MiB

(similar issue when installed from pip, not in a container with cuDNN v7 not sure if this would affect the Docker container)

**Describe the problem**
Using `tensorflow/tensorflow:nightly-gpu` image with the following code
```python
import tensorflow as tf

print(""hello world"")
```
yields
```
Floating point exception (core dumped)
```
as output

Using `tensorflow/tensorflow:latest-gpu` image works perfectly

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Fresh installation of ubuntu 18.04  
installed nvidia drivers and cuda through `apt-get` 
verified by building code samples and running the `deviceQuery` executable as described [here](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#running-binaries)

installed docker as described [here](https://docs.docker.com/install/linux/docker-ce/ubuntu/) and nvidia-docker as described [here](https://github.com/nvidia/nvidia-docker/wiki/Installation-(version-2.0))


same issue when using version installed from pip (`pip install --user tf-nightly-gpu`), not in a container
cuDNN v7, installation verified by running [mnistCUDNN](https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#verify) example


**Any other info / logs**
Running through gdb (not in a container, using pip version as above) gives the following
```
[Thread debugging using libthread_db enabled]
Using host libthread_db library ""/lib/x86_64-linux-gnu/libthread_db.so.1"".
[New Thread 0x7ffff4088700 (LWP 4806)]
[New Thread 0x7ffff3887700 (LWP 4807)]
[New Thread 0x7fffef086700 (LWP 4808)]
[New Thread 0x7fffec885700 (LWP 4809)]
[New Thread 0x7fffec084700 (LWP 4810)]
[New Thread 0x7fffe9883700 (LWP 4811)]
[New Thread 0x7fffe5082700 (LWP 4812)]
[New Thread 0x7fffe2881700 (LWP 4813)]
[New Thread 0x7fffe0080700 (LWP 4814)]
[New Thread 0x7fffdd87f700 (LWP 4815)]
[New Thread 0x7fffdb07e700 (LWP 4816)]
[New Thread 0x7fffd887d700 (LWP 4817)]
[New Thread 0x7fffd607c700 (LWP 4818)]
[New Thread 0x7fffd387b700 (LWP 4819)]
[New Thread 0x7fffd107a700 (LWP 4820)]
[New Thread 0x7fffce879700 (LWP 4821)]
[New Thread 0x7fffcc078700 (LWP 4822)]
[New Thread 0x7fffcb877700 (LWP 4823)]
[New Thread 0x7fffc7076700 (LWP 4824)]
[New Thread 0x7fffc4875700 (LWP 4825)]
[New Thread 0x7fffc2074700 (LWP 4826)]
[New Thread 0x7fffbf873700 (LWP 4827)]
[New Thread 0x7fffbf072700 (LWP 4828)]

Thread 1 ""python"" received signal SIGFPE, Arithmetic exception.
0x00007fff83ffdeba in _GLOBAL__sub_I_jit_avx512_common_conv_kernel.cpp ()
   from /home/ubuntu/.local/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```
"
25798,Speed of benchmark code in CPU windows is much slower than ubuntu,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12
- Python version: 3.5
- Bazel version (if compiling from source):0.18
- GCC/Compiler version (if compiling from source): MSVC 14 in windows 
- CUDA/cuDNN version: N/A
- GPU model and memory:N?A


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I have built the tensorflow v1.12. from source using bazel with the following configuration: 
```
build --action_env PYTHON_BIN_PATH=""C:/Python35/python.exe""
build --action_env PYTHON_LIB_PATH=""C:/Python35/lib/site-packages""
build --python_path=""C:/Python35/python.exe""
build:ignite --define with_ignite_support=true
build:xla --define with_xla_support=true
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build:opt --copt=/arch:AVX
build:opt --define with_default_optimizations=true
build --config monolithic
build --copt=-w --host_copt=-w
build --verbose_failures
build --distinct_host_configuration=false
build --experimental_shortened_obj_file_path=true
build --define=no_tensorflow_py_deps=true
build:v2 --define=tf_api_version=2
```
After succesful build I ran the benchmark code provided in ""tensorflow/tools/benchmark/benchmark_model_main.cc"".  But I see the speed in windows is much slower(almost twice) than in ubuntu.In windows the inference time is on average ~25 ms and in ubuntu the timing is ~14ms. I am using the same machine in dual boot mode.

**Code to reproduce the issue**
tensorflow/tools/benchmark/benchmark_model_main.cc"
25797,can not import libcublas.so.9.0 in centos7 systemctl service,"I try to put a service into systemctl service, so I write a systemctl service command. But when I run this systemctl service, it occur ImportError, my error message is below!
![image](https://user-images.githubusercontent.com/30991932/52897039-72e23580-320a-11e9-836e-af54822007e4.png)

this is my systemctl service command detail.
![image](https://user-images.githubusercontent.com/30991932/52897052-a8871e80-320a-11e9-8ecf-324ad1cd8297.png)

but when I import tensorflow in python command, it correct.
![image](https://user-images.githubusercontent.com/30991932/52897066-cbb1ce00-320a-11e9-9e7b-399f4beffd94.png)

this is my environment.
tensorflow=1.12
cuda=9.0
cudnn=7.4

Is this a bug of tensorflow 1.12"
25795,Can't rename checkpoint directories,"
**System information**
- TensorFlow version (you are using): 1.12.0
- Are you willing to contribute it (Yes/No): Probably, but I'm not sure exactly what needs to be changed and/or what change would be considered acceptable



**Describe the feature and the current behavior/state.**
Currently, if you rename or copy a checkpoint directory, tensorflow will refuse to load checkpoints from it. This is because it looks in the `checkpoint` file in that directory and loads a filepath from the protobuf. If that filepath doesn't match, it refuses to load any checkpoints for some reason.

**Will this change the current api? How?**
It will make loading checkpoints work as expected. I am not sure if anyone is relying on the current behavior.

**Who will benefit with this feature?**
Anyone who wants to easily back up and restore their checkpoints.

**Any Other info.**
I am not the first person to run into this issue. I see it was already reported [here](https://github.com/tensorflow/tensorflow/issues/13186) but that issue was closed with a request to resubmit (which apparently never happened)."
25794,[TF 2.0 API Docs] tf.AggregationMethod,"### Existing URLs containing the issue:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/AggregationMethod

### Description of issue (what needs changing):

* **Correct Links**
https://github.com/tensorflow/tensorflow/blob/master/python/ops/gradients_util.py is incorrect. The correct link should be https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_util.py.

* **Clear Description**
The description is not opinionated about when to use this symbol, and unclear on what aggregation methods for combining gradients would be useful for. 

* **Usage Example**
No usage example is provided.

* **Parameters Defined**
Parameters are poorly defined, and not formatted appropriately.

* **Returns Defined**
Returns are not defined.

* **Raises Listed and Defined**
Errors are not defined.

* **Visuals, if Applicable**
No visuals are included."
25790,Incompatibility between keras and tf.keras for layer.build,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.0rc1
- Python version: 3.6
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: No
- GPU model and memory: No


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

Here is the code below which should work (it works with keras-team/keras).

**Describe the expected behavior**

The code should run as expected, instead, we get the error message below. Note that changing the flag at the begining of the script makes the code work.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```python
use_tf_keras = True
if use_tf_keras:
    from tensorflow.keras.layers import Layer, Dense
    from tensorflow.keras import backend as K
    from tensorflow.keras import Sequential
else:
    from keras.layers import Layer, Dense
    from keras import backend as K
    from keras import Sequential


class MyLayer(Layer):

  def build(self, input_shape):
    print(type(input_shape))
    input_dim = input_shape[-1]
    output_dim = input_shape[-1] / 2

    self.kernel = self.add_weight(shape=(input_dim, output_dim),
                                  name='kernel',
                                  initializer='ones')
    super().build(input_shape)

  def call(self, inputs):
    return K.dot(inputs, self.kernel)

  def compute_output_shape(self, input_shape):
    input_shape = list(input_shape)
    input_shape[-1] = input_shape[-1] // 2
    return input_shape

model = Sequential()
model.add(Dense(8, input_shape=(20, 20)))
model.add(MyLayer())
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<class 'tensorflow.python.framework.tensor_shape.TensorShapeV1'>
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-2-255eda5c5005> in <module>()
     32 model = Sequential()
     33 model.add(Dense(8, input_shape=(20, 20)))
---> 34 model.add(MyLayer())

/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/checkpointable/base.py in _method_wrapper(self, *args, **kwargs)
    440     self._setattr_tracking = False  # pylint: disable=protected-access
    441     try:
--> 442       method(self, *args, **kwargs)
    443     finally:
    444       self._setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py in add(self, layer)
    178       # If the model is being built continuously on top of an input layer:
    179       # refresh its output.
--> 180       output_tensor = layer(self.outputs[0])
    181       if isinstance(output_tensor, list):
    182         raise TypeError('All layers in a Sequential model '

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, inputs, *args, **kwargs)
    536       if not self.built:
    537         # Build layer if applicable (if the `build` method has been overridden).
--> 538         self._maybe_build(inputs)
    539         # We must set self.built since user defined build functions are not
    540         # constrained to set self.built.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py in _maybe_build(self, inputs)
   1601     # Only call `build` if the user has manually overridden the build method.
   1602     if not hasattr(self.build, '_is_default'):
-> 1603       self.build(input_shapes)
   1604 
   1605   def __setattr__(self, name, value):

<ipython-input-2-255eda5c5005> in build(self, input_shape)
     15     print(type(input_shape))
     16     input_dim = input_shape[-1]
---> 17     output_dim = input_shape[-1] / 2
     18 
     19     self.kernel = self.add_weight(shape=(input_dim, output_dim),

TypeError: unsupported operand type(s) for /: 'Dimension' and 'int'
```

if using keras-team/keras, we get:
```
<class 'tuple'>
```

"
25789,Tensorflow 1.12.0 not working with CUDA 10.0,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): pip install tensorflow-gpu
- TensorFlow version: version 1.12.0
- Python version: 3.5.5
- Installed using virtualenv? pip? conda?: pip
- CUDA/cuDNN version: CUDA version 10.0
- GPU model and memory: Tesla V100

when I just try to import tensorflow I get this error message:


```
Python 3.5.5 |Anaconda custom (64-bit)| (default, May 13 2018, 21:12:35)
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/anaconda/envs/py35/lib/python3.5/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/anaconda/envs/py35/lib/python3.5/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/anaconda/envs/py35/lib/python3.5/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/anaconda/envs/py35/lib/python3.5/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```"
25788,image shift and interpolation,"I am trying to shift all images in a batch and have 'NEAREST' interpolation. For some reason, the resulting images are interpolated with black pixels... Is this a bug?

`
 translations = [100,0] * len(img_batch_list)
 img_batch = tf.contrib.image.translate(img_batch,
                                        translations,
                                        interpolation='NEAREST',
                                        name=""shift"")
`"
25787,Warning or error about incompatible device when I use tf.nn.rnn_cell.DropoutWrapper on multiple GPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.11.0 and 1.12.0
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8
- CUDA/cuDNN version: 9.0/7.3.0
- GPU model and memory: nvidia 1080ti, 11GB

**Describe the current behavior**
I write a very simple 2 layers LSTM model working on 2 GPUs. The model has no specific meaning, and I just want to test LSTM network. I randomly generalize some data as inputs, and take reduced sum of logits as loss. Then I compute the gradients and loss. I would get some warnings like below. And the same situation happens when I run [TensorFlow Neural Machine Translation Tutorial](https://github.com/tensorflow/nmt/tree/tf-1.4), which is the reason why I tested this simple code.

```
WARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.

Node-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation:
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>
No device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation.

No node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.
Device assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:
  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>
WARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.
```
If I don't set `allow_soft_placement=True`, I would get error.
```
InvalidArgumentError (see above for traceback): Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'
	 [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[""loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2"", ""loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div""], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=""/device:GPU:0""]()]]
```

**Describe the expected behavior**
get no warning or make sure that this situation would affect result or performance.

**Code to reproduce the issue**
**The warning only emerges when I use `tf.nn.rnn_cell.DropoutWrapper`.**
```
import tensorflow as tf
import numpy as np
from tensorflow.python.layers import core as layers_core


data_x = np.random.standard_normal([100, 5, 1000])

single_cell_0 = tf.nn.rnn_cell.LSTMCell(1000, forget_bias=1)
single_cell_0 = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell_0, input_keep_prob=0.8, seed=285)
single_cell_0 = tf.contrib.rnn.DeviceWrapper(single_cell_0, ""/gpu:0"")

single_cell_1 = tf.nn.rnn_cell.LSTMCell(1000, forget_bias=1)
single_cell_1 = tf.nn.rnn_cell.DropoutWrapper(cell=single_cell_1, input_keep_prob=0.8, seed=285)
single_cell_1 = tf.contrib.rnn.DeviceWrapper(single_cell_1, ""/gpu:1"")

cell = tf.contrib.rnn.MultiRNNCell([single_cell_0, single_cell_1])

target_input = tf.placeholder(dtype=tf.float32, shape=[None, 5, 1000], name=""input"")

output, final_state = tf.nn.dynamic_rnn(cell,
                                        target_input,
                                        dtype=tf.float32,
                                        time_major=True)
output = tf.reshape(output, [-1, 5000])
with tf.device(""/gpu:2""):
    logits = layers_core.Dense(100, use_bias=False, name=""output_projection"")(output)
    loss = tf.reduce_sum(logits)

params = tf.trainable_variables()
for param in params:
    print(""  %s, %s, %s"" % (param.name, str(param.get_shape()), param.op.device))
grad = tf.gradients(loss, params, colocate_gradients_with_ops=True)

config_proto = tf.ConfigProto(
    log_device_placement=False,
    allow_soft_placement=False)
config_proto.gpu_options.allow_growth = True
with tf.Session(config=config_proto) as sess:
    sess.run(tf.global_variables_initializer())
    g, l = sess.run([grad, loss], {target_input: data_x})
    print(g, l)
```

**Other info / logs**
Full info with warning
```
  rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0, (2000, 4000), /device:GPU:0
  rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0, (4000,), /device:GPU:0
  rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0, (2000, 4000), /device:GPU:1
  rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0, (4000,), /device:GPU:1
  output_projection/kernel:0, (5000, 100), /device:GPU:2
WARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.

Node-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation:
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>
No device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation.

No node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.
Device assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:
  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>
WARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' (defined at test.py:103) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:94) which had an incompatible device '/device:GPU:1'.

Node-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation:
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>
No device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation.

No node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.
Device assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:
  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>
2019-02-16 00:15:18.804994: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:18:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2019-02-16 00:15:19.010291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3b:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2019-02-16 00:15:19.227656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 2 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:86:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2019-02-16 00:15:19.408641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 3 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:af:00.0
totalMemory: 10.91GiB freeMemory: 10.36GiB
2019-02-16 00:15:19.415867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2, 3
2019-02-16 00:15:20.680526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-16 00:15:20.680570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 3 
2019-02-16 00:15:20.680576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y Y 
2019-02-16 00:15:20.680595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y Y 
2019-02-16 00:15:20.680599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N Y 
2019-02-16 00:15:20.680603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 3:   Y Y Y N 
2019-02-16 00:15:20.681368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10390 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:18:00.0, compute capability: 6.1)
2019-02-16 00:15:20.682959: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10398 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1)
2019-02-16 00:15:20.684307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10398 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:86:00.0, compute capability: 6.1)
2019-02-16 00:15:20.685688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10013 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:af:00.0, compute capability: 6.1)
```

Full info with error, no `allow_soft_placement=True`.
```
  rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0, (2000, 4000), /device:GPU:0
  rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0, (4000,), /device:GPU:0
  rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0, (2000, 4000), /device:GPU:1
  rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0, (4000,), /device:GPU:1
  output_projection/kernel:0, (5000, 100), /device:GPU:2
WARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' (defined at test.py:102) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:93) which had an incompatible device '/device:GPU:1'.

Node-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation:
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>
No device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' creation.

No node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.
Device assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:
  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>
WARNING:tensorflow:Tried to colocate op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' (defined at test.py:102) having device '/device:GPU:0' with op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' (defined at test.py:93) which had an incompatible device '/device:GPU:1'.

Node-device colocations active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation:
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:994>
  with tf.colocate_with(rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:354>
No device assignments were active during op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/f_acc' creation.

No node-device colocations were active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation.
Device assignments active during op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div' creation:
  with tf.device(/gpu:1): </home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py:1390>
2019-02-16 00:36:24.357307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:18:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2019-02-16 00:36:24.561462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 1 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:3b:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2019-02-16 00:36:24.760426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 2 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:86:00.0
totalMemory: 10.91GiB freeMemory: 10.75GiB
2019-02-16 00:36:24.941042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 3 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:af:00.0
totalMemory: 10.91GiB freeMemory: 10.36GiB
2019-02-16 00:36:24.948169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0, 1, 2, 3
2019-02-16 00:36:26.126392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-16 00:36:26.126439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 1 2 3 
2019-02-16 00:36:26.126446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N Y Y Y 
2019-02-16 00:36:26.126451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 1:   Y N Y Y 
2019-02-16 00:36:26.126455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 2:   Y Y N Y 
2019-02-16 00:36:26.126459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 3:   Y Y Y N 
2019-02-16 00:36:26.127293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10390 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:18:00.0, compute capability: 6.1)
2019-02-16 00:36:26.128911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 10398 MB memory) -> physical GPU (device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:3b:00.0, compute capability: 6.1)
2019-02-16 00:36:26.130298: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:2 with 10398 MB memory) -> physical GPU (device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:86:00.0, compute capability: 6.1)
2019-02-16 00:36:26.131678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:3 with 10013 MB memory) -> physical GPU (device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:af:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1292, in _do_call
    return fn(*args)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1275, in _run_fn
    self._extend_graph()
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1312, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'
	 [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[""loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2"", ""loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div""], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=""/device:GPU:0""]()]]

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""test.py"", line 109, in <module>
    sess.run(tf.global_variables_initializer())
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 887, in run
    run_metadata_ptr)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1286, in _do_run
    run_metadata)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/client/session.py"", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'
	 [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[""loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2"", ""loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div""], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=""/device:GPU:0""]()]]

Caused by op 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const', defined at:
  File ""test.py"", line 102, in <module>
    grad = tf.gradients(loss, params, colocate_gradients_with_ops=True)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 596, in gradients
    gate_gradients, aggregation_method, stop_gradients)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 776, in _GradientsHelper
    lambda: grad_fn(op, *out_grads))
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 398, in _MaybeCompile
    return grad_fn()  # Exit early
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py"", line 776, in <lambda>
    lambda: grad_fn(op, *out_grads))
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py"", line 972, in _RealDivGrad
    grad * math_ops.realdiv(math_ops.realdiv(-x, y), y), ry), sy))
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py"", line 5100, in neg
    ""Neg"", x=x, name=name)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1805, in __init__
    self._control_flow_post_processing()
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1816, in _control_flow_post_processing
    self._control_flow_context.AddOp(self)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2471, in AddOp
    self._AddOpInternal(op)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2492, in _AddOpInternal
    real_x = self.AddValue(x)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2424, in AddValue
    real_val = grad_ctxt.grad_state.GetRealValue(val)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 1136, in GetRealValue
    history_value = cur_grad_state.AddForwardAccumulator(cur_value)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 998, in AddForwardAccumulator
    max_size = constant_op.constant(-1, dtypes.int32)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 213, in constant
    name=name).outputs[0]
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 3272, in create_op
    op_def=op_def)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

...which was originally created as op 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div', defined at:
  File ""test.py"", line 93, in <module>
    time_major=True)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 664, in dynamic_rnn
    dtype=dtype)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 868, in _dynamic_rnn_loop
    swap_memory=swap_memory)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3274, in while_loop
    return_same_structure)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2994, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 2929, in _BuildLoop
    body_result = body(*packed_vars_for_body)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py"", line 3243, in <lambda>
    body = lambda i, lv: (i + 1, orig_body(*lv))
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 836, in _time_step
    (output, new_state) = call_cell()
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py"", line 822, in <lambda>
    call_cell = lambda: cell(input_t, state)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 233, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/layers/base.py"", line 364, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 769, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1484, in call
    cur_inp, new_state = cell(cur_inp, cur_state)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1391, in __call__
    return self._cell(inputs, state, scope=scope)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1279, in __call__
    self._input_keep_prob)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1260, in _dropout
    *[shallow_filtered_substructure, values])
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1053, in _enumerated_map_structure_up_to
    enumerated_fn, *args, **kwargs)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 643, in map_structure_up_to
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 643, in <listcomp>
    results = [func(*tensors) for tensors in zip(*all_flattened_up_to)]
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1049, in enumerated_fn
    r = map_fn(ix[0], *inner_args, **inner_kwargs)
  File ""/home/mjzhou/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1255, in dropout
    v, keep_prob=keep_prob, seed=self._gen_seed(salt_prefix, i))

InvalidArgumentError (see above for traceback): Cannot colocate nodes 'gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const' and 'rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div: Cannot merge devices with incompatible ids: '/device:GPU:1' and '/device:GPU:0'
	 [[{{node gradients/rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div_grad/Neg/Const}} = Const[_class=[""loc:@rnn/while/rnn/multi_rnn_cell/cell_0/lstm_cell/mul_2"", ""loc:@rnn/while/rnn/multi_rnn_cell/cell_1/dropout/div""], dtype=DT_INT32, value=Tensor<type: int32 shape: [] values: -1>, _device=""/device:GPU:0""]()]]
```"
25786,tf.keras.Model layers list is doubled in newer versions,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.14/ Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA 8.0 / cuDNN 7.1
- GPU model and memory: Titan XP / 12Gb


I am using tf.keras.Model to create my custom model class. When I call the class and build the model the layers properties, which is supposed to show the layers in the model, shows every layer twice. This behavior happens in 1.12.0, while it was fine in 1.9.0. I am not able to find documentation about this issue. 

This is a simple class that I am encountering this issue:
```python
class Lenet(tf.keras.Model):
    def __init__(self):
        super(Lenet, self).__init__()
        self.conv1 = tf.layers.Conv2D(filters=6,
                                     kernel_size=5,
                                     activation=tf.nn.relu)
        self.max_pool1 = tf.layers.MaxPooling2D(pool_size=2, strides=2)

        self.conv2 = tf.layers.Conv2D(filters=16,
                                     kernel_size=5,
                                     activation=tf.nn.relu)
        self.max_pool2 = tf.layers.MaxPooling2D(pool_size=2, strides=2)

        self.flatten = tf.layers.Flatten()

        self.fc1 = tf.layers.Dense(units=120, activation=tf.nn.relu)
        self.fc2 = tf.layers.Dense(units=84,  activation=tf.nn.relu)
        self.fc3 = tf.layers.Dense(units=10,  activation=tf.nn.relu)

        self.lenet_layers = [self.conv1, self.max_pool1, self.conv2, 
                                self.max_pool2, self.flatten, self.fc1, self.fc2, self.fc3]

    def call(self, input):
        out = input
        for layer in self.lenet_layers:
            out = layer(out)
        return out
```
After creating my Model and call it and pass a placeholder as input, I call:
```python
lenet = Lenet()
x = tf.placeholder(tf.float32,[None,32,32,1])
y = lenet(x)
lenet.layers
```
I expect to get a list with 8 layers' object on it, however, it returns a list with 16 layer objects( each layer twice on it). This was not the case back in the 1.9.0 version. I can simply ignore it and move on, but think it should not be like this. Maybe there is a purpose behind this behavior, which should be included in the documentation as well.
"
25784,Image resizing codes of iOS example camera apps might be wrong,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): N/A

**Describe the current behavior**

I think `in_x = (y * image_width) / wanted_input_width;` statements (mentioned below) would be `in_x = (x * image_width) / wanted_input_width;`. (The former perhaps rotates the input image?)

- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/ios/camera/CameraExampleViewController.mm#L302

```
  for (int y = 0; y < wanted_input_height; ++y) {
    float *out_row = out + (y * wanted_input_width * wanted_input_channels);
    for (int x = 0; x < wanted_input_width; ++x) {
      const int in_x = (y * image_width) / wanted_input_width; <--
      const int in_y = (x * image_height) / wanted_input_height; <--
      tensorflow::uint8 *in_pixel =
          in + (in_y * image_width * image_channels) + (in_x * image_channels);
      float *out_pixel = out_row + (x * wanted_input_channels);
      for (int c = 0; c < wanted_input_channels; ++c) {
        out_pixel[c] = (in_pixel[c] - input_mean) / input_std;
      }
    }
```

- https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/ios/camera/CameraExampleViewController.mm#L133

```
void ProcessInputWithFloatModel(
    uint8_t* input, float* buffer, int image_width, int image_height, int image_channels) {
  for (int y = 0; y < wanted_input_height; ++y) {
    float* out_row = buffer + (y * wanted_input_width * wanted_input_channels);
    for (int x = 0; x < wanted_input_width; ++x) {
      const int in_x = (y * image_width) / wanted_input_width; <--
      const int in_y = (x * image_height) / wanted_input_height; <--
      uint8_t* input_pixel =
          input + (in_y * image_width * image_channels) + (in_x * image_channels);
      float* out_pixel = out_row + (x * wanted_input_channels);
      for (int c = 0; c < wanted_input_channels; ++c) {
        out_pixel[c] = (input_pixel[c] - input_mean) / input_std;
      }
    }
  }
}
```"
25781,How can i build static framework for tf-lite under 300kb for iOS?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: master
- Doc Link:https://www.tensorflow.org/lite/overview


**Describe the documentation issue**

Dear supporter,
   I found ""Smaller in size: TensorFlow Lite is smaller than 300KB when all supported operators are linked and less than 200KB when using only the operators needed for supporting InceptionV3 and Mobilenet."" under TensorFlow Lite highlights. 
    I got libtensorflow-lite.a for each archtecture afte using build_ios_universal_lib.sh script. Binary size is far bigger, around 7.5MB. How can I get one smaller than 300KB?  Do I need to do any optimization on compiling and how?"
25780,Wrong semantic of Dense layer for tf.python.keras.Dense when input has rank > 2.,"**System information**
- OS Platform and Distribution: MacOs High Sierra
- TensorFlow installed from: installed from pip
- TensorFlow version: v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6

**Problem**
I think that tf.python.keras.Dense layer is not implemented correctly. The documentation states:

>  Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with `kernel`.

My interpretation of this claim is: if we have input tensor with shape `(None, 10, 10)` then dense layer with 20 hidden units should produce output with a shape `(None, 20)` and the kernel should have a shape of `(10 * 10, 20)`, and input tensor should be flattened to shape `(None, 100)` before product with kernel. 

**But**,  implementation is different. 

1. Kernel is initialized to shape `(10, 20)` (for the example above)
```python
self.kernel = self.add_weight(    # method Dense#build
        'kernel',
        shape=[input_shape[-1].value, self.units],
        initializer=self.kernel_initializer,
        regularizer=self.kernel_regularizer,
        constraint=self.kernel_constraint,
        dtype=self.dtype,
        trainable=True)
```

2. The output is computed as tensordot with summation over innermost dimensions
```python
# Broadcasting is required for the inputs.
outputs = standard_ops.tensordot(inputs, self.kernel, [[rank - 1], [0]]) # method Dense#call
```

This means that for `x.shape == (None, 10, 10)` and `units == 20` the output shape will be `(None, 10, 20)`, and that kernel is broadcasted for all other dimensions (this means weights are shared).

**Code to reproduce the issue**
```python
import tensorflow as tf
tf.enable_eager_execution()
x = tf.random.normal((10, 10, 10))
y = tf.layers.dense(x, 20)
print(y.shape)      # Produces (10, 10, 20) not (10, 20) as expected.
```

**Discussion**
Three possible things are happening here:

1. Documentation is wrong and implementation is correct, and a dense layer is meant to accept only rank 2-dimensional input (batchSize, a dimension of input vector) and if we pass it higher rank it treats it as a stack of input vectors. The solution is to change the documentation (specifically the note part), it should state something like numpy's documentation for `matmul`:  
> If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly
2. Implementation is wrong and there should be `flatten` before `tensordot` (no need for `tensordot`, we can now use `matmul`)
3. I do not understand what does dense layer should do (unlikely because I asked my colleagues for their opinion and they agree with my view of what kind of operation does dense layer perform)

I've ordered this list from most to least likely (in my opinion)."
25779,keras and tf.keras behave differently with custom loss function and fit_generator,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: 3.7.1
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: 10.0 / 7.4.2
- GPU model and memory: GTX 1080 ti
- Keras version: 2.2.4

**Describe the current behavior**
I am using Keras bundled with tensorflow. When I apply my own (rather obscure) loss I get a **NotFoundError**:
```
NotFoundError: Resource __per_step_7/_tensor_arraysloss/reshape_loss/map/TensorArray_1_1/N10tensorflow11TensorArrayE does not exist.
	 [[{{node training/Adam/gradients/loss/reshape_loss/map/while/TensorArrayReadV3_1_grad/TensorArrayGrad/TensorArrayGradV3}}]]
	 [[training/Adam/gradients/b_count_6/_233]]
```
when I use `fit_generator`, but when I use `fit` everything works ok. However, when I use vanilla Keras, both methods work just fine.

As a side question: my loss is rather slow, can someone give me some pointers in how to improve performance?

**Describe the expected behavior**
I would expect `fit_generator` to work just the same as in the vanilla-keras version or in the normal `fit` version.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
``` python
import numpy as np
import tensorflow as tf
# from tensorflow.keras import Input, Model
# from tensorflow.keras.layers import Dense, Flatten, Reshape
from keras import Input, Model
from keras.layers import Dense, Flatten, Reshape


def get_limb_lengths(person, limbs):
    """"""
    :param person: [ J * 3 ]
    :param limbs: {tf.constant} [ (a, b), (b, c), ... ]
    :return:
    """"""
    person = tf.reshape(person, (-1, 3))
    distances = tf.map_fn(
        lambda limb: tf.sqrt(
            tf.reduce_sum(
                tf.square(person[limb[0]] - person[limb[1]]))),
        limbs,
        dtype=tf.float32
    )
    return distances


def mean_limb_length_per_sequence(y_true, y_pred, limbs):
    """"""
    :param y_true: (n_frames, x J * 3)
    :param y_pred: (n_frames, x J * 3)
    :param limbs: {tf.constant}
    :return:
    """"""
    diff = tf.map_fn(
        lambda x:
        tf.reduce_mean(
            tf.abs(get_limb_lengths(x[0], limbs) -
                   get_limb_lengths(x[1], limbs))),
        (y_true, y_pred),
        dtype=tf.float32
    )
    return diff


def mean_limb_length_on_batch(y_true, y_pred, limbs):
    """""" This one is optimized for CMU-MoCap
    :param y_true: (batchsize x n_frames x J * 3)
    :param y_pred: (batchsize x n_frames x J * 3)
    :return:
    """"""
    loss = tf.map_fn(
        lambda x: tf.reduce_mean(
            mean_limb_length_per_sequence(x[0], x[1], limbs)),
        (y_true, y_pred),
        dtype=tf.float32
    )
    return tf.reduce_mean(loss)


def mean_limb_length(y_true, y_pred):
    """""" This one is optimized for CMU-MoCap
    :param y_true: (batchsize x n_frames x J * 3)
    :param y_pred: (batchsize x n_frames x J * 3)
    :return:
    """"""
    limbs = tf.constant([
        (0, 1), (1, 2), (2, 3), (3, 4)
    ])
    return mean_limb_length_on_batch(y_true,
                                     y_pred,
                                     limbs)


bs = 512
f1 = 20
f2 = 2
J = 5
dim = 3
X = np.random.random((bs, f1, J * dim))
Y = np.random.random((bs, f2, J * dim))

def create_gen():
    while True:
        yield X, Y

gen = create_gen()

inputs = Input(shape=X.shape[1:])
x = Flatten()(inputs)
x = Dense(f2 * J * dim)(x)
x = Reshape((f2, J, dim))(x)

model = Model(inputs=inputs, outputs=x)
model.summary()

optimizer = 'adam'

model.compile(loss=mean_limb_length,
              metrics=[mean_limb_length],
              optimizer=optimizer)

# model.fit(X, Y)
model.fit_generator(generator=gen,
                    validation_data=gen,
                    steps_per_epoch=10,
                    validation_steps=3,
                    epochs=5)
```

**Other info / logs**
Error message:
```
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-3-1232392e144b> in <module>
    104                     steps_per_epoch=10,
    105                     validation_steps=3,
--> 106                     epochs=5)

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)
   1500         use_multiprocessing=use_multiprocessing,
   1501         shuffle=shuffle,
-> 1502         initial_epoch=initial_epoch)
   1503 
   1504   def evaluate_generator(self,

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py in model_iteration(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)
    191       progbar.on_batch_begin(step, batch_logs)
    192 
--> 193       batch_outs = batch_function(*batch_data)
    194       if not isinstance(batch_outs, list):
    195         batch_outs = [batch_outs]

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in train_on_batch(self, x, y, sample_weight, class_weight, reset_metrics)
   1259       else:
   1260         self._make_fit_function()
-> 1261         outputs = self._fit_function(ins)  # pylint: disable=not-callable
   1262 
   1263     if reset_metrics:

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3080 
   3081     fetched = self._callable_fn(*array_vals,
-> 3082                                 run_metadata=self.run_metadata)
   3083     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3084     return nest.pack_sequence_as(self._outputs_structure,

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1438           ret = tf_session.TF_SessionRunCallable(
   1439               self._session._session, self._handle, args, status,
-> 1440               run_metadata_ptr)
   1441         if run_metadata:
   1442           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    542             None, None,
    543             compat.as_text(c_api.TF_Message(self.status.status)),
--> 544             c_api.TF_GetCode(self.status.status))
    545     # Delete the underlying status object from memory otherwise it stays alive
    546     # as there is a reference to status from this from the traceback due to
NotFoundError: Resource __per_step_7/_tensor_arraysloss/reshape_loss/map/TensorArray_1_1/N10tensorflow11TensorArrayE does not exist.
	 [[{{node training/Adam/gradients/loss/reshape_loss/map/while/TensorArrayReadV3_1_grad/TensorArrayGrad/TensorArrayGradV3}}]]
	 [[training/Adam/gradients/b_count_6/_233]]
```

"
25777,"What does a dim tensor have a dimension with 0-size? such as tf.Tensor([], shape=(2, 2, 0), dtype=int32)","- tensorShapes may have an 0-size dimension, but when are they going to be applied? Could you give me an example? This troubles me a lot...  THANKS A LOT!

for example  `tf.one_hot`  
On the implementation side, Tensorflow turns out to be verifying that `depth>= 0`, which are going to generate the tensor with a dimension with 0-size"
25776,convert yolov3 model to tflite,"not able to convert yolov3 model to tflite format

converted the weights file to pb format

then

./bazel-bin/tensorflow/lite/toco/toco --input_file=yolov3_finale_manga109.pb --output_file=foo.tflite --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_arrays=input --input_shapes=1,406,406,3 --output_arrays=output --allow_nonexistent_arrays

got error

2019-02-15 17:23:48.560625: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 2044 operators, 2743 arrays (0 quantized)
2019-02-15 17:23:48.601097: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After Removing unused ops pass 1: 0 operators, 2 arrays (0 quantized)
2019-02-15 17:23:48.601134: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!
2019-02-15 17:23:48.601217: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 0 operators, 2 arrays (0 quantized)
2019-02-15 17:23:48.601228: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!
2019-02-15 17:23:48.601237: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 0 operators, 2 arrays (0 quantized)
2019-02-15 17:23:48.601243: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:136] Model is empty!!!
2019-02-15 17:23:48.601249: W tensorflow/lite/toco/tooling_util.cc:1263] Fixing constant output array output by inserting a copy. This is not optimal.
2019-02-15 17:23:48.601260: F ./tensorflow/lite/toco/model.h:2134] Check failed: has_shape() 
"
25775,tf.nn.dropout is not working properly with keep_rate 1.,"**System information**
- Linux Ubuntu 18.04
- TensorFlow installed with conda
- TensorFlow version 1.12.0
- Python version 3.6.6
- CUDA/cuDNN version: 9.0/7
- GPU model and memory: GTX 1080 TI 11GB

**Describe the current behavior**
when using ```tf.nn.dropout``` with ```placeholder_with_default(1)``` during forwarding validation data it still makes use of random which changes the state for the next train operation and so the training procedure is not deterministic. 

**Describe the expected behavior**
My opinion is that the following code: ```tensor_util.constant_value(keep_prob)``` should return the default value which is 1 and should return ```x```, but for some reason the value is ```None``` and the tf.seed changes for the next batches of train data, which cause the instability of training procedure"
25773,Build error during TF build from the source with XLA enabled,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip without virtualenv
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): I'm not sure what version I'm using but, I'm using VS2015 and _MSC_VER is 1900
- CUDA/cuDNN version: CUDA v9.0.176 / cuDNN v7.4.2.24
- GPU model and memory: GTX960M (2GB)


**Describe the problem**
I want to build TF from source with XLA enabled.
So, I tried to build, configuring XLA to be enabled.
But the result was build failed and the log seems to say that 'xla_data.pb.h' file is having some trouble for TS build.
I could not find the real file (xla_data.pb.h), even if the log points where the file exist (bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h), because the path of the file is temporary during the build as I know.
Because even if I modify the bazel's temporary file, this problem would not be resolved radically, I want to find other way for this problem.
Could you give me some help about this problem ?

Thanks!

**Provide the exact sequence of commands / steps that you executed before running into the problem**

> C:\tensorflow>python ./configure.py
> WARNING: Running Bazel server needs to be killed, because the startup options are different.
> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
> You have bazel 0.15.0 installed.
> Please specify the location of python. [Default is C:\Users\nickeys\AppData\Local\Programs\Python\Python36\python.exe]:
> 
> 
> Found possible Python library paths:
>   C:\Users\nickeys\AppData\Local\Programs\Python\Python36\lib\site-packages
> Please input the desired Python library path to use.  Default is [C:\Users\nickeys\AppData\Local\Programs\Python\Python36\lib\site-packages]
> 
> Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: N
> No Apache Ignite support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with XLA JIT support? [y/N]: Y
> XLA JIT support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with ROCm support? [y/N]:
> No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: Y
> CUDA support will be enabled for TensorFlow.
> 
> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:
> 
> 
> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:
> 
> 
> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:
> 
> 
> Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:
> 
> 
> Please specify a list of comma-separated Cuda compute capabilities you want to build with.
> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
> Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0
> 
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
> 
> 
> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
> Eigen strong inline overridden.

And then,

> C:\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
INFO: From ProtoCompile tensorflow/core/protobuf/replay_log.pb.cc:
tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/protobuf/cluster.proto but not used.
tensorflow/core/protobuf/replay_log.proto: warning: Import tensorflow/core/framework/graph.proto but not used.
ERROR: C:/tensorflow/tensorflow/compiler/xla/service/cpu/BUILD:536:1: C++ compilation of rule '//tensorflow/compiler/xla/service/cpu:runtime_fft' failed (Exit 2): msvc_wrapper_for_nvcc.bat failed: error executing command
  cd C:/users/nickeys/_bazel_nickeys/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\INCLUDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\INCLUDE;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\ucrt;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.14393.0\winrt;
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\LIB\amd64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\ATLMFC\LIB\amd64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.14393.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.6.1\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.14393.0\um\x64;
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\BIN\amd64;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\IDE;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Common7\Tools;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio 14.0\Team Tools\Performance Tools;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Windows Kits\10\bin\x86;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;;C:\WINDOWS\system32
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TEMP=C:\Users\nickeys\AppData\Local\Temp
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
    SET TMP=C:\Users\nickeys\AppData\Local\Temp
  external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.bat /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/genfiles /Iexternal/nsync /Ibazel-out/x64_windows-opt/genfiles/external/nsync /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/genfiles/external/bazel_tools /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Iexternal/local_config_sycl /Ibazel-out/x64_windows-opt/genfiles/external/local_config_sycl /Iexternal/protobuf_archive /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/genfiles/external/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/genfiles/external/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/protobuf_archive/src /Ibazel-out/x64_windows-opt/genfiles/external/protobuf_archive/src /Ibazel-out/x64_windows-opt/bin/external/protobuf_archive/src /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG -w /arch:AVX -DEIGEN_AVOID_STL_ARRAY /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/xla/service/cpu/_objs/runtime_fft/runtime_fft.o /c tensorflow/compiler/xla/service/cpu/runtime_fft.cc
bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h(242): error C2059: syntax error: 'constant'
bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h(242): error C3805: 'constant': unexpected token, expected either '}' or a ','
bazel-out/x64_windows-opt/genfiles\tensorflow/compiler/xla/xla_data.pb.h(249): error C2065: 'TOKEN': undeclared identifier
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 2765.994s, Critical Path: 2700.12s
INFO: 2934 processes: 2934 local.
FAILED: Build did NOT complete successfully
```
"
25772,Unable to set estimator.model_dir after estimator initialization,"## System information:
Tensorflow version: 1.9.0 (installed from binary)
Python version: 3.6.8

## Problem
Setting the value of `estimator.model_dir` _after_ the estimator has already been created does not work.  I specifically want to set the value of this after initialization, but only see a way to set it _during_ initialization.  Is there a reason that tensorflow does not support setting the `model_dir` after estimator creation? If not, is there a viable workaround?

## Source code:
```
classifier = tf.estimator.DNNClassifier(
    feature_columns=some_feature_classifier,
    hidden_units=[10, 10],
    n_classes=3)
# other code...
classifier.model_dir = ""/tmp/model_dir_file""
```

## Error message:
```
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File ""<stdin>"" in <module>
----> 1 classifier.model_dir = ""/tmp/model_dir_file""

AttributeError: can't set attribute
```

Thank you!"
25768,tf.contrib.quantize.create_eval_graph does not add min/max node to bias,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux archlinux 4.19.2-arch1-1-ARCH #1 SMP PREEMPT Tue Nov 13 21:16:19 UTC 2018 x86_64 GNU/Linux`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `binary`
- TensorFlow version (use command below): `v1.12.0-0-ga6d8ffae09 1.12.0`
- Python version: `3.6.7`
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: `none`
- GPU model and memory: `none`

**Describe the current behavior**

I want to convert a model to TFLite with quantized uint8 as inference type. I add the min/max nodes to the graph by calling `tf.contrib.quantize.create_eval_graph`, then I load the trained values (both the weights and the min/max values). I try to convert the model with TFLiteConverter, but I get errors of this kind:

 `Array conv2d/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.`

I look at my graph (the one modified by `tf.contrib.quantize.create_eval_graph`) and I don't see min/max nodes for biases.

**Describe the expected behavior**

The biases should have min/max nodes to be used by TFLiteConverter after calling `tf.contrib.quantize.create_eval_graph`.

**Code to reproduce the issue**

This example can reproduce this bug using a single Conv2d layer. Note that for the sake of keeping the example simple, I simulate loading the weights and the values of the min/max nodes (learned during training) by simply running `tf.global_variables_initializer()`. 

```python
import tensorflow as tf
import tensorflow.contrib.lite as tflite

sess = tf.keras.backend.get_session()

# create model
x = x_input = tf.keras.layers.Input([10,10,3])
x = tf.keras.layers.Conv2D(1, 3)(x)
model = tf.keras.Model(x_input, x)

# add fake quantization nodes 
tf.contrib.quantize.create_eval_graph(sess.graph)

# simulate loading weights and quantization min/max values
sess.run(tf.global_variables_initializer())

# try to quantize
toco_converter = tflite.TFLiteConverter.from_session(sess, model.inputs, model.outputs)
toco_converter.post_training_quantize = True
toco_converter.inference_type = tflite.constants.QUANTIZED_UINT8
toco_converter.inference_input_type = tflite.constants.QUANTIZED_UINT8
toco_converter.quantized_input_stats = {model.inputs[0].name.split(':')[0]: (0., 255.)}
model_tflite_binary = toco_converter.convert()
```
gives the following error:
```
2019-02-14 15:12:39.980124: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 8 operators, 13 arrays (0 quantized)
2019-02-14 15:12:39.980218: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 8 operators, 13 arrays (0 quantized)
2019-02-14 15:12:39.980322: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 2 operators, 5 arrays (1 quantized)
2019-02-14 15:12:39.980352: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 2 operators, 5 arrays (1 quantized)
2019-02-14 15:12:39.980371: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before quantization graph transformations: 2 operators, 5 arrays (1 quantized)
2019-02-14 15:12:39.980401: F tensorflow/contrib/lite/toco/graph_transformations/quantize.cc:129] Array conv2d/BiasAdd does not have MinMax information, and is not a constant array. Cannot proceed with quantization.
```

When I look at the content of tf.global_variables(),  I see nothing related to bias_quant: 
```
[<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 1) dtype=float32>,
 <tf.Variable 'conv2d/bias:0' shape=(1,) dtype=float32>,
 <tf.Variable 'conv2d/weights_quant/min:0' shape=() dtype=float32_ref>,
 <tf.Variable 'conv2d/weights_quant/max:0' shape=() dtype=float32_ref>,
 <tf.Variable 'conv2d/act_quant/min:0' shape=() dtype=float32_ref>,
 <tf.Variable 'conv2d/act_quant/max:0' shape=() dtype=float32_ref>]
```

Same thing when I look as the graph in tensorboard:
```python
with tf.summary.FileWriter('/tmp/conv') as fw:
    fw.add_graph(sess.graph)
```

![picture-of-tensorboard](https://user-images.githubusercontent.com/14935326/52816460-48a84080-306f-11e9-9cb2-e3fb2a044f58.png)

**Other info / logs**

Event file from FileWriter: [log.zip](https://github.com/tensorflow/tensorflow/files/2866713/log.zip)"
25764,Different behavior between Keras and tensorflow.keras in v1.13.0rc1 on placeholder and variable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
 - TensorFlow installed from (source or binary): Binary I suppose
- TensorFlow version (use command below): 1.13.0rc1
- Python version: 3.6

**Describe the current behavior**

So I'm teaching a course and decide to use `tensorflow.keras` instead of Keras.
I then noticed how the two systems handle the `placeholder` and `variable` are different even though
they are using the same backend. I was trying to compile a simple adding function:

```python
# variable can be initialized with a value like this
init_variable_1 = np.zeros(shape=(2, 2))
variable_1 = K.variable(value=init_variable_1)

# variable can also be initialized with particular functions like this
variable_2 = K.ones(shape=(2, 2))

add_tensor = variable_1+variable_2
add_function = K.function(inputs=[variable_1, variable_2],
                          outputs=(add_tensor,))
```

Until this line, both Keras and `tensorflow.keras` allow me to run without errors or warning.

However, for the following code, Keras can pass while `tensorflow.keras features an error:

```
a = np.array([[1, 3], [2, 4]], dtype=np.float32)
b = np.array([[3, 2], [5, 6]], dtype=np.float32)

print(add_function((a,b)))
```

The error is:

```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-8-4df03c3d618b> in <module>()
     10 print(a, b)
     11 
---> 12 print(add_function((a,b)))
     13 
     14 # notice that the add_function created is independent of the variables

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3074 
   3075     fetched = self._callable_fn(*array_vals,
-> 3076                                 run_metadata=self.run_metadata)
   3077     self._call_fetch_callbacks(fetched[-len(self._fetches):])
   3078     return nest.pack_sequence_as(self._outputs_structure,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __call__(self, *args, **kwargs)
   1437           ret = tf_session.TF_SessionRunCallable(
   1438               self._session._session, self._handle, args, status,
-> 1439               run_metadata_ptr)
   1440         if run_metadata:
   1441           proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: ResourceHandleToInputTensor() received non-DT_RESOURCE Tensor: 1
	 [[{{node _arg_Variable_0_0}}]]
```

**Describe the expected behavior**

So the API `function` defines the input should be a list of `placeholders`.
So either someone (maybe @fchollet) need to change the definition in the API so both `placeholder` and `variable` are fine to be valid inputs (looks like they can be compiled fine in both systems). And make `tensorflow.keras` correct also. Or limit the input tensors to be `placeholder`s where some type checks need to be done.

By the way, I'm not sure this os a specific bug in v1.13.0rc1 and v1.12 or it happens to other releases as well since I was using Colab.

Regards,
Yuhuang."
25762,Missing clean_dep call in tf_custom_op_library_additional_deps,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13.0-rc0
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: 10

**Describe the problem**

I believe the 'if_windows' part of 
https://github.com/tensorflow/tensorflow/blob/248016edd1555315c4db540e668d42b95292c2c7/tensorflow/tensorflow.bzl#L1469
is missing a call to clean_deps() and should be:

    ] + if_windows([clean_dep(""//tensorflow/python:pywrap_tensorflow_import_lib"")])


Note that I am not actually using windows, and my custom op builds and works fine. However, when I call:

    bazel query ""deps(//learning/tensorflow_ops:custom_op_tf.so)""

I get the error

    ERROR: learning/tensorflow_ops/BUILD:9:1: no such package 'tensorflow/python': BUILD file not found on package path and referenced by '//learning/tensorflow_ops:custom_op_tf.so_check_deps'
    ERROR: Evaluation of query ""deps(//learning/tensorflow_ops:custom_op_tf.so)"" failed: errors were encountered while computing transitive closure"
25761,XLA makes incorrect calls into cuBLAS,"XLA GPU will sometimes make what looks like illegal calls into cuBLAS.

```
 $ cat /tmp/bad-dot.hlo
HloModule Main

ENTRY Main {
  p0 = f32[6656,96] parameter(0)
  p1 = f32[6656,384] parameter(1)
  dot = f32[96,384] dot(p0, p1), lhs_contracting_dims={0}, rhs_contracting_dims={0}
  ROOT tr = f32[384,96] transpose(dot), dimensions={1,0}
}
```

```
 $ bazel run -c opt tensorflow/compiler/xla/tools:replay_computation_gpu -- /tmp/bad-dot.hlo --use_fake_data
```

```
...
2019-02-14 09:17:41.060396: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened CUDA library libcudart.so.9.0
2019-02-14 09:17:41.062661: E tensorflow/compiler/xla/tools/replay_computation.cc:389] iteration complete.
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ** On entry to GEMM_EX  parameter number 9 had an illegal value
 ...
```"
25760,Sudden drop in conv3d_transpose GPU performance with large input sizes,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): _Yes_
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): _Ubuntu 18.10_
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: /
- TensorFlow installed from (source or binary): _Source_
- TensorFlow version (use command below): _'v1.12.0-0-ga6d8ffae09' 1.12.0_
- Python version: _3.6.7_
- Bazel version (if compiling from source): _0.18.1_
- GCC/Compiler version (if compiling from source): _4.8_
- CUDA/cuDNN version: _10.0, 7.3.1_
- CPU model: _Intel(R) Xeon(R) Bronze 3106 CPU @ 1.70GHz_
- GPU model and memory: _Nvidia GeForce GTX 1080 Ti 11GB_

**Describe the current behavior**
On GPU, the _conv3d_transpose_ operation shows a huge spike in terms of execution time when increasing input size from `(1, 128, 128, 128, 16)` to `(1, 256, 256, 256, 16)`.
This particular change makes average execution time without the first run change from **0.7s** to **92s**.
In comparison, the same operation on CPU goes from **2.3s** to **18.2s**.
This makes _conv3d_transpose_ perform better on CPU compared to CPU using this particular shape.

**Describe the expected behavior**
The _conv3d_transpose_ operation should not see such a sudden drop in performance when increasing input size.

**Code to reproduce the issue**
Attached is a python notebook that reproduces the issue.
[tf_conv3d.zip](https://github.com/tensorflow/tensorflow/files/2865718/tf_conv3d.zip)

**Other info / logs**
![download](https://user-images.githubusercontent.com/9284862/52798293-ee5fad80-3077-11e9-9b75-edc1ff504e8b.png)
This figure shows the average execution time of _conv3d_ and _conv3d_transpose_ operations on both CPU on GPU with a log scale for both x and y axis.
The x axis represents the resolution which determines the input shape in the following manner for _conv3d_ `(1, resolution, resolution, resolution, 16)` and determines the output shape for _conv3d_transpose_ `(1, resolution, resolution, resolution, 1)`.
GPU runs are in blue while CPU runs are in red.
_conv3d_ are represented with dash-dotted lines and _conv3d_tranpose_ with solid lines.
There is a sudden increase in the execution time of the _conv3d_transpose_ operation on GPU which goes from **0.7s** to **92s** when changing the resolution from **2^8** to **2^9**.

Attached is a full run of the attached python notebook that includes the full data.
[tf_conv3d.pdf](https://github.com/tensorflow/tensorflow/files/2865802/tf_conv3d.pdf)
"
25759,gpudelegate + mobilenet_v1_1.0_224.tflite on a gles 3.1 device that reports MAX_WORK_COMPUTE_GROUP_INVOCATIONS = 256,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
* OS Platform and Distribution: Linux Ubuntu 16.04
* Mobile device:  device running Android P and gpu with OpenGL ES 3.1  support
* TensorFlow Lite version on Android: 0.0.0-gpu-experimental
* Test app: TfLiteCameraDemo + GpuDelegate + mobilenet_v1_1.0_224.tflite

**Describe the current behavior**
Hello,
I have been trying to get TfLiteCameraDemo  working through the GpuDelegate on an Android P device with a gpu supporting opengles 3.1.
I am trying to run mobilenet v1_1 float model.

According to  OpenGL ES 3.1 spec, the total number of compute shader invocations in a single local work group (i.e. the product of all three values used to describe the local work group size) must not exceed the value reported for GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS. This limit is guaranteed to be no lower than 128.

The compute shaders being prepared by tensorfllow Gpu Delegate seem to not check this restriction and just try to dispatch  shaders with with a workgroup size of 1024 :
 "" layout(local_size_x = 16, local_size_y = 8, local_size_z = 8) in"", 
irrespective of what the platform declares for GL_MAX_COMPUTE_WORK_GROUP_INVOCATIONS.

Due to this, the shaders fail to compile. This particular gpu supports a workgroup size of 256, which is above the minum 128 required by gles 3.1 spec.

Gpu Delegate  webpage doesn't mention any other requirements except a device with  OpenGl ES >=  3.1.

Is there any chance the code can be tailored to check the limits  reported by the device and change the shaders accordingly?

Thank you"
25757,"tensorflow/stream_executor/cuda/cuda_dnn.cc:82] The primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided","I am running Keras with TF backend on venv Python 3.7.2.  

**System information**
** pip list: 
absl-py              0.6.1      
astor                0.7.1      
backcall             0.1.0      
biwrap               0.1.6      
bleach               3.0.2      
cloudpickle          0.6.1      
cycler               0.10.0     
dask                 1.0.0      
decorator            4.3.0      
defusedxml           0.5.0      
entrypoints          0.2.3      
gast                 0.2.0      
grpcio               1.17.0     
h5py                 2.8.0      
imageio              2.4.1      
imgaug               0.2.6      
ipykernel            5.1.0      
ipython              7.2.0      
ipython-genutils     0.2.0      
ipywidgets           7.4.2      
jedi                 0.13.1     
Jinja2               2.10       
jsonschema           2.6.0      
jupyter              1.0.0      
jupyter-client       5.2.4      
jupyter-console      6.0.0      
jupyter-core         4.4.0      
Keras                2.1.3      
Keras-Applications   1.0.6      
Keras-Preprocessing  1.0.5      
kiwisolver           1.0.1      
Markdown             3.0.1      
MarkupSafe           1.1.0      
matplotlib           3.0.2      
mistune              0.8.4      
mock                 2.0.0      
nbconvert            5.4.0      
nbformat             4.4.0      
networkx             2.2        
notebook             5.7.2      
numpy                1.15.4     
opencv-python        3.4.4.19   
pandas               0.24.1     
pandocfilters        1.4.2      
parso                0.3.1      
pbr                  5.1.1      
pexpect              4.6.0      
pickleshare          0.7.5      
Pillow               5.3.0      
pip                  18.1       
prometheus-client    0.5.0      
prompt-toolkit       2.0.7      
protobuf             3.6.1      
ptyprocess           0.6.0      
pydicom              1.2.1      
Pygments             2.3.0      
pyparsing            2.3.0      
python-dateutil      2.7.5      
pytz                 2018.9     
PyWavelets           1.0.1      
PyYAML               3.13       
pyzmq                17.1.2     
qtconsole            4.4.3      
scikit-image         0.14.1     
scipy                1.1.0      
Send2Trash           1.5.0      
setuptools           39.0.1     
Shapely              1.6.4.post2
six                  1.12.0     
tensorboard          1.12.0     
tensorflow           1.12.0rc0  
tensorflow-estimator 1.10.12    
tensorflow-plot      0.3.0.dev0 
termcolor            1.1.0      
terminado            0.8.1      
testpath             0.4.2      
toolz                0.9.0      
tornado              5.1.1      
traitlets            4.3.2      
wcwidth              0.1.7      
webencodings         0.5.1      
Werkzeug             0.14.1     
wheel                0.32.3     
widgetsnbextension   3.4.2 

** python3 --version
Python 3.7.2

** lsb_release -a
Description:	Ubuntu 16.04.5 LTS
Release:	16.04
Codename:	xenial

** lspci | grep VGA
01:00.0 VGA compatible controller: NVIDIA Corporation Device 1b02 (rev a1)
02:00.0 VGA compatible controller: NVIDIA Corporation Device 1b02 (rev a1)
 
** VGAs detected by TF
2019-02-14 13:56:21.125443: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): TITAN Xp, Compute Capability 6.1
2019-02-14 13:56:21.125447: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (1): TITAN Xp, Compute Capability 6.1
2019-02-14 13:56:21.125746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1431] Found device 0 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:01:00.0
totalMemory: 11.91GiB freeMemory: 11.15GiB
2019-02-14 13:56:21.125898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1431] Found device 1 with properties: 
name: TITAN Xp major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:02:00.0
totalMemory: 11.91GiB freeMemory: 11.75GiB

** python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
b'v1.12.0-rc0-4136-g840be30' 1.12.0-rc0

** cat /usr/local/cuda-10.0/include/cudnn.h | grep CUDNN_MAJOR -A 2
#define CUDNN_MAJOR 7
#define CUDNN_MINOR 4
#define CUDNN_PATCHLEVEL 1
--
#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)
#include ""driver_types.h""

**  nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2018 NVIDIA Corporation
Built on Sat_Aug_25_21:08:01_CDT_2018
Cuda compilation tools, release 10.0, V10.0.130




After couple of iterations within the first epoch of training I get the error ""E tensorflow/stream_executor/cuda/cuda_dnn.cc:82] The primary convolution algorithm failed memory allocation, while a secondary algorithm is not provided"" then my loss function becomes ""nan"". "
25754,Model.fit() leaves training arg and learning_phase unset (TF2-preview),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Mac OS X 10.13.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
VERSION='2.0.0-dev20190213'
GIT_VERSION=""v1.12.0-7959-gabeb4d0acd""
- Python version:
3.6.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
N/A
- GPU model and memory:
N/A

**Describe the current behavior**
When training a Keras model containing a custom layer whose behavior depends on the training phase, `training` is not set by the `fit()` method, and `K.learning_phase()` is `0`. I can work around this issue by calling `K.set_learning_phase(1)` before creating the model, but oddly this does not work if I call `K.set_learning_phase(1)` _after_ creating and compiling the model, and just before calling the `fit()` method. It looks like the `learning_phase` is ""hard-coded"" into the call() method's graph when the model is built?

**Describe the expected behavior**
I expected the `training` argument to be set to `True` (or `1`) automatically when I called the `fit()` method. Alternatively, I expected that at least `K.learning_phase()` would return `True` (or `1`), but it's always `0`.

**Code to reproduce the issue**
If you're in a hurry, just look at the failing tests: C, H and I.

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
K = keras.backend

class MyLayer(keras.layers.Layer):
    @tf.function
    def call(self, inputs, training=None):
        if training is None:
            training = K.learning_phase()
        tf.print(""training: "", training)
        return keras.backend.in_test_phase(inputs + 1., inputs + 2., training)

X = np.zeros((1, 2))

print(""Test A: MyLayer()(X)"")
y_pred = MyLayer()(X)
print(""Pred:"", y_pred)
print(""training should be 0\n"")

print(""Test B: MyLayer()(X, training=True)"")
y_pred = MyLayer()(X, training=True)
print(""Pred:"", y_pred)
print(""training should be True\n"")

print(""Test C: model = ...; model.compile(...); model.fit(...)"")
model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss=""mse"", optimizer=""sgd"")
history = model.fit(X, X, epochs=2)
print(""training should be 1 (used to fail, now passes)\n"")

print(""Test D: K.set_learning_phase(1); MyLayer()(X)"")
K.set_learning_phase(1)
y_pred = MyLayer()(X)
print(""Pred:"", y_pred)
print(""training should be 1\n"")
K.clear_session()

print(""Test E: layer = MyLayer(); K.set_learning_phase(1); layer(X)"")
layer = MyLayer()
K.set_learning_phase(1)
y_pred = layer(X)
print(""Pred:"", y_pred)
print(""training should be 1\n"")
K.clear_session()

print(""Test F: K.set_learning_phase(1); layer = MyLayer(); K.set_learning_phase(0); layer(X)"")
K.set_learning_phase(1)
layer = MyLayer()
K.set_learning_phase(0)
y_pred = layer(X)
print(""Pred:"", y_pred)
print(""training should be 0\n"")
K.clear_session()

print(""Test G: K.set_learning_phase(1); model = ...; model.compile(...); model.fit(...)"")
K.set_learning_phase(1)
model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss=""mse"", optimizer=""sgd"")
history = model.fit(X, X, epochs=2)
print(""training should be 1\n"")
K.clear_session()

print(""Test H: model = ...; model.compile(...); K.set_learning_phase(1); model.fit(...)"")
model = keras.models.Sequential([MyLayer(input_shape=[2])])
model.compile(loss=""mse"", optimizer=""sgd"")
K.set_learning_phase(1)
history = model.fit(X, X, epochs=2)
print(""training should be 1 (ERROR?)\n"")
K.clear_session()

print(""Test I: K.set_learning_phase(1); model = ...; K.set_learning_phase(0); model.compile(...); model.fit(...)"")
K.set_learning_phase(1)
model = keras.models.Sequential([MyLayer(input_shape=[2])])
K.set_learning_phase(0)
model.compile(loss=""mse"", optimizer=""sgd"")
history = model.fit(X, X, epochs=2)
print(""This test does not make much sense, why would you call fit with learning phase 0?\n"")
K.clear_session()
```"
25753,Failed to load native tensorflow ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- **Windows 7**
- **pip install tensorflow-gpu**
- Python version: **3.6.6**
- Installed using virtualenv? pip? conda?: **pip**
- CUDA/cuDNN version: **CUDA 8.0 cuDNN 7.1**
- GPU model and memory: **Model : Quadro 4000 , Memory : 18GB**



Installed using pip install tensorflow-gpu (No error, perfectly installed)
While importing tensorflow , i got a error : **Failed to load native tensorflow .ImportError: DLL load failed: The specified module could not be found**



"
25752,Complex zero to the power zero is nan,"This is a simple yet dramatic bug: 
0^0 does not return 1 for complex numbers  (left zero complex) !

**System information**

- OS Platform and Distribution : Linux Ubuntu 16.04 LTS
- TensorFlow installed from binary : pip 
- TensorFlow version : b'v1.13.0-rc1-0-g54c5616308' 1.13.0-rc1
- Python version: Python 3.5.2

**Describe the current behavior**

`tf.pow(tf.complex(0., 0.), 0)` returns `nan + nan j`

**Describe the expected behavior**

It should be `1 + 0j` as is returned by `np.complex(0., 0.)**0`

**Code to reproduce the issue**

`z = tf.complex(0., 0.);
sess = tf.Session();
sess.run( tf.pow(z, 0) )`


"
25751,[question][tflite][java] How to allocate mulit dimensional output array to feed in interpreter.run(),"I try to run a version `posenet `on an android app with tflite.
The app is based on the GPU delegate demo:
https://medium.com/tensorflow/tensorflow-lite-now-faster-with-mobile-gpus-developer-preview-e15797e6dee7
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/java/demo

Posenet takes an Image as Input and computes as output multiple arrays of the shape:
`1x14x14x17,  1x14x14x34, 1x14x14x32,  1x14x14x32`

I know how to allocate a bytbuffer for the image, so thats not the problem.
But how do i allocate a buffer for that output to be able to successfully feed the input and output buffer to the interpreter like:
```
import org.tensorflow.lite.Interpreter;
Interpreter tflite;

ByteBuffer input = null;
input = ByteBuffer.allocateDirect(...);
output = ?

tflite.run(input,output);
```

I tried something like this for the float version:
```
float[][][][] output = null;
output = new float[1*14*14*17][1*14*14*34][1*14*14*32][1*14*14*32];
```
but this leads to a memory oom. So how do I correctly allocate a buffe for the output with an array with the right dimensions. (I am not so used to java, more to python)

EDIT:
and I also tried something like this, but java does not allow it:
```
float[][][][] out1 = new float[1][14][14][17];
float[][][][] out2 = new float[1][14][14][34];
float[][][][] out3 = new float[1][14][14][32];
float[][][][] out4 = new float[1][14][14][32];
float[] output = new float[out1, out2, out3, out4];
```"
25747,"failed to ""bazel build -c opt //tensorflow:libtensorflow.so""","**System information**
- OS Platform and Distribution (centos 7):
- TensorFlow installed from (source ):
- TensorFlow version: master
- Python-2.7.5-76.el7.x86_64
- swig-2.0.10-5.el7.x86_64
- Bazel version (0.22):
- GPU model and memory:No

I followed ""https://github.com/tensorflow/tensorflow/tree/master/tensorflow/go/README.md""
1) install bazel
       bazel version: 
            INFO: Invocation ID: cbd6d19b-6c11-4652-a9c0-ffa1b57e10d0
            Build label: 0.22.0- (@non-git)
            Build target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel 
                       /BazelServer_deploy.jar
            Build time: Tue Jan 29 18:22:45 2019 (1548786165)
            Build timestamp: 1548786165
            Build timestamp as int: 1548786165

2) sudo yum install python swig python-numpy 
3) go get -d github.com/tensorflow/tensorflow/tensorflow/go
4) cd ${GOPATH}/src/github.com/tensorflow/tensorflow
   ./configure
     I press enter all the time.
5)  bazel build -c opt //tensorflow:libtensorflow.so 
    step 5 failed, the error info:

INFO: Invocation ID: 95b3b7e1-bdf2-4141-b259-38569d76b255
ERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):
	File ""/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 163
		_clone_or_update(ctx)
	File ""/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 73, in _clone_or_update
		fail((""error cloning %s:\n%s"" % (ctx....)))
error cloning build_bazel_rules_swift:
+ cd /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external
+ rm -rf /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift
+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift
+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift reset --hard tags/0.6.0
Unknown option: -C
usage: git [--version] [--help] [-c name=value]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]
+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0
Unknown option: -C
usage: git [--version] [--help] [-c name=value]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]
+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0
Unknown option: -C
usage: git [--version] [--help] [-c name=value]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]
ERROR: error loading package '': Encountered error while reading extension file 'swift/repositories.bzl': no such package '@build_bazel_rules_swift//swift': Traceback (most recent call last):
	File ""/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 163
		_clone_or_update(ctx)
	File ""/home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/bazel_tools/tools/build_defs/repo/git.bzl"", line 73, in _clone_or_update
		fail((""error cloning %s:\n%s"" % (ctx....)))
error cloning build_bazel_rules_swift:
+ cd /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external
+ rm -rf /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift
+ git clone --depth=1 https://github.com/bazelbuild/rules_swift.git /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift
+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift reset --hard tags/0.6.0
Unknown option: -C
usage: git [--version] [--help] [-c name=value]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]
+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch --depth=1 origin tags/0.6.0:tags/0.6.0
Unknown option: -C
usage: git [--version] [--help] [-c name=value]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]
+ git -C /home/zyx/.cache/bazel/_bazel_zyx/4724421b4e14286ac51ffea3d739523c/external/build_bazel_rules_swift fetch origin tags/0.6.0:tags/0.6.0
Unknown option: -C
usage: git [--version] [--help] [-c name=value]
           [--exec-path[=<path>]] [--html-path] [--man-path] [--info-path]
           [-p|--paginate|--no-pager] [--no-replace-objects] [--bare]
           [--git-dir=<path>] [--work-tree=<path>] [--namespace=<name>]
           <command> [<args>]
INFO: Elapsed time: 5.616s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    Fetching ; Cloning tags/0.6.0 of https://github.com/bazelbuild/rules_swift\
.git 5s
"
25746,Object detection ppn tflite works wrong on mobile,"
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):N
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: HUAWEI MATE20
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):1.10
- Python version:3.6
- Bazel version (if compiling from source):1.15
- GCC/Compiler version (if compiling from source):1.9
- CUDA/cuDNN version:8.0
- GPU model and memory:16GB

I used  object detection API, the ssd_mobilenetV2 is working well including the train, eval ,inference on pc ,and .tflite on phone.
Recently, i try ppn_mobilenetv1, the train and eval is well. The frozen_inference_graph.pb also work well on pc and the results are right. When i convert it to tflite_graph.pb, then convert it to detect.tflite. No errors occur. But when it runs on the mobile, it shows totally wrong results. I can not find anything wrong here. Could anyone please help me find it out?

"
25744,Error when converting MTCNN(P_Net) pb model to tflite model,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.10.0
- Python version: 2.7.6
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): 4.8.4
- CUDA/cuDNN version: * N/A (build without support CUDA)
- GPU model and memory: * N/A (build without support GPU)



**Describe the problem**
I failed to covert my pb file to tflite format because opeator max was not supported. I find that maximum is a builtin operator and I want ask if operator max is the same as maximum ? Or I need to add a custom operator?

**Provide the exact sequence of commands / steps that you executed before running into the problem**
./bazel-bin/tensorflow/contrib/lite/toco/toco \
	--input_file=/home/zhengrui/tensorflow/pnet_144-192.pb \
	--output_file=/home/zhengrui/tensorflow/pnet_144-192.tflite \
	--input_format=TENSORFLOW_GRAPHDEF \
	--output_format=TFLITE \
	--inference_type=FLOAT  \
	--input_shape='1,144,192,3' \
	--input_array=pnet/input \
	--output_arrays=pnet/conv4-2/BiasAdd,pnet/prob1  


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
2019-02-14 14:33:08.816780: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 52 operators, 68 arrays (0 quantized)
2019-02-14 14:33:08.817309: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 52 operators, 68 arrays (0 quantized)
2019-02-14 14:33:08.818129: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 29 operators, 45 arrays (0 quantized)
2019-02-14 14:33:08.818535: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 29 operators, 45 arrays (0 quantized)
2019-02-14 14:33:08.818828: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:329] Total transient array allocated size: 3237696 bytes, theoretical optimal value: 3237696 bytes.
2019-02-14 14:33:08.818936: I tensorflow/contrib/lite/toco/toco_tooling.cc:388] Estimated count of arithmetic ops: 0.0936027 billion (note that a multiply-add is counted as 2 ops).
2019-02-14 14:33:08.819150: F tensorflow/contrib/lite/toco/tflite/export.cc:363] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Max.
Aborted (core dumped)
"
25743,where I get branch or tag for tensorflowlite+gpudelegate?,"dear tensorflow team.
I did not find any branch for tensorflow-lite+gpudelegate.

Can you share me?

"
25734,TPU has XLA compilation issue on TF 1.13rc1,"I am getting an issue with using XLA on the cloud TPU on tensorflow version 1.13 (but not 1.12).

tag:comp:tpus
comp:tpus

**System information**
- Using Google's cloud TPU with Tensorflow 1.12

System info:
```
== cat /etc/issue ===============================================
Linux aportnoy 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux
VERSION_ID=""9""
VERSION=""9 (stretch)""

== are we in docker =============================================
No

== compiler =====================================================
c++ (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
Copyright (C) 2016 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux aportnoy 4.9.0-8-amd64 #1 SMP Debian 4.9.130-2 (2018-10-27) x86_64 GNU/Linux

== check pips ===================================================
numpy (1.16.1)
protobuf (3.6.1)
tensorflow (1.12.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH is unset
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
./tf_env_collect.sh: line 105: nvidia-smi: command not found

== cuda libs  ===================================================
```

**Describe the current behavior**

When I uninstall tensorflow 1.12 and install tensorflow 1.13 via:
`pip3 uninstall tensorflow`
`pip3 install tensorflow==1.13.0rc1`

Then run my model, I get a very strange error which looks like this:

```
aportnoy@aportnoy:~$ python3 script.py
2019-02-13 17:59:43.159373: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-13 17:59:43.165347: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-02-13 17:59:43.165597: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5642dd857ad0 executing computations on platform Host. Devices:
2019-02-13 17:59:43.165641: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>
WARNING:tensorflow:From /home/aportnoy/.local/lib/python3.5/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
2019-02-13 17:59:43.305988: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:179] Subgraph fingerprint:7855156155640739707
2019-02-13 17:59:43.401974: W tensorflow/core/framework/op_kernel.cc:1401] OP_REQUIRES failed at xla_ops.cc:310 : Invalid argument: Expected LHS feature dimension (value 19) to be a multiple of feature_group_count (value 728), and LHS feature 
dimension / feature_group_count = RHS feature dimension (value 1); got <conv>(f32[1,19,19,728,1], f32[1,19,19,728,1])
Dimension numbers: {kernel_input_feature_dimension: 4
kernel_output_feature_dimension: 1
kernel_spatial_dimensions: 0
kernel_spatial_dimensions: 2
kernel_spatial_dimensions: 3
input_batch_dimension: 4
input_feature_dimension: 1
output_batch_dimension: 3
output_feature_dimension: 4
input_spatial_dimensions: 0
input_spatial_dimensions: 2
input_spatial_dimensions: 3
output_spatial_dimensions: 0
output_spatial_dimensions: 1
output_spatial_dimensions: 2
}.
         [[{{node gradients/sep0/separable_conv2d/depthwise_grad/DepthwiseConv2dNativeBackpropFilter}}]]
```

**Describe the expected behavior**

When using tensorflow version 1.12 there isn't an error:
```
aportnoy@aportnoy:~$ python3 script.py
2019-02-13 18:19:58.250311: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-13 18:19:58.404755: I tensorflow/compiler/jit/encapsulate_xla_computations_pass.cc:179] Subgraph fingerprint:1939207595948626618
2019-02-13 18:19:58.499394: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz
2019-02-13 18:19:58.499806: I tensorflow/compiler/xla/service/service.cc:149] XLA service 0x7f39fc00b5b0 executing computations on platform Host. Devices:
2019-02-13 18:19:58.499871: I tensorflow/compiler/xla/service/service.cc:157]   StreamExecutor device (0): <undefined>, <undefined>
```

**Code to reproduce the issue**
My `script.py`, reproduce with `python3 script.py`

```
import os
import tensorflow as tf
from tensorflow.contrib.compiler import xla

data_format = 'channels_first'

def simple(features, labels):
  net = features

  net = tf.keras.layers.SeparableConv2D(728, [3, 3],
    strides=1,
    padding='same',
    data_format=data_format,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    name='sep0')(net)
  
  net = tf.keras.layers.Flatten(data_format=data_format)(net)
  net = tf.keras.layers.Dense(10, activation=None, use_bias=True, name='fully0')(net)

  labels = tf.stop_gradient(labels)
  loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=net) )

  train_step = tf.train.GradientDescentOptimizer(1e-6).minimize(loss)

  return net, train_step

config = tf.ConfigProto()
config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1
sess = tf.Session(config=config)

images = tf.constant(1.2, shape=[1, 728, 19, 19], dtype=tf.float32)
labels = tf.constant(1, shape=[1, 10], dtype=tf.float32)

[y] = xla.compile(simple, inputs=[images, labels])

sess.run(tf.global_variables_initializer())
sess.run(y)
```

**Other info / logs**

The issue only exists on the new 1.13.
I would use 1.12 but my situation requires me to use 'channels_first' for my model, XLA enabled, and 1.13.

Here are also images of me running the script ( First on 1.12 which is working then on 1.13 which doesn't work ) :
![screen shot 2019-02-13 at 10 20 31 am](https://user-images.githubusercontent.com/44978436/52759838-37edc100-2fc2-11e9-8cfc-955a15a098c8.png)

![screen shot 2019-02-13 at 10 00 27 am](https://user-images.githubusercontent.com/44978436/52759837-37edc100-2fc2-11e9-85cf-45b599252e27.png)


"
25732,ModuleNotFoundError: No module named 'numpy',"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12.0
- Python version: 3.6.8
- Installed using virtualenv? pip? conda?: pip without virtualenv
- Bazel version (if compiling from source): 0.15.0
- GCC/Compiler version (if compiling from source): I'm not sure what version I'm using but, I'm using VS2015 and _MSC_VER is 1900
- CUDA/cuDNN version: CUDA v9.0.176 / cuDNN v7.4.2.24
- GPU model and memory: GTX960M (2GB)



**Describe the problem**
I'm trying to install TF on my laptop from the source code.
I installed GPU driver, CUDA, cuDNN, python, bazel, MSYS2 and Visual Studio, following the official guide (https://www.tensorflow.org/install/source_windows).
Of course, I checked the system variables such as CUDA_PATH and BAZEL_SH.
Then, I cloned the git repository and attempted to build.
After I had launched the build, I saw the build working well.
So that, I just let it go and slept.
However, I found out the build failed in the morning.
I cannot understand why the build failed because I can see 'numpy' package installed when I type ""pip3 list"" as follows:

> C:\tensorflow>pip3 list
> Package             Version
> ------------------- -------
> absl-py             0.7.0
> astor               0.7.1
> gast                0.2.2
> grpcio              1.18.0
> h5py                2.9.0
> Keras-Applications  1.0.6
> Keras-Preprocessing 1.0.5
> Markdown            3.0.1
> numpy               1.16.1
> pip                 18.1
> protobuf            3.6.1
> setuptools          40.6.2
> six                 1.12.0
> tensorboard         1.12.2
> tensorflow-gpu      1.12.0
> termcolor           1.1.0
> Werkzeug            0.14.1
> wheel               0.32.3

And I have also searched similar issue by googling but I could not found proper solution for me.
Please give me any suggestion.

Thanks!

**Provide the exact sequence of commands / steps that you executed before running into the problem**

> C:\tensorflow>python configure.py
> WARNING: Running Bazel server needs to be killed, because the startup options are different.
> WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
> You have bazel 0.15.0 installed.
> Please specify the location of python. [Default is C:\Users\nickeys\AppData\Local\Programs\Python\Python36\python.exe]:
> 
> 
> Found possible Python library paths:
>   C:\Users\nickeys\AppData\Local\Programs\Python\Python36\lib\site-packages
> Please input the desired Python library path to use.  Default is [C:\Users\nickeys\AppData\Local\Programs\Python\Python36\lib\site-packages]
> 
> Do you wish to build TensorFlow with Apache Ignite support? [Y/n]: N
> No Apache Ignite support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with XLA JIT support? [y/N]: N
> No XLA JIT support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with ROCm support? [y/N]:
> No ROCm support will be enabled for TensorFlow.
> 
> Do you wish to build TensorFlow with CUDA support? [y/N]: Y
> CUDA support will be enabled for TensorFlow.
> 
> Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]:
> 
> 
> Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:
> 
> 
> Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]:
> 
> 
> Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0]:
> 
> 
> Please specify a list of comma-separated Cuda compute capabilities you want to build with.
> You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
> Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,7.0]: 5.0
> 
> 
> Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:
> 
> Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]: N
> Not overriding eigen strong inline, some compilations could take more than 20 mins.

> C:\tensorflow>bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
```
INFO: From Linking tensorflow/contrib/resampler/python/ops/_resampler_ops.so:
   Creating library bazel-out/x64_windows-opt/bin/tensorflow/contrib/resampler/python/ops/lib_resampler_ops.so.ifso and object bazel-out/x64_windows-opt/bin/tensorflow/contrib/resampler/python/ops/lib_resampler_ops.so.exp
ERROR: C:/tensorflow/tensorflow/BUILD:533:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1): bash.exe failed: error executing command
  cd C:/users/nickeys/_bazel_nickeys/xv6zejqw/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET CUDNN_INSTALL_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v9.0
    SET PATH=C:\msys64\usr\bin;C:\msys64\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\libnvvp;C:\Users\nickeys\AppData\Local\Programs\Python\Python36\Scripts;C:\Users\nickeys\AppData\Local\Programs\Python\Python36;C:\Users\nickeys\apps;C:\msys64\usr\bin;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0\extras\CUPTI\libx64;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\ProgramData\Oracle\Java\javapath;C:\Program Files (x86)\Intel\iCLS Client\;C:\Program Files\Intel\iCLS Client\;C:\Program Files\Dell\DW WLAN Card;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files\Intel\Intel(R) Management Engine Components\DAL;C:\Program Files (x86)\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Intel\Intel(R) Management Engine Components\IPT;C:\Program Files\Microsoft SQL Server\130\Tools\Binn\;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;C:\WINDOWS\System32\OpenSSH\;C:\Program Files\dotnet\;;C:\Program Files (x86)\NVIDIA Corporation\PhysX\Common;C:\Users\nickeys\AppData\Local\Programs\Python\Python36\Scripts\;C:\Users\nickeys\AppData\Local\Programs\Python\Python36\;C:\Users\nickeys\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Bandizip\
    SET PYTHON_BIN_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/python.exe
    SET PYTHON_LIB_PATH=C:/Users/nickeys/AppData/Local/Programs/Python/Python36/lib/site-packages
    SET TF_CUDA_CLANG=0
    SET TF_CUDA_COMPUTE_CAPABILITIES=5.0
    SET TF_CUDA_VERSION=9.0
    SET TF_CUDNN_VERSION=7
    SET TF_NEED_CUDA=1
    SET TF_NEED_OPENCL_SYCL=0
    SET TF_NEED_ROCM=0
  C:/msys64/usr/bin/bash.exe -c source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/x64_windows-opt/bin/tensorflow/create_tensorflow.python_api_1.exe --root_init_template=tensorflow/api_template.__init__.py --apidir=bazel-out/x64_windows-opt/genfiles/tensorflow_api/v1/ --apiname=tensorflow --apiversion=1 --package=tensorflow.python --output_package=tensorflow._api.v1 bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/app/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/bitwise/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/compat/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/data/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/data/experimental/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/debugging/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/distributions/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/dtypes/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/errors/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/feature_column/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/gfile/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/graph_util/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/io/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/activations/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/densenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_resnet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/inception_v3/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/mobilenet_v2/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/nasnet/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/resnet50/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg16/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/vgg19/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/applications/xception/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/backend/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/callbacks/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/constraints/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/boston_housing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar10/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/cifar100/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/fashion_mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/imdb/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/mnist/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/datasets/reuters/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/estimator/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/initializers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/models/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/optimizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/image/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/sequence/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/preprocessing/text/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/regularizers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/wrappers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/keras/wrappers/scikit_learn/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/layers/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/linalg/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/logging/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/losses/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/manip/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/math/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/metrics/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/nn/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/nn/rnn_cell/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/profiler/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/python_io/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/quantization/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/random/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/resource_loader/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/strings/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/builder/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/loader/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/main_op/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/signature_constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/signature_def_utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/tag_constants/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/saved_model/utils/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sets/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sparse/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/spectral/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/summary/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/sysconfig/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/test/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/train/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/train/queue_runner/__init__.py bazel-out/x64_windows-opt/genfiles/tensorflow/_api/v1/user_ops/__init__.py
Traceback (most recent call last):
  File ""\\?\C:\Users\nickeys\AppData\Local\Temp\Bazel.runfiles_s3knfdzs\runfiles\org_tensorflow\tensorflow\python\tools\api\generator\create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""\\?\C:\Users\nickeys\AppData\Local\Temp\Bazel.runfiles_s3knfdzs\runfiles\org_tensorflow\tensorflow\python\__init__.py"", line 47, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'numpy'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 3196.198s, Critical Path: 2857.14s
INFO: 992 processes: 992 local.
FAILED: Build did NOT complete successfully
```"
25731,AttributeError: 'Tensor' object has no attribute 'numpy' in image_captioning_with_attention.ipynb,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): **2.0**
- Python version:3.x

**Describe the current behavior**
When running the code in 

> Caching the features extracted from InceptionV3
AttributeError: 'Tensor' object has no attribute 'numpy'

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
for img, path in tqdm(image_dataset):
  batch_features = image_features_extract_model(img)
  batch_features = tf.reshape(batch_features, 
                              (batch_features.shape[0], -1, batch_features.shape[3]))

  for bf, p in zip(batch_features, path):
    path_of_feature = p.numpy().decode(""utf-8"")
    np.save(path_of_feature, bf.numpy())
**Other info / logs**

AttributeError                            Traceback (most recent call last)
<ipython-input-15-9b3dc8e2ecd4> in <module>()
     15   for bf, p in zip(batch_features, path):
     16     path_of_feature = p.numpy().decode(""utf-8"")
---> 17     np.save(path_of_feature, bf.numpy())

AttributeError: 'Tensor' object has no attribute 'numpy'"
25729,TF_SessionRun_wrapper: expected all values in input dict to be ndarray  (GPU nightly build),"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
       I have used the basic example from: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/1_Introduction/basic_operations.py along with my own code. I have also made a SO question on the matter (https://stackoverflow.com/questions/54678961/tf-sessionrun-wrapper-expected-all-values-in-input-dict-to-be-ndarray) 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.13.0-dev20190208  (GPU nightly build)
- Python version: Python 3.7.1
- CUDA/cuDNN version: V10.0.130 / 
- GPU model and memory: RTX 2070, 8GB

**Describe the current behavior**

I am running into this error ""TF_SessionRun_wrapper: expected all values in input dict to be ndarray"" for even very basic examples. I am aware that v1.13 nightly is unstable, but building from source on Windows is very inconvenient (I have tried many times).  What should I do/is there anything I can do? 


--- I will try Python 3.6.x when back home ---
"
25728,Floating point addition is not commutative - Unexpected behavior - leads to NaN issues,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows and Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): b'v1.12.0-rc2-3-ga6d8ffae09' 1.12.0
- Python version: Python 3.6.5
- CUDA/cuDNN version: v9.0 / 7.2.1
- GPU model and memory: GTX 1080 and also on GTX 1080 TI

**Describe the current behavior**

Swapping the position of operands gives different results:

- `1e-8 + 1.0 - tf.identity(1.0)` results in `0.0`
- `1.0 - tf.identity(1.0) + 1e-8` results in `1e-08`

**Describe the expected behavior**

Operations like addition are commutative. Swapping the position of operands doesn't change the result:

- `1e-8 + 1.0 - tf.identity(1.0)` results in `1e-08`
- `1.0 - tf.identity(1.0) + 1e-8` results in `1e-08`

**Code to reproduce the issue**

```
import tensorflow as tf
sess = tf.Session()

print(sess.run(1e-8 + 1.0 - tf.identity(1.0)))
print(sess.run(1.0 - tf.identity(1.0) + 1e-8))
```

Output:

> 0.0
> 1e-08

Expected output:

> 1e-08
> 1e-08

**Additional comment**

In combination with `tf.log` / `tf.divide` this results in `-inf` / `inf` instead of `-18.420681` / `100000000.0`
Working with further calculations, for example log loss,  this results in a sudden `NaN` error without any exploding or other indicators. This is unexpected and leads to confusion. Many people on the internet who have inexplicable `NaN` issues try a multitude of proposed solutions (use other weight init, lower learning rate, clipping, strange `tf.where` `NaN` treatment, ....) to work around this unexpected behavior. I don't think any of the before mentioned solutions is appropriate to handle this unexpected behavior and I wish for it to work as expected."
25724,GPU is idle even when there are operations ready to be executed,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.1 LTS
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0
- Python version: Python 2.7.15rc1
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
- CUDA/cuDNN version: 10.0/7.4.2
- GPU model and memory: NVIDIA Corporation GP100GL [Tesla P100 PCIe 12GB] (2 GPUs)


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I am running a toy matrix multiplication example ([code here ](https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984)and [timeline can be found here](https://gist.github.com/xilenteyex/c9565c20f32ea73383de96a484d6eae9)). If u look at the timeline, operations named **MatMul_1** and **MatMul_2** are placed on GPU1 while all other matrix multiplications are placed on GPU0. Operation named **MatMul_2** takes input from operations named **MatMul** and **MatMul_1**. **MatMul** and **MatMul_1** are completed almost at the same time while there is a large gap after completion of these operations before the execution of **MatMul_2** starts. I am not sure why is this gap there ? 

**Describe the expected behavior**
Expected behavior according to my understanding is that as soon as **MEMCPYPtoP** has completed (Op named **MatMul** is placed on GPU0), excution of MatMul_2 should be started. Is this a performance issue or am I missing something here ? 

**Code to reproduce the issue**
[toy_matmul code](https://gist.github.com/xilenteyex/f569b0e0984808e1fb3a284c03b99984)

**Other info / logs**
[timeline for toy_matmul](https://gist.github.com/xilenteyex/c9565c20f32ea73383de96a484d6eae9)
Following is the snapshot of the timeline using chrome-tracing visualizer.
![screen shot 2019-02-13 at 1 02 32 pm](https://user-images.githubusercontent.com/10864603/52733046-b4b37780-2f8f-11e9-9ab3-1ea51c7e2e14.png)


"
25723,TF 2.0: tf.estimator import issue.,"The TF 2.0 nightly build cannot import `tf.estimator`.

```python
>>> import tensorflow as tf
>>> import tensorflow_estimator.python.estimator.api._v2.estimator
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/__init__.py"", line 25, in <module>
    import tensorflow_estimator.python.estimator.estimator_lib
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator_lib.py"", line 54, in <module>
    from tensorflow_estimator.python.estimator.mode_keys import ModeKeysV2
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/mode_keys.py"", line 22, in <module>
    from tensorflow.python.training.mode_keys import ModeKeys
ModuleNotFoundError: No module named 'tensorflow.python.training.mode_keys'
>>> tf.__version__
'2.0.0-dev20190213'
>>> 
>>> exit()
```"
25722,[feature request] Alternating Multi-Head for Estimator-API,"Hello everyone,

I really like the Estimator-API! Its clean, works well (for the most models) and it does a lot of work for you.
However, if you want to train custom models with alternating losses, its almost impossible to use the Estimator-API - unfortunate. (With tf-2.0, you definitely could use eager-mode + tfe.defun(), but I like the idea of a static graph for better performance.)


**System information**
- TensorFlow version (you are using): 1.12

**Describe the feature and the current behavior/state.**
It would be great to have a `MultiHeadSpec`, which allows to define multiple losses and train_ops as well an interval, how often the train_ops should be run.

In case of a GAN: 
- Discriminator, head1: 
  - discriminator_loss, distriminator_train_op, run train_op 2 times
- Generator, head2: 
  - generator_loss, generator_train_op, run train_op once

```python
while not mon_sess.should_stop():
   for _estimator_spec in estimator_specs:
      for _ in range(0, _estimator_spec.iterations):
         _, loss = mon_sess.run([_estimator_spec.train_op, estimator_spec.loss])
```

**Will this change the current api? How?**
No. 

**Who will benefit with this feature?**
People, who train models with alternating loss-functions.

**Any Other info.**
- "
25721,convert_variables_to_constants raise issue of while/ReadVariableOp/Enter while dealing with LSTM and GRU,"<em>Please make sure that this is a bug. As per our [GitHub Policy]

**System information**

- TensorFlow version:1.12
- Python version:3.6
- NO GPU, and with GPU(GTX 1080Ti)

**Describe the current behavior**
convert_variables_to_constants first changes computed variables to constants, then converts ReadVariableOps to Identity node. However, in RNN models such as GRU and LSTM,  some variables are attached to Enter op which are incompatible with the generated constants.
**Describe the expected behavior**
The model can be successfully frozen and loaded in CNN, however, failed to work in RNN such as LSTM and GRU 
**Code to reproduce the issue**
```

from tensorflow.python.keras.models import Sequential
from tensorflow.python.keras.layers import Dense, LSTM
import numpy as np
import tensorflow as tf
from tensorflow.python.keras import backend as K
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):
    """"""
    Freezes the state of a session into a pruned computation graph.
    Creates a new computation graph where variable nodes are replaced by
    constants taking their current value in the session. The new graph will be
    pruned so subgraphs that are not necessary to compute the requested
    outputs are removed.
    @param session The TensorFlow session to be frozen.
    @param keep_var_names A list of variable names that should not be frozen,
                          or None to freeze all the variables in the graph.
    @param output_names Names of the relevant graph outputs.
    @param clear_devices Remove the device directives from the graph for better portability.
    @return The frozen graph definition.
    """"""
    from tensorflow.python.framework.graph_util import convert_variables_to_constants
    graph = session.graph
    with graph.as_default():
        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))
        output_names = output_names or []
        output_names += [v.op.name for v in tf.global_variables()]
        input_graph_def = graph.as_graph_def()
        if clear_devices:
            for node in input_graph_def.node:
                node.device = """"
        frozen_graph = convert_variables_to_constants(session, input_graph_def,
                                                      output_names, freeze_var_names)
        return frozen_graph

def load_graph(frozen_graph_filename):
    # We load the protobuf file from the disk and parse it to retrieve the
    # unserialized graph_def
    with tf.gfile.GFile(frozen_graph_filename, ""rb"") as f:
        graph_def = tf.GraphDef()
        graph_def.ParseFromString(f.read())

    # Then, we import the graph_def into a new Graph and returns it
    with tf.Graph().as_default() as graph:
        # The name var will prefix every op/nodes in your graph
        # Since we load everything in a new graph, this is not needed
        tf.import_graph_def(graph_def, name=""prefix"")
    return graph


def lstm_model(input_shape):
    model = Sequential()
    model.add(LSTM(256, name='LSTM', return_sequences=False, use_bias=False,  input_shape=(256,1)))
    model.add(Dense(100,activation='relu'))
    model.add(Dense(1, activation='linear', name='output'))
    return model



if __name__ == ""__main__"":
    #load lstm model
    input_shape = (None, 256)
    model = lstm_model(input_shape)

    x=np.random.rand(1000, 256, 1)
    y = np.random.rand(1000, 1)
    print(x.shape)

    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

    model.fit(x=x, y=y, epochs=2, validation_split=0.2)

    #freeze the session and save it to a pd.file
    frozen_graph = freeze_session(K.get_session(),
                                  output_names=[out.op.name for out in model.outputs])
    tf.train.write_graph(frozen_graph, './', './model.pb', as_text=False)

    #load the pb file into graph
    frozen_model_filename = './model.pb'
    graph = load_graph(frozen_model_filename)

```

**Other info / logs**
The model can be frozen but could not be load. If put LSTM/kernel to black list, the pd file can be load later on, however, cannot be used in c++ and c #.


ValueError: Input 0 of node prefix/LSTM/while/ReadVariableOp/Enter was passed float from prefix/LSTM/kernel:0 incompatible with expected resource.
"
25716, KeyError in Serve-DualProtoBuf.py,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
25711,Version/Build supporting python3.7 on macOS 10.11.x?,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.11.6
- TensorFlow installed from (source or binary): `pip` (unclear)
- TensorFlow version: 1.9-1.13
- Python version: 3.7
- Installed using virtualenv? pip? conda?: `pip`


**Describe the problem**

I've tried `pip` installing versions 1.9 all the way up to 1.13 on the machine described above. All failed either because the `.so` was built for macOS 10.12 or with the `async` failures mentioned in another issue (I don't have the reference handy). After fixing those, I ran into other issues: sometimes similar, one time a 3.6 (compile-time) != 3.7 (runtime) error.

Effectively, no matter which of the versions I tried, `python -c 'import tensorflow'` failed.

I've spent the better part of 2 hours hoping to find a (relatively painless) way to install tensorflow on my setup: I find it hard to believe python 3.7 supports macOS 10.11.6 but tensorflow somehow doesn't.

I would really appreciate either
1) an explanation of why this setup is not supported; or
2) an explanation of what version to use/what steps to take in order to install tensorflow and have it work.

N.B. upgrading to macOS 10.12 is not really a suitable option for me right now, otherwise I'd have done it."
25710,Tensorflow Build fail on rockchip arm64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Source
- TensorFlow version: 1.8.0
- Python version: 2.7
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.12.0
- GCC/Compiler version (if compiling from source):  gcc version 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

Tensorflow build ERROR:

**Build command** 
**bazel build -c opt --copt=""-funsafe-math-optimizations"" --copt=""-ftree-vectorize"" --copt=""-fomit-frame-pointer"" --verbose_failures tensorflow/tools/pip_package:build_pip_package**


**ERROR: /home/rock64/tensorflow/tensorflow/core/kernels/BUILD:2529:1: C++ compilation of rule '//tensorflow/core/kernels:matrix_exponential_op' failed (Exit 4): gcc failed: error executing command** 
  (cd /home/rock64/.cache/bazel/_bazel_rock64/c534027a01b43b19fa0469dd5f97fd9f/execroot/org_tensorflow && \
  exec env - \
    PATH=/home/rock64/.local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local \
    PWD=/proc/self/cwd \
  /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -g0 -DGEMMLOWP_ALLOW_SLOW_SCALAR_FALLBACK '-std=c++0x' -g0 -MD -MF bazel-out/host/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/tensorflow/core/kernels/matrix_exponential_op.pic.d '-frandom-seed=bazel-out/host/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/tensorflow/core/kernels/matrix_exponential_op.pic.o' -fPIC -DEIGEN_MPL2_ONLY -D__CLANG_SUPPORT_DYN_ANNOTATION__ -DTENSORFLOW_USE_ABSL -DTF_USE_SNAPPY -iquote . -iquote bazel-out/host/genfiles -iquote external/bazel_tools -iquote bazel-out/host/genfiles/external/bazel_tools -iquote external/eigen_archive -iquote bazel-out/host/genfiles/external/eigen_archive -iquote external/local_config_sycl -iquote bazel-out/host/genfiles/external/local_config_sycl -iquote external/com_google_absl -iquote bazel-out/host/genfiles/external/com_google_absl -iquote external/nsync -iquote bazel-out/host/genfiles/external/nsync -iquote external/gif_archive -iquote bazel-out/host/genfiles/external/gif_archive -iquote external/jpeg -iquote bazel-out/host/genfiles/external/jpeg -iquote external/protobuf_archive -iquote bazel-out/host/genfiles/external/protobuf_archive -iquote external/com_googlesource_code_re2 -iquote bazel-out/host/genfiles/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/host/genfiles/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/host/genfiles/external/fft2d -iquote external/highwayhash -iquote bazel-out/host/genfiles/external/highwayhash -iquote external/png_archive -iquote bazel-out/host/genfiles/external/png_archive -iquote external/zlib_archive -iquote bazel-out/host/genfiles/external/zlib_archive -isystem external/eigen_archive -isystem bazel-out/host/genfiles/external/eigen_archive -isystem external/nsync/public -isystem bazel-out/host/genfiles/external/nsync/public -isystem external/gif_archive/lib -isystem bazel-out/host/genfiles/external/gif_archive/lib -isystem external/protobuf_archive/src -isystem bazel-out/host/genfiles/external/protobuf_archive/src -isystem external/farmhash_archive/src -isystem bazel-out/host/genfiles/external/farmhash_archive/src -isystem external/png_archive -isystem bazel-out/host/genfiles/external/png_archive -isystem external/zlib_archive -isystem bazel-out/host/genfiles/external/zlib_archive -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare -fno-exceptions '-ftemplate-depth=900' -pthread -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c tensorflow/core/kernels/matrix_exponential_op.cc -o bazel-out/host/bin/tensorflow/core/kernels/_objs/matrix_exponential_op/tensorflow/core/kernels/matrix_exponential_op.pic.o)
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1788.353s, Critical Path: 102.71s
FAILED: Build did NOT complete successfully


Here GCC version info:
**rock64@rockpro64:/usr/share/doc/gcc-7$ gcc -v**
Using built-in specs.
COLLECT_GCC=gcc
COLLECT_LTO_WRAPPER=/usr/lib/gcc/aarch64-linux-gnu/7/lto-wrapper
**Target: aarch64-linux-gnu**
Configured with: ../src/configure -v --with-pkgversion='Ubuntu/Linaro 7.3.0-27ubuntu1~18.04' --with-bugurl=file:///usr/share/doc/gcc-7/README.Bugs --enable-languages=c,ada,c++,go,d,fortran,objc,obj-c++ --prefix=/usr --with-gcc-major-version-only --program-suffix=-7 --program-prefix=aarch64-linux-gnu- --enable-shared --enable-linker-build-id --libexecdir=/usr/lib --without-included-gettext --enable-threads=posix --libdir=/usr/lib --enable-nls --with-sysroot=/ --enable-clocale=gnu --enable-libstdcxx-debug --enable-libstdcxx-time=yes --with-default-libstdcxx-abi=new --enable-gnu-unique-object --disable-libquadmath --disable-libquadmath-support --enable-plugin --enable-default-pie --with-system-zlib --enable-multiarch --enable-fix-cortex-a53-843419 --disable-werror --enable-checking=release --build=aarch64-linux-gnu --host=aarch64-linux-gnu --target=aarch64-linux-gnu
Thread model: posix
**gcc version 7.3.0 (Ubuntu/Linaro 7.3.0-27ubuntu1~18.04)**


"
25709,Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution: Windows 10 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no 
- TensorFlow installed from (source or binary):source 
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source):N/A
- GCC/Compiler version (if compiling from source):N/A
- CUDA/cuDNN version: 9.0 / 7.1
- GPU model and memory:  GTX 1050 Ti


**Describe the current behavior**
i tried to train the model by using image datasets

**Code to reproduce the issue**
    
    from keras.callbacks import ModelCheckpoint, EarlyStopping
    from model import load_model
    import numpy as np 
    import argparse
    parser=argparse.ArgumentParser()
    parser.add_argument('n_epochs',type=int)

    args=parser.parse_args()
    X_train=np.load('training.npy')
    frames=X_train.shape[2]
    #Need to make number of frames divisible by 10

    frames=frames-frames%10
    X_train=X_train[:,:,:frames]
    X_train=X_train.reshape(-1,227,227,10)
    X_train=np.expand_dims(X_train,axis=4)
    Y_train=X_train.copy()

    epochs=args.n_epochs
    batch_size=1

    if __name__==""__main__"":

	    model=load_model()

	    callback_save = ModelCheckpoint(""model.h5"",
									monitor=""mean_squared_error"", save_best_only=True)

	    callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3)

	    print('Model has been loaded')

	    model.fit(X_train,Y_train,
			      batch_size=batch_size,
			      epochs=epochs,
			      callbacks = [callback_save,callback_early_stopping]
			  )

**Output**
2019-02-13 03:45:14.939208: E tensorflow/stream_executor/cuda/cuda_driver.cc:981] failed to synchronize the stop event: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure

2019-02-13 03:45:14.943572: E tensorflow/stream_executor/cuda/cuda_timer.cc:55] Internal: error destroying CUDA event in context 000001F501169B70: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure

2019-02-13 03:45:14.948104: E tensorflow/stream_executor/cuda/cuda_timer.cc:60] Internal: error destroying CUDA event in context 000001F501169B70: CUDA_ERROR_LAUNCH_FAILED: unspecified launch failure

2019-02-13 03:45:14.952242: F tensorflow/stream_executor/cuda/cuda_dnn.cc:231] Check failed: status == CUDNN_STATUS_SUCCESS (7 vs. 0)Failed to set cuDNN stream.
"
25708,DLL load failed with error code -1073741795,"---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     57 
---> 58   from tensorflow.python.pywrap_tensorflow_internal import *
     59   from tensorflow.python.pywrap_tensorflow_internal import __version__

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in <module>()
     27             return _mod
---> 28     _pywrap_tensorflow_internal = swig_import_helper()
     29     del swig_import_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py in swig_import_helper()
     23             try:
---> 24                 _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
     25             finally:

~\Anaconda3\lib\imp.py in load_module(name, file, filename, details)
    242         else:
--> 243             return load_dynamic(name, filename, file)
    244     elif type_ == PKG_DIRECTORY:

~\Anaconda3\lib\imp.py in load_dynamic(name, path, file)
    342             name=name, loader=loader, origin=path)
--> 343         return _load(spec)
    344 

ImportError: DLL load failed with error code -1073741795

During handling of the above exception, another exception occurred:

ImportError                               Traceback (most recent call last)
<ipython-input-2-64156d691fe5> in <module>()
----> 1 import tensorflow as tf

~\Anaconda3\lib\site-packages\tensorflow\__init__.py in <module>()
     22 
     23 # pylint: disable=g-bad-import-order
---> 24 from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
     25 
     26 try:

~\Anaconda3\lib\site-packages\tensorflow\python\__init__.py in <module>()
     47 import numpy as np
     48 
---> 49 from tensorflow.python import pywrap_tensorflow
     50 
     51 from tensorflow.python.tools import component_api_helper

~\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py in <module>()
     72 for some common reasons and solutions.  Include the entire stack trace
     73 above this error message when asking for help."""""" % traceback.format_exc()
---> 74   raise ImportError(msg)
     75 
     76 # pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long

ImportError: Traceback (most recent call last):
  File ""C:\Users\believe\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\believe\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\believe\Anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\believe\Anaconda3\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\believe\Anaconda3\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code -1073741795


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help."
25707,summary v2 create_file_writer() behavior leads to incorrect resource deletion,"This is a tracking bug for the known issue that the V2 summary API's `create_file_writer()` logic behaves unexpectedly in regards to the underlying resource lifecycle management.  This typically manifests as users seeing errors like `tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/logdir:./log/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: epoch_loss/`

Users can encounter this in 1.x when using `tf.contrib.summary.create_file_writer()` in eager mode, or in 2.0 as `tf.summary.create_file_writer()`, or via wrapper APIs like the Keras TensorBoard callback.

Example issues: #24632, #25524

The root cause is that `create_file_writer(logdir)` returns a SummaryWriter python object backed by a C++ SummaryWriterInterface resource, but the mapping is not 1:1, which violates other assumptions about resource management under eager mode.  In particular, two SummaryWriter instances created for the same logdir by default will both attempt to reference the same named resource.  (Sharing by logdir was intended to be a mechanism for using one eventfile per logdir akin to `tf.summary.FileWriterCache` in 1.x, which makes TensorBoard better behaved.)

This is a problem when one of the SummaryWriters is deleted (aka loses its last remaining reference), because it will unconditionally attempt to delete the named resource it was created with.  If a resource with that same name is referenced by any other SummaryWriters, those ones will now emit the above `NotFoundError` when attempting to use them.  Note that this happens even if the deleted SummaryWriter was properly `close()`'d before the new one was created, because the logdir name sharing means that it new writer's resource has the same name as the old one.

The correct fix is changing the resource names to be unique so that the mapping from SummaryWriter object <-> SummaryWriterInterface resource is 1:1 as designed.  This will likely require other workarounds for the one-eventfile-per-logdir problem.

Minimal repro:
```python
import tempfile
import tensorflow as tf

if tf.__version__.startswith('2.'):
  create_file_writer = tf.summary.create_file_writer
else:
  tf.enable_eager_execution()
  create_file_writer = tf.contrib.summary.create_file_writer

dir = tempfile.mkdtemp('shared-writer-bug')
w = create_file_writer(dir)
w.close()  # succeeds, deletes old resource
# 1) new SummaryWriter created w/ new resource (but same name)
# 2) reassigning w deletes old SummaryWriter along w/ new resource (BUG)
w = create_file_writer(dir)
w.close()  # raises NotFoundError; new resource has been incorrectly deleted
```"
25704,TypeError: Expected config_proto to be a ConfigProto,"    python /root/tensorflow/tensorflow/examples/image_retraining/retrain.py \
        --image_dir /home/training/data/images \
        --how_many_training_steps=4000 \
        --eval_step_interval=100 \       
        --output_graph /home/training/data/graph.pb \
        --summaries_dir /home/training/data/summaries \
        --output_labels /home/training/data/output_labels.txt \
        --bottleneck_dir /home/training/data/bottleneck/ \
        --intermediate_store_frequency 1000 \
        --intermediate_output_graphs_dir /home/training/data/intermediate \
        --saved_model_dir /home/training/data/saved_model       
        
        
     tensorflowjs_converter \
        --input_format=tf_saved_model \
        --output_node_names='final_result' \
        --saved_model_tags=serve \
        /home/training/data/saved_model/ \
        /home/training/data/saved_model_web/

Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0212 21:48:38.775988 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py:161: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.
W0212 21:48:42.307756 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1277: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0212 21:48:44.036837 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/tools/freeze_graph.py:232: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
W0212 21:48:44.037100 139652780599104 deprecation.py:323] From /usr/lib/python2.7/site-packages/tensorflow/python/framework/graph_util_impl.py:246: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
Traceback (most recent call last):
  File ""/bin/tensorflowjs_converter"", line 10, in <module>
    sys.exit(main())
  File ""/usr/lib/python2.7/site-packages/tensorflowjs/converters/converter.py"", line 322, in main
    strip_debug_ops=FLAGS.strip_debug_ops)
  File ""/usr/lib/python2.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py"", line 294, in convert_tf_saved_model
    skip_op_check=skip_op_check, strip_debug_ops=strip_debug_ops)
  File ""/usr/lib/python2.7/site-packages/tensorflowjs/converters/tf_saved_model_conversion_pb.py"", line 129, in optimize_graph
    rewriter_config, meta_graph, cluster=get_cluster())
  File ""/usr/lib/python2.7/site-packages/tensorflow/python/grappler/tf_optimizer.py"", line 36, in OptimizeGraph
    type(config_proto))
TypeError: Expected config_proto to be a ConfigProto, saw type <class 'tensorflow.core.protobuf.rewriter_config_pb2.RewriterConfig'>
"
25703,Cache intermediate results in custom op for backwards pass,"Hello,

I wonder if there is a feature in Tensorflow which allows caching of intermediate results in a custom operation for the backwards computation, similar to the the ctx->save_for_backward interface in Pytorch. Does the C++ context object that we get to work with provide such a functionality?

Best,

Ziheng Wang"
25701,The number of non-zero element exceed 32-bits integer,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Centos 7
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source):  1.12.0

**Provide the text output from tflite_convert**

```
InvalidArgumentError (see above for traceback): Number of non-zero elements exceeds int32 range [[node graphconvolution_1/mul (defined at /home/fuqiang/anaconda3/envs/python3.6/lib/python3.6/site-packages/gcn-1.0 py3.6.egg/gcn/layers.py:27)  = SparseDenseCwiseMul[T=DT_FLOAT, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](graphconvolution_1/GatherV2, graphconvolution_1/GatherV2_1, graphconvolution_1/Identity, graphconvolution_1/truediv)]]
```
I was running a model using a sparse tensor whose number of non-zero elements exceed int32 range. And I got this.

Does that mean I can't do sparsedenseMul on any sparse tensor whose number of non-zero elements exceeds int32 range?"
25700,Dataset iterator is stalled indefinitely with corrupt TFRecord despite using `ignore_errors`,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.6
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: 9/7
- GPU model and memory: n/a

Creating this issue per @mrry's [suggestion](https://github.com/tensorflow/tensorflow/issues/12701#issuecomment-462799443) with minimal code to reproduce the issue.

cc @guillaumekln You're [right](https://github.com/tensorflow/tensorflow/issues/13463#issuecomment-462654013), what I'm seeing may not have to do with `OutofRangeError`, but the execution stalls indefinitely when it encounters corrupt data within a TFRecord, despite using `tf.data.experimental.ignore_errors()`.

#### Example Data (for snippets below)
- [clean.tfrecord](https://drive.google.com/uc?export=view&id=1Y2yV67jAvYhqz8OJLPV7v0BN2w1Feps9)
- [corrupt.tfrecord](https://drive.google.com/uc?export=view&id=1Fh7Vah0i_XaGqdb724NIhUIZpR_t5YiG)

#### With `clean.tfrecord`:
```python3
import tensorflow as tf

filenames = ['/data/clean.tfrecord']
#filenames = ['/data/corrupt.tfrecord']

dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.apply(tf.data.experimental.ignore_errors())
dataset = dataset.batch(64)
dataset = dataset.repeat(1)

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    i = 0
    while True:
        try:
            print(i, sess.run(next_element).shape)
            i = i + 1
        except tf.errors.OutOfRangeError:
            print(""Dataset complete"")
            break
```

#### Output:
```
0 (64,)
1 (64,)
2 (64,)
3 (64,)
4 (64,)
5 (64,)
6 (64,)
7 (64,)
8 (64,)
9 (64,)
10 (64,)
11 (64,)
12 (64,)
13 (64,)
14 (64,)
15 (64,)
16 (64,)
17 (64,)
18 (64,)
19 (35,)
Dataset complete
```

#### With `corrupt.tfrecord`:
```python3
import tensorflow as tf

#filenames = ['/data/clean.tfrecord']
filenames = ['/data/corrupt.tfrecord']

dataset = tf.data.TFRecordDataset(filenames)
dataset = dataset.apply(tf.data.experimental.ignore_errors())
dataset = dataset.batch(64)
dataset = dataset.repeat(1)

iterator = dataset.make_one_shot_iterator()
next_element = iterator.get_next()

with tf.Session() as sess:
    i = 0
    while True:
        try:
            print(i, sess.run(next_element).shape)
            i = i + 1
        except tf.errors.OutOfRangeError:
            print(""Dataset complete"")
            break
```

#### Output:
```
0 (64,)
1 (64,)
2 (64,)
3 (64,)
4 (64,)
5 (64,)
6 (64,)
7 (64,)
8 (64,)
9 (64,)
10 (64,)
11 (64,)
12 (64,)
(execution stalled indefinitely here)
```

Not sure why the corrupt files within the TFRecord aren't being ignored with `tf.data.experimental.ignore_errors()`. @mrry any idea?"
25697,"FFT parallelized over feature maps, minibatches and within each 2-D transform","# Performance Issue

Note: This is not a bug, but it is a performance issue that I have faced, and it might or might not lead to a new feature request. So, I am bringing this up here for discussion.

**System information**
- I am using Kaggle Kernels for running my tensorflow code with the GPU option enabled.

Environment Information:
> You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
> You can also obtain the TensorFlow version with
> `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
> 
> 

Output:
> b'v1.12.0-0-ga6d8ffae09' 1.12.0

# The Issue
**Context**

I reproduced this [paper](https://www.groundai.com/project/fast-training-of-convolutional-networks-through-ffts/) that basically learns CNN maps in the frequency domain. However, when I run it and compare the epoch time on training of regular CNNs, that operates in the spatial domain, against using FFTs, that operates in the frequency domain, the FFT version is much slower. For experiments, I implemented the LeNet-5 and used the MNIST dataset.

| Experiment | Description | Avrg Epoch Time  | Wall time | Acc |
|:------------:|:-----------:|:-----------------:|-----------|-----|
|  MNIST - Baseline LeNet CNN | Baseline | 1.67633 s  | 25.2 s   | 98.79416 |
| MNIST - Mathieu LeNet CNN | Frequency Domain Convolutions | 5.69431 s | 56.4 s   | 97.26562 |

The paper, that claims a faster CNN, comes with the following note:

> Current GPU implementations of the FFT such as cuFFT are designed to parallelize over individual transforms. This can be useful for computing a limited number of transforms on large inputs, but is not suitable for our task since we are performing many FFTs over relatively small inputs. Therefore, we developed a custom CUDA implementation of the Cooley-Tukey FFT algorithm which enabled us to parallelize over feature maps, minibatches and within each 2-D transform. Note that 2-D FFTs lend themselves naturally to parallelization since they can be decomposed into two sets of 1-D FFTs (one over rows and the other over columns), and each set can be done in parallel.

I had no clue on how this implementation optimization would occur, so I put [this question](https://dsp.stackexchange.com/questions/54903/how-to-custom-optimize-cufft-for-a-mini-batch-of-multi-channel-images) up to an online community. I assume the paper is accurate on its claim, since one of the authors is the father of CNN, LeCan. So, the only feasible thing I could do, given the answer I got and my lack of understanding on actual `CUDA` development, was to check for the tensorflow implementation and see if`cufftPlanMany` is somehow used. And it is indeed somehow used.

**Problem**

Up to this point, I don't know how to reproduce the technique described in that paper. I wish I knew. If there is a better way to run FFT operations, I believe that lots of applications could benefit from it. In additional, the paper mention some memory considerations of their implementation. But I think that it is a separe issue. 
> Note that by keeping the Fourier representations in memory for all layers after the forward pass, we could avoid recomputing several of the FFTs during the backward pass. However, this might become pro- hibitively expensive in terms of memory for large networks. Therefore we reuse the same memory for all the different convolutions in the network, so that the necessary amount of memory is de- termined only by the largest convolution layer.

"
25695,TF-Gradient issue,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: Python 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0, V9.0.176
- GPU model and memory: GeForce GT 750M


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

I am trying to use @tf.custom gradient for a one of the layers in a RNN, and running to the following problem, when I compute the gradient of loss in the output layer with respect to weights in hidden layer, I get gradient values, however when I try to compute the gradient of the output of the hidden layer with respect to the input to the layer or states in the layer (y=f(x) and calculating dy/dx ) I get [None]. I am using tf.gradients to compute gradients. I am doing the second part to see if I have implemented the custom gradient correctly. 
this is the activation function from hidden state and output 
```
@tf.custom_gradient
def _calcualte_crossings(x):
    """"""input :x : a 2D tensor with batch x n
    outputs a tensor with the same size as x
    and values of 0 or 1 depending on comparison between
    x and threshold""""""
    dtype=x.dtype
    res=tf.greater_equal(x,0.0)
    def grad(dy):
        # calculate 1-|x|
        temp=1-tf.abs(x)
        dyres=tf.scalar_mul(0.3,tf.maximum(temp,0.0))
        return dyres
    return tf.cast(res,dtype=dtype), grad
```

"
25694,TF_gradient,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: Python 3.6.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9.0, V9.0.176
- GPU model and memory: GeForce GT 750M


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

I am trying to use @tf.custom gradient for a one of the layers in a RNN, and running to the following problem, when I compute the gradient of loss in the output layer with respect to weights in hidden layer, I get gradient values, however when I try to compute the gradient of the output of the hidden layer with respect to the input to the layer or states in the layer (y=f(x) and calculating dy/dx ) I get [None]. I am using tf.gradients to compute gradients. I am doing the second part to see if I have implemented the custom gradient correctly. 
this is the activation function from hidden state and output 
```
@tf.custom_gradient
def _calcualte_crossings(x):
    """"""input :x : a 2D tensor with batch x n
    outputs a tensor with the same size as x
    and values of 0 or 1 depending on comparison between
    x and threshold""""""
    dtype=x.dtype
    res=tf.greater_equal(x,0.0)
    def grad(dy):
        # calculate 1-|x|
        temp=1-tf.abs(x)
        dyres=tf.scalar_mul(0.3,tf.maximum(temp,0.0))
        return dyres
    return tf.cast(res,dtype=dtype), grad
```

"
25693,Enable Keras Applications with pretrained Weights to be used inside the Estimator API,"Examples like #25670 show you that is impossible to use Keras Applications inside the Estimator API. The issue has to do with some global tensors that Keras creates by itself that live or were created in a separate session, thus, when the Estimator API wants to construct the `model_fn` it raises an error saying that it found a Tensor from a different graph.

I think Keras should be modified to avoid this behavior, TFHub is nice but it doesn't let you use arbitrary input shapes. "
25692,pip install tf-nightly-2.0-preview on Windows 10: missing file TensorSyclConvertToDeviceExpression.h,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: tf-nightly-2.0-preview==2.0.0.dev20190208
- Python version:3.6.6 
- Installed using virtualenv? pip? conda?: pip (18.0)
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Cannot install  TF2.0 preview package for Windows 10 (private Windows 10 machine, admin, no proxy) because TensorSyclConvertToDeviceExpression.h is missing

One file cannot be found:
xxx/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'

I used pip install tf-nightly-2.0-preview==2.0.0.dev20190208 --no-clean and I could see that such file doesn't exist

**Provide the exact sequence of commands / steps that you executed before running into the problem**

pip install tf-nightly-2.0-preview==2.0.0.dev20190208

Collecting tfds-nightly (from -r C:\Users\steph\condaenv.19qkhena.requirements.txt (line 1))
  Downloading https://files.pythonhosted.org/packages/a6/66/8da21433386f85e2c5665c830ee86e79518f73e019846083513b84665d29/tfds_nightly-1.0.0.dev201902120105-py3-none-any.whl (393kB)
    100% || 399kB 6.6MB/s
Collecting tf-nightly-2.0-preview==2.0.0.dev20190208 (from -r C:\Users\steph\condaenv.19qkhena.requirements.txt (line 2))
  Downloading https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl (42.1MB)
    100% || 42.1MB 728kB/s
Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\Users\\steph\\AppData\\Local\\Temp\\pip-install-hwy1u5dr\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-2.0.0.dev20190208.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'

CondaValueError: pip returned an error 
**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

same command a above in verbose mode (selection):

Using version 2.0.0.dev20190208 (newest of versions: 2.0.0.dev20190208)
  Created temporary directory: C:\Users\steph\AppData\Local\Temp\pip-unpack-00jv942o
  Looking up ""https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl"" in the cache
  Current age based on date: 290
  Ignoring unknown cache-control directive: immutable
  Freshness lifetime from max-age: 365000000
  The response is ""fresh"", returning cached response
  365000000 > 290
  Using cached https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl
  Downloading from URL https://files.pythonhosted.org/packages/79/87/b2b471ad6df83f3b66fdc3bbe119f88f776307fb5a994be86b76184709b9/tf_nightly_2.0_preview-2.0.0.dev20190208-cp36-cp36m-win_amd64.whl#sha256=5b29449c373e7fb2fcd188cb6b0006facfb8da3414c0d555223cdce6307b1442 (from https://pypi.org/simple/tf-nightly-2-0-preview/)
Could not install packages due to an EnvironmentError.
Traceback (most recent call last):
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\commands\install.py"", line 299, in run
    resolver.resolve(requirement_set)
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\resolve.py"", line 102, in resolve
    self._resolve_one(requirement_set, req)
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\resolve.py"", line 256, in _resolve_one
    abstract_dist = self._get_abstract_dist_for(req_to_install)
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\resolve.py"", line 209, in _get_abstract_dist_for
    self.require_hashes
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\operations\prepare.py"", line 283, in prepare_linked_requirement
    progress_bar=self.progress_bar
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\download.py"", line 836, in unpack_url
    progress_bar=progress_bar
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\download.py"", line 677, in unpack_http_url
    unpack_file(from_path, location, content_type, link)
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\utils\misc.py"", line 578, in unpack_file
    flatten=not filename.endswith('.whl')
  File ""C:\Users\steph\Anaconda3\envs\env_gcp_dl_2_0\lib\site-packages\pip\_internal\utils\misc.py"", line 478, in unzip_file
    fp = open(fn, 'wb')
FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\steph\\AppData\\Local\\Temp\\pip-install-uvd9so6x\\tf-nightly-2.0-preview\\tf_nightly_2.0_preview-2.0.0.dev20190208.data/purelib/tensorflow/include/tensorflow/include/external/eigen_archive/unsupported/Eigen/CXX11/src/Tensor/TensorSyclConvertToDeviceExpression.h'
Cleaning up...

"
25691,Missing mathematical definitions of tf.norm,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version: 1.12
- Doc Link: https://www.tensorflow.org/api_docs/python/tf/norm


**Describe the documentation issue**
There are no mathematical definitions of the norms. This may result in ambiguity and confusion (e.g. are they Schatten p-norms?).

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?** Not anytime soon, and in any case I would face said ambiguity and confusion."
25690,Tensorflowlite: bazel windows build is broken,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
n/a

- TensorFlow installed from (source or binary):
Build from source

- TensorFlow version:
master, 07d4fe20b305178c9e1c86d0aee25ded59078dbf

- Python version:
2.7.15

- Installed using virtualenv? pip? conda?:
n/a

- Bazel version (if compiling from source):
0.21.0

- GCC/Compiler version (if compiling from source):
MSVC 2017

- CUDA/cuDNN version:
n/a

- GPU model and memory:
n/a

**Describe the problem**
Windows build of tensorflow lite fails. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**

D:\AI\tensorflow>bazel.exe --output_user_root ./builddir build --jobs 1 //tensorflow/lite:libtensorflowlite.so
WARNING: The following rc files are no longer being read, please transfer their contents or import their path into one of the standard rc files:
d:\ai\tensorflow/.bazelrc
INFO: Invocation ID: 4d63708d-d989-452b-8ad1-8cdde66edcdb
DEBUG: D:/ai/tensorflow/builddir/jkmrio3k/external/build_bazel_rules_apple/apple/repositories.bzl:35:5:
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag No
ne). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

INFO: Analysed target //tensorflow/lite:libtensorflowlite.so (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: D:/ai/tensorflow/tensorflow/lite/kernels/BUILD:259:1: C++ compilation of rule '//tensorflow/lite/kernels:lstm_eval' failed (Exit 2)
cl : Command line warning D9002 : ignoring unknown option '/Gw'
.\tensorflow/lite/c/c_api_internal.h(32) : fatal error C1083: Cannot open include file: 'stdbool.h': No such file or directory
Target //tensorflow/lite:libtensorflowlite.so failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 0.579s, Critical Path: 0.10s
INFO: 0 processes.
FAILED: Build did NOT complete successfully


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25689,cannot convert model to tflite,"
when I try to converto to tflite, occur below errors 


$ tflite_convert --output_file=./model1/test.tflite --keras_model_file=./model1/186-0.0481.hdf5

WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:1253: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/init_ops.py:96: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
2019-02-12 18:00:17.730635: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py:636: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.convert_variables_to_constants
WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/graph_util_impl.py:245: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.compat.v1.graph_util.extract_sub_graph
2019-02-12 18:00:18.207014: I tensorflow/core/grappler/devices.cc:53] Number of eligible GPUs (core count >= 8): 0 (Note: TensorFlow was not compiled with CUDA support)
2019-02-12 18:00:18.207141: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session
2019-02-12 18:00:18.210189: E tensorflow/core/grappler/grappler_item_builder.cc:636] Init node dense/kernel/Assign doesn't exist in graph
Traceback (most recent call last):
  File ""/usr/local/bin/tflite_convert"", line 10, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 442, in main
    app.run(main=run_main, argv=sys.argv[:1])
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 438, in run_main
    _convert_model(tflite_flags)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/tflite_convert.py"", line 191, in _convert_model
    output_data = converter.convert()
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/lite.py"", line 500, in convert
    **converter_kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py"", line 442, in toco_convert_impl
    input_data.SerializeToString())
  File ""/usr/local/lib/python2.7/dist-packages/tensorflow/lite/python/convert.py"", line 205, in toco_convert_protos
    ""TOCO failed. See console for info.\n%s\n%s\n"" % (stdout, stderr))
tensorflow.lite.python.convert.ConverterError: TOCO failed. See console for info.
2019-02-12 18:00:19.028566: F tensorflow/core/framework/function.cc:1640] Check failed: GetOpGradFactory()->insert({op, func}).second Duplicated gradient for MapAccumulate
Aborted (core dumped)



How can I fixed it?
somebody has any idea?"
25688,Compile issue for TF Lite for Android NDK wit TF ops enabled,"**System information**
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.13.0-rc0
- Python version: 3
- Bazel version: 0.21.0
- GCC/Compiler version (if compiling from source): NDK r14b, r18b, clang
- CUDA/cuDNN version: -
- GPU model and memory: -
- Android SDK/NDK API level: 21


**When trying to build TFlite as shared library for Android with the experimental TF ops enabled, I get an compile error**

**Steps**

Added the Android target to the _tensorflow/lite/BUILD_:
```
cc_binary( 
    name = ""libtensorflowLite.so"",
    linkopts=[
        ""-shared"", 
        ""-Wl,-soname=libtensorflowLite.so"",
    ],
    linkshared = 1,
    copts = tflite_copts(),
    deps = [
        "":framework"",
        ""//tensorflow/lite/kernels:builtin_ops"",
        ""//tensorflow/lite/delegates/flex:delegate""
    ],
)
```
I added the _flex_delegate_ as documented  [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/using_select_tf_ops.md)

Build with command:
```
bazel build //tensorflow/lite:libtensorflowLite.so --crosstool_top=//external:android/crosstool --cpu=armeabi-v7a --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cxxopt=""-std=c++11""
```

I tried with NDK version r18b and r14b 

**Any other info / logs**

This is the important part of the log:
```
[0m [91m[562 / 928] Compiling tensorflow/core/kernels/cwise_op_add_1.cc; 46s local ... (2 actions, 1 running)
[0m [91m[573 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 22s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 152s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 301s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 474s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 672s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 900s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 1223s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 1552s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 1971s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 2415s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 2923s local ... (2 actions running)
[0m [91m[574 / 928] Compiling tensorflow/core/kernels/cwise_op_mul_1.cc; 3507s local ... (2 actions running)
[0m [91mERROR: /tensorflow_src/tensorflow/core/kernels/BUILD:5770:1: C++ compilation of rule '//tensorflow/core/kernels:android_tensorflow_kernels' failed (Exit 254)
clang: error: unable to execute command: Killed
clang: error: clang frontend command failed due to signal (use -v to see invocation)
Android (4751641 based on r328903) clang version 7.0.2 (https://android.googlesource.com/toolchain/clang 003100370607242ddd5815e4a043907ea9004281) (https://android.googlesource.com/toolchain/llvm 1d739ffb0366421d383e04ff80ec2ee591315116) (based on LLVM 7.0.2svn)
Target: armv7-none-linux-android
Thread model: posix
InstalledDir: external/androidndk/ndk/toolchains/llvm/prebuilt/linux-x86_64/bin
clang: note: diagnostic msg: PLEASE submit a bug report to https://bugs.llvm.org/ and include the crash backtrace, preprocessed source, and associated run script.
clang: note: diagnostic msg: 
********************
PLEASE ATTACH THE FOLLOWING FILES TO THE BUG REPORT:
Preprocessed source(s) and associated run script(s) are located at:
clang: note: diagnostic msg: /tmp/cwise_op_mul_1-574804.cpp
clang: note: diagnostic msg: /tmp/cwise_op_mul_1-574804.sh
clang: note: diagnostic msg:
********************
[0m [91mTarget //tensorflow/lite:libtensorflowLite.so failed to build
[0m [91mUse --verbose_failures to see the command lines of failed build steps.
[0m [91mINFO: Elapsed time: 4707.414s, Critical Path: 3871.01s
INFO: 564 processes: 564 local.
[0m [91mFAILED: Build did NOT complete successfully
[0m [91mFAILED: Build did NOT complete successfully
```

I am looking forward to any advice if this is even possible in the current experimental stage
Thanks,
Thomas"
25687,Functional safety compliance,"We want to use Tensorflow for building Deep Learning based models in production for Automotive. 
The Automotive industry requires strict compliance with ISO 26262 for functional safety.

One of the clause in ISO 26262 is the classification of all the software tools used in development shall be performed according to ISO 26262-8, clause 11 Confidence in the use of software tools.
Such classification report basically means the tool (in this case Tensorflow) is functionally safe to use.

Is it possible to get such a classification report for Tensorflow? Whom to contact for this topic
Thanks.
"
25686,Running the transformer model with Tensor2Tensor using Mesh-Tensorflow(GPU implementation),"Hello I posted the below concern in mesh-tensorflow and tensor2tensor GitHub issues but did not get any response. So I am posting here if any one can help me about this issue.

I am trying to run the transformer model with Tensor2tensor using mesh-tensorflow (GPU-implementation) but I am facing few errors.

I am attaching my error log also.

**steps to reproduce:**

PROBLEM=translate_enfr_wmt32k
MODEL=mtf_transformer
HPARAMS=mtf_transformer_paper_tr_0_mesh_8
DATA_DIR=$HOME/t2t_data
TMP_DIR=/tmp/t2t_datagen
TRAIN_DIR=$HOME/t2t_train/$PROBLEM/$MODEL-$HPARAMS
mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR

datagen:
t2t-datagen \
--data_dir=$DATA_DIR \
--tmp_dir=$TMP_DIR \
--problem=$PROBLEM

train:
t2t-trainer \
--data_dir=$DATA_DIR \
--problem=$PROBLEM \
--model=$MODEL \
--hparams_set=$HPARAMS \
--output_dir=$TRAIN_DIR \
--train_steps=10
[mesh-error.txt](https://github.com/tensorflow/tensorflow/files/2854483/mesh-error.txt)
"
25682,"in the website ""https://tensorflow.google.cn"",the experience of auto-scroll is so bad for  focusing on the content. I wish it could be improved.Tks","in the website ""https://tensorflow.google.cn"",the experience of auto-scroll is so bad for  focusing on the content. I wish it could be improved. Thanks."
25681,Error running ./configure on Cuda 10 (Building from Source),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 18.04):Linux Ubuntu 18.04)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source ): yes
- TensorFlow version: master branch
- Python version: 2.7
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 21.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10 
- GPU model and memory: 1080 

Having the following error when trying to run ./configure
Since I'm trying to use CUDA 10 with Tensorflow I need to build from the source 

**
```
Invalid SYCL 1.2 library path. /usr/local/computecpp/lib/libComputeCpp.so cannot be found
Traceback (most recent call last):
  File ""./configure.py"", line 1701, in <module>
    main()
  File ""./configure.py"", line 1595, in main
    set_computecpp_toolkit_path(environ_cp)
  File ""./configure.py"", line 1387, in set_computecpp_toolkit_path
    suppress_default_error=True)
  File ""./configure.py"", line 657, in prompt_loop_or_load_from_env
    'Assuming to be a scripting mistake.' % (var_name, n_ask_attempts))
__main__.UserInputError: Invalid COMPUTECPP_TOOLKIT_PATH setting was provided 10 times in a row. Assuming to be a scripting mistake.
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I tried to complete installation starting from Tensorflow part of this blogpost https://medium.com/@vitali.usau/install-cuda-10-0-cudnn-7-3-and-build-tensorflow-gpu-from-source-on-ubuntu-18-04-3daf720b83fe but the first bazel version was not compatible with tensorflow so I needed to downgrade to 21 then this error poped out. 

"
25677,Build from source with MPI support,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source):4.8.5
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**

Bazel won't build tensorflow with MPI support. Error message says ""ImportError: libmpi.so.40: cannot open shared object file: No such file or directory"". However I had my $LD_LIBRARY_PATH set correctly.

I've using openmpi-3.0.3. Previously I tried openmpi-1.8.1 and it didn't work either.

PS: the tested tensorflow build configuration says to use bazel 0.15.0 but I got an error says ""Please upgrade your bazel installation to version 0.19.0 or higher to build TensorFlow!"" when I tried to configure. Then i switched to bazel 0.21.0.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

jon@jon-OptiPlex-3050:~/local_build/tensorflow$ ./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: a65fa066-3efc-406a-aba3-d8f7e8c10bfa
You have bazel 0.21.0 installed.
Please specify the location of python. [Default is /home/jon/anaconda3/bin/python]:

Found possible Python library paths:
  /home/jon/anaconda3/lib/python3.6/site-packages
Please input the desired Python library path to use.  Default is [/home/jon/anaconda3/lib/python3.6/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]:
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]:
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Do you wish to download a fresh release of clang? (Experimental) [y/N]:
Clang will not be downloaded.

Do you wish to build TensorFlow with MPI support? [Y/n]: Y
MPI support will be enabled for TensorFlow.

Please specify the MPI toolkit folder. [Default is /home/jon/openmpi-3.0.3]:

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare
]:

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=monolithic     # Config for mostly static monolithic build.
        --config=gdr            # Build with GDR support.
        --config=verbs          # Build with libverbs support.
        --config=ngraph         # Build with Intel nGraph support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=noaws          # Disable AWS S3 filesystem support.
        --config=nogcp          # Disable GCP support.
        --config=nohdfs         # Disable HDFS support.
        --config=noignite       # Disable Apache Ignite support.
        --config=nokafka        # Disable Apache Kafka support.
        --config=nonccl         # Disable NVIDIA NCCL support.
Configuration finished

----------------------------------
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Invocation ID: 26351414-5c95-4a85-91f0-54b998cf6311
WARNING: /home/jon/local_build/tensorflow/tensorflow/python/BUILD:3140:1: in py_library rule //tensorflow/python:standard_ops: target '//tensorflow/p
ython:standard_ops' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to Tens
orFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and wi
ll be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/jon/local_build/tensorflow/tensorflow/python/BUILD:100:1: in py_library rule //tensorflow/python:no_contrib: target '//tensorflow/pyth
on:no_contrib' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions has migrated to TensorFlo
w Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be
 removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/metrics/BUILD:16:1: in py_library rule //tensorflow/contrib/metrics:metrics_py: target '
//tensorflow/contrib/metrics:metrics_py' depends on deprecated target '//tensorflow/python/ops/distributions:distributions': TensorFlow Distributions
 has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive
 new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorf
low/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:exporter': No longer supported. Switch to SavedModel immed
iately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/learn/BUILD:17:1: in py_library rule //tensorflow/contrib/learn:learn: target '//tensorf
low/contrib/learn:learn' depends on deprecated target '//tensorflow/contrib/session_bundle:gc': No longer supported. Switch to SavedModel immediately
.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/bayesflow/BUILD:17:1: in py_library rule //tensorflow/contrib/bayesflow:bayesflow_py: ta
rget '//tensorflow/contrib/bayesflow:bayesflow_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Dis
tributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distribution
s are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/BUILD:13:1: in py_library rule //tensorflow/contrib:contrib_py: target '//tensorflow/con
trib:contrib_py' depends on deprecated target '//tensorflow/contrib/distributions:distributions_py': TensorFlow Distributions has migrated to TensorF
low Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, a
nd will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
INFO: Analysed target //tensorflow/tools/pip_package:build_pip_package (387 packages loaded, 23360 targets configured).
INFO: Found 1 target...
ERROR: /home/jon/local_build/tensorflow/tensorflow/BUILD:606:1: Executing genrule //tensorflow:tf_python_api_gen_v1 failed (Exit 1)
Traceback (most recent call last):
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/jon/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/jon/anaconda3/lib/python3.6/imp.py"", line 347, in load_dynamic
    return _load(spec)
__ImportError: libmpi.so.40: cannot open shared object file: No such file or directory__

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/tools/api/generator/create_python_api.py"", line 27, in <module>
    from tensorflow.python.tools.api.generator import doc_srcs
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""/home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.py
thon_api_1_tf_python_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""/home/jon/anaconda3/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/jon/anaconda3/lib/python3.6/imp.py"", line 347, in load_dynamic
    return _load(spec)
__ImportError: libmpi.so.40: cannot open shared object file: No such file or directory__

Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
ModuleSpec(name='_pywrap_tensorflow_internal', loader=<_frozen_importlib_external.ExtensionFileLoader object at 0x7f059de53c18>, origin='/home/jon/.c
ache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/execroot/org_tensorflow/bazel-out/host/bin/tensorflow/create_tensorflow.python_api_1_tf_python
_api_gen_v1.runfiles/org_tensorflow/tensorflow/python/_pywrap_tensorflow_internal.so')
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 28.562s, Critical Path: 4.92s
INFO: 0 processes.
FAILED: Build did NOT complete successfully


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


"
25674,Segfault when using VLOG with with MKL on 1.13rc1,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): NO
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): from source with MKL
```
bazel build --config=mkl --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx2 --copt=-mavx --copt=-mfma --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --copt=""-DEIGEN_USE_VML"" //tensorflow/tools/pip_package:build_pip_package
```
- TensorFlow version (use command below): 1.13 rc1
- Python version: 3.6
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): 5.3
- CUDA/cuDNN version: 10.0, 7.4.1
- GPU model and memory: Nvidia V100, 16GB


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
Segfault when running an estimator test

**Describe the expected behavior**
No crash

**Code to reproduce the issue**
```git clone https://github.com/tensorflow/models && cd models/samples/core/get_started && TF_CPP_MIN_VLOG_LEVEL=1 python premade_estimator.py```
https://github.com/tensorflow/models/blob/master/samples/core/get_started/premade_estimator.py

**Other info / logs**
```
#0  0x00007fff474a1e57 in tensorflow::OptimizationPassRegistry::RunGrouping(tensorflow::OptimizationPassRegistry::Grouping, tensorflow::GraphOptimizationPassOptions const&) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/../libtensorflow_framework.so
#1  0x00007fff4bb9b6dc in tensorflow::DirectSession::CreateGraphs(tensorflow::BuildGraphOptions const&, std::unordered_map<std::string, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> >, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::unique_ptr<tensorflow::Graph, std::default_delete<tensorflow::Graph> > > > >*, std::unique_ptr<tensorflow::FunctionLibraryDefinition, std::default_delete<tensorflow::FunctionLibraryDefinition> >*, tensorflow::DirectSession::RunStateArgs*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, absl::InlinedVector<tensorflow::DataType, 4ul, std::allocator<tensorflow::DataType> >*, long long*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#2  0x00007fff4bb9c19c in tensorflow::DirectSession::CreateExecutors(tensorflow::CallableOptions const&, std::unique_ptr<tensorflow::DirectSession::ExecutorsAndKeys, std::default_delete<tensorflow::DirectSession::ExecutorsAndKeys> >*, std::unique_ptr<tensorflow::DirectSession::FunctionInfo, std::default_delete<tensorflow::DirectSession::FunctionInfo> >*, tensorflow::DirectSession::RunStateArgs*) () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#3  0x00007fff4bb9e478 in tensorflow::DirectSession::GetOrCreateExecutors(absl::Span<std::string const>, absl::Span<std::string const>, absl::Span<std::string const>, tensorflow::DirectSession::ExecutorsAndKeys**, tensorflow::DirectSession::RunStateArgs*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#4  0x00007fff4bb9fad4 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#5  0x00007fff49b36ad5 in tensorflow::SessionRef::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#6  0x00007fff49d4c1ff in TF_Run_Helper(tensorflow::Session*, char const*, TF_Buffer const*, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, TF_Tensor**, std::vector<std::string, std::allocator<std::string> > const&, TF_Buffer*, TF_Status*) [clone .constprop.654] () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#7  0x00007fff49d4ca79 in TF_SessionRun () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#8  0x00007fff49b31ec1 in tensorflow::TF_SessionRun_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) ()
   from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#9  0x00007fff49b31fa2 in tensorflow::TF_SessionRun_wrapper(TF_Session*, TF_Buffer const*, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<_object*, std::allocator<_object*> > const&, std::vector<TF_Output, std::allocator<TF_Output> > const&, std::vector<TF_Operation*, std::allocator<TF_Operation*> > const&, TF_Buffer*, TF_Status*, std::vector<_object*, std::allocator<_object*> >*) () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
#10 0x00007fff49ae633c in _wrap_TF_SessionRun_wrapper () from /home/ubuntu/anaconda3/envs/python2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so
```"
25670,Unable to use keras.application within TF Estimator,"I am having problems using keras.applications within an estimator. After much trying out myself, I came up with a minimal version to indicate the problem. I presume that it is caused by keras only having one session at a time while estimators set up several sessions for training and evaluation.

This is my [original post from stackoverflow](https://stackoverflow.com/questions/54589386/tensorflow-integrate-keras-model-in-estimator-model-fn)

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, see below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12.0
- Python version: 3.6.7

**Describe the current behavior**
The estimator apparently does not learn the data, which can be observed by a constant loss (in evaluation) and a stagnant loss during training.

**Describe the expected behavior**
The estimator should learn the MNIST dataset at least as good from the model features as from the raw data.

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

from keras.datasets import mnist


# switch to example 1/2/3
EXAMPLE_CASE = 1

# flag for initial weights loading of keras model
_W_INIT = True


def dense_net(features, labels, mode, params):
    # --- code to load a keras application ---

    # commenting in this line leads to a bump in the loss everytime the
    # evaluation is run, this indicating that keras does not handle well the
    # two sessions of the estimator API
    # tf.keras.backend.set_learning_phase(mode == tf.estimator.ModeKeys.TRAIN)
    global _W_INIT

    model = tf.keras.applications.MobileNet(
        input_tensor=features,
        input_shape=(128, 128, 3),
        include_top=False,
        pooling='avg',
        weights='imagenet' if _W_INIT else None)

    # only initialize weights once
    if _W_INIT:
        _W_INIT = False

    # switch cases
    if EXAMPLE_CASE == 1:
        # model.output is the same as model.layers[-1].output
        img = model.layers[-1].output
    elif EXAMPLE_CASE == 2:
        img = model(features)
    elif EXAMPLE_CASE == 3:
        # do not use keras features
        img = tf.keras.layers.Flatten()(features)
    else:
        raise NotImplementedError

    # --- regular code from here on ---
    for units in params['dense_layers']:
        img = tf.keras.layers.Dense(units=units, activation='relu')(img)

    logits = tf.keras.layers.Dense(units=10,
                                   activation='relu')(img)

    # compute predictions
    probs = tf.nn.softmax(logits)
    predicted_classes = tf.argmax(probs, 1)

    # compute loss
    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)

    acc = tf.metrics.accuracy(labels, predicted_classes)
    metrics = {'accuracy': acc}
    tf.summary.scalar('accuarcy', acc[1])

    if mode == tf.estimator.ModeKeys.EVAL:
        return tf.estimator.EstimatorSpec(
            mode, loss=loss, eval_metric_ops=metrics)

    # create training operation
    assert mode == tf.estimator.ModeKeys.TRAIN

    optimizer = tf.train.AdagradOptimizer(learning_rate=0.01)
    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())
    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)


def prepare_dataset(in_tuple, n):
    feats = in_tuple[0][:n, :, :]
    labels = in_tuple[1][:n]
    feats = feats.astype(np.float32)
    feats /= 255
    labels = labels.astype(np.int32)
    return (feats, labels)


def _parse_func(features, labels):
    feats = tf.expand_dims(features, -1)
    feats = tf.image.grayscale_to_rgb(feats)
    feats = tf.image.resize_images(feats, (128, 128))
    return (feats, labels)


def load_mnist(n_train=10000, n_test=3000):
    train, test = mnist.load_data()
    train = prepare_dataset(train, n_train)
    test = prepare_dataset(test, n_test)
    return train, test


def train_input_fn(imgs, labels, batch_size):
    dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))
    dataset = dataset.map(_parse_func)
    dataset = dataset.shuffle(500)
    dataset = dataset.repeat().batch(batch_size)
    return dataset


def eval_input_fn(imgs, labels, batch_size):
    dataset = tf.data.Dataset.from_tensor_slices((imgs, labels))
    dataset = dataset.map(_parse_func)
    dataset = dataset.batch(batch_size)
    return dataset


def main(m_dir=None):
    # fetch data
    (x_train, y_train), (x_test, y_test) = load_mnist()

    train_spec = tf.estimator.TrainSpec(
        input_fn=lambda: train_input_fn(
            x_train, y_train, 30),
        max_steps=150)

    eval_spec = tf.estimator.EvalSpec(
        input_fn=lambda: eval_input_fn(
            x_test, y_test, 30),
        steps=100,
        start_delay_secs=0,
        throttle_secs=0)

    run_cfg = tf.estimator.RunConfig(
        model_dir=m_dir,
        tf_random_seed=2,
        save_summary_steps=2,
        save_checkpoints_steps=10,
        keep_checkpoint_max=1)

    # build network
    classifier = tf.estimator.Estimator(
        model_fn=dense_net,
        params={
            'dense_layers': [256]},
        config=run_cfg)

    # fit the model
    tf.estimator.train_and_evaluate(
        classifier,
        train_spec,
        eval_spec)


if __name__ == ""__main__"":
    main()
```

**Other info / logs**

"
25669,Docker images with tags `1.13.0rc0` and `1.13.0rc0-py3` contains wrong version of TensorFlow Estimator,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 17.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **-**
- TensorFlow installed from (source or binary): **Binary/Docker**
- TensorFlow version: **???**
- Python version: **2.7**, **3.5**
- Installed using virtualenv? pip? conda?: **Docker**
- Bazel version (if compiling from source): **-**
- GCC/Compiler version (if compiling from source): **-**
- CUDA/cuDNN version: **-**
- GPU model and memory: **-**

The problem is that Docker [images](https://hub.docker.com/r/tensorflow/tensorflow/tags) with tags `1.13.0rc0` and `1.13.0rc0-py3` contains wrong version of `tensorflow_estimator` (this [repository](https://github.com/tensorflow/estimator)).

Just to show:
```
$ docker run -it tensorflow/tensorflow:1.13.0rc0 bash

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/

root@d1f1dd416bdd:/# pip show tensorflow_estimator
pip show tensorflow_estimator
DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.
Name: tensorflow-estimator
Version: 1.10.12
```"
25667,Trigger point of object detection in tensorflow lite,"I am doing object detection using tensorflow  lite. My code detects the object & add a label with it & also display the label with it, but after that, I want to display the label of an object which is detected separately.
& I want that point (trigger point) in code from where the label is displayed .
can you please help me to find out code .
 I am giving link of  my source code :
https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android"
25666,tf.GradientTape.gradient raises error with tf.math.segment_max,"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): installed by pip
- TensorFlow version (use command below): 1.13-rc1
- Python version: 3.6

**Code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

tf.enable_eager_execution()

inputs = np.random.randn(5)
inputs_tensor = tf.Variable(inputs, dtype=tf.float32)
seg_ids = tf.convert_to_tensor([0, 0, 0, 1, 1])
with tf.GradientTape() as tape:
    y = tf.math.segment_max(inputs_tensor, seg_ids)
    loss = (y - tf.convert_to_tensor([1., 1.], tf.float32)) ** 2
    loss = tf.reduce_mean(loss, 0)

grads = tape.gradient(loss, inputs_tensor)
```

**Other info / logs**
```
TypeError                                 Traceback (most recent call last)
<ipython-input-2-b33dd17ed2d8> in <module>
      6     loss = tf.reduce_mean(loss, 0)
      7 print(y)
----> 8 grads = tape.gradient(loss, inputs_tensor)

/anaconda3/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
    944         flat_sources,
    945         output_gradients=output_gradients,
--> 946         unconnected_gradients=unconnected_gradients)
    947 
    948     if not self._persistent:

/anaconda3/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, unconnected_gradients)
     70       sources,
     71       output_gradients,
---> 72       compat.as_str(unconnected_gradients.value))

/anaconda3/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads)
    129     return [None] * num_inputs
    130 
--> 131   return grad_fn(mock_op, *out_grads)
    132 
    133 

/anaconda3/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py in _SegmentMaxGrad(op, grad)
    277 def _SegmentMaxGrad(op, grad):
    278   """"""Gradient for SegmentMax.""""""
--> 279   return _SegmentMinOrMaxGrad(op, grad)
    280 
    281 

/anaconda3/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py in _SegmentMinOrMaxGrad(op, grad)
    257   print('op:', op)
    258   print('op.outputs:', op.outputs)
--> 259   gathered_outputs = array_ops.gather(op.outputs[0], op.inputs[1])
    260   is_selected = math_ops.equal(op.inputs[0], gathered_outputs)
    261   num_selected = math_ops.segment_sum(math_ops.cast(is_selected, grad.dtype),

TypeError: 'NoneType' object is not subscriptable
```
I find that op.outputs for 'segment_max' is None, and is there any solution for that?"
25665,Type Error caused by using AttentionCellWrapper: Tensors cannot be iterated.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Manjaro Illyria 18.0.2
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:no mobile device related
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): ('v1.12.0-0-ga6d8ffae09', '1.12.0')
- Python version:2.7.15
- Bazel version (if compiling from source):installed from binary
- GCC/Compiler version (if compiling from source):installed from binary
- CUDA/cuDNN version:no CUDA
- GPU model and memory:no computing GPU

**Describe the current behavior**
I am trying to add tf.contrib.rnn.AttentionCellWrapper to tf.nn.rnn_cell.GRUCell. Before adding the wrapper, the program is completely running normally. After adding the wrapper, the program reports an error shown in the traceback below.

The error `Tensor objects are only iterable when eager execution is enabled` is confusing me, since I have tried enable the eager execution and the results are the same.
I have changed GRUCell to LSMMCell or other RNN-related Cell and the results are the same.
I have tried multiple tensorflow versions and the results are the same.
The code I am modifying is the same as [this code](https://github.com/Songweiping/GRU4Rec_TensorFlow/blob/master/model.py), and what I am trying is to add AttentionCellWrapper between 126 and 127 lines.

**Describe the expected behavior**
The program should just work fine without any error. All the changes I made compared to the normal code were to add the AttentionCellWrapper.
The official documentation also has no explanation for the cause of this error.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

As described above, the normal code with a AttentionCellWrapper, and the function that triggered the error is as follows. The complete code also covers other files in the [repo](https://github.com/Songweiping/GRU4Rec_TensorFlow).

```
def build_model(self):
        
        self.X = tf.placeholder(tf.int32, [self.batch_size], name='input')
        self.Y = tf.placeholder(tf.int32, [self.batch_size], name='output')
        self.state = [tf.placeholder(tf.float32, [self.batch_size, self.rnn_size], name='rnn_state') for _ in xrange(self.layers)]
        self.global_step = tf.Variable(0, name='global_step', trainable=False)

        with tf.variable_scope('gru_layer'):
            sigma = self.sigma if self.sigma != 0 else np.sqrt(6.0 / (self.n_items + self.rnn_size))
            if self.init_as_normal:
                initializer = tf.random_normal_initializer(mean=0, stddev=sigma)
            else:
                initializer = tf.random_uniform_initializer(minval=-sigma, maxval=sigma)
            embedding = tf.get_variable('embedding', [self.n_items, self.rnn_size], initializer=initializer)
            softmax_W = tf.get_variable('softmax_w', [self.n_items, self.rnn_size], initializer=initializer)
            softmax_b = tf.get_variable('softmax_b', [self.n_items], initializer=tf.constant_initializer(0.0))

            cell = rnn_cell.GRUCell(self.rnn_size, activation=self.hidden_act)
            attn_cell = tf.contrib.rnn.AttentionCellWrapper(cell, attn_length=20)  # state_is_tuple is True by default
            drop_cell = rnn_cell.DropoutWrapper(attn_cell, output_keep_prob=self.dropout_p_hidden)
            stacked_cell = rnn_cell.MultiRNNCell([drop_cell] * self.layers)
            
            inputs = tf.nn.embedding_lookup(embedding, self.X)
            output, state = stacked_cell(inputs, tuple(self.state))
            self.final_state = state

        if self.is_training:
            '''
            Use other examples of the minibatch as negative samples.
            '''
            sampled_W = tf.nn.embedding_lookup(softmax_W, self.Y)
            sampled_b = tf.nn.embedding_lookup(softmax_b, self.Y)
            logits = tf.matmul(output, sampled_W, transpose_b=True) + sampled_b
            self.yhat = self.final_activation(logits)
            self.cost = self.loss_function(self.yhat)
        else:
            logits = tf.matmul(output, softmax_W, transpose_b=True) + softmax_b
            self.yhat = self.final_activation(logits)

        if not self.is_training:
            return

        self.lr = tf.maximum(1e-5,tf.train.exponential_decay(self.learning_rate, self.global_step, self.decay_steps, self.decay, staircase=True)) 
        
        '''
        Try different optimizers.
        '''
        #optimizer = tf.train.AdagradOptimizer(self.lr)
        optimizer = tf.train.AdamOptimizer(self.lr)
        #optimizer = tf.train.AdadeltaOptimizer(self.lr)
        #optimizer = tf.train.RMSPropOptimizer(self.lr)

        tvars = tf.trainable_variables()
        gvs = optimizer.compute_gradients(self.cost, tvars)
        if self.grad_cap > 0:
            capped_gvs = [(tf.clip_by_norm(grad, self.grad_cap), var) for grad, var in gvs]
        else:
            capped_gvs = gvs 
        self.train_op = optimizer.apply_gradients(capped_gvs, global_step=self.global_step)
```


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


```
Traceback (most recent call last):
  File ""/home/syh/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/183.5429.31/helpers/pydev/pydevd.py"", line 1741, in <module>
    main()
  File ""/home/syh/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/183.5429.31/helpers/pydev/pydevd.py"", line 1735, in main
    globals = debugger.run(setup['file'], None, None, is_module)
  File ""/home/syh/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/183.5429.31/helpers/pydev/pydevd.py"", line 1135, in run
    pydev_imports.execfile(file, globals, locals)  # execute the script
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/GRU4Rec_TensorFlow-master/main.py"", line 86, in <module>
    gru4rec = model.GRU4Rec(sess, args)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/GRU4Rec_TensorFlow-master/model.py"", line 72, in __init__
    self.build_model()
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/GRU4Rec_TensorFlow-master/model.py"", line 142, in build_model
    output, state = multi_cell(inputs, tuple(self.state))
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 233, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1486, in call
    cur_inp, new_state = cell(cur_inp, cur_state)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1282, in __call__
    output, new_state = self._cell(inputs, state, scope=scope)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 233, in __call__
    return super(RNNCell, self).__call__(inputs, state)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/layers/base.py"", line 374, in __call__
    outputs = super(Layer, self).__call__(inputs, *args, **kwargs)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py"", line 1190, in call
    state, attns, attn_states = state
  File ""/run/media/syh/0D11080D0D11080D/workspace/Recommender-System/exp/venv/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 459, in __iter__
    ""Tensor objects are only iterable when eager execution is ""
TypeError: Tensor objects are only iterable when eager execution is enabled. To iterate over this tensor use tf.map_fn.
```
"
25663,[FR] Adding alpha channel support,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).
2. The form below must be filled out.
3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information
- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:
- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:
- **TensorFlow installed from (source or binary)**: binary
- **TensorFlow version (use command below)**: latest from apt
- **Python version**: 3.6.4
- **Bazel version (if compiling from source)**:
- **GCC/Compiler version (if compiling from source)**: not relevant
- **CUDA/cuDNN version**: not relevant
- **GPU model and memory**: not relevant
- **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

I am creating an artistic neural network and it would be amazing to handle the transparency. Unfortunately as I noticed, Tensorflow doesn't have such feature. Is there any particular reason why it's not a thing? Or would you mind considering adding it?

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
25661,NNAPI does not accept the reshape operation with the -1 parameter,"**System information**
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
* Mobile device: Xiaomi Mi MIX 2S (Android 9, API 28)
* TensorFlow Lite version on Android: 0.0.0-gpu-experimental



**Provide the text output from tflite_convert**

```
tflite_convert \
--graph_def_file=tflite_graph.pb \
--output_file=detect.tflite \
--input_shapes=1,300,300,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='raw_outputs/class_predictions','raw_outputs/box_encodings' \
--inference_type=QUANTIZED_UINT8 \
--mean_values=128 \
--std_dev_values=128 \
--change_concat_input_ranges=false \
--allow_custom_ops
```

**Describe the current behavior**

I use SSD mobilenet V1 model from [zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md). When I try to use this model in an Android application using NNAPI, I get an error message. The problem is that the Reshape operation has the new shape [1, -1, 91]. Will it be possible to use Reshape with the -1 parameter in NNAPI. And is it possible to change this parameter to static without retraining the model?
"
25660,TFL Detect app crash while using NNAPI,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 16.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Xiaomi Mi 8**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **1.12**
- Python version: **3.5**
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: **9.0/7.5**
- GPU model and memory: **GTX 1050 - 4GB**


**Describe the current behavior**
I am trying to build the [TF Lite detect app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/android) with using NNAPI, but when I set useNNAPI to **true** it crashes with an exception:
`Process: org.tensorflow.lite.demo, PID: 32661
    java.lang.IllegalArgumentException: Internal error: Failed to run on the given Interpreter: 
        at org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)
        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:145)
        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:229)
        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:196)
        at org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:247)
        at android.os.Handler.handleCallback(Handler.java:873)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:201)
        at android.os.HandlerThread.run(HandlerThread.java:65)`

But, when I set useNNAPI to **false** it runs without any issue. On the other hand, I also tried to use NNAPI with [TFL Classifier app](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteImageClassifier.java) and it runs flawlessly (it means there is no problem with my phone). The question is that, is it possible to use NNAPI with detector app or not? 

**Code to reproduce the issue**
Changed few lines in [TFLiteObjectDetectionAPIModel.java](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L120):
`
d.tfliteOptions.setUseNNAPI(true);
    try {
       d.tfLite = new Interpreter(loadModelFile(assetManager, modelFilename), d.tfliteOptions);
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
`

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25659,Keras Concatenate : AttributeError: 'list' object has no attribute 'shape',"I am using Google Colab.
Here is a small code I have written:
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv1D, Reshape, Concatenate

branch1 = Sequential()
branch1.add(Conv1D(2,3,activation='tanh',input_shape=(100, 1)))
branch1.add(Conv1D(4,3,activation='tanh'))
branch1.add(Conv1D(6,3,activation='tanh'))
branch1.add(Dropout(0.2))
branch1.add(Conv1D(8,3,activation='tanh'))
branch1.add(Conv1D(10,3,activation='relu'))
branch1.add(Flatten())
branch1.add(Dense(10))


branch2 = Sequential()
branch2.add(Dense(10,input_dim=1))
branch2.add(Dense(10,activation='linear'))

# Concatenate([branch1,branch2])


model = Sequential()
model.add(Concatenate([branch1, branch2]))
model.add(Dense(1,activation='relu'))
# branch1.add(Dropout(0.4))
model.add(Dense(1,activation='relu'))
model.compile(loss='mean_squared_error', optimizer='rmsprop')

model_info = model.fit([data_cnn,data_mw], target, epochs=1000, batch_size=100, verbose=2,validation_data=([val_data_cnn,val_data_mw],val_target))
```

Some Info about data : 
*  `data_cnn` : type = numpy.ndarray | shape = (2580, 100, 1) 
*  `data_mw` : type = numpy.ndarray | shape = (2580, 1)
*  `val_data_cnn` : type = numpy.ndarray | shape = (645, 100, 1)
*  `val_data_mw` : type = numpy.ndarray | shape = (645, 1)

Link to the notebook : https://colab.research.google.com/drive/1kpopbk_4u7Tsuaxoc9EbRCKzN1aZ4kSM"
25658,test,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
25657,TFLite GPU Delegate will block the thread who is calling interpreter.run(),"**System information**
- OS Platform and Distribution: Linux Ubuntu 16.04
- Mobile device: OnePlus 3, One Plus 5 and Pixel 2 XL
- TensorFlow Lite version on Android: 0.0.0-gpu-experimental
- Have I written custom code: a GitHub repo contains the codes to reproduce the issue. 
https://github.com/dailystudio/ml/tree/master/deeplab
- DeepLab v3 TFLite model: [DeepLab segmentation (257x257)](https://storage.googleapis.com/download.tensorflow.org/models/tflite/gpu/deeplabv3_257_mv_gpu.tflite)

**Describe the current behavior**
Using the following code snippet to create an Interpreter with GPU delegate
```java
        Interpreter.Options options = new Interpreter.Options();

        GpuDelegate delegate = new GpuDelegate();
        options.addDelegate(delegate);
        

        Interpreter interpreter = new Interpreter(mModelBuffer, options);
```
Calling the run() of the Interpreter with following lines of codes:
```java
        interpreter.run(mImageData, mOutputs);
```
If these two code snippets are called in two different threads, the thread which calls interpreter.run() will be blocked. interpreter.run() will never return.
If these two code snippets are called in the same thread, interpreter.run() will be executed properly and output correct results.

**Describe the expected behavior**
Developers needn't care about which threads are used for calling these APIs. Even these APIs are called in different threads, interpreter.run() should return correctly with blocking issue.

**Code to reproduce the issue**
The full code can be found here:
https://github.com/dailystudio/ml/blob/master/deeplab/app/src/main/java/com/dailystudio/deeplab/ml/DeepLabLite.java
Currently, the code in repository works fine because the new Interpreter() and interpreter.run() are called in the same thread. 
The DeepLabLite class has two important functions: **initialize()** and **segment()**. In **intialize()**, we read TFLite model from asset/ directory into a MappedByteBuffer:
```java

    @Override
    public boolean initialize(Context context) {
        if (context == null) {
            return false;
        }

        mModelBuffer = loadModelFile(context, MODEL_PATH);
        if (mModelBuffer == null) {
            return false;
        }

        ...
    }
```
In **segment()**, we use that MappedByteBuffer to create an Interpreter and call run() for inference:
```java
        ...
        Interpreter.Options options = new Interpreter.Options();

        if (USE_GPU) {
            GpuDelegate delegate = new GpuDelegate();
            options.addDelegate(delegate);
        }

        Interpreter interpreter = new Interpreter(mModelBuffer, options);
        ...
        final long start = System.currentTimeMillis();
        interpreter.run(mImageData, mOutputs);
        final long end = System.currentTimeMillis();
        ...
```
The DeepLabLite.initialize() is called in an AsyncTask after application is launched, while the DeepLabLite.segment() is called a Loader after users pick an image for segmentation. These codes will be no problem.  
But if we keep the codes of calling these two methods unchanged and move the following line from **segment()** to **initialize()**:
```java
        Interpreter interpreter = new Interpreter(mModelBuffer, options);
```
> P.S.: Of course, we need to declare a class member to hold this Interpreter for future using in segment().

Then the calling of interpreter.run() will be blocked forever. 

**Other information**
With my tests, I suspect this problem is independent of devices. It would happen on all Android devices. It should be related to GpuDelegate. If you do not call options.addDelegate() to add a GpuDelegate, the interpreter.run() will also run well."
25656,Distributed training using grpc+verbs got an assertion in verbs/rdma.cc:1557,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): 
Custom code using tf.estimator.DNNLinearCombinedClassifier
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): privileged ubuntu 16.04 docker 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
source built with verbs feature
bazel build --config=mkl -c opt --config=verbs //tensorflow/tools/pip_package:build_pip_package

- TensorFlow version (use command below): 1.12.0
- Python version: python3.6
- Bazel version (if compiling from source): 0.18
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10)
- CUDA/cuDNN version: N/A
- GPU model and memory: CPU only


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

b'v1.12.0-5-g7317495' 1.12.0

**Describe the current behavior**
I deployed one chief, one PS and one worker with protocol 'grpc+verbs'. This case uses only CPU device. The master and PS works properly. The worker meets an assertion, and exits.
Switched to protocol grpc, the code works well.
The result of rping shows that RDMA configure is good.

```
2019-02-11 14:16:19.248897: F tensorflow/contrib/verbs/rdma.cc:1557] Check failed: mr_ != nullptr  No memory region found for address 0x7f1b40000bc0: /job:ps/replica:0/task:0/device:CPU:0;dc5bce4913999973;/job:worker/replica:0/task:0/device:CPU:0;edge_247_report_uninitialized_variables_1/VarIsInitializedOp_4;0:0
```

**Describe the expected behavior**
The worker should run normally.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
    server = tf.train.Server(cluster_spec,
                           job_name=job_name,
                           task_index=task_index,
                            protocol = 'grpc+verbs')

  estimator = tf.estimator.DNNLinearCombinedClassifier(...)
  estimator.train()
```

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[worker-20190211-141600.log](https://github.com/tensorflow/tensorflow/files/2850045/worker-20190211-141600.log)

"
25652,export_saved_model raises TypeError: Failed to convert object of type <class 'dict_values'> to Tensor.,"`TPUEstimator.export_saved_model` raises `TypeError` (when used on CPU). The same code with TPUEstimator replaced with Estimator works correctly.

The error is:

```
TypeError: Failed to convert object of type <class 'dict_values'> to Tensor. Contents: dict_values([<tf.Tensor 'sat_prob:0' shape=(?,) dtype=float32>, <tf.Tensor 'policy_prob:0' shape=(?, ?, 2) dtype=float32>]). Consider casting elements to a supported type.
```

**The relevant code**

```
def serving_input_receiver_fn():
    feature = tf.placeholder(tf.float32, shape=[None, None, None, 2])

    return tf.estimator.export.TensorServingInputReceiver(feature, feature)

estimator.export_saved_model(FLAGS.export_dir, serving_input_receiver_fn)
```


**Current behaviour**

`export_saved_model` fails with `TypeError`. 

**Expected behaviour**

`export_saved_model` should export the model successfully.

**System information**

MacOS, Python 3.6, Tensorflow 1.12.0 from PyPI.

**Full code**

Since exporting a model requires a trained model, it wasn't easy to fully isolate the failing code, so I'm posting a full model file (with as much irrelevant stuff removed as possible).

With TPUEstimator, failing: https://gist.github.com/mluszczyk/d60ed4205060eb7ef53c309c490fbe48

With Estimator, working: https://gist.github.com/mluszczyk/bbd4f9136fc4788251079ac5b2176a01

**Traceback**

```
...
WARNING:tensorflow:From /Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.
Instructions for updating:
Pass your op to the equivalent parameter main_op instead.
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
WARNING:tensorflow:rewrite_for_inference (from tensorflow.contrib.tpu.python.tpu.tpu) is experimental and may change or be removed at any time, and without warning.
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Running infer on CPU
ERROR:tensorflow:Operation of type Placeholder (policy_labels) is not supported on the TPU. Execution will fail if this op is used in the graph. 
ERROR:tensorflow:Operation of type Placeholder (sat_labels) is not supported on the TPU. Execution will fail if this op is used in the graph. 
INFO:tensorflow:Done calling model_fn.
Traceback (most recent call last):
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 527, in make_tensor_proto
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 527, in <listcomp>
    str_values = [compat.as_bytes(x) for x in proto_values]
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/util/compat.py"", line 61, in as_bytes
    (bytes_or_text,))
TypeError: Expected binary or unicode string, got dict_values([<tf.Tensor 'sat_prob:0' shape=(?,) dtype=float32>, <tf.Tensor 'policy_prob:0' shape=(?, ?, 2) dtype=float32>])

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""neurosat_tpu.py"", line 253, in <module>
    tf.app.run()
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/platform/app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""neurosat_tpu.py"", line 248, in main
    estimator.export_saved_model(FLAGS.export_dir, serving_input_receiver_fn)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 734, in export_saved_model
    strip_default_attrs=True)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 663, in export_savedmodel
    mode=model_fn_lib.ModeKeys.PREDICT)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 789, in _export_saved_model_for_mode
    strip_default_attrs=strip_default_attrs)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 907, in _export_all_saved_models
    mode=model_fn_lib.ModeKeys.PREDICT)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2188, in _add_meta_graph_for_mode
    check_variables=False))
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py"", line 984, in _add_meta_graph_for_mode
    config=self.config)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2192, in _call_model_fn
    return self._call_model_fn_for_inference(features, labels, mode, config)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/contrib/tpu/python/tpu/tpu_estimator.py"", line 2253, in _call_model_fn_for_inference
    new_tensors.append(array_ops.identity(t))
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py"", line 81, in identity
    return gen_array_ops.identity(input, name=name)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 3454, in identity
    ""Identity"", input=input, name=name)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 513, in _apply_op_helper
    raise err
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py"", line 510, in _apply_op_helper
    preferred_dtype=default_dtype)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1146, in internal_convert_to_tensor
    ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 229, in _constant_tensor_conversion_function
    return constant(v, dtype=dtype, name=name)
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 208, in constant
    value, dtype=dtype, shape=shape, verify_shape=verify_shape))
  File ""/Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/framework/tensor_util.py"", line 531, in make_tensor_proto
    ""supported type."" % (type(values), values))
TypeError: Failed to convert object of type <class 'dict_values'> to Tensor. Contents: dict_values([<tf.Tensor 'sat_prob:0' shape=(?,) dtype=float32>, <tf.Tensor 'policy_prob:0' shape=(?, ?, 2) dtype=float32>]). Consider casting elements to a supported type.
```

**Output of the variant with Estimator**

```
WARNING:tensorflow:From /Users/michal/.virtualenvs/deepsat/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py:1044: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.
Instructions for updating:
Pass your op to the equivalent parameter main_op instead.
INFO:tensorflow:Assets added to graph.
INFO:tensorflow:No assets to write.
INFO:tensorflow:SavedModel written to: ../models/export/ex20/temp-b'1549813434'/saved_model.pb
```

**Flags**
Flag values to the attached files:
```
python neurosat_tpu.py --use_tpu=False --tpu=$TPU_NAME --train_file=$TRAIN_FILE --test_file=$TEST_FILE --train_steps=10 --test_steps=0 --model_dir=$MODEL_DIR --variable_number=8 --clause_number=80 --train_files_gzipped=False --batch_size=1 --iterations=1 --export_dir=$EXPORT_DIR --level_number=0
```"
25649,"How to fix - ImportError: DLL load failed, when importing keras","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- TensorFlow installed from (source or binary): Using pip, binary
- TensorFlow version: 1.12.0
- Python version: 3.6.0
- Installed using virtualenv? pip? conda?: Installed Using Pip and virtualenv
- CUDA/cuDNN version:  ??
- GPU model and memory: Intel(R) HD Graphics, 1664 MB

I am trying to install keras with tensorflow backend

I have run `pip install keras` at first, and then `pip install tensorflow` both commands finished succesfully, now when i'm trying to import Sequential from keras.models I get error

Here is my code
![image](https://user-images.githubusercontent.com/41778180/52533028-41b7c000-2d47-11e9-9b98-08ea334e8eb8.png)

Here is error

```
Using TensorFlow backend.
Traceback (most recent call last):
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed:         (DLL).

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""C:/Users/Admin/PycharmProjects/keras/test.py"", line 3, in <module>
    from keras.models import Sequential
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\__init__.py"", line 3, in <module>
    from . import utils
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\utils\__init__.py"", line 6, in <module>
    from . import conv_utils
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\utils\conv_utils.py"", line 9, in <module>
    from .. import backend as K
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\__init__.py"", line 89, in <module>
    from .tensorflow_backend import *
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\keras\backend\tensorflow_backend.py"", line 5, in <module>
    import tensorflow as tf
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Admin\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed:         (DLL).


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
```

I don't know why error is in Russian, maybe it is because of my system language, but here is translation

`Original error:
ImportError: DLL load failed:         (DLL)`

`Translation:
ImportError: DLL load failed: A crash occurred in the dynamic link library initialization program. (DLL)`

I am using

![image](https://user-images.githubusercontent.com/41778180/52533043-804d7a80-2d47-11e9-80c7-161e436504b3.png)

And Python 3.6.0

What can I do to solve this issue?"
25645,Inconsistent eigh gradients,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Mojave
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): `conda install -c conda-forge tensorflow` in a Python 3.6 environment
- TensorFlow version (use command below): v1.10.0-rc1-19-g656e7a2b34 1.10.0
- Python version: Python 3.6.8 :: Anaconda, Inc.
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
I'm porting an optimization involving calls to `eigh` from the [`autograd`](https://github.com/HIPS/autograd) library to Tensorflow. When computing the gradient of a loss function involving a subset of eigenvectors, I see fairly large large differences (in terms of `norm(grad_np - grad_tf)`).

In my application, this causes the optimization to fail completely under TensorFlow while succeeding with the `autograd` library (using same optimizer, step size etc). 

**Describe the expected behavior**
Different numerical implementations of derivatives should be consistent.  Comparing all other operations in the AD graph between autograd and TF showed no problems, and the same would be expected for eigh.

**Code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

def loss_z_np(z, G, w, ne=3):
    from autograd import jacobian, grad
    def loss(z):
        import autograd.numpy as np
        d = 2*z - np.sum(w, axis=1)
        e, v = np.linalg.eigh(np.diag(d) + w)
        ssev = np.sum(np.square(e[-ne:] * v[:, -ne:]), axis=-1)
        return np.sum(np.dot(G, ssev))
    return grad(loss)(z)

def lsa_loss(z, G, w, ne=3):
    d = 2*z - tf.reduce_sum(w, 1)
    e, v = tf.linalg.eigh(tf.diag(d) + w)
    ssev = tf.reduce_sum(tf.square(e[-ne:] * v[:, -ne:]), 1)
    return tf.reduce_sum(tf.matmul(G, tf.expand_dims(ssev, 1)))

def loss_z_tf(z, G, w, ne=3):
    G, w = [tf.convert_to_tensor(_, dtype=tf.float32) for _ in (G, w)]
    z = tf.Variable(tf.convert_to_tensor(z, dtype=tf.float32))
    loss = lsa_loss(z, G, w, ne=ne)
    lg = tf.gradients(loss, z)
    model = tf.global_variables_initializer()
    trace = []
    with tf.Session() as session:
        session.run(model)
        lg_np = session.run(lg)
    return lg_np


m, n = 79, 164
norms = []
for _ in range(30):
    G = np.random.rand(m, n).astype('f')
    w = np.random.rand(n, n).astype('f')
    z = - np.random.rand(n).astype('f')
    lg_np = loss_z_np(z, G, w, ne=10)
    lg_tf = loss_z_tf(z, G, w, ne=10)
    norms.append(np.linalg.norm(lg_np - lg_tf))
print('average norm', sum(norms)/len(norms))
```
This prints an average of ~30~ 16.

**Other info / logs**

n/a

_edit_ to switch the NumPy code to float32; problem persists."
25644,"Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Ubuntu 18.04:
- 
- TensorFlow installed from source
- TensorFlow version:
- Python version: 3.6.7
- Installed using virtualenv:
- Bazel version : 0.22.0
- GCC/Compiler version : 7.3
- CUDA/cuDNN version: 7.4
- GPU model and memory: RTX 2080 Ti



./configure
WARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command ""bazel shutdown"".
INFO: Invocation ID: 9cf9c46b-c379-4fdc-a9f5-13ad76711061
You have bazel 0.22.0 installed.
Please specify the location of python. [Default is /home/peter/PycharmProjects/Virtual_envs/tensorflow/bin/python]: 


Found possible Python library paths:
  /home/peter/PycharmProjects/Virtual_envs/tensorflow/lib/python3.6/site-packages
  /usr/local/lib/python3.6/dist-packages
  /usr/lib/python3/dist-packages
Please input the desired Python library path to use.  Default is [/home/peter/PycharmProjects/Virtual_envs/tensorflow/lib/python3.6/site-packages]

Do you wish to build TensorFlow with XLA JIT support? [Y/n]: y
XLA JIT support will be enabled for TensorFlow.

Do you wish to build TensorFlow with OpenCL SYCL support? [y/N]: n
No OpenCL SYCL support will be enabled for TensorFlow.

Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]: y
CUDA support will be enabled for TensorFlow.

Please specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 10.0]: 10.0


Please specify the location where CUDA 10.0 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Please specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7]: 7.4


Please specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: 


Do you wish to build TensorFlow with TensorRT support? [y/N]: n
No TensorRT support will be enabled for TensorFlow.

Please specify the locally installed NCCL version you want to use. [Default is to use https://github.com/nvidia/nccl]: 


Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build time and binary size. [Default is: 7.5]: 


Do you want to use clang as CUDA compiler? [y/N]: y
Clang will be used as CUDA compiler.

Do you wish to download a fresh release of clang? (Experimental) [y/N]: y
Clang will be downloaded and used to compile tensorflow.

Do you wish to build TensorFlow with MPI support? [y/N]: n
No MPI support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -march=native -Wno-sign-compare]: 


Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
	--config=mkl         	# Build with MKL support.
	--config=monolithic  	# Config for mostly static monolithic build.
	--config=gdr         	# Build with GDR support.
	--config=verbs       	# Build with libverbs support.
	--config=ngraph      	# Build with Intel nGraph support.
	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.
Preconfigured Bazel build configs to DISABLE default on features:
	--config=noaws       	# Disable AWS S3 filesystem support.
	--config=nogcp       	# Disable GCP support.
	--config=nohdfs      	# Disable HDFS support.
	--config=noignite    	# Disable Apache Ignite support.
	--config=nokafka     	# Disable Apache Kafka support.
	--config=nonccl      	# Disable NVIDIA NCCL support.
Configuration finished
(tensorflow) peter@Peter-Ubuntu:~/tensorflow$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
bash: ./bazel-bin/tensorflow/tools/pip_package/build_pip_package: No such file or directory
(tensorflow) peter@Peter-Ubuntu:~/tensorflow$ ./bazel-bin/tensorflow/tools/pip_package/build_pip_package/tmp/tensorflow_pkg
bash: ./bazel-bin/tensorflow/tools/pip_package/build_pip_package/tmp/tensorflow_pkg: No such file or directory
(tensorflow) peter@Peter-Ubuntu:~/tensorflow$ bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package
Starting local Bazel server and connecting to it...
INFO: Invocation ID: e9e2dd08-b8a0-4807-b1fe-cd6842a67a51
DEBUG: Rule 'build_bazel_rules_swift' modified arguments {""commit"": ""001736d056d7eae20f1f4da41bc9e6f036857296"", ""shallow_since"": ""1547844730 -0800""} and dropped [""tag""]
DEBUG: /home/peter/.cache/bazel/_bazel_peter/21121c77bac67ad4a1aed68d97d6cb00/external/build_bazel_rules_apple/apple/repositories.bzl:35:5: 
WARNING: `build_bazel_rules_apple` depends on `bazel_skylib` loaded from https://github.com/bazelbuild/bazel-skylib.git (tag 0.6.0), but we have detected it already loaded into your workspace from None (tag None). You may run into compatibility issues. To silence this warning, pass `ignore_version_differences = True` to `apple_rules_dependencies()`.

ERROR: /home/peter/tensorflow/tensorflow/tools/pip_package/BUILD:34:1: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather:
@local_config_cuda//cuda:using_nvcc
@local_config_cuda//cuda:using_clang
Multiple matches are not allowed unless one is unambiguously more specialized.
ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: 

/home/peter/tensorflow/tensorflow/tools/pip_package/BUILD:34:1: Illegal ambiguous match on configurable attribute ""deps"" in //tensorflow/tools/pip_package:included_headers_gather:
@local_config_cuda//cuda:using_nvcc
@local_config_cuda//cuda:using_clang
Multiple matches are not allowed unless one is unambiguously more specialized.
INFO: Elapsed time: 41.489s
INFO: 0 processes.
"
25643,Maybe create a new project tensorflow/help?,"Hi, I have recently started contributing to the TensorFlow community, and I'm seeing that a lot of issues that people post are essentially some questions meant for Stack Overflow.

These issues clutter the issue tracker of the main repository which is meant for development purposes, since a maintainer who is not familiar with the answer to the question would usually not close the issue right away, and would wait for someone else to address it, and then close it after it has been addressed.

I propose that we create another repository called **help** under the organization page of TensorFlow. All the issues that are essentially some questions meant for Stack Overflow can be [transferred](https://help.github.com/articles/transferring-an-issue-to-another-repository/) to that repository, so that anyone who is willing to answer them can do it over there.

The advantage would be that the issue tracker of the main repository would become less cluttered, since anyone who has the rights to both the repositories would be able to shift these issues to the help repository immediately, while also be assured that these questions would not go unanswered."
25641,tf.rnn wrappers are incompatible with tf.keras.layers cells,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installed: binary
- TensorFlow version: `2.0.0-dev20190206`
- Python version: 3.6.6

**Describe the current behavior**

In the TensorFlow 2.0 preview, the `tf.rnn.DropoutWrapper` and `tf.rnn.ResidualWrapper` wrappers are incompatible with cells from `tf.keras.layers`. It raises an error for missing a `zero_state` method.

**Describe the expected behavior**

The classes in `tf.rnn` should be compatible with cells defined in `tf.keras.layers`.

**Code to reproduce the issue**

```python
import tensorflow as tf

cell = tf.keras.layers.LSTMCell(10)
cell = tf.rnn.ResidualWrapper(cell)
cell.get_initial_state(batch_size=4, dtype=tf.float32)
```

**Other info / logs**

```text
Traceback (most recent call last):
  File ""test/rnn_v2.py"", line 5, in <module>
    cell.get_initial_state(batch_size=4, dtype=tf.float32)
  File ""/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 307, in get_initial_state
    return self.zero_state(batch_size, dtype)
  File ""/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py"", line 1545, in zero_state
    return self._cell.zero_state(batch_size, dtype)
AttributeError: 'LSTMCell' object has no attribute 'zero_state'
```

(If this API is not ready yet, please ignore this issue.)"
25639,"I want to reshape Tensor of shape[None,19,19,25] to [None,19,19,5,5] .Can anyone help me out","I want to reshape Tensor of shape[None,19,19,25] to [None,19,19,5,5] .Can anyone help me out"
25638,Can't build tensorflow master with CUDA & MPI enabled,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): master
- Python version: 3.5.2
- Bazel version (if compiling from source): 0.22.0 (tried with 0.21.0, 0.20.0, 0.19.0)
- GCC/Compiler version (if compiling from source): 5.4.0
- Nvidia Driver: 410.48
- CUDA/cuDNN version: 10.0.130 / 7.4.2
- GPU model and memory: Tesla V100

I use nvidia-docker/docker to build and test the master branch of tensorflow. Since a few days, I consistently get the same error over and over.

```shell
Execution platform: @bazel_tools//platforms:host_platform
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc: In lambda function:
tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:125:49: error: no matching function for call to 'tensorflow::TensorResponse::InitPartial(const tensorflow::RecvTensorResponse&)'
           tr.InitPartial(mpi_response.response());
                                                 ^
In file included from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:31:0:
./tensorflow/core/distributed_runtime/tensor_coding.h:79:8: note: candidate: void tensorflow::TensorResponse::InitPartial(const tensorflow::RecvTensorResponse&, const tensorflow::AllocationAttributes&)
   void InitPartial(const RecvTensorResponse& response,
        ^
./tensorflow/core/distributed_runtime/tensor_coding.h:79:8: note:   candidate expects 2 arguments, 1 provided
In file included from ./tensorflow/core/platform/default/logging.h:24:0,
                 from ./tensorflow/core/platform/logging.h:25,
                 from ./tensorflow/contrib/mpi/mpi_utils.h:25,
                 from ./tensorflow/contrib/mpi/mpi_rendezvous_mgr.h:34,
                 from tensorflow/contrib/mpi/mpi_rendezvous_mgr.cc:18:
./tensorflow/core/util/tensor_format.h: In instantiation of 'T tensorflow::GetFilterDim(tensorflow::gtl::ArraySlice<T>, tensorflow::FilterTensorFormat, char) [with T = long long int; tensorflow::gtl::ArraySlice<T> = absl::Span<const long long int>]':
./tensorflow/core/util/tensor_format.h:467:54:   required from here
./tensorflow/core/util/tensor_format.h:441:29: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]
   CHECK(index >= 0 && index < dimension_attribute.size())
                             ^
./tensorflow/core/platform/macros.h:87:47: note: in definition of macro 'TF_PREDICT_FALSE'
 #define TF_PREDICT_FALSE(x) (__builtin_expect(x, 0))
                                               ^
./tensorflow/core/util/tensor_format.h:441:3: note: in expansion of macro 'CHECK'
   CHECK(index >= 0 && index < dimension_attribute.size())
   ^
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 555.689s, Critical Path: 278.37s
INFO: 7384 processes: 7384 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
```

I build using the following:

```dockerfile
ENV LD_LIBRARY_PATH='/usr/local/lib:/usr/lib/x86_64-linux-gnu:/usr/local/cuda/lib64/:/usr/local/cuda/extras/CUPTI/lib64' \
    BLAS_INCLUDE='/usr/local/cuda/targets/x86_64-linux/include' \
    BLAS_LIB='/usr/local/cuda/targets/x86_64-linux/lib' \
    CPLUS_INCLUDE_PATH='/usr/local/cuda/$CPLUS_INCLUDE_PATH' \
    CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \
    GCC_HOST_COMPILER_PATH=""/usr/bin/gcc"" \
    TF_CUDA_CLANG=0 \
    TF_CUDA_COMPUTE_CAPABILITIES=5.2,6.0,6.1,7.0,7.5 \
    TF_CUDA_VERSION=10.0 \
    TF_CUDNN_VERSION=7 \
    TF_NCCL_VERSION=2 \
    TF_ENABLE_XLA=1 \
    TF_NEED_CUDA=1 \
    TF_NEED_JEMALLOC=1 \
    TF_NEED_HDFS=1 \
    TF_NEED_MPI=1 \
    TF_NEED_VERBS=0 \
    TF_NEED_OPENCL=0 \
    TF_NEED_GDR=0 \
    TF_NEED_GCP=0 \
    TF_NEED_S3=0 \
    TF_NEED_TENSORRT=0

# Get the TF branch for later build
RUN rm -rf /opt/tensorflow && \
    git clone --branch=$TF_BUILD_BRANCH --depth=1 $TF_REPO /opt/tensorflow && \
    cd /opt/tensorflow && \
    rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    echo ""/usr/local/cuda/lib64"" > /etc/ld.so.conf.d/cuda.conf && \
    echo ""/usr/local/cuda/targets/x86_64-linux/lib/stubs"" > /etc/ld.so.conf.d/cuda-stubs.conf && \
    bazel clean && \
    ldconfig && \
    tensorflow/tools/ci_build/builds/configured GPU \
    bazel build -c opt --copt=-mavx --config=cuda --verbose_failures \
        --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" \
        --action_env=LD_LIBRARY_PATH=${LD_LIBRARY_PATH} \
        tensorflow/tools/pip_package:build_pip_package && \
    mkdir /opt/tensorflow/pip_pkg && \
    bazel-bin/tensorflow/tools/pip_package/build_pip_package /opt/tensorflow/pip_pkg --gpu && \
    pip --no-cache-dir install --upgrade /opt/tensorflow/pip_pkg/tensorflow_*.whl && \
    rm -f /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    rm -rf /root/.cache && \
    rm -rf opt/tensorflow/pip_pkg
```

My script is perfectly working for the branches:  `r1.12` and `r1.13`. and fails on `master`. I guess that you have updated something, because it seems that your CI builds are still passing."
25637,c++ backend thread pool overhead,Is there a configuration to eliminate the intra and inter thread pool and always do computation in C++ client threads that call session->run().
25636,1.13.0rc1 requires numpy >=1.16.0 instead of >=1.13.3 stated in setup.py in r1.13 branch???,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 x64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.13.0rc1
- Python version: 3.7.1
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 10 / 7.4.2
- GPU model and memory: MX150 2GB

**Describe the problem**
Tensorflow 1.13.0rc1 said the numpy requires >=1.13.3 (I had np 1.15.4) in setup.py but it only works with >=1.16.0. Either the setup.py is wrong or there is something wrong while building the wheel??

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```py
import tensorflow
```

**Any other info / logs**
```py
PyBfloat16_Type.tp_base != nullptr
```"
25632,Tensorflow v1.13.0-rc1 compilation fails.  Bazel 0.19.0,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow installed from: Source. 
- TensorFlow version:  v1.13.0-rc1
- Python version: 3.6
- Installed using:  within a conda virtual environment
- Bazel version: 0.19.0 /  0.19.2 / 0.20.0 / 0.21.0  (same results)
- GCC/Compiler version: 4.8.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: RTX 2070 8,192 MB

**Describe the problem**
The compilation fails.  After running 
` bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...`
I get the following output:
`
WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
ERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:9:1: file '//tensorflow/python/tools/api/generator:api_init_files.bzl' does not contain symbol 'KERAS_API_INIT_FILES'
ERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:10:1: file '//tensorflow/python/tools/api/generator:api_init_files_v1.bzl' does not contain symbol 'KERAS_API_INIT_FILES_V1'
ERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:16:20: Traceback (most recent call last):
	File ""/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD"", line 12
		gen_api_init_files(name = ""keras_python_api_gen"", api..."", <5 more arguments>)
	File ""/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD"", line 16, in gen_api_init_files
		KERAS_API_INIT_FILES_V1
name 'KERAS_API_INIT_FILES_V1' is not defined
ERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:33:20: Traceback (most recent call last):
	File ""/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD"", line 28
		gen_api_init_files(name = ""keras_python_api_gen_com..."", <7 more arguments>)
	File ""/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD"", line 33, in gen_api_init_files
		KERAS_API_INIT_FILES_V1
name 'KERAS_API_INIT_FILES_V1' is not defined
ERROR: /home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD:50:20: Traceback (most recent call last):
	File ""/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD"", line 45
		gen_api_init_files(name = ""keras_python_api_gen_com..."", <7 more arguments>)
	File ""/home/f/GithubProyects/tensorflow/tensorflow/python/keras/api/BUILD"", line 50, in gen_api_init_files
		KERAS_API_INIT_FILES
name 'KERAS_API_INIT_FILES' is not defined (did you mean 'gen_api_init_files'?)
ERROR: package contains errors: tensorflow/python/keras/api
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/cpp/lib_cc_configure.bzl:115:5: 
Auto-Configuration Warning: 'TMP' environment variable is not set, using 'C:\Windows\Temp' as default
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/f/.cache/bazel/_bazel_f/b274795d451be3987cc86fcb20dc5b5b/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:224:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
ERROR: error loading package 'tensorflow/python/keras/api': Package 'tensorflow/python/keras/api' contains errors
INFO: Elapsed time: 3.717s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (417 packages loaded)
`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
`
  cd ~/tensorflow
 ./configure 
 bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...
`

**Additional Info**
Some weeks ago I compiled successfully tf v1.12 (and If i remember correctly also tf v1.13-rc0 with Cuda 10.0  and a previous Bazel version (0.17.2). 
This time (with v1.13-rc1) I first got an error saying that I needed Bazel >= 0.19.0  so thats why i installed that version. But now I have those Error messages..."
25629,tf.reshape function error using lambda layer in tensorflow backend keras,"Example - size of spectrum is (2240,1)
def rehape(spectrum):
    N_Sub=32
   spectrum=tf.manip.reshape(encode_complex,[(tf.size(encode_complex)/N_Sub),N_Sub])
   return spectrum
The resulting size is (2240,1) instead of (70,32)
Can anyone help me where I am going wrong?
<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**

**Describe the expected behavior**

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25625,Dependence of output on batch size,"The response from a convolutional neural network is sometimes dependent on the batch size. 

Code to reproduce 

    import tensorflow as tf
    import numpy as np

    reuse = False
    init = tf.initializers.variance_scaling()
    X = tf.placeholder(
        dtype=tf.float32,
        shape=[None] + [86]*3 + [1])

    with tf.variable_scope(""level1""):
        net1 = tf.layers.conv3d(X, 16, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init )

    with tf.variable_scope(""level2""):
        net2_in = tf.layers.conv3d(net1, 32, 2, strides=2, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='ds', kernel_initializer=init )
        net2 = tf.layers.conv3d(net2_in, 32, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init )

    with tf.variable_scope(""level3""):
        net3_in = tf.layers.conv3d(net2, 64, 2, strides=2, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='ds', kernel_initializer=init )
        net3 = tf.layers.conv3d(net3_in, 64, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init )

    with tf.variable_scope(""level4""):
        net4_in = tf.layers.conv3d(net3, 128, 2, strides=2, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='ds', kernel_initializer=init )
        net4 = tf.layers.conv3d(net4_in, 128, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)

    with tf.variable_scope(""level5""):
        net5 = tf.layers.conv3d_transpose(net4, 64, 2, strides=2, activation='elu',padding='same',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='trans', kernel_initializer=init)
        net5 = tf.concat([net5, tf.slice(net3,[0]+[(18-14)//2]*3+[0],[-1]+[14]*3+[64])], axis=4) #tf.slice(net8, begin, tf.shape(net18))
        net5 = tf.layers.conv3d(net5, 128, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)

    with tf.variable_scope(""level6""):
        net6 = tf.layers.conv3d_transpose(net5, 32, 2, strides=2, activation='elu',padding='same',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='trans', kernel_initializer=init)
        net6 = tf.concat([net6, tf.slice(net2,[0]+[(40-24)//2]*3+[0],[-1]+[24]*3+[32])], axis=4) #tf.slice(net5, begin, tf.shape(net18))
        net6 = tf.layers.conv3d(net6, 64, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)

    with tf.variable_scope(""level7""):
        net7 = tf.layers.conv3d_transpose(net6, 16, 2, strides=2, activation='elu',padding='same',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='trans', kernel_initializer=init)
        net7 = tf.concat([net7, tf.slice(net1,[0]+[(84-44)//2]*3+[0],[-1]+[44]*3+[16])], axis=4) #tf.slice(net5, begin, tf.shape(net18))
        net7 = tf.layers.conv3d(net7, 32, 3, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)

    with tf.variable_scope(""out""):
        net_out = tf.layers.conv3d(net7, 2, 1, activation='elu',padding='valid',kernel_regularizer=tf.contrib.layers.l2_regularizer(0.0001),reuse=reuse,name='conv', kernel_initializer=init)

    model = net_out

    sess = tf.Session()
    sess.run(tf.local_variables_initializer())
    sess.run(tf.global_variables_initializer())

    batch_size = 34
    full_batch = np.random.rand(154,86,86,86,1)
    num_batches = (len(full_batch)//batch_size)
    if len(full_batch) % batch_size: num_batches += 1
    y_out_list = []
    print(""Batch size of 34"")

    for i in range(0, num_batches):
        batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]
        batch1 = full_batch[2*i*batch_size//2:min(len(full_batch), (2*i+1)*batch_size//2)]
        batch2 = full_batch[(2*i+1)*batch_size//2:min(len(full_batch), (2*i+2)*batch_size//2)]
        y_out = sess.run(model, feed_dict={X:batch})
        y_out1 = sess.run(model, feed_dict={X:batch1})
        y_out2 = sess.run(model, feed_dict={X:batch2})
        print(""Should be true if independent of batch size: "", np.all(y_out==np.concatenate([y_out1,y_out2])), "" Shape: "", y_out.shape)

    batch_size = 20
    full_batch = np.random.rand(154,86,86,86,1)
    num_batches = (len(full_batch)//batch_size)
    if len(full_batch) % batch_size: num_batches += 1
    y_out_list = []
    print(""Batch size of 20"")

    for i in range(0, num_batches):
        batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]
        batch1 = full_batch[2*i*batch_size//2:min(len(full_batch), (2*i+1)*batch_size//2)]
        batch2 = full_batch[(2*i+1)*batch_size//2:min(len(full_batch), (2*i+2)*batch_size//2)]
        y_out = sess.run(model, feed_dict={X:batch})
        y_out1 = sess.run(model, feed_dict={X:batch1})
        y_out2 = sess.run(model, feed_dict={X:batch2})
        print(""Should be true if independent of batch size: "", np.all(y_out==np.concatenate([y_out1,y_out2])), "" Shape: "", y_out.shape)

    batch_size = 42
    full_batch = np.random.rand(154,86,86,86,1)
    num_batches = (len(full_batch)//batch_size)
    if len(full_batch) % batch_size: num_batches += 1
    y_out_list = []
    print(""Batch size of 42"")

    for i in range(0, num_batches):
        batch = full_batch[i*batch_size:min(len(full_batch), (i+1)*batch_size)]
        batch1 = full_batch[2*i*batch_size//2:min(len(full_batch), (2*i+1)*batch_size//2)]
        batch2 = full_batch[(2*i+1)*batch_size//2:min(len(full_batch), (2*i+2)*batch_size//2)]
        y_out = sess.run(model, feed_dict={X:batch})
        y_out1 = sess.run(model, feed_dict={X:batch1})
        y_out2 = sess.run(model, feed_dict={X:batch2})
        print(""Should be true if independent of batch size: "", np.all(y_out==np.concatenate([y_out1,y_out2])), "" Shape: "", y_out.shape)



== cat /etc/issue ===============================================
Linux  4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy (1.14.3)
numpydoc (0.7.0)
protobuf (3.5.2.post1)
tensorflow (1.8.0)

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.8.0
tf.GIT_VERSION = b'v1.8.0-3-gf91bd2f'
tf.COMPILER_VERSION = b'v1.8.0-3-gf91bd2f'
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Feb  8 18:17:16 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 970     Off  | 00000000:01:00.0  On |                  N/A |
|  0%   46C    P8    16W / 200W |    383MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 00000000:03:00.0 Off |                  N/A |
| 23%   30C    P2    58W / 250W |  11750MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a

== cat /etc/issue ===============================================
Linux  4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux
VERSION=""16.04.5 LTS (Xenial Xerus)""
VERSION_ID=""16.04""
VERSION_CODENAME=xenial

== are we in docker =============================================
No

== compiler =====================================================
c++ (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
Copyright (C) 2015 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.


== uname -a =====================================================
Linux 4.4.0-142-generic #168-Ubuntu SMP Wed Jan 16 21:00:45 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux

== check pips ===================================================
numpy               1.15.4    
protobuf            3.6.1     
tensorflow          1.9.0     
tensorflow-fold     0.0.1     
tensorflow-gpu      1.12.0    

== check for virtualenv =========================================
False

== tensorflow import ============================================
tf.VERSION = 1.12.0
tf.GIT_VERSION = v1.12.0-0-ga6d8ffae09
tf.COMPILER_VERSION = v1.12.0-0-ga6d8ffae09
Sanity check: array([1], dtype=int32)

== env ==========================================================
LD_LIBRARY_PATH :/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64
DYLD_LIBRARY_PATH is unset

== nvidia-smi ===================================================
Fri Feb  8 18:18:45 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.90                 Driver Version: 384.90                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 970     Off  | 00000000:01:00.0  On |                  N/A |
|  0%   44C    P8    16W / 200W |    383MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  TITAN Xp            Off  | 00000000:03:00.0 Off |                  N/A |
| 23%   30C    P2    58W / 250W |  11750MiB / 12189MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+

== cuda libs  ===================================================
/usr/local/cuda-9.0/doc/man/man7/libcudart.7
/usr/local/cuda-9.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-9.0/lib64/libcudart_static.a
/usr/local/cuda-9.0/lib64/libcudart.so.9.0.176
/usr/local/cuda-8.0/doc/man/man7/libcudart.7
/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart.so.8.0.61
/usr/local/cuda-8.0/targets/x86_64-linux/lib/libcudart_static.a
/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61
/usr/local/cuda-8.0/lib64/libcudart_static.a


"
25623,behaviour of tf.GPUOptions(allow_growth=False) seems different,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04.5 LTS
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 3.5.2:
- CUDA/cuDNN version: cuda-9.0, cudnn-7.3.1
- GPU model and memory: Titan Xp, Titan V

**Describe the current behavior**

In previous version, 1.10.0 wich I used, **per_process_gpu_memory_fraction** is occupied as soon as session is created.

However with this version (1.12.0), GPU memory occupied after a single **sess.run()**.


```
config = tf.ConfigProto(
        allow_soft_placement=True,
        log_device_placement=False,
        gpu_options=tf.GPUOptions(force_gpu_compatible=True,
                                  per_process_gpu_memory_fraction=0.5,
                                  allow_growth=False))

a = tf.constant(1)
sess = tf.Session(config=config)
# <- GPU memory is not fully occupied (v.1.12.0) 
#       but it was occupied right after in previous version (v.1.10.0)
sess.run(a)
# <- GPU memory is now fully occupied (v.1.12.0)
```


**Describe the expected behavior**
After session is created gpu memory should be fully occupied as specified in a variable **per_process_gpu_memory_fraction**


**Code to reproduce the issue**
shown above

"
25621,TopKV2 Op incorrectly requiring scalar input for k,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Mojave 10.14.2
- TensorFlow installed from (source or binary): binary (pip)
- TensorFlow version (use command below): ('v1.12.0-rc2-3-ga6d8ffae09', '1.12.0')
- Python version: 2.7.10


**Describe the current behavior**
I want to be able to compute the top k results for different values of k in a batched evaluation metric. (e.g. for sample 1, compute top 2, sample 2 compute top 1, etc.)
tf.nn.top_k is throwing a `ValueError` despite clearly using the `TopKV2` op under the hood.

the following toy example illustrates the problem:

```
import tensorflow as tf

y_true = tf.constant([[0., 1., 1., 0], [0., 1., 0., 0.], [0., 0., 0., 1]])  
y_pred = tf.constant([[.1, 0.4, 0.4, .1], [.1, .7, .1, .1], [.1, .1, .1, .7]])

(confidences, indices) = tf.nn.top_k(y_pred, tf.constant([2, 1, 2]))
```

I get the following error: 

`ValueError: This Input must be scalar but has rank 1 for 'TopKV2_20' (op: 'TopKV2') with input shapes: [3,4], [3] and with computed input tensors: input[1] = <2 1 2>`

Diving into the source code, it would seem that the TopKV2 op is _precisely_ intended for varying values of k, however, something at a higher level is preventing my code from utilizing this functionality. Additionally, there is a `top_kv2` function in `gen_nn_ops.py` that is not exported for use.

**Describe the expected behavior**

That the TopKV2 op functions as specified, or that the `top_kv2` python binding is exposed.


"
25620,"MirroredStrategy, dataset sharding, notion of keras epoch - inconsistent and confusing","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Both
- TensorFlow version (use command below): 1.12
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: 1080 Ti

**Background**

While I understand that in machine learning community the notion/concept of epoch may vary a bit, one of the more widely adopted definition is - an epoch is to go over all the samples of the dataset once. To some extent it is consistent with keras api as where in the fit API we provide 'steps_per_epoch' as an indicator of when an 'epoch' is to be considered finished and most of the time steps_per_epoch is computed  to be equal to the number of batches of your training dataset.

[I also appreciate that this definition is bit lose because if one is performing augmentation then above given definition of epoch starts to get bit murky]

**Setup**

I am trying to perform multi-gpu training using MirroredStrategy and Keras. My setup is - one machine with multiple gpus. In other words, not using the ClusterSpec i.e. not really ""distributed"" amongst many different machines.

**Observed Behavior**

I would expect that if I have a dataset of 100 samples and 2 GPUs, MirroredStrategy would distribute 50 samples to each GPU. An epoch will be done when both GPUs would have seen 50 samples each. Each GPU will see a portion of dataset reserved for it. In other words, they would see distinct samples.

A quick look at the API (https://www.tensorflow.org/api_docs/python/tf/contrib/distribute/MirroredStrategy) makes you believe that if you set `auto_shard_dataset` to True you may achieve above goal. However, after looking in the source code I can see that `auto_shard_dataset` is taken into consideration when a multi-worker/cluster setup is used (https://github.com/tensorflow/tensorflow/blob/a6d8ffae097d0132989ae4688d224121ec6d8f35/tensorflow/contrib/distribute/python/mirrored_strategy.py#L481).

This means that in my setup every GPUs would get 100 samples therefore the notion of 1 epoch in this case is essentially going over the dataset `twice` (but in parallel)

**Expected Behavior**

It is often interesting to evaluate the performance of a network after 1 epoch (given the more widely accepted definition/convention of epoch amongst others i.e. an epoch is to go over the dataset once). 

Given that MirroredStrategy does take the parameter `auto_shard_dataset` i.e. you (the tensorflow authors) did take notion of dividing the dataset in parts in consideration,  my question would be why it is not being used for the setup (1 machine multiple gpus) described above ? 

"
25619,Could I possibly count the detected object in tensorflow?,"here is my code 

import os
import cv2
import numpy as np
import tensorflow as tf
import sys

sys.path.append("".."")

from utils import label_map_util
from utils import visualization_utils as vis_util
from api import object_counting_api

MODEL_NAME = 'inference_graph'
IMAGE_NAME = 'tree.png'

CWD_PATH = os.getcwd()

PATH_TO_CKPT = os.path.join(CWD_PATH,MODEL_NAME,'frozen_inference_graph.pb')

PATH_TO_LABELS = os.path.join(CWD_PATH,'training','labelmap.pbtxt')

PATH_TO_IMAGE = os.path.join(CWD_PATH,IMAGE_NAME)

NUM_CLASSES = 2

label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)

detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')

    sess = tf.Session(graph=detection_graph)

image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')

detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')

detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')
detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')

num_detections = detection_graph.get_tensor_by_name('num_detections:0')

image = cv2.imread(PATH_TO_IMAGE)
image_expanded = np.expand_dims(image, axis=0)

(boxes, scores, classes, num) = sess.run(
    [detection_boxes, detection_scores, detection_classes, num_detections],
    feed_dict={image_tensor: image_expanded})

vis_util.visualize_boxes_and_labels_on_image_array(
    image,
    np.squeeze(boxes),
    np.squeeze(classes).astype(np.int32),
    np.squeeze(scores),
    category_index,
    use_normalized_coordinates=True,
    line_thickness=1,
    min_score_thresh=0.50)

cv2.imshow('Object detector', image)

cv2.waitKey(0)

cv2.destroyAllWindows()
"
25618,Docker image tensorflow/tensorflow:latest-gpu failed to use nvidia/cuda image,"Hello guys,

I am facing some problems when running image tensorflow/tensorflow:latest-gpu. When I try to execute the command ( which is found in [docs](https://www.tensorflow.org/install/docker#examples_using_gpu-enabled_images) ) :

```shell
docker run --runtime=nvidia -it --rm tensorflow/tensorflow:latest-gpu \
   python -c ""import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))""
```

I get the following error:

```
docker: Error response from daemon: OCI runtime create failed: container_linux.go:344: starting
container process caused ""process_linux.go:424: container init caused \""process_linux.go:407: running
prestart hook 1 caused \\\""error running hook: exit status 1, stdout: , stderr: exec command: 
[/usr/bin/nvidia-container-cli --load-kmods configure --ldconfig=@/sbin/ldconfig.real --device=all --
compute --utility --require=cuda>=10.0 brand=tesla,driver>=384,driver<385 --pid=7142 
/var/lib/docker/overlay2/c2f4ac7d7d905051e77682447a97f563cbdf7ccf2fe43afccdb521350adab0f4/mer
ged]\\\\nnvidia-container-cli: requirement error: unsatisfied condition: brand = tesla\\\\n\\\""\"""": unknown.

```
It may be caused by not properly installed NVidia driver, as seen [here](https://devtalk.nvidia.com/default/topic/1046289/cuda-setup-and-installation/command-quot-docker-run-runtime-nvidia-rm-nvidia-cuda-9-0-base-nvidia-smi-quot-fails-with-error-/).
But turns out that when I try to run the following command ( also seen in [docs](https://www.tensorflow.org/install/docker#gpu_support) ):

```shell
docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi
```

It gives me the same error. The above command only runs correctly when I run it with the nvidia/cuda:9.0-base image, specified in NVidia [github README](https://github.com/NVIDIA/nvidia-docker#ubuntu-140416041804-debian-jessiestretch):

```shell
docker run --runtime=nvidia --rm nvidia/cuda:9.0-base nvidia-smi
```

I think it is a problem with tensorflow:lastest-gpu, which may be linking its image with nvidia/cuda:latest  instead of nvidia/cuda:9.0-base image.
Is anyone else facing the same issue?

My configuration:
Ubuntu 18.04.1
GeForce 930M
Driver nvidia-driver-390 version 390.77, installed via Softwares and Updates.

## EDIT
TF version: Latest docker image tensorflow/tensorflow:latest-gpu which following the [docs](https://www.tensorflow.org/install/docker) does not need CUDA installed, just the driver.

 "
25616,How to get the minimum and maximum cuda compute capability for the given version of TensorFlow.,"Hello, is any way to get the supported versions of compute capablity. 
An exemplary function could look like this:
`>>>import tensorflow as tf`
`>>>tf.get_supported_compute_capablity()`
`(3.0, 7.0)`

**System information**
- TensorFlow version 1.10:
- Are you willing to contribute it (Yes/No): Yes

"
25614,Segmentation fault - Convolution in tflite,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution : Linux Ubuntu 16.04
- TensorFlow installed from github:
- TensorFlow version tf 1.13:
- Python version: 3.6
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA : 9.0
- RAM : 32GB

Hi,

I encountered a segmentation fault in the Im2Col of Convolution while running a tflite model.
I observed that when the Im2Col buffer size >  2GB, this crash occurs. 
For example : 
input : 1x828x1000x32 ; output: 1x828x1000x3  ;kernel: 9x9  does not crash

but for 
input:1x832x100x32 ; output: 1x832x1000x3; kernel 9x9  convolution crashes.


I want to know if there is any memory limit for convolution in tflite to run ?

I kindly request you to let me know this. sooner the better


Awaiting you reply,
Vedavyas.

"
25611,Docker images with tags `1.13.0rc0` and `1.13.0rc0-py3` contains wrong version of TensorFlow,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux Ubuntu 17.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **-**
- TensorFlow installed from (source or binary): **Binary/Docker**
- TensorFlow version: **???**
- Python version: **2.7**, **3.5**
- Installed using virtualenv? pip? conda?: **Docker**
- Bazel version (if compiling from source): **-**
- GCC/Compiler version (if compiling from source): **-**
- CUDA/cuDNN version: **-**
- GPU model and memory: **-**

The problem is that Docker [images](https://hub.docker.com/r/tensorflow/tensorflow/tags) with tags `1.13.0rc0` and `1.13.0rc0-py3` contains wrong version of `tensorflow`.

Just to show:
```
$ docker run -it tensorflow/tensorflow:1.13.0rc0 bash

________                               _______________                
___  __/__________________________________  ____/__  /________      __
__  /  _  _ \_  __ \_  ___/  __ \_  ___/_  /_   __  /_  __ \_ | /| / /
_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ / 
/_/    \___//_/ /_//____/ \____//_/    /_/      /_/  \____/____/|__/

root@d1f1dd416bdd:/# pip show tensorflow
DEPRECATION: Python 2.7 will reach the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 won't be maintained after that date. A future version of pip will drop support for Python 2.7.
Name: tensorflow
Version: 1.12.0
```"
25610,Make of benchmark tool,Is there a make to build benchmark tool instead of bazel?
25609,tf.nn.conv2d_transpose different behaviour if output_shape is a list,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
tf version 1.12. It probably relates to other versions too. System information doesn't matter.

**Describe the current behavior**
lines 1235, 1326, 1459, 2535 of nn_ops.py includes the following error check:
if isinstance(output_shape, (list, np.ndarray)):

However, if output_shape is a tuple instead of a list, then the error check is skipped.

**Describe the expected behavior**
Behaviour should be the same for lists or tuples. Fix is easy:

if isinstance(output_shape, (list, tuple, np.ndarray)):

"
25608,Building tensorflow from Source for arm64,"Hi All,

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.12.0
- Bazel installed version : 0.21.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): gcc 7.3 
- CUDA/cuDNN version: N/A
- GPU model and memory: N/a




I'm trying to build tensorflow from source on arm64 (aarch64) but got stucked at the following:

INFO: From Compiling tensorflow/core/kernels/diag_op.cc [for host]:
tensorflow/core/kernels/diag_op.cc:118:1: warning: multi-line comment [-Wcomment]
 // `new_index = i1*(s2*...sk*s1*...*sk) + i2*(s3*...*sk*s1*...*sk) +... + \
 ^
**ERROR: /home/rock64/tensorflow/tensorflow/core/kernels/BUILD:2187:1: C++ compilation of rule '//tensorflow/core/kernels:resource_variable_ops' failed (Exit 4)**
gcc: internal compiler error: Killed (program cc1plus)
Please submit a full bug report,
with preprocessed source if appropriate.
See <file:///usr/share/doc/gcc-7/README.Bugs> for instructions.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 3734.152s, Critical Path: 279.33s
INFO: 4043 processes: 4043 local.
FAILED: Build did NOT complete successfully

Bazel installed version : 0.21.0
Tensorflow : 1.12.0
installed gcc-aarch64-linux-gnu & g++-aarch64-linux-gnu





"
25607,Error while Building tensorflow using cross-tool for aarch64 ,"- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): - Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source trying to build and install for aarch64 (arm64)
- TensorFlow version:  1.12.0 (trying to install)
- Python version: 3.6
- Installed using virtualenv? pip? conda?: NO - virtualenv or pip
- Bazel version (if compiling from source): 0.19.0 (tried with new version same issue)
- GCC/Compiler version (if compiling from source): - gcc-aarch64-linux-gnu & g++-arch64-linux-gnu
- CUDA/cuDNN version: No CUDA or XcuDNN
- GPU model and memory:  NO GPU used 

Installed bazel - 0.19.0 version
Tensorflow source latest from git
Building in Ubuntu for arm64 bit(aarch64) cpu using cross-tool. 

Command for building:
bazel build -c opt //tensorflow/examples/label_image --cpu=aarch64 -- crosstool_top=//tools/aarch64_compiler:toolchain --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --verbose_failures

And here is the error logs:
****ERROR**: /home/cmp/.cache/bazel/_bazel_root/b7eb2acccd1b56cb3e56479bb5f07f48/external/mkl_dnn/BUILD.bazel:71:1: C++ compilation of rule '@mkl_dnn//:mkldnn_single_threaded' failed (Exit 1): aarch64-linux-gnu-gcc failed: error executing command** 
  (cd /home/cmp/.cache/bazel/_bazel_root/b7eb2acccd1b56cb3e56479bb5f07f48/execroot/org_tensorflow && \
  exec env - \
    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3.6 \
    PYTHON_LIB_PATH=/usr/lib/python3.6 \
    TF_DOWNLOAD_CLANG=0 \
    TF_NEED_CUDA=0 \
    TF_NEED_OPENCL_SYCL=0 \
    TF_NEED_ROCM=0 \
  /usr/bin/aarch64-linux-gnu-gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections '-std=c++11' -MD -MF bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.o' -iquote external/mkl_dnn -iquote bazel-out/aarch64-opt/genfiles/external/mkl_dnn -iquote bazel-out/aarch64-opt/bin/external/mkl_dnn -iquote external/bazel_tools -iquote bazel-out/aarch64-opt/genfiles/external/bazel_tools -iquote bazel-out/aarch64-opt/bin/external/bazel_tools -isystem external/mkl_dnn/include -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/include -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/include -isystem external/mkl_dnn/src -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src -isystem external/mkl_dnn/src/common -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/common -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/common -isystem external/mkl_dnn/src/cpu -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu -isystem external/mkl_dnn/src/cpu/gemm -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu/gemm -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu/gemm -isystem external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/aarch64-opt/genfiles/external/mkl_dnn/src/cpu/xbyak -isystem bazel-out/aarch64-opt/bin/external/mkl_dnn/src/cpu/xbyak -fexceptions '-DMKLDNN_THR=MKLDNN_THR_SEQ' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp -o bazel-out/aarch64-opt/bin/external/mkl_dnn/_objs/mkldnn_single_threaded/jit_avx512_core_x8s8s32x_conv_kernel.o)
In file included from external/mkl_dnn/src/cpu/cpu_isa_traits.hpp:35:0,
                 from external/mkl_dnn/src/cpu/jit_generator.hpp:21,
                 from external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.hpp:23,
                 from external/mkl_dnn/src/cpu/jit_avx512_core_x8s8s32x_conv_kernel.cpp:23:
external/mkl_dnn/src/cpu/xbyak/xbyak_util.h:84:21: **fatal error: cpuid.h: No such file or directory**
compilation terminated.
Target //tensorflow/examples/label_image:label_image failed to build
INFO: Elapsed time: 49.893s, Critical Path: 10.85s, Remote (0.00% of the time): [queue: 0.00%, setup: 0.00%, process: 0.00%]
INFO: 194 processes: 194 local.
FAILED: Build did NOT complete successfully

"
25606,~40% slow down since 1.13.0.dev20190202,"**System information**
- Have I written custom code: yes
- OS Platform and Distribution: public Colab instance and Debian
- TensorFlow installed from (source or binary): binary
- TensorFlow version: >= 1.13.0.dev20190202
- Python version: 3.6.7
- CUDA/cuDNN version: 10
- GPU model and memory: K80 (originally occurred on T4 and V100)

**Describe the current behavior**
Performance on some simple looped graphs has worsen by ~40% in between 1.13.0.dev20190201 and 1.13.0.dev20190202 and still exists in the current nightly build (see provided colab). This slow down was consistent across VMs and GPUs when running my research code. The provided code is my best attempt at boiling it down.

Here are the results from the colab benchmark copied for convenience. The `num_batched_iter` partially unrolls the `tf.while_loop` body for that many iterations to reduce its overhead and leverage other graph optimizations. Before the first timed `sess.run(...)` call, 5 warm-up call are made. Results reported aggregate 40 different runs. The colab runtimes were completely reset before benchmarking each version. 

On the newest tf-nightly-gpu build, I got:
```
1.13.0-dev20190207
Benchmarking simple_loop_case...
name                                                                         mean        min        max     q=0.05     q=0.95
----------------------------------------------------------------------  ---------  ---------  ---------  ---------  ---------
simple loop, num_batched_iter=1, back_prop=False, test_converge=True    0.526553   0.512475   0.551661   0.51285    0.543843
simple loop, num_batched_iter=1, back_prop=False, test_converge=False   1.18145    1.15179    1.2054     1.16411    1.1976
simple loop, num_batched_iter=20, back_prop=False, test_converge=True   0.0664629  0.0633538  0.0774522  0.0636203  0.0729571
simple loop, num_batched_iter=20, back_prop=False, test_converge=False  0.249716   0.238131   0.261069   0.241182   0.259239
```
Here are the results on the version just before the issue appears:
```
1.13.0-dev20190201
Benchmarking simple_loop_case...
name                                                                         mean        min        max     q=0.05     q=0.95
----------------------------------------------------------------------  ---------  ---------  ---------  ---------  ---------
simple loop, num_batched_iter=1, back_prop=False, test_converge=True    0.519208   0.483462   0.570542   0.487508   0.569501
simple loop, num_batched_iter=1, back_prop=False, test_converge=False   1.21199    1.18047    1.24812    1.18563    1.23686
simple loop, num_batched_iter=20, back_prop=False, test_converge=True   0.0483549  0.0455515  0.0589228  0.0463824  0.0505414
simple loop, num_batched_iter=20, back_prop=False, test_converge=False  0.175403   0.168438   0.192948   0.169144   0.18713
```

**Code to reproduce the issue**
https://gist.github.com/gehring/4e7bd9b6f0c5d73c545777236fc507d0

"
25601,Batchnorm does not work in Eager mode in TF 1.12: InternalError: Could not find valid device for node,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux based on Dockerfile tensorflow/tensorflow:1.12.0-devel-gpu-py3, that is Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
No
- TensorFlow installed from (source or binary):
Dockerfile tensorflow/tensorflow:1.12.0-devel-gpu-py3
- TensorFlow version (use command below):
1.12.0
- Python version:
Python 3.5.2
- Bazel version (if compiling from source):
No
- GCC/Compiler version (if compiling from source):
No
- CUDA/cuDNN version:
== cuda libs  ===================================================
/usr/local/lib/python3.5/dist-packages/torch/lib/libcudart-f7fdd8d7.so.9.0
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart.so.9.0.176
/usr/local/cuda-9.0/targets/x86_64-linux/lib/libcudart_static.a

- GPU model and memory:
GeForce GTX TITAN X 11.93GiB

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
 File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 3496, in _fused_batch_norm
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.
Node: {{node FusedBatchNorm}} = FusedBatchNorm[T=DT_DOUBLE, data_format=""NHWC"", epsilon=0.001, is_training=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)
All kernels registered for op FusedBatchNorm :
  device='XLA_GPU'; T in [DT_FLOAT]
  device='XLA_CPU'; T in [DT_FLOAT]
  device='XLA_CPU_JIT'; T in [DT_FLOAT]
  device='XLA_GPU_JIT'; T in [DT_FLOAT]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_FLOAT]
 [Op:FusedBatchNorm]
$ 

**Describe the expected behavior**
I expect Tensorflow to find that my GPU (or at least my CPU) is a valid device to execute a Batchnorm operation on. It is not.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import BatchNormalization
tf.enable_eager_execution()
x = np.random.rand(2,416,416,3) * 255
x = tf.convert_to_tensor(x)
bn = BatchNormalization()
x = bn(x)


**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
Tensorflow version 1.12.0
Keras version 2.1.6-tf
2019-02-07 22:59:14.551505: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2019-02-07 22:59:14.678286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:05:00.0
totalMemory: 11.92GiB freeMemory: 7.08GiB
2019-02-07 22:59:14.781607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 1 with properties: 
name: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate(GHz): 1.2155
pciBusID: 0000:09:00.0
totalMemory: 11.93GiB freeMemory: 11.81GiB
2019-02-07 22:59:14.782011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0, 1
2019-02-07 22:59:15.310401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-07 22:59:15.310438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 1 
2019-02-07 22:59:15.310450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N Y 
2019-02-07 22:59:15.310457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 1:   Y N 
2019-02-07 22:59:15.311331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6821 MB memory) -> physical GPU (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:05:00.0, compute capability: 5.2)
2019-02-07 22:59:15.311691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 11427 MB memory) -> physical GPU (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:09:00.0, compute capability: 5.2)
Traceback (most recent call last):
  File ""/home/niclasdn/.vscode/extensions/ms-python.python-2019.1.0/pythonFiles/ptvsd_launcher.py"", line 45, in <module>
    main(ptvsdArgs)
  File ""/home/niclasdn/.vscode/extensions/ms-python.python-2019.1.0/pythonFiles/lib/python/ptvsd/__main__.py"", line 348, in main
    run()
  File ""/home/niclasdn/.vscode/extensions/ms-python.python-2019.1.0/pythonFiles/lib/python/ptvsd/__main__.py"", line 253, in run_file
    runpy.run_path(target, run_name='__main__')
  File ""/usr/lib/python3.5/runpy.py"", line 254, in run_path
    pkg_name=pkg_name, script_name=fname)
  File ""/usr/lib/python3.5/runpy.py"", line 96, in _run_module_code
    mod_name, mod_spec, pkg_name, script_name)
  File ""/usr/lib/python3.5/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/niclasdn/development/yolov3_pytorch/run_yolov3_minitest.py"", line 70, in <module>
    x = bn(x)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 757, in __call__
    outputs = self.call(inputs, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 514, in call
    outputs = self._fused_batch_norm(inputs, training=training)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 401, in _fused_batch_norm
    training, _fused_batch_norm_training, _fused_batch_norm_inference)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/utils/tf_utils.py"", line 52, in smart_cond
    pred, true_fn=true_fn, false_fn=false_fn, name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/smart_cond.py"", line 56, in smart_cond
    return false_fn()
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/normalization.py"", line 398, in _fused_batch_norm_inference
    data_format=self._data_format)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py"", line 909, in fused_batch_norm
    name=name)
  File ""/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py"", line 3496, in _fused_batch_norm
    _six.raise_from(_core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InternalError: Could not find valid device for node.
Node: {{node FusedBatchNorm}} = FusedBatchNorm[T=DT_DOUBLE, data_format=""NHWC"", epsilon=0.001, is_training=false](dummy_input, dummy_input, dummy_input, dummy_input, dummy_input)
All kernels registered for op FusedBatchNorm :
  device='XLA_GPU'; T in [DT_FLOAT]
  device='XLA_CPU'; T in [DT_FLOAT]
  device='XLA_CPU_JIT'; T in [DT_FLOAT]
  device='XLA_GPU_JIT'; T in [DT_FLOAT]
  device='CPU'; T in [DT_FLOAT]
  device='GPU'; T in [DT_FLOAT]
 [Op:FusedBatchNorm]
$ "
25598,tf_upgrade_v2 upgrade script should do absl-py conversions,"Currently the tf_upgrade_v2 upgrade script changes uses of tf.app, tf.flags & tf.logging to tf.compat.v1.*. It seems that is would be more elegant for us to simply add the necessary absl-py imports and modify the code to use those instead."
25597,Dll exception,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
XPS 15 9570 specs --
Processor : Intel(r) Core(Tm) i7-8750H CPU @2.20GHz 2.21GHz
Ram : 16 Gb
System type : 64 bit, x64 based processor
Windows 10 Home single Language  --
version = 1803
OS Build 17134.523

- TensorFlow installed from (source or binary): 
Used pip install
- TensorFlow version:  
tensorflow-gpu = 1.12.0
- Python version: python 64bit 3.6.8
- Installed using virtualenv? pip? conda?: 
using pip
- CUDA/cuDNN version: 
Tried CUDA toolkit 9.2 and 10.0 with similar error. 
cudnn - 7.4.2 (Dec  14)

- GPU model and memory:
Nvidia : NVIDIA GeForce GTX 1050 Ti with Max-Q Design
Mem : 4GB GDDR5
Driver Version - 24.21.13.9793


**Describe the problem**
When I import tensorflow it gives me error:
ImportError: DLL load failed: The specified module could not be found.
Failed to load the native TensorFlow runtime.
**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the path as shown [here](https://www.youtube.com/watch?v=HExRhnO5Mqs).
1. Uninstalled python
2. First I downloaded visual studio community version 17.
3. Then I downloaded and installed Cuda toolkit 9.2
4. Then downloaded cudnn 7.4.2. Added the path of folders bin,include,lib and all internal folder of path variable.
5. Installed python
6: Run command 'pip install --ignore-installed --upgrade tensorflow-gpu'
7. Ran python on command prompt and Imported tensorflow



**Any other info / logs**
Complete tracebook:
Python 3.6.8 (tags/v3.6.8:3c6b436a57, Dec 24 2018, 00:16:47) [MSC v.1916 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow
Traceback (most recent call last):
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\__init__.py"", line 24, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\__init__.py"", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 74, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""C:\Users\Aniket\AppData\Local\Programs\Python\Python36\lib\imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: DLL load failed: The specified module could not be found.


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
>>>




Tried solution:
[This solution](https://stackoverflow.com/questions/42011070/on-windows-running-import-tensorflow-generates-no-module-named-pywrap-tenso)"
25594,Test sharding appears to be broken,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v1.8.0-17025-g3e713f9 1.12.0
- Python version: 3.5
- Bazel version (if compiling from source): 0.19.2
- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

I'm currently testing the AVX512 builds.  There are some tests that are known to fail on these builds.  Patches are pending for these issues (#21676) but they are not yet merged.  The weird thing is that the tests have been passing on my machine for the last two weeks even though they should fail.  I did some digging and it turned out that the tests only pass if sharding is enabled.  They fail as expected if sharding is disabled.  So if, on an AVX512 build,  I do 

bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test

the test passes

and if I do

 bazel test --test_sharding_strategy=disabled --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test

the test fails, as it should.  It turns out that the test is passing when sharding is enabled as not all of the sub-tests are being run.  This issue seems to have been caused by commit #87cc788 which changed the unit test framework.  As far as I can tell, filtering for sharding is now happening twice, once in googletest.py and once in absltest.

The issue can be reproduced on my machine, a 10 core SKX core i9 as follows

Here's how to reproduce the issue

```
$ bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test
```

Then check shard 18.  I see

```
$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/python/kernel_tests:embedding_ops_test
-----------------------------------------------------------------------------
Running tests under Python 3.5.2: /usr/bin/python3
----------------------------------------------------------------------
Ran 0 tests in 0.000s

OK
```
Note 0 tests run.

**Describe the expected behavior**
And what I'm expecting to see is 

```
$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log
exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/python/kernel_tests:embedding_ops_test
-----------------------------------------------------------------------------
Running tests under Python 3.5.2: /usr/bin/python3
2019-02-06 08:57:56.395259: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz
2019-02-06 08:57:56.396001: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x29fe960 executing computations on platform Host. Devices:
2019-02-06 08:57:56.396024: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
[ RUN      ] EmbeddingLookupTest.testMaxNorm
[       OK ] EmbeddingLookupTest.testMaxNorm
[ RUN      ] SafeEmbeddingLookupSparseTest.test_safe_embedding_lookup_sparse_3d_return_special_vector
W0206 08:57:56.425108 140697567004416 deprecation.py:506] From /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/kernel_tests/embedding_ops_test.py:787: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
W0206 08:57:56.500279 140697567004416 deprecation.py:323] From /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/kernel_tests/embedding_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/embedding_ops.py:527: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Deprecated in favor of operator or tf.math.divide.
[       OK ] SafeEmbeddingLookupSparseTest.test_safe_embedding_lookup_sparse_3d_return_special_vector
----------------------------------------------------------------------
Ran 2 tests in 0.151s

OK
```

Here I've chosen a shard with a test that passes on my machine but you can hopefully see the difference.

**Code to reproduce the issue**

On a 10 core machine

$ bazel test --config=opt --cache_test_results=no -- //tensorflow/python/kernel_tests:embedding_ops_test
$ cat /home/user/.cache/bazel/_bazel_user/59a925eeb655b30b5c683f8317fe569e/execroot/org_tensorflow/bazel-out/k8-opt/testlogs/tensorflow/python/kernel_tests/embedding_ops_test/shard_18_of_20/test.log

**Other info / logs**

I have a patch that I will post shortly.  It may not be the correct fix, but it should point to the part of the code where I believe the problem to be."
25593,"Python 3.7 (nightly) TF 1.12, GLIBC 2.23 import failure on Centos 7.5","Using Centos 7.5 which has glibc2.17, tensorflow nightly build fails on import
Since updating glibc on Centos appears is not advised, is their a hard requirement for GLIBC 2.23?
Thank you.

ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /python3.7/site                                                                          -packages/tensorflow/python/_pywrap_tensorflow_internal.so)

"
25592,tf.contrib.summary.image() fails silently and inconsistently,"**System information**

* Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  No
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  Ubuntu 18.04
* Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
  No
* TensorFlow installed from (source or binary):
  Source
* TensorFlow version (use command below):
  b'v1.12.0-5845-g764109a352' 1.12.0
* Python version:
  3.6.7
* Bazel version (if compiling from source):
  Invocation ID: 42251854-036f-415c-8a52-76aac8520ea0
  Build label: 0.21.0
  Build time: Wed Dec 19 12:58:44 2018 (1545224324)
* GCC/Compiler version (if compiling from source):
  gcc (Ubuntu 7.3.0-27ubuntu1~18.04) 7.3.0
* CUDA/cuDNN version:
  Cuda compilation tools, release 10.0, V10.0.130
* GPU model and memory:
  GTX 1060 Max Q, 6gb VRAM

**Describe the current behavior**
I am running into a very weird problem where sometimes images will be logged in tensorboard / the event file, but most of the time they are not. 

The issue with this is that I can't get consistent behaviour. One time I will run it and it will log normally, and then without changing any code, the next time it won't. And it is really hard to find out anything when `tensorboard --inspect` shows nothing under -image either.

I should mention that the input data is not always the same, although it is always taken from the same state space (2D game environment with finite number of possible states).

**Describe the expected behavior**
If it was not able to log my images, the function should at the very least provide some information to the user as to why it was not able to do so. 

**Code to reproduce the issue**
This is not possible, as I can not consistently reproduce it myself. It is seemingly completely random, and provides no log, error or warning information.

**Other info / logs**
As stated above, there are no logs of any sort."
25591,Bug in tf.image.resize_images,"### System information

I am running Ubuntu 18.0.4 on my PC with CUDA-10 and Python 3.6 and Pillow 5.4.1
```
libcudnn7:
  Installed: 7.4.1.5-1+cuda10.0
  Candidate: 7.4.2.24-1+cuda10.0
```
with Tensorflow version `b'v1.12.0-0-ga6d8ffae09' 1.12.0`


### Describe the problem
I am trying to resize an image with `tf.image.resize_images` using all available methods but I am getting completely crappy results for anything except NEAREST_NEIGHBOR

For example simply loading this image from the test set of Imagenet
![ilsvrc2012_test_00000002](https://user-images.githubusercontent.com/4192637/52434595-fff10480-2b0f-11e9-8e2c-6358b81760fe.JPEG)

I get colorful pixel salad if I put it through `resize_bilinear`
see code and output below.

### Source code / logs
To reproduce place the code and the image file above in the same folder or adjust the file-path accordingly:
```
import tensorflow as tf
from PIL import Image
import time
import numpy as np

file_read = tf.read_file('ILSVRC2012_test_00000002.JPEG')
loaded_image = tf.image.decode_jpeg(file_read, channels=3)
reshape_image = tf.expand_dims(loaded_image, 0)
resized_image = tf.image.resize_bilinear(reshape_image, size=[510, 520], align_corners=False)
resized_image_v2 = tf.image.resize_images(reshape_image, size=[300, 320],
                                          method=tf.image.ResizeMethod.BILINEAR, align_corners=False)
# resized_image_v2 = tf.image.resize_images(reshape_image, size=[300, 320],
                                          method=tf.image.ResizeMethod.NEAREST_NEIGHBOR, align_corners=True)
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    orig_image = sess.run(loaded_image)
    resized_v1 = sess.run(resized_image)
    resized_v2 = sess.run(resized_image_v2)
    orig_img = Image.fromarray(orig_image, 'RGB')
    resized_v1_img = Image.fromarray(np.squeeze(resized_v1), 'RGB')
    resized_v2_img = Image.fromarray(np.squeeze(resized_v2), 'RGB')
    orig_img.show()
    resized_v1_img.show()
    resized_v2_img.show()
    time.sleep(10)
    orig_img.close()
    resized_v1_img.close()
    resized_v2_img.close()
```
This produces for the original image:
![original_image](https://user-images.githubusercontent.com/4192637/52435280-ba353b80-2b11-11e9-8a38-494195b5f3a4.jpg)

for the larger resize: 
![resize_large](https://user-images.githubusercontent.com/4192637/52435296-c4573a00-2b11-11e9-9c4b-cad0f1a4ea2d.jpg)

for the smaller resize:
![resize_small](https://user-images.githubusercontent.com/4192637/52435319-d1742900-2b11-11e9-80b2-9e4261e5ab26.jpg)

I get similar crappy results if I set `align_corners=True`

However if I use `method=tf.image.ResizeMethod.NEAREST_NEIGHBOR` (i.e. the commented out line), I get a descent result
![resize_small2](https://user-images.githubusercontent.com/4192637/52435620-9a524780-2b12-11e9-9140-4ff7a168a595.jpg)

With `BICUBIC` and `AREA` the results are similar pixel garbage as with `BILINEAR`

Am I missing something here? This seems to be seriously broken.


"
25590,Build Tensorflow version that detects CPU instruction set at runtime and lights-up/down,"**System information**
- TensorFlow version (you are using): 1.12.0 CPU

**Describe the feature and the current behavior/state.**
Currently Tensorflow cross-compiles for different instruction sets and will warn if the CPU supports instructions that the TF build does not use, and fail to load if the CPU does not support an instruction set that the TF build uses.  This makes it impossible to build an application that runs on a variety of hardware and ensure that it achieves optimal results for that hardware (or even runs at all).

I understand that TF supports cross-compilation and developers can build their own library that works best for their hardware, but this doesn't solve the case where an application developer wants to ship an application that uses TF and runs on a variety of hardware.

I understand that having multiple codepaths with runtime light-up could increase the size of TF, that could be dealt with by making this a separate flavor/configuration of TF, eg: ""portable"" build, and that could be published as a binary zip/tarball along side the current builds.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
Applications and libraries redistributing tensor flow binaries to run on a variety of hardware.

"
25588,"Object Detection: Build for TF Lite Demo completes successfully, but custom model fails with operations error","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 29 x86_64
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Moto Z3 Play
- TensorFlow installed from (source or binary): Source
- TensorFlow version: v1.12.0 or v1.13.0-rc0
- Python version: 3.6 latest revision
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source): 0.21
- GCC/Compiler version (if compiling from source): gcc (GCC) 8.2.1 20181215 (Red Hat 8.2.1-6)
- CUDA/cuDNN version: none, TPU used for training
- GPU model and memory: none, TPU used for training

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. Model built using TPU training command listed in documentation for running on the cloud

2. Model downloaded from Google Cloud storage bucket

3. Exported model using:
```bash
python -m object_detection/export_tflite_ssd_graph \
--pipeline_config_path=$CONFIG_FILE \
--trained_checkpoint_prefix=$CHECKPOINT_PATH \
--output_directory=$OUTPUT_DIR \
--add_postprocessing_op=true
```

4. Converted model using TOCO:
```bash
bazel run -c opt //tensorflow/contrib/lite/toco:toco \
--incompatible_package_name_is_a_function=false \
-- \
--input_file=$OUTPUT_DIR/tflite_graph.pb \
--output_file=$OUTPUT_DIR/detect.tflite \
--input_shapes=1,640,640,3 \
--input_arrays=normalized_input_image_tensor \
--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \
--inference_type=FLOAT \
--allow_custom_ops
```

5. Edited the Bazel BUILD file to include exported model, and edited DetectionActivity to include custom model and change to non-quantized ops.

6. Built application using:
```bash
bazel build -c opt --config=android_arm64 --cxxopt='--std=c++11' ""//tensorflow/contrib/lite/examples/android:tflite_demo""
```

7. Ran demo with Android Studio APK profiler, and observed the following error message using Logcat: (snipped for clarity)
```
Cannot create interpreter: Didn't find custom op for name 'ResizeNearestNeighbor' with version 1
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Here are all of the files that I have present:

* Training Configuration: https://storage.cloud.google.com/robocubs-ml/debug/config/tpu.config
* Checkpoint Files
    * Checkpoint Descriptor: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/checkpoint
    * Graph: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/graph.pbtxt
    * Checkpoint Data: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/model.ckpt-246400.data-00000-of-00001
    * Checkpoint Index: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/model.ckpt-246400.index
    * Checkpoint Meta: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/model.ckpt-246400.meta
     * Pipeline Config: https://storage.cloud.google.com/robocubs-ml/debug/checkpoint/pipeline.config
* Saved Model
     * Checkpoint Descriptor: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/checkpoint
     * Frozen Inference Graph: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/checkpoint
     * Checkpoint Data: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/model.ckpt.data-00000-of-00001
     * Checkpoint Index: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/model.ckpt.index
     * Checkpoint Meta: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/model.ckpt.meta
     * Pipeline Config: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/pipeline.config
     * Saved Model: https://storage.cloud.google.com/robocubs-ml/debug/exported_model/saved_model/saved_model.pb
* TensorFlow Lite Files
     * TFLite Graph: https://storage.cloud.google.com/robocubs-ml/debug/tflite/tflite_graph.pb
     * TFLite Graph (PBTXT): https://storage.cloud.google.com/robocubs-ml/debug/tflite/tflite_graph.pbtxt
     * Final TFLite Model: https://storage.cloud.google.com/robocubs-ml/debug/tflite/detect.tflite"
25587,Could you please include small examples with the functions embedded in real code with the documentation?,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.0 preview
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
Could you please include small examples with the functions embedded in real code with the documentation?

**Will this change the current api? How?**
Make the usage more elucidate.

**Who will benefit with this feature?**
All the new and intermediate users.

**Any Other info.**
For example, you can see this documentation API doc: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/audio/decode_wav

This is the function call, but there isn't any boilerplate code to show the sequence of its usage.  This is just one example.  All the function call should be similar."
25585,Inconsistent results between tf1 and tf2-preview with Keras,"I'm just trying to migrate a simple code from `keras 2.2.4` towards `tensorflow 2.0.0-preview`.

Let's first consider the following input :

```python
import numpy as np

embedding_size = 4
vocab_size = 10

embedding_matrix = np.arange(embedding_size * vocab_size, dtype='float32')
embedding_matrix = embedding_matrix.reshape(vocab_size, embedding_size)
print(embedding_matrix)
# [[ 0.  1.  2.  3.]
#  [ 4.  5.  6.  7.]
#  [ 8.  9. 10. 11.]
#  [12. 13. 14. 15.]
#  [16. 17. 18. 19.]
#  [20. 21. 22. 23.]
#  [24. 25. 26. 27.]
#  [28. 29. 30. 31.]
#  [32. 33. 34. 35.]
#  [36. 37. 38. 39.]]
````

And the network with just an input layer and an embedding output layer:

```python
x = Input(shape=[1], name='input')
embedding = embedding_layer(x)
model = Model(inputs=x, outputs=embedding)
```

So mainly the only differences between both codes should be with the imports:

```python
from keras.layers import Embedding, Input
from keras.models import Model
````

is changed to:

```python
from tensorflow import keras
from tensorflow.keras.layers import Embedding, Input
from tensorflow.keras.models import Model
```

Nothing special about that.

Now I'm trying to project a single integer label into the matching embedding vector:

```python
labels_to_encode = np.array([[3]])
model.predict(labels_to_encode)
```

In the first case, with `tensorflow 1.9.0`, I get the desired and *correct* output : 

```python
array([[[12., 13., 14., 15.]]], dtype=float32)
```

But with `tensorflow 2.0.0-preview`, the weights are completely different and the output isn't correct:

```python
array([[[ 0.00646725,  0.00275606, -0.0351016 ,  0.01088192]]],
      dtype=float32)
```

Any explanation for this ? Is this an expect behaviour ? Am I missing something or it's a possible bug ? "
25582,Load multiple TensorRT graphs. Cannot add function 'TRTEngineOp_0_native_segment' error,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12
- Python version: 3.6.3
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.2.0
- CUDA/cuDNN version: 9.0
- GPU model and memory: 1080Ti


**Describe the current behavior**
I have optimized frozen graphs with tensorrt using example code from here: https://github.com/NVIDIA-AI-IOT/tf_trt_models
Everything works fine. Inference works faster. 

In my project, I have multiple models. And when I try to load both optimized graphs, following error occurs: **ValueError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.** Separately models work fine though. Even if pipeline includes only one optimized and others are non-optimized - it also works fine.

Both models are ssd_inception_v2.

**Other info / logs**
```
2019-02-07 10:56:31.606178: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 3300000000 Hz
2019-02-07 10:56:31.607938: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55ed1968ea70 executing computations on platform Host. Devices:
2019-02-07 10:56:31.607999: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): <undefined>, <undefined>
2019-02-07 10:56:31.612806: I tensorflow/stream_executor/platform/default/dso_loader.cc:154] successfully opened CUDA library libcuda.so.1 locally
2019-02-07 10:56:32.144695: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x55ed1973dda0 executing computations on platform CUDA. Devices:
2019-02-07 10:56:32.144755: I tensorflow/compiler/xla/service/service.cc:169]   StreamExecutor device (0): GeForce GTX 1080 Ti, Compute Capability 6.1
2019-02-07 10:56:32.145360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1434] Found device 0 with properties: 
name: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.582
pciBusID: 0000:17:00.0
totalMemory: 10.92GiB freeMemory: 10.76GiB
2019-02-07 10:56:32.145396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0
2019-02-07 10:56:32.257785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-07 10:56:32.257824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 
2019-02-07 10:56:32.257833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N 
2019-02-07 10:56:32.258003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10445 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
/root/Recognizer2/weights/ssd_inception_v2_transport_feimani/inference/frozen_inference_graph.pb
2019-02-07 10:56:32.870341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1513] Adding visible gpu devices: 0
2019-02-07 10:56:32.870395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] Device interconnect StreamExecutor with strength 1 edge matrix:
2019-02-07 10:56:32.870402: I tensorflow/core/common_runtime/gpu/gpu_device.cc:991]      0 
2019-02-07 10:56:32.870409: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1004] 0:   N 
2019-02-07 10:56:32.870562: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1116] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10445 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:17:00.0, compute capability: 6.1)
Traceback (most recent call last):
  File ""/opt/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/importer.py"", line 427, in import_graph_def
    graph._c_graph, serialized, options)  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot add function 'TRTEngineOp_0_native_segment' because a different function with the same name already exists.
```
"
25581,Issue regarding import library for tensorflow-gpu 1.0.1 version,"System information
- OS Platform: Linux Ubuntu 15.10
- TensorFlow installed from (source or binary):
- TensorFlow version: TensorFlow-GPU 1.0.1
- Python version: 3.7
- Installed using pip command: pip install tensorflow-gpu==1.0.1
- Using anaconda jupyter for programming
- CUDA/cuDNN version: 8
- GPU model and memory:NVIDIA Corporation GM204 [GeForce GTX 970]

I'm using tensorflow-gpu package to train seq2seq model on a dataset. But it gives error durring importing libraries.
----------------------------------------------------------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/home/comsats/Desktop/saher_tariq/Panday_group/reaction_prediction_seq2seq-master/bin/train.py"", line 29, in <module>
    import tensorflow as tf
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/__init__.py"", line 24, in <module>
    from tensorflow.python import *
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 72, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/__init__.py"", line 61, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 28, in <module>
    _pywrap_tensorflow = swig_import_helper()
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow', fp, pathname, description)
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py"", line 243, in load_module
    return load_dynamic(name, filename, file)
  File ""/home/comsats/yes/envs/my-rdkit-env/lib/python3.6/imp.py"", line 343, in load_dynamic
    return _load(spec)
ImportError: libcudart.so.8.0: cannot open shared object file: No such file or directory


Failed to load the native TensorFlow runtime.

See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/get_started/os_setup.md#import_error

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
----------------------------------------------------------------------------------------------------------------------------------
I have tried after re-installation of tensorflow but it gives same error. Model works fine when i had used only tensorflow 1.0.1 only for cpu March issue, but was very slow in working. So then i tried for tensorflow-gpu which gives me this error. And also this model scripts are compatible to tensorflow-gpu 1.0.1 version. As i got help to install the package from the discussion for the model working. https://github.com/pandegroup/reaction_prediction_seq2seq/issues/1 
Any help relating to issue is appreciated. 
Thank you."
25580,Edit Distance Matrix for CTC,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.12.2
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
The input will be labels and the predicted output(from ctc greedy/beam decoder, SparseTensor). It will calculate the following 4 things:
1. The edit distance between the output and the ground truth
2. Substitution Matrix: It will store all the entries where a value was substituted in the output to reach the ground truth. Shape = [n_symbols, n_symbols]. Like the confusion matrix
3. Insertion Matrix: It will store all the insertions to be made in the output to reach the grond truth. Shape = [n_symbols]
4. Deletion Matrix: It will store all the deletions to be made in the output to reach the grond truth. Shape = [n_symbols]

**Will this change the current api? How?**

**Who will benefit with this feature?**
The target audience is everyone who uses edit distance(word error rate, letter error rate) as a metric and wants to know where the model is failing. The 3 matrices will be quite helpful to debug errors in the output. 

**Any Other info.**
I'm willing to contribute. I can write it well in Python, C/C++. I want ideas from people and a little help in writing the tensorflow op"
25579,Linear alpha shape does not match the number of input channels.Node number 9 (GpuDelegate) failed to prepare.,"Dear TensorFlow developer,

I implemented my custom model in my app. And I tried to set GPU delegate.  
But if I instantiate `Interpreter` with GPU delegate option, the following error occurs.  
```
Linear alpha shape does not match the number of input channels.Node number 9 (GpuDelegate) failed to prepare.
```
What does this error message suppose to mean?  
Or is there anything I need to be careful on TFLite converting?  
Anything would be helpful.  
Thanks!  

The model I used is here.
[classify.tflite.zip](https://github.com/tensorflow/tensorflow/files/2839976/classify.tflite.zip)"
25578,CancelledError: Loop execution was cancelled,"I was doing some regression analysis using google colabs and until 2 days back everything was working fine but suddenly I'm getting following error. I din't know why the training is getting stopped in the middle. On stackoverflow I came to know that tensorflow was updated from 1.12 to 1.13.0rc0. The issue started the moment the tensorflow was updated. 

Here is the link to my [notebook](https://colab.research.google.com/drive/13LuWmVIjza-lfxuOyfWAEf_pJjtvAMEh) on colabs.

Following is the output from training:

```
 SA1 


Fold:  1



WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
WARNING:tensorflow:From <ipython-input-8-001ee577e90b>:225: DatasetV1.make_initializable_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_initializable_iterator(dataset)`.
WARNING:tensorflow:From <ipython-input-9-1bd488b4dd9a>:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From <ipython-input-9-1bd488b4dd9a>:348: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From <ipython-input-9-1bd488b4dd9a>:161: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.cast instead.
WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from data/vars/SA1/feeder.cpt
Training the model with He ...
0 MAPE: 11442.98324584961
100 MAPE: 1032.3982238769531
200 MAPE: 460.38122177124023
300 MAPE: 190.61466455459595
400 MAPE: 96.63236141204834
500 MAPE: 59.168386459350586
600 MAPE: 41.031333804130554
700 MAPE: 29.591849446296692
800 MAPE: 22.207358479499817
900 MAPE: 19.93158310651779
1000 MAPE: 17.7777960896492
1100 MAPE: 16.45900011062622
1200 MAPE: 16.250891983509064
1300 MAPE: 14.98890072107315
1400 MAPE: 14.976367354393005
1500 MAPE: 14.184540510177612
1600 MAPE: 13.81748765707016
1700 MAPE: 14.370763301849365
1800 MAPE: 13.738515973091125
1900 MAPE: 13.814045488834381
2000 MAPE: 13.300719857215881
2100 MAPE: 13.28342854976654
2200 MAPE: 13.087129592895508
2300 MAPE: 13.045386970043182
2400 MAPE: 12.72062063217163
2500 MAPE: 12.522627413272858
2600 MAPE: 12.2012197971344
2700 MAPE: 12.064149230718613
2800 MAPE: 11.975772678852081
2900 MAPE: 11.860555410385132
3000 MAPE: 11.887001246213913
3100 MAPE: 11.541987955570221
3200 MAPE: 11.460774391889572
3300 MAPE: 11.266357451677322
3400 MAPE: 11.597257852554321
3500 MAPE: 11.223434656858444
3600 MAPE: 11.532413214445114
3700 MAPE: 10.872676968574524
3800 MAPE: 11.064480245113373
3900 MAPE: 11.555147916078568
4000 MAPE: 11.46700382232666
4100 MAPE: 10.984917730093002
4200 MAPE: 10.663384199142456
4300 MAPE: 11.119940131902695
4400 MAPE: 10.703090578317642
4500 MAPE: 10.289034992456436
4600 MAPE: 9.894699603319168

---------------------------------------------------------------------------

CancelledError                            Traceback (most recent call last)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1333     try:
-> 1334       return fn(*args)
   1335     except errors.OpError as e:

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run_fn(feed_dict, fetch_list, target_list, options, run_metadata)
   1318       return self._call_tf_sessionrun(
-> 1319           options, feed_dict, fetch_list, target_list, run_metadata)
   1320 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list, run_metadata)
   1406         self._session, options, feed_dict, fetch_list, target_list,
-> 1407         run_metadata)
   1408 

CancelledError: Loop execution was cancelled.
	 [[{{node while/LoopCond}}]]


During handling of the above exception, another exception occurred:

CancelledError                            Traceback (most recent call last)

<ipython-input-11-86884b56c50d> in <module>()
     65                 while True:
     66                   try:
---> 67                     _, error = sess.run([train_model.train_op, train_model.mape])
     68                   except tf.errors.OutOfRangeError:
     69                     break

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)
    927     try:
    928       result = self._run(None, fetches, feed_dict, options_ptr,
--> 929                          run_metadata_ptr)
    930       if run_metadata:
    931         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)
   1150     if final_fetches or final_targets or (handle and feed_dict_tensor):
   1151       results = self._do_run(handle, final_targets, final_fetches,
-> 1152                              feed_dict_tensor, options, run_metadata)
   1153     else:
   1154       results = []

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)
   1326     if handle is None:
   1327       return self._do_call(_run_fn, feeds, fetches, targets, options,
-> 1328                            run_metadata)
   1329     else:
   1330       return self._do_call(_prun_fn, handle, feeds, fetches)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _do_call(self, fn, *args)
   1346           pass
   1347       message = error_interpolation.interpolate(message, self._graph)
-> 1348       raise type(e)(node_def, op, message)
   1349 
   1350   def _extend_graph(self):

CancelledError: Loop execution was cancelled.
	 [[node while/LoopCond (defined at <ipython-input-9-1bd488b4dd9a>:383) ]]

Caused by op 'while/LoopCond', defined at:
  File ""/usr/lib/python3.6/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib/python3.6/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py"", line 16, in <module>
    app.launch_new_instance()
  File ""/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py"", line 658, in launch_instance
    app.start()
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py"", line 477, in start
    ioloop.IOLoop.instance().start()
  File ""/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py"", line 888, in start
    handler_func(fd_obj, events)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 450, in _handle_events
    self._handle_recv()
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 480, in _handle_recv
    self._run_callback(callback, msg)
  File ""/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py"", line 432, in _run_callback
    callback(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py"", line 277, in null_wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 283, in dispatcher
    return self.dispatch_shell(stream, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 235, in dispatch_shell
    handler(stream, idents, msg)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py"", line 399, in execute_request
    user_expressions, allow_stdin)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py"", line 196, in do_execute
    res = shell.run_cell(code, store_history=store_history, silent=silent)
  File ""/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py"", line 533, in run_cell
    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2718, in run_cell
    interactivity=interactivity, compiler=compiler, result=result)
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2822, in run_ast_nodes
    if self.run_code(code, result):
  File ""/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py"", line 2882, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-11-86884b56c50d>"", line 59, in <module>
    asgd_decay=None)
  File ""<ipython-input-9-1bd488b4dd9a>"", line 70, in __init__
    inp.y_feature, inp.x_feature[:, -1, 0], init)
  File ""<ipython-input-9-1bd488b4dd9a>"", line 383, in decoder
    _, _, _, targets_ta, outputs_ta = tf.while_loop(cond_fn, loop_fn, loop_init)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3556, in while_loop
    return_same_structure)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3087, in BuildLoop
    pred, body, original_loop_vars, loop_vars, shape_invariants)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py"", line 3008, in _BuildLoop
    self._pivot = loop_cond(c, name=""LoopCond"")
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_control_flow_ops.py"", line 339, in loop_cond
    ""LoopCond"", input=input, name=name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py"", line 788, in _apply_op_helper
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py"", line 507, in new_func
    return func(*args, **kwargs)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 3300, in create_op
    op_def=op_def)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 1801, in __init__
    self._traceback = tf_stack.extract_stack()

CancelledError (see above for traceback): Loop execution was cancelled.
	 [[node while/LoopCond (defined at <ipython-input-9-1bd488b4dd9a>:383) ]]


```"
25577,Running code with Tensorflow GPU,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 1.12.0
- TensorFlow version: 1.12.0
- Python version: 3.6
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 0.22.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: cuda=10.0/cudnn=7.4.2
- GPU model and memory: RTX 2080



**Describe the problem**

I installed the everything and ran the tests for cuda and cudnn that were said from and they both passed normally. Running small tests with tensorflow also worked, i.e. a linear regression model. However, I was trying to run a file using keras and doing a more complicated [model](https://github.com/tensorflow/models/tree/master/samples/outreach/blogs/segmentation_blogpost) and it threw errors when it started running an individual epoch. I also just ran the code with only a small portion of the images.

```
I tensorflow/stream_executor/platform/default/dso_loader.cc:161] successfully opened CUDA library libcudnn.so.7 locally
E tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR
E tensorflow/stream_executor/cuda/cuda_dnn.cc:482] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR

...
...

tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
	 [[{{node conv2d/Conv2D}}]]
	 [[conv2d_transpose_2/Shape/_1171]]
```

Running the code normally with tensorflow works fine, only the gpu version is failing. I tried both building it from source and using tf-nightly-gpu."
25576,error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): [source](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)
- TensorFlow version: 1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: No
- Bazel version (if compiling from source): Not used Bazel. Built static library
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: No
- GPU model and memory: No GPU. RAM-8GB



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed the exact steps [here](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/makefile)

Upon running benchmark file,
`tensorflow/contrib/makefile/gen/bin/benchmark \
 --graph=$HOME/graphs/inception/tensorflow_inception_graph.pb`

I get the following error,
> error while loading shared libraries: libprotobuf.so.17: cannot open shared object file: No such file or directory


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25575,TFLite Converter not able to convert tf.keras model in TensorFlow 2.0,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below): 2.0.0-dev20190206
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
With TF 2.0 preview, the converter is not able to convert a tf.keras model to tflite either through python or command line

**Describe the expected behavior**
The TFlite converter converts the tf.keras model to tflite model.

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.
Please use this code for your testing:
https://github.com/margaretmz/test-tensorflow-2.0/blob/master/test-tflite-converter/test_tf2.0_keras_to_tflite.py
It contains code that you can use for testing - the conversion works for tf 1.11.0 and tf.12.0 but not for tf 2.0

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25570,Feature Request: tf.keras.layers.Conv2D supports channels_first when there are no GPU,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13.0-rc0,  b'v1.13.0-rc0-0-ga8e5c41c5b'
- Are you willing to contribute it (Yes/No): No


**Describe the feature and the current behavior/state.**
Although the API document explicitly says that data_format can be either ""channels_first"" and ""channel_last"", the implementation of tf.keras.layers.Conv2D only supports data_format=""channels_last"" currently. If I choose ""channel_first"", it raises ""InvalidArgumentError: Conv2DCustomBackpropFilterOp only supports NHWC.""

Cf: The API doc (https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) says:
`data_format: A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width). It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json. If you never set it, then it will be ""channels_last"".
`
Example Code:
```
import numpy as np
import tensorflow.keras as keras
model = keras.Sequential()
model.add(keras.layers.Conv2D(32, 8, 
        input_shape=(3, 128, 128), data_format=""channels_first""))
model.add(keras.layers.Flatten())
model.add(keras.layers.Dense(1))
model.compile(optimizer='adam', loss='mse')
x = np.ones((64, 3, 128, 128), dtype=np.float16)
y = np.ones((64, 1), dtype=np.float16)
model.fit(x, y)
```
Current output of the code:
```
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-3-580fd1a8364b> in <module>()
      9 x = np.ones((64, 3, 128, 128), dtype=np.float16)
     10 y = np.ones((64, 1), dtype=np.float16)
---> 11 model.fit(x, y)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)
    878           initial_epoch=initial_epoch,
    879           steps_per_epoch=steps_per_epoch,
--> 880           validation_steps=validation_steps)
    881 
    882   def evaluate(self,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py in model_iteration(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)
    327 
    328         # Get outputs.
--> 329         batch_outs = f(ins_batch)
    330         if not isinstance(batch_outs, list):
    331           batch_outs = [batch_outs]

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in __call__(self, inputs)
   3071         feed_symbols != self._feed_symbols or self.fetches != self._fetches or
   3072         session != self._session):
-> 3073       self._make_callable(feed_arrays, feed_symbols, symbol_vals, session)
   3074 
   3075     fetched = self._callable_fn(*array_vals,

/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in _make_callable(self, feed_arrays, feed_symbols, symbol_vals, session)
   3017       callable_opts.run_options.CopyFrom(self.run_options)
   3018     # Create callable.
-> 3019     callable_fn = session._make_callable_from_options(callable_opts)
   3020     # Cache parameters corresponding to the generated callable, so that
   3021     # we can detect future mismatches and refresh the callable.

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in _make_callable_from_options(self, callable_options)
   1469     """"""
   1470     self._extend_graph()
-> 1471     return BaseSession._Callable(self, callable_options)
   1472 
   1473 

/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py in __init__(self, session, callable_options)
   1423         with errors.raise_exception_on_not_ok_status() as status:
   1424           self._handle = tf_session.TF_SessionMakeCallable(
-> 1425               session._session, options_ptr, status)
   1426       finally:
   1427         tf_session.TF_DeleteBuffer(options_ptr)

/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py in __exit__(self, type_arg, value_arg, traceback_arg)
    526             None, None,
    527             compat.as_text(c_api.TF_Message(self.status.status)),
--> 528             c_api.TF_GetCode(self.status.status))
    529     # Delete the underlying status object from memory otherwise it stays alive
    530     # as there is a reference to status from this from the traceback due to

InvalidArgumentError: Conv2DCustomBackpropFilterOp only supports NHWC.
	 [[{{node training_2/Adam/gradients/conv2d_2/Conv2D_grad/Conv2DBackpropFilter}}]]
```

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
data_format=""channel_first"" is already widely used in lots of existing keras programs. This feature is somehow promised by Keras.


**Any Other info.**
"
25569,"When using MirroredStrategy in keras, custom callbacks receive an incorrect model instance ","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 9
- GPU model and memory:

**Describe the current behavior**

A keras custom callback typically has an access to the ""model"" object. There are many use cases where the access to model in a callback is required. 

When MirroredStrategy is used there is still access to ""model"" object however its type is not `tf.keras.models.Model` rather it is `DistributedModel`

When training such a model tensorflow generates following warnings -

```
WARNING:tensorflow:Your input callback is not one of the predefined Callbacks that supports DistributionStrategy. You might encounter an error if you access one of the model's attributes as part of the callback since these attributes are not set. You can access each of the individual distributed models using the `_grouped_model` attribute of your original model.
```

Inside the custom callback in this scenario self.model.layers is set to [] i.e. length is zero.

**Describe the expected behavior**

There are couple of issues with the existing behavior -:

a) Ideally one should not be required to change the code in the callbacks. But that is an ideal situation; it is understandable to some extent that in cases such as MirroredStrategy a custom callback may require extra conditions.

b) However, what is strange or rather would be considered  ill advised is the warning generated by tensorflow i.e. ""You can access each of the individual distributed models using the `_grouped_model` attribute of your original model""

The warning seems to suggest that one should try to access an `undocumented` and `private` property of an object. 

Also there is no `_grouped_model` attribute on self.model anyways; the one that I see is `_original_model`

That said, the expected behavior is to document if a custom callback is to be designed considering some of the caveats when using MirroredStrategy and provide a public attribute and/or method to have access to the original model.


**Code to reproduce the issue**

Here is an example created to reproduce this behavior (with the workaround) -

https://github.com/ksachdeva/tensorflow-bugs/tree/mirroredstrategy-model-access-in-callback
"
25568,Bug in CrossShardOptimizer for Windows running with TPU,"**System information**

* Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
* OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows server 2012
* Tensorflow installed from (source or binary): pip
* Tensorflow version (use command below): tensorflow-gpu 1.12.0 
* Python version: 3.6.6
* CUDA/cuDNN version: 9.0  V9.0.176
* GPU model and memory: Tesla Volta V100

Using tensorflow 1.12.0 (also tried with 1.10.0 on a windows OS running a Tesla V100 TPU, I was not able to optimize using the CrossShardOptimizer. I expected to define the optimization function so I could optimize a network (same as using an adam optimizer). However, when I wrap an adam optimizer in a crossshardoptimizer, I get an error when I call minimize or apply gradients.

The following is very simple code that illustrates the bug.

x = tf.placeholder(tf.float32, shape=(None, 32))
y = tf.layers.dense(x, 5)
yhat = tf.ones((20, 5))
loss = tf.reduce_mean(tf.square(y - yhat))

opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)

Bellow is the error message ( I tried to debug it myself simulating the Linux code, and that explanation is below).

AttributeError Traceback (most recent call last)
in 
4 loss = tf.reduce_mean(tf.square(y - yhat))
5
----> 6 opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)

D:\Anaconda3\envs\gq\lib\site-packages\tensorflow\python\training\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
408
409 return self.apply_gradients(grads_and_vars, global_step=global_step,
--> 410 name=name)
411
412 def compute_gradients(self, loss, var_list=None,

D:\Anaconda3\envs\gq\lib\site-packages\tensorflow\contrib\tpu\python\tpu\tpu_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
167 else:
168 with ops.colocate_with(grad):
--> 169 summed_grads_and_vars.append((tpu_ops.cross_replica_sum(
170 grad, self._group_assignment), var))
171 return self._opt.apply_gradients(summed_grads_and_vars, global_step, name)

AttributeError: module 'tensorflow.contrib.tpu.python.ops.tpu_ops' has no attribute 'cross_replica_sum'

Trying to build a workaround, within tpu_ops.py I found that most of the code was only defined for linux OS, so I tried to just define the functions I needed and run the imports I needed, which I believed to be:

from tensorflow.contrib.tpu.ops import gen_tpu_ops
from tensorflow.contrib.tpu.ops.gen_tpu_ops import *

```
from tensorflow.contrib.util import loader
from tensorflow.python.platform import resource_loader

def _create_default_group_assignment():
    num_shards = tpu_function.get_tpu_context().number_of_shards
    if num_shards is None:
        logging.warning(
            ""cross_replica_sum should be used within a tpu_shard_context, but ""
            ""got unset number_of_shards. Assuming 1."")
        num_shards = 1
    group_assignment = [list(range(num_shards))]
    return group_assignment

def cross_replica_sum(x, group_assignment=None, name=None):
    """"""Sum the input tensor across replicas according to group_assignment.

    Args:
    x: The local tensor to the sum.
    group_assignment: Optional 2d int32 lists with shape [num_groups,
        num_replicas_per_group]. `group_assignment[i]` represents the replica
        ids in the ith subgroup.
    name: Optional op name.

    Returns:
    A `Tensor` which is summed across replicas.
    """"""
    if group_assignment is None:
        group_assignment = _create_default_group_assignment()

    return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)
```
However, I still received an error message that made me believe the windows version was not built correctly in the binary files:

> > > x = tf.placeholder(tf.float32, shape=(None, 32))
> > > y = tf.layers.dense(x, 5)
> > > yhat = tf.ones((20, 5))
> > > loss = tf.reduce_mean(tf.square(y - yhat))
> > > opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)
> > > WARNING:tensorflow:CrossShardOptimizer should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.
> > > WARNING:tensorflow:cross_replica_sum should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.
> > > Traceback (most recent call last):
> > > File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 1628, in _create_c_op
> > > c_op = c_api.TF_FinishOperation(op_desc)
> > > tensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'CrossReplicaSum' in binary running on . Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File """", line 1, in 
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\training\optimizer.py"", line 410, in minimize
name=name)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\contrib\tpu\python\tpu\tpu_optimizer.py"", line 170, in apply_gradients
grad, self._group_assignment), var))
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\contrib\tpu\python\ops\tpu_ops.py"", line 417, in cross_replica_sum
return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\contrib\tpu\ops\gen_tpu_ops.py"", line 322, in cross_replica_sum
name=name)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
op_def=op_def)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
return func(*args, **kwargs)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
op_def=op_def)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 1792, in **init**
control_input_ops)
File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 1631, in _create_c_op
raise ValueError(str(e))
ValueError: Op type not registered 'CrossReplicaSum' in binary running on . Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'

> > >

Also to note, rather than calling minimize, I also tried compute gradients, apply gradients, and that also crashed similarly.

I hope that the tensorflow team can recommend a work around and/or work this fix into an upcoming version.
Thanks"
25567,Bug in CrossShardOptimizer for Windows running with TPU,"Using tensorflow 1.12.0 (also tried with 1.10.0 on a windows 7 OS running a Tesla V100 TPU, I was not able to optimize using the CrossShardOptimizer.

The following is very simple code that illustrates the bug. 

x = tf.placeholder(tf.float32, shape=(None, 32))
y = tf.layers.dense(x, 5)
yhat = tf.ones((20, 5))
loss = tf.reduce_mean(tf.square(y - yhat))

opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)


Bellow is the error message ( I tried to debug it myself simulating the Linux code, and that explanation is below).


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-18-c8e9584279b0> in <module>
      4 loss = tf.reduce_mean(tf.square(y - yhat))
      5 
----> 6 opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)

D:\Anaconda3\envs\gq\lib\site-packages\tensorflow\python\training\optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)
    408 
    409     return self.apply_gradients(grads_and_vars, global_step=global_step,
--> 410                                 name=name)
    411 
    412   def compute_gradients(self, loss, var_list=None,

D:\Anaconda3\envs\gq\lib\site-packages\tensorflow\contrib\tpu\python\tpu\tpu_optimizer.py in apply_gradients(self, grads_and_vars, global_step, name)
    167       else:
    168         with ops.colocate_with(grad):
--> 169           summed_grads_and_vars.append((tpu_ops.cross_replica_sum(
    170               grad, self._group_assignment), var))
    171     return self._opt.apply_gradients(summed_grads_and_vars, global_step, name)

AttributeError: module 'tensorflow.contrib.tpu.python.ops.tpu_ops' has no attribute 'cross_replica_sum'




Trying to build a workaround, within tpu_ops.py I found that most of the code was only defined for linux OS, so I tried to just define the functions I needed and run the imports I needed, which I believed to be:


from tensorflow.contrib.tpu.ops import gen_tpu_ops
    from tensorflow.contrib.tpu.ops.gen_tpu_ops import *

    from tensorflow.contrib.util import loader
    from tensorflow.python.platform import resource_loader

    def _create_default_group_assignment():
        num_shards = tpu_function.get_tpu_context().number_of_shards
        if num_shards is None:
            logging.warning(
                ""cross_replica_sum should be used within a tpu_shard_context, but ""
                ""got unset number_of_shards. Assuming 1."")
            num_shards = 1
        group_assignment = [list(range(num_shards))]
        return group_assignment

    def cross_replica_sum(x, group_assignment=None, name=None):
        """"""Sum the input tensor across replicas according to group_assignment.

        Args:
        x: The local tensor to the sum.
        group_assignment: Optional 2d int32 lists with shape [num_groups,
            num_replicas_per_group]. `group_assignment[i]` represents the replica
            ids in the ith subgroup.
        name: Optional op name.

        Returns:
        A `Tensor` which is summed across replicas.
        """"""
        if group_assignment is None:
            group_assignment = _create_default_group_assignment()

        return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)


However, I still received an error message that made me believe the windows version was not built correctly in the binary files:

>>> x = tf.placeholder(tf.float32, shape=(None, 32))
>>> y = tf.layers.dense(x, 5)
>>> yhat = tf.ones((20, 5))
>>> loss = tf.reduce_mean(tf.square(y - yhat))
>>>
>>> opt = tf.contrib.tpu.CrossShardOptimizer(tf.train.AdamOptimizer()).minimize(loss)
WARNING:tensorflow:CrossShardOptimizer should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.
WARNING:tensorflow:cross_replica_sum should be used within a tpu_shard_context, but got unset number_of_shards. Assuming 1.
Traceback (most recent call last):
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 1628, in _create_c_op
    c_op = c_api.TF_FinishOperation(op_desc)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Op type not registered 'CrossReplicaSum' in binary running on <ServerName>. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\training\optimizer.py"", line 410, in minimize
    name=name)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\contrib\tpu\python\tpu\tpu_optimizer.py"", line 170, in apply_gradients
    grad, self._group_assignment), var))
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\contrib\tpu\python\ops\tpu_ops.py"", line 417, in cross_replica_sum
    return gen_tpu_ops.cross_replica_sum(x, group_assignment, name=name)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\contrib\tpu\ops\gen_tpu_ops.py"", line 322, in cross_replica_sum
    name=name)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\op_def_library.py"", line 787, in _apply_op_helper
    op_def=op_def)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\util\deprecation.py"", line 488, in new_func
    return func(*args, **kwargs)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 3274, in create_op
    op_def=op_def)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 1792, in __init__
    control_input_ops)
  File ""D:\Anaconda3\envs\lt\lib\site-packages\tensorflow\python\framework\ops.py"", line 1631, in _create_c_op
    raise ValueError(str(e))
ValueError: Op type not registered 'CrossReplicaSum' in binary running on <ServerName>. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed. while building NodeDef 'CrossReplicaSum'
>>>

Also to note, rather than calling minimize, I also tried compute gradients, apply gradients, and that also crashed similarly.

I hope that the tensorflow team can recommend a work around and/or work this fix into an upcoming version.
Thanks"
25564,tf.keras.initializers.glorot_normal is not consistent with its documentation,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NO
- TensorFlow installed from (source or binary): Anaconda
- TensorFlow version (use command below): 1.12.0
- Python version: 3.5.0
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Describe the current behavior**
reference: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/glorot_normal

According to reference, it draws samples from a truncated normal distribution centered on 0 with stddev = sqrt(2 / (fan_in + fan_out)) where fan_in is the number of input units in the weight tensor and fan_out is the number of output units in the weight tensor.

I generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These should be 2!

**Describe the expected behavior**

I generated them with fan_in = 100, fan_out = 100 and ploted their histogram. I printed out the smallest and largest from the sampe and I got -0.22728574 0.22735965. These should be 2!

**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

fan_in = 100
fan_out = 100

tf.random.set_random_seed(337)

W = tf.get_variable(""W"", \
                    shape=(fan_in, fan_out), \
                    initializer=tf.keras.initializers.glorot_normal)

sigma = np.sqrt(2 / (fan_in + fan_out))

with tf.Session() as sess:
    tf.global_variables_initializer().run()
    
    W_now = sess.run(W)
    
    plt.hist(W_now.reshape((-1,)), bins=100)
    plt.show()
    
    print(2*sigma)
    print(np.min(W_now.reshape((-1,))), np.max(W_now.reshape((-1,))))

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25563,Can LSTM be fully quantized for inference?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (or github SHA if from source): 1.13.0-rc0

**Problem Summary and Errors Encountered**

I am trying to have the LSTM version from TFLite ([here](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/lite/experimental/examples/lstm/tflite_lstm.py)) quantized for inference through TFLite. 

I use `tf.contrib.quantize.create_training_graph()` for training and have added fake quant node after the input. But I think the use of `tf.unstack()` creates a problem when using the 'static_rnn()`

The model is as below:

```
input_frequency_size = model_settings['dct_coefficient_count']
input_time_size = model_settings['spectrogram_length']
fingerprint_4d = tf.reshape(fingerprint_input, 
                              [-1, input_time_size, input_frequency_size])
num_classes = model_settings['label_count']
projection_units = model_size_info[0]
LSTM_units = model_size_info[1]
with tf.name_scope(""LSTM-Layer""):
  with tf.variable_scope(""lstm""):
    lstm_cell = TFLiteLSTMCell(num_units=LSTM_units,
                                 use_peepholes=True,
                                 num_proj=projection_units)
    lstm_input = tf.unstack(fingerprint_4d, input_time_size, 1)
    # lstm_input = tf.fake_quant_with_min_max_args(lstm_input)
    _, last = tf.nn.static_rnn(cell=lstm_cell, inputs=lstm_input,
                                   dtype=tf.float32)
    # tf.quantization.fake_quant_with_min_max_args(last)
    flow = last[-1]

with tf.name_scope(""Output-Layer""):
  W_o = tf.get_variable('W_o', shape=[projection_units, num_classes],
                          initializer=tf.contrib.layers.xavier_initializer())
  b_o = tf.get_variable('b_o', shape=[num_classes],
                          initializer=tf.constant_initializer(0.01))
  logits = tf.add(tf.matmul(flow, W_o), b_o)
```

After training, I freeze the graph and use the following script to convert to TF lite:

```
graph_def_file = ""frozen_graph.pb""  # This is the .pb file.
input_arrays = [""mfcc_data""]            # This is the name of the input node
output_arrays = [""labels_softmax""]  # This is the name of the output node

# This is the main code to call tflite converter (i.e. toco)
converter = tf.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_arrays)
# Since fake quantization used during training, we can quantize the graph during converting
converter.inference_type = tf.lite.constants.QUANTIZED_UINT8
converter.reorder_across_fake_quant = True
input_arrays = converter.get_input_arrays()
converter.quantized_input_stats = {input_arrays[0] : (227., 1.)}  # mean, std_dev
tflite_model = converter.convert()
open(""lstm-tflite.tflite"", ""wb"").write(tflite_model)   # The resulting .tflite file.
```

But this then gives error as below:
```
ConverterError: TOCO failed. See console for info.
2019-02-06 11:57:30.492529: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 1098 operators, 1429 arrays (0 quantized)
2019-02-06 11:57:30.560084: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 1098 operators, 1429 arrays (0 quantized)
2019-02-06 11:57:30.646273: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 573 operators, 853 arrays (1 quantized)
2019-02-06 11:57:30.675022: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before pre-quantization graph transformations: 573 operators, 853 arrays (1 quantized)
2019-02-06 11:57:30.686976: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After pre-quantization graph transformations pass 1: 522 operators, 802 arrays (1 quantized)
2019-02-06 11:57:30.703210: F tensorflow/lite/toco/tooling_util.cc:1702] Array LSTM-Layer/lstm/unstack, which is an input to the Concatenation operator producing the output array LSTM-Layer/lstm/rnn/tf_lite_lstm_cell/concat, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.
```

I try to add the fake quant nodes manually after the unstack as you can see in the commented code above, but this will give an error from `static_rnn` because it will not remain a sequence anymore resulting in the following error while training:

```
Traceback (most recent call last):
  File ""train.py"", line 324, in <module>
    tf.app.run(main=main)
  File ""C:\Users\bhargav\AppData\Local\Continuum\miniconda3\envs\tf113cpu\lib\site-packages\tensorflow\python\platform\app.py"", line 125, in run
    _sys.exit(main(argv))
  File ""train.py"", line 140, in main
    logits, dropout_prob = models.create_model(
  File ""C:\Users\bhargav\Documents\audio\keyword-spotting\models.py"", line 58, in create_model
    model_size_info, is_training)
  File ""C:\Users\bhargav\Documents\audio\keyword-spotting\models.py"", line 198, in create_lstm_tflite_model
    dtype=tf.float32)
  File ""C:\Users\bhargav\AppData\Local\Continuum\miniconda3\envs\tf113cpu\lib\site-packages\tensorflow\python\util\deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""C:\Users\bhargav\AppData\Local\Continuum\miniconda3\envs\tf113cpu\lib\site-packages\tensorflow\python\ops\rnn.py"", line 1290, in static_rnn
    raise TypeError(""inputs must be a sequence"")
TypeError: inputs must be a sequence
```

I want to know if the full quantization of LSTM will be supported any time in near future. If GraphDef is needed, let me know."
25559,TensorRT version in  TensorFlow 1.13.0-rc0 ?,"What version of TensorRT does TensorFlow 1.13.0-rc0 support?  I don't see any mention about TensorRT in the release notes [link](https://github.com/tensorflow/tensorflow/releases/tag/v1.13.0-rc0)

Thanks"
25558,ImportError: No module named 'tensorflow' in official Docker image,"**System information**
- Windows 10 x64:
- Official tensorflow-jupiter Docker container (without GPU support), latest 06 Feb 2019
- TensorFlow version: unable to know
- Python version: sys.version_info(major=3, minor=5, micro=2, releaselevel='final', serial=0)

**Describe the problem**
`import tensorflow as tf ` command causing this error:

> ImportError                               Traceback (most recent call last)
> <ipython-input-7-7f6e5ebe327b> in <module>()
> ----> 1 import tensorflow as tf
> ImportError: No module named 'tensorflow'
> 

"
25557,Bazel test failed,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: v1.12
- Python version: 3.6.4
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: none 
- GPU model and memory: none

**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
After ./configure, the .tf_configure.bazelrc file is shown as:

```build --action_env PYTHON_BIN_PATH=""/home/jon/anaconda3/bin/python""
build --action_env PYTHON_LIB_PATH=""/home/jon/anaconda3/lib/python3.6/site-packages""
build --python_path=""/home/jon/anaconda3/bin/python""
build:xla --define with_xla_support=true
build --config=xla
build --action_env TF_NEED_OPENCL_SYCL=""0""
build --action_env TF_NEED_ROCM=""0""
build --action_env TF_NEED_CUDA=""0""
build --action_env TF_DOWNLOAD_CLANG=""0""
build:None --define with_mpi_support=true
build --config=None
build:opt --copt=-march=native
build:opt --copt=-Wno-sign-compare
build:opt --host_copt=-march=native
build:opt --define with_default_optimizations=true
build:v2 --define=tf_api_version=2
```
Then i ran 

`bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...` 

and got following error:

```
ERROR: /home/jon/local_build/tensorflow/tensorflow/contrib/BUILD:166:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' does not have mandatory providers: 'CcInfo'
ERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted
INFO: Elapsed time: 5.272s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
```

I'm not sure if this is due to the incompatibility of bazel. If so, can anyone suggest a bazel version and a gcc version to build tensorflow v1.12.0 or other release versions?

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
The full log is
```
jon@jon-OptiPlex-3050:~/local_build/tensorflow$ bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/contrib/lite/...
INFO: Invocation ID: af9321c5-5726-4ff2-a7ce-5f3d449cdb29
DEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:libtensorflow_jni: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:common_deps: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:eager_cheaders: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clib: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
DEBUG: /home/jon/.cache/bazel/_bazel_jon/7144622773f131b4a531d14f5da5d2cf/external/bazel_tools/tools/build_defs/pkg/pkg.bzl:246:17: @//tensorflow/tools/lib_package:clicenses: you provided a non dictionary to the pkg_tar `files` attribute. This attribute was renamed to `srcs`. Consider renaming it in your BUILD file.
WARNING: /home/jon/local_build/tensorflow/tensorflow/python/ops/distributions/BUILD:9:1: target '//tensorflow/python/ops/distributions:distributions' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.distributions will not receive new features, and will be removed by early 2019. You should update all usage of `tf.distributions` to `tfp.distributions`.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:86:1: target '//tensorflow/contrib/session_bundle:constants' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:94:1: target '//tensorflow/contrib/session_bundle:exporter' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:138:1: target '//tensorflow/contrib/session_bundle:gc' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:181:1: target '//tensorflow/contrib/session_bundle:session_bundle' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:200:1: target '//tensorflow/contrib/session_bundle:session_bundle_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:244:1: target '//tensorflow/contrib/session_bundle:session_bundle_py' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:312:1: target '//tensorflow/contrib/session_bundle:signature' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:293:1: target '//tensorflow/contrib/session_bundle:signature_lite' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/session_bundle/BUILD:349:1: target '//tensorflow/contrib/session_bundle:test_util' is deprecated: No longer supported. Switch to SavedModel immediately.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/distributions/BUILD:16:1: target '//tensorflow/contrib/distributions:bijectors_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
WARNING: /home/jon/local_build/tensorflow/tensorflow/contrib/distributions/BUILD:48:1: target '//tensorflow/contrib/distributions:distributions_py' is deprecated: TensorFlow Distributions has migrated to TensorFlow Probability (https://github.com/tensorflow/probability). Deprecated copies remaining in tf.contrib.distributions are unmaintained, unsupported, and will be removed by late 2018. You should update all usage of `tf.contrib.distributions` to `tfp.distributions`.
ERROR: /home/jon/local_build/tensorflow/tensorflow/contrib/BUILD:166:12: in deps attribute of cc_library rule //tensorflow/contrib:contrib_kernels: py_library rule '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' is misplaced here (expected cc_library, objc_library, cc_proto_library or cc_import) and '//tensorflow/contrib/mpi_collectives:mpi_collectives_py' does not have mandatory providers: 'CcInfo'
ERROR: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted: Analysis of target '//tensorflow/contrib:contrib_kernels' failed; build aborted
INFO: Elapsed time: 5.272s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
FAILED: Build did NOT complete successfully (0 packages loaded, 0 targets configured)
```
"
25554,Raspberry PI - Golang - Tensorflow - std::bad_alloc Issue,"@poxvoculi @penpornk @nhasabni @reedwm @shashishekhar @claynerobison @andydavis1 
Hi guys, I have checked some previous issues on ```bad_allocation``` and checked all the discussions and I could not reach to a conclusion how to resolve my ```bad_allocation``` issue. Below is my details for my issue with log. If I could get some insights, it will be very helpful.
**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Written custom code for face image classification. Model accepts 224x224x3 tensor. The model has 4,253,864 parameters and model depth is 88.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Raspberry Pi, CPU: 1.4Ghz, 980 Mb RAM.
- Programming Language
Golang 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) 
No
- TensorFlow installed from (source or binary):
Build from source
- TensorFlow version (use command below):
v1.12.0
- Python version:
NO
- Bazel version (if compiling from source):
v0.15.2
- GCC/Compiler version (if compiling from source):
6.3
- CUDA/cuDNN version:
NO
- GPU model and memory:
NO

**Describe the current behavior**
I am using GoLang enviroment to load Tensorflow model and run. My Tensorflow model is MobileNet.
It does face classification. I faced issue in memory allocation.
**Describe the expected behavior**
Model loading is fine and runs perfectly for 15-20 min and after that, it throws ```bad_allocation``` error and exits.
**Code to reproduce the issue**
1. Generate libtensorflow.so and libtensorflow_framework.so artifact for raspberry pi. 
2. Load tensorflow model into Golang Env. 
3. While model is running and after 15-20 min this happens.

**Other info / logs**
2019-02-06 10:41:44.057443: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:44.128595    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
2019-02-06 10:41:44.313010: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:44.314077    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
2019-02-06 10:41:44.409438: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:44.411047    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
2019-02-06 10:41:44.504644: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:44.505833    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
2019-02-06 10:41:44.601160: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:44.602079    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
I0206 10:41:48.258835    2719 camera_input.go:69] Captured Image, Len Queue 0
I0206 10:41:48.898151    2719 runner.go:85] Detection time 638.594898ms
2019-02-06 10:41:48.997640: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:48.999425    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
2019-02-06 10:41:49.100121: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:49.101226    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
2019-02-06 10:41:49.197890: W tensorflow/core/framework/op_kernel.cc:1412] OP_REQUIRES failed at conv_ops.cc:470 : Resource exhausted: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
E0206 10:41:49.199211    2719 classifier.go:148] Failed to classify face: OOM when allocating tensor with shape[1,112,112,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu
	 [[{{node mobilenet_1.00_224/conv_pw_1/convolution}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.
terminate called after throwing an instance of 'std::bad_alloc'
  what():  std::bad_alloc
Aborted

"
25552,RTX 2070 8GB tensorflow 1.13 build from source,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.13/1.12
- Python version: 3.6
- Installed using virtualenv? pip? conda?: pip/conda
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): Unsure
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: MSI Armor RTX 2070 8GB 



**Describe the problem**

Unable to build 1.13 from source using bazel. bazel is properly installed using chocolatey. I get to the end of configuring my build but I cannot build. TF version 1.12 does not work properly on CUDA 10.0. on TF 1.12, training data loads into the GPU's VRAM but uses the CPU to compute.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

On first pass, I was able to get to configure.py in tensorflow folder and go through the configuration process. Upon finishing, I was not able to build. After a few days, I am now attempting to configure the build and I am getting an output 'Cannot find bazel. Please install bazel. I installed bazel through chocolately. Attached are the logs showing bazel install through chocolately. I also have some logs during the first attempt at building. I forget where I located the tensorflow building logs. If you can direct me, I'll happily provide those. I'm also curious as to when the 1.13 release becomes official. Thanks. 



**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[chocolatey.log](https://github.com/tensorflow/tensorflow/files/2836941/chocolatey.log)
[choco.summary.log](https://github.com/tensorflow/tensorflow/files/2836942/choco.summary.log)"
25551,No TF import library to link custom operators on Windows,"I cannot reopen the issue here: https://github.com/tensorflow/tensorflow/issues/23740

But the issue persists, the file is still missing in the pip package as of today in TensorFlow 1.12.

Could you please reintroduce the library _pywrap_tensorflow_internal.lib in the pip pickage ? It is required to build custom ops on Windows platform.

Thanks,

"
25549,input_shapes error converting model to tflite,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra
- TensorFlow installed from (source or binary): over pip
- TensorFlow version (use command below): 1.12
- Python version: 3.6.5
- Bazel version (if compiling from source): 0.22.0



I get an error when converting a modified mobilenet_v1 (posenet to be exact)  frozen graph (.pb) with `input_shapes={'image':[1,225,225,3]} `to tflite.
When changing it to [1,224,224,3] it works.
I am using the function `tf.contrib.lite.TocoConverter.from_frozen_graph()`

The error is:
```
I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 213 operators, 318 arrays (0 quantized)\n2019-02-06 13:28:17.720907: 
I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 213 operators, 318 arrays (0 quantized)\n2019-02-06 13:28:17.732215: 
F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:991]  Check failed: height_with_paddings % block_height == 0 (1 vs. 0)\n'
None
```"
25548,Tensorflow AOT compilation error: No registered 'DecodeJpeg' OpKernel for XLA_CPU_JIT ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0 (tensorflow-gpu, tensorflow-base)
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: 7.1.2
- GPU model and memory: GeForce GTX 1080, 8GB

**Describe the current behavior**
I am trying to compile a pretrained InceptionV4 model from TF models repository. The last layer is modified to produce our custom set of classes (we're doing transfer learning). I successfully produced both graph.pb and graph.pb.txt files. When I tried to compile the graph, I received an error (see logs below).

* I have managed to successfully compile and run Keras ResNet50 and MobileNetV2 models before.
* I disabled XLA & CUDA when compiling TF from source.

**Other info / logs**
```
INVALID ARGUMENTS: Detected unsupported operations when trying to compile graph tfcompile on XLA_CPU_JIT: DecodeJpeg (No registered 'DecodeJpeg' OpKernel for XLA_CPU_JIT devices compatible with node {{node DecodeJpeg}}
	.  Registered:  <no registered kernels>
){{node DecodeJpeg}}

```"
25546,local_conv1d and local_conv2d are missing from the tensorflow.keras.backend namespace.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.13
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

Keras-team/keras provides `local_conv1d` and `local_conv2d` in `keras.backend`. tf.keras doesn't expose them. It's a compatibility issue between keras and tensorflow.keras. See https://keras.io/backend/ .

**Will this change the current api? How?**

It will add two functions to `tensorflow.keras.backend`

**Who will benefit with this feature?**

In keras-contrib, we expect keras and tf.keras to have the same public API. If the two are not compatibles, there are things that we can't do. 

**Any Other info.**
"
25543, tf.train.ExponentialMovingAverage scope issue,"I copied the tower_loss method from https://github.com/tensorflow/tensorflow/blob/r0.7/tensorflow/models/image/cifar10/cifar10_multi_gpu_train.py and I have a problem with the scope of the `loss_averages` variable. What is wrong and how can I fix it??
```
    def tower_loss(scope, img_batch, label_batch):
        # Calculate the total loss on a single tower running the model.
        # scope: unique prefix string identifying the tower, e.g. 'tower_0'
        # returns total loss for a batch of data
    
        # Build inference Graph.
        logits = inference(img_batch)
    
        # Build the portion of the Graph calculating the losses. Note that we will
        # assemble the total_loss using a custom function below.
        _ = model_loss(logits, label_batch)
    
        # Assemble all of the losses for the current tower only.
        losses = tf.get_collection('losses', scope)
    
        # Calculate the total loss for the current tower.
        total_loss = tf.add_n(losses, name='total_loss')
     
        loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')
    
        print(""current scope:"", scope)# tower_name_1/
        print(""loss_averages:"", loss_averages.name) # avg
        print(""total_loss:"", total_loss.name) # tower_name_1/total_loss_1:0
        print()
        # need scope:  tower_name_1/mean_sq_error/avg/
        loss_averages_op = loss_averages.apply(losses + [total_loss])
    
        
        with tf.control_dependencies([loss_averages_op]):
            total_loss = tf.identity(total_loss)
            
        return total_loss
```



Trace:
```
    total_loss: tower_name_0/total_loss_1:0
    curr scope: tower_name_0/
    loss_averages: avg
    
    Tensor(""tower_name_1/IteratorGetNext:0"", shape=(?, 227, 227, 3), dtype=float32, device=/device:GPU:1)
    Tensor(""tower_name_1/IteratorGetNext:1"", shape=(?,), dtype=float32, device=/device:GPU:1)
    total_loss: tower_name_1/total_loss_1:0
    curr scope: tower_name_1/
    loss_averages: avg
    
    Traceback (most recent call last):
      File ""new_FCN-LSTM.py"", line 363, in <module>
        train()
      File ""new_FCN-LSTM.py"", line 259, in train
        loss = tower_loss(scope, img_batch, label_batch)
      File ""new_FCN-LSTM.py"", line 165, in tower_loss
        loss_averages_op = loss_averages.apply(losses + [total_loss])
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\training\moving_averages.py"", line 415, in apply
        ""VarHandleOp""]))
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\training\slot_creator.py"", line 183, in create_zeros_slot
        colocate_with_primary=colocate_with_primary)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\training\slot_creator.py"", line 160, in create_slot_with_initializer
        dtype)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\training\slot_creator.py"", line 65, in _create_slot_var
        validate_shape=validate_shape)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\variable_scope.py"", line 1487, in get_variable
        aggregation=aggregation)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\variable_scope.py"", line 1237, in get_variable
        aggregation=aggregation)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\variable_scope.py"", line 540, in get_variable
        aggregation=aggregation)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\variable_scope.py"", line 492, in _true_getter
        aggregation=aggregation)
      File ""C:\Users\User\AppData\Roaming\Python\Python36\site-packages\tensorflow\python\ops\variable_scope.py"", line 879, in _get_single_variable
        ""reuse=tf.AUTO_REUSE in VarScope?"" % name)
    ValueError: Variable tower_name_1/mean_sq_error/avg/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?
```"
25542,Issues when training with keras.callbacks.tensorboard,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:Np
- TensorFlow installed from (source or binary):Through anaconda
- TensorFlow version (use command below):latest
- Python version:3.6
- Bazel version (if compiling from source):No
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:No
- GPU model and memory:Nvidia 920m


You can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with
python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""

**Code-Begins**
from nltk.tokenize import word_tokenize
from tensorflow import keras
import tensorflow as tf
import pandas as pd
import re
all_words = []
#importing the data
dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3)

#converting words present in each review to arrays
for i in range(0, 1000):
    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
    review = review.lower()
    review = review.split()
    review = ' '.join(review)
    all_words.append(word_tokenize(review))

#reading each word from all the reviews
words = []
for i in range(len(all_words)):
    for word in all_words[i]:
        words.append(word)

#removing repeated words
clean_words = []
for word in words:
    if word not in clean_words:
        clean_words.append(word)

#assigning a number to each word and creating sequence of words
count = []
for i in range(0,len(clean_words)):
    count.append(i)
sequence_of_words = dict(zip(clean_words,count))

with open('metadata.tsv', 'w') as metadata_file:
            metadata_file.write('ID\tWord\n')
            for Key, Value in sequence_of_words.items():
                metadata_file.write('{}\t{}\n'.format(Value, Key)) 

#saving the sequence of words
import pickle 
with open('sequenceofwords' , 'wb') as fid:
    pickle.dump(sequence_of_words , fid)
#reading the labels into an array
labels =[]
for i in range(0,1000):
    labels.append(dataset.Liked[i])

#converting each review to sequence of words
reverse_mapping=[]
for i in range(0, 1000):
    lop = []
    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])
    review = review.lower()
    for text in review.split():
        if text not in sequence_of_words.keys():
            lop.append('0')
        if text in sequence_of_words.keys():
            lop.append(sequence_of_words[text])
    reverse_mapping.append(lop)   

#deviding the data for training and testing
trainer_data = list(reverse_mapping[:700])
train_labels = [labels[:700]]
tester_data = list(reverse_mapping[700:])
test_labels = [labels[700:]]

train_data = keras.preprocessing.sequence.pad_sequences(trainer_data,
                                                        value=0,
                                                        padding='post',
                                                        maxlen=256)
test_data = keras.preprocessing.sequence.pad_sequences(tester_data, value = 0 , padding = 'post' , maxlen = 256)

#creating the neural network
vocab_size = 2021
model = keras.Sequential()
model.add(keras.layers.Embedding(vocab_size, 16, input_length = 256))
model.add(keras.layers.GlobalAveragePooling1D())
model.add(keras.layers.Dense(16, activation=tf.nn.relu))
model.add(keras.layers.Dense(1, activation=tf.nn.sigmoid))

#configuring the model
model.compile(optimizer='adam',
              loss='binary_crossentropy',
              metrics=['accuracy'])
#creating a meta file for tensorboard.
meta_file = open(r'E:\ENTERTAINMENT\OneDrive\Desktop\NLTK - Workspace\NLP\Natural_Language_Processing\review classifier\Tensorflow classifier\ML-\Training\metadata.tsv')
cb = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=5, batch_size=256, write_graph=True, write_grads=True, write_images=False, embeddings_freq=5, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=train_data)

#training the model
model.fit(train_data, train_labels ,epochs = 40, batch_size=6,validation_data=(test_data,test_labels) ,verbose= 1,callbacks = [cb])

model.summary()

results = model.evaluate(test_data, test_labels)

print(results)

#saving the model
keras_file = ""training.h5""
keras.models.save_model(model, keras_file)

**Code-Ends**

**Describe the expected behavior**
Create log files to run tensorboard. But gives error.

**Error-Begins**

InvalidArgumentError: Tensor embedding_6_input:0, specified in either feed_devices or fetch_devices was not found in the Graph

**Error-Ends**


"
25541,install tensoflow in python 3.6 but after that my code given an error no module name tensorflow,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
25540,tensorflow.python.framework.errors_impl.AlreadyExistsError while importing freezed graph,"- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): YES
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):Binary
- TensorFlow version (use command below):1.12.0
- Python version:3.5.2

**Describe the current behavior**
The functions used to freeze graph, I don't receive any error while freezing the graph        
```
tmp_g = tf.graph_util.convert_variables_to_constants(
          sess, tmp_g, [n.name[:-2] for n in output_tensors]
)
```   
```
with tf.gfile.GFile(""inference_graph"", ""wb"") as f:
    f.write(tmp_g.SerializeToString())
```       
The functions used to load graph        
``` 
with tf.gfile.GFile(""inference_graph"", ""rb"") as f:
            graph_def = tf.GraphDef()
            graph_def.ParseFromString(f.read())```
start_logits, end_logits = tf.import_graph_def(                                                                                        
                graph_def,
                input_map={k: features[k[:-2]] for k in input_names},
                return_elements=[""unstack:0"", ""unstack:1""],
            )
```    
The error,        
```
INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Graph was finalized.
2019-02-06 15:26:25.816370: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not c$
mpiled to use: AVX512F
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
2019-02-06 15:26:40.501236: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at variable_ops.cc:104 : Already exists: Resource __per_
step_6/import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
Traceback (most recent call last):
  File ""/home/dj/.local/share/virtualenvs/bert-squeeze-test12-a8EWmLrd/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1334, in
 _do_call
    return fn(*args)
  File ""/home/dj/.local/share/virtualenvs/bert-squeeze-test12-a8EWmLrd/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1319, in
 _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File ""/home/dj/.local/share/virtualenvs/bert-squeeze-test12-a8EWmLrd/lib/python3.5/site-packages/tensorflow/python/client/session.py"", line 1407, in
 _call_tf_sessionrun
    run_metadata)
tensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_6/import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_
1/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
         [[{{node import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1/tmp_var}} = TemporaryVariable[dtype=DT_FLOAT, shape=[1,384,768
], var_name=""import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1/tmp_var"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^
import/magic/while/embeddings/Reshape_3)]]

tensorflow.python.framework.errors_impl.AlreadyExistsError: Resource __per_step_6/import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add$
1/tmp_var/N10tensorflow19TemporaryVariableOp6TmpVarE
         [[{{node import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1/tmp_var}} = TemporaryVariable[dtype=DT_FLOAT, shape=[1,384,76$
], var_name=""import/magic/while/embeddings/ArithmeticOptimizer/AddOpsRewrite_add_1/tmp_var"", _device=""/job:localhost/replica:0/task:0/device:CPU:0""](^
import/magic/while/embeddings/Reshape_3)]]
```"
25539,Tensorflow AOT compiled graph does not use GPU,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 1.12.0 (tensorflow-gpu, tensorflow-base)
- Python version: 3.6.8
- Bazel version (if compiling from source): 0.21.0
- GCC/Compiler version (if compiling from source): 5.4.0 20160609
- CUDA/cuDNN version: 7.1.2
- GPU model and memory: GeForce GTX 1080, 8GB

**Describe the current behavior**
Following the https://gist.github.com/carlthome/6ae8a570e21069c60708017e3f96c9fd tutorial, which is based on https://www.tensorflow.org/xla/tfcompile, all the compilation steps succeed. I am also able to invoke the compiled graph, producing correct prediction results. A non-compiled predict of a single image using RestNet50 takes 5ms, a compiled version takes ~140ms.

Note: I have ran ./configure via a terminal to use force CUDA support (such option is not accepted in the example).

**Describe the expected behavior**
The compiled version prediction of a single image should be as fast as a non-compiled prediction. I suspect the compiled graph uses CPU for the inference.

**Code to reproduce the issue**
Follow the https://gist.github.com/carlthome/6ae8a570e21069c60708017e3f96c9fd tutorial. To evaluate the compiled and the non-compiled graph prediction of a single image duration, add these two lines to the end of the notebook:

`%timeit -n 20 predict(x)`
`%timeit -n 20 model.predict(x)`
"
25537,TFLite model performance on Android is worse than the one on TFLite Model Benchmark Tool,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.14.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (or github SHA if from source): 1.13.0.dev20190126
- Android Studio version: 3.3
- TensorFlow Lite version on Android: 0.0.0-gpu-experimental

**Descrive the current behavior**
I tried running my custom classify model on Android Studio. This model works well, but the performance is bad. It takes about **300 msec** to classify a 224 x 224 image on Simulator. I measured the run time of `Interpreter.run()` method.   
When I benchmark this model with TFLite Model Benchmark Tool, it takes about **34 msec** on desktop.  
What is the difference between these performances?

**The output of TFLite Model Benchmark Tool**

```
STARTING!
Min num runs: [50]
Min runs duration (seconds): [1]
Inter-run delay (seconds): [-1]
Num threads: [1]
Benchmark name: []
Output prefix: []
Min warmup runs: [1]
Min warmup runs duration (seconds): [0.5]
Graph: [../tensorflow_lite/classify.tflite]
Input layers: []
Input shapes: []
Use nnapi : [0]
Loaded model ../tensorflow_lite/classify.tflite
resolved reporter
Initialized session in 5.725ms
Running benchmark for at least 1 iterations and at least 0.5 seconds
count=11 first=53892 curr=34948 min=34948 max=55623 avg=45455.3 std=6945

Running benchmark for at least 50 iterations and at least 1 seconds
count=50 first=34139 curr=33605 min=32515 max=44607 avg=33959.3 std=1682

============================== Run Order ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	            0.000	    3.337	    3.394	  9.996%	  9.996%	     0.000	        1	[conv2d_1/BiasAdd]
	                   PRELU	            3.395	    0.849	    0.796	  2.344%	 12.340%	     0.000	        1	[p_re_lu_1/add]
	                 CONV_2D	            4.191	    9.352	    9.059	 26.678%	 39.019%	     0.000	        1	[conv2d_2/BiasAdd]
	                   PRELU	           13.250	    0.795	    0.798	  2.350%	 41.369%	     0.000	        1	[p_re_lu_2/add]
	                 CONV_2D	           14.048	    9.383	    9.081	 26.741%	 68.110%	     0.000	        1	[conv2d_3/BiasAdd]
	                   PRELU	           23.129	    0.754	    0.797	  2.347%	 70.458%	     0.000	        1	[p_re_lu_3/add]
	                 CONV_2D	           23.926	    8.803	    9.133	 26.895%	 97.353%	     0.000	        1	[conv2d_4/BiasAdd]
	         AVERAGE_POOL_2D	           33.060	    0.863	    0.898	  2.644%	 99.997%	     0.000	        1	[average_pooling2d_1/AvgPool]
	         FULLY_CONNECTED	           33.958	    0.001	    0.001	  0.003%	100.000%	     0.000	        1	[dense_1/BiasAdd]

============================== Top by Computation Time ==============================
	             [node type]	          [start]	  [first]	 [avg ms]	     [%]	  [cdf%]	  [mem KB]	[times called]	[Name]
	                 CONV_2D	           23.926	    8.803	    9.133	 26.895%	 26.895%	     0.000	        1	[conv2d_4/BiasAdd]
	                 CONV_2D	           14.048	    9.383	    9.081	 26.741%	 53.637%	     0.000	        1	[conv2d_3/BiasAdd]
	                 CONV_2D	            4.191	    9.352	    9.059	 26.678%	 80.315%	     0.000	        1	[conv2d_2/BiasAdd]
	                 CONV_2D	            0.000	    3.337	    3.394	  9.996%	 90.311%	     0.000	        1	[conv2d_1/BiasAdd]
	         AVERAGE_POOL_2D	           33.060	    0.863	    0.898	  2.644%	 92.955%	     0.000	        1	[average_pooling2d_1/AvgPool]
	                   PRELU	           13.250	    0.795	    0.798	  2.350%	 95.305%	     0.000	        1	[p_re_lu_2/add]
	                   PRELU	           23.129	    0.754	    0.797	  2.347%	 97.653%	     0.000	        1	[p_re_lu_3/add]
	                   PRELU	            3.395	    0.849	    0.796	  2.344%	 99.997%	     0.000	        1	[p_re_lu_1/add]
	         FULLY_CONNECTED	           33.958	    0.001	    0.001	  0.003%	100.000%	     0.000	        1	[dense_1/BiasAdd]

Number of nodes executed: 9
============================== Summary by node type ==============================
	             [Node type]	  [count]	  [avg ms]	    [avg %]	    [cdf %]	  [mem KB]	[times called]
	                 CONV_2D	        4	    30.665	    90.316%	    90.316%	     0.000	        4
	                   PRELU	        3	     2.390	     7.039%	    97.355%	     0.000	        3
	         AVERAGE_POOL_2D	        1	     0.897	     2.642%	    99.997%	     0.000	        1
	         FULLY_CONNECTED	        1	     0.001	     0.003%	   100.000%	     0.000	        1

Timings (microseconds): count=50 first=34137 curr=33603 min=32513 max=44604 avg=33957.1 std=1682
Memory (bytes): count=0
9 nodes observed


Average inference timings in us: Warmup: 45455.3, Init: 5725, no stats: 33959.3
```

**Model file**
[classify.tflite.zip](https://github.com/tensorflow/tensorflow/files/2835017/classify.tflite.zip)
"
25536,I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2,"I have installed tensorflow in my CPU based system using command:      
pip install tensorflow  

Installation completed without any error and as part of some initial verification I am able to see the tensorflow version installed:      
>>> import tensorflow     
>>> tensorflow.__version__    
 '1.5.0'
Operating system used : Ubuntu 16.04  

Now, when I tried running a python file having code to deal with a tensorflow model, I am getting the following error and the file did not execute:      
I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2  

I checked online for solution and could see the discussions mostly around: TensorFlow binary was not compiled to use: 'AVX AVX2'(where in my case it is: 'SSE4.1 SSE4.2')  I am new to tensorflow and finding the solutions bit overwhelming. 
Could you please help me to resolve this specific issue?  
Thanks in advance"
25530,ImportError: cannot import name 'toco_flags_pb2',"**System information**
- OS Platform : Windows 10 Home x64 version 1809
- TensorFlow installed from : pip
- TensorFlow version 1.12.0


**text output from tflite_convert**

```
# tflite_convert \ --output_file=/tmp/model.tflite \ --graph_def_file=/tmp/output_graph.pb \ --input_arrays=input \ --output_arrays=MobilenetV1/Predictions/Reshape_1
Traceback (most recent call last):
  File ""c:\users\somesh\appdata\local\programs\python\python36\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\somesh\appdata\local\programs\python\python36\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\somesh\AppData\Local\Programs\Python\Python36\Scripts\tflite_convert.exe\__main__.py"", line 5, in <module>
  File ""c:\users\somesh\appdata\local\programs\python\python36\lib\site-packages\tensorflow\contrib\__init__.py"", line 103, in <module>
    from tensorflow.contrib.lite.python import lite
  File ""c:\users\somesh\appdata\local\programs\python\python36\lib\site-packages\tensorflow\contrib\lite\python\lite.py"", line 40, in <module>
    from tensorflow.lite.python import lite_constants as constants
  File ""c:\users\somesh\appdata\local\programs\python\python36\lib\site-packages\tensorflow\lite\python\lite_constants.py"", line 21, in <module>
    from tensorflow.lite.toco import toco_flags_pb2 as _toco_flags_pb2
ImportError: cannot import name 'toco_flags_pb2 
```
"
25528,where is tensor_forest ?  i cannot find it  from API ?,"<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>


**System information**
- TensorFlow version:
- Doc Link:


**Describe the documentation issue**

**We welcome contributions by users. Will you be able to update submit a PR (use the [doc style guide](https://www.tensorflow.org/community/documentation)) to fix the doc Issue?**
"
25527,ROCm build doesn't detect GPU,"
**System information**
- OS Platform and Distribution  Ubuntu 18.10
- TensorFlow installed from source
- TensorFlow version tensorflow-1.13.0rc0-cp36-cp36m-linux_x86_64.whl
- Python version: 3.6
- Installed using pip with wheel built from source:
- Bazel version 0.21:
- GCC/Compiler version 8.2:
- CUDA/cuDNN version N/A:
- GPU model and memory: AMD Vega RX 64 



**Describe the problem**

Selecting ROCm in ""./configure"" doesn't find GPU in ""tf.session()""
"
25526,ModuleNotFoundError: No module named 'tensorflow_estimator' in bazel tests,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution: 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: Python 3.6.6
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): 0.20.0
- GCC/Compiler version (if compiling from source): 7.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**
According to https://www.tensorflow.org/install/source after downloading I should be able to run tests via:
bazel test -c opt -- //tensorflow/... -//tensorflow/compiler/... -//tensorflow/lite/...
But when I run this command a lot of tests fail like this one
bazel test tensorflow/contrib/eager/python/examples/gan:mnist_graph_test_gpu
fail on No module named 'tensorflow_estimator' even though later I've installed estimator.

Do I need to have some special environment? I've been even testing tensorflow/tensorflow docker image and it also fails with this error

**Provide the exact sequence of commands / steps that you executed before running into the problem**

git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
bazel test tensorflow/contrib/eager/python/examples/gan:mnist_graph_test_gpu
**Any other info / logs**

exec ${PAGER:-/usr/bin/less} ""$0"" || exit 1
Executing tests from //tensorflow/contrib/eager/python/examples/gan:mnist_graph_test_gpu
-----------------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test.py"", line 26, in <module>
    from tensorflow.contrib.eager.python.examples.gan import mnist
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/__init__.py"", line 41, in <module>
    from tensorflow.contrib import distribute
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/distribute/__init__.py"", line 33, in <module>
    from tensorflow.contrib.distribute.python.tpu_strategy import initialize_tpu_system
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/distribute/python/tpu_strategy.py"", line 27, in <module>
    from tensorflow.contrib.tpu.python.ops import tpu_ops
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/tpu/__init__.py"", line 73, in <module>
    from tensorflow.contrib.tpu.python.tpu.keras_support import tpu_model as keras_to_tpu_model
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/tpu/python/tpu/keras_support.py"", line 62, in <module>
    from tensorflow.contrib.tpu.python.tpu import tpu
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/tpu/python/tpu/tpu.py"", line 25, in <module>
    from tensorflow.contrib.compiler import xla
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/contrib/compiler/xla.py"", line 28, in <module>
    from tensorflow.python.estimator import model_fn as model_fn_lib
  File ""/home/tclbot/.cache/bazel/_bazel_tclbot/eee9defa2fa4c4fc557baa005719ebd9/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/eager/python/examples/gan/mnist_graph_test_gpu.runfiles/org_tensorflow/tensorflow/python/estimator/__init__.py"", line 26, in <module>
    from tensorflow_estimator.python import estimator
ModuleNotFoundError: No module named 'tensorflow_estimator'
"
25525,tf.layers.batch_normalization deprecation is a regression,"**System information**
- TensorFlow version (use command below):
('v1.12.0-7360-g5076adf64a', '1.13.0-dev20190204')

**Describe the behaviour**
[tensorflow/tensorflow/python/layers/normalization.py](https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/layers/base.py) has a deprecation warning: (Use keras.layers.batch_normalization instead.')

However, the BatchNormalization class in that file also inherits from base.Layer. In particular, this means it updates UPDATE_OPS which the keras layer doesn't. 

The workaround is cumbersome. One is use the following code:
    
    def batch_norm(net_input, is_training, trainable=True, momentum=0.99,
               name=""BatchNorm""):
    layer = tf.keras.layers.BatchNormalization(
        epsilon=1E-5, center=True, scale=True, momentum=momentum,
        trainable=trainable, name=name)
    add_elements_to_collection(layer.updates, tf.GraphKeys.UPDATE_OPS)

    return layer.apply(net_input, training=is_training)

With a separate method

    def add_elements_to_collection(elements, collection_list):
        elements = tf.nest.flatten(elements)
        collection_list = tf.nest.flatten(collection_list)
        for name in collection_list:
           collection = tf.get_collection_ref(name)
           collection_set = set(collection)
           for element in elements:
               if element not in collection_set:
                    collection.append(element)


The other workaround is to create a separate list, to be used instead of tf.GraphKeys.UPDATE_OPS.

In my view, this is a regression bug as avoiding the use of the deprecated methods involves the loss of useful feature and requirement of extra code.  The canned model such as [ResNet](https://github.com/tensorflow/models/blob/master/official/resnet/resnet_model.py) will be affected by this. "
25524,Tensorboard Keras fit callback error with TensorFlow 2.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- Tensorflow installed from (source or binary): pip
- Tensorflow version (use command below): tf-nightly-gpu-2.0-preview | 2.0.0.dev20190204 |
- Python version: 3.6.8
- CUDA/cuDNN version: 10.0, V10.0.130
- GPU model and memory: GeForce RTX 2018 Ti 11GB

**Describe the current behavior**
During the training of a simple ""toy-example"" classificator using TensorFlow 2.0 and tfds, I'm trying to use the Keras callback in order to log the results and to be able to use TensorBoard. When the first epoch of training ends I get the following error: `tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/logdir:./log/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: epoch_loss/
`

**Describe the expected behavior**
I expect to be able to use the TensorBoard callback without issues.

**Code to reproduce the issue**

**Classificator:**
This is a toy example based on the inception architecture and it is written inside a _model.py_ file using Keras Functional API.

```
def inception(input_shape: Tuple[int, int, int]) -> k.Model:
    # set the input
    input_img = k.layers.Input(shape=input_shape)

    tower_1 = k.layers.Conv2D(64, (1, 1), padding=""same"", activation=""relu"")(input_img)
    tower_1 = k.layers.Conv2D(64, (3, 3), padding=""same"", activation=""relu"")(tower_1)

    tower_2 = k.layers.Conv2D(64, (1, 1), padding=""same"", activation=""relu"")(input_img)
    tower_2 = k.layers.Conv2D(64, (5, 5), padding=""same"", activation=""relu"")(tower_2)

    tower_3 = k.layers.MaxPooling2D((3, 3), strides=(1, 1), padding=""same"")(input_img)
    tower_3 = k.layers.Conv2D(64, (1, 1), padding=""same"", activation=""relu"")(tower_3)

    output = k.layers.concatenate([tower_1, tower_2, tower_3], axis=3)

    output = k.layers.Flatten()(output)
    out = k.layers.Dense(10, activation=""softmax"")(output)

    model = k.models.Model(inputs=[input_img], outputs=[out])

    print(model.summary())

    return model
```

**Training code:** 
The training code it is inside a _test.py_ file and use the `tfds` to import the datasets and the Keras `fit` function to train.

```
import tensorflow_datasets as tfds
import tensorflow as tf
from tensorflow import keras as k

import multiprocessing
import model

DATADIR = ""/run/media/federico/XData/tensorflow_datasets""
CLASSES = 10
EPOCHS = 25
BATCH = 128
LRATE = 0.01
DECAY = LRATE / EPOCHS

# this is applied to every single data passed
def process_features(feature):
    image, label = feature[""image""], feature[""label""]

    # image conversion into [0, 1]
    image = image / 255
    image = tf.cast(image, tf.float32)
    feature[""image""] = image

    # label conversion into one-hot
    print(label)
    label = tf.one_hot(label, CLASSES)
    feature[""label""] = label

    return feature

# print(tfds.list_builders())

dataset, dataset_info = tfds.load(name=""cifar10"", data_dir=DATADIR, with_info=True)
train_dataset, test_dataset = dataset[""train""], dataset[""test""]

# process features
train_dataset = (
    train_dataset.map(process_features, num_parallel_calls=multiprocessing.cpu_count())
    .shuffle(1000)
    .batch(BATCH)
    .repeat(EPOCHS)
    # .prefetch(10)
)


test_dataset = test_dataset.map(
    process_features, num_parallel_calls=multiprocessing.cpu_count()
).batch(dataset_info.splits[""test""].get_proto().statistics.num_examples)

for f in test_dataset:
    images_t, labels_t = f[""image""], f[""label""]

model = model.inception(dataset_info.features[""image""].shape)

# compile the model
model.compile(
    optimizer=k.optimizers.RMSprop(learning_rate=LRATE, decay=DECAY),
    loss=k.losses.categorical_crossentropy,
    metrics=[k.metrics.categorical_accuracy],
)

tbCallback = [
    k.callbacks.TensorBoard(
        log_dir=""./log"", histogram_freq=0, write_graph=False, write_images=False
    )
]
# tbCallback = [k.callbacks.TensorBoard(log_dir='./log')]

step = 0
for f in train_dataset:
    images, labels = f[""image""], f[""label""]
    step += 1
    model.fit(
        images,
        labels,
        validation_data=(images_t, labels_t),
        steps_per_epoch=100,
        epochs=1,
        batch_size=BATCH,
        callbacks=tbCallback,
    )

```

**Other info / logs**

```
Traceback (most recent call last):
  File ""/run/media/federico/XData/PycharmProjectsXData/classificator/test.py"", line 105, in <module>
    callbacks=tbCallback,
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 963, in fit
    steps_name='steps_per_epoch')
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py"", line 386, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 261, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1389, in on_epoch_end
    self._write_custom_summaries(step, logs)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/keras/callbacks.py"", line 1335, in _write_custom_summaries
    summary_ops_v2.scalar(name, value, step=step)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 564, in scalar
    return summary_writer_function(name, tensor, function, family=family)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 508, in summary_writer_function
    should_record_summaries(), record, _nothing, name="""")
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/framework/smart_cond.py"", line 54, in smart_cond
    return true_fn()
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 501, in record
    with ops.control_dependencies([function(tag, scope)]):
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 562, in function
    name=scope)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py"", line 727, in write_scalar_summary
    writer, step, tag, value, name=name, ctx=_ctx)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/ops/gen_summary_ops.py"", line 763, in write_scalar_summary_eager_fallback
    attrs=_attrs, ctx=_ctx, name=name)
  File ""/run/media/federico/XData/virtualenvs/python36_tf2preview/lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 66, in quick_execute
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.NotFoundError: Resource localhost/logdir:./log/N10tensorflow22SummaryWriterInterfaceE does not exist. [Op:WriteScalarSummary] name: epoch_loss/
```
"
25522,MATLAB and CUDNN: Could not create cudnn handle CUDNN_STATUS_NOT_INITIALIZED,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 1.12
- Python version: 3.6.8
- Bazel version (if compiling from source): No
- GCC/Compiler version (if compiling from source): No
- CUDA/cuDNN version: 9.0
- GPU model and memory: Titan Xp


I have a CNN developed in PyCharm IDE that runs smoothly. Now I am trying to run the network through MATLAB system command by invoking the Python script and I am getting the following error:

`...
2019-02-05 12:37:45.681913: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2019-02-05 12:37:45.682038: E tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
2019-02-05 12:37:45.682192: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_NOT_INITIALIZED
2019-02-05 12:37:45.682306: E tensorflow/stream_executor/cuda/cuda_dnn.cc:377] Error retrieving driver version: Unimplemented: kernel reported driver version not implemented on Windows
...
tensorflow.python.framework.errors_impl.UnknownError: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.
...`

It is so weird because if I launch exactly the same command through the Windows cmd or through the PyCharm IDE all works perfectly. But if I launch it inside MATLAB, through the system command, it returns this error.

Some idea?

Thank you very much.

Javier.
"
25520,Normal.log_cdf raises exception when called with float64,"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): 
Binary (pip)
- TensorFlow version (use command below): 
v1.12.0-0-ga6d8ffae09 1.12.0
- Python version: 
Python 3.6.4 :: Anaconda, Inc.


**Describe the current behavior**

Calling Normal.log_cdf(x) with a float64 raises a TypeError exception.


**Code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate the problem.

```
import tensorflow as tf

normal = tf.distributions.Normal(loc=0.0, scale=1.0)
normal.log_cdf(tf.constant(3.5, dtype=tf.float32))  # Operation created successfully as expected
normal.log_cdf(tf.constant(3.5, dtype=tf.float64))  # Exception
```

Exception is:

> TypeError: Input 'y' of 'Sub' Op has type float32 that does not match type float64 of argument 'x'.

**Other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

[trace.txt](https://github.com/tensorflow/tensorflow/files/2831779/trace.txt)
"
25519,Please offer binary without SSE4.1 instructions,"Some machines don't have SSE4.1 CPU instructions, resulting in this error:
https://github.com/tensorflow/tensorflow/issues/7138

Instead of forcing users to manually compile their version of tensorflow you should offer a compiled binary (on pip) with SSE4.1 instructions disabled."
25518,prediction of image segmentation after training the network tensorflow,"hello everyone,
i am working for vessel segmentation from retinal image. i am using an autoencoder network with softmax activation function for the final layer. this is my following code for training and prediction:

** training code
//loss function
losses =tf.nn.softmax_cross_entropy_with_logits(labels=net_output, logits=network)

cost = tf.reduce_mean(losses)
tf.summary.scalar(""cross_entropy"", cost)
//Adam optimizer
opt = tf.train.AdamOptimizer(args.learning_rate).minimize(cost, var_list=[var for var in tf.trainable_variables()])

init = tf.global_variables_initializer()

correct_prediction = tf.equal(tf.argmax(network, 1), tf.argmax(net_output, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

for epoch in range(0, args.num_epochs):

```
for i in range(0, len(train_input_names)):

   input_black_ring_image = preprocessing(train_input_names[i])
   input_image, segmented_image = data_augmentation(input_black_ring_image, train_segmented_names[i])
       for k in range(0, len(input_image)):

            for j in range(0, args.batch_size):

                index= k*args.batch_size + j
                    with tf.device('/gpu:0'):
                      input_slice = np.float32(input_image[index]) / 255.0

                      output_slice = np.float32(helpers.one_hot_it(label=segmented_image[index], label_values=label_values))


                                input_image_batch.append(np.expand_dims(input_slice, axis=0))
                                segmented_image_batch.append(np.expand_dims(output_slice, axis=0))                        
            if args.batch_size == 1:
                input_img = input_image_batch[0]


                 output_img = segmented_image_batch[0]

            else:
                 input_img = np.squeeze(np.stack(input_image_batch, axis=1))
                 output_img = np.squeeze(np.stack(segmented_image_batch, axis=1))  
```

//training
_,current=sess.run([opt, cost],feed_dict={net_input:input_img, net_output:output_img})
print(""loss of img ""+str(i) +"" slice "" +str(k)+"" =""+str(current))

**prediction code

```
            //image for prediction
             input_slice_test = np.float32(input_image_test[15]) / 255.0 


            input_image_test = np.expand_dims(input_slice_test, axis=0)
            print(""input     "", input_image_test)
         //prediction    
            output_image = sess.run(network, feed_dict={net_input:input_image_test})
            print(""output"", output_image)`
```

the value of accuracy look logic it converge after each epoch but when i try to predict an image after training all the epochs, the value of predicted array output look like this:

//result of prediction
[[[[4.4425573e-02 8.8295966e-01]
[5.4390044e-03 9.7242403e-01]
[1.9392670e-03 9.8569876e-01]

and the image is full white.
any help please!!"
