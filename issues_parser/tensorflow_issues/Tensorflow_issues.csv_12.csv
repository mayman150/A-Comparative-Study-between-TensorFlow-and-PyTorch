Issue Number,Issue Title,Issue Body
50685,resnet50 inference result is wrong with tensorflow+oneDNN,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 x86_64
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4
- Python version: Python 3.7.9
- Bazel version (if compiling from source): Build label: 3.1.0
- GCC/Compiler version (if compiling from source): 5.4.0
- CUDA/cuDNN version: run on CPU
- GPU model and memory: run on CPU

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
we do not use these JIT primitives: convolution,jit:sse41, convolution,jit_dw:sse41, and bnorm_jit:msa by  comment out the configurations. We rebuild tensorflow 2.4 with oneDNN and run resnet50, but  the inference result is incorrect: 
Predicted: [[('n02672831', 'accordion', 0.045002583), ('n03976657', 'pole', 0.03239749), ('n02966193', 'carousel', 0.029394835), ('n03063599', 'coffee_mug', 0.028039759), ('n02909870', 'bucket', 0.023439417)]]
predict_result:  accordion

**Describe the expected behavior**
the expected result is:
Predicted: [[('n02124075', 'Egyptian_cat', 0.9613135), ('n02127052', 'lynx', 0.021235), ('n02123597', 'Siamese_cat', 0.010190687), ('n02123159', 'tiger_cat', 0.0036373248), ('n02123045', 'tabby', 0.0015628876)]]
predict_result:  Egyptian_cat

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
1、we found that result is correct when running tensorflow in single thread configuration;
2、result is also correct when open  convolution,jit:sse41
"
50684,element_spec in tf.data.experimental.load - Variable Image Shapes,"I am facing an issue while using 
```
check_val_ds = tf.data.experimental.load(val_ds_path , 
                                                                  element_spec=(tf.TensorSpec(shape=(224, 224, 3), dtype=tf.uint8, name=None),
                                                                   tf.TensorSpec(shape=(), dtype=tf.int64, name=None)))
```
Here the syntax requires element_spec argument to be filled, while I have images of variable shapes in the dataset, how do I mention the shape in such case.

Please suggest.


"
50683,Add support for WebP decoding in tf.io.decode_image,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): yes

**Describe the feature and the current behavior/state.**

tf.io.decode_image does not support WebP decoding. This makes using tfds and loading lsun/car impossible to use due to the fact that all images are encoded in WebP. 

I'm somewhat surprised that DecodeImageV2Op doesn't support this relatively popular file format.

**Will this change the current api? How?**
No

**Who will benefit with this feature?**
All researchers trying to move away from using JPEG

**Any Other info.**
The code needed to decode WebP already exists in Tensorflow in:
https://www.tensorflow.org/io/api_docs/python/tfio/image/decode_webp
"
50681,Provide XLA as a standalone library,"**System information**
- TensorFlow version (you are using): n/a
- Are you willing to contribute it (Yes/No): Maybe, depends on the complexity of the problem (my C++ knowledge is minimal).

**Describe the feature and the current behavior/state.**
XLA is packaged with TensorFlow. I'd like to be able to install it as a standalone library.

**Will this change the current api? How?**
I don't really know. It may change the location of XLA files, but not the functionality itself.

**Who will benefit with this feature?**
People using XLA without TensorFlow, such as (possibly) JAX, but also my project."
50678,"Keras model compiled with custom loss raises ""Cannot convert a symbolic Keras input/output to a numpy array"" error.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
When trying to train a keras model compiled with a custom loss, model.train_on_batch raises the following exception (full traceback is below):
```  
TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.
```
This error only occurs in the `gradient_penalty_loss` and not in the `wasserstein_loss` which is also a custom loss function

**Describe the expected behavior**
The `gradient_penalty_loss` should accept an extra argument like seen **[here](https://github.com/tensorflow/tensorflow/issues/38319#issuecomment-610475806)**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
I am a bit new to the topic I am working on so I couldn't reduce the code further than this but i included `TODO`-comments to mark the important lines of code
```
from __future__ import print_function, division

import numpy as np
import tensorflow.keras.backend as K
import tensorflow.keras.layers
from tensorflow.keras.layers import BatchNormalization, Bidirectional, LSTM
from tensorflow.keras.layers import Input, Dense, Reshape
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import RMSprop
from tensorflow.python.keras.layers import LeakyReLU
from tensorflow.python.keras.layers.merge import _Merge


class RandomWeightedAverage(_Merge):
    """"""Provides a (random) weighted average between real and generated trajectory samples""""""

    def _merge_function(self, inputs):
        alpha = K.random_uniform((1, 144, 1))
        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])

def gradient_penalty_loss(averaged_samples):
    def loss(y_true, y_pred):
        """"""
        Computes gradient penalty based on prediction and weighted real / fake samples
        """"""
        gradients = K.gradients(y_pred, averaged_samples)[0]
        # compute the euclidean norm by squaring ...
        gradients_sqr = K.square(gradients)
        #   ... summing over the rows ...
        gradients_sqr_sum = K.sum(gradients_sqr,
                                  axis=np.arange(1, len(gradients_sqr.shape)))
        #   ... and sqrt
        gradient_l2_norm = K.sqrt(gradients_sqr_sum)
        # compute lambda * (1 - ||grad||)^2 still for each single sample
        gradient_penalty = K.square(1 - gradient_l2_norm)
        # return the mean as loss over all the batch samples
        return K.mean(gradient_penalty)

    return loss


def wasserstein_loss(y_true, y_pred):
    return K.mean(y_true * y_pred)


class WGANGP():
    def __init__(self):
        self.max_length = 144
        self.features = 1
        self.traj_shape = (self.max_length, self.features)
        self.latent_dim = 100

        # Following parameter and optimizer set as recommended in paper
        self.n_discriminator = 5
        optimizer = RMSprop(learning_rate=0.00005)

        # Build the generator and discriminator
        self.generator = self.build_generator()
        self.discriminator = self.build_discriminator()

        # -------------------------------
        # Construct Computational Graph
        #       for the Discriminator
        # -------------------------------

        # Freeze generator's layers while training discriminator
        self.generator.trainable = False

        # Trajectory input (real sample)
        real_traj = Input(shape=self.traj_shape)

        # Noise input
        noise_d = Input(shape=(self.latent_dim,))
        # Generate trajectory based of noise (fake sample)
        fake_traj = self.generator(noise_d)

        # Discriminator determines validity of the real and fake trajectories
        fake = self.discriminator(fake_traj)
        valid = self.discriminator(real_traj)

        # Construct weighted average between real and fake trajectories
        interpolated_traj = RandomWeightedAverage()([real_traj, fake_traj])
        # Determine validity of weighted sample
        validity_interpolated = self.discriminator(interpolated_traj)

        self.discriminator_model = Model(inputs=[real_traj, noise_d],
                                         outputs=[valid, fake, validity_interpolated])
        self.discriminator_model.compile(loss=[wasserstein_loss,
                                               wasserstein_loss,
                                               gradient_penalty_loss(averaged_samples=interpolated_traj)],
                                         optimizer=optimizer,
                                         loss_weights=[1, 1, 10])
        # -------------------------------
        # Construct Computational Graph
        #         for Generator
        # -------------------------------

        # For the generator we freeze the discriminator's layers
        self.discriminator.trainable = False
        self.generator.trainable = True

        # Sampled noise for input to generator
        noise_gen = Input(shape=(self.latent_dim,))
        # Generate trajectory based of noise
        traj = self.generator(noise_gen)
        # Discriminator determines validity
        valid = self.discriminator(traj)
        # Defines generator model
        self.generator_model = Model(noise_gen, valid)
        self.generator_model.compile(loss=wasserstein_loss, optimizer=optimizer)

    def build_generator(self):

        model = Sequential()

        model.add(Dense(256, input_dim=self.latent_dim))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(1024))
        model.add(LeakyReLU(alpha=0.2))
        model.add(BatchNormalization(momentum=0.8))
        model.add(Dense(np.prod(self.traj_shape), activation='tanh'))
        model.add(Reshape(self.traj_shape))

        model.summary()

        noise = Input(shape=(self.latent_dim,))
        img = model(noise)

        return Model(noise, img)

    def build_discriminator(self):

        model = Sequential()

        model.add(LSTM(512, input_shape=self.traj_shape, return_sequences=True))
        model.add(Bidirectional(LSTM(512)))
        model.add(Dense(512))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(256))
        model.add(LeakyReLU(alpha=0.2))
        model.add(Dense(1, activation='tanh'))

        model.summary()

        traj = Input(shape=self.traj_shape)
        validity = model(traj)

        return Model(traj, validity)

    def train(self, epochs, batch_size, sample_interval=50):
        # Training data
        X_train = np.load('data/preprocessed/train.npy', allow_pickle=True)

        # Adversarial ground truths
        valid = -np.ones((batch_size, 1))
        fake = np.ones((batch_size, 1))
        dummy = np.zeros((batch_size, 1))  # Dummy gt for gradient penalty
        for epoch in range(epochs):

            for _ in range(self.n_discriminator):
                # ---------------------
                #  Train Discriminator
                # ---------------------

                # Select a random batch of trajectories
                idx = np.random.randint(0, X_train.shape[0], batch_size)
                trajs = X_train[idx]
                # Sample generator input
                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))
                # Train the discriminator
                #TODO: Error appears here after the first iteration
                d_loss = self.discriminator_model.train_on_batch([trajs, noise],
                                                                 [valid, fake, dummy])

            # ---------------------
            #  Train Generator
            # ---------------------

            g_loss = self.generator_model.train_on_batch(noise, valid)

            # Plot the progress
            print(""%d [D loss: %f] [G loss: %f]"" % (epoch, d_loss[0], g_loss))


if __name__ == '__main__':
    wgan = WGANGP()
    wgan.train(epochs=2000, batch_size=64, sample_interval=10)
```

**Other info / logs**
I attached some sample data to use to reproduce here: [train.zip](https://github.com/tensorflow/tensorflow/files/6785899/train.zip)
I mainly copied the code from **[here](https://github.com/keras-team/keras-contrib/blob/master/examples/improved_wgan.py)** where it seams to work using the partial function but that threw an error, so i applied the proposed changes from **[this issue](https://github.com/tensorflow/tensorflow/issues/38319#issuecomment-610475806)** but now i get the described error.
Full traceback of this issue:
```
Traceback (most recent call last):
  File ""C:\Users\test\AppData\Local\Programs\Python\Python38\lib\contextlib.py"", line 131, in __exit__
    self.gen.throw(type, value, traceback)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\ops\variable_scope.py"", line 2833, in variable_creator_scope
    yield
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1825, in train_on_batch
    logs = self.train_function(iterator)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 763, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 3050, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\function.py"", line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\eager\def_function.py"", line 672, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 986, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    C:\path\to\project\venv\lib\site-packages\tensorflow\python\keras\engine\training.py:855 train_function  *
        return step_function(self, iterator)
    CC:/path/to/project/lstm_wgan.py:37 loss  *
        gradients = K.gradients(y_pred, averaged_samples)[0]
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\keras\backend.py:4131 gradients  **
        return gradients_module.gradients(
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\ops\gradients_impl.py:169 gradients
        return gradients_util._GradientsHelper(
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\ops\gradients_util.py:531 _GradientsHelper
        xs = ops.internal_convert_n_to_tensor_or_indexed_slices(
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\indexed_slices.py:369 internal_convert_n_to_tensor_or_indexed_slices
        internal_convert_to_tensor_or_indexed_slices(
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\indexed_slices.py:330 internal_convert_to_tensor_or_indexed_slices
        return ops.convert_to_tensor(value, dtype=dtype, name=name, as_ref=as_ref)
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\profiler\trace.py:163 wrapped
        return func(*args, **kwargs)
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\ops.py:1566 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\constant_op.py:339 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\constant_op.py:264 constant
        return _constant_impl(value, dtype, shape, name, verify_shape=False,
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\constant_op.py:281 _constant_impl
        tensor_util.make_tensor_proto(
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\framework\tensor_util.py:435 make_tensor_proto
        values = np.asarray(values)
    C:\path\to\project\venv\lib\site-packages\numpy\core\_asarray.py:83 asarray
        return array(a, dtype, copy=False, order=order)
    C:\path\to\project\venv\lib\site-packages\tensorflow\python\keras\engine\keras_tensor.py:254 __array__
        raise TypeError(

    TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.

```
"
50676,"""Given shapes (...) are not broadcastable"" after conversion from tensorflow to tflite","### 1. System information

-    OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
-    TensorFlow installation (pip package or built from source): `tb-nightly==2.6.0a20210706` installed via pip


### 2. Code

I am trying to convert a model from Pytorch -> ONNX -> Tensorflow -> tflite. The conversion works, but the results diverge between Tensorflow and tflite.

The original model is a Faster-RCNN with a ResNet backbone (`torchvision.models.detection.fasterrcnn_resnet50_fpn` to be exact).

```python
import tensorflow as tf
import numpy as np

# tensorflow inference: works
model_tf = tf.saved_model.load('assets/tfsavedmodel')
sample = np.zeros((1, 3, 256, 256), dtype=np.float32)
model_tf(input=sample)  #works

# tflite inference: fails
interpreter = tf.lite.Interpreter('model-conversion/assets/model.tflite')
model_tflite = interpreter.get_signature_runner()
out_tflite = model_tflite(input=sample)
```

Fails with error:

```
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""/home/florian/projects/video-analysis-prototype/model-conversion/venv/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py"", line 231, in __call__
    self._interpreter.allocate_tensors()
  File ""/home/florian/projects/video-analysis-prototype/model-conversion/venv/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py"", line 453, in allocate_tensors
    return self._interpreter.AllocateTensors()
RuntimeError: Given shapes, [1, 199, 199, 256] and [1, 200, 200, 256], are not broadcastable.Node number 365 (ADD) failed to prepare.
```

Model files mentioned can be found here: https://florianletsch.de/media/assets-tflite.zip (468 MB)

The model was converted following the answer on that issue: https://github.com/tensorflow/tensorflow/issues/50635
**Conversion code** (full conversion code:  https://github.com/florianletsch/torch2tflite-failing):

```
converter = tf.lite.TFLiteConverter.from_saved_model('assets/tfsavedmodel')
converter.experimental_enable_resource_variables = True  # requires tf-nightly or tf-nightly-cpu
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS,
  tf.lite.OpsSet.SELECT_TF_OPS
]

tflite_model = converter.convert()
with open('assets/model.tflite', 'wb') as f:
    f.write(tflite_model)
```


### 3. Failure after conversion

- Conversion works but model can't be loaded for inference, see error message above.

### Environment 

Output of `pip freeze`

```
absl-py==0.13.0
astunparse==1.6.3
cachetools==4.2.2
certifi==2021.5.30
chardet==4.0.0
flatbuffers==1.12
gast==0.4.0
google-auth==1.32.1
google-auth-oauthlib==0.4.4
google-pasta==0.2.0
grpcio==1.38.1
h5py==3.1.0
idna==2.10
keras-nightly==2.6.0.dev2021062500
Keras-Preprocessing==1.1.2
libclang==12.0.0
Markdown==3.3.4
netron==5.0.0
numpy==1.19.2
oauthlib==3.1.1
onnx==1.9.0
onnx-tf==1.8.0
onnxruntime==1.8.0
opt-einsum==3.3.0
Pillow==8.3.1
protobuf==3.17.3
pyasn1==0.4.8
pyasn1-modules==0.2.8
PyYAML==5.4.1
requests==2.25.1
requests-oauthlib==1.3.0
rsa==4.7.2
six==1.15.0
tb-nightly==2.6.0a20210706
tensorboard-data-server==0.6.1
tensorboard-plugin-wit==1.8.0
tensorflow-addons==0.13.0
termcolor==1.1.0
tf-estimator-nightly==2.6.0.dev2021062501
tf-nightly-cpu==2.7.0.dev20210706
torch==1.9.0
torchvision==0.10.0
typeguard==2.12.1
typing-extensions==3.7.4
urllib3==1.26.6
Werkzeug==2.0.1
wrapt==1.12.1
```
"
50675,How to use weights to create a new op in Tensorflow,"I'd like to define a convolutional op by following this: https://www.tensorflow.org/guide/create_op. How can I access filter weights in the Compute method? In the call to the REGISTER_OP macro, should I define filter dimensions? Where the weights are updated?"
50674,BigBiGAN Loss Function,"Hi, 
I wanted to ask whether the BigBiGAN network contains KL Divergence Loss in the implementation? Because they have mentioned non-deterministic encoder architecture for sampling z vector. They used Variational Bayes for sampling as mentioned.

If anyone can help me out, it would be great!

Thanks in advance. 
"
50673,Module 'tensorflow._api.v2.summary' has no attribute 'scalar',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Custom code
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Red Hat 7.6
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary):
Binary
- TensorFlow version (use command below):
2.5.0
- Python version:
3.6.8
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
NA
- CUDA/cuDNN version:
- NA
- GPU model and memory:
NA

**Describe the current behavior**
When running one machine, when I execute the line `tf.summary.scalar(""train_ll_per_seq"", ll_per_seq)` I get `module 'tensorflow._api.v2.summary' has no attribute 'scalar'.  Interestingly enough, on another machine, also RHEL 7.6, the code executes as expected.

**Describe the expected behavior**
The above `scalar` should be recognized on both machines.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
`import tensorflow as tf`
``tf.summary.scalar(""train_ll_per_seq"", ll_per_seq)`
"
50670,Internal: libdevice not found at ./libdevice.10.bc,"
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
CentOS 7.4 

- TensorFlow installed from (source or binary): pip install 
- TensorFlow version: 1.15
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip 

- CUDA/cuDNN version:
Cudatoolkit 10.0.130, cuDNN 7.6.5.32

- GPU model and memory:
```
| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  Off  | 00000000:15:00.0 Off |                    0 |
| N/A   37C    P0    53W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |

```

**Describe the problem**

I'm trying to run my tensorflow based project on a cluster, I've installed all relevant dependencies within my anaconda environment the exact same way I did on my local machine where the project runs but I'm getting this error message:

    tensorflow.python.framework.errors_impl.InternalError: 2 root error(s) found.
      (0) Internal: libdevice not found at ./libdevice.10.bc
             [[{{node cluster_2_1/xla_compile}}]]
             [[cluster_1_1/merge_oidx_20/_1]]
      (1) Internal: libdevice not found at ./libdevice.10.bc
             [[{{node cluster_2_1/xla_compile}}]]



**Any other info / logs**

```
2021-06-30 08:27:50.484735: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:69] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.
    2021-06-30 08:27:50.484775: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:70] Searched for CUDA in the following directories:
    2021-06-30 08:27:50.484781: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   ./cuda_sdk_lib
    2021-06-30 08:27:50.484784: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   /usr/local/cuda
    2021-06-30 08:27:50.484787: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:73]   .
    2021-06-30 08:27:50.484791: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:75] You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work. 
```

This section of the traceback makes me think that tensorflow is searching for cuda locally instead of within the conda environment, to fix this do I need to set the XLA_FLAGS to `/u/usr/anaconda3/envs/Project_BM/lib/libdevice.10.bc`, if not where can I find the `/cuda/` directory within the `Project_BM` environment? 

Its also worth knowing that I'm running this on a cluster so I don't have root permissions.

Full traceback - https://pastebin.com/njqNFWvC
"
50669,Internal Error with TF_GPU_ALLOCATOR=cuda_malloc_async,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Pop-OS 20.04
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.9.5
- CUDA/cuDNN version: CUDA 11.4 / cuDNN 8.2.2
- GPU model and memory: RTX 3080

**Describe the current behavior**
When using the TF_GPU_ALLOCATOR=cuda_malloc_async, TF throws an internal error after allocation of GPU: 
```
2021-07-08` 12:44:26.553800: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-08 12:44:27.009583: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-07-08 12:44:27.034925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.035193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:2d:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.76GiB deviceMemoryBandwidth: 707.88GiB/s
2021-07-08 12:44:27.035207: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-08 12:44:27.036831: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-07-08 12:44:27.036855: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-07-08 12:44:27.037745: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-07-08 12:44:27.037863: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-07-08 12:44:27.038095: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2021-07-08 12:44:27.038451: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-07-08 12:44:27.038515: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-07-08 12:44:27.038573: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.038841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.039405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-07-08 12:44:27.039873: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-07-08 12:44:27.040284: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.040520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:2d:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6
coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.76GiB deviceMemoryBandwidth: 707.88GiB/s
2021-07-08 12:44:27.040556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.040800: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.041145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-07-08 12:44:27.041164: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-08 12:44:27.296173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-07-08 12:44:27.296199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-07-08 12:44:27.296206: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-07-08 12:44:27.296339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.296598: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.296835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-07-08 12:44:27.297045: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:210] Using CUDA malloc Async allocator for GPU.
2021-07-08 12:44:27.297081: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
Traceback (most recent call last):
  File ""/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/./training.py"", line 424, in <module>
    log_writer = tf.summary.create_file_writer(logdir) if log else None
  File ""/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/ops/summary_ops_v2.py"", line 479, in create_file_writer_v2
    with ops.name_scope(name, ""create_file_writer"") as scope, ops.device(""cpu:0""):
  File ""/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/framework/ops.py"", line 5255, in device
    return context.device(device_name_or_function)
  File ""/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 2072, in device
    ensure_initialized()
  File ""/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 1867, in ensure_initialized
    context().ensure_initialized()
  File ""/home/sebltm/OneDrive/KCL/Individual_Project/FaceCapsNet/facecapsnet/lib/python3.9/site-packages/tensorflow/python/eager/context.py"", line 525, in ensure_initialized
    context_handle = pywrap_tfe.TFE_NewContext(opts)
tensorflow.python.framework.errors_impl.InternalError: No allocator statistics
```"
50668,Can't use preprocess_input with mixed precision and Lambda layer,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac and Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: None
- GPU model and memory: None

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I recently upgraded tensorflow from 2.4 to 2.5 and I have a break in my code that didn't happen before. It is related to the use of mixed precision and preprocessing functions in tf.keras.applications. When using both with a Lambda layer:
```python
input = tf.keras.Input((224, 224, 3), name=""image"")
tf.keras.layers.Lambda(tf.keras.applications.densenet.preprocess_input, name=""x_preprocess"")(input)
```
, I have the following error:
```bash
TypeError: x and y must have the same dtype, got tf.float16 != tf.float32
```
You can find a minimal example [here](https://colab.research.google.com/drive/1BrYCu4Bo7Y0D7_MaLbDlFW1_uHzaRuGE?usp=sharing)

The issue isn't raised if I don't use Lambda:
```python
input = tf.keras.Input((224, 224, 3), name=""image"")
tf.keras.applications.densenet.preprocess_input(input)
```
I can use this solution, but the Lambda layer was useful to group all the preprocessing operations into a single layer.

**Describe the expected behavior**

Before TF 2.5, this worked, so I assume this should still work

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

https://colab.research.google.com/drive/1BrYCu4Bo7Y0D7_MaLbDlFW1_uHzaRuGE?usp=sharing
"
50667,Converting Boosted tree model to tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): mac os Big Sur
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.4.1

### 2. Code

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

`converter = tf.lite.TFLiteConverter.from_saved_model('edr_cust_model_new/1625736127/') # path to the SavedModel directory
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

tflite_model = converter.convert()


// Save the model.
with open('edr_cust_model_temp/model.tflite', 'wb') as f:
  f.write(tflite_model)`

### 3. Short Error log
I am getting following error while conversion:

ConverterError: <unknown>:0: error: loc(""boosted_trees""): 'tf.BoostedTreesEnsembleResourceHandleOp' op is neither a custom op nor a flex op
<unknown>:0: error: loc(""boosted_trees/BoostedTreesPredict""): 'tf.BoostedTreesPredict' op is neither a custom op nor a flex op
<unknown>:0: error: loc(""boosted_trees/head/predictions/str_classes""): 'tf.AsString' op is neither a custom op nor a flex op
<unknown>:0: error: failed while converting: 'main': Ops that need custom implementation (enabled via setting the -emit-custom-ops flag):
	tf.AsString {device = """", fill = """", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}
	tf.BoostedTreesEnsembleResourceHandleOp {container = """", device = """", shared_name = ""boosted_trees/""}
	tf.BoostedTreesPredict {device = """", logits_dimension = 7 : i64, num_bucketized_features = 18 : i64}


### 4. For detailed error, please refer to this file 
[tf_boosted_tree_convert_error_log.txt](https://github.com/tensorflow/tensorflow/files/6783426/tf_boosted_tree_convert_error_log.txt)
"
50666,Quadratic runtime behavior in model.fit/predict with tf.keras.layers.experimental.preprocessing.TextVectorization,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
O(n^2) runtime behavior when calling model.fit or model.predict with a single TextVectorization layer. E.g. calling predict 2 times with n/2 vectors is twice as fast compared to calling predict one time with n vectors. For model.fit the batch_size seems to reduce the runtime linearly. Runtime seems to be O(n * number_of_iterations).

**Describe the expected behavior**
The runtime should be linear with n (O(n))

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np

print(tf.__version__)

# generate some dummy data
n = 1000000
x = np.random.randint(1, 100, (n, 1)).astype(str)
y = np.random.uniform(0, 1, (n, 1))

input = tf.keras.layers.Input(shape=(1,), dtype=tf.string)
vectorization = tf.keras.layers.experimental.preprocessing.TextVectorization()
vectorization.adapt(np.unique(x))
output = vectorization(input)
model = tf.keras.Model(inputs=[input], outputs=[output])
model.compile(loss=""mse"", optimizer=""adam"")

# compare runtime of model.fit with different batch_size
# -> for large n, doubling the batch_size halves the execution time!!!
model.fit(x, y, epochs=1,batch_size=32)
model.fit(x, y, epochs=1,batch_size=64)
model.fit(x, y, epochs=1,batch_size=8192)

# compare runtime of model.predict with different input size
# -> for large n, calling predict 2 times with n/2 is twice as fast!!!
pred = model.predict(x,verbose=1)
pred = model.predict(x[:int(n/2)],verbose=1)
pred = model.predict(x[int(n/2+1):],verbose=1)
```

**Other info / logs** 
Observed runtimes. Absolute values may depend on your hardware. Ratios between runtimes should be reproducible.
```
31250/31250 [==============================] - 207s 7ms/step - loss: 3416.8721 (model.fit with batch_size = 32)
15625/15625 [==============================] - 121s 8ms/step - loss: 3416.8367 (model.fit with batch_size = 64)
123/123 [==============================] - 1s 10ms/step - loss: 3416.8489 (model.fit with batch_size = 8192)
31250/31250 [==============================] - 221s 7ms/step (model.predict on n vectors)
15625/15625 [==============================] - 56s 4ms/step (model.predict on n/2 vectors)
15625/15625 [==============================] - 59s 4ms/step (model.predict on n/2 vectors)

```"
50665,Can tf.function fully support control flow,"Hi， In eager mode, control flow is fully supported;
however, in graph mode, Does tf2.x fully support control flow. if so,  we write whether the control flow can be expressed in python statements, or tf2.x specific API 。

"
50663,[Performance] Sparse Operation is tf.keras is super slow,"I am training a network using sparse operation.  The input matrix is large (4656 columns), the output (following) matrix is a dense one with 2328 outputs. Average density of the input was just 0.5%

Despite any methods works on GPU (Quadro P1000), either the input was scipy.COO, scipy.CSR or tf.SparseTensor, the performance was dropped by half compared to dense input , even I have activated the sparse=True in the input layer.

Timing: 
Input(sparse=False) -> ~ 20s
Input(sparse=True) -> ~ 44s

Input Matrix: [50000, 4656] -> Output: [50000, 2328]

TF Version: 2.5
CudaToolkit: 11.2
cuDNN: 8.2"
50662,transpose op code behavior mismatch with comments in xla,"tensorflow 1.15.5
https://github.com/tensorflow/tensorflow/blob/v1.15.5/tensorflow/compiler/xla/service/shape_inference.cc#L2740

```c++
  // Permute(dimensions,input) computes output[dimensions[i]]=input[i]. However,
  // we need output[i]=input[dimensions[i]] which is
  // Permute(Inverse(dimensions),input).
  return ShapeUtil::PermuteDimensions(InversePermutation(dimensions), operand);
```

https://github.com/tensorflow/tensorflow/blob/v1.15.5/tensorflow/compiler/xla/util.h#L340

```c++
// Applies `permutation` on `input` and returns the permuted array.
// For each i, output[permutation[i]] = input[i].
//
// Precondition:
// 1. `permutation` is a permutation of 0..permutation.size()-1.
// 2. permutation.size() == input.size().
template <typename Container>
std::vector<typename Container::value_type> Permute(
    absl::Span<const int64> permutation, const Container& input) {
  using T = typename Container::value_type;
  absl::Span<const T> data(input);
  CHECK(IsPermutation(permutation, data.size()));
  std::vector<T> output(data.size());
  for (size_t i = 0; i < permutation.size(); ++i) {
    output[permutation[i]] = data[i];
  }
  return output;
}
```

my question is 
accoding to the comments, **if using inverse permutation, we should use output[i]=input[dimensions[i]].**
**but, code still use  output[dimensions[i]]=input[i].**

seems like a bug or mismatch?
I checked latest code, this is still there, but inverse moved to other place.
so I used 1.15.5, as the inverse code is together with permute.



BR.
Mingting

"
50661,Calculation of gradient of 2D convolution operation through GradientTape returns None,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Mint 20.1
- TensorFlow installed from (source or binary): conda
- TensorFlow version (use command below): 2.4.1
- Python version: 3.9.4
- CUDA/cuDNN version: 10.1.243, 7.6.5 (both through conda)
- GPU model and memory: GeForce RTX 2060 Rev. A, 6GB

**Describe the current behavior**
Currently, I am trying to use `GradientTape` to capture the gradient of a 2D convolution operation `tf.nn.conv2d` on a test matrix. However, when I go to actually fetch the gradients, `tape.gradient()` returns `None` instead of the expected gradient. I tested the exact same code, except I disabled eager execution and didn't use `GradientTape`, and it returned the gradients just fine.

**Describe the expected behavior**
When using `tf.nn.conv2d` inside of `GradientTape`, TensorFlow should successfully calculate the gradients.

**Standalone code to reproduce the issue**
This code works:
```
import os
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
import numpy as np
import tensorflow as tf
tf.compat.v1.disable_eager_execution()

# sizes, fixed strides, in_channel, out_channel be 1 for now
x_size = 4
w_size = 3  # use an odd number here
x_shape = (1, x_size, x_size, 1)
w_shape = (w_size, w_size, 1, 1)
out_shape = (1, x_size - w_size + 1, x_size - w_size + 1, 1)
strides = (1, 1, 1, 1)

# numpy value
x_np = np.random.randint(10, size=x_shape)
w_np = np.random.randint(10, size=w_shape)
out_scale_np = np.random.randint(10, size=out_shape)

# tf forward
x = tf.constant(x_np, dtype=tf.float32)
w = tf.constant(w_np, dtype=tf.float32)
out = tf.nn.conv2d(input=x, filters=w, strides=strides, padding='VALID')
out_scale = tf.constant(out_scale_np, dtype=tf.float32)
f = tf.reduce_sum(tf.multiply(out, out_scale))

# tf backward
d_out = tf.gradients(f, out)[0]
d_x = tf.gradients(f, x)[0]
d_w = tf.gradients(f, w)[0]

print(d_out, d_x, d_w)
```

This code does not:
```
import os
os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'
import numpy as np
import tensorflow as tf

# sizes, fixed strides, in_channel, out_channel be 1 for now
with tf.GradientTape() as g:
    x_size = 4
    w_size = 3  # use an odd number here
    x_shape = (1, x_size, x_size, 1)
    w_shape = (w_size, w_size, 1, 1)
    out_shape = (1, x_size - w_size + 1, x_size - w_size + 1, 1)
    strides = (1, 1, 1, 1)

    # numpy value
    x_np = np.random.randint(10, size=x_shape)
    w_np = np.random.randint(10, size=w_shape)
    out_scale_np = np.random.randint(10, size=out_shape)

    # tf forward
    x = tf.constant(x_np, dtype=tf.float32)
    w = tf.constant(w_np, dtype=tf.float32)
    out = tf.nn.conv2d(input=x, filters=w, strides=strides, padding='VALID')
    out_scale = tf.constant(out_scale_np, dtype=tf.float32)
    f = tf.reduce_sum(tf.multiply(out, out_scale))

# tf backward
d_out = g.gradient(f, out)[0]
d_x = g.gradient(f, x)[0]
d_w = g.gradient(f, w)[0]

print(d_out, d_x, d_w)
```"
50660,Fail to converted Tensorflow models to TensorFlow Lite,"### 1. System information

- OS Platform and Distribution (Linux Ubuntu 20.04):
- TensorFlow installation (pip package):
- TensorFlow library (tensorflow2.5):

### 2. Code
Model just use DNNLinearCombinedClassifier, when I converter .pb to tflite, it failed.

Model: https://drive.google.com/file/d/18FTOAx_HJlbnX0HQYqOwzvXdAm8dzyCL/view?usp=sharing
#### Way1
```
saved_model_obj = tf.saved_model.load(export_dir=model_path)

# Load the specific concrete function from the SavedModel.
concrete_func = saved_model_obj.signatures['serving_default']
# Set the shape of the input in the concrete function.
concrete_func.inputs[0].set_shape((None,))

# Convert the model to a TFLite model.
converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_func])
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()
with open('model.tflite','wb') as f:
    f.write(tflite_model)
```
**Error:** 
E        ValueError: Failed to import metagraph, check error log for more info.

#### Way2
```
model = tf.saved_model.load(model_path)
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir=model_dir,signature_keys=['serving_default','predict','classification'])
tflite_model = converter.convert()

with open('model.tflite','wb') as f:
    f.write(tflite_model)
```
**Error:**      
>       raise ValueError(""Only support a single signature key."")
E       ValueError: Only support a single signature key.
"
50659,AttributeError: '_UserObject' object has no attribute 'add_slot',"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 11.4
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): built on tf 2.5.0 converted on tf_nightly 2.7.0-dev20210707

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.target_spec.supported_types = [tf.float16]
    tflite_quant_model = converter.convert()


### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Traceback (most recent call last):
  File ""/Users/redjyve/PycharmProjects/particleflow/quantization_convert_test.py"", line 12, in <module>
    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
  File ""/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/lite/python/lite.py"", line 1353, in from_saved_model
    saved_model = _load(saved_model_dir, tags)
  File ""/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 863, in load
    result = load_internal(export_dir, tags, options)[""root""]
  File ""/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 902, in load_internal
    loader = loader_cls(object_graph_proto, saved_model_proto, export_dir,
  File ""/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 162, in __init__
    self._load_all()
  File ""/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 259, in _load_all
    self._load_nodes()
  File ""/Users/redjyve/PycharmProjects/particleflow/venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py"", line 448, in _load_nodes
    slot_variable = optimizer_object.add_slot(
AttributeError: '_UserObject' object has no attribute 'add_slot'
"
50658,Tensorflow 2.3.x link error when use delay load,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 / Visual Studio 2017
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.0~2.3.3
- Python version: N/A
- CUDA/cuDNN version: cuda 10.1 cudnn 7.6.5
- GPU model and memory: NVIDIA Quadro P4000

**Describe the current behavior**
I try to use delay load tensorflow and it work for TF 1.15.0 and 2.4.0 but not for all 2.3.x version.
Note I need stick with TF 2.3.x and error in build Release not Debug
Get link error as:
Error	LNK1194	cannot delay-load 'tensorflow.dll' due to import of data symbol '__imp_TF_DeleteStatus'; link without /DELAYLOAD:tensorflow.dll

`
#include <windows.h>
#include <delayimp.h>
#include <mutex>

namespace
{
    bool tf_useGPU = true;// true;
    bool tf_dllLoaded = false;

    FARPROC WINAPI DelayLoadHook(unsigned dliNotify, PDelayLoadInfo pdli)
    {
        switch (dliNotify)
        {
            //case dliNotePreLoadLibrary:
        case dliFailLoadLib:
            if (lstrcmpiA(pdli->szDll, ""tensorflow.dll"") == 0)
            {
                FARPROC p = NULL;
                if (tf_useGPU)
                {
                    // Disable the error window if the dll cannot be loaded
                    // Save the original mode and restore it after the load
                    UINT origin_mode = SetErrorMode(SEM_FAILCRITICALERRORS);

                    // Load the dll
                    p = (FARPROC)LoadLibraryA(""tensorflow_gpu.dll"");

                    // Restore the original mode.
                    SetErrorMode(origin_mode);

                    if (p)
                    {
                        std::cout << ""TensorFlow GPU dll loaded\n"" << std::endl;
                    }
                }

                if (!p)
                {
                    p = (FARPROC)LoadLibraryA(""tensorflow_cpu.dll"");

                    if (p)
                    {
                        std::cout <<""TensorFlow CPU dll loaded\n"" << std::endl;
                    }
                }

                tf_dllLoaded = p != NULL;

                return p;
            }
        default:
            return NULL;
        }

        return NULL;
    }

    extern ""C"" const PfnDliHook __pfnDliFailureHook2 = DelayLoadHook;
}

void LoadTensorFlow(int iGPUOption)
{
    // dll can only be loaded once
    if (tf_dllLoaded)
    {
        assert(0);
        return;
    }
    tf_useGPU = iGPUOption != 0;

    // Call any tensorflow function to trigger the dll load
    const char* version = TF_GetVersion();
}
`

**Describe the expected behavior**
Link without error as other version of Tensorflow."
50656,Which is the smallest SSD model to use in a MCU like ESP-EYE and using tensorflow lite micro?,"Hi,

I am trying to work with the smallest SSD model of tensorflow, but my problem is that I am working with an [ESP-EYE](https://www.espressif.com/en/products/devkits/esp-eye/overview) and this MCU has only 4 MB of memory.

All SSD models that I saw in the [pretrained models Garden of tensorflow](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md), are too big (more than 4 MB) or they dont have a "".tflite"" file. 
In this last case, I saw that there is a way to convert the file "".pb"" to "".tflite"" [HERE](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/r1/convert/python_api.md#convert-a-frozen-graphdef-from-file-) and a video where explain which parameters you must use [VIDEO](https://youtu.be/EAE-vjL2Nps?t=238), but the size of the files increase a lot.
For example, if the size of a "".pb"" file is 5 MB, the conversion makes a file of 25 MB and things like this.

Here is a example of the code in python that I use for the conversion:



```python
import tensorflow as tf

# Convert the model.
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
    graph_def_file='tflite/tflite_graph.pb', # Dir of the "".pb"" file
    input_arrays=['normalized_input_image_tensor'],
    input_shapes={'normalized_input_image_tensor' : [1, 300, 300,3]}, # Shape of the images of the model you will need to change the 300 and 300
    output_arrays=['TFLite_Detection_PostProcess', 'TFLite_Detection_PostProcess:1', 'TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'],
)
converter.allow_custom_ops=True

# converter.quantized_input_stats = {'input' : (0., 1.)}  # mean, std_dev (input range is [-1, 1])
# converter.inference_type = tf.int8 # this is the recommended type.
# converter.inference_input_type=tf.uint8 #optional
# converter.inference_output_type=tf.uint8 #optional
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

# Save the model.
with open('quantized_model.tflite', 'wb') as f:
  f.write(tflite_model)
```

Soo my questions are:

1. Do you know some SSD model with a size less than 4 MB, for example, 3MB or 2MB, with the capability to detect persons?
2. Is posible to work with a SSD model and make the ""Invoke""(prediction) of the model in the ESP-EYE or is a lot of work for a MCU?
I found some documentation, but I´m not sure if it works:
- [Documentation of tensorflow](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/cc/task#object-detector)
- [Code](https://github.com/tensorflow/tflite-support/tree/master/tensorflow_lite_support/cc/task#object-detector)

Thanks for read and have a good day.

See you XD
"
50654,Tensorflow int32 matmul slower than float32 matmul,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Centos 7.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.15.4
- Python version: 3.7
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v1.15.3-68-gdf8c55c 1.15.4

**Describe the current behavior**
int32 matmul is almost twice as slow as float32 matmul for large-ish matrices.

**Describe the expected behavior**
int32 multiplication is typically faster on CPU, so I expect matmul speed should be at least close to that of float32. 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import tensorflow as tf
import time
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
for dtype in [tf.float32, tf.int32, tf.float32, tf.int32]:
  a = tf.constant([list(range(int(1e6), int(1e6) + 1024)) for _ in range(1024)], dtype=dtype)
  start = time.time()
  with tf.Session() as sess:
    for _ in range(100):
        sess.run(tf.matmul(a, a))
  end = time.time()
  print(dtype, end - start)
```
print result:

> <dtype: 'float32'> 5.6830058097839355
> <dtype: 'int32'> 7.759643077850342
> <dtype: 'float32'> 3.646965742111206
> <dtype: 'int32'> 8.087013483047485

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50653,exporter_main_v2.py on official TF2 OD checkpoints produces saved_model.pb different than official saved_model.pb,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.3.0
- Python version: 3.6.9
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): -
- CUDA/cuDNN version: CUDA 11.4
- GPU model and memory: GeForce RTX 3090

I am trying to convert a re-trained TF2 object detection SSD MobilenetV2 model to a proprietary framework. I have successfully re-trained the network and it runs properly. However, I am having trouble with converting the saved_model.pb to the other framework. The conversion script from the SDK I am working with performs optimization on the saved_model.pb, using 'meta_optimizer.cc', which returns an empty graph after running through my re-trained model. I used '_exporter_main_v2.py_' to export my re-trained checkpoint to the saved_model.pb which I am having trouble with. 

The issue is not with my training or checkpoints, but with the exporting process from checkpoint to a saved_model.pb using '_exporter_main_v2.py_'. I know this because I downloaded the SSD MobilenetV2 model from the TF2 Zoo to test with it. I have no issue converting the official saved_model.pb file found in the official repo, but when I try to convert the official checkpoints found in the repo to a saved_model.pb using '_exporter_main_v2.py_', I face the same issue trying to convert the newly produced saved_model.pb file to the proprietary framework. This means that something wrong is happening when executing the 'exporter_main_v2.py' script.

**Describe the expected behavior**
The exported saved_model.pb file should not be different than the official saved_model.pb file found in the official repo. 

The following is what I get, showing 0 nodes and 0 edges
![grappler_empty_graph](https://user-images.githubusercontent.com/84783887/124790681-5c42f300-df19-11eb-9201-7c5258e8d59b.png)

**Standalone code to reproduce the issue**
The model I downloaded is: http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz 

The command I used to export the official checkpoint to a saved_model.pb is: **python ~/models/research/object_detection/exporter_main_v2.py --input_type image_tensor --pipeline_config_path pipeline.config --trained_checkpoint_dir checkpoint/ --output_directory exported_model/**
"
50652,"Quantized Version of tf-lite model returning ""ERROR: Didn't find op for builtin opcode 'CONV_2D' version '5'"" when measuring performance","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy A50
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.7.2

**Describe the current behavior**

Currently following along this [guide](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/tools/benchmark/android) to measure performance of tf-lite models. When doing this: 

`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/non-quantized.tflite --enable_op_profiling=true `

I am able to successfully return results like this: 

`Average inference timings in us: Warmup: 432930, Init: 35157, no stats: 387831`

However, when I'm using a quantized version of the model:

`adb shell /data/local/tmp/benchmark_model --graph=/data/local/tmp/quantized.tflite --enable_op_profiling=true` 

I am getting this error

```
Loaded model /data/local/tmp/quantized.tflite
resolved reporter
ERROR: Didn't find op for builtin opcode 'CONV_2D' version '5'

ERROR: Registration failed.

Failed to construct interpreter
Aborted
```

"
50650,OpKernelConstruction allocate tensor that can persistent for Compute ,"How can we allocate a tensor or buffer that could exist among each Compute invocation?  If so, how can we free them after usage? 

Based on the description in op_kernel.h, `allocate_temp` seems to only allow temporary tensor allocation during the construction. Is this correct?

Thanks a lot. "
50649,same host memory address copy tensor to different device memory address,"I have a question. I export TF_CPP_MIN_VLOG_LEVEL=3 want to dump the log, then I find something I can't understand. I saw tensorflow from the same host memory address copy tensor to different device memory address, I want to know why does tensor flow do this? What are the advantages?
I think it will rely on CUDA Events for synchronization. Why not from different host memory  address?  
Looking forward to a reply.

![tf_log](https://user-images.githubusercontent.com/27774893/124728011-d0918c80-df41-11eb-8168-598cf9803599.jpg)

"
50648,Input shapes incompatible when using `tensorrt.Converter`,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Using Colab (with GPU) and TensorFlow 2.5

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

I have an image classification model that takes 224x224x3 inputs. Now when I am trying to convert it to TensorRT with the pre-built engine options enabled it's running into:

```python
/usr/local/lib/python3.7/dist-packages/tensorflow/python/compiler/tensorrt/trt_convert.py in build(self, input_fn)
   1187       if not first_input:
   1188         first_input = inp
-> 1189       func(*map(ops.convert_to_tensor, inp))
   1190 
   1191     if self._need_trt_profiles():

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   1709       TypeError: If the arguments do not match the function's signature.
   1710     """"""
-> 1711     return self._call_impl(args, kwargs)
   1712 
   1713   def _call_impl(self, args, kwargs, cancellation_manager=None):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/wrap_function.py in _call_impl(self, args, kwargs, cancellation_manager)
    245     else:
    246       return super(WrappedFunction, self)._call_impl(
--> 247           args, kwargs, cancellation_manager)
    248 
    249   def prune(self, feeds, fetches, name=None, input_signature=None):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_impl(self, args, kwargs, cancellation_manager)
   1727             raise structured_err
   1728 
-> 1729       return self._call_with_flat_signature(args, kwargs, cancellation_manager)
   1730 
   1731   def _call_with_flat_signature(self, args, kwargs, cancellation_manager):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py in _call_with_flat_signature(self, args, kwargs, cancellation_manager)
   1749           ""{} takes {} positional arguments but {} were given"".format(
   1750               self._flat_signature_summary(), self._num_positional_args,
-> 1751               len(args)))
   1752     args = list(args)
   1753     kwargs = dict(kwargs)

TypeError: pruned(input_1) takes 1 positional arguments but 224 were given
```

**Describe the expected behavior**

It's not clear as of now. [The documentation](https://www.tensorflow.org/api_docs/python/tf/experimental/tensorrt/Converter) does mention about the specifications but following those didn't help. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```python
params = tf.experimental.tensorrt.ConversionParams(
    precision_mode=""FP16"",
    maximum_cached_engines=16)
converter = tf.experimental.tensorrt.Converter(
    input_saved_model_dir=""classification_model"", conversion_params=params)
converter.convert()

def data_gen():
    for image in images[:100]:
        yield image

converter.build(input_fn=data_gen)  
converter.save(""tensorrt_embedding_model"")  
```

where, `classification_model` is the location of the SavedModel (generated from `tf.keras.applications.DenseNet121`). 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50647,Custommask warning when saving to h5 using built-in keras layers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ubuntu 18.04
- TensorFlow installed from (source or binary): pypi
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.5

**Describe the current behavior**
When saving a model to h5 format, and the model contains one of the following layers:
```
tf.keras.layers.experimental.preprocessing.Resizing()
tf.keras.layers.experimental.preprocessing.CenterCrop()
```

there is a:

`CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.`

When trying to load the keras model with tf.keras.models.load_model, it'll fail with:

`TypeError: ('Keyword argument not understood:', 'crop_to_aspect_ratio')`

**Describe the expected behavior**
It should both correctly save and load

**Standalone code to reproduce the issue**
"
50646,Problem resuming training with checkpoints in TF > 2.4.1,"**System information**
- Windows 10 Pro
- TF 2.4.1 and TF 2.5.0

**Describe the current behavior**
It is no longer possible to resume training with checkpoints in TF > 2.4.1. This is not the case in previous versions of TF. The following error is displayed: tensorflow ValueError: Tensor conversion requested dtype float32 for Tensor with dtype int32: <tf.Tensor: shape=(), dtype=int32, numpy=0>. It appears to be caused by OptimizerV2, where all the hyperparameters are assumed to be float when loading the adam optimizer, yet the optimizer saves some of the hyperparameters as integers. 

"
50645,The word vector obtained by the word2vec tutorial is very bad,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/text/word2vec

## Description of issue (what needs changing):

I run the word2vec code (without change) from the tutorial. But the word vector from the model is very bad, not reflect the semantic meaning at all.

Here is the result shown in the embedding projector:
![Selection_002](https://user-images.githubusercontent.com/1263428/124710517-7be51600-df2f-11eb-9dfb-7efd80bedeb6.png)

"
50644,TensorFlow Lite android compiling failed on Docker,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
I compile tensorflow lite in android mode according to the online guide: https://www.tensorflow.org/lite/guide/build_android

- the docker file is provided by tflite 
    - https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile

-  I entered into docker container but tflite compiling failed.
   - jdk version: 1.8
   - command : `bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite`

![image](https://user-images.githubusercontent.com/45189361/124571996-dbcdb500-de7a-11eb-8190-1d498580b2fb.png)




**Describe the problem**
- related issue [#50644 ]

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50643,[Adaptation] Indices of SparseTensor should adapt with current indices if (explicitly/implicitly) specified,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No):

I am using TensorFlow version 2.5.0 and try to adapt Sparse Tensor into Deep Learning model. However, interestingly that despite the indices has been specified by numpy, the indices always cast to uint64 despite our trial, even from numpy or tensorflow (tf.convert_to_tensor || tf.constant). As our matrix has its shape (10000, 7465), it is recommended to use uint16 as datatype indices rather than uint64. 

If indices was pass as Python List or Tuple, casting to uint64 is prefered, but if numpy array as input, it would be great to be modified with respective datatype with equivalent range rather than safely cast to uint64 as input

This implementation can help to reduce maximum of computing resources as storing indices may quite costly even if the matrix was not sparse enough. In my case, luckily, the data is uint8 with 2.24 % density so there are no problems about that. 

Example:
Current Version: Memory Cost: data (uint8), indices (uint64) -> Memory Cost: 38.08% compared to dense array
This implementation 1: data (uint8), indices (uint16) -> Memory Cost: 11.02% compared to dense array
This implementation 2: data (uint8), indices (uint32) -> Memory Cost: 20.16% compared to dense array

Moreover, it should be possible to make keras.Input(sparse=True) be compatible with Scipy.sparse rather than making warnings"
50642,RaggedTensor not broadcasting correctly,"Ubuntu 20.04, Tensorflow 2.5

A broadcast should be able to work across a RaggedTensor, so long as the dimension has fixed size. See the following example:

```python
import tensorflow as tf
BATCHES = 4
CHANNELS = 8

# Two methods to create an ""equivalent"" RaggedTensor
ragged_1 = tf.expand_dims(tf.RaggedTensor.from_row_lengths(
	tf.random.normal((10*BATCHES, CHANNELS)),
	[12,8,7,13]
), -2)
ragged_2 = tf.expand_dims(tf.RaggedTensor.from_row_lengths(
	tf.RaggedTensor.from_uniform_row_length(
		tf.random.normal((10*BATCHES*CHANNELS, )),
		CHANNELS
	),
	[12,8,7,13]
), -2)

# A simple multiplication kernel
kernel = tf.random.normal((1, 1, 16, CHANNELS))

# Try to multiply
print(""ragged 1 shape: "", ragged_1.shape)
print(""ragged 2 shape: "", ragged_2.shape)
print(""kernel shape: "", kernel.shape)

# ragged_1 succeeds
out_1 = tf.multiply(ragged_1, kernel)
print(""mult 1 success"")
# ragged_2 fails !
out_2 = tf.multiply(ragged_2, kernel)
print(""mult 2 success"")
```
Output:
```
ragged 1 shape:  (4, None, 1, 8)
ragged 2 shape:  (4, None, 1, 8)
kernel shape:  (1, 1, 16, 8)
mult 1 success
Traceback (most recent call last):
   ...
tensorflow.python.framework.errors_impl.InvalidArgumentError: Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: b'Unable to broadcast: dimension size mismatch in dimension'
2
b'lengths='
16
b'dim_size='
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
```
Even though `ragged_1` and `ragged_2` have identical shapes, the multiply only succeeds for the first. Seems like an over-zealous assertion is preventing the `ragged_2` multiply?"
50641,ERROR: Didn't find op for builtin opcode 'MUL' version '5',"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 7
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): model built with tf2.5 converted to tflite with tf_nightly 2.7.0-dev20210705


**Output from tflite benchmark**

```
STARTING!
Log parameter values verbosely: [0]
Graph: [model_float16.tflite]
Enable op profiling: [1]
Loaded model model_float16.tflite
ERROR: Didn't find op for builtin opcode 'MUL' version '5'. An older version of this builtin might be supported. Are you using an old TFLite binary with a newer model?

ERROR: Registration failed.

Failed to initialize the interpreter
Benchmarking failed.

```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

The benchmark run was the prebuilt one provided on https://www.tensorflow.org/lite/performance/measurement#ios_benchmark_app.
I can't use any earlier version of TensorFlow to convert the model because of recent additions to tf_nightly
"
50637,model.fit with dataset generator results in deadlock/hang ,"TF: 2.5.0
Python: 3.7.10

 Model.fit hangs during training. It appears dataset generator seems go into a deadlock while reading tfrecords. A simplified naive model is in the github gist below which reproduces the issue. Directly iterating on the generator seems to work fine but feeding it to model.fit seems to deadlock. Appreciate any color. 


https://gist.github.com/talipini/d82d1fa2f5d6b46f3222f2367570543a#file-tf-model-fit-hangs-with-dataset-generator-ipynb"
50636,TFLite request to support bulit-in split op for int64 type,"**System information**
- OS Platform and Distribution MacOS
- TensorFlow installed from: recent nightly build, after issue https://github.com/tensorflow/tensorflow/issues/50445
- TensorFlow version (or github SHA if from source): master HEAD



**Standalone code to reproduce the issue** 


```
def test_split_export():

    import tensorflow as tf

    @tf.function
    def test_fn(inp: tf.Tensor) -> tf.Tensor:
        return tf.concat(tf.split(inp, 3, axis=1), axis=1)

    concrete_f = test_fn.get_concrete_function(tf.TensorSpec((None, 3), tf.int64))
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])

    tflite_model = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
```

after fix #50445, running this test issues the following converter error: 

```
E         tensorflow.lite.python.convert_phase.ConverterError: /Users/dmlyubim/opt/anaconda3/envs/p38/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py:2129:0: error: 'tf.Split' op is neither a custom op nor a flex op
```

We think it is a fairly easily fixable gap in the tflite builtin op coverage. 

The supporting arguments are as follows: 

* The main argument is that int64 type is supported in tflite2 built-in set; in fact, the concat op as in following test, works no problem:

```
def test_concat_export():
    import tensorflow as tf

    @tf.function
    def test_fn(*inp: tf.Tensor) -> tf.Tensor:
        return tf.concat(inp, axis=1)

    concrete_f = test_fn.get_concrete_function(
        *([tf.TensorSpec((None, 3), tf.int64)] * 3)
    )
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])

    tflite_model = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
```

Split is a complementary (reverse) operation of the tensor concat op, we think there's no reason to support one but not another. 

* It should be a rather easy fix (split is already supported for int32 etc., so adding new type should be much easier than if we had to create a whole new op).

* In the age when models are transplantable, licensable, and/or expensive to train, any work-around solutions assuming model re-graphing or/and re-training to work around int64 issue are non-solutions. We have to deal with the topology and models we already have a license to evaluate. 

* Using TF_SELECT_OPS runitme introduces some problems with our requirements (mostly size, but also build system). This is a smaller and perhaps weaker support argument but it is still a big trouble for us mostly on requirement grounds (we are not targeting Android platform, or at least not exclusively; but we do target a C++ environment, not python). 

* Also, this op was identified as the only missing op precluding us from transition from TFLite1 to TFLite2. That's the only operator that really does not work. Everything else seems to be covered by built-ins for the widest range of models we investigated.

* Oddly enough, our tflite conversions (and evaluations) for the same models worked in TFLite1. Probably due to another bug or mysterious context of the op use in the models we observed; a similar int64 input and split as in the example, does not work with TFLite1.15 test either for me. However, it mysteriously works for different contexts of int64 split use in the models we tried to convert with TFLite 1.15 but not TFLite2.

"
50635,Converting FasterRCNN from tf -> tflite fails: could not rewrite use of immutable bound input,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installation (pip package or built from source): `tensorflow==2.5.0` installed via pip

### 2. Code

Full code to reproduce my issue: https://github.com/florianletsch/torch2tflite-failing

I am trying to convert a Faster-RCNN with Resnet backbone following these 3 steps:  

1. PyTorch -> ONNX
2. ONNX -> Tensorflow
3. Tensorflow -> tflite

Step 3 takes a while (~18 minutes) and then fails. This is the code of step 3:

```
converter = tf.lite.TFLiteConverter.from_saved_model('assets/tfsavedmodel')
tflite_model = converter.convert()
```

Error message:

```
loc(callsite(callsite(""onnx_tf_prefix_If_1347@__inference___call___15681"" at ""StatefulPartitionedCall@__inference_signature_wrapper_16224"") at ""StatefulPartitionedCall"")): error: could not rewrite use of immutable bound input
Traceback (most recent call last):
  File ""/home/florian/projects/torch2tflite-failing/venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py"", line 291, in toco_convert_protos
    model_str = wrap_toco.wrapped_toco_convert(model_flags_str,
  File ""/home/florian/projects/torch2tflite-failing/venv/lib/python3.8/site-packages/tensorflow/lite/python/wrap_toco.py"", line 32, in wrapped_toco_convert
    return _pywrap_toco_api.TocoConvert(
Exception: <unknown>:0: error: loc(callsite(callsite(""onnx_tf_prefix_If_1347@__inference___call___15681"" at ""StatefulPartitionedCall@__inference_signature_wrapper_16224"") at ""StatefulPartitionedCall"")): could not rewrite use of immutable bound input
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
```

### The op node where it fails

The error message mentions op node `If_1347`. According to the onnx model, the then/else branches of that node contain the following results of PyTorch's JIT:

```
  %2632 = If[else_branch = <graph torch-jit-export2>, then_branch = <graph torch-jit-export1>](%2631)

(...)

graph torch-jit-export1 {
  %2633 = Constant[value = <Tensor>]()
  %2634 = ConstantOfShape[value = <Tensor>](%2633)
  return %2634
}

graph torch-jit-export2 {
  %2635 = ReduceMax[keepdims = 0](%2622)
  %2636 = Cast[to = 1](%2626)
  %2637 = Constant[value = <Scalar Tensor []>]()
  %2638 = Cast[to = 1](%2637)
  %2639 = Add(%2635, %2638)
  %2640 = Mul(%2636, %2639)
  %2641 = Unsqueeze[axes = [1]](%2640)
  %2642 = Add(%2622, %2641)
  %2643 = Unsqueeze[axes = [0]](%2642)
  %2644 = Unsqueeze[axes = [0]](%2624)
  %2645 = Unsqueeze[axes = [0]](%2644)
  %2646 = Constant[value = <Tensor>]()
  %2647 = Constant[value = <Tensor>]()
  %2648 = NonMaxSuppression(%2643, %2645, %2646, %2647)
  %2649 = Constant[value = <Tensor>]()
  %2650 = Gather[axis = 1](%2648, %2649)
  %2651 = Squeeze[axes = [1]](%2650)
  return %2651
}
```

I don't understand what's going on here, is there a pointer that explains the issue hidden here?

### Logs
Logs of my 3 conversion steps can be found here: https://github.com/florianletsch/torch2tflite-failing/tree/main/logs

### Converter issue or an unrelated problem?
Is this an issue or the tflite converter? Or is this likely an issue of an earlier step in the conversion pipeline? I am grateful for any pointers.
"
50634,Can't build tensorflow lite wheel in docker image for aarch64,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- TensorFlow version: latest from github

**Describe the problem**
Following the instructions from [here](https://www.tensorflow.org/lite/guide/build_cmake_pip) I tried to build a python wheel for tensorflow lite for the aarch64 (mendel 4.0/coral dev board) architecture. 

`tensorflow/tools/ci_build/ci_build.sh PI-PYTHON38   tensorflow/lite/tools/pip_package/build_pip_package_with_cmake.sh aarch64`

When I run the suggested command an error occurs which indicates that cmake can't find the correct compiler. 

```
CMakeFiles/cmTC_8c684.dir/testCCompiler.c.o  -o cmTC_8c684 
    aarch64-linux-gnu-gcc: fatal error: -fuse-linker-plugin, but liblto_plugin.so not found
    compilation terminated.
    CMakeFiles/cmTC_8c684.dir/build.make:89: recipe for target 'cmTC_8c684' failed
    make[1]: *** [cmTC_8c684] Error 1
    make[1]: Leaving directory '/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeTmp'
    Makefile:124: recipe for target 'cmTC_8c684/fast' failed
    make: *** [cmTC_8c684/fast] Error 2
    
    

  

  CMake will not be able to correctly generate this project.
Call Stack (most recent call first):
  CMakeLists.txt:40 (project)


CMake Error at CMakeLists.txt:40 (project):
  The CMAKE_CXX_COMPILER:

    /workspace/tensorflow/lite/tools/cmake/toolchains/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-g++

  is not a full path to an existing compiler tool.

  Tell CMake where to find the compiler by setting either the environment
  variable ""CXX"" or the CMake cache entry CMAKE_CXX_COMPILER to the full path
  to the compiler, or to the compiler name if it is in the PATH.


-- Configuring incomplete, errors occurred!
See also ""/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeOutput.log"".
See also ""/workspace/tensorflow/lite/tools/pip_package/gen/tflite_pip/python3.8/cmake_build/CMakeFiles/CMakeError.log"".

```
After some investigation it seems that for some reason the tar command has problems unpacking the downloaded toolchain. The error (if manually run) is 
`
tar: gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-g++: Cannot hard link to ‘gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu/bin/aarch64-linux-gnu-c++’: Operation not supported
`
Any suggestions how to fix this issue?"
50633,Compiling a keras model during a callback causes TypeError: 'NoneType' object is not callable,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): binary from pip/anaconda
- TensorFlow version (use command below): Tried with the following 
  - v2.4.0-49-g85c8b2a817f 2.4.1
  - v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
  - v1.12.1-59787-gd89220bfa4c 2.7.0-dev20210706 (tf-nightly)
- Python version: 3.8.5

**Describe the current behavior**

When compiling a model in a callback, the training breaks with the error `TypeError: 'NoneType' object is not callable`
I'm pretty sure there is a regression here, as it works when using TensorFlow v <= 2.3

**Describe the expected behavior**

No crash, and having the model recompiled with the new parameters passed during compilation. 

**Standalone code to reproduce the issue**

```
import tensorflow as tf

class MyCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epochs, logs=None):
        # we recompile at the end of the first epoch
        if epochs == 0:
            self.model.compile(
                optimizer=""rmsprop"",
                loss=""mse"",
            )

model = tf.keras.Sequential([tf.keras.layers.Dense(1,input_shape=(1,))])
my_callback = MyCallback()
x = tf.random.uniform((100,1))
y = tf.random.uniform((100,1))

model.compile(optimizer=""rmsprop"", loss=""mse"")

model.fit(x,y, callbacks=[my_callback], epochs=2)
```

**Other info / logs** Include any logs or source code that would be helpful to

The Traceback: 
```

Traceback (most recent call last):
  File ""compile_callback.py"", line 28, in <module>
    model.fit(x,y, callbacks=[my_callback], epochs=2)
  File ""/home/user/miniconda3/envs/tf24/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1100, in fit
    tmp_logs = self.train_function(iterator)
TypeError: 'NoneType' object is not callable
```
"
50632,Test //tensorflow/compiler/mlir/lite/tests/end2end:quant_stats.pbtxt.test succeeds regardless of check patterns,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source): a4dfb8d1a71385bd6d122e4f27f86dcebb96712d (v2.5.0)

### 2. Code

`bazel test //tensorflow/compiler/mlir/lite/tests/end2end:quant_stats.pbtxt.test`

always passes regardless of the check patterns in 

`tensorflow/compiler/mlir/lite/tests/end2end/quant_stats.pbtxt`

E.g. even when this file is replaced with the deliberately mangled version the test still passes.

[mangled_quant_stats.pbtxt.txt](https://github.com/tensorflow/tensorflow/files/6770259/mangled_quant_stats.pbtxt.txt)

[always_passes.log](https://github.com/tensorflow/tensorflow/files/6770295/always_passes.log)

### 3. (optional) Any other info / logs

Running the relevant FileCheck command-line manually fails in the expected way.

[always_passes.log](https://github.com/tensorflow/tensorflow/files/6770295/always_passes.log)

The committed version of the test will also fail if run manually in this way (minor changes in operator attribute)."
50631,Container __per_step_0 does not exist,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Win 10
- TensorFlow version (use command below):  2.3.1
- Python version: 3.7.7
- CUDA/cuDNN version:  library cudart64_101.dll v2.3.0-54-gfcc4b966f1 2.3.1

**Describe the current behavior**

While running multiple object detection inference in parallel the session crashed and the below error occurred.

Traceback (most recent call last):
    output_dict = model(input_tensor)
  File ""/home//.conda/envs//lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1605, in __call__
    return self._call_impl(args, kwargs)
  File ""/home//.conda/envs//lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1645, in _call_impl
    return self._call_flat(args, self.captured_inputs, cancellation_manager)
  File ""/home//.conda/envs//lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 1746, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/home//.conda/envs//lib/python3.6/site-packages/tensorflow/python/eager/function.py"", line 598, in call
    ctx=ctx)
  File ""/home//.conda/envs//lib/python3.6/site-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.NotFoundError:  Container __per_step_0 does not exist. (Could not find resource: __per_step_0/_tensor_arraysBatchMultiClassNonMaxSuppression/map/TensorArray_11_950)
         [[node BatchMultiClassNonMaxSuppression/map/while/TensorArrayWrite_5/TensorArrayWriteV3 (defined at /home/workspace//utils/field_detection.py:15) ]] [Op:__inference_pruned_41885]

Function call stack:
pruned

**Describe the expected behavior**

**Standalone code to reproduce the issue**

Reproducing the issue was not possible.

"
50630,How can i know the order of multiple inputs when i use TrtGraphConverterV2 build. ,"tensorflow version: tensorflow-gpu 2.0.4
cuda version: 10.0
cudnn version: 7.4
tensorrt version: 5.1.5

When i use tensorrt converter to convert my model, i can't know the order of multiple inputs. Such as:

```
converter = trt.TrtGraphConverterV2(***)
converter.convert()
def my_inp_fn():
    inp_1 = np.zeros((2000, 120), np.int32)
    inp_2 = np.zeros((1, 47), np.int32)
    inp_3 = np.zeros((2000, 178), np.int32)
    inp_4 = np.zeros((1, 1493), np.int32)
    inp_5 = np.zeros((2000, 12), np.int32)
    inp_6 = np.zeros((1, 54), np.int32)
    inp_7 = np.zeros((2000, 178), np.float32)
    inp_8 = np.zeros((1, 1493), np.float32)
    inp_9 = np.zeros((2000, 93), np.float32)
    inp_10 = np.zeros((1, 12), np.float32)
    yield [inp_1, inp_2, inp_3, inp_4, inp_5, inp_6, inp_7, inp_8, inp_9, inp_10]
converter.build(input_fn=my_inp_fn)
```

I use saved_model_cli tool to check the signature input. But the signature input order is not true order for trt convert build function. Build function will report error with wrong order. Like:
```
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute __inference_pruned_26473 as input #8(zero-based) was expected to be a int32 tensor but is a float tensor [Op:__inference_pruned_26473]
```
I can't analyse the order with the too simple error log.

Thanks very much!"
50629,Confusing about GPU state when ran out of memory,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: not mobile device
- TensorFlow installed from (source or binary): pip install tensorflow-gpu==1.15
- TensorFlow version (use command below): tensorflow-gpu==1.15
- Python version: 3.6.12
- Bazel version (if compiling from source): 0.18.0-
- GCC/Compiler version (if compiling from source):4.8.5
- CUDA/cuDNN version: 10.0
- GPU model and memory: 

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
My GPU model ran out of memory during training process, it reports the GPU state as follows:

![截屏2021-07-06 下午6 51 53](https://user-images.githubusercontent.com/5723913/124589692-dc6f4700-de8c-11eb-868f-f0365e989bdd.png)

As shown above, ""Limit"" indicates that the total memory of my GPU is  22,676,357,120 bytes, which is equal to my Tesla P40 GPU's 22G memory. ""InUse"" means the GPU has allocated 11,866,777,088 bytes. Thus the remaining memory should be 22,676,357,120 - 11,866,777,088 = 10,809,580,032 bytes. Now the process need to allocate 5,352,895,488 bytes, which is less than 10,809,580,032 bytes. So why does it report the process ran out memory?

And what is the meaning of ""MaxInUse"" and ""MaxAllocSize"" ?
"
50628,Issue with Conv1D when groups > 1 and using our own TensorFlow builds,"**System information**
- OS Platform and Distribution: Linux Red Hat Enterprise 8.1
- TensorFlow installed from: source
- TensorFlow version: v2.5.0-0-ga4dfb8d1a71 2.5.0 (more generally any version starting from 2.3.1)
- Python version: 3.7.10
- Bazel version: 3.7.2
- GCC/Compiler version: 8.3.1
- CUDA/cuDNN version: 11.2 / 8.0
- GPU model and memory: NVIDIA V100

**Describe the current behavior**

When using our own TensorFlow builds (starting at version 2.3.1 which added the  `groups` parameter), the following code snippet fails for `groups` greater than 1:
```python
import tensorflow as tf
import traceback

for g in (1,2,4):
    try:
        print(f""groups={g}"")
        c = tf.keras.layers.Conv1D(4,4,groups=g)
        print(c(tf.ones((2,16,4))))
    except Exception as e:
        traceback.print_exc()
    finally:
        print()
```

Output is as follow:
```
groups=1
tf.Tensor(
[[[ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]]

 [[ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]
  [ 0.28654966  0.9454404  -1.1466699  -0.91166556]]], shape=(2, 13, 4), dtype=float32)

groups=2
Traceback (most recent call last):
  File ""groups.py"", line 8, in <module>
    print(c(tf.ones((2,16,4))))
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1030, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 249, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1019, in convolution_v2
    name=name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1149, in convolution_internal
    name=name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 602, in new_func
    return func(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 602, in new_func
    return func(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1892, in conv1d
    name=name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 932, in conv2d
    _ops.raise_from_not_ok_status(e, name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: input and filter must have the same depth: 4 vs 2 [Op:Conv2D]

groups=4
Traceback (most recent call last):
  File ""groups.py"", line 8, in <module>
    print(c(tf.ones((2,16,4))))
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1030, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 249, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1019, in convolution_v2
    name=name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1149, in convolution_internal
    name=name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 602, in new_func
    return func(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/util/deprecation.py"", line 602, in new_func
    return func(*args, **kwargs)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1892, in conv1d
    name=name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 932, in conv2d
    _ops.raise_from_not_ok_status(e, name)
  File ""/.../pub/anaconda-py3/2020.11/envs/tensorflow-gpu-2.5.0/lib/python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: input and filter must have the same depth: 4 vs 1 [Op:Conv2D]
```

**Describe the expected behavior**

If I run the same code snippet using a TensorFlow build from `pip`, I do not get any errors but I have a hard time understanding how this can build dependent:
```
groups=1
tf.Tensor(
[[[-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]]

 [[-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]
  [-0.05965281 -1.473797   -0.5487337  -0.34858704]]], shape=(2, 13, 4), dtype=float32)

groups=2
tf.Tensor(
[[[ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]]

 [[ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]
  [ 0.555179   -0.11330175  0.22858751 -1.1606797 ]]], shape=(2, 13, 4), dtype=float32)

groups=4
tf.Tensor(
[[[-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]]

 [[-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]
  [-0.9354149   0.7340534  -0.01698902  0.4960574 ]]], shape=(2, 13, 4), dtype=float32)
```

Any ideas would be greatly appreciated!"
50627,Cannot link libtensorflowlite.dylib for iOS,"**System information**
- OS Platform and Distribution : OS X 11.3
- TensorFlow version: 2.5
- Python version: python3.9
- Bazel version (if compiling from source): bazel 3.7.2
- GCC/Compiler version (if compiling from source): Apple clang version 12.0.5 (clang-1205.0.22.11), Xcode 12.5.1

I tried to build C++ API libtensorflowlite.dylib for iOS. I use build command:

`
bazel build -c opt --config=ios_arm64 //tensorflow/lite:libtensorflowlite.dylib --verbose_failures
`
Error:

```
ld: warning: option -s is obsolete and being ignored
ld: unknown option: -z
clang: error: linker command failed with exit code 1 (use -v to see invocation)
```

Steps:
1. Run ./Configure
2. Run bazel build -c opt --config=ios_arm64 //tensorflow/lite:libtensorflowlite.dylib --verbose_failures
3. Error.

**Any other info / logs**
```
ERROR: /Users/o.sh/work/tensorflow/tensorflow/lite/BUILD:909:24: Linking of rule '//tensorflow/lite:libtensorflowlite.dylib' failed (Exit 1): cc_wrapper.sh failed: error executing command 
  (cd /private/var/tmp/_bazel_o.sh/916305f91b5f074316230c12ec2f3c72/execroot/org_tensorflow && \
  exec env - \
    APPLE_SDK_PLATFORM=iPhoneOS \
    APPLE_SDK_VERSION_OVERRIDE=14.5 \
    PATH=/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/Apple/usr/bin \
    PYTHON_BIN_PATH=/usr/local/opt/python@3.9/bin/python3.9 \
    PYTHON_LIB_PATH=/usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
    XCODE_VERSION_OVERRIDE=12.5.1.12E507 \
  external/local_config_cc/cc_wrapper.sh -lc++ -fobjc-link-runtime -framework UIKit -shared -o bazel-out/ios_arm64-opt/bin/tensorflow/lite/libtensorflowlite.dylib -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libbuiltin_ops_all_linked.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libbuiltin_op_kernels.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/liblstm_eval.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/flatbuffers/src/libflatbuffers.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libaudio_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libkernel_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libeigen_support.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libkernel_util.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libquantization_util.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/tools/optimize/sparsity/libformat_converter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libtensor_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libneon_tensor_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libportable_tensor_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libtranspose_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libcpu_backend_gemm.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/internal/libcpu_check.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libcontext_get_ctx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libfrontend.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_arm.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_avx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_avx2_fma.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libkernel_avx512.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libapply_multiplier.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_arm.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_avx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_avx2_fma.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libpack_avx512.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libprepare_packed_matrices.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libtrmul.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libblock_map.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/kernels/libcpu_backend_context.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libcontext.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libctx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/liballocator.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libhave_built_path_for_avx.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libhave_built_path_for_avx2_fma.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libhave_built_path_for_avx512.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libprepacked_cache.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libsystem_aligned_alloc.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libtune.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libcpuinfo.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/cpuinfo/libcpuinfo_impl.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/clog/libclog.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libthread_pool.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libblocking_counter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libwait.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/fft2d/libfft2d.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/profiler/libinstrumentation.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/farmhash_archive/libfarmhash.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/liboptional_debug_tools.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libcc_api.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/liballocation.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libarena_planner.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libexternal_cpu_backend_context.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libmutable_op_resolver.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libsimple_memory_arena.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/core/util/libversion_info.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/delegates/libtelemetry.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/profiling/libplatform_profiler.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/profiling/libsignpost_profiler.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/core/api/libapi.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/core/api/libop_resolver.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/core/api/liberror_reporter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/c/libcommon.lo -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/schema/libschema_utils.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/micro/libdebug_log.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/external/ruy/ruy/libdenormal.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libstderr_reporter.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libminimal_logging.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libutil.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/experimental/resource/libresource.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libstring_util.a -Wl,-force_load,bazel-out/ios_arm64-opt/bin/tensorflow/lite/libtflite_with_xnnpack_optional.a -headerpad_max_install_names -Wl,-z,defs -Wl,--version-script,tensorflow/lite/tflite_version_script.lds -s '-Wl,-rpath,$ORIGIN/,-rpath,$ORIGIN/..' -Wl,-install_name,@rpath/libtensorflowlite.dylib -lm -pthread -pthread -pthread -lpthread -lm -pthread -ldl -no-canonical-prefixes -target arm64-apple-ios '-miphoneos-version-min=14.5')
Execution platform: @local_execution_config_platform//:platform
ld: warning: option -s is obsolete and being ignored
ld: unknown option: -z
clang: error: linker command failed with exit code 1 (use -v to see invocation)
Target //tensorflow/lite:libtensorflowlite.dylib failed to build

```"
50626,TensorFlow 2.3.0 does not respect no_proxy environment variable,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binay
- TensorFlow version (use command below): v2.3.0-rc2-23-gb36436b087 2.3.0
- Python version: Python 3.6.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
TensorFlow does not recognize environment variable `no_proxy` and `no_grpc_proxy`. 
We have set `http_proxy` and `no_proxy` in the environment variable, and tf seems only pickup http_proxy, use the http_proxy for the inter worker grpc communication and got error like this
> {""created"":""@1625564310.040212700"",""description"":""Error received from peer ipv4:proxy_ip:8080"",""file"":""external/com_github_grpc_grpc/src/core/lib/surface/call.cc"",""file_line"":1056,""grpc_message"":""Socket closed"",""grpc_status"":14} [Op:CollectiveBcastRecv] 

This is the TF_CONFIG: 
> TF_CONFIG={""cluster"": {""worker"": [""10.0.0.7:45661"", ""10.0.0.7:46771""]}, ""task"": {""type"": ""worker"", ""index"": 0}, ""environment"": ""cloud""}

This is proxy_related env:
> http_proxy=http://proxy-ip:8080/
no_proxy=proxy-ip,localhost,10.0.0.4,10.0.0.5,10.0.0.6,10.244.0.0/16,10.0.0.0/8


**Describe the expected behavior**
GRPC should not use proxy since we have defined the ip not to use proxy.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50625,Make layers from `tensorflow.keras.layers.experimental.preprocessing` usable in TF Lite,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
It would be awesome if layers from `tensorflow.keras.layers.experimental.preprocessing` like `Resizing` would be convertable to TF Lite without implementing custom ops.

Currently a custom op definition is needed for (some of) them to work.
Especially Lanczos3 for Resizing would be nice.

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
It would be very easy to add preprocessing inside a model. Therefore people who want to use those models do not need information about what input format should be used.
"
50624,tensorflow lite compiling failed on docker,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
I compile tensorflow lite in android mode according to the online guide: https://www.tensorflow.org/lite/guide/build_android

- the docker file provided by tflite can not be built successfully by docker
    - https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/tools/dockerfiles/tflite-android.Dockerfile

![image](https://user-images.githubusercontent.com/45189361/124571655-927d6580-de7a-11eb-9297-02e55932a6f7.png)

- However, I modified the docker file and rebuilt the docker successfully. I entered into docker container but tflite compiling still failed.
   - jdk version: 1.8
   - command : `bazel build -c opt --fat_apk_cpu=x86,x86_64,arm64-v8a,armeabi-v7a   --host_crosstool_top=@bazel_tools//tools/cpp:toolchain   //tensorflow/lite/java:tensorflow-lite`

![image](https://user-images.githubusercontent.com/45189361/124571996-dbcdb500-de7a-11eb-8190-1d498580b2fb.png)




**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50622,tf.shape can't infer sparse tensor shape in graph mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.10
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0

**Describe the current behavior**

tf.shape can't infer shape of a sparse tensor in graph mode.

**Describe the expected behavior**

tf.shape should infer shape of a sparse tensor in graph mode.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3
import tensorflow as tf;

inputs = tf.keras.Input((2,3,4));
print(tf.shape(inputs)); # tf.shape can infer dense tensor

inputs = tf.keras.Input((2,3,4), sparse = True);
print(tf.shape(inputs)); # tf.shape cannot infer sparse tensor
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50620,tf.math.sigmoid/tanh for RaggedTensor,"I was surprised to see that two common activation functions, sigmoid and tanh, are not provided in `tf.ragged` module. Makes me wonder how many people are actually using RaggedTensor features with no feature requests for these.

I am currently calculating both via mathematical identities. However, I still think the API's between regular Tensor and RaggedTensor should be as close as possible, as ideally you should be able to just swap in a RaggedTensor and have everything just work."
50619,tf.repeat for RaggedTensor,"Wondering if we can get `tf.repeat` for the `tf.ragged` module?

The `tf.stack` and `tf.tile` ops are implemented, so doesn't seem like there would be a technical limitation for `tf.repeat`. My particular case is doing the repeat on one of the non-ragged dims. But seems you could do a repeat on a ragged tensor, so long as the `repeats` arg is scalar or matches the maximum row length. I did see a related issue discussing the possibility of an nd_repeat operation, which would perhaps prove more useful for repeats on ragged dims."
50618,error reporting model(x) vs model.predict(x),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.5

**Describe the current behavior**

The error and warning messages seem to be different when `model(x)` is used instead of `model.predict(x)`  to make a prediction.
Is this intentional and documented somewhere? 
For example, in the following simple model there is a warning when `model.predict(x)` is used, but none for `model(x)`. 
Moreover, it would be generally good to know which warnings (or errors) might not occur when a specific prediction method is used and also which warnings will be the same.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
model = keras.Sequential([
keras.layers.ReLU(threshold=3,  input_shape=(2,))])
x = tf.constant([[[[1,2,3,4,5]]]])
print (np.array2string(model(x).numpy(), separator=', '))
#print (np.array2string(model.predict(x), separator=', '))
```
"
50617,How to access elements of an input tensor in custom layers ?,"System: Python 3.8.3 64-bit | Qt 5.12.9 | PyQt5 5.12.3 | Linux 5.8.0-55-generic
tensorflow version: 2021-07-05 20:04:06.422679
keras version: 2021-07-05 20:04:06.422679

I wanted to assign an element of input tensor X to an element of an output tensor Y inside a custom layer like

Y[0,0,0,0] = X[0,1,1,1]

For example:

```
from keras.layers import Layer
import numpy as np
from keras.layers import Input
from keras.models import Model 

Images = [[[[255,0,0],[255,0,0]],[[255,0,0],[0,144,0]]]]

InputTensor = np.array(Images)

class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, X):
        Y = [[[[0,0,0,0]]]] # Output tensor
        Y = np.array(Y)
        Y[0,0,0,0] = X[0,1,1,1]   # <------ does not work because X[0,1,1,1] is a sequence instead a scalar
        return np.ndarray.tolist(Y)
    
inputlayer = Input(shape = InputTensor[0].shape)
layer = MyLayer()(inputlayer)

model = Model(inputlayer, layer ) 
model.summary()

Output = model.predict(InputTensor)
Output = np.array(Output)
print(""Output = ""+str(Output))
```
If the call-function returns X , then X[0,1,1,1] is not anymore a sequence after it passed the modell.
It make not sense to use custom layers if you have no access to the elements of an input tensor as expected.
For example:
```
from keras.layers import Layer
import numpy as np
from keras.layers import Input
from keras.models import Model 

Images = [[[[255,0,0],[255,0,0]],[[255,0,0],[0,144,0]]]]

InputTensor = np.array(Images)

class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, X):
        return X
    
inputlayer = Input(shape = InputTensor[0].shape)
layer = MyLayer()(inputlayer)

model = Model(inputlayer, layer ) 
model.summary()

Output = model.predict(InputTensor)
Output = np.array(Output)
print(""Output = ""+str(Output))
```
The Output is:

```________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_78 (InputLayer)        [(None, 2, 2, 3)]         0         
_________________________________________________________________
my_layer_77 (MyLayer)        (None, 2, 2, 3)           0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
Output = [[[[255.   0.   0.][255.   0.   0.]][[255.   0.   0.][  0. 144.   0.]]]]
```"
50616,Multi-GPU issue: second A40 GPU returns zeros ,"
**System information**
- Have I written custom code: yes (code below)
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71
- Python version: Python 3.6.9
- CUDA/cuDNN version: 11.2
- GPU model and memory: A40, 45634MiB (2X)

**Describe the current behavior**
I'm having difficulties running a training script on a server with TF 2.5 and two A40s. Basically, any data on the second gpu defaults to `0.0`.

The included example script pinpoints the problem. In few words, the same variable computed on `/GPU:0` and `/GPU:1` must return the same value when queried. Instead, the variable on `/GPU:1` is always read as `0.0`

I have tried my original training script and the toy example below on another server running TF 1.15 with 8 RTX5000s, and everything works as expected.

**Standalone code to reproduce the issue**
```
import tensorflow.compat.v1 as tf
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = '0,1'

if __name__ == ""__main__"":
	
	config = tf.ConfigProto()
	config.gpu_options.allow_growth = True
	graph = tf.Graph()
	sess = tf.Session(config=config, graph=graph)
	
	with graph.as_default():
		with tf.device('/GPU:0'):
			with tf.name_scope(""tower_0"") as scope:
				with tf.variable_scope(""model"", reuse=False):
					v1 = tf.Variable(1.0, name=""v"")
					x1 = v1 * 1.0
		with tf.device('/GPU:1'), tf.variable_scope(""model"", reuse=True):
			with tf.name_scope(""tower_1"") as scope:
				with tf.variable_scope(""model"", reuse=True):
					x2 = v1 * 1.0
			init = tf.global_variables_initializer()
		print(tf.trainable_variables())
	
	sess.run(init)
	print(sess.run([x1, x2]))
```

**Output** 
```
2021-07-05 15:58:54.653001: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-05 15:58:55.487115: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-07-05 15:58:55.490402: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-07-05 15:58:57.440834: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:25:00.0 name: A40 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s
2021-07-05 15:58:57.442125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties:
pciBusID: 0000:81:00.0 name: A40 computeCapability: 8.6
coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s
2021-07-05 15:58:57.442157: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-05 15:58:57.443855: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-07-05 15:58:57.443881: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-07-05 15:58:57.444443: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-07-05 15:58:57.444587: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-07-05 15:58:57.445076: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2021-07-05 15:58:57.445566: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-07-05 15:58:57.445701: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-07-05 15:58:57.450860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2021-07-05 15:58:57.450885: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-05 15:58:57.994357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-07-05 15:58:57.994427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1
2021-07-05 15:58:57.994437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y
2021-07-05 15:58:57.994442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N
2021-07-05 15:58:58.001069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43676 MB memory) -> physical GPU (device: 0, name: A40, pci bus id: 0000:25:00.0, compute capability: 8.6)
2021-07-05 15:58:58.002705: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 43676 MB memory) -> physical GPU (device: 1, name: A40, pci bus id: 0000:81:00.0, compute capability: 8.6)
[<tf.Variable 'tower_0/model/v:0' shape=() dtype=float32>]
2021-07-05 15:58:58.031548: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3493525000 Hz
[1.0, 0.0]
```"
50614,Tensorflow GPU installation don't want to run this code,"**System information**
- Ubuntu 18.04 LTS
- Deep learning SageMaker with pre installed drivers and etc.
- AWS EC2 g4dn.xlarge

**ERORR**


**Describe the current behavior**
- Tensorflow GPU installation don't want to run this code
- while the CPU installation (other anaconda environment, built in AWS SageMaker anaconda evironment: ""tensorflow2_p36"") runs this code no problem (it is just slow as expected)

**Describe the expected behavior**
- Tensorflow GPU installation run this code withouth error

**Standalone code to reproduce the issue**
- https://github.com/NioushaR/LSTM-TensorFlow-for-Timeseries-forecasting
- Final Code bit
```
# Create GRU model
def create_gru(units):
    model = Sequential()
    # Input layer 
    model.add(GRU (units = units, return_sequences = True, 
                 input_shape = [X_train.shape[1], X_train.shape[2]]))
    model.add(Dropout(0.2)) 
    # Hidden layer
    model.add(GRU(units = units))                 
    model.add(Dropout(0.2))
    model.add(Dense(units = 1)) 
    #Compile model
    model.compile(optimizer='adam',loss='mse')
   
    return model
model_gru = create_gru(64)

# Create BiLSTM model
def create_bilstm(units):
    model = Sequential()
    # Input layer
    model.add(Bidirectional(LSTM(units = units, return_sequences=True), 
                            input_shape=(X_train.shape[1], X_train.shape[2])))
    # Hidden layer
    model.add(Bidirectional(LSTM(units = units)))
    model.add(Dense(1))
    #Compile model
    model.compile(optimizer='adam',loss='mse')
    return model

model_bilstm = create_bilstm(64)

def fit_model(model):
    early_stop = keras.callbacks.EarlyStopping(monitor = 'val_loss',
                                               patience = 10)
    history = model.fit(X_train, y_train, epochs = 100, validation_split = 0.2,
                    batch_size = 16, shuffle = False, callbacks = [early_stop])
    return history

history_gru = fit_model(model_gru)
history_bilstm = fit_model(model_bilstm)
```

**ERROR Message**
```
Epoch 1/100
---------------------------------------------------------------------------
UnknownError                              Traceback (most recent call last)
<ipython-input-14-ac9269b205d7> in <module>
     38     return history
     39 
---> 40 history_gru = fit_model(model_gru)
     41 history_bilstm = fit_model(model_bilstm)

<ipython-input-14-ac9269b205d7> in fit_model(model)
     35                                                patience = 10)
     36     history = model.fit(X_train, y_train, epochs = 100, validation_split = 0.2,
---> 37                     batch_size = 16, shuffle = False, callbacks = [early_stop])
     38     return history
     39 

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1181                 _r=1):
   1182               callbacks.on_train_batch_begin(step)
-> 1183               tmp_logs = self.train_function(iterator)
   1184               if data_handler.should_sync:
   1185                 context.async_wait()

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    948         # Lifting succeeded, so variables are initialized and we can run the
    949         # stateless function.
--> 950         return self._stateless_fn(*args, **kwds)
    951     else:
    952       _, _, _, filtered_flat_args = \

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/eager/function.py in __call__(self, *args, **kwargs)
   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)
   3023     return graph_function._call_flat(
-> 3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   3025 
   3026   @property

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/eager/function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1959       # No tape is watching; skip to running the function.
   1960       return self._build_call_outputs(self._inference_function.call(
-> 1961           ctx, args, cancellation_manager=cancellation_manager))
   1962     forward_backward = self._select_forward_and_backward_functions(
   1963         args,

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/eager/function.py in call(self, ctx, args, cancellation_manager)
    594               inputs=args,
    595               attrs=attrs,
--> 596               ctx=ctx)
    597         else:
    598           outputs = execute.execute_with_cancellation(

~/anaconda3/envs/tfall/lib/python3.7/site-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

UnknownError:    Fail to find the dnn implementation.
	 [[{{node CudnnRNN}}]]
	 [[sequential/gru/PartitionedCall]] [Op:__inference_train_function_7423]

Function call stack:
train_function -> train_function -> train_function
```


**Tried Already**

- https://github.com/tensorflow/tensorflow/issues/24496
- https://github.com/tensorflow/tensorflow/issues/36508
- https://www.tensorflow.org/guide/gpu"
50613,`tensorflow/python/keras` leads to inconsistencies when mixed with new Keras pip package in 2.6.0,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.6.0rc0
- Python version: 3.8

**Describe the current behavior**

Starting with TF 2.6.0rc0 [keras-team/keras](https://github.com/keras-team/keras) is replacing the code which previously located in `tensorflow/python/keras`. This changes the `tf.keras` endpoint to use the code from the OSS Keras pip package. 

Unfortunately the old code is still shipped with the TensorFlow pip package which can lead to very subtle inconsistencies in user code. This occurs especially in library code that tries to support different TF versions, which might be forced to rely on private imports since some functions might not be available in certain TF version. (E.g. `is_keras_tensor` has been only a public API since 2.1).

This duplicated code can lead to very weird behaviours since `isinstance` checks or similar will now break if users mix and match code. E.g. take these lines which previously where equivalent:
```python
from keras.engine.keras_tensor import KerasTensor  # imported from keras-team/keras

from tensorflow.python.keras.engine.keras_tensor import KerasTensor as KerasTensorFromTF # This import should not exist anymore

assert KerasTensorFromTF == KerasTensor   # This breaks!!!
```

```python
import tensorflow as tf

from tensorflow.keras.backend import is_keras_tensor
from tensorflow.python.keras.backend import is_keras_tensor as is_keras_tensor_tf  # this import should not exist anymore

assert is_keras_tensor(tf.keras.Input([10]))
assert is_keras_tensor_tf(tf.keras.Input([10])) # This breaks!!!
```
Checkout [this notebook](https://colab.research.google.com/drive/1-TcroLTdI4fx4dk5P3Cq_vk5AfiYuVOo?usp=sharing) for a full reproduction.

**Describe the expected behavior**

Since the intention to switch to keras-team/keras is clear I think the only way to get rid of this weirdness is to remove the `tensorflow/python/keras` code completely from the shipped pip package. This will likely frustrate some users relying on internal functions, but at least it won't lead to unintended behaviour or silent bugs that might be very hard to debug for inexperienced users. Also since `tensorflow.python` always has been a private API, I don't think breaking users of it will be a big problem. @fchollet @mihaimaruseac What do you think?"
50612,cropping layer additional error message ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.5

**Describe the current behavior**
When a cropping layer (Cropping1D, Cropping2D,Cropping3D) is used, it can happen that the cropping value is accidentally selected too big. Then, the output is just an empty list. It would be good to have an error or warning message for this case, since such an issue can be really cumbersome to find when it occurs deep in a complex graph model.
The issue is illustrated in the simplified example below.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
in0Cro69514 = tf.keras.layers.Input(shape=([1, 2]))
Cro69514 = keras.layers.Cropping1D(cropping=((5, 5)), name = 'Cro69514', )(in0Cro69514)
model = tf.keras.models.Model(inputs=[in0Cro69514], outputs=Cro69514)
in0Cro69514 = tf.constant([[[1.6058, 1.8537]]])
print (np.array2string(model.predict([in0Cro69514],steps=1), separator=', '))
```
"
50611,GPU memory usage does not increase when using multithreading in python ,"Hi, I am working on deploy some yolo v3 models (keras) on the GPU of our server. However, I found that, when I use multithreading programming in python to deploy multiple models and let them predict in parallel, the GPU memory usage does not increase, the GPU memory usage is almost as much as one single model, which makes me very confused. Could someone help me?  

load and predict one by one: 
**model 0
gpu usage：total 16280.88 MB， used 859.56 MB， unused 15421.31 MB
model 1
gpu usage：total 16280.88 MB， used 1371.56 MB， unused 14909.31 MB
model 2
gpu usage：total 16280.88 MB， used 1371.56 MB， unused 14909.31 MB
model 3
gpu usage：total 16280.88 MB， used 1371.56 MB， unused 14909.31 MB

2021-07-05 16:45:00.227119: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7

gpu usage：total 16280.88 MB， used 6649.56 MB， unused 9631.31 MB
gpu usage：total 16280.88 MB， used 6649.56 MB， unused 9631.31 MB
gpu usage：total 16280.88 MB， used 6649.56 MB， unused 9631.31 MB
gpu usage：total 16280.88 MB， used 6649.56 MB， unused 9631.31 MB
**

load one by one and predict through multithread: 
**model 0
gpu usage：total 16280.88 MB， used 861.56 MB， unused 15419.31 MB
model 1
gpu usage：total 16280.88 MB， used 1373.56 MB， unused 14907.31 MB
model 2
gpu usage：total 16280.88 MB， used 1373.56 MB， unused 14907.31 MB
model 3
gpu usage：total 16280.88 MB， used 2397.56 MB， unused 13883.31 MB

2021-07-05 17:06:22.285915: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7

thread 0 gpu usage：total 16280.88 MB， used 6651.56 MB， unused 9629.31 MB
thread 1 gpu usage：total 16280.88 MB， used 6651.56 MB， unused 9629.31 MB
thread 2 gpu usage：total 16280.88 MB， used 6651.56 MB， unused 9629.31 MB
thread 3gpu usage：total 16280.88 MB， used 6651.56 MB， unused 9629.31 MB
**"
50607,How to use the TF_VARIANT with C APIs?,"Hi,

Currently, we can create or get a `TF_Tensor` with `TF_FLOAT` data type, and then manipulate the raw data buffer by the pointer from `TF_TensorData`. For example,

```
TF_Tensor* output = TF_AllocateOutput(xxx, TF_FLOAT, xxx);
float* output_raw_buffer = reinterpret_cast<float*>(TF_TensorData(output));
// do some calculation on output_raw_buffer
```

But for the data type TF_VARIANT, we can't manipulate it directly like float type. Do we have any examples about how to use it ?

Thanks"
50606,InternalError:  dnn PoolBackward launch failed in average_pooling2d/AvgPool/AvgPoolGrad,"[Link to Full notebook of training](https://github.com/RushilVerma/different-cats/blob/911b9c1d6cb4439626323697c6f634d4cce9701d/Training%20model_.ipynb) 
I have recently run into an error after I enabled XLA devices  in tensor flow by :
os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'
After ward I was able to run predictions over the GPU but it gave following errors when I was fitting the model

**Code:**

```
history = model.fit(X_train, [y_train, y_train, y_train],
                    validation_split=0.1,
                    epochs=epochs, batch_size=256, )#callbacks=[lr_sc]
model.save('model.model')
```

**Result:**
```

Epoch 1/2
---------------------------------------------------------------------------
InternalError                             Traceback (most recent call last)
<ipython-input-8-b1480782523a> in <module>
----> 1 history = model.fit(X_train, [y_train, y_train, y_train],
      2                     validation_split=0.1,
      3                     epochs=epochs, batch_size=256, )#callbacks=[lr_sc]
      4 model.save('model.model')

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1098                 _r=1):
   1099               callbacks.on_train_batch_begin(step)
-> 1100               tmp_logs = self.train_function(iterator)
   1101               if data_handler.should_sync:
   1102                 context.async_wait()

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py in __call__(self, *args, **kwds)
    826     tracing_count = self.experimental_get_tracing_count()
    827     with trace.Trace(self._name) as tm:
--> 828       result = self._call(*args, **kwds)
    829       compiler = ""xla"" if self._experimental_compile else ""nonXla""
    830       new_tracing_count = self.experimental_get_tracing_count()

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
    886         # Lifting succeeded, so variables are initialized and we can run the
    887         # stateless function.
--> 888         return self._stateless_fn(*args, **kwds)
    889     else:
    890       _, _, _, filtered_flat_args = \

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py in __call__(self, *args, **kwargs)
   2940       (graph_function,
   2941        filtered_flat_args) = self._maybe_define_function(args, kwargs)
-> 2942     return graph_function._call_flat(
   2943         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   2944 

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1916         and executing_eagerly):
   1917       # No tape is watching; skip to running the function.
-> 1918       return self._build_call_outputs(self._inference_function.call(
   1919           ctx, args, cancellation_manager=cancellation_manager))
   1920     forward_backward = self._select_forward_and_backward_functions(

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    553       with _InterpolateFunctionError(self):
    554         if cancellation_manager is None:
--> 555           outputs = execute.execute(
    556               str(self.signature.name),
    557               num_outputs=self._num_outputs,

c:\users\rushi\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

InternalError:  dnn PoolBackward launch failed
	 [[node gradient_tape/inception_v1/average_pooling2d/AvgPool/AvgPoolGrad (defined at <ipython-input-8-b1480782523a>:1) ]] [Op:__inference_train_function_5930]

Function call stack:
train_function
```"
50599,ConvLSTM2D layer wrong computation,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.5

**Describe the current behavior**

There seems to be something wrong with the computation of the ConvLSTM2D layer. I tried to do the computation manually for a simplified example and I come to a different result. I think there might be a bug in the last round of the layer.  The issue is illustrated in the example below. 

**Describe the expected behavior**
The output that I expected from the simple example model is `[[[[[   0.], [   8.]]], [[[ 0.],[1053.]]]]]`, but instead the actual output is `[[[[[   0.], [   8.]]], [[[ 512.],[1053.]]]]]`.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
tf.keras.backend.set_floatx('float64')
model = keras.Sequential([
keras.layers.ConvLSTM2D(1, (1, 2), return_sequences=True, padding='valid', strides=(1,1),recurrent_activation='linear', activation='linear',  input_shape=(2, 1, 3, 1))])
w = model.get_weights()
w[0] = np.array([[[[1, 1, 1, 1]],[[1, 1, 1, 1]]]])
w[1] = np.array([[[[1, 1, 1, 1]],[[1, 1, 1, 1]]]])
w[2] = np.array([0, 0, 0, 0])
model.set_weights(w)
x = tf.constant([[[[[0], [0], [2]]], [[[0], [0], [1]]]]])
print (np.array2string(model.predict(x,steps=1), separator=', '))
```
"
50598,TFLite conversion of LSTM model does not work with multiple batch size,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Kaggle Notebook (standard ubuntu)
- TensorFlow installed from (source or binary): Pip install tensorflow==2.5.0
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7
- CUDA/cuDNN version: CPU Only
- GPU model and memory: CPU Only

**Describe the current behavior**

I trained (on gpu) and saving an LSTM model in keras and converted it to tflite. 
```
model = Sequential()
model.add(Masking(mask_value=-1, input_shape=(n_timesteps, n_features))) # This layer is used in the final model
model.add(LSTM(64, input_shape=(n_timesteps,n_features))) # This layer is used in the final model
model.add(RepeatVector(n_timesteps))
model.add(LSTM(64, return_sequences=True))
model.add(TimeDistributed(Dense(n_features)))
optimizer = Adam(learning_rate=0.001, epsilon=1e-04)
model.compile(optimizer= optimizer, loss='mse')
model.fit(x_train, x_train, epochs=1000, verbose=2)

# Saving
encoder = Model(inputs=model.inputs, outputs=model.layers[1].output)
encoder.save('encoder.h5')
```
![image](https://user-images.githubusercontent.com/5482978/124398609-6ff91a00-dccb-11eb-81e2-55bdaecc8b1f.png)


Using both experimental and non-experimental converter, I was able to convert the model to tflite where the shape signature is (-1, X, Y) where the first dimension is batch size.

```
from tensorflow.keras.models import load_model
encoder = load_model('encoder.h5')

# Following code from https://www.tensorflow.org/lite/convert/rnn
run_model = tf.function(lambda x: encoder(x))
BATCH_SIZE = None
STEPS = 6952
INPUT_SIZE = 20
concrete_func = run_model.get_concrete_function(tf.TensorSpec([BATCH_SIZE, STEPS, INPUT_SIZE], encoder.inputs[0].dtype))
encoder.save('/encoder', save_format=""tf"", signatures=concrete_func)
```

Saving the model as tflite:
```
converter = tf.lite.TFLiteConverter.from_saved_model('/encoder')
converter.experimental_new_converter = True
tflite_model = converter.convert()
with open('encoder.tflite', 'wb') as f:
  f.write(tflite_model)
```

Now when I use the model:
```
interpreter = tf.lite.Interpreter(model_path='encoder.tflite')
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
data = np.vstack([a,a]).astype(np.float32) # Shape: (2, 6952, 20)
interpreter.resize_tensor_input(input_details[0]['index'], data .shape) # Shape: (2, 6952, 20)
interpreter.resize_tensor_input(output_details[0]['index'], (data .shape[0], 64)) # Shape: (2, 64)
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0]['index'], data )
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
```

I get the following error on invoke:
```
RuntimeError: tensorflow/lite/kernels/concatenation.cc:80 t->dims->data[d] != t0->dims->data[d] (2 != 1)Node number 26 (CONCATENATION) failed to prepare.
Node number 28 (WHILE) failed to invoke.
```

**Describe the expected behavior**

An output of size (2,64)


Related issues that I've looked at https://github.com/tensorflow/tensorflow/issues/34620 https://github.com/tensorflow/tensorflow/issues/24607 https://github.com/tensorflow/tensorflow/issues/37012 "
50596,How to access elements of an input tensor in custom layers?,"I wanted to assign an element of input tensor X to an element of an output tensor Y inside a custom layer like 

Y[0,0,0,0] = X[0,1,1,1]

For example:

```
from keras.layers import Layer
import numpy as np
from keras.layers import Input
from keras.models import Model 

Images = [[[[255,0,0],[255,0,0]],[[255,0,0],[0,144,0]]]]

InputTensor = np.array(Images)

class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, X):
        Y = [[[[0,0,0,0]]]] # Output tensor
        Y = np.array(Y)
        Y[0,0,0,0] = X[0,1,1,1]   # <------ does not work because X[0,1,1,1] is a sequence instead a scalar
        return np.ndarray.tolist(Y)
    
inputlayer = Input(shape = InputTensor[0].shape)
layer = MyLayer()(inputlayer)

model = Model(inputlayer, layer ) 
model.summary()

Output = model.predict(InputTensor)
Output = np.array(Output)
print(""Output = ""+str(Output))
```

If the call-function returns X , then X[0,1,1,1] is not anymore a sequence after it passed the modell.
It make not sense to use custom layers if you have no access to the elements of an input tensor as expected.
For example:
```
from keras.layers import Layer
import numpy as np
from keras.layers import Input
from keras.models import Model 

Images = [[[[255,0,0],[255,0,0]],[[255,0,0],[0,144,0]]]]

InputTensor = np.array(Images)

class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, X):
        return X
    
inputlayer = Input(shape = InputTensor[0].shape)
layer = MyLayer()(inputlayer)

model = Model(inputlayer, layer ) 
model.summary()

Output = model.predict(InputTensor)
Output = np.array(Output)
print(""Output = ""+str(Output))
```

The Output is:
```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_78 (InputLayer)        [(None, 2, 2, 3)]         0         
_________________________________________________________________
my_layer_77 (MyLayer)        (None, 2, 2, 3)           0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
Output = [[[[255.   0.   0.][255.   0.   0.]][[255.   0.   0.][  0. 144.   0.]]]]
```"
50595,"Please support TFLite gradient operations: BroadcastGradientArgs, StridedSliceGrad and some other","Within our company we trained a breakthrough realtime model which requires automatic differentiation feature at runtime. To be more specific, the model's output should be forward pass outputs + derivative of runtime loss function with respect to some input control variables.

We use TFLite to run the model, but we can't use TF Select kernels because our runtime environment is very strict (video gaming consoles). In the current state, when we try to use `tf.gradients` within our model the TFLite converter does not convert because of some operations to be not supported: `BroadcastGradientArgs`, `DynamicStitch`, `EluGrad`, `Sign`, `StridedSliceGrad`, `UnsortedSegmentSum`

As a workaround, we approximate the derivatives with finite difference method, but obviously it requires to make extra forward passes + it introduces some numerical instability to the model. It would be very helpful to have cheap analytical derivatives with a backpropagation step within TFLite.

**System information**
- `Linux rb15 5.12.14-arch1-1 #1 SMP PREEMPT Thu, 01 Jul 2021 07:26:06 +0000 x86_64 GNU/Linux`
- `TensorFlow 2.5.0` from `tensorflow-opt-cuda` package (https://archlinux.org/packages/community/x86_64/tensorflow-opt-cuda)

**Provide the text output from tflite_convert**
```
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: BroadcastGradientArgs, DynamicStitch, EluGrad, Sign, StridedSliceGrad, UnsortedSegmentSum
Details:
	tf.BroadcastGradientArgs {device = """"}
	tf.DynamicStitch {device = """"}
	tf.EluGrad {device = """"}
	tf.Sign {device = """"}
	tf.StridedSliceGrad {begin_mask = 0 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 0 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 1 : i64}
	tf.StridedSliceGrad {begin_mask = 0 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 0 : i64, device = """", ellipsis_mask = 1 : i64, end_mask = 0 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 1 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 0 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 1 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 1 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 2 : i64}
	tf.StridedSliceGrad {begin_mask = 1 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 1 : i64, new_axis_mask = 2 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 3 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 0 : i64}
	tf.StridedSliceGrad {begin_mask = 3 : i64, device = """", ellipsis_mask = 0 : i64, end_mask = 3 : i64, new_axis_mask = 0 : i64, shrink_axis_mask = 4 : i64}
	tf.UnsortedSegmentSum {device = """"}
```"
50594,tensorflow.data.experimental.load not working inside tensorflow.data.Dataset().flatmap,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 and Google Colab 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: CPU 8GB


**Describe the current behavior** tensorflow.data.experimental.load should return dataset when called from inside Dataset.interleave's map_function.

**Describe the expected behavior** Throwing error 'TypeError: expected str, bytes or os.PathLike object, not Tensor'

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):yes
- Briefly describe your candidate solution(if contributing): Create a constructor just like 'TextLineDataset' constructor which takes tensor and also strings of file paths.

**Standalone code to reproduce the issue**
````
import tensorflow as tf
path = ""saved_data""

# Save a dataset
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)



filenames = [path]
dataset = tf.data.Dataset.from_tensor_slices(filenames)
def parse_fn(filename):
  print(filename)
  return tf.data.experimental.load(filename)
dataset = dataset.flat_map(lambda x:
    parse_fn(x))

for item in dataset.as_numpy_iterator():
    print(item)
````

**Other info / logs** 

````
Traceback (most recent call last):
  File ""C:/Users/kurud/Documents/ineaurondeeplearn/internship/DrowsyDetectCNNmodel/DataPipelineIssues.py"", line 19, in <module>
    dataset = dataset.interleave(lambda x:
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 2065, in interleave
    return InterleaveDataset(self, map_func, cycle_length, block_length)
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 4596, in __init__
    self._map_func = StructuredFunctionWrapper(
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3712, in __init__
    self._function = fn_factory()
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\eager\function.py"", line 3134, in get_concrete_function
    graph_function = self._get_concrete_function_garbage_collected(
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\eager\function.py"", line 3100, in _get_concrete_function_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\eager\function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\eager\function.py"", line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\framework\func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3687, in wrapped_fn
    ret = wrapper_helper(*args)
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\ops\dataset_ops.py"", line 3617, in wrapper_helper
    ret = autograph.tf_convert(self._func, ag_ctx)(*nested_args)
  File ""C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 695, in wrapper
Tensor(""args_0:0"", shape=(), dtype=string)
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    C:/Users/kurud/Documents/ineaurondeeplearn/internship/DrowsyDetectCNNmodel/DataPipelineIssues.py:20 None  *
        lambda x:
    C:/Users/kurud/Documents/ineaurondeeplearn/internship/DrowsyDetectCNNmodel/DataPipelineIssues.py:18 parse_fn  *
        return tf.data.experimental.load(filename)
    C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\experimental\ops\io.py:227 load  **
        return _LoadDataset(
    C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\site-packages\tensorflow\python\data\experimental\ops\io.py:134 __init__
        with gfile.GFile(os.path.join(path, DATASET_SPEC_FILENAME), ""rb"") as f:
    C:\Users\kurud\Anaconda3\envs\DrowsyDetectCNNmodel\lib\ntpath.py:78 join
        path = os.fspath(path)

    TypeError: expected str, bytes or os.PathLike object, not Tensor
````"
50593,Multi-GPU doesn't work for model(inputs) nor when computing the gradients,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Desktop
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.0/8
- GPU model and memory: RTX5000-16GB

**Describe the current behavior**

When using multiple GPUs to perform inference on a model (e.g. the call method: `model(inputs)`) and calculate its gradients, the machine only uses one GPU, leaving the rest idle. 

**Describe the expected behavior**

I feel this should be the same as using `model.predict()` where all the GPUs are working. 

**Standalone code to reproduce the issue**

```
import tensorflow as tf
import numpy as np
import os

# Make the tf-data
path_filename_records = 'your_path_to_records'
bs = 128

dataset = tf.data.TFRecordDataset(path_filename_records)
dataset = (dataset
           .map(parse_record, num_parallel_calls=tf.data.experimental.AUTOTUNE)
           .batch(bs)
           .prefetch(tf.data.experimental.AUTOTUNE)
          )

# Load model trained using MirroredStrategy
path_to_resnet = 'your_path_to_resnet'
mirrored_strategy = tf.distribute.MirroredStrategy()
with mirrored_strategy.scope():
    resnet50 = tf.keras.models.load_model(path_to_resnet)

for pre_images, true_label in dataset:
    with tf.GradientTape() as tape:
       tape.watch(pre_images)
       outputs = resnet50(pre_images)
       grads = tape.gradient(outputs, pre_images)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

You can profile the behavior of the GPUs with nvidia-smi. I don't know if it is supposed to be like this, both the `model(inputs)` and `tape.gradient` to not have multi-GPU support. But if it is, then it's a big problem because if you have a large dataset and need to calculate the gradients with respect to the inputs (e.g. interpretability porpuses) it might take days with one GPU. 
Another thing I tried was using `model.predict()` but this isn't possible with `tf.GradientTape`.
"
50592,Closing as stale. Please reopen if you'd like to work on this further.,"Closing as stale. Please reopen if you'd like to work on this further.

_Originally posted by @google-ml-butler[bot] in https://github.com/tensorflow/tensorflow/issues/49920#issuecomment-873439262_"
50591,Undefined symbols for architecture arm64,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac BigSur
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Ipad 2021
- TensorFlow installed from (source or binary): 
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:  Mac Mini 2012

Sorry, I'm beginner programmer
I don't know how to check some above info,
But it's not about hardward or the above I think

**Describe the problem**
I want to run mnist on ipad but get problem
Undefined symbols for architecture arm64:
  ""_OBJC_CLASS_$_MLModelConfiguration"", referenced from:
      objc-class-ref in TensorFlowLiteCCoreML
  ""_OBJC_CLASS_$_MLModel"", referenced from:
      objc-class-ref in TensorFlowLiteCCoreML
  ""_OBJC_CLASS_$_MLPredictionOptions"", referenced from:
      objc-class-ref in  
  ""_OBJC_CLASS_$_MLFeatureValue"", referenced from:
      objc-class-ref in TensorFlowLiteCCoreML
  ""_OBJC_CLASS_$_MLMultiArray"", referenced from:
      objc-class-ref in TensorFlowLiteCCoreML
ld: symbol(s) not found for architecture arm64
clang: error: linker command failed with exit code 1 (use -v to see invocation)


I think this may be solution
https://github.com/tensorflow/tensorflow/issues/41039
but I don't know how to set it

"
50590,C++ compilation of rule error with/ Error C2664 -> Build did NOT complete successfully,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 64 bit
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): 3.7.2
- CUDA/cuDNN version: CUDA: 11.3, 8.0 
- GPU model and memory: RTX 2060



**Describe the problem**
I'm trying to benchmark my tensorflow-lite models and I'm trying to follow along the [guide](https://www.tensorflow.org/lite/performance/measurement)
I'm at the step where I'm building the native benchmark binary from source on my PC, but it results to an error.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I'm executing the following command, but it results to an error
`bazel build -c opt --cxxopt=/std:c++latest //tensorflow/lite/tools/benchmark:benchmark_model`

I'm getting the following error:
```
ERROR: /tensorflow/tensorflow/lite/delegates/external/BUILD:23:11: C++ compilation of rule '//tensorflow/lite/delegates/external:external_delegate' failed (Exit 2): cl.exe failed: error ex
ecuting command
  cd C:/users/administrator/_bazel_administrator/aj7l7svv/execroot/org_tensorflow
  SET ANDROID_BUILD_TOOLS_VERSION=30.0.3
    SET ANDROID_NDK_API_LEVEL=21
    SET ANDROID_NDK_HOME=Sdk/android-ndk-r21e-windows-x86_64/android-ndk-r21e
    SET ANDROID_SDK_API_LEVEL=30
    SET ANDROID_SDK_HOME=Sdk/
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30037\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30037\include;C:\Prog
ram Files (x86)\Windows Kits\NETFXSDK\4.6.1\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits
\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.29.30037\bin\HostX
64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program File
s (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (
x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\
Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.6.1 Tools\x64\;C:\Program
 Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual S
tudio\2019\Community\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Commu
nity\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Co
mmon7\IDE\CommonExtensions\Microsoft\CMake\Ninja
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/ProgramData/Anaconda3/envs/tf-n-gpu/python.exe
    SET PYTHON_LIB_PATH=C:/ProgramData/Anaconda3/envs/tf-n-gpu/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\ADMINI~1\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TMP=C:\Users\ADMINI~1\AppData\Local\Temp
  C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.29.30037/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS
 /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw /W0 /D_US
E_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /std:c++14 /std:c++latest /Fobazel-out/x64_windows-opt/bin/tensorflow/lite/delegates/external/_objs/external_delegate/
external_delegate.obj /c tensorflow/lite/delegates/external/external_delegate.cc
Execution platform: @local_execution_config_platform//:platform
cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
cl : Command line warning D9025 : overriding '/std:c++14' with '/std:c++latest'
tensorflow/lite/delegates/external/external_delegate.cc(35): error C2664: 'void *tflite::SharedLibrary::LoadLibraryA(const wchar_t *)': cannot convert argument 1 from 'const _Elem *' to 'const wchar_t *'
        with
        [
            _Elem=char
        ]
tensorflow/lite/delegates/external/external_delegate.cc(35): note: Types pointed to are unrelated; conversion requires reinterpret_cast, C-style cast or function-style cast
.\tensorflow/lite/shared_library.h(32): note: see declaration of 'tflite::SharedLibrary::LoadLibraryA'
Target //tensorflow/lite/tools/benchmark:benchmark_model failed to build
```"
50589,W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found,"**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): pip 
- TensorFlow version: 2.5.0
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: pip

- CUDA/cuDNN version: 11.4
- GPU model and memory: NVIDIA GTX 1650Ti 4GB



Upon writing `import tensorflow as tf` in the command prompt-bases python interpreter, it displays the following error



![image](https://user-images.githubusercontent.com/70141886/124354459-bdbd4600-dc29-11eb-9d18-178768bf98d7.png)
![image](https://user-images.githubusercontent.com/70141886/124354446-a8481c00-dc29-11eb-9c96-a7728cfbbc66.png)
![image](https://user-images.githubusercontent.com/70141886/124354485-d75e8d80-dc29-11eb-8e13-724ecd456265.png)
![image](https://user-images.githubusercontent.com/70141886/124354596-6c618680-dc2a-11eb-950a-0fa956682c83.png)


I have even set the 4 PATHs to be set for use

`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.4\bin`
`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.4\libnvvp`
`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.4\include`
`C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.4\extras\CUPTI\lib64`


I have gone through previous issues and other Google links before deciding to come here to create this issue. I have also restarted my computer several times in the hope that the path is updated, if that was the cause of this error. Please let me know how I can fix this problem. Thank you"
50587,"custom implementation request: SparseReorder, SparseTensorDenseMatMul","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows10, x86-64
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 2.7.0-dev20210702


**Provide the text output from tflite_convert**

```
2021-07-02 16:52:27.481365: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1851] The following operation(s) need TFLite custom op implementation(s):
Custom ops: SparseReorder, SparseTensorDenseMatMul
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.

Details:
	tf.SparseReorder(tensor<?x3xi64>, tensor<?xf32>, tensor<3xi64>) -> (tensor<?x3xi64>, tensor<?xf32>) : {T = f32, device = """"}
	tf.SparseTensorDenseMatMul(tensor<?x2xi64>, tensor<?xf32>, tensor<2xi64>, tensor<6400x256xf32>) -> (tensor<?x256xf32>) : {T = f32, Tindices = i64, adjoint_a = false, adjoint_b = false, device = """"}
"
50585,Error in using next() method with DirectoryIterator,"`
train_images, train_labels = next(batch_obj['train'])
`

errors out with,

`
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-6-674a9be77381> in <module>
----> 1 train_images, train_labels = next(batch_obj['train'])

/opt/conda/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in __next__(self, *args, **kwargs)
    102 
    103     def __next__(self, *args, **kwargs):
--> 104         return self.next(*args, **kwargs)
    105 
    106     def next(self):

/opt/conda/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in next(self)
    114         # The transformation of images is not under thread lock
    115         # so it can be done in parallel
--> 116         return self._get_batches_of_transformed_samples(index_array)
    117 
    118     def _get_batches_of_transformed_samples(self, index_array):

/opt/conda/lib/python3.8/site-packages/keras_preprocessing/image/iterator.py in _get_batches_of_transformed_samples(self, index_array)
    229                            target_size=self.target_size,
    230                            interpolation=self.interpolation)
--> 231             x = img_to_array(img, data_format=self.data_format)
    232             # Pillow images should be closed after `load_img`,
    233             # but not PIL images.

/opt/conda/lib/python3.8/site-packages/keras_preprocessing/image/utils.py in img_to_array(img, data_format, dtype)
    307     # or (channel, height, width)
    308     # but original PIL image has format (width, height, channel)
--> 309     x = np.asarray(img, dtype=dtype)
    310     if len(x.shape) == 3:
    311         if data_format == 'channels_first':

/opt/conda/lib/python3.8/site-packages/numpy/core/_asarray.py in asarray(a, dtype, order)
     81 
     82     """"""
---> 83     return array(a, dtype, copy=False, order=order)
     84 
     85 

TypeError: __array__() takes 1 positional argument but 2 were given
`"
50584,RBF Custom Layer in Tensorflow can not learn the centers (mu) parameters,"Using Google Colab with TF version 2.5
"
50583,[Colab TPU] [TF 2.5]  UnavailableError: Socket closed,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  - Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
  - Google Colab  enviroment
- TensorFlow version (use command below):
 - 2.5
- Python version:
  - 3.7.10

### Context: 
My model was training ok untill a certain number of steps then throwing the error mentioned in issue #50522 which i then came to realize that it was related to the in my tfrecord files causing different batch output sizes. As an attempt to fix it, i adapted the code to run with a dinamic batch size by using  ```tf.shape(x)```.  Coincidentally (or not) when trying to perform training the error ```UnavailableError: Socket closed``` began showing up.  After further research, as pointed by the [doc](https://cloud.google.com/tpu/docs/troubleshooting#dynamic_shapes_not_supported) i removed back the tf.shape op and added ```drop_remainder=True``` to my dataset.batch config. Despite that the error persisted, although after enabling drop_remainder the code was identical to the previous working version.

Obs: Dropping remainders and tf.shape were added as an attempt to deal with batch lenght different than the original configuration, as the data needs to be properly reshaped accordingly to the length of the augmented batch and the batch_size varies accordingly to the number of samples within my tfrecords that are not multiples. 

Code: 
```python

def train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, model):
  data_augmentation = preprocessing_model()

  train_filenames = get_filenamesTPU(train_path)
  random.shuffle(train_filenames)

  validation_filenames = get_filenamesTPU(validation_path)
  random.shuffle(validation_filenames)

  dataset_length = 91758  
  train_size =  dataset_length * 0.7
  validation_size = dataset_length - train_size

  batch_size = 42 * tpu_strategy.num_replicas_in_sync

  shape = 32 * batch_size

  data_reshape = lambda x,y: (tf.reshape(x,shape=(shape,224,224,3)), (tf.reshape(y[0],shape=(shape,1000)), tf.reshape(y[1],shape=(shape,516)),tf.reshape(y[2],shape=(shape,124))))
  
  augmentation_pipeline = lambda x,y: (data_augmentation(tf.expand_dims(x,axis=0)),(tf.tile(tf.reshape(y[0],[1,1000]),[32,1]),tf.tile(tf.reshape(y[1],[1,516]),[32,1]),tf.tile(tf.reshape(y[2],[1,124]),[32,1]))) # apply augment then tile the labels to the correct length
  
  AUTO = tf.data.AUTOTUNE
  train_dataset = tf.data.TFRecordDataset(buffer_size=int(1e+8),num_parallel_reads=AUTO,filenames=train_filenames).map(parsing_fn,num_parallel_calls=AUTO).shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)
  train_dataset = train_dataset.map(augmentation_pipeline, num_parallel_calls=AUTO).batch(batch_size=batch_size, drop_remainder=True)
  train_dataset = train_dataset.map(data_reshape,num_parallel_calls=AUTO)
  train_dataset = train_dataset.repeat()
  train_dataset = train_dataset.prefetch(AUTO)
 
  # Create a validation dataset
  validation_dataset = tf.data.TFRecordDataset(num_parallel_reads=AUTO,filenames=validation_filenames).map(parsing_fn,num_parallel_calls=AUTO)
  validation_dataset = validation_dataset.batch(batch_size)
  validation_dataset = validation_dataset.prefetch(AUTO)
  validation_dataset = validation_dataset.repeat(1)

  validation_steps = validation_size / batch_size 
  history = model.fit(x=train_dataset,
                          epochs=epochs,
                          steps_per_epoch=steps_per_epoch,                        
                          validation_data=validation_dataset,
                          validation_steps=validation_steps)
  return history
```
### Perform training
```python
loss={
        ""class_0"": 'CategoricalCrossentropy',
        ""class_1"": 'CategoricalCrossentropy',
        ""class2"": 'CategoricalCrossentropy',
},

metrics = ['categorical_accuracy']
optimizer = Adam(learning_rate=5e-3)

weight_file = None
with tpu_strategy.scope():
  resnet_50V2 = load_and_configure_model(optimizer, loss, metrics, weight_file)
resnet_50V2 = load_and_configure_model(optimizer, loss, metrics, weight_file)
base_directory = 'gs://2015_tfrecords'

train_path = base_directory+'/train/'
validation_path = base_directory+'/validation/'

buffer_size = 10240
epochs = 30
steps_per_epoch = 192

resnet_50V2.summary()
history = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)
plot_training_history3(history)
```

### Error
```
==================================================================================================
Total params: 26,925,160
Trainable params: 7,825,000
Non-trainable params: 19,100,160
__________________________________________________________________________________________________
Epoch 1/30

---------------------------------------------------------------------------

UnavailableError                          Traceback (most recent call last)

<ipython-input-48-3332ce7f2ca9> in <module>()
      4 
      5 resnet_50V2.summary()
----> 6 history = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)
      7 plot_training_history3(history)
      8 

14 frames

/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

UnavailableError: Socket closed
```"
50582,(0) Invalid argument:  In[0] mismatch In[1] shape: ," `tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.`
`     (0) Invalid argument:  In[0] mismatch In[1] shape: 1108 vs. 1120: [42,1108] [1120,256] 0 0`
 `   [[node model/dense/MatMul (defined at rnn_flickr_fit.py:273) ]]`
`   (1) Invalid argument:  In[0] mismatch In[1] shape: 1108 vs. 1120: [42,1108] [1120,256] 0 0`
   ` [[node model/dense/MatMul (defined at rnn_flickr_fit.py:273) ]]`
  `  [[Adam/Cast_6/ReadVariableOp/_6]]`
 `   0 successful operations.`
`    0 derived errors ignored. [Op:__inference_train_function_7431]`
    
   ` Function call stack:`
`    train_function -> train_function`
    
 `   2021-07-02 14:23:52.766840: W tensorflow/core/kernels/data/generator_dataset_op.cc:107] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.`
`    	 [[{{node PyFunc}}]]`

what should i check to solve it 

code is 

`def define_model(vocab_size, max_length, curr_shape):`
 `   inputs1 = Input(shape=curr_shape)`
  `  fe1 = Dropout(0.5)(inputs1)`
   ` fe2 = Dense(256, activation='relu')(fe1)`
    `inputs2 = Input(shape=(max_length,))`
  `  se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)`
   ` se2 = Dropout(0.5)(se1)`
   ` se3 = LSTM(256)(se2)`
   ` decoder1 = Concatenate()([fe2, se3])`
    `decoder2 = Dense(256, activation='relu')(decoder1)`
    `outputs = Dense(vocab_size, activation='softmax')(decoder2)`
    `model = Model(inputs=[inputs1, inputs2], outputs=outputs)`
   ` model.compile(loss='categorical_crossentropy', optimizer='adam')`
    `return model`"
50579,tf.data.Dataset.cache('filename') loads entire dataset into memory,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): bin (both pip and docker)
- TensorFlow version (use command below): happens on 2.4.1, 2.5.0 and nightly (2.7.0-dev20210630)
- Python version: 3.8.5

**Describe the current behavior**
When caching a data set on local disk Tensorflow first loads the entire data set into memory. This obviously crashes the job since we are using Dataset and caching on disk because of memory restrictions.

**Describe the expected behavior**
I expected Tensorflow to continuously write to disk while reading the data set from where ever it is coming from. In a streaming manner.  
Alternatively, I could imagine a chunking approach where Tensorflow writes multiple files in the cache to limit the amount of data held in memory at any one time (or writes multiple times to the same file).

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
[gist](https://gist.github.com/grofte/f617f12a8abef1d085ecaf64500be8b4)

```python
import tensorflow as tf
import tensorflow_datasets as tfds

# This data set is too big to run in Colaboratory, ~30 GB
train = tfds.load(name=""bair_robot_pushing_small:2.0.0"", split=""train"",
                  shuffle_files=False, data_dir='./tensorflow_datasets')
train = train.cache('./cache/train')

# We need to do some kind of operation over the data to trigger the caching
batch_size = 64
n = train.batch(batch_size).reduce(0, lambda accum, _: accum + batch_size)
print(n)
```

This is of course a minimal example. We first noticed this problem when loading our from TFRecords files. Caching on file is bad for the training data since you lose a lot of shuffling ability but it's fine for the validation data. If we cache it on a local disk then we can avoid network traffic if the data is elsewhere and the local disk is faster too in our case. So it is still useful - just not when it crashes your job. And really, we could run these training jobs faster if we cached the validation data in memory (but several jobs run on the same machines so we just have to get lucky and not have two RAM intensive parts running at the same time).

"
50577,CMAKE ERROR for Android_arm64 of TFLite on MacOS.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **MacOS Big Sur 11.2.3**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **source**
- TensorFlow version: **2.6.0**
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):  **cmake 3.20.0 && androidNDK-r20b**
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
CMAKE ERROR occurred when I cmake andoird_arm64 binary of TF-Lite.



**Provide the exact sequence of commands / steps that you executed before running into the problem**
I used cmake cmmand like [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/guide/build_cmake.md](url) says:

```
mkdir tflite_build
cd tflite_build
cmake -DCMAKE_TOOLCHAIN_FILE=<NDK path>/build/cmake/android.toolchain.cmake \
  -DANDROID_ABI=arm64-v8a ../tensorflow_src/tensorflow/lite
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
CMAKE LOGs in terminal:

```
-- Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.
-- ANDROID_PLATFORM not set. Defaulting to minimum supported version
16.
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: /Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: /Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++ - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
TFLITE_ENABLE_XNNPACK = OFF
TFLITE_ENABLE_GPU = ON
-- Looking for pthread.h
-- Looking for pthread.h - found
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD
-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
-- Looking for pthread_create in pthreads
-- Looking for pthread_create in pthreads - not found
-- Looking for pthread_create in pthread
-- Looking for pthread_create in pthread - not found
-- Check if compiler accepts -pthread
-- Check if compiler accepts -pthread - yes
-- Found Threads: TRUE  
-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11
-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11 - Success
-- Performing Test COMPILER_SUPPORT_std=cpp03
-- Performing Test COMPILER_SUPPORT_std=cpp03 - Success
-- Performing Test standard_math_library_linked_to_automatically
-- Performing Test standard_math_library_linked_to_automatically - Failed
-- Performing Test standard_math_library_linked_to_as_m
-- Performing Test standard_math_library_linked_to_as_m - Failed
CMake Error at build_arm64/eigen/CMakeLists.txt:105 (message):
  Can't link to the standard math library.  Please report to the Eigen
  developers, telling them about your platform.


-- Configuring incomplete, errors occurred!
See also ""/lite/build_arm64/CMakeFiles/CMakeOutput.log"".
See also ""/lite/build_arm64/CMakeFiles/CMakeError.log"".
```

And some pieces of “CMakeError.log”:
```
Performing C SOURCE FILE Test CMAKE_HAVE_LIBC_PTHREAD failed with the following output:
Change Dir: /Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp

Run Build Command(s):/usr/bin/make -f Makefile cmTC_0c08b/fast && /Applications/Xcode.app/Contents/Developer/usr/bin/make  -f CMakeFiles/cmTC_0c08b.dir/build.make CMakeFiles/cmTC_0c08b.dir/build
Building C object CMakeFiles/cmTC_0c08b.dir/src.c.o
/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang --target=aarch64-none-linux-android21 --gcc-toolchain=/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64 --sysroot=/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/sysroot -DCMAKE_HAVE_LIBC_PTHREAD  -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -Wa,--noexecstack -Wformat -Werror=format-security   -fPIE -MD -MT CMakeFiles/cmTC_0c08b.dir/src.c.o -MF CMakeFiles/cmTC_0c08b.dir/src.c.o.d -o CMakeFiles/cmTC_0c08b.dir/src.c.o -c /Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.c
/Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.c:13:3: warning: implicit declaration of function 'pthread_cancel' is invalid in C99 [-Wimplicit-function-declaration]
  pthread_cancel(thread);
  ^
1 warning generated.
Linking C executable cmTC_0c08b
/usr/local/Cellar/cmake/3.20.0/bin/cmake -E cmake_link_script CMakeFiles/cmTC_0c08b.dir/link.txt --verbose=1
/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang --target=aarch64-none-linux-android21 --gcc-toolchain=/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64 --sysroot=/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/sysroot -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -Wa,--noexecstack -Wformat -Werror=format-security   -Wl,--exclude-libs,libgcc.a -Wl,--exclude-libs,libatomic.a -static-libstdc++ -Wl,--build-id -Wl,--warn-shared-textrel -Wl,--fatal-warnings -Wl,--no-undefined -Qunused-arguments -Wl,-z,noexecstack -Wl,--gc-sections   CMakeFiles/cmTC_0c08b.dir/src.c.o -o cmTC_0c08b  -latomic -lm 
CMakeFiles/cmTC_0c08b.dir/src.c.o: In function `main':
/Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.c:13: undefined reference to `pthread_cancel'
/Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.c:13: undefined reference to `pthread_cancel'
clang: error: linker command failed with exit code 1 (use -v to see invocation)
make[1]: *** [cmTC_0c08b] Error 1
make: *** [cmTC_0c08b/fast] Error 2
```

```
Performing C++ SOURCE FILE Test standard_math_library_linked_to_automatically failed with the following output:
Change Dir: /Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp

Run Build Command(s):/usr/bin/make -f Makefile cmTC_6117b/fast && /Applications/Xcode.app/Contents/Developer/usr/bin/make  -f CMakeFiles/cmTC_6117b.dir/build.make CMakeFiles/cmTC_6117b.dir/build
Building CXX object CMakeFiles/cmTC_6117b.dir/src.cxx.o
/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/bin/clang++ --target=aarch64-none-linux-android21 --gcc-toolchain=/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64 --sysroot=/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/sysroot -Dstandard_math_library_linked_to_automatically  -g -DANDROID -fdata-sections -ffunction-sections -funwind-tables -fstack-protector-strong -no-canonical-prefixes -fno-addrsig -Wa,--noexecstack -Wformat -Werror=format-security   -std=c++03  -fPIE -MD -MT CMakeFiles/cmTC_6117b.dir/src.cxx.o -MF CMakeFiles/cmTC_6117b.dir/src.cxx.o.d -o CMakeFiles/cmTC_6117b.dir/src.cxx.o -c /Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.cxx
In file included from /Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.cxx:2:
/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/sysroot/usr/include/c++/v1/cmath:622:68: error: too many arguments provided to function-like macro invocation
  static_assert(is_same<_FloatT, float>::value || is_same<_FloatT, double>::value
                                                                   ^
/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/sysroot/usr/include/c++/v1/__config:861:13: note: macro 'static_assert' defined here
#    define static_assert(__b, __m) _Static_assert(__b, __m)
            ^
In file included from /Users/Desktop/TFLite_test/lite/build_arm64/CMakeFiles/CMakeTmp/src.cxx:2:
/Users/Desktop/android-ndk-r20b/toolchains/llvm/prebuilt/darwin-x86_64/sysroot/usr/include/c++/v1/cmath:622:3: error: use of undeclared identifier 'static_assert'
  static_assert(is_same<_FloatT, float>::value || is_same<_FloatT, double>::value
  ^
2 errors generated.
make[1]: *** [CMakeFiles/cmTC_6117b.dir/src.cxx.o] Error 1
make: *** [cmTC_6117b/fast] Error 2
```

**Seems like there are some problems in ndk who doesn't contain full pthread supports？？？Please guys, I need your help ! thanks a lot!**"
50576,add multiple classifications  without network,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):arm-linux
- TensorFlow installed from (source or binary):source
- TensorFlow version (or github SHA if from source):tensorflow1.13.0
hello!
Users don't understand any code, and they may be in a environment without network. I'm using the classification algorithm model.How can users add multiple categories to the usage model? In addition, in model deployment, I use C + + to deploy the tflite model. What do I need to do on the tflite model node, hoping to give me a little idea?"
50575,Grappler error when Softmax input has a variable dimension,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from: binary
- TensorFlow version: 2.5.0
- Python version: 3.6.9
- CUDA/cuDNN version: 11.2/8.1.0
- GPU model and memory: NVIDIA Quadro P620 (4GB)

**Describe the current behavior**

The grappler pass is logging an error when Softmax is used in a `tf.function` with one or more variable dimensions (e.g. a variable batch size). This log started appearing in TensorFlow 2.5.

It does not seem to cause issues when running the model, but it might indicate a bug in the grappler implementation.

**Describe the expected behavior**

This operation should not produce any warning or error.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? No
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

input_signature = (tf.TensorSpec([None, 20], tf.float32),)
softmax = tf.function(tf.nn.softmax, input_signature=input_signature)
softmax(tf.random.uniform([2, 20]))
```

**Other info / logs**

The code above logs the following error:

> `2021-07-02 07:34:12.478300: W tensorflow/core/grappler/costs/op_level_cost_estimator.cc:689] Error in PredictCost() for the op: op: ""Softmax"" attr { key: ""T"" value { type: DT_FLOAT } } inputs { dtype: DT_FLOAT shape { unknown_rank: true } } device { type: ""GPU"" vendor: ""NVIDIA"" model: ""Quadro P620"" frequency: 1442 num_cores: 4 environment { key: ""architecture"" value: ""6.1"" } environment { key: ""cuda"" value: ""11020"" } environment { key: ""cudnn"" value: ""8100"" } num_registers: 65536 l1_cache_size: 24576 l2_cache_size: 524288 shared_memory_size_per_multiprocessor: 98304 memory_size: 3092316160 bandwidth: 96128000 } outputs { dtype: DT_FLOAT shape { unknown_rank: true } }`"
50574,"build fails on AArch64, Fedora 33 due to --config=numa, missing sysctl.h","[jw@cn05 master]$ bazel --output_user_root=/tmp/bazel build --local_ram_resources=6144 --config=numa //tensorflow/tools/pip_package:build_pip_package --verbose_failures
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from /data/jw/tensorflow/master/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /data/jw/tensorflow/master/.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from /data/jw/tensorflow/master/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/usr/bin/python3 --action_env PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages --python_path=/usr/bin/python3
INFO: Found applicable config definition build:short_logs in file /data/jw/tensorflow/master/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /data/jw/tensorflow/master/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:numa in file /data/jw/tensorflow/master/.bazelrc: --define=with_numa_support=true
INFO: Found applicable config definition build:linux in file /data/jw/tensorflow/master/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false
INFO: Found applicable config definition build:dynamic_kernels in file /data/jw/tensorflow/master/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: /tmp/bazel/b636c1968d8e13825d4613e0f3304062/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 0 targets configured).
INFO: Found 1 target...
ERROR: /tmp/bazel/b636c1968d8e13825d4613e0f3304062/external/hwloc/BUILD.bazel:229:11: C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1): gcc failed: error executing command
  (cd /tmp/bazel/b636c1968d8e13825d4613e0f3304062/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/FJSVstclanga/default/lib64:/opt/FJSVstclanga/default/lib64: \
    PATH=/home/jw/c4aarch64_installer/bin:/home/jw/.sdkman/candidates/groovy/current/bin:/home/jw/.sdkman/candidates/gradle/current/bin:/opt/FJSVstclanga/default/bin:/usr/lib64/qt-3.3/bin:/usr/condabin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/jw/.composer/vendor/bin:/home/jw/.dotnet/tools:/var/lib/snapd/snap/bin:/opt/FJSVstclanga/default/bin:/home/jw/.local/bin:/home/jw/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib64/ccache/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/aarch64-opt/bin/external/hwloc/_objs/hwloc/topology.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/hwloc/_objs/hwloc/topology.o' -iquoteexternal/hwloc -iquotebazel-out/aarch64-opt/bin/external/hwloc -isystem external/hwloc/hwloc -isystem bazel-out/aarch64-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/aarch64-opt/bin/external/hwloc/include -w -DAUTOLOAD_DYNAMIC_KERNELS -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/hwloc/hwloc/topology.c -o bazel-out/aarch64-opt/bin/external/hwloc/_objs/hwloc/topology.o)
Execution platform: @local_execution_config_platform//:platform
external/hwloc/hwloc/topology.c:45:10: fatal error: sys/sysctl.h: No such file or directory
   45 | #include <sys/sysctl.h>
      |          ^~~~~~~~~~~~~~
compilation terminated.
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /data/jw/tensorflow/master/tensorflow/lite/toco/python/BUILD:89:10 C++ compilation of rule '@hwloc//:hwloc' failed (Exit 1): gcc failed: error executing command
  (cd /tmp/bazel/b636c1968d8e13825d4613e0f3304062/execroot/org_tensorflow && \
  exec env - \
    LD_LIBRARY_PATH=/opt/FJSVstclanga/default/lib64:/opt/FJSVstclanga/default/lib64: \
    PATH=/home/jw/c4aarch64_installer/bin:/home/jw/.sdkman/candidates/groovy/current/bin:/home/jw/.sdkman/candidates/gradle/current/bin:/opt/FJSVstclanga/default/bin:/usr/lib64/qt-3.3/bin:/usr/condabin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/jw/.composer/vendor/bin:/home/jw/.dotnet/tools:/var/lib/snapd/snap/bin:/opt/FJSVstclanga/default/bin:/home/jw/.local/bin:/home/jw/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/bin/python3 \
    PYTHON_LIB_PATH=/usr/lib/python3.9/site-packages \
    TF2_BEHAVIOR=1 \
  /usr/lib64/ccache/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections -fdata-sections -MD -MF bazel-out/aarch64-opt/bin/external/hwloc/_objs/hwloc/topology.d '-frandom-seed=bazel-out/aarch64-opt/bin/external/hwloc/_objs/hwloc/topology.o' -iquoteexternal/hwloc -iquotebazel-out/aarch64-opt/bin/external/hwloc -isystem external/hwloc/hwloc -isystem bazel-out/aarch64-opt/bin/external/hwloc/hwloc -isystem external/hwloc/include -isystem bazel-out/aarch64-opt/bin/external/hwloc/include -w -DAUTOLOAD_DYNAMIC_KERNELS -I. -Ihwloc -Iinclude -Wno-vla '-DHWLOC_DUMPED_HWDATA_DIR=' '-DRUNSTATEDIR=' -fno-canonical-system-headers -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -c external/hwloc/hwloc/topology.c -o bazel-out/aarch64-opt/bin/external/hwloc/_objs/hwloc/topology.o)
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 3.058s, Critical Path: 0.74s
INFO: 98 processes: 49 internal, 49 local.
FAILED: Build did NOT complete successfully
[jw@cn05 master]$ 

Cf. issue #104801, which refers also to #45861. "
50571,custom trained ssd_mobilenet_v1_fpn_shared_coco model cannot detect object with Tensorflow,"### System information
-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MacOS BigSur version:11.2.1
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:Not
-   **TensorFlow installed from (source or binary)**:source
-   **TensorFlow version (use command below)**:1.14.0
-   **Python version**:3.6
-   **Bazel version (if compiling from source)**:-
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:-
-   **GPU model and memory**:-
-   **Exact command to reproduce**:-

### Describe the problem
Hi all,

I created a customer trained tflite model for my Tensorflow object detection project(which will be running on ios),
but somehow inferecing(object detection) does not work properly.
During inferecing, the model detects always the first trained item(the first item which is in my labelmap.txt file) and
gives some wrong scores as object detection prediction.

Does anyone an idea what the problem could be?

Here is step by step my project flow:

1- I trained my images with ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync model:
1.1 Code:
https://github.com/tensorflow/models
(master branch)
1.2 Command:
python3.7 train.py
--logtostderr
--train_dir=/Users/Documents/Temp/tensorflow_last/models/train
--pipeline_config_path=/Users/Documents/Temp/tensorflow_last/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
Note:I can also attach my config file, if it is related
1.3 Output:
...
INFO:tensorflow:global step 89547: loss = 0.0628 (8.283 sec/step)
I0629 19:34:56.030186 4505345536 learning.py:512] global step 89547: loss = 0.0128 (8.283 sec/step)
INFO:tensorflow:global step 89548: loss = 0.0596 (9.801 sec/step)
I0629 19:35:05.831597 4505345536 learning.py:512] global step 89548: loss = 0.0196 (9.801 sec/step)
...

2-Converted trained model to tflite_graph.pb file:
2.1Code:
https://github.com/tensorflow/models
(master branch)
2.2Command:
python3.6 models/research/object_detection/object_detection/export_tflite_ssd_graph.py
--pipeline_config_path=/Users/emre/Documents/Temp/tensorflow_last/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config
--trained_checkpoint_prefix=/Users/emre/Documents/Temp/tensorflow_last/models/train/model.ckpt-90728
--output_directory=/Users/emre/Documents/Temp/tensorflow_last/models/Lite
2.3Output:
/usr/local/bin/python3.6 /Users/emre/Documents/Temp/tensorflow_last/models/research/object_detection/object_detection/export_tflite_ssd_graph.py --pipeline_config_path=/Users/emre/Documents/Temp/tensorflow_last/models/ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync.config --trained_checkpoint_prefix=/Users/emre/Documents/Temp/tensorflow_last/models/train/model.ckpt-89545 --output_directory=/Users/emre/Documents/Temp/tensorflow_last/models/Lite
['/Users/emre/Documents/Temp/tensorflow_last/models/research/object_detection/object_detection', '/Users/emre/Documents/Temp/tensorflow_last/models/research/object_detection', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python36.zip', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/lib-dynload', '/Users/emre/Library/Python/3.6/lib/python/site-packages', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/aeosa', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/object_detection-0.1-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/lvis-0.5.3-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/apache_beam-2.27.0-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/avro_python3-1.10.1-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_model_optimization-0.5.0-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_datasets-4.2.0-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow_addons-0.12.1-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/seqeval-1.2.2-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/sentencepiece-0.1.95-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/py_cpuinfo-7.0.0-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/psutil-5.8.0-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/opencv_python_headless-4.5.1.48-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/oauth2client-4.1.3-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/kaggle-1.5.10-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/google_cloud_bigquery-2.7.0-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/gin_config-0.4.0-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/dataclasses-0.8-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/opencv_python-4.5.1.48-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pymongo-3.11.3-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pydot-1.4.1-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pyarrow-2.0.0-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/mock-2.0.0-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/httplib2-0.17.4-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/hdfs-2.5.8-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/fastavro-1.3.1-py3.6-macosx-10.9-x86_64.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/dill-0.3.1.1-py3.6.egg', '/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/crcmod-1.7-py3.6-macosx-10.9-x86_64.egg']
2021-06-29 19:23:16.734088: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
INFO:tensorflow:Found and fixed 2 matches
I0629 19:23:17.298635 4339817984 exporter.py:140] Found and fixed 2 matches
INFO:tensorflow:Found and fixed 0 matches
I0629 19:23:17.328562 4339817984 exporter.py:140] Found and fixed 0 matches
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
W0629 19:23:17.652231 4339817984 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
INFO:tensorflow:Restoring parameters from /Users/emre/Documents/Temp/tensorflow_last/models/train/model.ckpt-89545
I0629 19:23:18.414107 4339817984 saver.py:1280] Restoring parameters from /Users/emre/Documents/Temp/tensorflow_last/models/train/model.ckpt-89545
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
W0629 19:23:19.047316 4339817984 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.convert_variables_to_constants`
WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
W0629 19:23:19.047602 4339817984 deprecation.py:323] From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.compat.v1.graph_util.extract_sub_graph`
INFO:tensorflow:Froze 333 variables.
I0629 19:23:19.428654 4339817984 graph_util_impl.py:311] Froze 333 variables.
INFO:tensorflow:Converted 333 variables to const ops.
I0629 19:23:19.579565 4339817984 graph_util_impl.py:364] Converted 333 variables to const ops.
2021-06-29 19:23:19.800005: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes

3-Converted tflite_graph.pb to tflite file:
3.1Code:
https://github.com/tensorflow/models
(master branch)
3.2Command:
python3.6 tflite_convert.py --output_file=test.tflite --graph_def_file=tflite_graph.pb --input_arrays=normalized_input_image_tensor --output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3'  --input_shape=1,640,640,3 --allow_custom_ops
3.3Output:
2021-06-29 19:15:19.235482: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
hallo
2021-06-29 19:15:19.931976: I tensorflow/lite/toco/import_tensorflow.cc:1336] Converting unsupported operation: TFLite_Detection_PostProcess
2021-06-29 19:15:19.976190: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 900 operators, 1329 arrays (0 quantized)
2021-06-29 19:15:20.001280: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 900 operators, 1329 arrays (0 quantized)
2021-06-29 19:15:20.111332: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 187 operators, 417 arrays (0 quantized)
2021-06-29 19:15:20.116690: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 182 operators, 407 arrays (0 quantized)
2021-06-29 19:15:20.121787: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before Group bidirectional sequence lstm/rnn: 182 operators, 407 arrays (0 quantized)
2021-06-29 19:15:20.124878: I tensorflow/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 182 operators, 407 arrays (0 quantized)
2021-06-29 19:15:20.132192: I tensorflow/lite/toco/allocate_transient_arrays.cc:345] Total transient array allocated size: 52428800 bytes, theoretical optimal value: 39321600 bytes.
2021-06-29 19:15:20.133140: I tensorflow/lite/toco/toco_tooling.cc:433] Estimated count of arithmetic ops: 103.489 billion (note that a multiply-add is counted as 2 ops).
2021-06-29 19:15:20.134138: W tensorflow/lite/toco/tflite/operator.cc:2112] Ignoring unsupported type in list attribute with key '_output_types'

4-I downloaded following projects for tflite object detection and camera capture:
Code:
https://github.com/teticio/kivy-tensorflow-helloworld
and
https://kivy.org/doc/stable/examples/gen__camera__main__py.html

5-I modified and merged them a little bit, so if I click the capture button, captures the camera view, it detects objects (which are trained in my model) from captured photo and prints out
the item number(class_id in labelmap.txt) and score of the detected object predication.

6-If I execute my custom trained model(on Macos), my code prints out always the first class_id and some wrong scores, it does not matter which photos I show:
6.1Ex:(first photo)
2021-06-29 19:19:14.526 Python[14486:591568] Selected FPS (30) not available on this platform.
{'bounding_box': array([0.77786434, 0.4134923 , 0.8920009 , 0.5161122 ], dtype=float32), 'class_id': 0.0, 'score': 0.84353364}
6.2Ex:(second photo)
2021-06-29 19:19:20.729 Python[14486:591568] Selected FPS (30) not available on this platform.
{'bounding_box': array([0.7591856 , 0.41594303, 0.8673101 , 0.5112757 ], dtype=float32), 'class_id': 0.0, 'score': 0.9267361}
{'bounding_box': array([0.42481518, 0.6758945 , 0.53535295, 0.7300209 ], dtype=float32), 'class_id': 0.0, 'score': 0.50106883}
{'bounding_box': array([0.6508195 , 0.6713367 , 0.7614183 , 0.72627753], dtype=float32), 'class_id': 0.0, 'score': 0.4232775}
6.3Ex:(third photo)
2021-06-29 19:19:27.549 Python[14486:591568] Selected FPS (30) not available on this platform.
{'bounding_box': array([0.6415653 , 0.2917411 , 0.7683315 , 0.38982102], dtype=float32), 'class_id': 0.0, 'score': 0.9865758}

If I use another tflite pre-trained model with my code(racoon and squirrel detection SD-MobileNet-V2-Quantized-COCO model
link:https://github.com/EdjeElectronics/TensorFlow-Lite-Object-Detection-on-Android-and-Raspberry-Pi)
it works totally fine. I get following output if I show a squirrel or racoon photo to camera and capture.
6.4 Ex: racoon
2021-07-01 21:27:53.884 Python[21077:785260] Selected FPS (30) not available on this platform.
{'bounding_box': array([0.10106477, 0.5392377 , 0.5041685 , 0.9078557 ], dtype=float32), 'class_id': 2.0, 'score': 0.9921875}
6.5 Ex: squirrel
2021-07-01 21:28:15.506 Python[21077:785260] Selected FPS (30) not available on this platform.
{'bounding_box': array([0.28669962, 0.52812684, 0.83007085, 0.8363091 ], dtype=float32), 'class_id': 1.0, 'score': 0.95703125}

Additional Note: If I use pretrained frozen “ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync” model
Link:https://docs.openvinotoolkit.org/latest/omz_models_model_ssd_mobilenet_v1_fpn_coco.html

With the code from following website:
Link:https://github.com/EdjeElectronics/TensorFlow-Object-Detection-API-Tutorial-Train-Multiple-Objects-Windows-10
It works also totally fine. So I do not think that ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync model has a basic problem.
So I guess something wrong is happening in Step 2(converting .pb file to tflite_graph.pb) and/or step3
(converting tflite_graph.pb file to tflite)

I would be grateful, if you give me some advices.

Thanks a lot in advance.

### Source code / logs
-
"
50567,Fit failed in TPU when model contains conv_transpose,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): On Kaggle's default kernel. 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): 
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.10
- Bazel version (if compiling from source): 
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:   
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When my model includes `tf.nn.conv_transpose` and compiles on the TPU, it failed. The simplest code for the error is as follows:
```
from tensorflow import keras
from tensorflow.keras import losses,layers,optimizers,Model
import tensorflow as tf
import numpy as np 

class Test(layers.Layer):
    def __init__(self):
        super().__init__() 
        self.f = self.add_weight(name = 'kernel', 
                                 trainable = True,
                                 shape = [12,12,3,3], 
                                 initializer = tf.random_uniform_initializer()) 
    def call(self, inp):  
        print(inp.shape)
        _,H,W,C = inp.shape
        y = tf.nn.conv_transpose(inp, self.f, [-1,H*4, W*4,C], (4,4), 'SAME') 
        return y 

tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
strategy = tf.distribute.experimental.TPUStrategy(tpu) 
 
with strategy.scope():
    model = keras.Sequential()
    model.add(Test())
    model.build([1,48, 48, 3])
    model.compile(optimizers.Adam(), losses.mean_absolute_error)

lr_data = np.zeros([256, 48, 48, 3]).astype(np.float32)
hr_data = np.zeros([256, 48*4, 48*4, 3]).astype(np.float32)
print('Fit Begin')
model.fit(lr_data, hr_data, epochs=5, batch_size=32, verbose=1)  
print('Fit End')
```
It outpus:
```
(1, 48, 48, 3)
Fit Begin
Epoch 1/5
(None, 48, 48, 3)
(4, 48, 48, 3)
(4, 48, 48, 3)
---------------------------------------------------------------------------
UnavailableError                          Traceback (most recent call last)
<ipython-input-1-3fe6c59dcd42> in <module>
     31 hr_data = np.zeros([256, 48*4, 48*4, 3]).astype(np.float32)
     32 print('Fit Begin')
---> 33 model.fit(lr_data, hr_data, epochs=5, batch_size=32, verbose=1)
     34 print('Fit End')

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1103               logs = tmp_logs  # No error, now safe to assign to logs.
   1104               end_step = step + data_handler.step_increment
-> 1105               callbacks.on_train_batch_end(end_step, logs)
   1106               if self.stop_training:
   1107                 break

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
    452     """"""
    453     if self._should_call_train_batch_hooks:
--> 454       self._call_batch_hook(ModeKeys.TRAIN, 'end', batch, logs=logs)
    455 
    456   def on_test_batch_begin(self, batch, logs=None):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook(self, mode, hook, batch, logs)
    294       self._call_batch_begin_hook(mode, batch, logs)
    295     elif hook == 'end':
--> 296       self._call_batch_end_hook(mode, batch, logs)
    297     else:
    298       raise ValueError('Unrecognized hook: {}'.format(hook))

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_end_hook(self, mode, batch, logs)
    314       self._batch_times.append(batch_time)
    315 
--> 316     self._call_batch_hook_helper(hook_name, batch, logs)
    317 
    318     if len(self._batch_times) >= self._num_batches_for_timing_check:

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _call_batch_hook_helper(self, hook_name, batch, logs)
    354       hook = getattr(callback, hook_name)
    355       if getattr(callback, '_supports_tf_logs', False):
--> 356         hook(batch, logs)
    357       else:
    358         if numpy_logs is None:  # Only convert once.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in on_train_batch_end(self, batch, logs)
   1018 
   1019   def on_train_batch_end(self, batch, logs=None):
-> 1020     self._batch_update_progbar(batch, logs)
   1021 
   1022   def on_test_batch_end(self, batch, logs=None):

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py in _batch_update_progbar(self, batch, logs)
   1082     if self.verbose == 1:
   1083       # Only block async when verbose = 1.
-> 1084       logs = tf_utils.to_numpy_or_python_type(logs)
   1085       self.progbar.update(self.seen, list(logs.items()), finalize=False)
   1086 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in to_numpy_or_python_type(tensors)
    512     return t  # Don't turn ragged or sparse tensors to NumPy.
    513 
--> 514   return nest.map_structure(_to_single_numpy_or_python_type, tensors)
    515 
    516 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    657 
    658   return pack_sequence_as(
--> 659       structure[0], [func(*x) for x in entries],
    660       expand_composites=expand_composites)
    661 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    657 
    658   return pack_sequence_as(
--> 659       structure[0], [func(*x) for x in entries],
    660       expand_composites=expand_composites)
    661 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/utils/tf_utils.py in _to_single_numpy_or_python_type(t)
    508   def _to_single_numpy_or_python_type(t):
    509     if isinstance(t, ops.Tensor):
--> 510       x = t.numpy()
    511       return x.item() if np.ndim(x) == 0 else x
    512     return t  # Don't turn ragged or sparse tensors to NumPy.

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in numpy(self)
   1069     """"""
   1070     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1071     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1072     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1073 

/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1037       return self._numpy_internal()
   1038     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1039       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1040 
   1041   @property

/opt/conda/lib/python3.7/site-packages/six.py in raise_from(value, from_value)

UnavailableError: Socket closed
```
**Describe the expected behavior**
I don't know what caused this error. I tried to change ` tf.nn.conv_ Transpose 'transpose' to other up-sampling methods, and there is no error. But I do need to use transposed convolution to build my model. If it can be solved, I will be very grateful.
**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50562,Tensorflow 2.5/2.4 ptxas and gpu issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow version: 2.4.0/2.4.2/2.5.0 gpu
- Python version: 3.8.0/3.8.10
- Installed using virtualenv? pip? conda?:  pip
- CUDA/cuDNN version: cuda 11.0/11.1/11.2.1/11.2.2, cudnn8.1.1



when running CNN, log shows:

> 2021-07-01 22:39:34.767676: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: 
> Relying on driver to perform ptx compilation. 
> Modify $PATH to customize ptxas location.
> This message will be only logged once.

the log also shows that it is **potentially using CPU and oneDNN**, even though gpu is available (and tf obviously can find it according to log)
the same message appears for combinations of tf2.4.0/2.4.2/2.5.0 with cuda11.0/11.1/11.2.1/11.2.2 (I tried several of these combinations, and the message always exist)
current combination on my machine: tf2.5.0+cuda11.2.1

**also tried: adding ptxas.exe path to system PATH**
doesn't help

similar to issue https://github.com/tensorflow/tensorflow/issues/49824 , but it seems that post is caused by signal handling, which I did not use

code:
```
import tensorflow as tf
import numpy as np

data = np.random.random((12800, 120, 120, 3,))
label = np.array([1 for _ in range(12800)])
print(data)

model = tf.keras.Sequential(
    [tf.keras.layers.Conv2D(256, kernel_size=5, strides=2, input_shape=(120, 120, 3,), activation=""relu""),
     tf.keras.layers.Conv2D(256, kernel_size=5, strides=2, input_shape=(120, 120, 3,), activation=""relu""),
     tf.keras.layers.GlobalAvgPool2D(),
     tf.keras.layers.Dense(1, activation=""sigmoid"")]
)
model.compile(loss=tf.keras.losses.BinaryCrossentropy())
model.fit(x=data, y=label, batch_size=32, epochs=20)
```
**note: this is just for demonstration, the same logging message appears whenever I use Conv2d in model**

full log:

> 2021-07-01 22:39:31.597866: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
> 2021-07-01 22:39:31.623038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
> coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s
> 2021-07-01 22:39:31.623217: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
> 2021-07-01 22:39:31.629267: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
> 2021-07-01 22:39:31.629359: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
> 2021-07-01 22:39:31.632662: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
> 2021-07-01 22:39:31.633830: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
> 2021-07-01 22:39:31.641547: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
> 2021-07-01 22:39:31.644421: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
> 2021-07-01 22:39:31.645145: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
> 2021-07-01 22:39:31.645317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
> 2021-07-01 22:39:31.645646: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
> To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
> 2021-07-01 22:39:31.646131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
> pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1660 Ti computeCapability: 7.5
> coreClock: 1.59GHz coreCount: 24 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 268.26GiB/s
> 2021-07-01 22:39:31.646377: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
> 2021-07-01 22:39:32.111044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
> 2021-07-01 22:39:32.111146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
> 2021-07-01 22:39:32.111202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
> 2021-07-01 22:39:32.111410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 3983 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5)
> 2021-07-01 22:39:33.207902: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
> Epoch 1/20
> 2021-07-01 22:39:33.624659: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
> 2021-07-01 22:39:34.113561: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101
> 2021-07-01 22:39:34.767676: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output: 
> Relying on driver to perform ptx compilation. 
> Modify $PATH to customize ptxas location.
> This message will be only logged once.
> 2021-07-01 22:39:34.848389: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
> 2021-07-01 22:39:35.307315: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
> 400/400 [==============================] - 27s 60ms/step - loss: 0.0016
> Epoch 2/20
> 400/400 [==============================] - 24s 60ms/step - loss: 2.3526e-08
> Epoch 3/20
> 400/400 [==============================] - 24s 60ms/step - loss: 2.3526e-08...

"
50559,Fail to use custom trained model on developing mobile app,"## URL(s) with the issue:

https://github.com/tensorflow/examples/tree/master/lite/examples/object_detection/android

Have been downloaded the official example of object detection mobile app for android and have been run successfully. But when I replaced it with my custom trained model using SSD MobileNet V2 FPNLite 320x320, the app does not manage to start. I've been following the guide to change the .tflite and labelmap.txt and even set the quantized to false."
50558,XLA Operation Semantics documentation has self-conflict,"

## URL(s) with the issue:

https://www.tensorflow.org/xla/operation_semantics#slice

## Description of issue (what needs changing):

### Clear description
the number of parameters are not match.
![image](https://user-images.githubusercontent.com/22614078/124128517-74e18200-daaf-11eb-9a91-5007d4dee8b0.png)
"
50557,Trying to install tensorflow==2.0.3 with bazel on google app engine wont work ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Ubuntu Google App Engine

- TensorFlow installed from (source or binary):
- TensorFlow version: 2.0.3
- Python version: 3x
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source): latest from google app engine deploy bazel
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.0/7.4
- GPU model and memory: google app engine



**Describe the problem**

I have followed the exact commands from: https://www.tensorflow.org/install/source

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50556,kotlin.UninitializedPropertyAccessException: lateinit property styleTransferModelExecutor,"
System information

Android 9 
Mobile device :One plus 8
when launcher google style transfer demo,it show Force close issue

07-01 16:32:53.894  5715  5715 E AndroidRuntime: FATAL EXCEPTION: main
07-01 16:32:53.894  5715  5715 E AndroidRuntime: Process: org.tensorflow.lite.examples.styletransfersample, PID: 5715
07-01 16:32:53.894  5715  5715 E AndroidRuntime: kotlin.UninitializedPropertyAccessException: lateinit property styleTransferModelExecutor has not been initialized
07-01 16:32:53.894  5715  5715 E AndroidRuntime:        at org.tensorflow.lite.examples.styletransfer.MainActivity.startRunningModel(MainActivity.kt:320)"
50555,mbed compile -m DISCO_F746NG -t GCC_ARM error: mbed error: /usr/bin/python3 returned error,"**System information**
- OS Platform and Distribution:Linux ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: STM32F746G
- TensorFlow installed from (source or binary): 
- TensorFlow version: 2.3.0
- Python version:3.8
- Installed using virtualenv? pip? conda?:pip
- GCC/Compiler version (if compiling from source):



**Describe the problem**
When I followed this project https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world#deploy-to-STM32F746
After I finished the whole process, it was successful, but when I run this command(in readme.txt) to use C++11 
‘’’’’’’
python3 -c ‘import fileinput, glob;
for filename in glob.glob(“mbed-os/tools/profiles/*.json”):
for line in fileinput.input(filename, inplace=True):
print(line.replace(""""-std=gnu++98"""",""""-std=c++11"", “-fpermissive”""))’

‘’’’’’


**Any other info / logs**
The error shows below:

    [mbed] Working path “~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed” (library)
    [mbed] Program path “~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed”
    WARNING: MBED_ARM_PATH set as environment variable but doesn’t exist
    Traceback (most recent call last):
    File “~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py”, line 421, in
    main()
    File “~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py”, line 354, in main
    build_profile=extract_profile(parser, options, internal_tc_name),
    File “~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/options.py”, line 128, in extract_profile
    contents = load(open(filename))
    File “/usr/lib/python3.8/json/init.py”, line 293, in load
    return loads(fp.read(),
    File “/usr/lib/python3.8/json/init.py”, line 357, in loads
    return _default_decoder.decode(s)
    File “/usr/lib/python3.8/json/decoder.py”, line 337, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
    File “/usr/lib/python3.8/json/decoder.py”, line 355, in raw_decode
    raise JSONDecodeError(“Expecting value”, s, err.value) from None
    json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
    [mbed] ERROR: “/usr/bin/python3” returned error.
    Code: 1
    Path: “~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed”
    Command: “/usr/bin/python3 -u ~/tensorflow/tensorflow/lite/micro/tools/make/gen/disco_f746ng_cortex-m4_default/prj/hello_world/mbed/mbed-os/tools/make.py -t GCC_ARM -m DISCO_F746NG --source . --build ./BUILD/DISCO_F746NG/GCC_ARM”
    Tip: You could retry the last command with “-v” flag for verbose output"
50554,map_fn can't process sparse tensor in graph mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.5
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0

**Describe the current behavior**

the following customized keras layer can't be executed in graph mode.

```python
#!/usr/bin/python3

import numpy as np;
import tensorflow as tf;

# please ignore this function, it is just for generating a sparse tensor
def Dense2Sparse():
  dense = tf.keras.Input((None, None, None)); # dense.shape = (batch, num_heads, query_length, key_length)
  mask = tf.keras.Input((1, None, None)); # mask.shape = (batch, 1, query_length or 1, key_length)
  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.cond(tf.math.not_equal(tf.shape(x[0])[2], tf.shape(x[1])[2]), lambda: tf.tile(x[0], [1,1,tf.shape(x[1])[2],1,]), lambda: x[0]))([mask, dense]); # mask.shape = (batch, 1, query_length, key_length)
  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.tile(x[0],[1,tf.shape(x[1])[1],1,1]))([reshaped_mask, dense]); # mask.shape = (batch, num_heads, query_length, key_length)
  indices = tf.keras.layers.Lambda(lambda x: tf.where(tf.cast(x, dtype = tf.int32)))(reshaped_mask); # indices.shape = (num non zero values, 4)
  values = tf.keras.layers.Lambda(lambda x: tf.gather_nd(x[0], x[1]))([dense, indices]); # values.shape = (num non zero values)
  sparse = tf.keras.layers.Lambda(lambda x: tf.sparse.SparseTensor(x[0], values = x[1], dense_shape = tf.cast(tf.shape(x[2]), dtype = tf.int64)))([indices, values, dense]);
  return tf.keras.Model(inputs = (dense, mask), outputs = sparse);

# NOTE: ***this layer cannot be executed in graph mode***
class SparseDenseMatMul(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super(SparseDenseMatMul, self).__init__(**kwargs);
  def call(self, inputs):
    a = inputs[0]; # a.shape = (batch, heads, query_length, key_length)
    b = inputs[1]; # b.shape = (batch, heads, key_length, value_dim)
    reshaped_a = tf.sparse.reshape(a, (-1, tf.shape(a)[-2], tf.shape(a)[-1])); # reshaped_a.shape = (batch * heads, query_length, key_length)
    reshaped_b = tf.reshape(b, (-1, tf.shape(b)[-2], tf.shape(b)[-1])); # reshaped_b.shape = (batch * heads, key_length, value_dim)
    def dot(x):
      a = x[0];
      b = x[1];
      c = tf.sparse.sparse_dense_matmul(a,b);
      return c; # c.shape = (query_length, value_dim)
    results = tf.map_fn(dot, (reshaped_a, reshaped_b), fn_output_signature = tf.TensorSpec((tf.shape(reshaped_a)[-2], tf.shape(reshaped_b)[-1]), dtype = tf.float32));
    results = tf.reshape(results, (tf.shape(a)[0], tf.shape(a)[1], tf.shape(results)[-2], tf.shape(results)[-1]));
    return results;

a = np.random.normal(size = (4,3,10,40));
mask = np.random.randint(low = 0, high = 2, size = (4,1,10,40));
a = Dense2Sparse()([a, mask]);
b = np.random.normal(size = (4,3,40,20)).astype(np.float32);

print(SparseDenseMatMul()([a,b])); # eager mode is OK

inputs_a = tf.keras.Input((None, None, None), sparse = True);
inputs_b = tf.keras.Input((None, None, None));
results_c = SparseDenseMatMul()([inputs_a,inputs_b]);
model = tf.keras.Model(inputs = (inputs_a, inputs_b), outputs = results_c);
print(model([a,b])); # graph mode is failed
```

**Describe the expected behavior**

map_fn should process sparse tensor in both eager and graph mode.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no 
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
#!/usr/bin/python3

import numpy as np;
import tensorflow as tf;

# please ignore this function, it is just for generating a sparse tensor
def Dense2Sparse():
  dense = tf.keras.Input((None, None, None)); # dense.shape = (batch, num_heads, query_length, key_length)
  mask = tf.keras.Input((1, None, None)); # mask.shape = (batch, 1, query_length or 1, key_length)
  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.cond(tf.math.not_equal(tf.shape(x[0])[2], tf.shape(x[1])[2]), lambda: tf.tile(x[0], [1,1,tf.shape(x[1])[2],1,]), lambda: x[0]))([mask, dense]); # mask.shape = (batch, 1, query_length, key_length)
  reshaped_mask = tf.keras.layers.Lambda(lambda x: tf.tile(x[0],[1,tf.shape(x[1])[1],1,1]))([reshaped_mask, dense]); # mask.shape = (batch, num_heads, query_length, key_length)
  indices = tf.keras.layers.Lambda(lambda x: tf.where(tf.cast(x, dtype = tf.int32)))(reshaped_mask); # indices.shape = (num non zero values, 4)
  values = tf.keras.layers.Lambda(lambda x: tf.gather_nd(x[0], x[1]))([dense, indices]); # values.shape = (num non zero values)
  sparse = tf.keras.layers.Lambda(lambda x: tf.sparse.SparseTensor(x[0], values = x[1], dense_shape = tf.cast(tf.shape(x[2]), dtype = tf.int64)))([indices, values, dense]);
  return tf.keras.Model(inputs = (dense, mask), outputs = sparse);

# NOTE: ***this layer cannot be executed in graph mode***
class SparseDenseMatMul(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super(SparseDenseMatMul, self).__init__(**kwargs);
  def call(self, inputs):
    a = inputs[0]; # a.shape = (batch, heads, query_length, key_length)
    b = inputs[1]; # b.shape = (batch, heads, key_length, value_dim)
    reshaped_a = tf.sparse.reshape(a, (-1, tf.shape(a)[-2], tf.shape(a)[-1])); # reshaped_a.shape = (batch * heads, query_length, key_length)
    reshaped_b = tf.reshape(b, (-1, tf.shape(b)[-2], tf.shape(b)[-1])); # reshaped_b.shape = (batch * heads, key_length, value_dim)
    def dot(x):
      a = x[0];
      b = x[1];
      c = tf.sparse.sparse_dense_matmul(a,b);
      return c; # c.shape = (query_length, value_dim)
    results = tf.map_fn(dot, (reshaped_a, reshaped_b), fn_output_signature = tf.TensorSpec((tf.shape(reshaped_a)[-2], tf.shape(reshaped_b)[-1]), dtype = tf.float32));
    results = tf.reshape(results, (tf.shape(a)[0], tf.shape(a)[1], tf.shape(results)[-2], tf.shape(results)[-1]));
    return results;

a = np.random.normal(size = (4,3,10,40));
mask = np.random.randint(low = 0, high = 2, size = (4,1,10,40));
a = Dense2Sparse()([a, mask]);
b = np.random.normal(size = (4,3,40,20)).astype(np.float32);

print(SparseDenseMatMul()([a,b])); # eager mode is OK

inputs_a = tf.keras.Input((None, None, None), sparse = True);
inputs_b = tf.keras.Input((None, None, None));
results_c = SparseDenseMatMul()([inputs_a,inputs_b]);
model = tf.keras.Model(inputs = (inputs_a, inputs_b), outputs = results_c);
print(model([a,b])); # graph mode is failed
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
Traceback (most recent call last):
  File ""test.py"", line 44, in <module>
    results_c = SparseDenseMatMul()([inputs_a,inputs_b]);
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 969, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1107, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 840, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 880, in _infer_output_signature
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py"", line 695, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    test.py:32 call  *
        results = tf.map_fn(dot, (reshaped_a, reshaped_b), fn_output_signature = tf.TensorSpec((tf.shape(reshaped_a)[-2], tf.shape(reshaped_b)[-1]), dtype = tf.float32));
    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_spec.py:51 __init__  **
        self._shape = tensor_shape.TensorShape(shape)
    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:765 __init__
        self._dims = [Dimension(d) for d in dims]
    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:765 <listcomp>
        self._dims = [Dimension(d) for d in dims]
    /home/xieyi/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:206 __init__
        six.raise_from(
    <string>:3 raise_from
        

    TypeError: Dimension value must be integer or None or have an __index__ method, got value '<tf.Tensor 'sparse_dense_mat_mul_1/strided_slice_4:0' shape=() dtype=int32>' with type '<class 'tensorflow.python.framework.ops.Tensor'>'

```"
50551,Using `tf.cond` to change behaviour for training/inference with dropout causes crash when using XLA,"**System information**
_Some irrelevant fields have been deleted_
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.4 LTS (Bionic Beaver)
- TensorFlow installed from (source or binary): Binary - installed using `pip`
- TensorFlow version (use command below): Tested with both 2.5.0 (`v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0`) and 1.15.5 (`v1.15.4-39-g3db52be 1.15.5`)
- Python version: Python 3.6.9
- GPU model and memory: N/A - Problem occurs just on CPU (originally identified using Graphcore IPU's)

**Describe the current behavior**

The code fails with the following error:

```
Traceback (most recent call last):
  File ""/localdata/callumm/Z2876/upstream_tf_venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call
    return fn(*args)
  File ""/localdata/callumm/Z2876/upstream_tf_venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1348, in _run_fn
    self._extend_graph()
  File ""/localdata/callumm/Z2876/upstream_tf_venv/lib/python3.6/site-packages/tensorflow_core/python/client/session.py"", line 1388, in _extend_graph
    tf_session.ExtendSession(self._session)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Merge nodes {cond_1/gradients/cond/Identity/Switch_grad/cond_grad,cond_1/gradients/cond/dropout/mul/Switch_grad/cond_grad} directly dominated by switch nodes with different predicates (cond_1/gradients/cond/Merge_grad/cond_grad/Switch:1 vs is_training_0_arg:0).
```
Interestingly, the code fails at the call to `sess.run(tf.global_variables_initializer())` - if you remove all code after that line it should still fail.

**Describe the expected behavior**

The model - if you can even call it that - should run without issue. It runs fine on the CPU without XLA. To see this, uncomment the line `cpu_result = my_net(placeholder, is_training)`, comment out the next two lines and change `xla_result` to `cpu_result` in the `sess.run()` calls.

**Do you want to contribute a PR? (yes/no):** No, I do not yet understand the issue enough

**Standalone code to reproduce the issue**

The code here isn't for any particular neural network, it's just a minimal reproducer. The original model is much bigger, but gives rise to the same issue.

```python
import numpy as np
import tensorflow.compat.v1 as tf
from tensorflow.python.ops import math_ops
from tensorflow.python.training import gradient_descent

tf.disable_v2_behavior()

# Config
BATCH_SIZE = 1
MAX_LENGTH = 16
VOCAB_SIZE = 1000
DROPOUT_RATE = 0.9

# Generate random data
train_data = np.random.randint(0, VOCAB_SIZE, size=(BATCH_SIZE, MAX_LENGTH), dtype=np.int32)

# Placeholders
with tf.device(""cpu""):
    placeholder = tf.placeholder(dtype=tf.int32, shape=[BATCH_SIZE, MAX_LENGTH], name='placeholder')
    is_training = tf.placeholder_with_default(input=tf.constant(True), shape=(), name=""is_training"")

# Define network
def my_net(x, training):
    embedding_table = tf.get_variable('embedding_table',
                                      [VOCAB_SIZE, 2],
                                      dtype=tf.float32)
    input_embedding = tf.nn.embedding_lookup(embedding_table, x)

    def dropped_inputs():
        #return input_embedding * 3
        return tf.nn.dropout(input_embedding, rate=DROPOUT_RATE)
    def identity():
        return tf.identity(input_embedding)

    input_embedding = tf.cond(training, dropped_inputs, identity)
    loss = math_ops.reduce_sum(math_ops.square(input_embedding))
    optimizer = gradient_descent.GradientDescentOptimizer(0.0005)

    def minimize():
        return optimizer.minimize(loss)
    def no_op():
        return tf.no_op()
    train = tf.cond(training, minimize, no_op)

    return loss, train

# Compile and run

# Uncomment to see the code working
#cpu_result = my_net(placeholder, is_training)

# Comment out these two lines to see code working
with tf.device('/device:XLA_CPU:0'):
    xla_result = tf.xla.experimental.compile(my_net, inputs=[placeholder, is_training])


with tf.Session() as sess:

    # Uncomment to write graph Protobuf to file
    # tf.io.write_graph(sess.graph, 'xla_tf_cond_issue', 'xla_tf_cond_issue.pb', as_text=False)    

    sess.run(tf.global_variables_initializer())

    print('Training...')
    for _ in range(5):
        # Change `xla_result` to `cpu_result` to see code working
        result = sess.run(xla_result, feed_dict={placeholder: train_data, is_training: True})
        print(result)

    print('Testing...')
    for _ in range(5):
        # Change `xla_result` to `cpu_result` to see code working
        result = sess.run(xla_result, feed_dict={placeholder: train_data, is_training: False})
        print(result)
```

**Other info / logs**

I have traced the issue to `tensorflow/compiler/tf2xla/functionalize_cond.cc`. Looking at the TensorFlow graph, the complaint raised in the error message is legitimate: the Merge nodes named are indeed directly downstream of two Switch nodes - `cond_1/gradients/Switch` with predicate `cond_1/gradients/cond/Merge_grad/cond_grad/Switch:1` and `cond_1/gradients/cond/mul_grad/Mul/Switch` with predicate `cond_1/pred_id`. 

I'm guessing that, assuming that such a situation should even be allowed to arise at all, it's a result of an optimisation which takes advantage of the fact that the two predicates are essentially the same. Either way, the code in `tensorflow/compiler/tf2xla/functionalize_cond.cc` seems to assume that this will never happen. I would like to use XLA because using it is necessary to run TensorFlow programs on the Graphcore IPU.

It appears that 

I've included a line in the reproducer so that you can dump and inspect the graph for yourself using a tool such as Netron - simply uncomment the line `# tf.io.write_graph(sess.graph, 'xla_tf_cond_issue', 'xla_tf_cond_issue.pb', as_text=False)`.

Please let me know if there is any more useful information I can provide. I will continue to investigate this issue for myself in the meantime. Thank you!"
50549,`Failed to convert a NumPy array to a Tensor (Unsupported object type dict).`,"`history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps)`

`def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):`
` X1, X2, y = list(), list(), list()`
` for desc in desc_list:`
`seq = tokenizer.texts_to_sequences([desc])[0]`
` for i in range(1, len(seq)):`
`in_seq, out_seq = seq[:i], seq[i] in_seq = pad_sequences([in_seq], maxlen=max_length)[0]`
` out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]`
` X1.append(photo)`
` X2.append(in_seq)`
` y.append(out_seq)`
` return array(X1), array(X2), array(y)`

`def data_generator(descriptions, photos, tokenizer, max_length, imgsIds, vocab_size):`
`while 1:`
` for ind in range(len(imgsIds)): `
`photo = photos[ind]`
` key = imgsIds[ind]`
` desc_list = descriptions[str(key)] `
`in_img, in_seq, out_word = create_sequences( tokenizer, max_length, desc_list, photo, vocab_size)`
` yield [in_img, in_seq], out_word`

i got

`Failed to convert a NumPy array to a Tensor (Unsupported object type dict).`
if there is anything i should add it please comment … Thanks

`Traceback (most recent call last): File ""fit.py"", line 271, in <module> main(sys.argv) File ""fit.py"", line 268, in main fit_model(train, train_descriptions, train_rnn_input, val, val_descriptions, val_rnn_input) File ""fit.py"", line 255, in fit_model history = model.fit_generator(train_generator, epochs=epochs, steps_per_epoch=train_steps, verbose=1, callbacks=[checkpoint], validation_data=val_generator, validation_steps=val_steps) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py"", line 324, in new_func return func(*args, **kwargs) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1479, in fit_generator initial_epoch=initial_epoch) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper return method(self, *args, **kwargs) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 872, in fit return_dict=True) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 66, in _method_wrapper return method(self, *args, **kwargs) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py"", line 1057, in evaluate model=self) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1112, in __init__ model=model) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 775, in __init__ peek = _process_tensorlike(peek) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1013, in _process_tensorlike inputs = nest.map_structure(_convert_numpy_and_scipy, inputs) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 617, in map_structure structure[0], [func(*x) for x in entries], File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/util/nest.py"", line 617, in <listcomp> structure[0], [func(*x) for x in entries], File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/data_adapter.py"", line 1008, in _convert_numpy_and_scipy return ops.convert_to_tensor(x, dtype=dtype) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py"", line 1341, in convert_to_tensor ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/tensor_conversion_registry.py"", line 52, in _default_conversion_function return constant_op.constant(value, dtype, name=name) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 262, in constant allow_broadcast=True) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 270, in _constant_impl t = convert_to_eager_tensor(value, ctx, dtype) File ""/path/.local/lib/python3.6/site-packages/tensorflow/python/framework/constant_op.py"", line 96, in convert_to_eager_tensor return ops.EagerTensor(value, ctx.device_name, dtype) ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type dict). 2021-06-27 04:46:22.936001: W tensorflow/core/kernels/data/generator_dataset_op.cc:103] Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated. [[{{node PyFunc}}]]`

"
50546,graph simplifications examples,"I saw Tatiana Shpeisman's presentation about TF graph optimization at Stanford.

I want to simplify a frozen graph (tf 1.x) from pb.file

Do you have any end-to-end example on how to simplify a TF frozen graph?

For example, using tensorflow.python.grappler.tf_optimizer or  tf.config.optimizer.get_experimental_options?

When I contacted to Tatiana Shpeisman(shpeisman@google.com), she encouraged to post this request here.

Thanks

Taehee
"
50545,Reappeared bug: TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.,"The bug https://github.com/tensorflow/tensorflow/issues/42596 which was fixed in Tensorflow 2.4 seems to have reappeared in Tensorflow 2.5 a different way.
I already commented in that bug, but as it is closed and I'm not able to reopen it and the cause seems to be a bit different I opened this new bug report.
So when running my code on TF 2.4 everything works fine including the ipynb example from https://www.tensorflow.org/tutorials/keras/classification but when I run it with TF 2.5 I get this error:

```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-22-16231aa6b55c> in <module>
----> 1 model = tf.keras.Sequential([
      2     tf.keras.layers.Flatten(input_shape=(28, 28)),
      3     tf.keras.layers.Dense(128, activation='relu'),
      4     tf.keras.layers.Dense(10)
      5 ])

/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    520     self._self_setattr_tracking = False  # pylint: disable=protected-access
    521     try:
--> 522       result = method(self, *args, **kwargs)
    523     finally:
    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py in __init__(self, layers, name)
    112     """"""
    113     # Skip the init in FunctionalModel since model doesn't have input/output yet
--> 114     super(functional.Functional, self).__init__(  # pylint: disable=bad-super-call
    115         name=name, autocast=False)
    116     base_layer.keras_api_gauge.get_cell('Sequential').set(True)

/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    520     self._self_setattr_tracking = False  # pylint: disable=protected-access
    521     try:
--> 522       result = method(self, *args, **kwargs)
    523     finally:
    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in __init__(self, *args, **kwargs)
    316     self._steps_per_execution = None
    317 
--> 318     self._init_batch_counters()
    319     self._base_model_initialized = True
    320 

/usr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py in _method_wrapper(self, *args, **kwargs)
    520     self._self_setattr_tracking = False  # pylint: disable=protected-access
    521     try:
--> 522       result = method(self, *args, **kwargs)
    523     finally:
    524       self._self_setattr_tracking = previous_value  # pylint: disable=protected-access

/usr/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in _init_batch_counters(self)
    324     # `evaluate`, and `predict`.
    325     agg = variables.VariableAggregationV2.ONLY_FIRST_REPLICA
--> 326     self._train_counter = variables.Variable(0, dtype='int64', aggregation=agg)
    327     self._test_counter = variables.Variable(0, dtype='int64', aggregation=agg)
    328     self._predict_counter = variables.Variable(

/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    260       return cls._variable_v1_call(*args, **kwargs)
    261     elif cls is Variable:
--> 262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
    264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)

/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in _variable_v2_call(cls, initial_value, trainable, validate_shape, caching_device, name, variable_def, dtype, import_scope, constraint, synchronization, aggregation, shape)
    242     if aggregation is None:
    243       aggregation = VariableAggregation.NONE
--> 244     return previous_getter(
    245         initial_value=initial_value,
    246         trainable=trainable,

/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in <lambda>(**kws)
    235                         shape=None):
    236     """"""Call on Variable class. Useful to force the signature.""""""
--> 237     previous_getter = lambda **kws: default_variable_creator_v2(None, **kws)
    238     for _, getter in ops.get_default_graph()._variable_creator_stack:  # pylint: disable=protected-access
    239       previous_getter = _make_getter(getter, previous_getter)

/usr/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py in default_variable_creator_v2(next_creator, **kwargs)
   2660   shape = kwargs.get(""shape"", None)
   2661 
-> 2662   return resource_variable_ops.ResourceVariable(
   2663       initial_value=initial_value,
   2664       trainable=trainable,

/usr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py in __call__(cls, *args, **kwargs)
    262       return cls._variable_v2_call(*args, **kwargs)
    263     else:
--> 264       return super(VariableMetaclass, cls).__call__(*args, **kwargs)
    265 
    266 

/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in __init__(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)
   1582       self._init_from_proto(variable_def, import_scope=import_scope)
   1583     else:
-> 1584       self._init_from_args(
   1585           initial_value=initial_value,
   1586           trainable=trainable,

/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _init_from_args(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)
   1736           else:
   1737             shape = initial_value.shape
-> 1738           handle = eager_safe_variable_handle(
   1739               initial_value=initial_value,
   1740               shape=shape,

/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in eager_safe_variable_handle(initial_value, shape, shared_name, name, graph_mode)
    235   """"""
    236   dtype = initial_value.dtype.base_dtype
--> 237   return _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name,
    238                                                graph_mode, initial_value)
    239 

/usr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py in _variable_handle_from_shape_and_dtype(shape, dtype, shared_name, name, graph_mode, initial_value)
    175     handle_data.is_set = True
    176     handle_data.shape_and_type.append(
--> 177         cpp_shape_inference_pb2.CppShapeInferenceResult.HandleShapeAndType(
    178             shape=shape.as_proto(), dtype=dtype.as_datatype_enum))
    179 

TypeError: Parameter to MergeFrom() must be instance of same class: expected tensorflow.TensorShapeProto got tensorflow.TensorShapeProto.
```"
50542,tf.data Service Dispatcher/ Worker behaviour,"## Tensorflow Version
TF v2.5.0


## URL(s) with the issue:

1. https://github.com/tensorflow/community/blob/master/rfcs/20200113-tf-data-service.md
2. https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/distribute
3. https://www.tensorflow.org/api_docs/python/tf/data/experimental/service/from_dataset_id 

## Description

Hello, I'm trying to use tf.data.experimental.service to offload pre-processing into Compute Servers alongside my training jobs. I am also using [Horovod](https://github.com/horovod/horovod) as my distributed ML framework, so I will be referring to `worker_index` as `rank`.

I'm a little confused with the documentation. I understand that this was originally a contribution from the community so document (1) might not be up-to-date to the implementation supported by Tensorflow.

My understanding is as follows:
1. Datasets are registered into a DispatcherServer
2. A job is created by a training process to consume the dataset
3. The dataset is distributed across the Workers based on the distribution strategy


My questions follow:

1. Are jobs autonomously run on the Workers i.e. batches are autonomously computed, and stored by the Worker in-memory. Or are they only computed per request from dataset.apply(distribute()), which is called per training step/iteration?

2. When using `register_dataset()` and `from_dataset_id()` instead of `distribute()`, from my understanding we can use the dataset across multiple training processes from the same training job. In this case, one process would be in charge of registering the dataset and the others could simply retrieve elements using from_dataset_id(). In this way each process receives a partition of the dataset through one full sweep. Why is the parameter `processing_mode` in from_dataset_id() instead of register_dataset() in this case? Does this mean that each training process will process the dataset independently, and hence receive all of the dataset?

3. What is the difference between the `job_name` that links datasets to actual jobs in tf.data service and using `register_dataset()` and `from_dataset_id()`. Implementation wise, what's the difference between these two options in pseudocode:

### Option 1
``` python
# Input pipeline of ALL training processes
dataset = dataset.map(foo)
                 .batch(mini_batch_size)
                 .apply(tf.data.experimental.service.distribute(
                        service=dispatcher_address,
                        job_name=""job1""
                  ))
                 .prefetch()
model.fit(dataset)
```
### Option 2
``` python
# Input pipeline of training process with index 0
dataset = dataset.map(foo)
                 .batch(mini_batch_size)
dataset_id = dataset.register_dataset(
                                        service=dispatcher_address
                                        dataset=dataset
                                   )
dataset = tf.data.experimental.service.from_dataset_id(
                           processing_mode=""parallel_epochs"",
                           service=dispatcher_string,
                           dataset_id=dataset_id,
                           element_spec=dataset.element_spec)
dataset = dataset.prefetch()
model.fit(dataset)
                           
# Input pipeline of training process with index 1 to n
# Assume we get dataset_id and element_spec from somewhere
dataset = tf.data.experimental.service.from_dataset_id(
                           processing_mode=""parallel_epochs"",
                           service=dispatcher_string,
                           dataset_id=dataset_id,
                           element_spec=element_spec)
dataset = dataset.prefetch()
model.fit(dataset)
```

4. What's the path for a data element once it is produced? Worker -> Dispatcher -> training process? Worker -> training process? I am assuming the latter

5. From document (2), does this imply the memory used on the training process side or the Worker side?
> **max_outstanding_requests**
(Optional.) A limit on how many elements may be requested at the same time. You can use this option to control the amount of memory used, since distribute won't use more than element_size * max_outstanding_requests of memory.


6. Is there a way to share pre-processing across simultaneous independent training jobs with tf.data service ?

"
50541,/libmetal_plugin.dylib: Symbol not found: _TF_AssignUpdateVariable,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Big Sur OS X 11.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- Binary
- TensorFlow version (use command below):
-  python -m pip install tensorflow-metal
- Tensorflow Metal - v2.5
- Python version:
- 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
AMD Radeon Pro 5700 XT
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
ipython
Python 3.8.5 (default, Sep  4 2020, 02:22:02) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.24.1 -- An enhanced Interactive Python. Type '?' for help.

In [1]: import tensorflow
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-1-d6579f534729> in <module>
----> 1 import tensorflow

ModuleNotFoundError: No module named 'tensorflow'

In [2]: quit()
(tensorflow-metal) (base) davidlaxer@x86_64-apple-darwin13 notebooks % otool -hV /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib
/Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib:
Mach header
      magic  cputype cpusubtype  caps    filetype ncmds sizeofcmds      flags
MH_MAGIC_64   X86_64        ALL  0x00       DYLIB    22       3160   NOUNDEFS DYLDLINK TWOLEVEL WEAK_DEFINES BINDS_TO_WEAK NO_REEXPORTED_DYLIBS MH_HAS_TLV_DESCRIPTORS

 nm -n  /Users/davidlaxer/tensorflow-metal/lib/python3.8/site-packages/tensorflow-plugins/libmetal_plugin.dylib | grep TF_AssignUpdate
                 U _TF_AssignUpdateVariable

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50540,The ball in “hello world” can only achieve a quarter cycle,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux ubuntu 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: STM32F746
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:2.3.0
-   **Python version**:3.8
-   **Exact command to reproduce**:


### Describe the problem
I have followed readme.txt to deploy to STM32F746 with https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world
When I finished the whole process, the animation on the screen can only show a quarter cycle of sine?
### Source code / logs
Here is the screenshot of compiling
![微信图片_20210630213910](https://user-images.githubusercontent.com/52616905/123970379-a6911500-d9eb-11eb-9c63-2682b87c6439.jpg)

"
50539,Build with tpu support fails with protobuf error during api generation,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.6.0
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: n/a
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the problem**

The bazel build process fails with a protobuf error during api generation. Tf successfully builds without tpu support enabled (when removing  --config=tpu).
The tensorflow source code is derived from r2.6.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

bazel --output_user_root=<...> build --verbose_failures  --config=tpu //tensorflow/tools/pip_package:build_pip_package


**Any other info / logs**

The main error I get is:

`[libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/descriptor_database.cc:118] File already exists in database: google/protobuf/any.proto

[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/descriptor.cc:1379] CHECK failed: GeneratedDatabase()->Add(encoded_file_descriptor, size):  terminate called after throwing an instance of 'google::protobuf::FatalException' `

Attached is the full output of the bazel command.
[bazel.log](https://github.com/tensorflow/tensorflow/files/6741250/bazel.log)

Thanks a lot for the help!
"
50538,QNNPACK for TensorFlow Lite,"I am looking for an alternative for QNNPACK to execute quantized DNN effectively on CPU, ARM-based devices. In other words, what is the recommended approach to effectively interpret quantized NN?

Please can I have some examples?

Thank you."
50536,Grammatical Mistakes in the documentation of tf.keras.Model.load_weights,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/Model#load_weights

## Description of issue (what needs changing): 

The documentation is a bit confusing with the Terms like **`False Weights`**, **`should be the same as when`**, etc.. It can be refined."
50535,OSError: SavedModel file does not exist at: error,"Hello Please Help,

I am not much of an expert on tensorflow. I have followed the AI GUYS tutorial and created custom my yolov4 weights. I cannot convert it to tensorflow. Please help.

(yolov4-gpu) C:\Users\LENOVO-L3\yolov4-custom-functions>python detect.py --weights ./checkpoints/custom-416 --size 416 --model yolov4 --images ./data/images/car2.jpg --plate
Traceback (most recent call last):
File ""detect.py"", line 146, in
app.run(main)
File ""C:\Users\LENOVO-L3\anaconda3\envs\yolov4-gpu\lib\site-packages\absl\app.py"", line 312, in run
_run_main(main, args)
File ""C:\Users\LENOVO-L3\anaconda3\envs\yolov4-gpu\lib\site-packages\absl\app.py"", line 258, in _run_main
sys.exit(main(argv))
File ""detect.py"", line 49, in main
saved_model_loaded = tf.saved_model.load(FLAGS.weights, tags=[tag_constants.SERVING])
File ""C:\Users\LENOVO-L3\anaconda3\envs\yolov4-gpu\lib\site-packages\tensorflow\python\saved_model\load.py"", line 590, in load
return load_internal(export_dir, tags, options)
File ""C:\Users\LENOVO-L3\anaconda3\envs\yolov4-gpu\lib\site-packages\tensorflow\python\saved_model\load.py"", line 601, in load_internal
loader_impl.parse_saved_model_with_debug_info(export_dir))
File ""C:\Users\LENOVO-L3\anaconda3\envs\yolov4-gpu\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 56, in parse_saved_model_with_debug_info
saved_model = _parse_saved_model(export_dir)
File ""C:\Users\LENOVO-L3\anaconda3\envs\yolov4-gpu\lib\site-packages\tensorflow\python\saved_model\loader_impl.py"", line 113, in parse_saved_model
constants.SAVED_MODEL_FILENAME_PB))
OSError: SavedModel file does not exist at: ./checkpoints/custom-416/{saved_model.pbtxt|saved_model.pb}

I have run the below command before running the above. But the weight files did not converted as expected I think.
python save_model.py --weights ./data/yolov4.weights --output ./checkpoints/yolov4-416 --input_size 416 --model yolov4
Any help is much appreciated."
50534,How to apply a hierarchical mask in Tensorflow2.0 (tf.keras)?,"I am trying to build a hierarchical sequence model for time series classification (refer to the paper: hierarchical attention networks for document classification). But I'm very confused about how to mask the hierarchical sequences.

My data is a hierarchical time series. Specifically, each sample is composed of multiple sub-sequences and each sub-sequence is a multiple multivariate time series (just like word--> sentence -->document in NLP). So I need to pad and mask it twice. This is critical as a document will often not have the same number of sentences (or all sentences the same number of words). Finally, I get data as follows:



```
array([[[[0.21799476, 0.26063576],
         [0.2170655 , 0.53772384],
         [0.18505535, 0.30702454],
         [0.22714901, 0.17020395],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ]],

        [[0.2160176 , 0.23789616],
         [0.2675753 , 0.21807681],
         [0.26932836, 0.21914595],
         [0.26932836, 0.21914595],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ]]],

       [[[0.03941338, 0.3380829 ],
         [0.04766269, 0.3031088 ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ]],

        [[0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ],
         [0.        , 0.        ]]]], dtype=float32)
```
Then I build a hierarchical model as follows:
```
inputs = Input(shape=(maxlen_event, maxlen_seq, 2))
x = TimeDistributed(
        Sequential([
            Masking(),
            LSTM(units=8, return_sequences=False)
        ])
    )(inputs)
x = LSTM(units=32, return_sequences=False)(x)
x = Dense(16, activation='relu')(x)
output = Dense(16, activation='sigmoid')(x)
```

As my data is padded in on both dimensions, I don't know how to mask it correctly. I have two questions about it:
Q1: In TimeDistributed, do I use the masking layer correctly to mask the first padding?
Q2: How to mask the second padding?

Thank you."
50533,shared_embedding_columns are not supported when eager execution is enabled?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
when i use tf.feature_column.shared_embedding(...)，code raises the RuntimeValue about ""shared_embedding_columns are not supported when eager execution is enabled.""

Then i see the feature_column_v2.py，find 
```
  if context.executing_eagerly():
    raise RuntimeError('shared_embedding_columns are not supported when eager '
                       'execution is enabled.')
```

and i also find the tf.feature_column.embedding_column don't check eagerly。

So, my question is why they should be different? and can remove eager check?

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50532,Change batching in timeseries_dataset_from_array to be optional,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
The ``keras.preprocessing.timeseries_dataset_from_array`` function works well for creating timeseries datasets, but when creating the dataset from multiple independent timeseries (e.g. different locations) one would need to merge the datasets.
As the current implementation requires batching to be used and to my knowledge batched datasets cannot be concatenated, developers cannot use the generating function in that case.
A simple fix would be to allow the developer to provide ``None`` for the batch size and do not perform batching in that case.
This would be in line with the other parameters.

**Will this change the current api? How?**
Small change, the documentation of ``batch_size`` would need to be updated.

**Who will benefit with this feature?**
Users wanting to train more complex timeseries problems.

**Any Other info.**
"
50531,tensorflow v2.0.0 compatibility with scipy ,"hello ,
i'm developing a neural network to train a  data set , i'm getting this error:
raise ImportError('Image transformations require SciPy. '

ImportError: Image transformations require SciPy. Install SciPy.

i'm using tensorflow v2.0.0 what version of SciPy is compatible with tensorflow 2.0.0 ? what version of Scipy should i install and how ? I'm working with Nvidia Jetson  nano .
please help 
 "
50530,tensorflow v2.0.0 compatibility with scipy ,"Please go to Stack Overflow for help and support:
, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**:
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**:
-   **Python version**:
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.
"
50529,Linking with TFLite v2.5.0 fails due to undefined reference to rdft2d,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NVIDIA Xavier Jetson AGX, JetPack 4.5.1 (Ubuntu 18.04.5)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): TFLite installed from source
- TensorFlow version: v2.5.0
- Python version: v3.6.9
- CMake version: v1.19.5
- Installed using virtualenv? pip? conda?: no
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**

Installing TensorFlow Lite via [Collective Knowledge](http://cknowledge.org/) (which uses CMake), as described in this [Jupyter notebook](https://github.com/krai/ck-mlperf/blob/master/jnotebook/image-classification-tflite-loadgen/image-classification-tflite-loadgen.ipynb), with a new patch to support v2.5.0, would go fine but then fail with a linking error with `rdft2d`.

**Provide the exact sequence of commands / steps that you executed before running into the problem**

```bash
$ ck install package --tags=lib,tflite,via-cmake,with.ruy,v2.5.0
$ ck compile program:image-classification-tflite --dep_add_tags.library=tflite,v2.5.0
...
/home/katya/CK-TOOLS/lib-tflite-src-static-gcc-7.5.0-v2.5.0-with.ruy-linux-64/lib/libtensorflow-lite.a(rfft2d.cc.o): In function `tflite::ops::builtin::rfft2d::Rfft2dImpl(int, int, double**, int*, double*)':
rfft2d.cc:(.text+0x978): undefined reference to `rdft2d'
collect2: error: ld returned 1 exit status
```

**Any other info / logs**
What's weird is that despite the [apparent intent](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt#L54) to ""export"" `fftsg2d` which contains `rdft2d`, only `fftsg.c.o` would be linked against `libtensorflow-lite.a`. As a not completely logical workaround, I've managed just about to get it working with the following [patch](https://github.com/krai/ck-tensorflow/commit/09f8a605f4e69cc0e91a19844cef8f95a0d01f84#diff-8c0c02ef5dc0cea5405bd0bf376468e1107e5d721f8a99c534f9912b7ac59f35):

<pre><b>diff --git a/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt b/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt</b>
<b>index e7a5ed9b443..04c5b7c05db 100644</b>
<b>--- a/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt</b>
<b>+++ b/tensorflow/lite/tools/cmake/modules/fft2d/CMakeLists.txt</b>
<font color=""#2AA198"">@@ -37,7 +37,7 @@</font> target_include_directories(fft2d_alloc PUBLIC &quot;${FFT2D_SOURCE_DIR}&quot;)
 add_library(fft2d_fft4f2d &quot;${FFT2D_SOURCE_DIR}/fft4f2d.c&quot;)
 target_include_directories(fft2d_fft4f2d PRIVATE &quot;${FFT2D_SOURCE_DIR}&quot;)

<font color=""#DC322F"">-add_library(fft2d_fftsg &quot;${FFT2D_SOURCE_DIR}/fftsg.c&quot;)</font>
<font color=""#859900"">+add_library(fft2d_fftsg &quot;${FFT2D_SOURCE_DIR}/fftsg.c&quot; &quot;${FFT2D_SOURCE_DIR}/fftsg2d.c&quot;)</font>

 # Requires implementation of fft2d_alloc.
 add_library(fft2d_fftsg2d &quot;${FFT2D_SOURCE_DIR}/fftsg2d.c&quot;)
</pre>"
50526,Numpy Version,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): from source
- TensorFlow version: 2.5, 2.6, etc
- Python version: 3.9

Where can I find the nupmy version requirements for a Tensorflow build?
Please write requirements for numpy as you write for cuda.
"
50525,AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode',"def input_parser(self, case, slice_num):
	# read image and select the desire slice
	case = case.decode(""utf-8"")


File ""D:\anaconda\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 241, in __call__
    return func(device, token, args)

  File ""D:\anaconda\lib\site-packages\tensorflow\python\ops\script_ops.py"", line 130, in __call__
    ret = self._func(*args)

  File ""D:\anaconda\lib\site-packages\tensorflow\python\autograph\impl\api.py"", line 309, in wrapper
    return func(*args, **kwargs)

  File ""D:\Vnet2\NiftiDataset2D.py"", line 122, in input_parser
    case = case.decode(""utf-8"")

AttributeError: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'decode'

I have encountered this trouble, I hope someone can help me, thank you very much"
50524,Does tf.data.Dataset.from_tensor_slices() and tf.cast() creates extra memory,"Suppose ""imageX"", ""imageY"" are numpy arrays of size 20GB, is
dataset = tf.data.Dataset.from_tensor_slices((imageX, imageY))
creating an extra dataset using 40GB memory, or it will re-use the numpy array memory occupied by ""imageX"" and ""imageY""?

Also, does tf.cast() create extra memory or re-use exist numpy memory?"
50522,[Colab] [TF2.5] Reshaping images and labels after augmentation pipeline generates error during training,"**System information**
- Have I written custom code: 
  - Adapting this [tutorial](https://colab.research.google.com/notebooks/tpu.ipynb#scrollTo=LtAVr-4CP1rp) to run over custom implementation of model training example.
- OS Platform and Distribution:
  - Google colab enviroment (Tensorflow 2.5.0, **TPU & GPU backends**)
- Python version:
  - Python 3.7.10

### I have a tf.data pipeline (adapted to run on tpu according to the tutorial mentioned above).

### After parsing the tfrecord, it basically maps to an augmentation pipeline (model) that produces 32 augmented versions of every input image. Then it tile the labels and reshape the data (transform it into batches). 

Training runs fine untill  step 249 from first epoch when reshape generates an error. 
The ""Requested shape"" is equal to 1233125376 which corresponds to batch_size x num_generated_images (augmentation) x image_dimensions, namely: (32x8 x 32 x 224 x 224 x 3), whereas the input tensor which is equal to 125239296, actually corresponds to (26 x 32 x 224 x 224 x 3)  i don't have a clue why this is happening.  

### Error:

```python
Epoch 1/30
 249/2007 [==>...........................] - ETA: 32:46 - loss: 14.8993 - class_1_loss: 5.9974 - class_2_loss: 5.1770 - class_3_loss: 3.7249 - class_1_categorical_accuracy: 0.0472 - class_2_categorical_accuracy: 0.0721 - class_3_categorical_accuracy: 0.1524

---------------------------------------------------------------------------

InvalidArgumentError                      Traceback (most recent call last)

<ipython-input-23-c13881470e34> in <module>()
      4 
      5 resnet_50V2.summary()
----> 6 history = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)
      7 plot_training_history3(history)
      8 

InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: {{function_node __inference_train_function_214827}} Input to reshape is a tensor with 125239296 values, but the requested shape has 1233125376
	 [[{{node Reshape}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNext]]
  (1) Invalid argument: {{function_node __inference_train_function_214827}} Input to reshape is a tensor with 125239296 values, but the requested shape has 1233125376
	 [[{{node Reshape}}]]
	 [[MultiDeviceIteratorGetNextFromShard]]
	 [[RemoteCall]]
	 [[IteratorGetNext]]
	 [[cluster_train_function/_execute_2_0/_59]]
0 successful operations.
7 derived errors ignored.
```
``` python
def train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, model):
  data_augmentation = preprocessing_model()

  train_filenames = get_filenamesTPU(train_path)
  random.shuffle(train_filenames

  validation_filenames = get_filenamesTPU(validation_path)
  random.shuffle(validation_filenames)

  dataset_length = 91758  
  train_size =  dataset_length * 0.7
  validation_size = dataset_length - train_size

  batch_size = 32 * tpu_strategy.num_replicas_in_sync # (32 x 8)
  
  tpu_arg_0 = 32 * batch_size 
  
  data_reshape = lambda x,y: (tf.reshape(x,shape=(tpu_arg_0,224,224,3)), (tf.reshape(y[0],shape=(tpu_arg_0,1000)), tf.reshape(y[1],shape=(tpu_arg_0,516)),tf.reshape(y[2],shape=(tpu_arg_0,124))))
  
  augmentation_pipeline = lambda x,y: (data_augmentation(tf.expand_dims(x,axis=0)),(tf.tile(tf.reshape(y[0],[1,1000]),[32,1]),tf.tile(tf.reshape(y[1],[1,516]),[32,1]),tf.tile(tf.reshape(y[2],[1,124]),[32,1])))
  
  AUTO = tf.data.AUTOTUNE
  train_dataset = tf.data.TFRecordDataset(buffer_size=int(1e+8),num_parallel_reads=AUTO,filenames=train_filenames).map(parsing_fn,num_parallel_calls=AUTO).shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)
  train_dataset = train_dataset.map(augmentation_pipeline, num_parallel_calls=AUTO).batch(batch_size)
  train_dataset = train_dataset.map(data_reshape,num_parallel_calls=AUTO)
  train_dataset = train_dataset.repeat()
  train_dataset = train_dataset.prefetch(AUTO)

  # Create a validation dataset
  validation_dataset = tf.data.TFRecordDataset(num_parallel_reads=AUTO,filenames=validation_filenames).map(parsing_fn,num_parallel_calls=AUTO)
  validation_dataset = validation_dataset.map(augmentation_pipeline,num_parallel_calls=AUTO).batch(batch_size).map(data_reshape,num_parallel_calls=AUTO)
  validation_dataset = validation_dataset.prefetch(AUTO)
  validation_dataset = validation_dataset.repeat(1)

  validation_steps = validation_size / batch_size
  history = model.fit(x=train_dataset,
                          epochs=epochs,
                          steps_per_epoch=steps_per_epoch,                        
                          validation_data=validation_dataset,
                          validation_steps=validation_steps)
  return history

```

### Training fn:

```python
loss={
        ""class1"": 'CategoricalCrossentropy',
        ""class2"": 'CategoricalCrossentropy',
        ""class3"": 'CategoricalCrossentropy',
},

metrics = ['categorical_accuracy']
optimizer = Adam(lr=5e-3)

weight_file = None
with tpu_strategy.scope():
  resnet_50V2 = load_and_configure_model(optimizer, loss, metrics, weight_file)

base_directory = 'gs://2015_tfrecords'

train_path = base_directory+'/train/'
validation_path = base_directory+'/validation/'
buffer_size = 10240
epochs = 30
steps_per_epoch = 2007

resnet_50V2.summary()
history = train_model(train_path, validation_path, buffer_size, epochs, steps_per_epoch, resnet_50V2)
plot_training_history3(history)
```


I also accept suggestions on how to vectorize the data augmentation pipeline, since when i input a batch of images instead of a single tensor, it runs on parallel making the generated images come in a random sequence, hence making tiling the labels unfeasible."
50521,Specifying resource subtypes when converting graphdef to MLIR,"**System information**
- TensorFlow version (you are using): 2.7.0-dev20210629
- Are you willing to contribute it (Yes/No): Yes (but may need some guidance).

**Describe the feature and the current behavior/state.**
The current tools that allow conversion of TensorFlow models to MLIR, do not seem to allow specifying the subtype for resource type tensors in the input arguments.
Consider the following snippet:
```python
import tensorflow as tf
from tensorflow.python.pywrap_mlir import import_graphdef

SIZE = 3

class MyModel(tf.keras.Model):
    def build(self, input_shape):
        self.w = self.add_weight(shape=(SIZE,), trainable=True)

    def call(self, input):
        self.w.assign_add(input)
        return input

if __name__ == ""__main__"":
    model = MyModel()

    func = tf.function(model)
    concrete_func = func.get_concrete_function(
        tf.TensorSpec(shape=(SIZE,), dtype=tf.float32)
    )
    graph = concrete_func.graph

    mlir_tf = import_graphdef(
        graph.as_graph_def(add_shapes=True),
        ""tf-standard-pipeline"",
        False,
        input_names=[t.name for t in graph.inputs],
        input_data_types=[""DT_FLOAT"", ""DT_RESOURCE""],
        input_data_shapes=["","".join(str(d) for d in t.shape) for t in graph.inputs],
        output_names=[t.name for t in graph.outputs],
    )
    print(mlir_tf)

    with open(""model.mlir"", ""w"") as f:
        f.write(mlir_tf)
```
which outputs the following MLIR:
```mlir
module attributes {tf.versions = {bad_consumers = [], min_consumer = 0 : i32, producer = 811 : i32}}  {
  func @main(%arg0: tensor<3xf32>, %arg1: tensor<!tf.resource>) -> tensor<3xf32> attributes {tf.entry_function = {control_outputs = """", inputs = ""args_0:0,my_model/AssignAddVariableOp/resource:0"", outputs = ""Identity:0""}} {
    ""tf.AssignAddVariableOp""(%arg1, %arg0) {device = """"} : (tensor<!tf.resource>, tensor<3xf32>) -> ()
    %0 = ""tf.Identity""(%arg0) {device = """"} : (tensor<3xf32>) -> tensor<3xf32>
    return %0 : tensor<3xf32>
  }
}
```
Trying to lower this to `hlo` produces an error:
```console
>>> tf-opt --tf-to-hlo-pipeline model.mlir -o out.mlir
model.mlir:2:3: error: expects resource type of argument 1 to have one subtype, got '!tf.resource'
  func @main(%arg0: tensor<3xf32>, %arg1: tensor<!tf.resource>) -> tensor<3xf32> attributes {tf.entry_function = {control_outputs = """", inputs = ""args_0:0,my_model/AssignAddVariableOp/resource:0"", outputs = ""Identity:0""}} {
  ^
model.mlir:2:3: note: see current operation: ""func""() ( {
^bb0(%arg0: tensor<3xf32>, %arg1: tensor<!tf.resource>):  // no predecessors
  %0 = ""tf.ReadVariableOp""(%arg1) : (tensor<!tf.resource>) -> tensor<*xf32>
  %1 = ""tf.AddV2""(%0, %arg0) : (tensor<*xf32>, tensor<3xf32>) -> tensor<*xf32>
  ""tf.AssignVariableOp""(%arg1, %1) : (tensor<!tf.resource>, tensor<*xf32>) -> ()
  ""std.return""(%arg0) : (tensor<3xf32>) -> ()
}) {sym_name = ""main"", tf.entry_function = {control_outputs = """", inputs = ""args_0:0,my_model/AssignAddVariableOp/resource:0"", outputs = ""Identity:0""}, type = (tensor<3xf32>, tensor<!tf.resource>) -> tensor<3xf32>} : () -> ()
```
After manually modifying the type of `%arg1` to `tensor<!tf.resource<tensor<3xf32>>>`, the conversion to `hlo` works as expected.

It would be nice if there was a way to directly specify subtypes for resources when using `import_graphdef` (or `tf_mlir_translate` with the `--tf-input-arrays` option).

**Will this change the current api? How?**

Yes. One idea is that the `input_data_types` argument of `import_graphdef` could accept subtypes for `DT_RESOURCE` with a syntax like this:
```python
input_data_types=[""DT_FLOAT"", ""DT_RESOURCE<tensor<3xf32>>""],
```
or like this
```python
input_data_types=[""DT_FLOAT"", ""DT_RESOURCE(3:DT_FLOAT)""],
```
or something similar.
`tf_mlir_translate --tf-input-arrays` could adopt the same syntax.

**Who will benefit with this feature?**

Anyone who wants to export models with resource inputs to MLIR (e.g. the graph of a single training step).

**Any Other info.**

Please let me know if there is a different method to emit the desired MLIR."
50518,Label_image always giving same outputs,"Hi all !

While testing our models with basic [label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) example, we faced with strange results.

We tried _label_image_ with 2 different YOLO models with their corresponding label.txt. However regardless the input images and labels, the test output is always in the same order as [2 3 0 1]. Finally we also gave the Grace Hopper image from official source and still we faced with same issue. You can see the outputs in the image below.

Normally our models are working smoothly on PC. But, it is important for us to test them in our iMX6 device with this basic sample. Do you have any idea or solution about this problem ?
Thank you in advance.

![image](https://user-images.githubusercontent.com/56031118/123631875-8b6dac00-d817-11eb-8ba0-17047e522da4.png)

"
50517,tensorflow2.0.0 compatibility with scipy ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50516,Recover callback history when training is interrupted,"I could not find a solution to my demand, so I believe it should be a feature to be implemented.

**System information**
- TensorFlow version (you are using): 2
- Are you willing to contribute it (Yes/No): Yes

**Describe the feature and the current behavior/state.**
I was wondering if it would be possible to checkpoint and recover the callback history, so that my callbacks can continue whatever they were tracking when training is interrupted for any reason.

**Will this change the current api? How?**
I don't know.

**Who will benefit with this feature?**
Whoever performs long training sessions.

**Any Other info.**
NA"
50514,tflite-runtime (2.5) needs GLIBC_2.29 with Python 3.8,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mendel Linux (Coral Dev Board)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: tflite-runtime 2.5
- Python version: 3.8
- Installed using virtualenv? pip? conda?: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: --- 
- GPU model and memory: ---

Hey,

I'm not sure if this is the right place but I'm having problems with the tflite-runtime and python 3.8 on the coral dev board.
I've upgraded to python 3.8 due to some other requirements and installed tflite-runtime with version 2.5.
Now when I try to import the runtime I get this error:

```
Python 3.8.0 (default, May 17 2021, 08:51:42) 
[GCC 8.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tflite_runtime.interpreter as tflite
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/mendel/env3.8/lib/python3.8/site-packages/tflite_runtime/interpreter.py"", line 36, in <module>
    from tflite_runtime import _pywrap_tensorflow_interpreter_wrapper as _interpreter_wrapper
ImportError: /lib/aarch64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/mendel/env3.8/lib/python3.8/site-packages/tflite_runtime/_pywrap_tensorflow_interpreter_wrapper.cpython-38-aarch64-linux-gnu.so)
```

Any suggestions how to fix this?

Bruno
"
50513,please help me ,"[
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras.layers import Dense,Conv1D,Flatten
from tensorflow.keras.models import Sequential, Model
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
import keras
from sklearn.metrics import accuracy_score,confusion_matrix
import plotly.offline as py
import xgboost as xgb
%matplotlib inline
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)
model=Sequential()
model.add(Flatten(input_shape=(10,)))
model.add(Dense(100,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(optimizer='adam',metrics=['accuracy'],loss='SparseCategoricalCrossentropy')
model.fit(x_train,y_train,batch_size=64,validation_split=0.1,epochs=25)
preds=model.predict(x_test)
preds=np.where(preds>0.5,1,0)
accuracy_score(y_test,press)]
AttributeErrorTraceback (most recent call last)
<ipython-input-18-a68cc7861501> in <module>()
      3 model.add(Dense(100,activation='relu'))
      4 model.add(Dense(1,activation='sigmoid'))
----> 5 model.compile(optimizer='adam',metrics=['accuracy'],loss='SparseCategoricalCrossentropy')
      6 model.fit(x_train,y_train,batch_size=64,validation_split=0.1,epochs=25)
      7 preds=model.predict(x_test)

3 frames
/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/keras/engine/training_utils.pyc in get_loss_function(loss)
   1186   return losses.LossFunctionWrapper(
   1187       loss_fn,
-> 1188       name=loss_fn.__name__,
   1189       reduction=losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE)
   1190 

AttributeError: 'SparseCategoricalCrossentropy' object has no attribute '__name__'
"
50512,"InvalidArgumentError:  indices[10] = -1 is not in [0, 16081) 	 [[node model_1/embedding_1/embedding_lookup (defined at <ipython-input-34-8168eb82f052>:1) ]] [Op:__inference_test_function_10607]","I am building a Wide & Deep model with Keras. However, after training and testing the model I get this error. I find the -1 very strange.

This is my code:

```
from tensorflow.keras.layers.experimental.preprocessing import StringLookup


def encode_inputs(inputs, use_embedding=False):
    encoded_features = []
    num_oov_indices = 10
    for feature_name in inputs:
        if feature_name in CATEGORICAL_FEATURE_NAMES:
            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]
            # Create a lookup to convert string values to an integer indices.
            # Since we are not using a mask token nor expecting any out of vocabulary
            # (oov) token, we set mask_token to None and  num_oov_indices to 0.
            lookup = StringLookup(
                vocabulary=vocabulary,
                mask_token=None,
                num_oov_indices=num_oov_indices,
                output_mode=""int"" if use_embedding else ""binary"",
            )
            if use_embedding:
                # Convert the string input values into integer indices.
                encoded_feature = lookup(inputs[feature_name])
                #embedding_dims = int(math.sqrt(len(vocabulary)))
                embedding_dims = int(np.ceil(len(vocabulary) ** 0.25))
                
                # Create an embedding layer with the specified dimensions.
                embedding = layers.Embedding(
                    input_dim=len(vocabulary), output_dim=embedding_dims
                )
                # Convert the index values to embedding representations.
                encoded_feature = embedding(encoded_feature)
            else:
                # Convert the string input values into a one hot encoding.
                encoded_feature = lookup(tf.expand_dims(inputs[feature_name], -1))
        else:
            # Use the numerical features as-is.
            encoded_feature = tf.expand_dims(inputs[feature_name], -1)

        encoded_features.append(encoded_feature)

    all_features = layers.concatenate(encoded_features)
    return all_features
```


The model creation:
```
def create_wide_and_deep_model():

    inputs = create_model_inputs()
    wide = encode_inputs(inputs)
    wide = layers.BatchNormalization()(wide)

    deep = encode_inputs(inputs, use_embedding=True)
    for units in hidden_units:
        deep = layers.Dense(units)(deep)
        deep = layers.BatchNormalization()(deep)
        deep = layers.ReLU()(deep)
        deep = layers.Dropout(dropout_rate)(deep)

    merged = layers.concatenate([wide, deep])
    outputs = layers.Dense(units=1, activation=""linear"")(merged)
    model = keras.Model(inputs=inputs, outputs=outputs)
    return model
```

When I try: ```wide_and_deep_model.fit(train_dataset, epochs=num_epochs)```

I get the following error:
```
InvalidArgumentError:  indices[119] = 16083 is not in [0, 16081)
	 [[node model/embedding_1/embedding_lookup (defined at <ipython-input-28-db18f4de269d>:1) ]] [Op:__inference_train_function_2502]

Errors may have originated from an input operation.
Input Source operations connected to node model/embedding_1/embedding_lookup:
 model/embedding_1/embedding_lookup/1869 (defined at /usr/local/Cellar/python@3.9/3.9.4/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:117)

Function call stack:
train_function```


"
50511,ValueError: Python inputs incompatible with input_signature:,"environment：tf2.2\ linux \2080Ti \cuda:10.2

I use **tf.kerns.model** and **tf.gradienttape** to build my (BERT-CRF) custom model, and now I want to use **tensorflow_ model_ Serving** deploys my model, so the first thing I need to do is save the model as a **"". Pb"" file**

I tried to use **tf. saved_ model. save()** to save the model, but I encountered the problem of **dtype**, because in tf2.2, I can't change the dtype of tensor to tf.int32, whether I use **constant, cast or conver_to_tensor and variable** can't change dtype to tf.int32, and their output forms are changed to int32

I saw from the Internet that by setting **@tf. function (tf. TensorSpec())** before the **call()** function, I still encountered the problem that dtype could not be converted to tf. Int32

What I want to ask is, if you use tf.keras.model to build your own model, how can you save the model as "". Pb"", or can you tell me how to change the tensor of tf2.2 to tf.int32?

[https://github.com/1148330040/nlp_cvs/blob/main/Models/bert_crf.py](url)

`
model:


 class MyBertCrf(tf.keras.Model):

    def __init__(self, use_crf, input_dim, output_dim):
        super(MyBertCrf, self).__init__(use_crf, input_dim, output_dim)
        self.use_crf = use_crf
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.bert = TFBertModel.from_pretrained('hfl/chinese-bert-wwm-ext')
        self.dropout = tf.keras.layers.Dropout(0.3)
        self.dense = tf.keras.layers.Dense(self.output_dim)
        self.other_params = tf.Variable(tf.random.uniform(shape=(output_dim, output_dim)))

    @tf.function(input_signature=[tf.TensorSpec([None, 128], name='ids', dtype=tf.int32),
                                  tf.TensorSpec([None, 128], name='mask', dtype=tf.int32),
                                  tf.TensorSpec([None, 128], name='tokens', dtype=tf.int32),
                                  tf.TensorSpec([None, 128], name='target', dtype=tf.int32),
                                  tf.TensorSpec([1], name='input_seq_len', dtype=tf.int32)])
    def call(self, ids, masks, tokens, target, input_seq_len):
        hidden = self.bert(ids, masks, tokens)[0]
        dropout_inputs = self.dropout(hidden, 1)
        logistic_seq = self.dense(dropout_inputs)
        print(hidden)
        print(ids, masks, tokens, target, input_seq_len)
        if self.use_crf:
            log_likelihood, self.other_params = tfa.text.crf.crf_log_likelihood(logistic_seq,
                                                                                target,
                                                                                input_seq_len,
                                                                                self.other_params )
            decode_predict, crf_scores = tfa.text.crf_decode(logistic_seq, self.other_params , input_seq_len)

            return decode_predict, log_likelihood, crf_scores
        else:
            prob_seq = tf.nn.softmax(logistic_seq)

            return prob_seq, None, None`

error:

`    
raise ValueError(""Python inputs incompatible with input_signature:\n%s"" %
ValueError: Python inputs incompatible with input_signature:
  inputs: (
    tf.Tensor(
[[ 101 6163 1906 ...    0    0    0]
 [ 101  872 4761 ...    0    0    0]
 ...
 [ 101 1259 6163 ...    0    0    0]
 [ 101 2769 2682 ...    0    0    0]], shape=(16, 128), dtype=int32),
    tf.Tensor(
[[1 1 1 ... 0 0 0]
 ...
 [1 1 1 ... 0 0 0]], shape=(16, 128), dtype=int32),
    tf.Tensor(
[[0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]], shape=(16, 128), dtype=int32),
    tf.Tensor(
[[2 2 2 ... 0 0 0]
 ...
 [1 1 1 ... 0 0 0]], shape=(16, 128), dtype=int32),
    tf.Tensor([19 19 28 28 21 27 19 20 32 22 23 29 27 21 29 25], shape=(16,), dtype=int32))
  input_signature: (
    TensorSpec(shape=(None, 128), dtype=tf.int32, name='ids'),
    TensorSpec(shape=(None, 128), dtype=tf.int32, name='mask'),
    TensorSpec(shape=(None, 128), dtype=tf.int32, name='tokens'),
    TensorSpec(shape=(None, 128), dtype=tf.int32, name='target'),
    TensorSpec(shape=(1,), dtype=tf.int32, name='input_seq_len'))

Process finished with exit code 1
`"
50510,The Phi definition of GELU is incorrect,"## URL(s) with the issue:

https://www.tensorflow.org/addons/api_docs/python/tfa/activations/gelu?hl=en

## Description of issue (what needs changing):

### Clear description

Currently, the definition of `\Phi(x)` in the GELU document is as the follow:

```
    \Phi(x) = \frac{x}{2} \left[ 1 + \tanh(\sqrt{\frac{2}{\pi}} \cdot (x + 0.044715 \cdot x^3)) \right]
```

But it seems that this is the definition of `gelu(x)`, is not it of `\Phi(x)` in the original paper.

I expect that a definition of `\Phi(x)` was as the follow:

```
    \Phi(x) = \frac{1}{2} \left[ 1 + \tanh(\sqrt{\frac{2}{\pi}} \cdot (x + 0.044715 \cdot x^3)) \right]
```

Or, the document defined `gelu(x)` directly when `approximate` is `True` like the follow:

```
    gelu(x) = \frac{x}{2} \left[ 1 + \tanh(\sqrt{\frac{2}{\pi}} \cdot (x + 0.044715 \cdot x^3)) \right]
```"
50509,"For Keras training, can we relax the steps checking when distributed dataset is passed in","Recently, when we tried to use MultiWorkerMirroredStrategy with Keras, we found:
1) When Keras wrap our passed in dataset with experimental distributed dataset, we found we cannot scale over x nodes because it needs us pass in a global batch size and global batch size needs to take number of workers into consideration (global batch size = batch size * num of workers * num of replica). Therefore, when we have a lot of workers, compared with Mirrored strategy, we start seeing job failure due to OOM
2) We try to get around this issue by passing in distribute_datasets_from_function that we can have full control over per replica batch and sharding logic (and get around OOM issue). Then our job failed at: https://github.com/tensorflow/tensorflow/blob/1923123d32ea41d92b70a27a3f6ecf0763b56f6c/tensorflow/python/keras/engine/data_adapter.py#L733
When we passed in normal dataset, it has UNKNOWN cardinality and leverage https://github.com/tensorflow/tensorflow/blob/1923123d32ea41d92b70a27a3f6ecf0763b56f6c/tensorflow/python/keras/engine/data_adapter.py#L710 to recreate iterator for every epoch. Our use case is to have validation step to exhaust our dataset instead of hard coding steps. I wonder if we can relax check in L733 altogether with change to L714. Then we can support no steps input from users? If you agree, I can submit the PR to make the change. 

Please let me know if any downside of doing so.

Thanks"
50507,Iterating through DistributedDataset created from dataset with raggedTensor raises InvalidArgumentError ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): dockerhub 2.5.0-gpu
- TensorFlow version (use command below): 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: cuda_11.2
- GPU model and memory:  4 * Tesla T4  with 15109MiB memory


**Describe the current behavior**
After creating a DistributedDataset instances from dataset with raggedTensor elements inside, I tried to iterate through it
but InvalidArgumentError raised.  What' more, I found that only raggedTensor with string dtype has this problem and int raggedtenor works fine. 

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])

# it works
ragged_dict = {'ragged':tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], [], [3, 1, 4, 1], [3, 1], [2, 1, 4, 1]])}
ragged_ds = tf.data.Dataset.from_tensor_slices(ragged_dict ).batch(2)
dist_dataset = strategy.experimental_distribute_dataset(ragged_ds)
ds = iter(dist_dataset)
next(ds)

# it doesn't work
ragged_dict = {'ragged' : tf.ragged.constant([['3', '1', '4', '1'], [], ['5', '9', '2'], ['6'], [], ['3', '1', '4', '1'], ['3', '1'], ['2', '1', '4', '1']])}
ragged_ds = tf.data.Dataset.from_tensor_slices(ragged_dict).batch(2)
dist_dataset = strategy.experimental_distribute_dataset(ragged_ds)
ds = iter(dist_dataset)
next(ds)
```

**Other info / logs** 
```
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 686, in __next__
    return self.get_next()
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 730, in get_next
    self, self._strategy, return_per_replica=False)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 604, in _get_next_as_optional
    iterator._iterators[i].get_next_as_list())  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/input_lib.py"", line 1948, in get_next_as_list
    self._iterator.get_next_as_optional())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/multi_device_iterator_ops.py"", line 604, in get_next_as_optional
    result.append(self._device_iterators[i].get_next_as_optional())
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 825, in get_next_as_optional
    self.element_spec)), self.element_spec)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2809, in iterator_get_next_as_optional
    _ops.raise_from_not_ok_status(e, name)
  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.
  (0) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
  (1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string
0 successful operations.
0 derived errors ignored.
         [[{{node RemoteCall}}]] [Op:IteratorGetNextAsOptional]

```
"
50504,Is there a way to update a dynamic variable used to filter a part of your training data during training?,"```
def myfilter(x, my_var):
    return tf.equal(x['vars'], my_var)


data = tf.data.TFRecordDataset(tf.io.match_filenames_once('part-*'))
my_var  = tf.Variable(1, trainable=False, name='my_var', dtype=tf.int64)

data = data.map(parsing_func, num_parallel_calls=multiprocessing.cpu_count() - 1)
data= data.filter(lambda x : myfilter(x, my_var) )
data = data.batch(batch_size=32)

```

Here, using static `my_var`, I am able to filter the data. However, I want to keep updating the var value from [1, 2, .... n]. Any ideas on how to do this during training? 

I was trying something like this:

```
class CustomVarScheduler(tf.keras.callbacks.Callback):
    def __init__(self, my_var):
        super(CustomPhaseScheduler, self).__init__()
        self.var = my_var

    def on_epoch_end(self, epoch, logs=None):
        # Set the value back to the optimizer before this epoch starts
        tf.keras.backend.set_value(self.my_var, tf.math.add(self.my_var, 1))
        print(""\nEpoch %05d: my_var is %6.4f."" % (epoch, self.my_var))

```
Not able to get it to work properly. Any help? or Possible Documentation/Feature. 

Also, willing to contribute to such documentation.  

Thanks.
"
50503,Is there a plan to support horizontal scaling for KPL?,"Based on current preprocessing layer code: https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/engine/base_preprocessing_layer.py#L237

horizontal scaling is not supported for KPL. For example, I cannot have TextVectorization run on different machines with different sharded data and merge at the end. I wonder is there any plan for KPL to support horizontal scaling so that it can adapt with my large dataset?"
50502,"The TFLite Converter has errors when ""training=True"" is used in Dropout and BatchNormalization layers","### 1. System information

- OS Platform and Distribution (e.g., Windows 10):
- TensorFlow installation (pip package):
- TensorFlow library (TF 2.5.0):

### 2. Code

When training GANs, some Dropout and BatchNormalization layers need to have ""training=True"" set for the training to work correctly.  This is as per a [book on GANs](https://machinelearningmastery.com/generative_adversarial_networks/) where the chapter on pix2pix translation is based on [this article](https://phillipi.github.io/pix2pix/).   Unfortunately ""training=True"" trips up the TensorFlow Lite Converter with the errors shown in the sample code below.  It would be good if the TensorFlow Lite Converter ignored the ""training=True"" arguments.

The bug can be recreated with the following code.
```
import tensorflow as tf
from keras.models import Input
from keras.models import Model
from keras.layers import Conv2D
from keras.layers import Dropout
from keras.layers import BatchNormalization

# Create minimalist model to show the errors (actual model is much bigger)
in_image = Input(shape=(256,256,3))
out_image = Conv2D(64, (4,4))(in_image)
# In the following two lines, the arguments ""training=True"" cause errors in the the TensorFlow Lite Converter.
# Removing both ""training=True"" arguments below makes the conversion work fine, but including the ""training=True"" causes the errors shown.
out_image = Dropout(0.5)(out_image, training=True)          # Causes error: ""TF Select ops: RandomUniform, Details: tf.RandomUniform {device = """", seed = 0 : i64, seed2 = 0 : i64}"" below
out_image = BatchNormalization()(out_image, training=True)  # Causes error: ""0 incompatible with expected resource"" below
model = Model(in_image, out_image)
model.compile()

# Convert model to TensorFlow Lite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)
```
There might be other layers which can have ""training=True"" set."
50501,test,"<em>Please make sure that this is an issue related to keras.
tag:keras_template</em>

## **Important Notice**

Please note that `tf.keras` code was moved entirely to
[keras-team/keras](https://github.com/keras-team/keras) repository

You can open any code/doc bugs, performance issues, and feature requests
 in [keras-team/keras](https://github.com/keras-team/keras/issues) repository

`tf.keras` related issues opened in
[tensorflow/tensorflow](https://github.com/tensorflow/tensorflow) repository may
not get attention as [keras-team/keras](https://github.com/keras-team/keras)
repository is dedicated for the development of `keras` code
"
50497,tensorflow lite (C++) crashes in tflite::reference_ops::Gather,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Catalina, Linux Ubuntu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4, 2.5, current
- Python version: 3.8
- Bazel version (if compiling from source): N/A (tensorflow-lite was compiled w/ cmake)
- GCC/Compiler version (if compiling from source): clang-12.0.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
After a few iterations over the input data tensorflow-lite crashes in tflite::reference_ops::Gather<>

**Describe the expected behavior**
Not to crash!

**Standalone code to reproduce the issue**
[standalone.tar.gz](https://www.dropbox.com/s/dwkypf5skrxzfqa/standalone.tar.gz?dl=0)

**Other info / logs** 
Backtrace (using lldb on Mac OS Catalina):
```
(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = signal SIGABRT
  * frame #0: 0x00007fff69bb033a libsystem_kernel.dylib`__pthread_kill + 10
    frame #1: 0x00007fff69c6ce60 libsystem_pthread.dylib`pthread_kill + 430
    frame #2: 0x00007fff69b37808 libsystem_c.dylib`abort + 120
    frame #3: 0x000000010049ec05 standalone`void tflite::reference_ops::Gather<float, int>(op_params=0x00007ffeefbfeee0, input_shape=0x00007ffeefbfef48, input_data=0x0000000104136ce4, coords_shape=0x00007ffeefbfef28, coords_data=0x000000011261a000, output_shape=0x00007ffeefbfef08, output_data=0x000000011261d6c0) at gather.h:76:9
    frame #4: 0x000000010049cdc6 standalone`TfLiteStatus tflite::ops::builtin::gather::Gather<float, int>(params=0x000000010302a780, input=0x000000010380a310, positions=0x000000010380a000, output=0x000000010380a770) at gather.cc:125:3
    frame #5: 0x000000010049c9b9 standalone`tflite::ops::builtin::gather::Eval(context=0x0000000103038eb8, node=0x000000010381e000) at gather.cc:166:16
    frame #6: 0x0000000100039543 standalone`tflite::Subgraph::OpInvoke(this=0x0000000103038e90, op_reg=0x000000010381e050, node=0x000000010381e000) at subgraph.h:434:12
    frame #7: 0x000000010003919b standalone`tflite::Subgraph::Invoke(this=0x0000000103038e90) at subgraph.cc:1104:9
    frame #8: 0x00000001006b91af standalone`tflite::Interpreter::Invoke(this=0x0000000103038da0) at interpreter.cc:273:3
    frame #9: 0x0000000100004b2a standalone`main((null)=<unavailable>, (null)=<unavailable>) at main.cc:139:5 [opt]
    frame #10: 0x00007fff69a68cc9 libdyld.dylib`start + 1
    frame #11: 0x00007fff69a68cc9 libdyld.dylib`start + 1
(lldb) 
```"
50494,Getting error while performing one hot encoding,"MemoryError                               Traceback (most recent call last)
<ipython-input-333-6e9121367701> in <module>
      1 ## categorical processing
      2 
----> 3 df_cat_dum = pd.get_dummies(df_cat)

~\anaconda3\lib\site-packages\pandas\core\reshape\reshape.py in get_dummies(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)
    887         for (col, pre, sep) in zip(data_to_encode.items(), prefix, prefix_sep):
    888             # col is (column_name, column), use just column data here
--> 889             dummy = _get_dummies_1d(
    890                 col[1],
    891                 prefix=pre,

~\anaconda3\lib\site-packages\pandas\core\reshape\reshape.py in _get_dummies_1d(data, prefix, prefix_sep, dummy_na, sparse, drop_first, dtype)
   1003 
   1004     else:
-> 1005         dummy_mat = np.eye(number_of_cols, dtype=dtype).take(codes, axis=0)
   1006 
   1007         if not dummy_na:

MemoryError: Unable to allocate 654. MiB for an array with shape (26187, 26187) and data type uint8"
50492,Pipeline sharing between tf.data Dispatcher and Worker,"## Tensorflow Version
Tensorflow v2.5.0

## Description of issue (what needs changing):
How does the Workers find out about the input pipeline code? Is it serialised over the network from the main Tensorflow process, to the Dispatcher and then to the Worker?

### Clear description
I'm looking to find out to see if there's any need for encryption between the process.

### Parameters defined
N/A

### Returns defined
N/A


### Raises listed and defined
N/A

### Usage example
N/A

### Request visuals, if applicable
N/A 

### Submit a pull request?
N/A
"
50490,TFLite Quantization Old Spec version?,"### 1. System information

- OS Ubuntu 16.04:

----

Hi I have a question:

I'm intended to use tflite with vx-delegate (provided by VeriSillicon) for A311D SoC. The SoC has chip with INT8 support.
Im able to convert tf to tflite and run my tflite model, no problem here. Both on ARM CPU and INT8 chip.

The problem is performance: when I use TF2.4 for conversion and tflite runtime, INT8 performance is subpar, because as I was told, A311D doesn't support ""new"" per channel quantization. And in order to get most out of the chip I need per tensor quantization.

_So, the question is: how to quantize model in ""old"" per tensor quantization format?_

thanks! "
50488,LSTM performs worse than CuDNNLSTM,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
NA
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version:
3.8
- Bazel version (if compiling from source):
NA
- GCC/Compiler version (if compiling from source):
NA
- CUDA/cuDNN version:
CUDA = 11.3
cuDNN = 8201
- GPU model and memory:
Nvidia RTX 3070 (24gb)

**Describe the current behavior**
`tensorflow.python.keras.layers.CuDNNLSTM` is much faster than `tensorflow.python.keras.layers.LSTM`.

**Describe the expected behavior**
GPU utilisation of deprecated `tensorflow.python.keras.layers.CuDNNLSTM` should be in  `tensorflow.python.keras.layers.LSTM` since TF 2.0

**Standalone code to reproduce the issue**
```python
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.callbacks import ModelCheckpoint
from tensorflow.python.keras.layers import LSTM, CuDNNLSTM

X = [
    [[1, 1, 1]],
    [[1, 1, 1]]
]

Y = [
    [1, 1, 1],
    [1, 1, 1]
]

X = numpy.array(X)
Y = numpy.array(Y)

use_cud = True

model = Sequential()  # initialize a sequential model
if use_cud:
   model.add(CuDNNLSTM(3, return_sequences=False, input_shape=(1, 3))) 
else:
   model.add(LSTM(3, return_sequences=False, input_shape=(1, 3))) 
model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])


model.fit(X, Y, batch_size=1, epochs=100, validation_split=0.5)
```

**Other info / logs** 

For the sake of simplicity I kept the model simple, so the difference between duration isn't that big here. But with larger models, the difference is day and night. CuDNNLSTM taking 20 seconds whereas LSTM takes 3 minutes. If needed I might be able to provide a simple model that showcases this. 

### CuDNNLSTM logs
```
Python 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)] on win32
runfile('C:/Users/31619/PycharmProjects/nn-drum-midi-synthesis/stackoverflow_2.py', wdir='C:/Users/31619/PycharmProjects/nn-drum-midi-synthesis')
2021-06-28 18:22:10.218773: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-28 18:22:13.402089: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
2021-06-28 18:22:13.427953: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-06-28 18:22:13.428334: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-28 18:22:13.433853: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-28 18:22:13.434053: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-06-28 18:22:13.436251: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
2021-06-28 18:22:13.437295: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
2021-06-28 18:22:13.439241: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
2021-06-28 18:22:13.441256: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
2021-06-28 18:22:13.442057: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-28 18:22:13.442322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-28 18:22:13.443064: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-28 18:22:13.443999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-06-28 18:22:13.444555: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-28 18:22:14.074090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-28 18:22:14.074295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-06-28 18:22:14.074411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-06-28 18:22:14.074697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5235 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-06-28 18:22:18.643413: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/100
2021-06-28 18:22:23.781377: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-28 18:22:28.853394: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
1/1 [==============================] - 15s 15s/step - loss: 0.9057 - accuracy: 0.0000e+00 - val_loss: 0.8996 - val_accuracy: 0.0000e+00
Epoch 2/100
1/1 [==============================] - 0s 22ms/step - loss: 0.8996 - accuracy: 0.0000e+00 - val_loss: 0.8952 - val_accuracy: 0.0000e+00
Epoch 3/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8952 - accuracy: 0.0000e+00 - val_loss: 0.8915 - val_accuracy: 0.0000e+00
Epoch 4/100
1/1 [==============================] - 0s 18ms/step - loss: 0.8915 - accuracy: 0.0000e+00 - val_loss: 0.8882 - val_accuracy: 0.0000e+00
Epoch 5/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8882 - accuracy: 0.0000e+00 - val_loss: 0.8852 - val_accuracy: 0.0000e+00
Epoch 6/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8852 - accuracy: 0.0000e+00 - val_loss: 0.8823 - val_accuracy: 0.0000e+00
Epoch 7/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8823 - accuracy: 0.0000e+00 - val_loss: 0.8797 - val_accuracy: 0.0000e+00
Epoch 8/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8797 - accuracy: 0.0000e+00 - val_loss: 0.8771 - val_accuracy: 0.0000e+00
Epoch 9/100
1/1 [==============================] - 0s 24ms/step - loss: 0.8771 - accuracy: 0.0000e+00 - val_loss: 0.8746 - val_accuracy: 0.0000e+00
Epoch 10/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8746 - accuracy: 0.0000e+00 - val_loss: 0.8722 - val_accuracy: 0.0000e+00
Epoch 11/100
1/1 [==============================] - 0s 20ms/step - loss: 0.8722 - accuracy: 0.0000e+00 - val_loss: 0.8699 - val_accuracy: 0.0000e+00
Epoch 12/100
1/1 [==============================] - 0s 22ms/step - loss: 0.8699 - accuracy: 0.0000e+00 - val_loss: 0.8676 - val_accuracy: 0.0000e+00
Epoch 13/100
1/1 [==============================] - 0s 22ms/step - loss: 0.8676 - accuracy: 0.0000e+00 - val_loss: 0.8654 - val_accuracy: 0.0000e+00
Epoch 14/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8654 - accuracy: 0.0000e+00 - val_loss: 0.8632 - val_accuracy: 0.0000e+00
Epoch 15/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8632 - accuracy: 0.0000e+00 - val_loss: 0.8610 - val_accuracy: 0.0000e+00
Epoch 16/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8610 - accuracy: 0.0000e+00 - val_loss: 0.8589 - val_accuracy: 0.0000e+00
Epoch 17/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8589 - accuracy: 0.0000e+00 - val_loss: 0.8568 - val_accuracy: 0.0000e+00
Epoch 18/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8568 - accuracy: 0.0000e+00 - val_loss: 0.8547 - val_accuracy: 0.0000e+00
Epoch 19/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8547 - accuracy: 0.0000e+00 - val_loss: 0.8526 - val_accuracy: 0.0000e+00
Epoch 20/100
1/1 [==============================] - 0s 18ms/step - loss: 0.8526 - accuracy: 0.0000e+00 - val_loss: 0.8505 - val_accuracy: 0.0000e+00
Epoch 21/100
1/1 [==============================] - 0s 18ms/step - loss: 0.8505 - accuracy: 0.0000e+00 - val_loss: 0.8485 - val_accuracy: 0.0000e+00
Epoch 22/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8485 - accuracy: 0.0000e+00 - val_loss: 0.8465 - val_accuracy: 0.0000e+00
Epoch 23/100
1/1 [==============================] - 0s 22ms/step - loss: 0.8465 - accuracy: 0.0000e+00 - val_loss: 0.8444 - val_accuracy: 0.0000e+00
Epoch 24/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8444 - accuracy: 0.0000e+00 - val_loss: 0.8424 - val_accuracy: 0.0000e+00
Epoch 25/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8424 - accuracy: 0.0000e+00 - val_loss: 0.8404 - val_accuracy: 0.0000e+00
Epoch 26/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8404 - accuracy: 0.0000e+00 - val_loss: 0.8384 - val_accuracy: 0.0000e+00
Epoch 27/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 0.8365 - val_accuracy: 0.0000e+00
Epoch 28/100
1/1 [==============================] - 0s 18ms/step - loss: 0.8365 - accuracy: 0.0000e+00 - val_loss: 0.8345 - val_accuracy: 0.0000e+00
Epoch 29/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8345 - accuracy: 0.0000e+00 - val_loss: 0.8325 - val_accuracy: 0.0000e+00
Epoch 30/100
1/1 [==============================] - 0s 18ms/step - loss: 0.8325 - accuracy: 0.0000e+00 - val_loss: 0.8306 - val_accuracy: 0.0000e+00
Epoch 31/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8306 - accuracy: 0.0000e+00 - val_loss: 0.8286 - val_accuracy: 0.0000e+00
Epoch 32/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8286 - accuracy: 0.0000e+00 - val_loss: 0.8267 - val_accuracy: 0.0000e+00
Epoch 33/100
1/1 [==============================] - 0s 20ms/step - loss: 0.8267 - accuracy: 0.0000e+00 - val_loss: 0.8247 - val_accuracy: 0.0000e+00
Epoch 34/100
1/1 [==============================] - 0s 20ms/step - loss: 0.8247 - accuracy: 0.0000e+00 - val_loss: 0.8228 - val_accuracy: 0.0000e+00
Epoch 35/100
1/1 [==============================] - 0s 22ms/step - loss: 0.8228 - accuracy: 0.0000e+00 - val_loss: 0.8209 - val_accuracy: 0.0000e+00
Epoch 36/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8209 - accuracy: 0.0000e+00 - val_loss: 0.8189 - val_accuracy: 0.0000e+00
Epoch 37/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8189 - accuracy: 0.0000e+00 - val_loss: 0.8170 - val_accuracy: 0.0000e+00
Epoch 38/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8170 - accuracy: 0.0000e+00 - val_loss: 0.8151 - val_accuracy: 0.0000e+00
Epoch 39/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8151 - accuracy: 0.0000e+00 - val_loss: 0.8132 - val_accuracy: 0.0000e+00
Epoch 40/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8132 - accuracy: 0.0000e+00 - val_loss: 0.8113 - val_accuracy: 0.0000e+00
Epoch 41/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8113 - accuracy: 0.0000e+00 - val_loss: 0.8094 - val_accuracy: 0.0000e+00
Epoch 42/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8094 - accuracy: 0.0000e+00 - val_loss: 0.8075 - val_accuracy: 0.0000e+00
Epoch 43/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8075 - accuracy: 0.0000e+00 - val_loss: 0.8056 - val_accuracy: 0.0000e+00
Epoch 44/100
1/1 [==============================] - 0s 19ms/step - loss: 0.8056 - accuracy: 0.0000e+00 - val_loss: 0.8037 - val_accuracy: 0.0000e+00
Epoch 45/100
1/1 [==============================] - 0s 21ms/step - loss: 0.8037 - accuracy: 0.0000e+00 - val_loss: 0.8018 - val_accuracy: 0.0000e+00
Epoch 46/100
1/1 [==============================] - 0s 20ms/step - loss: 0.8018 - accuracy: 0.0000e+00 - val_loss: 0.7999 - val_accuracy: 0.0000e+00
Epoch 47/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7999 - accuracy: 0.0000e+00 - val_loss: 0.7980 - val_accuracy: 0.0000e+00
Epoch 48/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7980 - accuracy: 0.0000e+00 - val_loss: 0.7961 - val_accuracy: 0.0000e+00
Epoch 49/100
1/1 [==============================] - 4s 4s/step - loss: 0.7961 - accuracy: 0.0000e+00 - val_loss: 0.7942 - val_accuracy: 0.0000e+00
Epoch 50/100
1/1 [==============================] - 0s 25ms/step - loss: 0.7942 - accuracy: 0.0000e+00 - val_loss: 0.7924 - val_accuracy: 0.0000e+00
Epoch 51/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7924 - accuracy: 0.0000e+00 - val_loss: 0.7905 - val_accuracy: 0.0000e+00
Epoch 52/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7905 - accuracy: 0.0000e+00 - val_loss: 0.7886 - val_accuracy: 0.0000e+00
Epoch 53/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7886 - accuracy: 0.0000e+00 - val_loss: 0.7868 - val_accuracy: 0.0000e+00
Epoch 54/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7868 - accuracy: 0.0000e+00 - val_loss: 0.7849 - val_accuracy: 0.0000e+00
Epoch 55/100
1/1 [==============================] - 0s 22ms/step - loss: 0.7849 - accuracy: 0.0000e+00 - val_loss: 0.7830 - val_accuracy: 0.0000e+00
Epoch 56/100
1/1 [==============================] - 0s 17ms/step - loss: 0.7830 - accuracy: 0.0000e+00 - val_loss: 0.7812 - val_accuracy: 0.0000e+00
Epoch 57/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7812 - accuracy: 0.0000e+00 - val_loss: 0.7793 - val_accuracy: 0.0000e+00
Epoch 58/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7793 - accuracy: 0.0000e+00 - val_loss: 0.7775 - val_accuracy: 0.0000e+00
Epoch 59/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7775 - accuracy: 0.0000e+00 - val_loss: 0.7756 - val_accuracy: 0.0000e+00
Epoch 60/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7756 - accuracy: 0.0000e+00 - val_loss: 0.7738 - val_accuracy: 0.0000e+00
Epoch 61/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7738 - accuracy: 0.0000e+00 - val_loss: 0.7720 - val_accuracy: 0.0000e+00
Epoch 62/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7720 - accuracy: 0.0000e+00 - val_loss: 0.7701 - val_accuracy: 0.0000e+00
Epoch 63/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7701 - accuracy: 0.0000e+00 - val_loss: 0.7683 - val_accuracy: 0.0000e+00
Epoch 64/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7683 - accuracy: 0.0000e+00 - val_loss: 0.7665 - val_accuracy: 0.0000e+00
Epoch 65/100
1/1 [==============================] - 0s 28ms/step - loss: 0.7665 - accuracy: 0.0000e+00 - val_loss: 0.7647 - val_accuracy: 0.0000e+00
Epoch 66/100
1/1 [==============================] - 0s 22ms/step - loss: 0.7647 - accuracy: 0.0000e+00 - val_loss: 0.7629 - val_accuracy: 0.0000e+00
Epoch 67/100
1/1 [==============================] - 0s 21ms/step - loss: 0.7629 - accuracy: 0.0000e+00 - val_loss: 0.7610 - val_accuracy: 0.0000e+00
Epoch 68/100
1/1 [==============================] - 0s 17ms/step - loss: 0.7610 - accuracy: 0.0000e+00 - val_loss: 0.7592 - val_accuracy: 0.0000e+00
Epoch 69/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7592 - accuracy: 0.0000e+00 - val_loss: 0.7574 - val_accuracy: 0.0000e+00
Epoch 70/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7574 - accuracy: 0.0000e+00 - val_loss: 0.7556 - val_accuracy: 0.0000e+00
Epoch 71/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7556 - accuracy: 0.0000e+00 - val_loss: 0.7538 - val_accuracy: 0.0000e+00
Epoch 72/100
1/1 [==============================] - 0s 27ms/step - loss: 0.7538 - accuracy: 0.0000e+00 - val_loss: 0.7520 - val_accuracy: 0.0000e+00
Epoch 73/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7520 - accuracy: 0.0000e+00 - val_loss: 0.7502 - val_accuracy: 0.0000e+00
Epoch 74/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7502 - accuracy: 0.0000e+00 - val_loss: 0.7485 - val_accuracy: 0.0000e+00
Epoch 75/100
1/1 [==============================] - 0s 17ms/step - loss: 0.7485 - accuracy: 0.0000e+00 - val_loss: 0.7467 - val_accuracy: 0.0000e+00
Epoch 76/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7467 - accuracy: 0.0000e+00 - val_loss: 0.7449 - val_accuracy: 0.0000e+00
Epoch 77/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7449 - accuracy: 0.0000e+00 - val_loss: 0.7431 - val_accuracy: 0.0000e+00
Epoch 78/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7431 - accuracy: 0.0000e+00 - val_loss: 0.7414 - val_accuracy: 0.0000e+00
Epoch 79/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7414 - accuracy: 0.0000e+00 - val_loss: 0.7396 - val_accuracy: 0.0000e+00
Epoch 80/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7396 - accuracy: 0.0000e+00 - val_loss: 0.7378 - val_accuracy: 0.0000e+00
Epoch 81/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7378 - accuracy: 0.0000e+00 - val_loss: 0.7361 - val_accuracy: 0.0000e+00
Epoch 82/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7361 - accuracy: 0.0000e+00 - val_loss: 0.7343 - val_accuracy: 0.0000e+00
Epoch 83/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7343 - accuracy: 0.0000e+00 - val_loss: 0.7326 - val_accuracy: 0.0000e+00
Epoch 84/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7326 - accuracy: 0.0000e+00 - val_loss: 0.7309 - val_accuracy: 0.0000e+00
Epoch 85/100
1/1 [==============================] - 0s 21ms/step - loss: 0.7309 - accuracy: 0.0000e+00 - val_loss: 0.7291 - val_accuracy: 0.0000e+00
Epoch 86/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7291 - accuracy: 0.0000e+00 - val_loss: 0.7274 - val_accuracy: 0.0000e+00
Epoch 87/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7274 - accuracy: 0.0000e+00 - val_loss: 0.7257 - val_accuracy: 0.0000e+00
Epoch 88/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7257 - accuracy: 0.0000e+00 - val_loss: 0.7239 - val_accuracy: 0.0000e+00
Epoch 89/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7239 - accuracy: 0.0000e+00 - val_loss: 0.7222 - val_accuracy: 0.0000e+00
Epoch 90/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7222 - accuracy: 0.0000e+00 - val_loss: 0.7205 - val_accuracy: 0.0000e+00
Epoch 91/100
1/1 [==============================] - 0s 19ms/step - loss: 0.7205 - accuracy: 0.0000e+00 - val_loss: 0.7188 - val_accuracy: 0.0000e+00
Epoch 92/100
1/1 [==============================] - 0s 21ms/step - loss: 0.7188 - accuracy: 0.0000e+00 - val_loss: 0.7171 - val_accuracy: 0.0000e+00
Epoch 93/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7171 - accuracy: 0.0000e+00 - val_loss: 0.7154 - val_accuracy: 0.0000e+00
Epoch 94/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7154 - accuracy: 0.0000e+00 - val_loss: 0.7137 - val_accuracy: 0.0000e+00
Epoch 95/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7137 - accuracy: 0.0000e+00 - val_loss: 0.7120 - val_accuracy: 0.0000e+00
Epoch 96/100
1/1 [==============================] - 4s 4s/step - loss: 0.7120 - accuracy: 0.0000e+00 - val_loss: 0.7104 - val_accuracy: 0.0000e+00
Epoch 97/100
1/1 [==============================] - 0s 20ms/step - loss: 0.7104 - accuracy: 0.0000e+00 - val_loss: 0.7087 - val_accuracy: 0.0000e+00
Epoch 98/100
1/1 [==============================] - 0s 24ms/step - loss: 0.7087 - accuracy: 0.0000e+00 - val_loss: 0.7070 - val_accuracy: 0.0000e+00
Epoch 99/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7070 - accuracy: 0.0000e+00 - val_loss: 0.7053 - val_accuracy: 0.0000e+00
Epoch 100/100
1/1 [==============================] - 0s 18ms/step - loss: 0.7053 - accuracy: 0.0000e+00 - val_loss: 0.7037 - val_accuracy: 0.0000e+00
```

### LSTM logs
```
Python 3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)] on win32
runfile('C:/Users/31619/PycharmProjects/nn-drum-midi-synthesis/stackoverflow_2.py', wdir='C:/Users/31619/PycharmProjects/nn-drum-midi-synthesis')
2021-06-28 18:24:32.143194: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-28 18:24:35.406725: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
2021-06-28 18:24:35.430977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-06-28 18:24:35.431368: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-28 18:24:35.436952: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-28 18:24:35.437214: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-06-28 18:24:35.440505: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
2021-06-28 18:24:35.441703: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
2021-06-28 18:24:35.443734: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
2021-06-28 18:24:35.445838: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
2021-06-28 18:24:35.446559: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-28 18:24:35.446818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-28 18:24:35.447414: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-28 18:24:35.448554: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3070 computeCapability: 8.6
coreClock: 1.725GHz coreCount: 46 deviceMemorySize: 8.00GiB deviceMemoryBandwidth: 417.29GiB/s
2021-06-28 18:24:35.449097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-28 18:24:36.313003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-28 18:24:36.313208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-06-28 18:24:36.313326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-06-28 18:24:36.313617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1714 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-06-28 18:24:36.882377: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
Epoch 1/100
2021-06-28 18:24:39.850036: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-28 18:24:44.822070: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-06-28 18:24:44.822358: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
1/1 [==============================] - 8s 8s/step - loss: 1.0950 - accuracy: 1.0000 - val_loss: 1.0895 - val_accuracy: 1.0000
Epoch 2/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0895 - accuracy: 1.0000 - val_loss: 1.0856 - val_accuracy: 1.0000
Epoch 3/100
1/1 [==============================] - 0s 26ms/step - loss: 1.0856 - accuracy: 1.0000 - val_loss: 1.0822 - val_accuracy: 1.0000
Epoch 4/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0822 - accuracy: 1.0000 - val_loss: 1.0793 - val_accuracy: 1.0000
Epoch 5/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0793 - accuracy: 1.0000 - val_loss: 1.0766 - val_accuracy: 1.0000
Epoch 6/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0766 - accuracy: 1.0000 - val_loss: 1.0741 - val_accuracy: 1.0000
Epoch 7/100
1/1 [==============================] - 0s 23ms/step - loss: 1.0741 - accuracy: 1.0000 - val_loss: 1.0717 - val_accuracy: 1.0000
Epoch 8/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0717 - accuracy: 1.0000 - val_loss: 1.0694 - val_accuracy: 1.0000
Epoch 9/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0694 - accuracy: 1.0000 - val_loss: 1.0673 - val_accuracy: 1.0000
Epoch 10/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0673 - accuracy: 1.0000 - val_loss: 1.0652 - val_accuracy: 1.0000
Epoch 11/100
1/1 [==============================] - 0s 27ms/step - loss: 1.0652 - accuracy: 1.0000 - val_loss: 1.0631 - val_accuracy: 1.0000
Epoch 12/100
1/1 [==============================] - 0s 24ms/step - loss: 1.0631 - accuracy: 1.0000 - val_loss: 1.0611 - val_accuracy: 1.0000
Epoch 13/100
1/1 [==============================] - 0s 26ms/step - loss: 1.0611 - accuracy: 1.0000 - val_loss: 1.0591 - val_accuracy: 1.0000
Epoch 14/100
1/1 [==============================] - 0s 26ms/step - loss: 1.0591 - accuracy: 1.0000 - val_loss: 1.0571 - val_accuracy: 1.0000
Epoch 15/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0571 - accuracy: 1.0000 - val_loss: 1.0552 - val_accuracy: 1.0000
Epoch 16/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0552 - accuracy: 1.0000 - val_loss: 1.0533 - val_accuracy: 1.0000
Epoch 17/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0533 - accuracy: 1.0000 - val_loss: 1.0515 - val_accuracy: 1.0000
Epoch 18/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0515 - accuracy: 1.0000 - val_loss: 1.0497 - val_accuracy: 1.0000
Epoch 19/100
1/1 [==============================] - 0s 19ms/step - loss: 1.0497 - accuracy: 1.0000 - val_loss: 1.0479 - val_accuracy: 1.0000
Epoch 20/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0479 - accuracy: 1.0000 - val_loss: 1.0460 - val_accuracy: 1.0000
Epoch 21/100
1/1 [==============================] - 0s 26ms/step - loss: 1.0460 - accuracy: 1.0000 - val_loss: 1.0443 - val_accuracy: 1.0000
Epoch 22/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0443 - accuracy: 1.0000 - val_loss: 1.0425 - val_accuracy: 1.0000
Epoch 23/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0425 - accuracy: 1.0000 - val_loss: 1.0407 - val_accuracy: 1.0000
Epoch 24/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0407 - accuracy: 1.0000 - val_loss: 1.0390 - val_accuracy: 1.0000
Epoch 25/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0390 - accuracy: 1.0000 - val_loss: 1.0372 - val_accuracy: 1.0000
Epoch 26/100
1/1 [==============================] - 0s 25ms/step - loss: 1.0372 - accuracy: 1.0000 - val_loss: 1.0355 - val_accuracy: 1.0000
Epoch 27/100
1/1 [==============================] - 2s 2s/step - loss: 1.0355 - accuracy: 1.0000 - val_loss: 1.0338 - val_accuracy: 1.0000
Epoch 28/100
1/1 [==============================] - 0s 25ms/step - loss: 1.0338 - accuracy: 1.0000 - val_loss: 1.0321 - val_accuracy: 1.0000
Epoch 29/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0321 - accuracy: 1.0000 - val_loss: 1.0304 - val_accuracy: 1.0000
Epoch 30/100
1/1 [==============================] - 0s 19ms/step - loss: 1.0304 - accuracy: 1.0000 - val_loss: 1.0287 - val_accuracy: 1.0000
Epoch 31/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0287 - accuracy: 1.0000 - val_loss: 1.0270 - val_accuracy: 1.0000
Epoch 32/100
1/1 [==============================] - 0s 25ms/step - loss: 1.0270 - accuracy: 1.0000 - val_loss: 1.0252 - val_accuracy: 1.0000
Epoch 33/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0252 - accuracy: 1.0000 - val_loss: 1.0236 - val_accuracy: 1.0000
Epoch 34/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0236 - accuracy: 1.0000 - val_loss: 1.0219 - val_accuracy: 1.0000
Epoch 35/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0219 - accuracy: 1.0000 - val_loss: 1.0202 - val_accuracy: 1.0000
Epoch 36/100
1/1 [==============================] - 0s 24ms/step - loss: 1.0202 - accuracy: 1.0000 - val_loss: 1.0185 - val_accuracy: 1.0000
Epoch 37/100
1/1 [==============================] - 0s 21ms/step - loss: 1.0185 - accuracy: 1.0000 - val_loss: 1.0169 - val_accuracy: 1.0000
Epoch 38/100
1/1 [==============================] - 0s 25ms/step - loss: 1.0169 - accuracy: 1.0000 - val_loss: 1.0152 - val_accuracy: 1.0000
Epoch 39/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0152 - accuracy: 1.0000 - val_loss: 1.0136 - val_accuracy: 1.0000
Epoch 40/100
1/1 [==============================] - 0s 22ms/step - loss: 1.0136 - accuracy: 1.0000 - val_loss: 1.0119 - val_accuracy: 1.0000
Epoch 41/100
1/1 [==============================] - 0s 23ms/step - loss: 1.0119 - accuracy: 1.0000 - val_loss: 1.0103 - val_accuracy: 1.0000
Epoch 42/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0103 - accuracy: 1.0000 - val_loss: 1.0086 - val_accuracy: 1.0000
Epoch 43/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0086 - accuracy: 1.0000 - val_loss: 1.0070 - val_accuracy: 1.0000
Epoch 44/100
1/1 [==============================] - 0s 26ms/step - loss: 1.0070 - accuracy: 1.0000 - val_loss: 1.0053 - val_accuracy: 1.0000
Epoch 45/100
1/1 [==============================] - 0s 26ms/step - loss: 1.0053 - accuracy: 1.0000 - val_loss: 1.0036 - val_accuracy: 1.0000
Epoch 46/100
1/1 [==============================] - 0s 20ms/step - loss: 1.0036 - accuracy: 1.0000 - val_loss: 1.0020 - val_accuracy: 1.0000
Epoch 47/100
1/1 [==============================] - 0s 24ms/step - loss: 1.0020 - accuracy: 1.0000 - val_loss: 1.0003 - val_accuracy: 1.0000
Epoch 48/100
1/1 [==============================] - 0s 24ms/step - loss: 1.0003 - accuracy: 1.0000 - val_loss: 0.9986 - val_accuracy: 1.0000
Epoch 49/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9986 - accuracy: 1.0000 - val_loss: 0.9970 - val_accuracy: 1.0000
Epoch 50/100
1/1 [==============================] - 0s 20ms/step - loss: 0.9970 - accuracy: 1.0000 - val_loss: 0.9953 - val_accuracy: 1.0000
Epoch 51/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9953 - accuracy: 1.0000 - val_loss: 0.9936 - val_accuracy: 1.0000
Epoch 52/100
1/1 [==============================] - 0s 24ms/step - loss: 0.9936 - accuracy: 1.0000 - val_loss: 0.9919 - val_accuracy: 1.0000
Epoch 53/100
1/1 [==============================] - 0s 20ms/step - loss: 0.9919 - accuracy: 1.0000 - val_loss: 0.9903 - val_accuracy: 1.0000
Epoch 54/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9903 - accuracy: 1.0000 - val_loss: 0.9886 - val_accuracy: 1.0000
Epoch 55/100
1/1 [==============================] - 0s 23ms/step - loss: 0.9886 - accuracy: 1.0000 - val_loss: 0.9869 - val_accuracy: 1.0000
Epoch 56/100
1/1 [==============================] - 0s 23ms/step - loss: 0.9869 - accuracy: 1.0000 - val_loss: 0.9852 - val_accuracy: 1.0000
Epoch 57/100
1/1 [==============================] - 0s 25ms/step - loss: 0.9852 - accuracy: 1.0000 - val_loss: 0.9835 - val_accuracy: 1.0000
Epoch 58/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9835 - accuracy: 1.0000 - val_loss: 0.9818 - val_accuracy: 1.0000
Epoch 59/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9818 - accuracy: 1.0000 - val_loss: 0.9801 - val_accuracy: 1.0000
Epoch 60/100
1/1 [==============================] - 0s 20ms/step - loss: 0.9801 - accuracy: 1.0000 - val_loss: 0.9784 - val_accuracy: 1.0000
Epoch 61/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9784 - accuracy: 1.0000 - val_loss: 0.9766 - val_accuracy: 1.0000
Epoch 62/100
1/1 [==============================] - 0s 19ms/step - loss: 0.9766 - accuracy: 1.0000 - val_loss: 0.9749 - val_accuracy: 1.0000
Epoch 63/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9749 - accuracy: 1.0000 - val_loss: 0.9732 - val_accuracy: 1.0000
Epoch 64/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9732 - accuracy: 1.0000 - val_loss: 0.9715 - val_accuracy: 1.0000
Epoch 65/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9715 - accuracy: 1.0000 - val_loss: 0.9698 - val_accuracy: 1.0000
Epoch 66/100
1/1 [==============================] - 0s 28ms/step - loss: 0.9698 - accuracy: 1.0000 - val_loss: 0.9680 - val_accuracy: 1.0000
Epoch 67/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9680 - accuracy: 1.0000 - val_loss: 0.9663 - val_accuracy: 1.0000
Epoch 68/100
1/1 [==============================] - 0s 24ms/step - loss: 0.9663 - accuracy: 1.0000 - val_loss: 0.9646 - val_accuracy: 1.0000
Epoch 69/100
1/1 [==============================] - 0s 20ms/step - loss: 0.9646 - accuracy: 1.0000 - val_loss: 0.9628 - val_accuracy: 1.0000
Epoch 70/100
1/1 [==============================] - 2s 2s/step - loss: 0.9628 - accuracy: 1.0000 - val_loss: 0.9611 - val_accuracy: 1.0000
Epoch 71/100
1/1 [==============================] - 0s 23ms/step - loss: 0.9611 - accuracy: 1.0000 - val_loss: 0.9593 - val_accuracy: 1.0000
Epoch 72/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9593 - accuracy: 1.0000 - val_loss: 0.9576 - val_accuracy: 1.0000
Epoch 73/100
1/1 [==============================] - 0s 27ms/step - loss: 0.9576 - accuracy: 1.0000 - val_loss: 0.9558 - val_accuracy: 1.0000
Epoch 74/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9558 - accuracy: 1.0000 - val_loss: 0.9541 - val_accuracy: 1.0000
Epoch 75/100
1/1 [==============================] - 0s 25ms/step - loss: 0.9541 - accuracy: 1.0000 - val_loss: 0.9523 - val_accuracy: 1.0000
Epoch 76/100
1/1 [==============================] - 0s 19ms/step - loss: 0.9523 - accuracy: 1.0000 - val_loss: 0.9506 - val_accuracy: 1.0000
Epoch 77/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9506 - accuracy: 1.0000 - val_loss: 0.9488 - val_accuracy: 1.0000
Epoch 78/100
1/1 [==============================] - 0s 19ms/step - loss: 0.9488 - accuracy: 1.0000 - val_loss: 0.9471 - val_accuracy: 1.0000
Epoch 79/100
1/1 [==============================] - 0s 34ms/step - loss: 0.9471 - accuracy: 1.0000 - val_loss: 0.9453 - val_accuracy: 1.0000
Epoch 80/100
1/1 [==============================] - 0s 24ms/step - loss: 0.9453 - accuracy: 1.0000 - val_loss: 0.9436 - val_accuracy: 1.0000
Epoch 81/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9436 - accuracy: 1.0000 - val_loss: 0.9418 - val_accuracy: 1.0000
Epoch 82/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9418 - accuracy: 1.0000 - val_loss: 0.9401 - val_accuracy: 1.0000
Epoch 83/100
1/1 [==============================] - 0s 29ms/step - loss: 0.9401 - accuracy: 1.0000 - val_loss: 0.9383 - val_accuracy: 1.0000
Epoch 84/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9383 - accuracy: 1.0000 - val_loss: 0.9365 - val_accuracy: 1.0000
Epoch 85/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9365 - accuracy: 1.0000 - val_loss: 0.9348 - val_accuracy: 1.0000
Epoch 86/100
1/1 [==============================] - 0s 20ms/step - loss: 0.9348 - accuracy: 1.0000 - val_loss: 0.9330 - val_accuracy: 1.0000
Epoch 87/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9330 - accuracy: 1.0000 - val_loss: 0.9313 - val_accuracy: 1.0000
Epoch 88/100
1/1 [==============================] - 0s 25ms/step - loss: 0.9313 - accuracy: 1.0000 - val_loss: 0.9295 - val_accuracy: 1.0000
Epoch 89/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9295 - accuracy: 1.0000 - val_loss: 0.9277 - val_accuracy: 1.0000
Epoch 90/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9277 - accuracy: 1.0000 - val_loss: 0.9260 - val_accuracy: 1.0000
Epoch 91/100
1/1 [==============================] - 0s 23ms/step - loss: 0.9260 - accuracy: 1.0000 - val_loss: 0.9243 - val_accuracy: 1.0000
Epoch 92/100
1/1 [==============================] - 0s 25ms/step - loss: 0.9243 - accuracy: 1.0000 - val_loss: 0.9225 - val_accuracy: 1.0000
Epoch 93/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9225 - accuracy: 1.0000 - val_loss: 0.9207 - val_accuracy: 1.0000
Epoch 94/100
1/1 [==============================] - 0s 23ms/step - loss: 0.9207 - accuracy: 1.0000 - val_loss: 0.9189 - val_accuracy: 1.0000
Epoch 95/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9189 - accuracy: 1.0000 - val_loss: 0.9171 - val_accuracy: 1.0000
Epoch 96/100
1/1 [==============================] - 0s 21ms/step - loss: 0.9171 - accuracy: 1.0000 - val_loss: 0.9154 - val_accuracy: 1.0000
Epoch 97/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9154 - accuracy: 1.0000 - val_loss: 0.9137 - val_accuracy: 1.0000
Epoch 98/100
1/1 [==============================] - 0s 26ms/step - loss: 0.9137 - accuracy: 1.0000 - val_loss: 0.9119 - val_accuracy: 1.0000
Epoch 99/100
1/1 [==============================] - 0s 20ms/step - loss: 0.9119 - accuracy: 1.0000 - val_loss: 0.9102 - val_accuracy: 1.0000
Epoch 100/100
1/1 [==============================] - 0s 22ms/step - loss: 0.9102 - accuracy: 1.0000 - val_loss: 0.9084 - val_accuracy: 1.0000

```"
50487,OSError: [Errno 9] Bad file descriptor raised on program exit,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): `v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0`
- Python version: `Python 3.8.5`
- CUDA/cuDNN version: `11.2` / `8.1.0.77-1`
- GPU model and memory: P100

**Describe the current behavior**

When using `MirroredStrategy` as a context manager, Python raises an ignored exception on program exit:

```
Exception ignored in: <function Pool.__del__ at 0x7f21f942e4c0>
Traceback (most recent call last):
  File ""/root/miniconda3/lib/python3.8/multiprocessing/pool.py"", line 268, in __del__
    self._change_notifier.put(None)
  File ""/root/miniconda3/lib/python3.8/multiprocessing/queues.py"", line 368, in put
    self._writer.send_bytes(obj)
  File ""/root/miniconda3/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File ""/root/miniconda3/lib/python3.8/multiprocessing/connection.py"", line 411, in _send_bytes
    self._send(header + buf)
  File ""/root/miniconda3/lib/python3.8/multiprocessing/connection.py"", line 368, in _send
    n = write(self._handle, buf)
OSError: [Errno 9] Bad file descriptor
```

**Describe the expected behavior**

Python exits without the aforementioned exception. (In my testing, there is no such exception raised on TensorFlow 2.4.0, so this seems new in TensorFlow 2.5.0.)

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): No

**Standalone code to reproduce the issue**

```py
import tensorflow


def f():
    strategy = tensorflow.distribute.MirroredStrategy()
    with strategy.scope():
        tensorflow.keras.layers.Conv2D(64, (3, 3), activation=""relu"", padding=""same"")(
            tensorflow.keras.layers.Input(shape=(88, 88, 3))
        )


f()
```

Removing the `strategy.scope()` causes the program to exit without the ignored exception, as does removing the function definition (i.e., getting rid of `def f()` and `f()`, and invoking at the top level).
"
50486,Label_image demo always gives same outputs ,"@tensorflow/micro

Hi all !

While testing our models with basic [label_image](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/examples/label_image) example, we faced with strange results.

We tried _label_image_ with 2 different YOLO models with their corresponding label.txt. However regardless the input images and labels, the test output is always in the same order as [2 3 0 1]. Finally we also gave the Grace Hopper image from official source and still we faced with same issue. You can see the outputs in the image below.

Normally our models are working smoothly on PC. But, it is important for us to test them in our iMX6 device with this basic sample. Do you have any idea or solution about this problem ?
Thank you in advance.

![image](https://user-images.githubusercontent.com/56031118/123631875-8b6dac00-d817-11eb-8ba0-17047e522da4.png)




"
50484,"The TfLite ""split"" kernel fails with kTfLiteInt64 input tensor, even though ""split_v"" and ""concatenate"" work","**System information**
- OS Platform and Distribution: MacOS
- TensorFlow: Converter installed from binary, Interpreter built from source.
- TensorFlow version: 2.5.0
- Python version: 3.8

**Describe the current behavior**
We run the TF code shown below.
Then we run the resulting TF model through the TF 2.5.0 tflite Converter.
Then we run the resulting tflite model in the TfLite 2.5.0 Interpreter.

We get an error about an unsupported input type in `split::Prepare` and (if we ignore that) an error about an unsupported input type in `split::Eval`.

Perhaps due to operations in the 1.15 TfLite converter, we did not encounter this issue when using TF1.15.

**Describe the expected behavior**

The `split` kernel should operate correctly on kTfLiteInt64 input tensors.

The `split_v` kernel allows this, and uses the exact same underlying `reference_ops::Split` to do so!

Our process would be greatly eased if we did not need to modify our C++ build process for the Interpreter. Specifically, SELECT_TF_OPS generates an excessively large binary, and the AAR build steps are not appropriate for our product.

The use of a custom op would require extraordinary upgrades of third-party code. 

- Do you want to contribute a PR? If necessary, sure I will! I'll even add a test for this!

- Briefly describe your candidate solution(if contributing):
Add kTfLiteInt64 to the assertion in `split::Prepare`, and add the obvious lines in `split::Eval` (see, for example, `split_v::Eval`).
I have added this change locally to the source code without any _obvious_ issues.

**Standalone code to reproduce the issue**
```
def test_split_export():
    @tf.function
    def test_fn(inp: tf.Tensor) -> tf.Tensor:
        return tf.concat(tf.split(inp, 3, axis=1), axis=1)

    concrete_f = test_fn.get_concrete_function(tf.TensorSpec((None, 3), tf.int64))
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])
    tflite_model = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()
```

"
50483,Is there an example or a document for golang or java calling tfs by grpc ?,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): No

"
50482,tf dataset builder issues,"config = tfds.download.DownloadConfig(verify_ssl=False)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: __init__() got an unexpected keyword argument 'verify_ssl'
>>> config = tfds.download.DownloadConfig()
>>> builder.download_and_prepare(download_config=config)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 262, in download_and_prepare
    download_config=download_config)
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/dataset_builder.py"", line 662, in _make_download_manager
    register_checksums=download_config.register_checksums,
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/api_utils.py"", line 52, in disallow_positional_args_dec
    return fn(*args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/download/download_manager.py"", line 177, in __init__
    self._sizes_checksums = checksums.get_all_sizes_checksums()
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/download/checksums.py"", line 129, in get_all_sizes_checksums
    data = _get_sizes_checksums(path)
  File ""/home/ubuntu/anaconda3/envs/test_gpu/lib/python3.7/site-packages/tensorflow_datasets/core/download/checksums.py"", line 119, in _get_sizes_checksums
    url, size, checksum = line.rsplit(' ', 2)
ValueError: not enough values to unpack (expected 3, got 1)



i am getting this error after trying to build dataset config file.

> # Name                    Version                   Build  Channel
_libgcc_mutex             0.1                        main
_openmp_mutex             4.5                       1_gnu
_tflow_select             2.1.0                       gpu    anaconda
absl-py                   0.13.0           py37h06a4308_0
aiohttp                   3.6.3            py37h7b6447c_0    anaconda
astor                     0.8.1                    py37_0    anaconda
astunparse                1.6.3                      py_0    anaconda
async-timeout             3.0.1                    py37_0    anaconda
attrs                     20.2.0                     py_0    anaconda
backcall                  0.2.0                    pypi_0    pypi
blas                      1.0                         mkl    anaconda
blinker                   1.4                      py37_0    anaconda
brotlipy                  0.7.0           py37h7b6447c_1000    anaconda
c-ares                    1.17.1               h27cfd23_0
ca-certificates           2020.10.14                    0    anaconda
cached-property           1.5.2                    pypi_0    pypi
cachetools                4.1.1                      py_0    anaconda
certifi                   2020.6.20                py37_0    anaconda
cffi                      1.14.3           py37he30daa8_0    anaconda
chardet                   3.0.4                 py37_1003    anaconda
click                     7.1.2                      py_0    anaconda
coverage                  5.3              py37h7b6447c_0    anaconda
cryptography              3.1.1            py37h1ba5d50_0    anaconda
cudatoolkit               10.1.243             h6bb024c_0    anaconda
cudnn                     7.6.5                cuda10.1_0    anaconda
cupti                     10.1.168                      0    anaconda
cycler                    0.10.0                   pypi_0    pypi
cython                    0.29.23                  pypi_0    pypi
decorator                 5.0.9                    pypi_0    pypi
dill                      0.3.4                    pypi_0    pypi
dm-tree                   0.1.6                    pypi_0    pypi
flatbuffers               1.12                     pypi_0    pypi
future                    0.18.2                   py37_1    anaconda
gast                      0.4.0                      py_0    anaconda
gin-config                0.4.0                    pypi_0    pypi
google-api-core           1.30.0                   pypi_0    pypi
google-api-python-client  2.10.0                   pypi_0    pypi
google-auth               1.32.0                   pypi_0    pypi
google-auth-httplib2      0.1.0                    pypi_0    pypi
google-auth-oauthlib      0.4.1                      py_2    anaconda
google-cloud-bigquery     2.20.0                   pypi_0    pypi
google-cloud-core         1.7.1                    pypi_0    pypi
google-crc32c             1.1.2                    pypi_0    pypi
google-pasta              0.2.0                      py_0    anaconda
google-resumable-media    1.3.1                    pypi_0    pypi
googleapis-common-protos  1.53.0                   pypi_0    pypi
grpcio                    1.34.1                   pypi_0    pypi
h5py                      3.1.0                    pypi_0    pypi
hdf5                      1.10.6               hb1b8bf9_0    anaconda
httplib2                  0.19.1                   pypi_0    pypi
idna                      2.10                       py_0    anaconda
importlib-metadata        2.0.0                      py_1    anaconda
importlib-resources       5.2.0                    pypi_0    pypi
intel-openmp              2020.2                      254    anaconda
ipython                   7.25.0                   pypi_0    pypi
ipython-genutils          0.2.0                    pypi_0    pypi
jedi                      0.18.0                   pypi_0    pypi
joblib                    1.0.1                    pypi_0    pypi
kaggle                    1.5.12                   pypi_0    pypi
keras-nightly             2.5.0.dev2021032900          pypi_0    pypi
keras-preprocessing       1.1.2              pyhd3eb1b0_0
kiwisolver                1.3.1                    pypi_0    pypi
ld_impl_linux-64          2.35.1               h7274673_9
libffi                    3.3                  he6710b0_2
libgcc-ng                 9.3.0               h5101ec6_17
libgfortran-ng            7.3.0                hdf63c60_0    anaconda
libgomp                   9.3.0               h5101ec6_17
libprotobuf               3.14.0               h8c45485_0
libstdcxx-ng              9.3.0               hd4cf53a_17
markdown                  3.3.2                    py37_0    anaconda
matplotlib                3.4.2                    pypi_0    pypi
matplotlib-inline         0.1.2                    pypi_0    pypi
mediapy                   1.0.2                    pypi_0    pypi
mkl                       2019.4                      243    anaconda
mkl-service               2.3.0            py37he904b0f_0    anaconda
mkl_fft                   1.2.0            py37h23d657b_0    anaconda
mkl_random                1.1.0            py37hd6b4f25_0    anaconda
multidict                 4.7.6            py37h7b6447c_1    anaconda
ncurses                   6.2                  he6710b0_1
numpy                     1.19.5                   pypi_0    pypi
numpy-base                1.19.1           py37hfa32c7d_0    anaconda
oauth2client              4.1.3                    pypi_0    pypi
oauthlib                  3.1.0                      py_0    anaconda
opencv-python-headless    4.5.2.54                 pypi_0    pypi
openssl                   1.1.1k               h27cfd23_0
opt-einsum                3.3.0                    pypi_0    pypi
opt_einsum                3.1.0                      py_0    anaconda
packaging                 20.9                     pypi_0    pypi
pandas                    1.2.5                    pypi_0    pypi
parso                     0.8.2                    pypi_0    pypi
pexpect                   4.8.0                    pypi_0    pypi
pickleshare               0.7.5                    pypi_0    pypi
pillow                    8.2.0                    pypi_0    pypi
pip                       21.1.2           py37h06a4308_0
portalocker               2.0.0                    pypi_0    pypi
promise                   2.3                      py37_0    anaconda
prompt-toolkit            3.0.19                   pypi_0    pypi
proto-plus                1.18.1                   pypi_0    pypi
protobuf                  3.14.0           py37h2531618_1
psutil                    5.8.0                    pypi_0    pypi
ptyprocess                0.7.0                    pypi_0    pypi
py-cpuinfo                8.0.0                    pypi_0    pypi
pyasn1                    0.4.8                      py_0    anaconda
pyasn1-modules            0.2.8                      py_0    anaconda
pycocotools               2.0.2                    pypi_0    pypi
pycparser                 2.20                       py_2    anaconda
pygments                  2.9.0                    pypi_0    pypi
pyjwt                     1.7.1                    py37_0    anaconda
pyopenssl                 19.1.0                     py_1    anaconda
pyparsing                 2.4.7                    pypi_0    pypi
pysocks                   1.7.1                    py37_1    anaconda
python                    3.7.10               h12debd9_4
python-dateutil           2.8.1                    pypi_0    pypi
python-flatbuffers        1.12               pyhd3eb1b0_0
python-slugify            5.0.2                    pypi_0    pypi
pytz                      2021.1                   pypi_0    pypi
pyyaml                    5.4.1                    pypi_0    pypi
readline                  8.1                  h27cfd23_0
requests                  2.24.0                     py_0    anaconda
requests-oauthlib         1.3.0                      py_0    anaconda
rsa                       4.6                        py_0    anaconda
sacrebleu                 1.5.1                    pypi_0    pypi
scikit-learn              0.24.2                   pypi_0    pypi
scipy                     1.6.2            py37h91f5cce_0
sentencepiece             0.1.96                   pypi_0    pypi
seqeval                   1.2.2                    pypi_0    pypi
setuptools                52.0.0           py37h06a4308_0
six                       1.15.0                     py_0    anaconda
sqlite                    3.36.0               hc218d9a_0
tensorboard               2.5.0                      py_0
tensorboard-data-server   0.6.1                    pypi_0    pypi
tensorboard-plugin-wit    1.6.0                      py_0    anaconda
tensorflow                2.5.0                    pypi_0    pypi
tensorflow-addons         0.13.0                   pypi_0    pypi
tensorflow-base           2.4.1           gpu_py37h29c2da4_0
tensorflow-datasets       4.3.0                    pypi_0    pypi
tensorflow-estimator      2.5.0              pyh7b7c402_0
tensorflow-gpu            2.4.1                h30adc30_0
tensorflow-hub            0.12.0                   pypi_0    pypi
tensorflow-metadata       1.1.0                    pypi_0    pypi
tensorflow-model-optimization 0.6.0                    pypi_0    pypi
termcolor                 1.1.0                    py37_1    anaconda
text-unidecode            1.3                      pypi_0    pypi
tf-models-official        2.5.0                    pypi_0    pypi
tf-slim                   1.1.0                    pypi_0    pypi
threadpoolctl             2.1.0                    pypi_0    pypi
tk                        8.6.10               hbc83047_0
tqdm                      4.61.1                   pypi_0    pypi
traitlets                 5.0.5                    pypi_0    pypi
typeguard                 2.12.1                   pypi_0    pypi
typing_extensions         3.7.4.3                    py_0    anaconda
uritemplate               3.0.1                    pypi_0    pypi
urllib3                   1.25.11                    py_0    anaconda
wcwidth                   0.2.5                    pypi_0    pypi
werkzeug                  1.0.1                      py_0    anaconda
wheel                     0.36.2             pyhd3eb1b0_0
wrapt                     1.12.1           py37h7b6447c_1    anaconda
xz                        5.2.5                h7b6447c_0
yaml                      0.2.5                h7b6447c_0    anaconda
yarl                      1.6.2            py37h7b6447c_0    anaconda
zipp                      3.3.1                      py_0    anaconda
zlib                      1.2.11               h7b6447c_3
"
50481,Wrong access modifiers in tf::ResourceHandle,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): all
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: no
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.6
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.1
- GPU model and memory: all

**Describe the current behavior**
During implementing an TensorFlow extension I stumbled upon this code line: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/resource_handle.h#L99

I assume it should be ```private``` instead of ```public```.

**Describe the expected behavior**
Class probably should prevent direct access to class members.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
N/A

**Other info / logs**
N/A"
50480,"int8 conversion error ""num_input_elements != num_output_elements"" using TensorFlow 1.15","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): 1.15.0; 2.3.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

####Paste your code here or provide a link to a custom end-to-end colab
I followed *post-training quantization)[https://www.tensorflow.org/lite/performance/post_training_quantization] to convert my pb file to an int8 tf lite model in TensorFlow 1.15.0. But I meet an error: 
**num_input_elements != num_output_elements (32400 != 874800)Node number 81 (RESHAPE) failed to prepare.**

##### conversion code
```
    converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
        ""model.pb"", 
        [""input_tensor""], 
        [""final_output""], 
        input_shapes=input_shape
        )
    converter.allow_custom_ops = True

    converter.optimizations = [tf.lite.Optimize.DEFAULT] # require representative dataset
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8 
    converter.inference_output_type = tf.int8 

    # necessary for calibartion
    converter.representative_dataset = representative_data_gen
    tflite_int8_model = converter.convert()
  
    with open(os.path.join(model_root_dir, ""convertedTFLite/int8_default.lite""), ""wb"") as f:
        f.write(tflite_int8_model)
```

##### representative dataset
```
def preprocess_image(src_image, input_tensor_size):
    output_image = src_image[:, :, (2, 1, 0)] # cvtColor
    output_image = cv2.resize(
        output_image,
        dsize=(input_tensor_size[0], input_tensor_size[1]),
        interpolation=cv2.INTER_LINEAR
    )
    output_image = output_image.astype('float32') / 255.0
    img_mean = np.array([0.5, 0.5, 0.5].reshape((1, 1, len([0.5, 0.5, 0.5])))
    img_std = np.array([0.5, 0.5, 0.5]).reshape((1, 1, len([0.5, 0.5, 0.5])))
    output_image -= img_mean
    output_image /= img_std
    
    return output_image
```

```
def representative_data_gen(): 
    DATASET_DIR = ""train_image""
    GT_DIR = ""classID""
    
    train_images = glob.glob(os.path.join(DATASET_DIR, ""*.png""))
    
    counter = 0
    input_tensor_size = [int(360), int(360)]
    
    for image_path in train_images:
        if os.path.exists(image_path):
            print(str(counter) + "" / "" + str(len(train_images)) + "": "" + image_path)
            input_image = cv2.imread(image_path)
            preprocessed_image = preprocess_image(input_image, input_tensor_size)
            image_in = preprocessed_image[np.newaxis, ...].astype(np.float32)
            counter +=1
            yield [image_in.astype(np.float32)]

        else:
            continue
```

### 3. Failure after conversion
When I convert the pb file to TFLite int8 model with TensorFlow 1.15.0, the process failed with the following error message:

```
RuntimeError: tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (32400 != 874800)Node number 81 (RESHAPE) failed to prepare.
tensorflow/lite/kernels/reshape.cc:66 num_input_elements != num_output_elements (32400 != 874800)Node number 81 (RESHAPE) failed to prepare.
```


For comparison, I converted the pb model to fp16 and uint8 precision without representative dataset, and the models were successfully converted. I tried to switch to TensorFlow 2.3.0 and convert the pb model to int8 precision with representative dataset, and the model was successfully converted. 
As a result, the error only happens when converting with TensorFlow 1.15.0. 
Is there any method to solve this error?

The attachment contains the pb file and the images for representative dataset.
[model_dataset.tar.gz](https://github.com/tensorflow/tensorflow/files/6722569/model_dataset.tar.gz)

Best regards,
Rahn
"
50479,BoringSSL external is including OpenSSL headers and bombing the build.,"
Wonderful things happening in release 2.4.2 compilation.
wonderful beautiful things indeed.

Bazel 4.1.0
Fedore 34
gcc version 11.1.1 20210428 (Red Hat 11.1.1-1) (GCC) 

So for some reason or another the 'external' code with anything involving bazel is giving me all the trouble.

Here this code is mixing boringssl with openssl files.


What compiler suite are the externals usually built on, btw ?  I seem to remember an argument involving how one suite or another was deviating from the standard when it expected includes from the cc file to propagate to the h file which was included before  the file defining the needed symols.

eg   numeric_precision was needed by 1.h  but was included in 1.cc

That kind of suggests something different.


> ERROR: /home/john/.cache/bazel/_bazel_john/e3cd31b22e9272d157c657db51cec2c6/external/boringssl/BUILD:147:11: Compiling src/ssl/tls_method.cc failed: (Exit 1): gcc failed: error executing command /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction-sections ... (remaining 40 argument(s) skipped)
> In file included from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/ex_data.h:181:52: error: macro ""CRYPTO_cleanup_all_ex_data"" passed 1 arguments, but takes just 0
>   181 | OPENSSL_EXPORT void CRYPTO_cleanup_all_ex_data(void);
>       |                                                    ^
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:200: note: macro ""CRYPTO_cleanup_all_ex_data"" defined here
>   200 | # define CRYPTO_cleanup_all_ex_data() while(0) continue
>       | 
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:120:41: error: macro ""CRYPTO_num_locks"" passed 1 arguments, but takes just 0
>   120 | OPENSSL_EXPORT int CRYPTO_num_locks(void);
>       |                                         ^
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:212: note: macro ""CRYPTO_num_locks"" defined here
>   212 | #  define CRYPTO_num_locks()            (1)
>       | 
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:131:55: error: macro ""CRYPTO_get_locking_callback"" passed 1 arguments, but takes just 0
>   131 | OPENSSL_EXPORT void (*CRYPTO_get_locking_callback(void))(int mode, int lock_num,
>       |                                                       ^
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:214: note: macro ""CRYPTO_get_locking_callback"" defined here
>   214 | #  define CRYPTO_get_locking_callback()         (NULL)
>       | 
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:176:45: error: macro ""CRYPTO_get_dynlock_create_callback"" passed 1 arguments, but takes just 0
>   176 |     *CRYPTO_get_dynlock_create_callback(void))(const char *file, int line);
>       |                                             ^
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:251: note: macro ""CRYPTO_get_dynlock_create_callback"" defined here
>   251 | #  define CRYPTO_get_dynlock_create_callback()          (NULL)
>       | 
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:179:60: error: macro ""CRYPTO_get_dynlock_lock_callback"" passed 1 arguments, but takes just 0
>   179 | OPENSSL_EXPORT void (*CRYPTO_get_dynlock_lock_callback(void))(
>       |                                                            ^
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:252: note: macro ""CRYPTO_get_dynlock_lock_callback"" defined here
>   252 | #  define CRYPTO_get_dynlock_lock_callback()            (NULL)
>       | 
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:183:63: error: macro ""CRYPTO_get_dynlock_destroy_callback"" passed 1 arguments, but takes just 0
>   183 | OPENSSL_EXPORT void (*CRYPTO_get_dynlock_destroy_callback(void))(
>       |                                                               ^
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:253: note: macro ""CRYPTO_get_dynlock_destroy_callback"" defined here
>   253 | #  define CRYPTO_get_dynlock_destroy_callback()         (NULL)
>       | 
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:318:13: error: conflicting declaration 'typedef int CRYPTO_THREADID'
>   318 | typedef int CRYPTO_THREADID;
>       |             ^~~~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:231:3: note: previous declaration as 'typedef struct crypto_threadid_st CRYPTO_THREADID'
>   231 | } CRYPTO_THREADID;
>       |   ^~~~~~~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:388:29: error: conflicting declaration 'typedef struct ecdsa_sig_st ECDSA_SIG'
>   388 | typedef struct ecdsa_sig_st ECDSA_SIG;
>       |                             ^~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:22,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ec.h:1127:29: note: previous declaration as 'typedef struct ECDSA_SIG_st ECDSA_SIG'
>  1127 | typedef struct ECDSA_SIG_st ECDSA_SIG;
>       |                             ^~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:390:30: error: conflicting declaration 'typedef struct env_md_ctx_st EVP_MD_CTX'
>   390 | typedef struct env_md_ctx_st EVP_MD_CTX;
>       |                              ^~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:25,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ossl_typ.h:92:30: note: previous declaration as 'typedef struct evp_md_ctx_st EVP_MD_CTX'
>    92 | typedef struct evp_md_ctx_st EVP_MD_CTX;
>       |                              ^~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:391:26: error: conflicting declaration 'typedef struct env_md_st EVP_MD'
>   391 | typedef struct env_md_st EVP_MD;
>       |                          ^~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:25,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ossl_typ.h:91:26: note: previous declaration as 'typedef struct evp_md_st EVP_MD'
>    91 | typedef struct evp_md_st EVP_MD;
>       |                          ^~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:395:34: error: conflicting declaration 'typedef struct evp_encode_ctx_st EVP_ENCODE_CTX'
>   395 | typedef struct evp_encode_ctx_st EVP_ENCODE_CTX;
>       |                                  ^~~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:25,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ossl_typ.h:100:34: note: previous declaration as 'typedef struct evp_Encode_Ctx_st EVP_ENCODE_CTX'
>   100 | typedef struct evp_Encode_Ctx_st EVP_ENCODE_CTX;
>       |                                  ^~~~~~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:411:32: error: conflicting declaration 'typedef struct sha256_state_st SHA256_CTX'
>   411 | typedef struct sha256_state_st SHA256_CTX;
>       |                                ^~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:30,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/sha.h:56:3: note: previous declaration as 'typedef struct SHA256state_st SHA256_CTX'
>    56 | } SHA256_CTX;
>       |   ^~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:412:32: error: conflicting declaration 'typedef struct sha512_state_st SHA512_CTX'
>   412 | typedef struct sha512_state_st SHA512_CTX;
>       |                                ^~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:30,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/sha.h:103:3: note: previous declaration as 'typedef struct SHA512state_st SHA512_CTX'
>   103 | } SHA512_CTX;
>       |   ^~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:413:29: error: conflicting declaration 'typedef struct sha_state_st SHA_CTX'
>   413 | typedef struct sha_state_st SHA_CTX;
>       |                             ^~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/x509.h:30,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:20,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/sha.h:39:3: note: previous declaration as 'typedef struct SHAstate_st SHA_CTX'
>    39 | } SHA_CTX;
>       |   ^~~~~~~
> In file included from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/ex_data.h:181:21: error: variable or field 'CRYPTO_cleanup_all_ex_data' declared void
>   181 | OPENSSL_EXPORT void CRYPTO_cleanup_all_ex_data(void);
>       |                     ^~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/include/openssl/ex_data.h:184:13: error: conflicting declaration 'typedef int CRYPTO_EX_dup(CRYPTO_EX_DATA*, const CRYPTO_EX_DATA*, void**, int, long int, void*)'
>   184 | typedef int CRYPTO_EX_dup(CRYPTO_EX_DATA *to, const CRYPTO_EX_DATA *from,
>       |             ^~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:170:13: note: previous declaration as 'typedef int CRYPTO_EX_dup(CRYPTO_EX_DATA*, const CRYPTO_EX_DATA*, void*, int, long int, void*)'
>   170 | typedef int CRYPTO_EX_dup (CRYPTO_EX_DATA *to, const CRYPTO_EX_DATA *from,
>       |             ^~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/ex_data.h:194:8: error: redefinition of 'struct crypto_ex_data_st'
>   194 | struct crypto_ex_data_st {
>       |        ^~~~~~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:86:8: note: previous definition of 'struct crypto_ex_data_st'
>    86 | struct crypto_ex_data_st {
>       |        ^~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:123:16: error: declaration does not declare anything [-fpermissive]
>   123 | OPENSSL_EXPORT void CRYPTO_set_locking_callback(
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:127:16: error: declaration does not declare anything [-fpermissive]
>   127 | OPENSSL_EXPORT void CRYPTO_set_add_lock_callback(int (*func)(
>       |                ^~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:235:58: error: expected unqualified-id before numeric constant
>   235 | #  define CRYPTO_THREADID_set_callback(threadid_func)   (0)
>       |                                                          ^
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:235:58: error: expected ')' before numeric constant
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:143:16: error: declaration does not declare anything [-fpermissive]
>   143 | OPENSSL_EXPORT void CRYPTO_THREADID_set_numeric(CRYPTO_THREADID *id,
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:147:16: error: declaration does not declare anything [-fpermissive]
>   147 | OPENSSL_EXPORT void CRYPTO_THREADID_set_pointer(CRYPTO_THREADID *id, void *ptr);
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:150:16: error: declaration does not declare anything [-fpermissive]
>   150 | OPENSSL_EXPORT void CRYPTO_THREADID_current(CRYPTO_THREADID *id);
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:153:16: error: declaration does not declare anything [-fpermissive]
>   153 | OPENSSL_EXPORT void CRYPTO_set_id_callback(unsigned long (*func)(void));
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:158:3: error: conflicting declaration 'typedef struct CRYPTO_dynlock CRYPTO_dynlock'
>   158 | } CRYPTO_dynlock;
>       |   ^~~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:63:3: note: previous declaration as 'typedef struct CRYPTO_dynlock CRYPTO_dynlock'
>    63 | } CRYPTO_dynlock;
>       |   ^~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/../crypto/internal.h:114,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/thread.h:161:16: error: declaration does not declare anything [-fpermissive]
>   161 | OPENSSL_EXPORT void CRYPTO_set_dynlock_create_callback(
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:166:16: error: declaration does not declare anything [-fpermissive]
>   166 | OPENSSL_EXPORT void CRYPTO_set_dynlock_lock_callback(void (*dyn_lock_function)(
>       |                ^~~~
> external/boringssl/src/include/openssl/thread.h:170:16: error: declaration does not declare anything [-fpermissive]
>   170 | OPENSSL_EXPORT void CRYPTO_set_dynlock_destroy_callback(
>       |                ^~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/ssl/../crypto/internal.h:642:1: error: expected constructor, destructor, or type conversion before 'typedef'
>   642 | typedef struct {
>       | ^~~~~~~
> external/boringssl/src/ssl/../crypto/internal.h:648:3: error: 'CRYPTO_EX_DATA_CLASS' does not name a type; did you mean 'CRYPTO_EX_DATA_FUNCS'?
>   648 | } CRYPTO_EX_DATA_CLASS;
>       |   ^~~~~~~~~~~~~~~~~~~~
>       |   CRYPTO_EX_DATA_FUNCS
> external/boringssl/src/ssl/../crypto/internal.h:658:44: error: 'int CRYPTO_get_ex_new_index' redeclared as different kind of entity
>   658 | OPENSSL_EXPORT int CRYPTO_get_ex_new_index(CRYPTO_EX_DATA_CLASS *ex_data_class,
>       |                                            ^~~~~~~~~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:172:12: note: previous declaration 'int CRYPTO_get_ex_new_index(int, long int, void*, void (*)(void*, void*, CRYPTO_EX_DATA*, int, long int, void*), int (*)(CRYPTO_EX_DATA*, const CRYPTO_EX_DATA*, void*, int, long int, void*), void (*)(void*, void*, CRYPTO_EX_DATA*, int, long int, void*))'
>   172 | __owur int CRYPTO_get_ex_new_index(int class_index, long argl, void *argp,
>       |            ^~~~~~~~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/ssl/../crypto/internal.h:658:44: error: 'CRYPTO_EX_DATA_CLASS' was not declared in this scope; did you mean 'CRYPTO_EX_DATA_FUNCS'?
>   658 | OPENSSL_EXPORT int CRYPTO_get_ex_new_index(CRYPTO_EX_DATA_CLASS *ex_data_class,
>       |                                            ^~~~~~~~~~~~~~~~~~~~
>       |                                            CRYPTO_EX_DATA_FUNCS
> external/boringssl/src/ssl/../crypto/internal.h:658:66: error: 'ex_data_class' was not declared in this scope
>   658 | OPENSSL_EXPORT int CRYPTO_get_ex_new_index(CRYPTO_EX_DATA_CLASS *ex_data_class,
>       |                                                                  ^~~~~~~~~~~~~
> external/boringssl/src/ssl/../crypto/internal.h:659:44: error: expected primary-expression before 'int'
>   659 |                                            int *out_index, long argl,
>       |                                            ^~~
> external/boringssl/src/ssl/../crypto/internal.h:659:60: error: expected primary-expression before 'long'
>   659 |                                            int *out_index, long argl,
>       |                                                            ^~~~
> external/boringssl/src/ssl/../crypto/internal.h:660:44: error: expected primary-expression before 'void'
>   660 |                                            void *argp,
>       |                                            ^~~~
> external/boringssl/src/ssl/../crypto/internal.h:661:59: error: expected primary-expression before '*' token
>   661 |                                            CRYPTO_EX_free *free_func);
>       |                                                           ^
> external/boringssl/src/ssl/../crypto/internal.h:661:60: error: 'free_func' was not declared in this scope
>   661 |                                            CRYPTO_EX_free *free_func);
>       |                                                            ^~~~~~~~~
> external/boringssl/src/ssl/../crypto/internal.h:673:21: error: conflicting declaration of C function 'void CRYPTO_new_ex_data(CRYPTO_EX_DATA*)'
>   673 | OPENSSL_EXPORT void CRYPTO_new_ex_data(CRYPTO_EX_DATA *ad);
>       |                     ^~~~~~~~~~~~~~~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:182:5: note: previous declaration 'int CRYPTO_new_ex_data(int, void*, CRYPTO_EX_DATA*)'
>   182 | int CRYPTO_new_ex_data(int class_index, void *obj, CRYPTO_EX_DATA *ad);
>       |     ^~~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/ssl/../crypto/internal.h:677:21: error: variable or field 'CRYPTO_free_ex_data' declared void
>   677 | OPENSSL_EXPORT void CRYPTO_free_ex_data(CRYPTO_EX_DATA_CLASS *ex_data_class,
>       |                     ^~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/../crypto/internal.h:677:41: error: 'CRYPTO_EX_DATA_CLASS' was not declared in this scope; did you mean 'CRYPTO_EX_DATA_FUNCS'?
>   677 | OPENSSL_EXPORT void CRYPTO_free_ex_data(CRYPTO_EX_DATA_CLASS *ex_data_class,
>       |                                         ^~~~~~~~~~~~~~~~~~~~
>       |                                         CRYPTO_EX_DATA_FUNCS
> external/boringssl/src/ssl/../crypto/internal.h:677:63: error: 'ex_data_class' was not declared in this scope
>   677 | OPENSSL_EXPORT void CRYPTO_free_ex_data(CRYPTO_EX_DATA_CLASS *ex_data_class,
>       |                                                               ^~~~~~~~~~~~~
> external/boringssl/src/ssl/../crypto/internal.h:678:41: error: expected primary-expression before 'void'
>   678 |                                         void *obj, CRYPTO_EX_DATA *ad);
>       |                                         ^~~~
> external/boringssl/src/ssl/../crypto/internal.h:678:67: error: expected primary-expression before '*' token
>   678 |                                         void *obj, CRYPTO_EX_DATA *ad);
>       |                                                                   ^
> external/boringssl/src/ssl/../crypto/internal.h:678:68: error: 'ad' was not declared in this scope
>   678 |                                         void *obj, CRYPTO_EX_DATA *ad);
>       |                                                                    ^~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant
>   130 | #  define OPENSSL_FILE __FILE__
>       |                        ^~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: error: conflicting declaration of C function 'void* CRYPTO_malloc(size_t, int)'
>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:266:7: note: previous declaration 'void* CRYPTO_malloc(size_t, const char*, int)'
>   266 | void *CRYPTO_malloc(size_t num, const char *file, int line);
>       |       ^~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant
>   130 | #  define OPENSSL_FILE __FILE__
>       |                        ^~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:9: error: conflicting declaration of C function 'void CRYPTO_free(void*, int)'
>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:271:6: note: previous declaration 'void CRYPTO_free(void*, const char*, int)'
>   271 | void CRYPTO_free(void *ptr, const char *file, int line);
>       |      ^~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant
>   130 | #  define OPENSSL_FILE __FILE__
>       |                        ^~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:122:9: error: conflicting declaration of C function 'void* CRYPTO_realloc(void*, size_t, int)'
>   122 |         CRYPTO_realloc(addr, num, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:273:7: note: previous declaration 'void* CRYPTO_realloc(void*, size_t, const char*, int)'
>   273 | void *CRYPTO_realloc(void *addr, size_t num, const char *file, int line);
>       |       ^~~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant
>   130 | #  define OPENSSL_FILE __FILE__
>       |                        ^~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:132:9: error: conflicting declaration of C function 'char* CRYPTO_strdup(const char*, int)'
>   132 |         CRYPTO_strdup(str, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:269:7: note: previous declaration 'char* CRYPTO_strdup(const char*, const char*, int)'
>   269 | char *CRYPTO_strdup(const char *str, const char *file, int line);
>       |       ^~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant
>   130 | #  define OPENSSL_FILE __FILE__
>       |                        ^~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:134:9: error: conflicting declaration of C function 'char* CRYPTO_strndup(const char*, size_t, int)'
>   134 |         CRYPTO_strndup(str, n, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:270:7: note: previous declaration 'char* CRYPTO_strndup(const char*, size_t, const char*, int)'
>   270 | char *CRYPTO_strndup(const char *str, size_t s, const char *file, int line);
>       |       ^~~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:130:22: error: 'void* CRYPTO_memdup' redeclared as different kind of entity
>   130 |         CRYPTO_memdup((str), s, OPENSSL_FILE, OPENSSL_LINE)
>       |                      ^
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:268:7: note: previous declaration 'void* CRYPTO_memdup(const void*, size_t, const char*, int)'
>   268 | void *CRYPTO_memdup(const void *str, size_t siz, const char *file, int line);
>       |       ^~~~~~~~~~~~~
> external/boringssl/src/include/openssl/mem.h:137:22: error: expected primary-expression before 'const'
>   137 | OPENSSL_EXPORT void *OPENSSL_memdup(const void *data, size_t size);
>       |                      ^~~~~~~~~~~~~~
> external/boringssl/src/include/openssl/mem.h:137:22: error: expected ')' before 'const'
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:130:23: note: to match this '('
>   130 |         CRYPTO_memdup((str), s, OPENSSL_FILE, OPENSSL_LINE)
>       |                       ^
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected identifier before string constant
>   130 | #  define OPENSSL_FILE __FILE__
>       |                        ^~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/opensslconf.h:130:24: error: expected ',' or '...' before string constant
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:126:9: error: conflicting declaration of C function 'void CRYPTO_clear_free(void*, size_t, int)'
>   126 |         CRYPTO_clear_free(addr, num, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:272:6: note: previous declaration 'void CRYPTO_clear_free(void*, size_t, const char*, int)'
>   272 | void CRYPTO_clear_free(void *ptr, size_t num, const char *file, int line);
>       |      ^~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/internal.h:157,
>                  from external/boringssl/src/ssl/tls_method.cc:65:
> external/boringssl/src/include/openssl/mem.h: In static member function 'static void bssl::internal::DeleterImpl<char>::Free(char*)':
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:9: error: 'OPENSSL_free' was not declared in this scope
>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~
> external/boringssl/src/include/openssl/mem.h: In static member function 'static void bssl::internal::DeleterImpl<unsigned char>::Free(uint8_t*)':
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:9: error: 'OPENSSL_free' was not declared in this scope
>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~
> external/boringssl/src/ssl/internal.h: In function 'T* bssl::New(Args&& ...)':
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: error: there are no arguments to 'OPENSSL_malloc' that depend on a template parameter, so a declaration of 'OPENSSL_malloc' must be available [-fpermissive]
>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:192:13: note: in expansion of macro 'OPENSSL_malloc'
>   192 |   void *t = OPENSSL_malloc(sizeof(T));
>       |             ^~~~~~~~~~~~~~
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: note: (if you use '-fpermissive', G++ will accept your code, but allowing the use of an undeclared name is deprecated)
>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:192:13: note: in expansion of macro 'OPENSSL_malloc'
>   192 |   void *t = OPENSSL_malloc(sizeof(T));
>       |             ^~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:65:
> external/boringssl/src/ssl/internal.h:194:26: error: expected primary-expression before ',' token
>   194 |     OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);
>       |                          ^
> external/boringssl/src/ssl/internal.h:194:5: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]
>   194 |     OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);
>       |     ^~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h: In member function 'bool bssl::Array<T>::Init(size_t)':
> external/boringssl/src/ssl/internal.h:317:28: error: expected primary-expression before ',' token
>   317 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);
>       |                            ^
> external/boringssl/src/ssl/internal.h:317:30: error: 'ERR_R_OVERFLOW' was not declared in this scope; did you mean 'EOVERFLOW'?
>   317 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);
>       |                              ^~~~~~~~~~~~~~
>       |                              EOVERFLOW
> external/boringssl/src/ssl/internal.h:317:7: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]
>   317 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);
>       |       ^~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/internal.h:157,
>                  from external/boringssl/src/ssl/tls_method.cc:65:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:118:9: error: there are no arguments to 'OPENSSL_malloc' that depend on a template parameter, so a declaration of 'OPENSSL_malloc' must be available [-fpermissive]
>   118 |         CRYPTO_malloc(num, OPENSSL_FILE, OPENSSL_LINE)
>       |         ^~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:320:35: note: in expansion of macro 'OPENSSL_malloc'
>   320 |     data_ = reinterpret_cast<T *>(OPENSSL_malloc(new_size * sizeof(T)));
>       |                                   ^~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:65:
> external/boringssl/src/ssl/internal.h:322:28: error: expected primary-expression before ',' token
>   322 |       OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);
>       |                            ^
> external/boringssl/src/ssl/internal.h:322:7: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]
>   322 |       OPENSSL_PUT_ERROR(SSL, ERR_R_MALLOC_FAILURE);
>       |       ^~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h: In member function 'bool bssl::GrowableArray<T>::MaybeGrow()':
> external/boringssl/src/ssl/internal.h:423:28: error: expected primary-expression before ',' token
>   423 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);
>       |                            ^
> external/boringssl/src/ssl/internal.h:423:30: error: 'ERR_R_OVERFLOW' was not declared in this scope; did you mean 'EOVERFLOW'?
>   423 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);
>       |                              ^~~~~~~~~~~~~~
>       |                              EOVERFLOW
> external/boringssl/src/ssl/internal.h:423:7: error: there are no arguments to 'OPENSSL_PUT_ERROR' that depend on a template parameter, so a declaration of 'OPENSSL_PUT_ERROR' must be available [-fpermissive]
>   423 |       OPENSSL_PUT_ERROR(SSL, ERR_R_OVERFLOW);
>       |       ^~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h: At global scope:
> external/boringssl/src/ssl/internal.h:732:3: error: 'ScopedEVP_MD_CTX' does not name a type; did you mean 'ScopedEVP_AEAD_CTX'?
>   732 |   ScopedEVP_MD_CTX hash_;
>       |   ^~~~~~~~~~~~~~~~
>       |   ScopedEVP_AEAD_CTX
> external/boringssl/src/ssl/internal.h:1122:7: error: field 'body' has incomplete type 'CBS' {aka 'cbs_st'}
>  1122 |   CBS body;
>       |       ^~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:376:16: note: forward declaration of 'CBS' {aka 'struct cbs_st'}
>   376 | typedef struct cbs_st CBS;
>       |                ^~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:65:
> external/boringssl/src/ssl/internal.h:1125:7: error: field 'raw' has incomplete type 'CBS' {aka 'cbs_st'}
>  1125 |   CBS raw;
>       |       ^~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h:376:16: note: forward declaration of 'CBS' {aka 'struct cbs_st'}
>   376 | typedef struct cbs_st CBS;
>       |                ^~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:65:
> external/boringssl/src/ssl/internal.h:1885:11: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?
>  1885 |     const SSL_CLIENT_HELLO *client_hello, CBS *contents);
>       |           ^~~~~~~~~~~~~~~~
>       |           SSL_CLIENT_HELLO_CB
> external/boringssl/src/ssl/internal.h:1917:31: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?
>  1917 |                         const SSL_CLIENT_HELLO *client_hello);
>       |                               ^~~~~~~~~~~~~~~~
>       |                               SSL_CLIENT_HELLO_CB
> external/boringssl/src/ssl/internal.h:1955:44: error: 'SSL_CLIENT_HELLO' has not been declared
>  1955 | bool ssl_client_hello_init(const SSL *ssl, SSL_CLIENT_HELLO *out,
>       |                                            ^~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:1958:43: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?
>  1958 | bool ssl_client_hello_get_extension(const SSL_CLIENT_HELLO *client_hello,
>       |                                           ^~~~~~~~~~~~~~~~
>       |                                           SSL_CLIENT_HELLO_CB
> external/boringssl/src/ssl/internal.h:1962:11: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?
>  1962 |     const SSL_CLIENT_HELLO *client_hello, uint16_t id);
>       |           ^~~~~~~~~~~~~~~~
>       |           SSL_CLIENT_HELLO_CB
> external/boringssl/src/ssl/internal.h:2249:16: error: 'SSL_TICKET_KEY_NAME_LEN' was not declared in this scope
>  2249 |   uint8_t name[SSL_TICKET_KEY_NAME_LEN] = {0};
>       |                ^~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:2262:3: error: 'ssl_cert_compression_func_t' does not name a type
>  2262 |   ssl_cert_compression_func_t compress = nullptr;
>       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:2263:3: error: 'ssl_cert_decompression_func_t' does not name a type
>  2263 |   ssl_cert_decompression_func_t decompress = nullptr;
>       |   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/ssl/internal.h:2271:1: error: expected unqualified-id before 'namespace'
>  2271 | BSSL_NAMESPACE_BEGIN
>       | ^~~~~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:65:
> external/boringssl/src/ssl/internal.h:3173:59: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?
>  3173 |   ssl_select_cert_result_t (*select_certificate_cb)(const SSL_CLIENT_HELLO *) =
>       |                                                           ^~~~~~~~~~~~~~~~
>       |                                                           SSL_CLIENT_HELLO_CB
> external/boringssl/src/ssl/internal.h:3179:34: error: 'SSL_CLIENT_HELLO' does not name a type; did you mean 'SSL_CLIENT_HELLO_CB'?
>  3179 |   int (*dos_protection_cb)(const SSL_CLIENT_HELLO *) = nullptr;
>       |                                  ^~~~~~~~~~~~~~~~
>       |                                  SSL_CLIENT_HELLO_CB
> external/boringssl/src/ssl/internal.h:3093:30: error: 'SSL_DEFAULT_SESSION_TIMEOUT' was not declared in this scope
>  3093 |   uint32_t session_timeout = SSL_DEFAULT_SESSION_TIMEOUT;
>       |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:3097:38: error: 'SSL_DEFAULT_SESSION_PSK_DHE_TIMEOUT' was not declared in this scope
>  3097 |   uint32_t session_psk_dhe_timeout = SSL_DEFAULT_SESSION_PSK_DHE_TIMEOUT;
>       |                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:3377:9: error: 'SSL3_STATE' in namespace 'bssl' does not name a type
>  3377 |   bssl::SSL3_STATE *s3 = nullptr;   // TLS variables
>       |         ^~~~~~~~~~
> external/boringssl/src/ssl/internal.h:3378:9: error: 'DTLS1_STATE' in namespace 'bssl' does not name a type
>  3378 |   bssl::DTLS1_STATE *d1 = nullptr;  // DTLS variables
>       |         ^~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:3419:45: error: 'ssl_renegotiate_never' was not declared in this scope; did you mean 'ssl_renegotiate_mode_t'?
>  3419 |   ssl_renegotiate_mode_t renegotiate_mode = ssl_renegotiate_never;
>       |                                             ^~~~~~~~~~~~~~~~~~~~~
>       |                                             ssl_renegotiate_mode_t
> external/boringssl/src/ssl/internal.h:3495:22: error: 'SSL_DEFAULT_SESSION_TIMEOUT' was not declared in this scope
>  3495 |   uint32_t timeout = SSL_DEFAULT_SESSION_TIMEOUT;
>       |                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/internal.h:3499:27: error: 'SSL_DEFAULT_SESSION_TIMEOUT' was not declared in this scope
>  3499 |   uint32_t auth_timeout = SSL_DEFAULT_SESSION_TIMEOUT;
>       |                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc: In function 'void bssl::ssl3_on_handshake_complete(SSL*)':
> external/boringssl/src/ssl/tls_method.cc:79:12: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>    79 |   if (ssl->s3->hs_buf && ssl->s3->hs_buf->length == 0) {
>       |            ^~
> external/boringssl/src/ssl/tls_method.cc:79:31: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>    79 |   if (ssl->s3->hs_buf && ssl->s3->hs_buf->length == 0) {
>       |                               ^~
> external/boringssl/src/ssl/tls_method.cc:80:10: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>    80 |     ssl->s3->hs_buf.reset();
>       |          ^~
> external/boringssl/src/ssl/tls_method.cc: In function 'bool bssl::ssl3_set_read_state(SSL*, bssl::UniquePtr<bssl::SSLAEADContext>)':
> external/boringssl/src/ssl/tls_method.cc:87:26: error: expected primary-expression before ',' token
>    87 |     OPENSSL_PUT_ERROR(SSL, SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE);
>       |                          ^
> external/boringssl/src/ssl/tls_method.cc:87:28: error: 'SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE' was not declared in this scope
>    87 |     OPENSSL_PUT_ERROR(SSL, SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE);
>       |                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:87:5: error: 'OPENSSL_PUT_ERROR' was not declared in this scope
>    87 |     OPENSSL_PUT_ERROR(SSL, SSL_R_BUFFERED_MESSAGES_ON_CIPHER_CHANGE);
>       |     ^~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:88:5: error: 'ssl_send_alert' was not declared in this scope; did you mean 'ssl_process_alert'?
>    88 |     ssl_send_alert(ssl, SSL3_AL_FATAL, SSL_AD_UNEXPECTED_MESSAGE);
>       |     ^~~~~~~~~~~~~~
>       |     ssl_process_alert
> external/boringssl/src/ssl/tls_method.cc:92:23: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>    92 |   OPENSSL_memset(ssl->s3->read_sequence, 0, sizeof(ssl->s3->read_sequence));
>       |                       ^~
> external/boringssl/src/ssl/tls_method.cc:92:57: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>    92 |   OPENSSL_memset(ssl->s3->read_sequence, 0, sizeof(ssl->s3->read_sequence));
>       |                                                         ^~
> external/boringssl/src/ssl/tls_method.cc:93:8: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>    93 |   ssl->s3->aead_read_ctx = std::move(aead_ctx);
>       |        ^~
> external/boringssl/src/ssl/tls_method.cc: In function 'bool bssl::ssl3_set_write_state(SSL*, bssl::UniquePtr<bssl::SSLAEADContext>)':
> external/boringssl/src/ssl/tls_method.cc:102:23: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>   102 |   OPENSSL_memset(ssl->s3->write_sequence, 0, sizeof(ssl->s3->write_sequence));
>       |                       ^~
> external/boringssl/src/ssl/tls_method.cc:102:58: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>   102 |   OPENSSL_memset(ssl->s3->write_sequence, 0, sizeof(ssl->s3->write_sequence));
>       |                                                          ^~
> external/boringssl/src/ssl/tls_method.cc:103:8: error: 'SSL' {aka 'struct ssl_st'} has no member named 's3'
>   103 |   ssl->s3->aead_write_ctx = std::move(aead_ctx);
>       |        ^~
> external/boringssl/src/ssl/tls_method.cc: At global scope:
> external/boringssl/src/ssl/tls_method.cc:109:5: error: 'ssl3_new' was not declared in this scope
>   109 |     ssl3_new,
>       |     ^~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:110:5: error: 'ssl3_free' was not declared in this scope; did you mean 'SSL_free'?
>   110 |     ssl3_free,
>       |     ^~~~~~~~~
>       |     SSL_free
> external/boringssl/src/ssl/tls_method.cc:111:5: error: 'ssl3_get_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?
>   111 |     ssl3_get_message,
>       |     ^~~~~~~~~~~~~~~~
>       |     ssl_hs_read_message
> external/boringssl/src/ssl/tls_method.cc:112:5: error: 'ssl3_next_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?
>   112 |     ssl3_next_message,
>       |     ^~~~~~~~~~~~~~~~~
>       |     ssl_hs_read_message
> external/boringssl/src/ssl/tls_method.cc:113:5: error: 'ssl3_open_handshake' was not declared in this scope; did you mean 'ssl_open_handshake'?
>   113 |     ssl3_open_handshake,
>       |     ^~~~~~~~~~~~~~~~~~~
>       |     ssl_open_handshake
> external/boringssl/src/ssl/tls_method.cc:114:5: error: 'ssl3_open_change_cipher_spec' was not declared in this scope; did you mean 'ssl_open_change_cipher_spec'?
>   114 |     ssl3_open_change_cipher_spec,
>       |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
>       |     ssl_open_change_cipher_spec
> external/boringssl/src/ssl/tls_method.cc:115:5: error: 'ssl3_open_app_data' was not declared in this scope; did you mean 'ssl_open_app_data'?
>   115 |     ssl3_open_app_data,
>       |     ^~~~~~~~~~~~~~~~~~
>       |     ssl_open_app_data
> external/boringssl/src/ssl/tls_method.cc:116:5: error: 'ssl3_write_app_data' was not declared in this scope; did you mean 'ssl_open_app_data'?
>   116 |     ssl3_write_app_data,
>       |     ^~~~~~~~~~~~~~~~~~~
>       |     ssl_open_app_data
> external/boringssl/src/ssl/tls_method.cc:117:5: error: 'ssl3_dispatch_alert' was not declared in this scope
>   117 |     ssl3_dispatch_alert,
>       |     ^~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:118:5: error: 'ssl3_init_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?
>   118 |     ssl3_init_message,
>       |     ^~~~~~~~~~~~~~~~~
>       |     ssl_hs_read_message
> external/boringssl/src/ssl/tls_method.cc:119:5: error: 'ssl3_finish_message' was not declared in this scope
>   119 |     ssl3_finish_message,
>       |     ^~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:120:5: error: 'ssl3_add_message' was not declared in this scope; did you mean 'ssl_hs_read_message'?
>   120 |     ssl3_add_message,
>       |     ^~~~~~~~~~~~~~~~
>       |     ssl_hs_read_message
> external/boringssl/src/ssl/tls_method.cc:121:5: error: 'ssl3_add_change_cipher_spec' was not declared in this scope; did you mean 'ssl_open_change_cipher_spec'?
>   121 |     ssl3_add_change_cipher_spec,
>       |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~
>       |     ssl_open_change_cipher_spec
> external/boringssl/src/ssl/tls_method.cc:122:5: error: 'ssl3_flush_flight' was not declared in this scope
>   122 |     ssl3_flush_flight,
>       |     ^~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:1867:33: error: redefinition of 'const SSL_METHOD* TLS_method()'
>  1867 | #define SSLv23_method           TLS_method
>       |                                 ^~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:197:19: note: in expansion of macro 'SSLv23_method'
>   197 | const SSL_METHOD *SSLv23_method(void) {
>       |                   ^~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:188:19: note: 'const SSL_METHOD* TLS_method()' previously defined here
>   188 | const SSL_METHOD *TLS_method(void) {
>       |                   ^~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:273:19: error: redefinition of 'const SSL_METHOD* TLS_server_method()'
>   273 | const SSL_METHOD *TLS_server_method(void) {
>       |                   ^~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:1868:33: note: 'const SSL_METHOD* TLS_server_method()' previously defined here
>  1868 | #define SSLv23_server_method    TLS_server_method
>       |                                 ^~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:265:19: note: in expansion of macro 'SSLv23_server_method'
>   265 | const SSL_METHOD *SSLv23_server_method(void) {
>       |                   ^~~~~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:277:19: error: redefinition of 'const SSL_METHOD* TLS_client_method()'
>   277 | const SSL_METHOD *TLS_client_method(void) {
>       |                   ^~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/ssl/tls_method.cc:57:
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:1869:33: note: 'const SSL_METHOD* TLS_client_method()' previously defined here
>  1869 | #define SSLv23_client_method    TLS_client_method
>       |                                 ^~~~~~~~~~~~~~~~~
> external/boringssl/src/ssl/tls_method.cc:269:19: note: in expansion of macro 'SSLv23_client_method'
>   269 | const SSL_METHOD *SSLv23_client_method(void) {
>       |                   ^~~~~~~~~~~~~~~~~~~~
> In file included from external/boringssl/src/include/openssl/ex_data.h:112,
>                  from external/boringssl/src/ssl/../crypto/internal.h:112,
>                  from external/boringssl/src/ssl/tls_method.cc:64:
> external/boringssl/src/include/openssl/base.h: In instantiation of 'void bssl::internal::Deleter<T>::operator()(T*) [with T = evp_pkey_st]':
> /usr/lib/gcc/x86_64-redhat-linux/11/../../../../include/c++/11/bits/unique_ptr.h:361:17:   required from 'std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = evp_pkey_st; _Dp = bssl::internal::Deleter<evp_pkey_st>]'
> external/boringssl/src/ssl/internal.h:2101:39:   required from here
> external/boringssl/src/include/openssl/base.h:508:25: error: 'Free' is not a member of 'bssl::internal::DeleterImpl<evp_pkey_st, void>'
>   508 |     DeleterImpl<T>::Free(ptr);
>       |     ~~~~~~~~~~~~~~~~~~~~^~~~~
> In file included from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/comp.h:16,
>                  from /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/ssl.h:17,
>                  from external/boringssl/src/ssl/tls_method.cc:57:
> external/boringssl/src/ssl/internal.h: In instantiation of 'void bssl::Delete(T*) [with T = bssl::DC]':
> external/boringssl/src/ssl/internal.h:216:34:   required from 'static void bssl::internal::DeleterImpl<T, typename std::enable_if<T::kAllowUniquePtr>::type>::Free(T*) [with T = bssl::DC]'
> external/boringssl/src/include/openssl/base.h:508:25:   required from 'void bssl::internal::Deleter<T>::operator()(T*) [with T = bssl::DC]'
> /usr/lib/gcc/x86_64-redhat-linux/11/../../../../include/c++/11/bits/unique_ptr.h:361:17:   required from 'std::unique_ptr<_Tp, _Dp>::~unique_ptr() [with _Tp = bssl::DC; _Dp = bssl::internal::Deleter<bssl::DC>]'
> external/boringssl/src/ssl/internal.h:2097:22:   required from here
> /Data/Documents/CensusProject/gdallib/src/compiled/include/openssl/crypto.h:128:20: error: 'OPENSSL_free' was not declared in this scope
>   128 |         CRYPTO_free(addr, OPENSSL_FILE, OPENSSL_LINE)
>       |                    ^
> external/boringssl/src/ssl/internal.h:207:5: note: in expansion of macro 'OPENSSL_free'
>   207 |     OPENSSL_free(t);
>       |     ^~~~~~~~~~~~
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> Use --verbose_failures to see the command lines of failed build steps.
> INFO: Elapsed time: 633.309s, Critical Path: 42.73s
> INFO: 1201 processes: 71 internal, 1130 local.
> FAILED: Build did NOT complete successfully
> "
50478,TensorFlow Lite no consistency between C++ and C library names,"Question related with TensorFlow Lite. Library for C API is named `tensorflowlite_c`. Library for C++ API `tensorflow-lite`. What the logic behind? I think consistency should be between library names. For example:
1. `tensorflow-lite-c` and `tensorflow-lite`.
2. `tensorflow_lite_c` and `tensorflow_lite`.
3. `tensorflowlite_c` and `tensorflowlite`."
50477,"Changing default monitored metric for ReduceLRonPlateau from ""val_loss"" to ""loss""","- TensorFlow version: v2 (not relevant, feature change request is documentation-based)
- Willing to contribute it: Yes, it should be a one line change


Based on the documentation and my experience with tf, the default monitored metric for the callback ReduceLRonPlateau is ""val_loss"".
I suggest it should be changed to ""loss"" (the training loss).
My understanding of the rational for reducing the learning rate is that a learning rate that is too high prevents the model from converging to the nearby local optima. While I understand that the end goal is to minimize the validation loss and that monitoring this quantity is tempting, I do not understand the rational for monitoring it to reduce the learning rate.


Specifically, I consider 4 situations as a 2x2 matrix. The training loss is either decreasing, or it is not. Similarly, the validation loss is either decreasing, or it is not.

(1) If both the training and the validation losses are decreasing, then it does not matter which one is monitored (and the learning rate should not be changed).

(2) Similarly, if neither the training loss nor the validation loss is decreasing, then it does not matter which one is monitored (and the learning rate should be decreased).

(3) Now, if the training loss is decreasing, but the validation loss is not, then there are two explanations: 
(A) we are likely overfitting and reducing the learning rate is going to make the problem worse. 
(B) The validation loss is actually going down on average, but it is too noisy to be captured by the model. In that case too, reducing the learning rate is premature. 
/!\ However this is what the current default metric choice does. /!\ 
At this point the model should probably be early stopped anyway, but if we decide to keep training it, then we should not make the problem worse (maybe we should even increase the learning rate to help the model escape its local minima, but this is not the point of this post). 

(4) If the training loss is not decreasing but the validation loss is, then I would agree that the current default metric could be better. However I fail to see how this could happen? The only two scenarios I could imagine are:
(A) if the training loss is jumping from one local minima to another, with similar train losses but some local minima leading to better generalization. In that case I agree that reducing the learning rate would lead to premature converging in a local minima, but again this scenario seems unlikely to me.
(B) More likely, there is a higher standard error on the validation metric, and no actual underlying improvement of the generalization of the model. The learning rate should be reduced to improve the training loss, but the current default does not allow it.

I would agree that this is in the grey area, but so far the coworkers with whom I shared my thoughts ended up agreeing that monitoring the training loss makes more sense and is more intuitive. Some figures do not even mention the validation set in the context of learning rate reduction:
![ReduceLROnPlateau](https://user-images.githubusercontent.com/16949397/123551658-c6e97700-d740-11eb-9eb4-d999a33d6791.png)
Source: https://towardsdatascience.com/the-subtle-art-of-fixing-and-modifying-learning-rate-f1e22b537303

I personally do not mind changing this parameter for my projects, and I anticipate that the difference in performance will be small, but I did not find any issue regarding this question, so I wanted to at least raise it.

Thank you."
50476,I face same problem when I build VGG,"
`import numpy as np
import tensorflow as tf
from PIL import Image
from tensorflow.keras import Model,Sequential
from tensorflow.keras.layers import Conv2D,MaxPooling2D,Flatten,Dense,Dropout,BatchNormalization
from sklearn.model_selection import train_test_split
from keras_preprocessing.image import ImageDataGenerator
from PIL import Image
import os.path
import glob
import random




image_path=glob.glob('gar/*/*.jpg')

label_type=[image_p.split('\\')[1] for image_p in image_path]
labels=np.unique(label_type)

index_to_label=dict((l,n)for l,n in enumerate(labels))
label_to_index=dict((n,l)for l,n in index_to_label.items())

all_labels=[label_to_index.get(name)for name in label_type]

np.random.seed(22)
random_index=np.random.permutation(len(image_path))

path=np.array(image_path)[random_index]
signal_label=np.array(all_labels)[random_index]

sep=int(len(image_path)*0.7)
train_image_path=image_path[:sep]
train_y=all_labels[:sep]
test_image_path=image_path[sep:]
test_y=all_labels[sep:]

train=tf.data.Dataset.from_tensor_slices((train_image_path,train_y))
test=tf.data.Dataset.from_tensor_slices((test_image_path,test_y))

def load_pic(path,label):
    image=tf.io.read_file(path)
    image=tf.image.decode_jpeg(image,channels=3)
    image=tf.image.resize(image,[256,256])
    image=tf.cast(image,tf.float32)
    image=image/255
    return image,label
train=train.map(load_pic)

train=train.shuffle(1000).batch(64)

def get_model():
    gmodel=Sequential([
    Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu,input_shape=(256,256,3)),
    Conv2D(64, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),

    # Conv-Conv-Pooling单元2,输出通道提升至128，高宽大小减半
    Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    Conv2D(128, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    # 高宽减半
    MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),

    # Conv-Conv-Pooling单元3,输出通道提升至256，高宽大小减半
    Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    Conv2D(256, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),

    # Conv-Conv-Pooling单元4,输出通道提升至512，高宽大小减半
    Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),

    # Conv-Conv-Pooling单元5,输出通道提升至512，高宽大小减半
    Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    Conv2D(512, kernel_size=[3, 3], padding=""same"", activation=tf.nn.relu),
    MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),
    Dense(4)
    ])


    return gmodel
model=get_model()
# model.summary()

model.compile(
    optimizer=tf.keras.optimizers.Adam(0.005),
    loss=tf.keras.losses.sparse_categorical_crossentropy,
    metrics=['acc']
)
history=model.fit(train,epochs=10,validation_data=test)`


The problem show that
 tensorflow.python.framework.errors_impl.InvalidArgumentError:  Incompatible shapes: [64,1] vs. [64,8,8]"
50475,'CuDNNLSTM' object has no attribute 'unroll',"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
binary
- TensorFlow version (use command below):
2.5.0
- Python version:
3.8
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
CUDA = 11.3
cuDNN = 8201
- GPU model and memory:
Nvidia RTX 3070 (24gb)

**Describe the current behavior**
Model fails to save using ModelCheckPoint callback. 

**Describe the expected behavior**
Model saves best version using ModelCheckPoint callback

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
No
- Briefly describe your candidate solution(if contributing):
N/A

**Standalone code to reproduce the issue**
```Python
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
import numpy
from tensorflow.python.keras import Sequential
from tensorflow.python.keras.callbacks import ModelCheckpoint
from tensorflow.python.keras.layers import CuDNNLSTM

X = [
    [[1, 1, 1]],
    [[1, 1, 1]]
]

Y = [
    [1, 1, 1],
    [1, 1, 1]
]

X = numpy.array(X)
Y = numpy.array(Y)

model = Sequential()  # initialize a sequential model
model.add(CuDNNLSTM(3, return_sequences=False, input_shape=(1, 3)))  # 2nd layer of LSTM
model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])

model_checkpoint_callback = ModelCheckpoint(
    filepath='/tmp/checkpoint',
    save_weights_only=False,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

model.fit(X, Y, batch_size=1, epochs=100, callbacks=[model_checkpoint_callback], validation_split=0.5)

```

**Other info / logs** Include any logs or source code that would be helpful to

> Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""C:\Users\31619\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\211.7442.45\plugins\python\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""C:\Users\31619\AppData\Local\JetBrains\Toolbox\apps\PyCharm-P\ch-0\211.7442.45\plugins\python\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""C:/Users/31619/PycharmProjects/nn-drum-midi-synthesis/stackoverflow.py"", line 30, in <module>
    model.fit(X, Y, batch_size=1, epochs=100, callbacks=[model_checkpoint_callback], validation_split=0.5)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 1229, in fit
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 435, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 1369, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\callbacks.py"", line 1421, in _save_model
    self.model.save(filepath, overwrite=True, options=self._options)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 2111, in save
    save.save_model(self, filepath, overwrite, include_optimizer, save_format,
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\save.py"", line 150, in save_model
    saved_model_save.save(model, filepath, overwrite, include_optimizer,
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\save.py"", line 89, in save
    saved_nodes, node_paths = save_lib.save_and_return_nodes(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1103, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def,
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1290, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def,
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\saved_model\save.py"", line 1207, in _build_meta_graph_impl
    signatures = signature_serialization.find_function_to_export(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\saved_model\signature_serialization.py"", line 99, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\saved_model\save.py"", line 154, in list_functions
    obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\engine\training.py"", line 2713, in _list_functions_for_serialization
    functions = super(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\engine\base_layer.py"", line 3016, in _list_functions_for_serialization
    return (self._trackable_saved_model_saver
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\base_serialization.py"", line 92, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 73, in functions_to_serialize
    return (self._get_serialized_attributes(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 89, in _get_serialized_attributes
    object_dict, function_dict = self._get_serialized_attributes_internal(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\model_serialization.py"", line 53, in _get_serialized_attributes_internal
    super(ModelSavedModelSaver, self)._get_serialized_attributes_internal(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 99, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\save_impl.py"", line 154, in wrap_layer_functions
    original_fns = _replace_child_layer_functions(layer, serialization_cache)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\save_impl.py"", line 284, in _replace_child_layer_functions
    child_layer._trackable_saved_model_saver._get_serialized_attributes(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 89, in _get_serialized_attributes
    object_dict, function_dict = self._get_serialized_attributes_internal(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 151, in _get_serialized_attributes_internal
    super(RNNSavedModelSaver, self)._get_serialized_attributes_internal(
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\layer_serialization.py"", line 99, in _get_serialized_attributes_internal
    functions = save_impl.wrap_layer_functions(self.obj, serialization_cache)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\save_impl.py"", line 161, in wrap_layer_functions
    call_collection = LayerCallCollection(layer)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\save_impl.py"", line 411, in __init__
    self._input_signature = self._generate_input_signature(layer)
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\saving\saved_model\save_impl.py"", line 436, in _generate_input_signature
    layer._use_input_spec_as_call_signature):  # pylint: disable=protected-access
  File ""C:\Users\31619\PycharmProjects\nn-drum-midi-synthesis\venv\lib\site-packages\tensorflow\python\keras\layers\recurrent.py"", line 445, in _use_input_spec_as_call_signature
    if self.unroll:
AttributeError: 'CuDNNLSTM' object has no attribute 'unroll'"
50474,"Adding a feature of ""Hard"" attention could be very useful","<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.5
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
As of now we can get probability distribution after attention but would be quite useful if there is a way to use hard attention instead of a soft attention

**Will this change the current api? How?**
No
**Who will benefit with this feature?**
Could be quite useful while implementing ViT related architectures because using hard attention makes more sense for images instead of text 
**Any Other info.**
"
50473,[API docs] `tf.keras.Sequential.predict_classes` is deprecated,"https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict_classes

`tf.keras.Sequential.predict_classes` is deprecated - there should be a ""deprecated"" sign, similar to `Model.predict_generator`'s API doc:

[`/sequential.py`](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/engine/sequential.py#L441-L468):
```
    warnings.warn('`model.predict_classes()` is deprecated and '
                  'will be removed after 2021-01-01. '
                  'Please use instead:'
                  '* `np.argmax(model.predict(x), axis=-1)`, '
                  '  if your model does multi-class classification '
                  '  (e.g. if it uses a `softmax` last-layer activation).'
                  '* `(model.predict(x) > 0.5).astype(""int32"")`, '
                  '  if your model does binary classification '
                  '  (e.g. if it uses a `sigmoid` last-layer activation).')
```

![image](https://user-images.githubusercontent.com/19637339/123527506-2e1f1100-d6d8-11eb-96d9-50e0e6a83b5d.png)

cc @lamberta @MarkDaoust"
50472,"TypeError: An op outside of the function building code is being passed a ""Graph"" tensor.","Hi I got following error when I tried to include the KL loss in Flipout layers.  Any helps will be appreciated.  Thanks,



**TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: dense_flipout/divergence_kernel:0**

A example code that would create the same error as follows:

```
import numpy as np
import matplotlib.pyplot as plt
import os

def f(x, sigma):
    epsilon = np.random.randn(*x.shape) * sigma
    return 10 * np.sin(2 * np.pi * (x)) + epsilon

import tensorflow as tf

from tensorflow.keras import backend as K
from tensorflow.keras import activations, initializers
from tensorflow.keras.layers import Layer
import tensorflow_probability as tfp

tfd = tfp.distributions

import warnings
warnings.filterwarnings('ignore')

from tensorflow.keras.layers import Input
from tensorflow.keras.models import Model

train_size = 32
noise = 1.0

X = np.linspace(-0.5, 0.5, train_size).reshape(-1, 1)
y = f(X, sigma=noise)
y_true = f(X, sigma=0.0)

plt.scatter(X, y, marker='+', label='Training data')
plt.plot(X, y_true, label='Truth')
plt.title('Noisy training data and ground truth')
plt.savefig('data.png')
plt.close()

batch_size = train_size
num_batches = train_size / batch_size
kl_weight = 1.0 / num_batches

x_in = Input(shape=(1,))

x = tfp.layers.DenseFlipout(20, activation='relu')(x_in)
x = tfp.layers.DenseFlipout(20, activation='relu')(x)
x = tfp.layers.DenseFlipout(1)(x)
model = Model(x_in, x)

from tensorflow.keras import callbacks, optimizers, utils

def neg_log_likelihood(y_obs, y_pred, sigma=noise):
    dist = tfp.distributions.Normal(loc=y_pred, scale=sigma)
    return K.sum(-dist.log_prob(y_obs)) 

kl = sum(model.losses)
loss = neg_log_likelihood + kl 

model.compile(loss=loss, optimizer=optimizers.Adam(lr=0.08), metrics=['mse'])
utils.plot_model(model, to_file = 'model_flipout.png',  show_shapes = True, show_layer_names = True, show_dtype = True, dpi = 600)
model.summary()
model.fit(X, y, batch_size=batch_size, epochs=1500, verbose=0);
model.save(f'flipout.h5')    

import tqdm

X_test = np.linspace(-1.5, 1.5, 1000).reshape(-1, 1)
y_pred_list = []

for i in tqdm.tqdm(range(500)):
    y_pred = model.predict(X_test)
    y_pred_list.append(y_pred)
#import ipdb; ipdb.set_trace()                
y_preds = np.concatenate(y_pred_list, axis = 1)
y_mean = np.mean(y_preds, axis = 1)
y_sigma = np.std(y_preds, axis = 1)

plt.plot(X_test, y_mean, 'r-', label = 'Predictive mean');
plt.plot(X, y_true, 'b-', label='Truth')

plt.scatter(X, y, marker = '+', label = 'Training data')
plt.fill_between(X_test.ravel(), 
                 y_mean + 2 * y_sigma, 
                 y_mean - 2 * y_sigma, 
                 alpha = 0.5, label='Epistemic uncertainty')
plt.title('Prediction')
plt.legend();
plt.savefig('result_flipout.png')
plt.close()
    

```"
50471,About the k210 from tflite to kmodel,"System information :

OS Platform and Distribution : Windows 10
TensorFlow version : tf-nightly 2.6.0 
Python version: 3.8

Hi,
    I have converted my darknet weights to pb, and converted to tflite. Howerver, when I converted my tflite file to kmodel on nncase, it has the error message Fatal: inputs are not compatible to concat. 
    And I found that 
![image](https://user-images.githubusercontent.com/70742122/123503360-31d57800-d685-11eb-88bc-a60418ca6327.png) when I converted pb to tflite, and it has the tflite file, but I don't know whether the int8 conversion is fine? The original tflite also has the error message.
Below is my yolov4-tiny-int8.tflite on Netron
![image](https://user-images.githubusercontent.com/70742122/123503463-bde79f80-d685-11eb-96e6-c2fccd817c1a.png)

Does any one know the solution or give me some advice about the error?
Thanks.
    
"
50470,Error in .summary() on .pb model imported using 'tf.saved_model.load()',"I was trying to load .pb model which was saved in ""20180402-114759/"" directory .
model can be found [here]( https://github.com/davidsandberg/facenet) under the section ""pretrained model"" .
So I just tried to check the summary with .summary() method but got an error .

Code : 
```
import tensorflow as tf
model_path = ""20180402-114759/""
model = tf.saved_model.load(model_path)
print(model.summary())

```

Error :
```
Traceback (most recent call last):
  File ""E:\Pathik\KJ\internships\Facial Expression recognition\20180402-114759\test.py"", line 7, in <module>
    print(model.summary())
AttributeError: 'AutoTrackable' object has no attribute 'summary'

```
System information :
* OS Platform and Distribution : Windows 10
* TensorFlow version : 2.3.0
* Python version: 3.7.8

How to resolve this error ? "
50467,Softmax layer unexpected and confusing error message,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.4.0-rc4-71-g582c8d236cb 2.4.0
- Python version: 3.8.5

**Describe the current behavior**
I am getting a strange error message for the following model. The error seems to originate from the softmax layer at the end. 
The message is the following:
""tensorflow.python.framework.errors_impl.InvalidArgumentError: Dimensions must be equal, but are 2 and 4 for '{{node Sof43628/add/add}} = AddV2[T=DT_DOUBLE](Placeholder, Sof43628/mul)' with input shapes: [?,4,2,1], [?,4,2].""
The error is unexpected since I think the model should work and it is confusing since in the softmax layer there is no requirement for matching dimensions. What could be the reason for this error?

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
in0Mas92349 = tf.keras.layers.Input(shape=([4, 2, 1]))
in0Dep9315 = tf.keras.layers.Input(shape=([1, 2, 1]))
in0Ave93999 = tf.keras.layers.Input(shape=([2, 1]))
Mas92349 = keras.layers.Masking(mask_value=1, name = 'Mas92349', )(in0Mas92349)
Dep9315 = keras.layers.DepthwiseConv2D((1, 2),strides=(1, 1), padding='same', name = 'Dep9315', )(in0Dep9315)
Zer31495 = keras.layers.ZeroPadding2D(padding=((3, 0), (0, 0)), name = 'Zer31495', )(Dep9315)
Max17507 = keras.layers.Maximum(name = 'Max17507', )([Mas92349,Zer31495])
Ave93999 = keras.layers.AveragePooling1D(pool_size=(2), name = 'Ave93999', )(in0Ave93999)
Res89 = keras.layers.Reshape((1, 1, 1), name = 'Res89', )(Ave93999)
Zer84427 = keras.layers.ZeroPadding2D(padding=((3, 0), (1, 0)), name = 'Zer84427', )(Res89)
Sub42930 = keras.layers.Subtract(name = 'Sub42930', )([Max17507,Zer84427])
Sof43628 = keras.layers.Softmax(axis=1, name = 'Sof43628', )(Sub42930)
model = tf.keras.models.Model(inputs=[in0Mas92349,in0Dep9315,in0Ave93999], outputs=Sof43628)

in0Mas92349 = tf.constant([[[[1.8457], [1.4313]], [[1.8795], [1.1303]], [[1.4792], [1.9199]], [[1.996], [1.583]]]])
in0Dep9315 = tf.constant([[[[0.6111], [0.9861]]]])
in0Ave93999 = tf.constant([[[1.1779], [1.8115]]])
print (np.array2string(model.predict([in0Mas92349,in0Dep9315,in0Ave93999],steps=1), separator=', '))
```
"
50466,TensorFlow 2.x: Model Training Indefinitely in Graph Mode,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **Yes**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Windows 10**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **N/A**
- TensorFlow installed from (source or binary): **binary**
- TensorFlow version (use command below): **pip install tensorflow**
- Python version: **python 3.9.5**
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am training the model in both the eager execution mode and the graph mode. The model is training well in the eager execution mode, however running for indefinitely in the graph mode. I tried to debug in multiple way with no success.

**Describe the expected behavior**
The model should train in the same way in both eager execution and graph mode. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
import numpy as np
import tensorflow as tf

tf.compat.v1.disable_eager_execution() # comment to enable eager execution mode
print(""[INFO] Eager mode: "", tf.executing_eagerly()) # For easy reset of notebook state.

class CustomModelV2(tf.keras.Model):
    def __init__(self):
        super(CustomModelV2, self).__init__()
        self.encoder = Encoder(32)
        self.encoder.build(input_shape=(None, 32))
        self.loss_tracker = tf.keras.metrics.Mean(name=""loss"")
        
    def call(self, inputs, training):
        return self.encoder(inputs, training)
        
    @property
    def metrics(self):
        # We list our `Metric` objects here so that `reset_states()` can be
        # called automatically at the start of each epoch
        # or at the start of `evaluate()`.
        # If you don't implement this property, you have to call
        # `reset_states()` yourself at the time of your choosing.
        return [self.loss_tracker]

    @tf.function
    def train_step(self, data):
        # Unpack the data. Its structure depends on your model and
        # on what you pass to `fit()`.
        x, y = data

        with tf.GradientTape() as tape:
            y_pred = self.call(x, training=True)  # Forward pass

            # Compute the loss value
            # (the loss function is configured in `compile()`)
            r_loss = tf.keras.losses.mean_squared_error(y, y_pred)
            loss = r_loss 

        # Compute gradients
        trainable_vars = self.trainable_variables
        gradients = tape.gradient(loss, trainable_vars)
        
        # Update weights
        self.optimizer.apply_gradients(zip(gradients, trainable_vars))
        
        # Update metrics (includes the metric that tracks the loss)
        self.loss_tracker.update_state(loss)
        
        # Return a dict mapping metric names to current value
        return {""loss"": self.loss_tracker.result()}

class Encoder(tf.keras.Model):
    def __init__(self, input_size):
        super(Encoder, self).__init__(name = 'Encoder')
        self.input_layer   = DenseLayer(128, input_size, 0.0, 0.0, 'float32')
        self.hidden_layer1 = DenseLayer(128, 128, 0.001, 0.0, 'float32')
        self.dropout_laye1 = tf.keras.layers.Dropout(0.2)
        self.hidden_layer2 = DenseLayer(64, 128, 0.001, 0.0, 'float32')      
        self.dropout_laye2 = tf.keras.layers.Dropout(0.2)
        self.hidden_layer3 = DenseLayer(64, 64, 0.001, 0.0, 'float32')
        self.dropout_laye3 = tf.keras.layers.Dropout(0.2)           
        self.output_layer  = LinearLayer(64, 64, 0.001, 0.0, 'float32')
        
    def call(self, input_data, training):
        fx = self.input_layer(input_data)        
        fx = self.hidden_layer1(fx)
        if training:
            fx = self.dropout_laye1(fx)     
        fx = self.hidden_layer2(fx)
        if training:
            fx = self.dropout_laye2(fx) 
        fx = self.hidden_layer3(fx)
        if training:
            fx = self.dropout_laye3(fx) 
        return self.output_layer(fx)

class LinearLayer(tf.keras.layers.Layer):

    def __init__(self, units, input_dim, weights_regularizer, bias_regularizer, d_type):
        super(LinearLayer, self).__init__()
        self.w = self.add_weight(name='w_linear',
                                shape = (input_dim, units), 
                                initializer = tf.keras.initializers.RandomUniform(
                                    minval=-tf.cast(tf.math.sqrt(6/(input_dim+units)), dtype = d_type), 
                                    maxval=tf.cast(tf.math.sqrt(6/(input_dim+units)), dtype = d_type), 
                                    seed=16751),                                                                   
                                regularizer = tf.keras.regularizers.l1(weights_regularizer), 
                                trainable = True)
        self.b = self.add_weight(name='b_linear',
                                 shape = (units,),    
                                 initializer = tf.zeros_initializer(),
                                 regularizer = tf.keras.regularizers.l1(bias_regularizer),
                                 trainable = True)

    def call(self, inputs):
        return tf.matmul(inputs, self.w) + self.b

class DenseLayer(tf.keras.layers.Layer):

    def __init__(self, units, input_dim, weights_regularizer, bias_regularizer, d_type):
        super(DenseLayer, self).__init__()
        self.w = self.add_weight(name='w_dense',
                                 shape = (input_dim, units), 
                                 initializer = tf.keras.initializers.RandomUniform(
                                     minval=-tf.cast(tf.math.sqrt(6.0/(input_dim+units)), dtype = d_type),  
                                     maxval=tf.cast(tf.math.sqrt(6.0/(input_dim+units)), dtype = d_type),  
                                     seed=16751), 
                                 regularizer = tf.keras.regularizers.l1(weights_regularizer), 
                                 trainable = True)
        self.b = self.add_weight(name='b_dense',
                                 shape = (units,),    
                                 initializer = tf.zeros_initializer(),
                                 regularizer = tf.keras.regularizers.l1(bias_regularizer),
                                 trainable = True)

    def call(self, inputs):
        x = tf.matmul(inputs, self.w) + self.b
        return tf.nn.elu(x)

# Just use `fit` as usual
x = tf.data.Dataset.from_tensor_slices(np.random.random((5000, 32)))

y_numpy = np.random.random((5000, 1))
y_numpy[:, 3:] = None
y = tf.data.Dataset.from_tensor_slices(y_numpy)

x_window = x.window(30, shift=10, stride=1)
flat_x = x_window.flat_map(lambda t: t)
flat_x_scaled = flat_x.map(lambda t: t * 2)

y_window = y.window(30, shift=10, stride=1)
flat_y = y_window.flat_map(lambda t: t)
flat_y_scaled = flat_y.map(lambda t: t * 2)

z = tf.data.Dataset.zip((flat_x_scaled, flat_y_scaled)).batch(32).cache().shuffle(buffer_size=32).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

# Stopping criteria if the training loss doesn't go down by 1e-3
early_stop_cb = tf.keras.callbacks.EarlyStopping(
    monitor='loss', min_delta = 1e-3, verbose = 1, mode='min', patience = 3, 
    baseline=None, restore_best_weights=True)

# Construct and compile an instance of CustomModel
model = CustomModelV2()


  
model.compile(optimizer=tf.optimizers.Adagrad(0.01))

history = model.fit(z, epochs=3, callbacks=[early_stop_cb])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Output in Graph Mode: 
```
WARNING:tensorflow:Output output_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to output_1.
WARNING:tensorflow:From C:\Users\jain432\Anaconda3\envs\tf\lib\site-packages\tensorflow\python\keras\optimizer_v2\adagrad.py:87: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
Train on None steps
Epoch 1/3
 479916/Unknown - 667s 1ms/step - batch: 239957.5000 - size: 1.0000 - loss: 2.1716e-04
```

Output in Eager Mode: 
```
Epoch 1/3
468/468 [==============================] - 2s 3ms/step - loss: 0.4173
Epoch 2/3
468/468 [==============================] - 1s 3ms/step - loss: 0.3695
Epoch 3/3
468/468 [==============================] - 1s 3ms/step - loss: 0.3608
```
"
50463,TFLiteConverter Generates Quantized i64 bias that MLIR Quant dialect doesn't support,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): nightly
- Python version: 3.6.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 7.5.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
I use TFLiteConverter with post-quantization to generate int16 x int8 test, where the bias is int64.
Current TFLiteConverter generates bias in quantized form:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/tools/optimize/quantization_utils.cc#L684

but in MLIR Quant dialect the max storage bits can be up to 32:
https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/Dialect/Quant/QuantTypes.h#L57

**Describe the expected behavior**
N/A

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): Yes if applicable.
- Briefly describe your candidate solution(if contributing):
1. TFLite Converter generates non-quantized bias
2. Loosen the restriction in MLIR Quant dialect

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Here's the example MLIR (test.tflite.mlir) translated from a normal 16x8 conv2d .tflite:
```
module attributes {tfl.description = ""MLIR Converted."", tfl.schema_version = 3 : i32}  {
  func @main(%arg0: tensor<1x32x32x8x!quant.uniform<i16:f32, 6.1037018895149231E-5>>) -> tensor<1x32x32x16x!quant.uniform<i16:f32, 5.6973122991621494E-4>> attributes {tf.entry_function = {inputs = ""placeholder_0"", outputs = ""Identity""}} {
    %0 = ""tfl.pseudo_qconst""() {qtype = tensor<16x1x1x8x!quant.uniform<i8<-127:127>:f32:0, {0.014107929542660713,0.015315329656004906,0.012964749708771706,0.015471030026674271,0.010782554745674133,0.01520597655326128,0.012333446182310581,0.013902087695896626,0.012822438031435013,0.014312445186078548,0.012691914103925228,0.015708990395069122,0.013873680494725704,0.014024644158780575,0.014720370061695576,0.0149689931422472}>>, value = dense<""0x5CFE4C1F6C9E8106C4389D71E8CD6481CB81BC7C9AE67C6B44C58B87BF0FAA7F40CD91A0797F15BBF28844A881A9C422BD7A9C7F33D21EB47E344BCCFE8172075D26812D56EC747481BFBFCB7BC4453D811ECF3622BAD4EF06B9546FB757CF8192F01B818A7244ECDAFDE281DC33308487D2EC74AB8110D829F88281F248D011""> : tensor<16x1x1x8xi8>} : () -> tensor<16x1x1x8x!quant.uniform<i8<-127:127>:f32:0, {0.014107929542660713,0.015315329656004906,0.012964749708771706,0.015471030026674271,0.010782554745674133,0.01520597655326128,0.012333446182310581,0.013902087695896626,0.012822438031435013,0.014312445186078548,0.012691914103925228,0.015708990395069122,0.013873680494725704,0.014024644158780575,0.014720370061695576,0.0149689931422472}>>
    %1 = ""tfl.pseudo_qconst""() {qtype = tensor<16x!quant.uniform<i64:f32:0, {8.6110594565980136E-7,9.348020739707863E-7,7.9132968267003889E-7,9.443055546398682E-7,6.581349794032576E-7,9.2812746288473135E-7,7.5279677957951208E-7,8.4854201531925355E-7,7.8264338299049996E-7,8.7358898781531025E-7,7.7467660730690113E-7,9.5882990081008757E-7,8.4680812051374232E-7,8.5602249555449816E-7,8.9848748530130251E-7,9.1366274546089698E-7}>>, value = dense<[1707424, 600679, -527282, -1446348, 2659806, 910839, 1976213, -434242, -2548601, 1757995, 44353, -1558553, 1132077, -1680781, 2215969, -1760827]> : tensor<16xi64>} : () -> tensor<16x!quant.uniform<i64:f32:0, {8.6110594565980136E-7,9.348020739707863E-7,7.9132968267003889E-7,9.443055546398682E-7,6.581349794032576E-7,9.2812746288473135E-7,7.5279677957951208E-7,8.4854201531925355E-7,7.8264338299049996E-7,8.7358898781531025E-7,7.7467660730690113E-7,9.5882990081008757E-7,8.4680812051374232E-7,8.5602249555449816E-7,8.9848748530130251E-7,9.1366274546089698E-7}>>
    %2 = ""tfl.conv_2d""(%arg0, %0, %1) {dilation_h_factor = 1 : i32, dilation_w_factor = 1 : i32, fused_activation_function = ""NONE"", padding = ""SAME"", stride_h = 1 : i32, stride_w = 1 : i32} : (tensor<1x32x32x8x!quant.uniform<i16:f32, 6.1037018895149231E-5>>, tensor<16x1x1x8x!quant.uniform<i8<-127:127>:f32:0, {0.014107929542660713,0.015315329656004906,0.012964749708771706,0.015471030026674271,0.010782554745674133,0.01520597655326128,0.012333446182310581,0.013902087695896626,0.012822438031435013,0.014312445186078548,0.012691914103925228,0.015708990395069122,0.013873680494725704,0.014024644158780575,0.014720370061695576,0.0149689931422472}>>, tensor<16x!quant.uniform<i64:f32:0, {8.6110594565980136E-7,9.348020739707863E-7,7.9132968267003889E-7,9.443055546398682E-7,6.581349794032576E-7,9.2812746288473135E-7,7.5279677957951208E-7,8.4854201531925355E-7,7.8264338299049996E-7,8.7358898781531025E-7,7.7467660730690113E-7,9.5882990081008757E-7,8.4680812051374232E-7,8.5602249555449816E-7,8.9848748530130251E-7,9.1366274546089698E-7}>>) -> tensor<1x32x32x16x!quant.uniform<i16:f32, 5.6973122991621494E-4>>
    return %2 : tensor<1x32x32x16x!quant.uniform<i16:f32, 5.6973122991621494E-4>>
  }
}
```
And you can see the problem when you read this with tf-opt, even without specifying any transformation:


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
$ bazel-bin/tensorflow/compiler/mlir/tf-opt test.tflite.mlir
test.tflite.mlir:4:66: error: illegal storage type size: 64
    %1 = ""tfl.pseudo_qconst""() {qtype = tensor<16x!quant.uniform<i64:f32:0, {9.4795944960424094E-7,9.1321464879001724E-7,9.4685782414671848E-7,8.5114061221247539E-7,8.9218377752331434E-7,7.9372739492100663E-7,6.839824209237122E-7,9.5105099262582371E-7,6.5581531316638575E-7,6.6347195115668001E-7,8.5451210907194763E-7,9.0817928821707027E-7,9.1536480795184616E-7,8.8241159801327739E-7,8.0079468034455203E-7,9.1778696287292405E-7}>>, value = dense<0> : tensor<16xi64>} : () -> tensor<16x!quant.uniform<i64:f32:0, {9.4795944960424094E-7,9.1321464879001724E-7,9.4685782414671848E-7,8.5114061221247539E-7,8.9218377752331434E-7,7.9372739492100663E-7,6.839824209237122E-7,9.5105099262582371E-7,6.5581531316638575E-7,6.6347195115668001E-7,8.5451210907194763E-7,9.0817928821707027E-7,9.1536480795184616E-7,8.8241159801327739E-7,8.0079468034455203E-7,9.1778696287292405E-7}>>

```"
50461,Why the output tensor device type is GPU even though I set the HostMemory attr?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: A100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Hi folks,
I am trying to create an Op recently. This is part of my Op kernel builder:
```
REGISTER_KERNEL_BUILDER(Name(""Test"").Device(::tensorflow::DEVICE_GPU)
                                                      .HostMemory(""output""),
                        TestOp);

REGISTER_OP(""Test"")
    .Input(""tensor: T"")
    .Output(""output: T"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      ::tensorflow::shape_inference::ShapeHandle output;
      TF_RETURN_IF_ERROR(c->ReplaceDim(c->input(0), 0, c->UnknownDim(), &output));
      c->set_output(0, output);
      return ::tensorflow::Status::OK();
    })
```
For my Op, it's expected the input is on the device while the output will be on the host. Inside the op_kernel, I used api ```cuPointerGetAttribute``` provided by NVIDIA to verify that the output memory is allocated on the host. However, after I call Op using Python and obtain the ```output``` tensor, I tried to print the device type ```print(output.device)```. I found that the output tensor is on the ```GPU```, which is supposed to be ```CPU``` for my case? I tried the scope ```with tf.device(""/gpu:0"")``` and ```with tf.device(""/cpu:0""), but there was no difference

I also tried to set the attribute manually inside the Op_kernel.
```
::tensorflow::AllocatorAttributes alloc_attrs;
alloc_attrs.set_on_host(true);
OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, result_shape, &output), done);
```
But the result ```print(output.device)``` from Python is still on the device(GPU). 

I am concerned that TF do the Host2Device memory copy automatically.

So I wrote another DataPtr Op which reads the output from the above TestOp, and tried to print the memory address:

```
class DataPtrOp : public ::tensorflow::OpKernel {
    public:
    explicit DataPtrOp(::tensorflow::OpKernelConstruction* context) : OpKernel(context) {}

    void Compute(::tensorflow::OpKernelContext* context) override {
        // Grab the input tensor

        auto input_tensor = context->input(0);
        auto input = std::make_shared<TFTensor>(input_tensor, GetDeviceID(context));
        std::size_t address = reinterpret_cast<std::size_t>(input->data());
        std::cout << ""Test. address:"" << address << std::endl;
        // 
       //skip some unrelated code here
       ///
    }
    };


REGISTER_KERNEL_BUILDER(Name(""DataPtr"").Device(::tensorflow::DEVICE_CPU),
                        DataPtrOp);

REGISTER_OP(""DataPtr"")
        .Attr(""T: {uint8, int8, uint16, int16, int32, int64, float16, float32, float64, bool}"")  // 
        .Input(""input: T"")  // Tensor
        .Output(""output: uint64"")  // scalar
        .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
        c->set_output(0, {}); // scalar
        return ::tensorflow::Status::OK();
        });

```

The output tensor memory address from TestOp on the host is the same as the input tensor memory address from DataPtr Op.

So I believe that the ```output``` tensor is allocated on the Host. So, is this a bug or issue with ```tensor.device``` API?
It is very confusing somehow.

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50460,Blocking on webgl for first initialization,On the first run of model using webgl backend it takes very long time. I understand that internally it has to set up the gpu but the method blocks and takes a long time(I see this with predict/execute and execute async.) Is this the expected behavior? What is the recommended behavior and any tips on speeding this up? Should we run in webworker? I have about six models I need to initialize. Is there a way to run them all at once?
50459,JSON error when saving Tensorflow model. ,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10 x64 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 1.14.0 gpu
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10
- GPU model and memory: GeForce GTX TITAN X


**Describe the current behavior**
When saving the model the following error occurs: typeerror: ('not json serializable:', b'\n\x03add\x12\x03add\x1a\x14dropout_1/cond/merge\x1a\x12dropout/cond/merge*\x07\n\x01t\x12\x020\x01') addition

**Describe the expected behavior**
Saving of the model and continuation to the next epoch.




**Standalone code to reproduce the issue**
The code for the model:
`#!/usr/bin/env python
# coding: utf-8

# In[1]:


import tensorflow
import miscnn
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, concatenate
from tensorflow.keras.layers import Conv3D, MaxPooling3D, Conv3DTranspose
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv2DTranspose, SeparableConv2D, DepthwiseConv2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.layers import Concatenate, Add, concatenate
from tensorflow.keras.layers import ReLU
from tensorflow.keras.initializers import glorot_normal, Identity
#from tensorflow.keras.contrib.layers import repeat
#from tensorflow.keras.contrib.framework import Arg_Scope
from tensorflow.keras.regularizers import l2

from miscnn.neural_network.architecture.abstract_architecture import Abstract_Architecture

fc = Dense
conv = Conv2D
deconv = Conv2DTranspose
relu = ReLU
maxpool = MaxPooling2D
dropout_layer = Dropout
batchnorm = BatchNormalization
winit = glorot_normal()
#repeat = repeat
#arg_scope = Aarg_Scope
l2_regularizer = l2

class Architecture(Abstract_Architecture):
    def __init__(self,  n_classes=1, l2=None, is_training=True, upsampling=1):
#         self.input_x = input_x
        self.n_classes = n_classes
        self.l2=None
        self.is_training=True
        self.upsampling=1
    
    def create_model_2D(self, input_shape, n_classes=1, n_labels=1,is_training=True,l2=None,upsampling=1):
        inputs = Input(input_shape) 
    
        x = downsample(inputs, n_filters_in=3, n_filters_out=16, is_training=is_training, l2=l2, name=""d1"")
        

        x = downsample(x, n_filters_in=16, n_filters_out=64, is_training=is_training, l2=l2, name=""d2"")
        
        x = encoder_module(x,n_filters=64, is_training=is_training, dilation=[1, 1], l2=l2, name=""fres3"", dropout=0.0)
        
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres4"", dropout=0.0)
        
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres5"", dropout=0.0)
        
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres6"", dropout=0.0)
        
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres7"", dropout=0.0)
        
    
        x = downsample(x,  n_filters_in=64, n_filters_out=128, is_training=is_training, l2=l2, name=""d8"")
        
        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 2], l2=l2, name=""fres9"", dropout=0.25)
        
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 4], l2=l2, name=""fres10"", dropout=0.25)
        
        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 8], l2=l2, name=""fres11"", dropout=0.25)
        
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 16], l2=l2, name=""fres12"", dropout=0.25)
        

        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres13"", dropout=0.25)
        
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 2], l2=l2, name=""fres14"", dropout=0.25)
        
        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 8], l2=l2, name=""fres15"", dropout=0.25)
        
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 16], l2=l2, name=""fres16"", dropout=0.25)
        
    
        x = upsample(x, n_filters=64, is_training=is_training, l2=l2, name=""up17"")
        
        x3 = downsample(inputs, n_filters_in=3, n_filters_out=16, is_training=is_training, l2=l2, name=""d7"")
        
        x3 = downsample(x3, n_filters_in=16, n_filters_out=66, is_training=is_training, l2=l2, name=""d7"")
        print(x.shape)
        print(x3.shape)
        
        #x = x+x3
        x = Add()([x, x3])

        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres18"", dropout=0)
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres19"", dropout=0)
        x = upsample(x, n_filters=1, is_training=is_training, l2=l2, name=""up23"", last=True)

        #if upsampling > 1:
        x = tensorflow.image.resize(x, size= [128,128], align_corners=True)
        print(x.shape)
        
        x = Conv2D(1, (1, 1), activation='sigmoid')(x)
        
        model = Model(inputs=[inputs], outputs=[x])
        return model

    def create_model_3D(self, input_shape, n_labels=1, is_training=True):
        
        inputs = Input(input_shape)    
        
        x = downsample(inputs, n_filters_in=3, n_filters_out=16, is_training=is_training, l2=l2, name=""d1"")

        x = downsample(x, n_filters_in=16, n_filters_out=64, is_training=is_training, l2=l2, name=""d2"")
        x = encoder_module(x,n_filters=64, is_training=is_training, dilation=[1, 1], l2=l2, name=""fres3"", dropout=0.0)
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres4"", dropout=0.0)
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres5"", dropout=0.0)
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres6"", dropout=0.0)
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres7"", dropout=0.0)
    
        x = downsample(x,  n_filters_in=64, n_filters_out=128, is_training=is_training, l2=l2, name=""d8"")
        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 2], l2=l2, name=""fres9"", dropout=0.25)
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 4], l2=l2, name=""fres10"", dropout=0.25)
        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 8], l2=l2, name=""fres11"", dropout=0.25)
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 16], l2=l2, name=""fres12"", dropout=0.25)

        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres13"", dropout=0.25)
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 2], l2=l2, name=""fres14"", dropout=0.25)
        x = encoder_module_multi(x, n_filters=128,is_training=is_training, dilation=[1, 8], l2=l2, name=""fres15"", dropout=0.25)
        x = encoder_module_multi(x,n_filters=128, is_training=is_training, dilation=[1, 16], l2=l2, name=""fres16"", dropout=0.25)
    
        x = upsample(x, n_filters=64, is_training=is_training, l2=l2, name=""up17"")
        x3 = downsample(input_x, n_filters_in=3, n_filters_out=16, is_training=is_training, l2=l2, name=""d7"")
        x3 = downsample(x3, n_filters_in=16, n_filters_out=64, is_training=is_training, l2=l2, name=""d7"")
        #x = x+x3
        x = Add()([x, x3])

        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres18"", dropout=0)
        x = encoder_module(x, n_filters=64,is_training=is_training, dilation=[1, 1], l2=l2, name=""fres19"", dropout=0)
        x = upsample(x, n_filters=n_classes, is_training=is_training, l2=l2, name=""up23"", last=True)

        if upsampling > 1:
            x = tensorflow.image.resize_bilinear(x, size=[x.shape[1] * upsampling, x.shape[2] * upsampling], align_corners=True)
        
        
        model = Model(inputs=[inputs], outputs=[x])
        return model

def residual_separable(input, n_filters,  is_training=True, dropout=0.3, dilation=1, l2=None, name=""down""):
        x = SeparableConv2D(n_filters, (3, 3), strides=1, padding='same', activation='relu', dilation_rate=dilation, use_bias=False, depthwise_initializer=winit, pointwise_initializer=winit, pointwise_regularizer=l2_regularizer(0.00004))(input)
        x = BatchNormalization()(x)
        x = dropout_layer(rate=dropout)(x)
        if input.shape[3] == x.shape[3]:
            x = Add()([x, input])
        return x

def residual_separable_multi(input, n_filters,  is_training, dropout=0.3, dilation=1, l2=None, name=""down""):
        input_b = input
        d = DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1,  activation='relu', padding='same', use_bias=False)
        x = d(input)
        x = BatchNormalization()(x)

        d2= DepthwiseConv2D(3, strides=(1, 1), depth_multiplier=1,  activation='relu', padding='same', use_bias=False)
        d2.dilation_rate = (dilation, dilation)
        x2 = d2(input)
        x2 = BatchNormalization()(x2)

        #x +=x2
        x = Add()([x, x2])

        x = Conv2D(n_filters, 1, strides=1, padding='same', activation='relu', kernel_initializer=winit,
                         dilation_rate=1, use_bias=False, kernel_regularizer=l2_regularizer(0.00004))(x)
        x = BatchNormalization()(x)

        x = dropout_layer(rate=dropout)(x)

        if input.shape[3] == x.shape[3]:
            x = Add()([x, input_b])

        return x


def encoder_module(input, n_filters,  is_training = True, dropout=0.3, dilation=[1,1], l2=None, name=""down""):
        x = residual_separable(input, n_filters,  is_training, dropout=dropout, dilation=dilation[0], l2=l2, name=name)
        x = residual_separable(x, n_filters,  is_training, dropout=dropout, dilation=dilation[1], l2=l2, name=name)
        return x

def encoder_module_multi(input, n_filters,  is_training = True, dropout=0.3, dilation=[1,1], l2=None, name=""down""):
        x = residual_separable_multi(input, n_filters,  is_training, dropout=dropout, dilation=dilation[0], l2=l2, name=name)
        x = residual_separable_multi(x, n_filters,  is_training, dropout=dropout, dilation=dilation[1], l2=l2, name=name)
        return x

def upsample(x, n_filters, is_training=True, last=False, l2=None, name=""down""):
        x = Conv2DTranspose(n_filters, 3, strides=2, padding='same',use_bias=True,
                               kernel_initializer=winit, activation='relu',
                                   kernel_regularizer=l2_regularizer(0.00004))(x)
        if not last:
            x = BatchNormalization()(x)

        return x
    
def upsample2(x, n_filters, is_training=True, last=False, l2=None, name=""down""):
        x = Conv2DTranspose(n_filters, 3, strides=4, padding='same',use_bias=True,
                               kernel_initializer=winit, activation='relu',
                                   kernel_regularizer=l2_regularizer(0.00004))(x)
        if not last:
            x = BatchNormalization()(x)

        return x    

def downsample(input, n_filters_in, n_filters_out, is_training=True, bn=False, use_relu=False, l2=None, name=""down""):
    
        maxpool_use = n_filters_in < n_filters_out

        if not maxpool_use:
            filters_conv = n_filters_out
        else:
            filters_conv = n_filters_out - n_filters_in

        x = Conv2D(filters_conv, kernel_size = 3,  strides=2, padding='same', activation='relu', kernel_initializer=winit,
                        dilation_rate=1, use_bias=False, kernel_regularizer=l2_regularizer(0.00004))(input)
        if maxpool_use:
            y = MaxPooling2D(pool_size=2, strides=2)(input)
            x = Concatenate(axis=-1)([x, y]) 

        x = BatchNormalization()(x)
#         x = ReLU(x)
        return x`

And the code I'm running:
`#!/usr/bin/env python
# coding: utf-8

# In[7]:


import os
import tensorflow as tf
import keras

import keras.backend.tensorflow_backend

gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.40)
session = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))
keras.backend.tensorflow_backend.set_session(session)

os.environ[""CUDA_VISIBLE_DEVICES""]=""1""

os.environ[""LD_LIBRABY_PATH""]=""/usr/local/cuda-10.2/lib64""

import sys
import miscnn
from miscnn.evaluation import cross_validation
from miscnn.data_loading.interfaces.png_IO import png_IO
#from miscnn.data_loading.interfaces.image_io import Image_interface
from miscnn.neural_network.metrics import dice_soft, dice_coefficient, dice_coefficient_loss
from miscnn.evaluation import split_validation
from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
#from tf2cv.model_provider import get_model as tf2cv_get_model
import numpy as np

#os.environ[""CUDA_VISIBLE_DEVICES""]=""2""


# In[3]:


data_path = ""Zip_files/CCA_ICA_Snake""#sys.argv[1]
output_path = ""Zip_files/Results/MiniNet_Training_1_MiniNet_GPU_notebook"" #sys.argv[2]
fold = 1 #int(sys.argv[3])
# print(fold)
# miscnn_surf(data_path,output_path)


# In[4]:


print('standard')
interface = png_IO( channels=1,classes=1,three_dim=False, im_width=128, im_height=128)
#interface = png_IO(classes=1, img_type=""grayscale"", img_format=""png"", pattern=None)
# Create the Data I/O object
data_io = miscnn.Data_IO(interface, data_path,output_path=output_path)
    # Create and configure the Data Augmentation class
data_aug = miscnn.Data_Augmentation(cycles=1, scaling=True, rotations=True,
                                        elastic_deform=True, mirror=True,
                                        brightness=True, contrast=True,
                                        gamma=True, gaussian_noise=True)
        
        # Create and configure the Preprocessor class
pp = miscnn.Preprocessor(data_io, data_aug=data_aug,batch_size=10,analysis=""fullimage"")

    # Import standard U-Net architecture and Soft Dice
from miscnn.neural_network.architecture.unet.MiniNetMiscnn import Architecture


#model = MiniNet2


# In[5]:


from miscnn.neural_network.metrics import dice_soft_loss
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Input, Dense, Flatten

#import warnings
#warnings.filterwarnings('ignore')

#H =  256;
#W = 256;
#n_classes = 1
#input_xx = Input((H,W,1),name='img')
#input_xx = tf.placeholder(tf.float32,shape=[1,H,W,3], name = 'input')
#training_flag = True
#output_resize_factor = 1
#Mininet = MiniNet2(input_xx, n_classes, is_training=training_flag, upsampling=output_resize_factor)

MiniNet = Architecture(n_classes=1, l2=None, is_training=False, upsampling=2)

# Create a deep learning neural network model with a standard U-Net architecture
model = miscnn.Neural_Network(preprocessor=pp, architecture=MiniNet,
                                 metrics=[dice_soft, dice_coefficient],
                                learninig_rate=0.0001,loss=dice_coefficient_loss)#, gpu_number=1)

sample_list = data_io.get_indiceslist()	

print(""done"")


# In[8]:


cross_validation(sample_list, model, k_fold=2, epochs=200,
                    iterations=None, evaluation_path=output_path+""evaluation"",
                  draw_figures=False, run_detailed_evaluation=False,
                  callbacks=[], save_models=True, return_output=False)


# In[ ]:


#MiniNet


# In[ ]:


#model.model.summary()


# In[ ]:


model.model.save_weights('saved_model_MiniNet/weights')


# In[ ]:




`



Complete error log:
`Epoch 00001: val_loss improved from inf to 0.95021, saving model to Zip_files/Results/MiniNet_Training_1_MiniNet_GPU_notebookevaluation/fold_0/model.hdf5
Traceback (most recent call last):
  File ""script_werkend_MiniNet.py"", line 105, in <module>
    callbacks=[], save_models=True, return_output=False)
  File ""/data/mapit/Data_Joerik/miscnn/evaluation/cross_validation.py"", line 84, in cross_validation
    iterations=iterations, callbacks=cb_list)
  File ""/data/mapit/Data_Joerik/miscnn/neural_network/model.py"", line 269, in evaluate
    max_queue_size=self.batch_queue_size)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 673, in fit
    initial_epoch=initial_epoch)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py"", line 1433, in fit_generator
    steps_name='steps_per_epoch')
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py"", line 331, in model_iteration
    callbacks.on_epoch_end(epoch, epoch_logs)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 311, in on_epoch_end
    callback.on_epoch_end(epoch, logs)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 969, in on_epoch_end
    self._save_model(epoch=epoch, logs=logs)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/callbacks.py"", line 1000, in _save_model
    self.model.save(filepath, overwrite=True)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py"", line 1211, in save
    saving.save_model(self, filepath, overwrite, include_optimizer, save_format)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py"", line 113, in save_model
    model, filepath, overwrite, include_optimizer)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/keras/saving/hdf5_format.py"", line 101, in save_model_to_hdf5
    default=serialization.get_json_type).encode('utf8')
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/json/__init__.py"", line 238, in dumps
    **kw).encode(obj)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/home/mapit/anaconda3/envs/MiniNet/lib/python3.7/site-packages/tensorflow/python/util/serialization.py"", line 69, in get_json_type
    raise TypeError('Not JSON Serializable:', obj)
TypeError: ('Not JSON Serializable:', b'\n\x15resize/ResizeBilinear\x12\x0eResizeBilinear\x1a\x17conv2d_transpose_1/Relu\x1a\x0bresize/size*\x13\n\ralign_corners\x12\x02(\x01*\x18\n\x12half_pixel_centers\x12\x02(\x00*\x07\n\x01T\x12\x020\x01')
`
"
50458,C API does not appear to register GPU XLA platform correctly,"### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Yes
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:       not mobile device.
-   **TensorFlow installed from (source or binary)**: both
-   **TensorFlow version (use command below)**: 2.4.1, 2.5.0, master branch(as of 6/24)
        ---tested in standard 2.5.0 libtensorflow tarball as well as rebuilt source
-   **Python version**: C_API bug
-   **Bazel version (if compiling from source)**: 3.7.2
            == bazel version  ===============================================
            Build label: 3.7.2- (@non-git)
            Build time: Mon May 24 14:27:25 2021 (1621866445)
            Build timestamp: 1621866445
            Build timestamp as int: 1621866445
-   **GCC/Compiler version (if compiling from source)**:
        gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)
-   **CUDA/cuDNN version**:
       11.0, libcudnn.so.8.0.5
-   **GPU model and memory**:
        Nvidia V100 16G


### Describe the problem
The XLA_GPU ops do not appear to be registered correctly in any recent version of libtensorflow.so.  This includes the standard 2.5.0 tarball, the tf-nightly as of this morning as well as my own compiles from 2.4.1 (patched) 2.5.0 and the master branch as of a day ago.
The enclosed program demonstrates the problem.  Running it with a saved-model in the current directory
called ""saved_model"" results in a failure with the error messages enclosed below.  This problem
is not affected by any environment variable settings.  setting ""TF_XLA_FLAGS=--tf_xla_enable_xla_devices""
will result in the XLA_CPU device being registered, but not the XLA_GPU device.
In my environment all versions of tf-python register XLA ops correctly and do XLA just fine.

```

**Standalone code to reproduce the issue**
    ```c++
    #include ""tensorflow/c/c_api.h""
    #include ""tensorflow/c/c_api_experimental.h""
    #include <iostream>

    main()
    {
        auto sessionOpts = TF_NewSessionOptions();
        auto graph = TF_NewGraph();
        auto meta_graph_def = TF_NewBuffer();
        auto status = TF_NewStatus();
        const char *tags[] = {""serve""};

        TF_EnableXLACompilation(sessionOpts, 1u);   // All other methods of doing this also seem to fail

        auto session = TF_LoadSessionFromSavedModel(
            sessionOpts, /* RunOptions */ nullptr,
            ""saved_model"", tags,  1, graph, meta_graph_def, status);

        TF_DeleteSession(session, status);
        TF_DeleteSessionOptions(sessionOpts);
        TF_DeleteStatus(status);
        TF_DeleteBuffer(meta_graph_def);
        TF_DeleteGraph(graph);

        return 0;
    }
    ```

**Other info / logs** 

2021-06-25 06:40:17.652978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-06-25 06:40:17.715316: I tensorflow/cc/saved_model/reader.cc:32] Reading SavedModel from: saved_model
2021-06-25 06:40:18.241233: I tensorflow/cc/saved_model/reader.cc:55] Reading meta graph with tags { serve }
2021-06-25 06:40:18.241281: I tensorflow/cc/saved_model/reader.cc:93] Reading SavedModel debug info (if present) from: saved_model
2021-06-25 06:40:18.241348: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-25 06:40:18.244220: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-06-25 06:40:18.245683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1
2021-06-25 06:40:18.335973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties:
pciBusID: 0000:0f:00.0 name: Tesla V100-DGXS-16GB computeCapability: 7.0
coreClock: 1.53GHz coreCount: 80 deviceMemorySize: 15.78GiB deviceMemoryBandwidth: 836.37GiB/s
2021-06-25 06:40:18.336004: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-06-25 06:40:18.339161: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11
2021-06-25 06:40:18.339198: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11
2021-06-25 06:40:18.341082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10
2021-06-25 06:40:18.341400: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10
2021-06-25 06:40:18.343734: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10
2021-06-25 06:40:18.344506: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11
2021-06-25 06:40:18.344662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8
2021-06-25 06:40:18.349429: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-06-25 06:40:18.349466: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0
2021-06-25 06:40:18.926592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-25 06:40:18.926630: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0
2021-06-25 06:40:18.926642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N
2021-06-25 06:40:18.930171: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14783 MB memory) -> physical GPU (device: 0, name: Tesla V100-DGXS-16GB, pci bus id: 0000:0f:00.0, compute capability: 7.0)
2021-06-25 06:40:20.699787: I tensorflow/cc/saved_model/loader.cc:206] Restoring SavedModel bundle.
2021-06-25 06:40:20.971245: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2198735000 Hz
2021-06-25 06:40:24.264247: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at xla_ops.cc:406 : Not found: could not find registered platform with id: 0x7f9b890fdb28
2021-06-25 06:40:24.273317: I tensorflow/cc/saved_model/loader.cc:277] SavedModel load for tags { serve }; Status: fail: Not found: 2 root error(s) found.
  (0) Not found: could not find registered platform with id: 0x7f9b890fdb28
         [[{{node cluster_0_1/xla_compile}}]]
         [[cluster_0_1/data_as_ctrl/_2]]
  (1) Not found: could not find registered platform with id: 0x7f9b890fdb28
         [[{{node cluster_0_1/xla_compile}}]]
0 successful operations.
0 derived errors ignored.. Took 6558004 microseconds.

```"
50457,Tensorflow failed to build with ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading MSVCpackage 'tensorflow/tools/pip_package'  on windows with MSVC,"Hi All,

Tensorflow failed to build with ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in F:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl: in F:/gitp/tensorflow/tensorflow/tensorflow/core/platform/rules_cc.bzl: in F:/gitp/tensorflow/tensorflow/tensorflow/core/platform/default/rules_cc.bzl: in F:/bazeltemp/2powapgf/external/rules_cc/cc/defs.bzl: Extension file 'cc/private/rules_impl/cc_flags_supplier.bzl' has errors on windows with MSVC. This issue can be reproduced on latest version 32634e9, Could you please look at this issue?

Repro steps:
1. git clone https://github.com/tensorflow/tensorflow F:\gitP\tensorflow\tensorflow
2. git -C ""F:\gitP\tensorflow\tensorflow"" submodule update --init --recursive
3. Apply tensorflow_set_bazel_version.patch
4. Open a vs2019 x64 command prompt
5. cd F:\gitP\tensorflow\tensorflow
6. pip3 install six numpy wheel
7. pip3 install keras_applications==1.0.6 --no-deps
8. pip3 install keras_preprocessing==1.0.5 --no-deps
9. set PATH=F:\gitP\tensorflow\tensorflow\..\tools;%path%
10. set PATH=F:\gitP\tensorflow\tensorflow\..\tools\msys64\usr\bin;%path%
11. yes """" 2>nul | python ./configure.py
12. set BAZEL_VC=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC
13. set BAZEL_VC_FULL_VERSION=14.29.30037
14. bazel --output_user_root F:\bazelTemp build --config=opt --subcommands //tensorflow/tools/pip_package:build_pip_package

Patch file:
[tensorflow_set_bazel_version.zip](https://github.com/tensorflow/tensorflow/files/6715357/tensorflow_set_bazel_version.zip)

Build log:
[build.log](https://github.com/tensorflow/tensorflow/files/6715358/build.log)

Error info:
WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/4b20d65ea59434890477d1f24f4b660fd5a0491f.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
...
ERROR: F:/bazeltemp/2powapgf/external/rules_cc/cc/private/rules_impl/cc_flags_supplier.bzl:28:21: unexpected keyword 'incompatible_use_toolchain_transition', for call to function rule(implementation, test = False, attrs = None, outputs = None, executable = False, output_to_genfiles = False, fragments = [], host_fragments = [], _skylark_testable = False, toolchains = [], doc = '', provides = [], exec_compatible_with = [], analysis_test = False, build_setting = None, cfg = None)
ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': error loading package 'tensorflow/tools/pip_package': in F:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl: in F:/gitp/tensorflow/tensorflow/tensorflow/core/platform/rules_cc.bzl: in F:/gitp/tensorflow/tensorflow/tensorflow/core/platform/default/rules_cc.bzl: in F:/bazeltemp/2powapgf/external/rules_cc/cc/defs.bzl: Extension file 'cc/private/rules_impl/cc_flags_supplier.bzl' has errors
WARNING: Target pattern parsing failed.
ERROR: error loading package 'tensorflow/tools/pip_package': in F:/gitp/tensorflow/tensorflow/tensorflow/tensorflow.bzl: in F:/gitp/tensorflow/tensorflow/tensorflow/core/platform/rules_cc.bzl: in F:/gitp/tensorflow/tensorflow/tensorflow/core/platform/default/rules_cc.bzl: in F:/bazeltemp/2powapgf/external/rules_cc/cc/defs.bzl: Extension file 'cc/private/rules_impl/cc_flags_supplier.bzl' has errors
INFO: Elapsed time: 19.654s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (1 packages loaded)
FAILED: Build did NOT complete successfully (1 packages loaded)
##[debug] Command #6 exited with code [1].
##[error] Detected error code [1]."
50455,De-standardizing the output in LSTM model if the input was not standardized,"I was using the LSTM model for time series prediction when I observed that the model standardises the input when given non-standard input. While predicting, the output is also standardized, but there seems no way to de-standardize. 
The current solution is to standardize beforehand (using sklearn or any other) and perform the invert operation on the output.


**System information**
- TensorFlow version : 2.5.0

**The feature and the current behaviour.**
There seems no way to de-standardize the output. One way is to add another attribute that de-standardizes when called or make a flag in the model, which tell whether the input is standardised or not, and if it's not, then standardize and return the de-standardized output.

**Change the current API**
I am asking for an attribute addition, at the least.

**Other Info**
The following links might be of some help:
[https://stackoverflow.com/a/49398454/13428777](https://stackoverflow.com/a/49398454/13428777)
[https://stackoverflow.com/questions/44100837/disable-keras-batch-normalization-standardization](https://stackoverflow.com/questions/44100837/disable-keras-batch-normalization-standardization)"
50454,assign_add() not working on GPU?,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution: Linux Ubuntu 16.04
- TensorFlow version: 2.5.0
- Python version: 3.9
- CUDA/cuDNN version: 11.2
- GPU model and memory: 4x GeForce 1080 Ti

**Describe the current behavior**

I have implemented a patch for optimizers which should perform simple gradient accumulation (see [link on stackoverflow](https://stackoverflow.com/questions/68121006/why-are-these-gradient-accumulation-implementations-not-working)).

The interesting observation here is that using just `assign()`, the implementation seems to work (on CPU as well as on GPU). Obviously, this would not accumulate gradients and is therefore pointless.

However, using `assign_add()`, as it would be required for GA, the model does not converge anymore if I run the training with one GPU. I have also tried to work around this by computing `acc_grad[i].assign(acc_grad[i] + new_grad[i] / n)` but this is not working either.

![image](https://user-images.githubusercontent.com/43335432/123426304-8f08f500-d5c3-11eb-8bbf-6c5a5544d926.png)

**Describe the expected behavior**

If `assign_add()` worked here, the model should do at least _something_ but in the best case start to converge.

**Standalone code to reproduce the issue**

It's not entirely standalone but the code below would be use as follows:

```python
model.build()
optimizer = get_patched_optimizer(optimizer, n, trainable_variables)
model.compile(optimizer=optimizer)
```

where we have

```python
class _GradientAccumulationPatch:

    def __init__(
        self,
        n: int,
        orig_apply_gradients,
        trainable_variables
    ):
        self.n = tf.constant(n, dtype=tf.int64)
        policy = tf.keras.mixed_precision.global_policy()
        self.variable_dtype = policy.variable_dtype
        self.accu_gradients = [
            tf.Variable(
                tf.zeros(g.shape, dtype=g.dtype),
            ) for g in trainable_variables
        ]

        self._current_step = tf.Variable(0, dtype=tf.int64)
        self._orig_apply_gradients = orig_apply_gradients

    def apply_gradients(self, grads_and_vars, *args, **kwargs):

        can_apply = self._can_apply_on_next_step()
        # 1.0 whenever we want to apply gradients; 0.0 otherwise
        apply = tf.cast(can_apply, dtype=self.variable_dtype)
        # Will be 0.0 if apply is 1.0 and vice versa
        keep = tf.cast(tf.logical_not(can_apply), dtype=self.variable_dtype)

        grads_and_vars = list(grads_and_vars)
        gradients = [grad for (grad, _) in grads_and_vars]
        trainable_variables = [var for (_, var) in grads_and_vars]

        # Accumulate gradients
        for i, grad in enumerate(gradients):
            # FIXME should be assign_add()
            self.accu_gradients[i].assign(grad / tf.cast(self.n, dtype=grad.dtype))

        # Multiply each gradient with our apply-signal
        final_gradients = [grad * apply for grad in self.accu_gradients]

        self._orig_apply_gradients(zip(final_gradients, trainable_variables), *args, **kwargs)

        # This will reset our buffer whenever ""keep"" is 0.0
        for g in self.accu_gradients:
            g.assign(g * keep)

    def apply_accu_gradients(self, trainable_variables, *args, **kwargs):

        # Call the original apply_gradients() function
        self._orig_apply_gradients(zip(self.accu_gradients, trainable_variables), *args, **kwargs)

        # Reset all accumulated gradients to zero
        for i in range(len(self.accu_gradients)):
            self.accu_gradients[i].assign(tf.zeros_like(trainable_variables[i]))

    def _can_apply_on_next_step(self):
        """"""
        :return: True if gradients should be applied; False otherwise.
        """"""
        # Increment (always do this first)
        self._current_step.assign_add(1)
        count_mod_steps = tf.math.mod(self._current_step, self.n)
        return tf.equal(count_mod_steps, 0)


def get_patched_optimizer(optimizer, n, trainable_variables):
    """"""Patch optimizer for gradient accumulation.

    :param optimizer:
        The optimizer to patch.
    :param n:
        The number of accumulation steps before applying gradients.
    :param trainable_variables:
        Trainable parameters of the model
    :return:
        A patched patched optimizer for gradient accumulation.
    """"""
    accumulator = _GradientAccumulationPatch(
        n=n,
        orig_apply_gradients=optimizer.apply_gradients,
        trainable_variables=trainable_variables
    )

    # Replace the original function
    optimizer.apply_gradients = accumulator.apply_gradients

    return optimizer
```

----

### Update

I have now tried this on MNIST and it appears to work for that example. 

Any idea why this might not be working in my example above .. 😞 

<details>
<summary>MNIST example (click me)</summary>

```python
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers


class _GradientAccumulationPatch:

    def __init__(
        self,
        n: int,
        orig_apply_gradients,
        trainable_variables
    ):
        self.n = tf.constant(n, dtype=tf.int64)
        policy = tf.keras.mixed_precision.global_policy()
        self.variable_dtype = policy.variable_dtype
        self.accu_gradients = [
            tf.Variable(
                tf.zeros(g.shape, dtype=g.dtype),
            ) for g in trainable_variables
        ]

        self._current_step = tf.Variable(0, dtype=tf.int64)
        self._orig_apply_gradients = orig_apply_gradients

    def apply_gradients(self, grads_and_vars, *args, **kwargs):

        can_apply = self._can_apply_on_next_step()
        # 1.0 whenever we want to apply gradients; 0.0 otherwise
        apply = tf.cast(can_apply, dtype=self.variable_dtype)
        # Will be 0.0 if apply is 1.0 and vice versa
        keep = tf.cast(tf.logical_not(can_apply), dtype=self.variable_dtype)

        grads_and_vars = list(grads_and_vars)
        gradients = [grad for (grad, _) in grads_and_vars]
        trainable_variables = [var for (_, var) in grads_and_vars]

        # Accumulate gradients
        for i, grad in enumerate(gradients):
            self.accu_gradients[i].assign_add(grad / tf.cast(self.n, dtype=self.variable_dtype))

        # Multiply each gradient with our apply-signal
        final_gradients = [grad * apply for grad in self.accu_gradients]

        self._orig_apply_gradients(zip(final_gradients, trainable_variables), *args, **kwargs)

        # This will reset our buffer whenever ""keep"" is 0.0
        for g in self.accu_gradients:
            g.assign(g * keep)

    def apply_accu_gradients(self, trainable_variables, *args, **kwargs):

        # Call the original apply_gradients() function
        self._orig_apply_gradients(zip(self.accu_gradients, trainable_variables), *args, **kwargs)

        # Reset all accumulated gradients to zero
        for i in range(len(self.accu_gradients)):
            self.accu_gradients[i].assign(tf.zeros_like(trainable_variables[i]))

    def _can_apply_on_next_step(self):
        """"""
        :return: True if gradients should be applied; False otherwise.
        """"""
        # Increment (always do this first)
        self._current_step.assign_add(1)
        count_mod_steps = tf.math.mod(self._current_step, self.n)
        return tf.equal(count_mod_steps, 0)


def get_patched_optimizer(optimizer, n, trainable_variables):
    """"""Patch optimizer for gradient accumulation.

    :param optimizer:
        The optimizer to patch.
    :param n:
        The number of accumulation steps before applying gradients.
    :param trainable_variables:
        Trainable parameters of the model
    :return:
        A patched patched optimizer for gradient accumulation.
    """"""
    accumulator = _GradientAccumulationPatch(
        n=n,
        orig_apply_gradients=optimizer.apply_gradients,
        trainable_variables=trainable_variables
    )

    # Replace the original function
    optimizer.apply_gradients = accumulator.apply_gradients

    return optimizer


def get_ffn_model(input_size: int, output_size: int, hidden_size: int = 64) -> keras.Model:
    inputs = layers.Input(shape=(input_size,))
    x = inputs
    x = layers.Dense(units=hidden_size, activation='tanh')(x)
    x = layers.Dense(units=hidden_size, activation='tanh')(x)
    x = layers.Dense(units=output_size, activation='softmax')(x)
    return keras.Model(inputs=inputs, outputs=x)


def make_dataset(inputs, targets, batch_size: int):
    def sample_generator_():
        while True:
            idx = np.random.randint(0, len(inputs))
            yield inputs[idx].flatten(), tf.one_hot(targets[idx], depth=num_classes)

    inputs = inputs.astype(np.float32) / 255.0
    inputs = np.expand_dims(inputs, axis=-1)
    num_classes = len(set(targets))

    input_shape = (np.prod(inputs[0].shape),)
    target_shape = (num_classes,)

    return tf.data.Dataset.from_generator(
        lambda: sample_generator_(),
        output_types=(tf.float32, tf.float32),
        output_shapes=(input_shape, target_shape)
    ).padded_batch(batch_size)


def main():
    train_batch_size = 1
    valid_batch_size = 10
    grad_acc_n = 10
    steps_per_epoch = 1000 # * grad_acc_n  # Make sure we have the same number of updates

    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
    train_data = make_dataset(x_train, y_train, batch_size=train_batch_size)
    valid_data = make_dataset(x_test, y_test, batch_size=valid_batch_size)

    input_size = train_data.element_spec[0].shape[-1]
    output_size = train_data.element_spec[1].shape[-1]

    model = get_ffn_model(input_size=input_size, output_size=output_size, hidden_size=128)
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
    optimizer = get_patched_optimizer(optimizer, n=grad_acc_n, trainable_variables=model.trainable_variables)

    model.compile(
        optimizer=optimizer,
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    model.fit(
        train_data,
        epochs=10,
        steps_per_epoch=steps_per_epoch // train_batch_size,
        validation_data=valid_data,
        validation_steps=10
    )


if __name__ == '__main__':
    main()
```
</details>"
50453,how to translate known tensor?,"hi,dear
In dist training, I must use **for** to get matrices concat, but I can not translate it to constant tensor,
the codes down for the bug reproduce
```
import tensorflow as tf
import numpy as np
np.random.seed(123)
tf.random.set_seed(123)

class MyModel(tf.keras.Model):
    def __init__(self,):
        super(MyModel,self).__init__()
    def call(self,inputs):
        @tf.function(autograph=True)
        def f():
            v = tf.constant((0,))
            for i in tf.range(3):
                tf.autograph.experimental.set_loop_options(
                shape_invariants=[(v, tf.TensorShape([None]))]
                )
                v = tf.concat((v, [i]), 0)
            return v
        print(tf.constant(f()))#bug
        
        return tf.math.reduce_sum(f())
strategy = tf.distribute.MirroredStrategy(
        devices=[""device:GPU:%d"" % i for i in range(2)],
        cross_device_ops=None)
train_dataset = tf.data.Dataset.from_tensor_slices((np.random.randint(0,12,size=(100,3)).astype(np.int32))).shuffle(10).batch(4)
train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)
with strategy.scope():
    model=MyModel()

def train_step(inputs):
    with tf.GradientTape() as tape:
        predictions = model(inputs, training=True)
    return predictions
@tf.function
def distributed_train_step(dataset_inputs):
    per_replica_losses = strategy.run(train_step, args=(dataset_inputs,))
    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,
                         axis=None)
p=0
for x in train_dist_dataset:
    p+=distributed_train_step(x)
```
I want to translate the known tensor to constant,how to deal with it ?
thx
"
50452,api_docs tensor incoherent sentence,"
## URL(s) with the issue:

https://www.tensorflow.org/api_docs/python/tf/Tensor 

## Description of issue (what needs changing):
The first code snippet in the URL is coming in between the sentence which makes it incoherent.

### Submit a pull request?

I am new to open-source and wish to fix it myself. Can you please direct me to the file wherein the ""view aliases"" and ""code snippets"" are included. In the definition given in https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/framework/ops.py#L264 would removing the newline character after first line in multiline comment solve the issue?  
"
50449,Load in-cluster config in tf.distribute.cluster_resolver.KubernetesClusterResolver,"
**System information**
- TensorFlow version (use command below): `2.5.0`

**Describe the current behavior**

  Currently the `KubernetesClusterResolver` class can only load configs from kube-config file:

https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/distribute/cluster_resolver/kubernetes_cluster_resolver.py#L97

**Describe the expected behavior**

  When running in worker pods in cluster, we want to be able to load configs using `.load_incluster_config()` ([example](https://github.com/kubernetes-client/python/blob/v17.17.0/examples/in_cluster_config.py#L55))

We can't use the `override_client` arg with `kubernetes` package either, because once installed the `ImportError` does not happen 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):

  Yes

- Briefly describe your candidate solution(if contributing):

  Try to do `k8sconfig.load_incluster_config()` first, if failed then do `k8sconfig.load_kube_config()`

"
50448,Tflite: issue with dilated convolution in a dynamic shape CNN,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (for model creation)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: OnePlus 8 (Android 11), Sony Xperia 10 (Android 10)
- TensorFlow installed from (source or binary): from pip
- TensorFlow version (use command below): 2.5.0 (v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0)
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.3
- GPU model and memory: Geforce RTX 3060


**Describe the current behavior**

I'm trying to use a pre-trained model for image processing in my android app. The input shape is [None, None, None, 3]
and the output shape is inferred from the input shape. My issue happens when the model has a convolution layer with 
dilation (same models without dilation work fine).

The android code I use for inference is has the following part:
    
    interpreter.resizeInput(0, new int[]{1, height, width, 3});
    interpreter.allocateTensors();
    Tensor tensor = interpreter.getOutputTensor(0);
    int[] outShape = tensor.shape();

My issue is that the content of outShape is always [1, 1, 1, outChannels]. I'm also not able to recover the output
of the model with interpreter.runForMultipleInputsOutputs even if I provide arrays with the correct output sizes.

**Describe the expected behavior**

The content of shape should depend on the input (in the example I provide it should be [1, height, width, 16])


**Standalone code to reproduce the issue**

Here is a link to minimal models which show the issue I described:

[dilated_conv.zip](https://github.com/tensorflow/tensorflow/files/6712927/dilated_conv.zip)

I've tried 3 different quantization options but it doesn't change anything.

Here is the python code I used to produce the tflite models. 

    import tensorflow as tf
    import numpy as np
    
    
    def convert(model, fname, quantize_level=0):
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        if quantize_level > 0:
            converter.optimizations = [tf.lite.Optimize.DEFAULT]
            if quantize_level > 1:
                converter.target_spec.supported_types = [tf.float16]
                fname = ""{}_quantized_f16"".format(fname)
            else:
                fname = ""{}_quantized_default"".format(fname)
    
        tflite_model = converter.convert()
        with open(""{}.tflite"".format(fname), 'wb') as f:
            f.write(tflite_model)
    
    
    x = tf.keras.layers.Input([None, None, 3], dtype=tf.float32)
    conv = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding=""same"", dilation_rate=6, activation=None)
    conv.build(x.shape)
    conv.set_weights([np.random.rand(3, 3, 3, 16), np.random.rand(16)])
    y = conv(x)
    model = tf.keras.Model(x, y)
    
    for i in range(3):
        convert(model, ""dilated_conv"", quantize_level=i)

I have also tried to add a reshape layer but it doesn't make any difference:

    x = tf.keras.layers.Input([None, None, 3], dtype=tf.float32)
    conv = tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), padding=""same"", dilation_rate=6, activation=None)
    conv.build(x.shape)
    conv.set_weights([np.random.rand(3, 3, 3, 16), np.random.rand(16)])
    y = conv(x)
    y = tf.reshape(y, tf.shape(x))
    model = tf.keras.Model(x, y)

For the android part, I am using the java API and the issue is there with tflite versions 2.2.0, 2.3.0 and 2.5.0.
Just creating an interpreter from one of the above models (without GPU or NNApi delegate) and trying to resize the 
input shows the behavior I described. (if you feed it an image it will do some computations and produce an output with the
wrong shape)"
50447,Why the output tensor device type is GPU even though I set the HostMemory attr?,"Hi folks,
I am trying to create an Op recently. This is part of my Op kernel builder:
```
REGISTER_KERNEL_BUILDER(Name(""Test"").Device(::tensorflow::DEVICE_GPU)
                                                      .HostMemory(""output""),
                        TestOp);

REGISTER_OP(""Test"")
    .Input(""tensor: T"")
    .Output(""output: T"")
    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {
      ::tensorflow::shape_inference::ShapeHandle output;
      TF_RETURN_IF_ERROR(c->ReplaceDim(c->input(0), 0, c->UnknownDim(), &output));
      c->set_output(0, output);
      return ::tensorflow::Status::OK();
    })
```
For my Op, it's expected the input is on the device while the output will be on the host. However, after I call my python Op and obtain the ```output``` tensor, I tried to print the device type ```print(output.device)```. I found that the output tensor is on the ```GPU```, which is supposed to be ```CPU``` for my case?

I also tried to set the attribute manually inside the Op_kernel.
```
::tensorflow::AllocatorAttributes alloc_attrs;
alloc_attrs.set_on_host(true);
OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, result_shape, &output), done);
```
But the result ```print(output.device)``` is still on the device(GPU). Anyone can help me figure out this issue?"
50445,"TFLite converts splits of int64 tensors, but fails to evaluate the conversion","
**System information**
- OS Platform and Distribution MacOS 
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from  binary:
- TensorFlow version 2.5.0:
- Python version: 3.8

**Standalone code to reproduce the issue**

```
def test_split_export():
    @tf.function
    def test_fn(inp: tf.Tensor) -> tf.Tensor:
        return tf.concat(tf.split(inp, 3, axis=1), axis=1)

    concrete_f = test_fn.get_concrete_function(tf.TensorSpec((None, 3), tf.int64))
    converter = tf.lite.TFLiteConverter.from_concrete_functions([concrete_f])
    tflite_model = converter.convert()
    interpreter = tf.lite.Interpreter(model_content=tflite_model)
    interpreter.allocate_tensors()

```
**Describe the current behavior**

The example converts fine, but attempt to instantiate evaluation breaks in allocate_tensors with the error:

E     RuntimeError: tensorflow/lite/kernels/split.cc:90 input_type == kTfLiteFloat32 || input_type == kTfLiteUInt8 || input_type == kTfLiteInt8 || input_type == kTfLiteInt16 || input_type == kTfLiteInt32 was not true.Node number 0 (SPLIT) failed to prepare.

**Describe the expected behavior**
TFLite1 did not produce this error. Ideally behavior should be identical to TFLite1 (I think it downconverted all int64s to int32. If TFLite does not do downconversion, then the split should be able to support int64 tensors.(

Thank you."
50443,OpenImageDebugger ,Can be somehow TensorFlow used with OpenImageDebugger (Linux platform)? Is there any example of how to use it together for debugging purposes?
50440,"tf.profiler.experimental.Trace only works with name is ""TraceContext""","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.1
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 10.1
- GPU model and memory: 1080ti

**Describe the current behavior**
The name argument to `Trace` has to be exactly `""TraceContext""` otherwise the ""overview"" page doesn't give any info. Interesting the other pages still work and have all the right data. But the title page is broken, so most users wouldn't think to look at the other pages and just assume the whole thing was broken.

**Describe the expected behavior**
It works with any name

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
```
from datetime import datetime
import tensorflow as tf

b = tf.random.uniform([32,28,28,1])

model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(10, activation='softmax')
])

def test(trace_name):
    logs = f""logs/{trace_name}"" + datetime.now().strftime(""%Y%m%d-%H%M%S"")

    epoch = 0
    tf.profiler.experimental.start(logs)
    for step in range(100):
        if 1 < step < 100:
            with tf.profiler.experimental.Trace(trace_name):
                model(b)
        else:
            model(b)
    tf.profiler.experimental.stop()

test(""TraceContext"") # works fine
test(""Broken"") # no step marker observed error
```

**Other info / logs**
![trace_context](https://user-images.githubusercontent.com/4010770/123279273-b4b7d080-d4d5-11eb-9e75-e91cff2982cc.png)
![broken](https://user-images.githubusercontent.com/4010770/123279286-b6819400-d4d5-11eb-8515-8b5998658a00.png)

"
50439,Android Studio is unable to Load https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.


After cloning the repository https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification/android when I try to open the project in Android Studio version 4.2, the left window of IDE shows loading but it never loads.."
50438,Code from TF2.4.1 does not work in TF2.5,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.9
- CUDA/cuDNN version: 11.2/8
- GPU model and memory: RTX3070

**Describe the current behavior**
Install tensorflow 2.4 through
```
pip install tensorflow==2.4.1
```

Notice that the following code works: 
```python
import tensorflow.experimental.numpy as tnp

arange = tnp.arange(10, dtype=tnp.float32)
pos_vs = tnp.tile(arange, (4, 10, 1))
pos_us = tnp.transpose(pos_vs, [0, 2, 1])
print(pos_us)
```

Install tensorflow 2.5 through
```
pip install tensorflow==2.5.0
```

The code above fails with the following error:
```
AttributeError: 
        'EagerTensor' object has no attribute 'reshape'.
        If you are looking for numpy-related methods, please run the following:
        import tensorflow.python.ops.numpy_ops.np_config
        np_config.enable_numpy_behavior()
```

**Describe the expected behavior**
The code in TF2.5 should work just fine, as no related breaking change has been introduced.

Gist with link to colab: https://gist.github.com/benjs/6955171d577536fdf0df3528ea673674
"
50437,An macro defination mistake in test_benchmark.h,"branch r2.5
tensorflow/tensorflow/core/platform/default/test_benchmark.h: 32
```cpp
#define BENCHMARK(n)                                            \
  static ::tensorflow::testing::Benchmark* TF_BENCHMARK_CONCAT( \
      __benchmark_, n, __LINE__) TF_ATTRIBUTE_UNUSED =          \
      (new ::tensorflow::testing::Benchmark(#n, (n)))
#define TF_BENCHMARK_CONCAT(a, b, c) TF_BENCHMARK_CONCAT2(a, b, c)
#define TF_BENCHMARK_CONCAT2(a, b, c) a##b##c
```
conv_benchmark使用宏时，其形式参数也包含宏

```cpp
BENCHMARK(BM_NAME(BM_Conv2D, type, N, H, W, C, FW, FH, FC))->Arg(/*unused arg*/ 1);
```
实际上BM_NAME并没有被展开，此benchmar test 的名字将变成
e.g. BM_NAME(BM_Conv2D, cpu, 16, 1, 8, 8, 16, 16, 8)

提供一层封装才会展开
```cpp
#define _BENCHMARK(n)                                           \
  static ::tensorflow::testing::Benchmark* TF_BENCHMARK_CONCAT( \
      __benchmark_, n, __LINE__) TF_ATTRIBUTE_UNUSED =          \
      (new ::tensorflow::testing::Benchmark(#n, (n)))
#define TF_BENCHMARK_CONCAT(a, b, c) TF_BENCHMARK_CONCAT2(a, b, c)
#define TF_BENCHMARK_CONCAT2(a, b, c) a##b##c

#define BENCHMARK(n) _BENCHMARK(n)
```"
50436,DistributedDataset does not work on Python 3.8,"**System information**

- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `no`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `docker ubuntu:focal`
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `no`
- TensorFlow installed from (source or binary): `binary`
- TensorFlow version (use command below): `v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0`
- Python version: `3.8.5`
- Bazel version (if compiling from source): `n/a`
- GCC/Compiler version (if compiling from source): `n/a`
- CUDA/cuDNN version: `n/a`
- GPU model and memory: `n/a`

**Describe the current behavior**

When I use Python 3.8.5,

MirroredStrategy:

```
Exception ignored in: <function Pool.__del__ at 0x7fbd5c04d280>
Traceback (most recent call last):
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 268, in __del__
  File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 362, in put
AttributeError: 'NoneType' object has no attribute 'dumps'
```

MultiWorkerMirroredStrategy:

```
terminate called without an active exception
```

Nothing happens in Python 3.6.9.

**Describe the expected behavior**

The process exits with code 0.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import sys
import tensorflow as tf


print(sys.version_info)
print(tf.version.GIT_VERSION, tf.version.VERSION)


def distribute_dataset_fn(input_context: tf.distribute.InputContext):
  return tf.data.Dataset.from_tensors(([1.0])).repeat(64).batch(2)


@tf.function
def step(x):
  return x


if __name__ == '__main__':
  strategy = tf.distribute.MultiWorkerMirroredStrategy()
  #strategy = tf.distribute.MirroredStrategy()
  with strategy.scope():
    dist_ds = strategy.distribute_datasets_from_function(distribute_dataset_fn)

  for x in dist_ds:
    strategy.run(step, args=[x])
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

```
2021-06-24 04:46:17.224210: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
2021-06-24 04:46:17.280529: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-24 04:46:17.281658: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199970000 Hz
sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)
v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
Exception ignored in: <function Pool.__del__ at 0x7fbd5c04d280>
Traceback (most recent call last):
  File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 268, in __del__
  File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 362, in put
AttributeError: 'NoneType' object has no attribute 'dumps'
```

```
2021-06-24 04:52:19.023657: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-24 04:52:19.031611: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> tf-multi-worker-v5xfh-worker-0.pipelines.svc:2222, 1 -> tf-multi-worker-v5xfh-worker-1.pipelines.svc:2222}
2021-06-24 04:52:19.033039: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://tf-multi-worker-v5xfh-worker-0.pipelines.svc:2222
2021-06-24 04:52:20.069924: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-24 04:52:20.071033: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199970000 Hz
sys.version_info(major=3, minor=8, micro=5, releaselevel='final', serial=0)
v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
terminate called without an active exception
```"
50433,Saved Keras model cannot be loaded using hub.KerasLayer API,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): `Yes`
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): `Linux 8d350c6303d5 5.4.104+ #1 SMP Sat Jun 5 09:50:34 PDT 2021 x86_64 x86_64 x86_64 GNU/Linux`

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: `N/A`
- TensorFlow installed from (source or binary): `binary`
- TensorFlow version (use command below): `v2.5.0-0-ga4dfb8d1a71 2.5.0`
- Python version: `sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)`
- Bazel version (if compiling from source):  `N/A`
- GCC/Compiler version (if compiling from source):  `N/A`
- CUDA/cuDNN version:  `N/A`
- GPU model and memory:  `N/A`

**Describe the current behavior**

After creating a simple `tf.keras.Model` with `IntegerLookup`, the model can be saved using `save` API but cannot be load using `hub.KerasLayer` API. It throws the following error:

```

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-9-a9c000ccc830> in <module>()
----> 1 loaded_model = hub.KerasLayer('./saved_model')

/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py in __init__(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)
    170 
    171     self._callable = self._get_callable()
--> 172     self._setup_layer(trainable, **kwargs)
    173 
    174   def _setup_layer(self, trainable=False, **kwargs):

/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py in _setup_layer(self, trainable, **kwargs)
    187       for v in self._func.variables:
    188         if id(v) not in trainable_variables:
--> 189           self._add_existing_weight(v, trainable=False)
    190 
    191     # Forward the callable's regularization losses (if any).

/usr/local/lib/python3.7/dist-packages/tensorflow_hub/keras_layer.py in _add_existing_weight(self, weight, trainable)
    201     """"""Calls add_weight() to register but not create an existing weight.""""""
    202     if trainable is None: trainable = weight.trainable
--> 203     self.add_weight(name=weight.name, shape=weight.shape, dtype=weight.dtype,
    204                     trainable=trainable, getter=lambda *_, **__: weight)
    205 

AttributeError: 'NoneType' object has no attribute 'name'
```

**Describe the expected behavior**

Based on [Tensorflow's doc](https://www.tensorflow.org/hub/tf2_saved_model#saving_from_keras), the expected behavior is:
> Starting with TensorFlow 2, `tf.keras.Model.save()` and `tf.keras.models.save_model()` default to the SavedModel format (not HDF5). The resulting SavedModels that can be used with `hub.load()`, `hub.KerasLayer` and similar adapters for other high-level APIs as they become available.

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Please find the gist here: https://gist.github.com/bxshi/b0b99c4ed537fd57c05e2b1ef8964c52 

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50429,Add Spresense instructions in README.md files on each examples of lite micro.,"@tensorflow/micro

**System information**
- Target platform : Sony Spresense

**Describe the problem**
There is no instructions for Sony Spresense board in README.md files of each examples in lite micro.

**Please provide the exact sequence of commands/steps when you ran into the problem**
Nothing. Just README.md file.
"
50426,Training with tf.data API gives different results than passing in data directly using Numpy,"
**System information**
- OS Platform and Distribution : **Linux Ubuntu 18.04**
- TensorFlow installed from  : **source**
- TensorFlow version : **2.5.0**  
- Python version: **3.7.10**
- CUDA/cuDNN version: **CUDA 11.0/cuDNN 8**
- GPU model and memory: **Tesla T4 15GB**

**Describe the current behavior**
Hello,
I have a big difference in the results when I train with the tf.data API compared to when I pass the data directly using Numpy.
Indeed, after 5 epochs, training with tf.data gives the following results 
`Epoch 5/5
690/690 [==============================] - 28s 41ms/step - loss: 1.0746 - accuracy: 0.4408 - val_loss: 1.1136 - val_accuracy: 0.3683`
compared to these results when I train with Numpy:
`Epoch 5/5
690/690 [==============================] - 28s 41ms/step - loss: 0.3204 - accuracy: 0.8693 - val_loss: 5.4233 - val_accuracy: 0.3453`


**Describe the expected behavior**
The expected behavior is to have no difference between tf.data API and Numpy data.


**Standalone code to reproduce the issue**
Here is a link to Google Colab to reproduce the issue: [Google Colab](https://colab.research.google.com/drive/1Ffq2qB7A5jg_ATBUN1qad-3QMDLD2a2p?usp=sharing)

If you can't run Google Colab, here is the code:  [Code to reproduce ](https://gist.github.com/oubathAI/4e20d7b1d7b17b5b4a30a31c3cc8983a)

To download the data, here are the links: 

[train_inputs](https://ec2-spotter.s3.eu-west-1.amazonaws.com/tmp/train_inputs.npy.pkl )
[train_labels](https://ec2-spotter.s3.eu-west-1.amazonaws.com/tmp/train_labels.npy.pkl )
[val_inputs](https://ec2-spotter.s3.eu-west-1.amazonaws.com/tmp/val_inputs.npy.pkl )
[val_labels](https://ec2-spotter.s3.eu-west-1.amazonaws.com/tmp/val_labels.npy.pkl)

**Other info / logs** 
Full training logs using tf.data 
```
Epoch 1/5
690/690 [==============================] - 64s 43ms/step - loss: 1.3643 - accuracy: 0.3942 - val_loss: 1.0925 - val_accuracy: 0.4102
Epoch 2/5
690/690 [==============================] - 28s 40ms/step - loss: 1.0938 - accuracy: 0.3999 - val_loss: 1.1260 - val_accuracy: 0.4157
Epoch 3/5
690/690 [==============================] - 28s 41ms/step - loss: 1.0866 - accuracy: 0.4276 - val_loss: 1.0832 - val_accuracy: 0.4199
Epoch 4/5
690/690 [==============================] - 28s 40ms/step - loss: 1.0871 - accuracy: 0.4186 - val_loss: 1.0825 - val_accuracy: 0.4062
Epoch 5/5
690/690 [==============================] - 28s 41ms/step - loss: 1.0746 - accuracy: 0.4408 - val_loss: 1.1136 - val_accuracy: 0.3683
```

Full training logs using Numpy array
```
Epoch 1/5
690/690 [==============================] - 33s 44ms/step - loss: 1.1418 - accuracy: 0.5239 - val_loss: 1.3560 - val_accuracy: 0.3668
Epoch 2/5
690/690 [==============================] - 28s 41ms/step - loss: 0.6565 - accuracy: 0.7227 - val_loss: 2.4518 - val_accuracy: 0.3578
Epoch 3/5
690/690 [==============================] - 28s 41ms/step - loss: 0.4320 - accuracy: 0.8248 - val_loss: 4.3772 - val_accuracy: 0.3306
Epoch 4/5
690/690 [==============================] - 28s 41ms/step - loss: 0.3603 - accuracy: 0.8544 - val_loss: 4.9844 - val_accuracy: 0.3387
Epoch 5/5
690/690 [==============================] - 28s 41ms/step - loss: 0.3204 - accuracy: 0.8693 - val_loss: 5.4233 - val_accuracy: 0.3453
```
"
50425,Cannot Compile tensorflow 1.14.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOs 11.2.3
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14.0
- Python version: 3.7
- Installed using virtualenv? pip? conda?: pip


**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**
One script I need to use has the line `from tensorflow.examples.tutorials.mnist import input_data`. Since the tutorials folder is removed from tensorflow2, I activated another environment to use Python 3.7 and installed tensorflow 1.14.0 using `pip3 install https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.14.0-py3-none-any.whl
` found from [https://stackoverflow.com/questions/63059979/cannot-install-tensorflow-1-x](https://stackoverflow.com/questions/63059979/cannot-install-tensorflow-1-x).

However, when running the script, I have an error saying `ModuleNotFoundError: No module named 'keras_applications', but I do have installed keras.

I found a related issue here [https://github.com/tensorflow/tensorflow/issues/21518](https://github.com/tensorflow/tensorflow/issues/21518) which was posted quite a while ago. Based on this pose, I ran successfully
```
pip install keras_applications==1.0.4 --no-deps
pip install keras_preprocessing==1.0.2 --no-deps
pip install h5py==2.8.0
```
but encountered another error saying `ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`` Below is my traceback.

**Any other info / logs**
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/__init__.py"", line 3, in <module>
    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
ModuleNotFoundError: No module named 'tensorflow.keras.layers.experimental.preprocessing'
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/IPython/core/interactiveshell.py"", line 2869, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""<ipython-input-2-c54fbe678627>"", line 1, in <module>
    runfile('/Users/zhli12/Desktop/scGAIN/example.py', wdir='/Users/zhli12/Desktop/scGAIN')
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/_pydev_bundle/pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/Users/zh12/Desktop/scGAIN/example.py"", line 25, in <module>
    from keras import applications
  File ""/Applications/PyCharm CE.app/Contents/plugins/python-ce/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 21, in do_import
    module = self._system_import(name, *args, **kwargs)
  File ""/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/__init__.py"", line 6, in <module>
    'Keras requires TensorFlow 2.2 or higher. '
ImportError: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`
"
50423,“Cloud TPU” console spam on every TensorFlow import,"**System information**

  *  Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
  *  OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
  *  Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
  *  TensorFlow installed from (source or binary): binary
  *  TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
  *  Python version: Python 3.9.5
  *  Bazel version (if compiling from source): N/A
  *  GCC/Compiler version (if compiling from source): N/A
  *  CUDA/cuDNN version: N/A
  *  GPU model and memory: N/A

**Describe the current behavior**

Importing TensorFlow prints an unnecessary and unhelpful warning:

   ` tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.`

**Describe the expected behavior**

Importing TensorFlow should not print any messages about Cloud TPUs.
This is a normal desktop installation that doesn’t have anything to do
with TPUs, and doesn’t need them.

**Code to reproduce the issue**

```
$ python -c ""import logging; logging.basicConfig(level=logging.DEBUG); import tensorflow""
DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.
DEBUG:h5py._conv:Creating converter from 7 to 5
DEBUG:h5py._conv:Creating converter from 5 to 7
DEBUG:h5py._conv:Creating converter from 7 to 5
DEBUG:h5py._conv:Creating converter from 5 to 7

```
**Other info / logs**

Likely introduced by 5364121.


The commit that supposedly fixes #35547 doesn't actually fixes the issue but only changes the log level from warning to debug.

It should instead not print any messages on a non TPU environment.


Thanks for your report! It should be fixed starting with the nightly build tomorrow morning (`2.1.0-dev20200104`).

_Originally posted by @frankchn in https://github.com/tensorflow/tensorflow/issues/35547#issuecomment-570694131_"
50422,Tensorflow type 21 not convertible to numpy dtype.,"A minimal example to reproduce the exception:

tf-version: 2.5.0
CPU
python 3.9

```
import tensorflow.compat.v1 as tf

tf.disable_v2_behavior()

ss = tf.constant([[1, 2, 3, 4, 5], [0, 0, 0, 2, 1]], dtype=tf.float32)
indecies = tf.where(tf.not_equal(ss, 0))

b = tf.compat.v1.raw_ops.DenseToCSRSparseMatrix(dense_input=ss, indices=indecies)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(b))
```

Another way to reproduce the error:

```
import tensorflow.compat.v1 as tf

from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops

tf.compat.v1.disable_eager_execution()

ss = tf.constant([[1, 2, 3, 4, 5], [0, 0, 0, 2, 1]], dtype=tf.float32)
a_st = tf.sparse.from_dense(ss)
tmp = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(a_st.indices, a_st.values, a_st.dense_shape)


with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(tmp))

```"
50421,"When you use math symbols instead of tf.math, tensorflow.keras.load_model does not properly load model for stuff like not equal","<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): on mac 10.14.6  but on colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): default colab install
- TensorFlow version (use command below): 2.5 
- Python version: 3.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
When I used some stuff such as != or == in code and save graph, I am not able to load the graph
but when I use tf.math I am able too. 
**Describe the expected behavior**
I expect to be able to use that behavior and load the model. 

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1gm5DujworX-7X1z01qUToOlzE8qLbfPO?usp=sharing
Please toggle on and off the commented line

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50419,Simple TFLITE Quant uint8 model does not work properly on TFLITE-MICRO,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 5.10.42-1-MANJARO
- TensorFlow installed from (source or binary):  Both:
In python : Pip
Tflite-micro code
```
  Fetch URL: https://github.com/tensorflow/tflite-micro.git
  commit 217ad9fcb5d44028570923b411783457b127c291
```
- Tensorflow version (commit SHA if source): 2.5.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RISCV Virtual Prototype

Hi,
I have created a simple MobileNetV2 model with tensorflow_model_maker for object detection with the following code:
```
import os

import numpy as np

import tensorflow as tf
assert tf.__version__.startswith('2')

from tflite_model_maker import model_spec
from tflite_model_maker import image_classifier
from tflite_model_maker.config import ExportFormat
from tflite_model_maker.config import QuantizationConfig
from tflite_model_maker.image_classifier import DataLoader

import matplotlib.pyplot as plt

# %%

data = DataLoader.from_folder(""data/train"")
train_data, rest_data = data.split(0.8)
validation_data, test_data = rest_data.split(0.5)

# %%
#mobilenet_v3_spec = image_classifier.ModelSpec(uri=""https://tfhub.dev/google/imagenet/mobilenet_v3_large_100_224/classification/5"")
#mobilenet_v3_spec.input_image_shape = [224, 224]
model = image_classifier.create(train_data, validation_data=validation_data, model_spec=""mobilenet_v2"")

# %%
loss, accuracy = model.evaluate(test_data)

post_quant_conf = QuantizationConfig.for_int8(representative_data=train_data)

model.export(export_dir='export', tflite_filename=""model_mobilenetv2_quant.tflite"", export_format=ExportFormat.TFLITE, quantization_config=post_quant_conf)

model.evaluate_tflite('export/model_mobilenetv2_quant.tflite', test_data
```

Furthermore i exported this model with 
```
xxd -i model_mobilenetv2_quant.tflite > model.cc
```
to run it in a tflite-micro environment.

The following code is a modification of the hello_world which initializes the tflite-micro environment end allocates the tensors:
```
    tflite::InitializeTarget();

    static tflite::MicroErrorReporter micro_error_reporter;
    error_reporter = &micro_error_reporter;

    // Map the model into a usable data structure. This doesn't involve any
    // copying or parsing, it's a very lightweight operation.
    model = tflite::GetModel(g_model);
    if(model->version() != TFLITE_SCHEMA_VERSION) {
        TF_LITE_REPORT_ERROR(error_reporter,
                             ""Model provided is schema version %d not equal ""
                             ""to supported version %d."",
                             model->version(), TFLITE_SCHEMA_VERSION);
        return;
    }

    static tflite::AllOpsResolver resolver;

    // Build an interpreter to run the model with.
    static tflite::MicroInterpreter static_interpreter(
            model, resolver, tensor_arena, kTensorArenaSize, error_reporter);
    interpreter = &static_interpreter;

    // Allocate memory from the tensor_arena for the model's tensors.
    TfLiteStatus allocate_status = interpreter->AllocateTensors();
    if(allocate_status != kTfLiteOk) {
        TF_LITE_REPORT_ERROR(error_reporter, ""AllocateTensors() failed"");
        return;
    }

    // Obtain pointers to the model's input and output tensors.
    input = interpreter->input(0);

    output = interpreter->output(0);

    std::cout << ""Input type: "" << TfLiteTypeGetName(input->type) << std::endl;
```

This code fails with the following error:
```
tensorflow/lite/micro/kernels/quantize_common.cc:51 input->type == kTfLiteFloat32 || input->type == kTfLiteInt16 || input->type == kTfLiteInt8 was not true.

Node QUANTIZE (number 0f) failed to prepare with status 1
```

The input-type of the Node is kTfLiteUInt8 which is not in the list of expected types but I don't know why. As far as I know, tflite-micro supports uint8 quantization. The ```tf.lite.converter``` in tensorflow_model_maker forces the input to be ``` tf.uint8``` which is okay because it is an image classifier. Moreover, the line
```
    std::cout << ""Input type"" << TfLiteTypeGetName(input->type) << std::endl;
```
produces the output: ```Input type: FLOAT32```
which is a surprise because the model should have an input_type of ""uint8"" according to the converters parameters.
However, in python this input-type has the correct type, where
```
interpreter = tf.lite.Interpreter(model_path=""export/model_mobilenetv3_quant.tflite"")
interpreter.allocate_tensors()
interpreter.get_input_details()
```
results in:
```
[{'name': 'input_1',
  'index': 0,
  'shape': array([  1, 224, 224,   3], dtype=int32),
  'shape_signature': array([ -1, 224, 224,   3], dtype=int32),
  'dtype': numpy.int8,
  'quantization': (0.003921568859368563, -128),
  'quantization_parameters': {'scales': array([0.00392157], dtype=float32),
   'zero_points': array([-128], dtype=int32),
   'quantized_dimension': 0},
  'sparsity_parameters': {}}]

```
Can anybody explain this behavior to me? Maybe I missed some documented behavior or forgot something. As a workaround I converted the model to int8 which seems to work for the first error. The input_type in the micro environment however is also wrong. Moreover, with this i have to convert my input to int8 which I would rather not do manually because it is not really transparent which preprocessing the model does by itself.



Thx for your help.

"
50418,TFlite memory persistence,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS (GNU/Linux 5.4.0-73-lowlatency x86_64)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Desktop
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.6.0
- Python version: N/A
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source): GCC 10.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Doing multiple inference runs directly after each other indicates good speed (after the initial warmup run). However, if I add a 5 seconds delay between inference runs, the speed is much worse. My guess is that this has something to do with memory persistence?

No delay between inference runs:
```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Prediction 1: 105 us
Prediction 2: 32 us
Prediction 3: 26 us
Prediction 4: 23 us
Prediction 5: 25 us
```

5 seconds delay between inference runs:
```
INFO: Created TensorFlow Lite XNNPACK delegate for CPU.
Prediction 1: 72 us
Prediction 2: 94 us
Prediction 3: 97 us
Prediction 4: 192 us
Prediction 5: 100 us
```

**Describe the expected behavior**
Inference speed should be similar even if there is a few seconds delay between runs.

**Standalone code to reproduce the issue**
```
#include <tensorflow/lite/interpreter.h>
#include <tensorflow/lite/model.h>
#include <tensorflow/lite/kernels/register.h>
#include <iomanip>
#include <iostream>
#include <mutex>
#include <fstream>
#include <chrono>
#include <thread>

std::mutex mtx;

using namespace std;
using namespace std::chrono_literals;
using std::chrono::high_resolution_clock;
using std::chrono::duration_cast;
using std::chrono::duration;
using std::chrono::microseconds;	

tflite::ops::builtin::BuiltinOpResolver resolver;
std::unique_ptr<tflite::FlatBufferModel> model;
std::unique_ptr<tflite::Interpreter> interpreter;

float predict() {
	std::scoped_lock lock{mtx};
	int* input;
	float* output; 
	
	input = interpreter->typed_input_tensor<int>(0);
	
	for (int i=0; i<20; ++i) {
		input[i] = rand()%((25000 - 0) + 1) + 0; 
	}
	
	interpreter->Invoke();
	
	output = interpreter->typed_output_tensor<float>(0);

	return output[0];
}

void init_model(){

	model = tflite::FlatBufferModel::BuildFromFile(""/home/gustaf/cprojects/pacific/nlp/sv/model.tflite"");

	tflite::InterpreterBuilder(*model, resolver)(&interpreter);

	//interpreter->SetNumThreads(1);

	interpreter->AllocateTensors();
	
	interpreter->Invoke();

}


int main (){
	
	init_model();
	
	int delay = 5;
	
	auto t1 = high_resolution_clock::now();
	predict();
	auto t2 = high_resolution_clock::now();
	
	std::this_thread::sleep_for(std::chrono::seconds(delay));
	
	auto t3 = high_resolution_clock::now();
	predict();
	auto t4 = high_resolution_clock::now();
	
	std::this_thread::sleep_for(std::chrono::seconds(delay));
	
	auto t5 = high_resolution_clock::now();
	predict();
	auto t6 = high_resolution_clock::now();
	
	std::this_thread::sleep_for(std::chrono::seconds(delay));
	
	auto t7 = high_resolution_clock::now();
	predict();
	auto t8 = high_resolution_clock::now();
	
	std::this_thread::sleep_for(std::chrono::seconds(delay));
	
	auto t9 = high_resolution_clock::now();
	predict();
	auto t10 = high_resolution_clock::now();


auto ms1 = duration_cast<microseconds>(t2 - t1);
auto ms2 = duration_cast<microseconds>(t4 - t3);
auto ms3 = duration_cast<microseconds>(t6 - t5);
auto ms4 = duration_cast<microseconds>(t8 - t7);
auto ms5 = duration_cast<microseconds>(t10 - t9);

cout << ""Prediction 1: "" << ms1.count() << "" us"" << endl;
cout << ""Prediction 2: "" << ms2.count() << "" us"" << endl;
cout << ""Prediction 3: "" << ms3.count() << "" us"" << endl;
cout << ""Prediction 4: "" << ms4.count() << "" us"" << endl;
cout << ""Prediction 5: "" << ms5.count() << "" us"" << endl;

}


```
"
50417,TensorRT Converter *Core Dumped*,"**System information**
`tensorflow:2.5.0-gpu` base Docker container

**Describe the current behavior**
Execution is aborted (attached image)
<img width=""603"" alt=""Capture"" src=""https://user-images.githubusercontent.com/45488144/123081935-2be25b80-d427-11eb-8952-2afa8e5475c2.PNG"">

**Describe the expected behavior**
Performed a successful model conversion 2 weeks ago with the same script.

**Standalone code to reproduce the issue**
```
        converter = trt.TrtGraphConverterV2(
            input_saved_model_dir = self.model_dir)
        converter.convert()

        saved_model_dir_trt = os.path.join(self.model_dir, self.model_id + '_trt')
        converter.save(saved_model_dir_trt)
```
"
50415,image.pad_to_bounding_box throws unexpectedly,"I'm trying to add an arbitrary amount of padding to images in a preprocessing Apache Beam pipeline. 

I calculate the amount of padding needed and if it's greater than 0 I call `tf.image.pad_to_bounding_box()` as follows:

```python
if padding > 0:
    try:
        target_height = int(org_height + (2 * padding))
        target_width = int(org_width + (2 * padding))
        image_decoded = tf.image.pad_to_bounding_box(
            image=image_decoded,
            offset_height=padding,
            offset_width=padding,
            target_height=target_height,
            target_width=target_width,
        )
    except Exception as e:
        print(""/*/*/*/*/*/*/*/*/*/*/*/*/*/*/*/"")
        print(""padding failed"")
        print(
            f""""""
        {e}
        org height: {org_height}
        org width: {org_width}

        after padding height: {target_height - padding - org_height}
        after padding width: {target_width - padding - org_width}

        target width: {int(org_width + (2 * padding))}
        target height: {int(org_height + (2 * padding))}

        padding: {padding}
        """"""
        )
        print(""/*/*/*/*/*/*/*/*/*/*/*/*/*/*/*/"")
```

an example of an unexpected exception looks like this:

```
/*/*/*/*/*/*/*/*/*/*/*/*/*/*/*/
padding failed

                        width must be <= target - offset
                        org height: 3744
                        org width: 5616

                        after padding height: 126
                        after padding width: 126

                        target width: 5868
                        target height: 3996

                        padding: 126
                        
/*/*/*/*/*/*/*/*/*/*/*/*/*/*/*/
```

Clearly the ""after padding width"" is greater than 0, so I do not understand why it throws here.

any help is greatly appreciated.

https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/ops/image_ops_impl.py#L1078"
50414,Bazel build ERROR with TF2.4 Cuda11.0 Cudnn8 Python3.8 MSVC2019 bazel 3.1.0,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform : Win10
- TensorFlow installed from (source or binary): TF2.4
- TensorFlow version:TF2.4 and TF-GPU2.4
- Python version: 3.8
- Installed using virtualenv? pip? conda?: no, yes, no
- Bazel version (if compiling from source): 3.1.0
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0/8
- GPU model and memory: Nvidia GeForce 840M



**Describe the problem**
Bazel does not build my TF
I followed all these information: https://www.tensorflow.org/install/source_windows
And I have this:
tensorflow-2.4.0	python3.8	MSVC 2019	Bazel 3.1.0
tensorflow_gpu-2.4.0	python3.8	MSVC 2019	Bazel 3.1.0	8.0	11.0
I saw people indicating the same problem, but I did not find any solution anywhere.



**Provide the exact sequence of commands / steps that you executed before running into the problem**
using
bazel build //tensorflow/tools/pip_package:build_pip_package
or
bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package
Give the same result ! Error!



**Any other info / logs**

**ERROR: An error occurred during the fetch of repository 'local_config_cuda':**
   Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1400
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 217, in execute
                fail(<1 more arguments>)
Repository command failed
'C:/Program' is not recognized as an internal or external command,
operable program or batch file.
**ERROR: Skipping '//tensorflow/tools/pip_package:build_pip_package': no such package '@local_config_cuda//cuda':** Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1400
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 217, in execute
                fail(<1 more arguments>)
Repository command failed
'C:/Program' is not recognized as an internal or external command,
operable program or batch file.
WARNING: Target pattern parsing failed.
**ERROR: no such package '@local_config_cuda//cuda':** Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1400
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 217, in execute
                fail(<1 more arguments>)
Repository command failed
**'C:/Program' is not recognized as an internal or external command,
operable program or batch file.**
INFO: Elapsed time: 2.304s
INFO: 0 processes.
FAILED: Build did NOT complete successfully (0 packages loaded)
    currently loading: tensorflow/tools/pip_package


**FIRST PART OF THE LOG**

C:\tensorflow>**bazel build //tensorflow/tools/pip_package:build_pip_package**
Starting local Bazel server and connecting to it...
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Program Files/Python38/python.exe
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/Python38/lib/site-packages --python_path=C:/Program Files/Python38/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file c:\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  C:/tensorflow/third_party/gpus/cuda_configure.bzl:1430:18: in <toplevel>
ERROR: An error occurred during the fetch of repository 'local_config_cuda':
   Traceback (most recent call last):
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1400
                _create_local_cuda_repository(<1 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 1075, in _create_local_cuda_repository
                _find_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 606, in _find_libs
                _check_cuda_libs(repository_ctx, <2 more arguments>)
        File ""C:/tensorflow/third_party/gpus/cuda_configure.bzl"", line 501, in _check_cuda_libs
                execute(repository_ctx, <1 more arguments>)
        File ""C:/tensorflow/third_party/remote_config/common.bzl"", line 217, in execute
                fail(<1 more arguments>)
Repository command failed
'C:/Program' is not recognized as an internal or external command,
operable program or batch file.

**OR** with

C:\tensorflow>**bazel build --config=opt --config=cuda --define=no_tensorflow_py_deps=true //tensorflow/tools/pip_package:build_pip_package**
WARNING: The following configs were expanded more than once: [cuda, using_cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=120
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Program Files/Python38/python.exe
INFO: Reading rc options for 'build' from c:\tensorflow\.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from c:\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Program Files/Python38/python.exe --action_env PYTHON_LIB_PATH=C:/Program Files/Python38/lib/site-packages --python_path=C:/Program Files/Python38/python.exe --config=xla --action_env CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.0 --action_env TF_CUDA_COMPUTE_CAPABILITIES=5.0 --config=cuda --define=override_eigen_strong_inline=true --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file c:\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file c:\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file c:\tensorflow\.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:opt in file c:\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX --define with_default_optimizations=true
INFO: Found applicable config definition build:cuda in file c:\tensorflow\.bazelrc: --config=using_cuda --define=using_cuda_nvcc=true
INFO: Found applicable config definition build:using_cuda in file c:\tensorflow\.bazelrc: --define=using_cuda=true --action_env TF_NEED_CUDA=1 --crosstool_top=@local_config_cuda//crosstool:toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=1
INFO: Found applicable config definition build:windows in file c:\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --experimental_strict_action_env=true --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file c:\tensorflow\.bazelrc: --define framework_shared_object=false
INFO: Repository local_config_cuda instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule cuda_configure defined at:
  C:/tensorflow/third_party/gpus/cuda_configure.bzl:1430:18: in <toplevel>"
50413,Image transformations require Scipy but Scipy already installed,"Hello, I’m running Tensorflow 2.4.1 on a Windows 10 computer (system info below). I’m having problems trying to launch a CNN model using the ImageDataGenerator feeding from a Pandas dataframe. After defining the data generator and the model, I get the following error when running model.fit: **Image transformations require SciPy. Install SciPy.** However, when I can confirm that Scipy is installed by running “import scipy” without errors.

Image generator is defined as follows:
`
train_datagen = ImageDataGenerator(horizontal_flip=False,
                                   vertical_flip=False,
                                   rescale=1/255.0).flow_from_dataframe(dataframe=X,
                                                                        x_col='image_name',
                                                                        y_col='response',
                                                                        shuffle=False, 
                                                                        directory=src_path,
                                                                        target_size=(128, 128),
                                                                        class_mode=None
                                                                       )
`
The error occurs when running:
`
model.fit(train_datagen, epochs=5)
`

**System information**
- OS Platform and Distribution: Windows 10
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.8.5
- CUDA/cuDNN version: 11.3/ 8.2.1
- GPU model and memory: NVIDIA GeForce RTX 2060"
50412,Tensorflow build is failing on Bazel@HEAD (Bazel CI),"rules_apple need to be upgraded to the latest version 0.31.2

**System information**
Link to failed build:
https://buildkite.com/bazel/bazel-at-head-plus-downstream/builds/2077#409bdb15-84cf-45b2-8fcc-7aeb2dc56c1e

**Describe the problem**
2021-05-03 apple_common.objc_proto_aspect was removed from Bazel (https://github.com/bazelbuild/bazel/commit/d6830672311d54c8072e0b48c440c2edde65314d)

The problem was only detected in today's build, because it was obscured by other breakages.

**Fix**
rules_apple need to be upgraded to the latest version 0.31.2 (which doesn't call that function)

/cc @allevato"
50411,faster_rcnn_resnet101_v1_640x640_coco17_tpu-8 model showing error on Tensorflow 2.5,"I am using python3.8 and Tensorflow 2.5.  I am trying to use faster_rcnn_resnet101_v1_640x640_coco17_tpu-8 model to do object detection but it is showing me following error
I0623 11:55:46.608306  5796 config_util.py:552] Maybe overwriting use_bfloat16: False
Traceback (most recent call last):
  File ""Tensorflow\models\research\object_detection\model_main_tf2.py"", line 113, in <module>
    tf.compat.v1.app.run()
  File ""C:\Users\priyabrata\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\priyabrata\anaconda3\envs\new_tfod_env\lib\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""C:\Users\priyabrata\anaconda3\envs\new_tfod_env\lib\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""Tensorflow\models\research\object_detection\model_main_tf2.py"", line 104, in main
    model_lib_v2.train_loop(
  File ""C:\Users\priyabrata\anaconda3\envs\new_tfod_env\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\model_lib_v2.py"", line 535, in train_loop
    detection_model = MODEL_BUILD_UTIL_MAP['detection_model_fn_base'](
  File ""C:\Users\priyabrata\anaconda3\envs\new_tfod_env\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\builders\model_builder.py"", line 1199, in build
    return build_func(getattr(model_config, meta_architecture), is_training,
  File ""C:\Users\priyabrata\anaconda3\envs\new_tfod_env\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\builders\model_builder.py"", line 391, in _build_ssd_model
    _check_feature_extractor_exists(ssd_config.feature_extractor.type)
  File ""C:\Users\priyabrata\anaconda3\envs\new_tfod_env\lib\site-packages\object_detection-0.1-py3.8.egg\object_detection\builders\model_builder.py"", line 263, in _check_feature_extractor_exists
    raise ValueError('{} is not supported. See `model_builder.py` for features '
ValueError:  is not supported. See `model_builder.py` for features extractors compatible with different versions of Tensorflow
"
50407,Issue with custom metrics with model class inheritance and using GPU’s with mirrored strategy with TF version 2.4.1 and TFP version 0.12.1 (Also tested and found same issue on TF 2.5.0/TFP 0.12.1).,"
### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: Modified Google Colab Example
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: NA
-   **TensorFlow installed from (source or binary)**:
-   **TensorFlow version (use command below)**: 2.4.1 (and 2.5 via colab)
-   **Python version**: 3.8.8
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**: 
  nvcc: NVIDIA (R) Cuda compiler driver
  Copyright (c) 2005-2020 NVIDIA Corporation
  Built on Thu_Jun_11_22:26:38_PDT_2020
  Cuda compilation tools, release 11.0, V11.0.194
  Build cuda_11.0_bu.TC445_37.28540450_0
-   **GPU model and memory**:
-   **Exact command to reproduce**:


I am having issues getting custom metrics to work nice with tensorflow models designed with model class inheritance and distributed across GPUs with mirrored strategy. This is on TF version 2.4.1 and TFP 0.12.1.  The model fits and metric works on fitting. However, when I rerun the model to retrieve the estimated distribution object the following error is returned:
**_ValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`_**

 If I remove the custom metric, then no error is returned. If I use a tensorflow.keras defined metric such as ""mse,"" no error is returned.  A similar issue was raised previously but closed as resolved: [tensorflow issue 40366](https://github.com/tensorflow/tensorflow/issues/40366.)

I am using code from the google tutorial on google colab called Probabilistic Layers Regression. It can be found here: [colab link](https://colab.research.google.com/github/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_Layers_Regression.ipynb
)

I have modified the linked code slightly to incorporate class inheritance because in my project I want to return the distribution object, and be able to calculate the negative log likelihood in a metric and a modified negative log likelihood in the loss (using custom metrics and custom losses).

For example, the following code will run and the metric works when calling model.fit:

```

## define custom loss
def negloglik_loss(y_true, y_pred):
    nll,_,_ = y_pred
    return nll
## define custom metric
def negloglik_metric(y_true, y_pred):
    nll,_,_ = y_pred
    return nll
    
    
os.environ[""CUDA_VISIBLE_DEVICES""]=""0,1,2,3""


gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu,True)

devices_names = [d.name.split('e:')[1] for d in gpus]

print(devices_names)
strategy = tf.distribute.MirroredStrategy(
           devices=devices_names)


## create dataset
w0 = 0.125
b0 = 5.
x_range = [-20, 60]

def load_dataset(n=150, n_tst=150):
  np.random.seed(43)
  def s(x):
    g = (x - x_range[0]) / (x_range[1] - x_range[0])
    return 3 * (0.25 + g**2.)
  x = (x_range[1] - x_range[0]) * np.random.rand(n) + x_range[0]
  eps = np.random.randn(n) * s(x)
  y = (w0 * x * (1. + np.sin(x)) + b0) + eps
  #x = x[..., np.newaxis]
  x_tst = np.linspace(*x_range, num=n_tst).astype(np.float32)
  #x_tst = x_tst[..., np.newaxis]
  return y, x, x_tst

y, x, x_tst = load_dataset()

## build model from model class inheritance
class tfp_prob_reg(tf.keras.Model):
    
    def __init__(self):
        super(tfp_prob_reg, self).__init__()
        self.block_1 = tf.keras.layers.Dense(1, activation='relu')
        
    def call(self, inputs):
        input_x, input_y = inputs
        x_mu = self.block_1(input_x)
        dist = tfp.layers.DistributionLambda(lambda x_mu: tfd.Normal(loc=x_mu, scale=1))(x_mu)
        
        
        return -dist.log_prob(input_y), dist, x_mu

## run on gpus
with strategy.scope():
    ## define inputs
    input_x = tf.keras.Input(shape=(1,))
    input_y = tf.keras.Input(shape=(1,))

    ## define output
    outputs_x = tfp_prob_reg()([input_x, input_y])

    ## build model
    tfp_model = tf.keras.Model(inputs = [input_x, input_y], outputs=outputs_x)
    tfp_model.add_loss(negloglik_loss(input_y, outputs_x))
    tfp_model.add_metric(negloglik_metric(input_y, outputs_x), name='metric')

    ## compile model
    tfp_model.compile(optimizer=tf.optimizers.Nadam(learning_rate=1e-5))

## fit model
tfp_model.fit([x,y],
              epochs=10,
              validation_split = .1,
              verbose=True)
```

However, re-running the fitted model on data to retrieve the estimated distribution does not run and returns the following error:


`return_outputs_on_new_data = tfp_model([x_tst, y])`
```

ValueError                                Traceback (most recent call last)
<ipython-input-38-10667b80afe2> in <module>
----> 1 return_outputs_on_new_data = tfp_model([x_tst, y])

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-> 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    422         a list of tensors if there are more than one outputs.
    423     """"""
--> 424     return self._run_internal_graph(
    425         inputs, training=training, mask=mask)
    426 

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    558 
    559         args, kwargs = node.map_arguments(tensor_dict)
--> 560         outputs = node.layer(*args, **kwargs)
    561 
    562         # Update tensor_dict.

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-> 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in call(self, inputs)
   3264 
   3265   def call(self, inputs):
-> 3266     self.add_metric(inputs, aggregation=self.aggregation, name=self.metric_name)
   3267     return inputs
   3268 

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py in add_metric(self, value, name, **kwargs)
   1748 
   1749       if should_update_state:
-> 1750         metric_obj(value)
   1751     else:
   1752       if from_metric_obj:

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in __call__(self, *args, **kwargs)
    234 
    235     from tensorflow.python.keras.distribute import distributed_training_utils  # pylint:disable=g-import-not-at-top
--> 236     return distributed_training_utils.call_replica_local_fn(
    237         replica_local_fn, *args, **kwargs)
    238 

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/distribute/distributed_training_utils.py in call_replica_local_fn(fn, *args, **kwargs)
     64     with strategy.scope():
     65       return strategy.extended.call_for_each_replica(fn, args, kwargs)
---> 66   return fn(*args, **kwargs)

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in replica_local_fn(*args, **kwargs)
    215         update_op = None
    216       else:
--> 217         update_op = self.update_state(*args, **kwargs)  # pylint: disable=not-callable
    218       update_ops = []
    219       if update_op is not None:

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/utils/metrics_utils.py in decorated(metric_obj, *args, **kwargs)
     88 
     89     with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):
---> 90       update_op = update_state_fn(*args, **kwargs)
     91     if update_op is not None:  # update_op will be None in eager execution.
     92       metric_obj.add_update(update_op)

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in update_state_fn(*args, **kwargs)
    175         control_status = ag_ctx.control_status_ctx()
    176         ag_update_state = autograph.tf_convert(obj_update_state, control_status)
--> 177         return ag_update_state(*args, **kwargs)
    178     else:
    179       if isinstance(obj.update_state, def_function.Function):

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in wrapper(*args, **kwargs)
    665       try:
    666         with conversion_ctx:
--> 667           return converted_call(f, args, kwargs, options=options)
    668       except Exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in converted_call(f, args, kwargs, caller_fn_scope, options)
    348   if conversion.is_in_allowlist_cache(f, options):
    349     logging.log(2, 'Allowlisted %s: from cache', f)
--> 350     return _call_unconverted(f, args, kwargs, options, False)
    351 
    352   if ag_ctx.control_status_ctx().status == ag_ctx.Status.DISABLED:

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py in _call_unconverted(f, args, kwargs, options, update_cache)
    476 
    477   if kwargs is not None:
--> 478     return f(*args, **kwargs)
    479   return f(*args)
    480 

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/keras/metrics.py in update_state(self, values, sample_weight)
    377     value_sum = math_ops.reduce_sum(values)
    378     with ops.control_dependencies([value_sum]):
--> 379       update_total_op = self.total.assign_add(value_sum)
    380 
    381     # Exit early if the reduction doesn't have a denominator.

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/distribute/values.py in assign_add(self, value, use_locking, name, read_value)
   1186           not values_util.in_replica_update_context()):
   1187         values_util.mark_as_unsaveable()
-> 1188         return values_util.on_read_assign_add_cross_replica(
   1189             self, value, read_value=read_value)
   1190       else:

/hpc/apps/pyhpc/dist/conda/x86_64/envs/cuda-11.0/lib/python3.8/site-packages/tensorflow/python/distribute/values_util.py in on_read_assign_add_cross_replica(var, value, read_value)
    199     if ds_context.in_cross_replica_context():
    200       if var.aggregation == vs.VariableAggregation.SUM:
--> 201         raise ValueError(
    202             ""SyncOnReadVariable does not support `assign_add` in ""
    203             ""cross-replica context when aggregation is set to ""

ValueError: SyncOnReadVariable does not support `assign_add` in cross-replica context when aggregation is set to `tf.VariableAggregation.SUM`

```
If you copy this code to the google colab link and modify it to run in that notebook, you get the same errors. meaning, this error is also present on TF version 2.5.0/TFP 0.12.1.
"
50406,Tensorflow shuffle leak memory,My tensorflow pipeline use shuffle and it always end up with OOM. I try to follow solution at https://github.com/tensorflow/tensorflow/issues/44176 but it did not work for me. My tensorflow version is 2.3.0. ANyone know how to fix this?
50405,How to remove custom signature of savedmodel and re-export the model,"Hello,

I currently have a saved model, it is using a custom signature, but it needs to have serving_default signature def, so it should look like this: 
`signature_def['serving_default'].`

If I do saved_model_cli show it currently shows

```
signature_def['image_quality']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['input_image'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 224, 224, 3)
        name: input_1:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['quality_prediction'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 10)
        name: dense_1/Softmax:0
  Method name is: tensorflow/serving/predict
```
Is there a way to remove the custom name and re-export the model in tensorflow v2 in order to have the serving_default signature?
"
50403,Issue in using tfio.experimental.filter.sobel in Keras model.fit with Colab TPU,"Hello,

I am training a GAN for image inpainting. I am computing the Sobel edges of my generator's output and giving them as an input to the discriminator. 
1. I am using tfio.experimental.filter.sobel function.
2. I have also tried tf.image.sobel_edges but it is not compatible with TPU.
2. I have defined this as a tf.keras.Model class and then doing the training using model.fit function.
3. I am training this on Google Colab's TPU.

Whenever, I give the sobel edges of the generator's output to the discriminator, the training loss goes to ""NaN"". I have checked that the computed sobel edges don't contain any NaN and my dataset also doesn't contain any NaN. If I don't give this sobel edges as an input to discriminator then the code works fine without leading to NaN.

Has anyone faced any such similar situation before? Please let me know how can I debug it. If you have faced something similar then, please let me know the solution. Any help is appreciated. Thank you."
50401,Support input of temporal sample_weights for model training on ragged tensors,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Currently tensorflow throws an error if we input temporal [sample_weights](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) for a model that's fitting inputs/outputs that are in the format of ragged tensors. Example:
```python
#input in general has shape (N_inputs, variable length, N_input_channels)    
X = [[[4.,3,2],[2,1,3],[-1,2,1]],
     [[1,2,3],[3,2,4]]]
X = tf.ragged.constant(X, ragged_rank=1, dtype=tf.float64)

#output in general has shape (N_inputs, variable but same as corresponding input, N_classification_classes)
Y = [[[0,0,1],[0,1,0],[1,0,0]],
     [[0,0,1],[1,0,0]]]
Y = tf.ragged.constant(Y, ragged_rank=1)

#Documentation says for temporal data we can pass 2D array with shape (samples, sequence_length) 
weights = [[100,1,1],
           [100,1]]
weights = np.array(weights)

model = SimpleModel(width=16, in_features=3, out_features=3)
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(X,Y) #works fine
model.fit(X,Y, sample_weight=weights) #throws error
```
Where the error thrown is `ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list)`. If we do the equivalent operator for a non-ragged tensors 
```python
#input in general has shape (N_inputs, 2, N_input_channels)    
X = [[[4.,3,2],[2,1,3]],
     [[1,2,3],[3,2,4]]]
X = tf.constant(X, dtype=tf.float64)

#output in general has shape (N_inputs, 2, N_classification_classes)
Y = [[[0,0,1],[0,1,0]],
     [[0,0,1],[1,0,0]]]
Y = tf.constant(Y)

#Documentation says for temporal data we can pass 2D array with shape (samples, sequence_length) 
weights = [[100,1],
           [100,1]]
weights=np.array(weights)

model = SimpleModel(width=16, in_features=3, out_features=3)
model.compile(loss='categorical_crossentropy', optimizer='adam')
model.fit(X,Y) #works fine
model.fit(X,Y, sample_weight=weights) #also works fine
```
Everything works fine.  The desired feature would allow passing of sample_weights for ragged tensors in the same way we could pass sample_weights for non-ragged tensors 


**Will this change the current api? How?**
This would change the `tf.keras.Model.fit` api so that ragged sample_weights are supported 


**Who will benefit with this feature?**
People working with variable length data. This occurs in areas like computer vision and applications of deep learning to particle physics. This feature would allow people working with ragged tensors to deal with underrepresented classes in temporal data via reweighing. 


**Any Other info.**
Definition of SimpleLayer and SimpleModel used above
```python
class SimpleLayer(tf.keras.layers.Layer):
    """"""Just dummy layer to illustrate sample_weight for layer""""""
    def __init__(self, in_features, out_features, n):
        super(SimpleLayer, self).__init__()
        self.out_features = out_features
        self.in_features = in_features

        self.Gamma = self.add_weight(name='Gamma'+str(n),
                shape=(in_features, out_features), 
                initializer='glorot_normal', trainable=True)

    def call(self, inputs):
        #uses ragged map_flat_values for Ragged tensors to handle
        #variable number of jet
        xG = tf.ragged.map_flat_values(tf.matmul, inputs, self.Gamma)
        return xG

    
class SimpleModel(tf.keras.Model):
    """"""Composes SimpleLayer above to create simple network for ragged tensors""""""
    def __init__(self, width, in_features, out_features, Sigma=tf.nn.leaky_relu):
        super(SimpleModel, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.width = width
        self.first_layer = SimpleLayer(self.in_features, self.width, 0)
        self.hidden = SimpleLayer(self.width, self.width, 1)
        self.last_layer = SimpleLayer(self.width, self.out_features, 2)
        self.Sigma = Sigma

    def call(self, inputs):
        #use map_flat_values to apply activation to ragged tensor
        x = tf.ragged.map_flat_values(self.Sigma, self.first_layer(inputs))
        x = tf.ragged.map_flat_values(self.Sigma, self.hidden(x))
        x = tf.ragged.map_flat_values(tf.nn.softmax, self.last_layer(x))
        return x
```
Full traceback of error
```
ValueError                                Traceback (most recent call last)
<ipython-input-32-ce7d043b6b3f> in <module>
     60 model.compile(loss='categorical_crossentropy', optimizer='adam')
     61 model.fit(X,Y) #works fine
---> 62 model.fit(X,Y, sample_weight=weights) #throws error

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1131          training_utils.RespectCompiledTrainableState(self):
   1132       # Creates a `tf.data.Dataset` and handles batch and epoch iteration.
-> 1133       data_handler = data_adapter.get_data_handler(
   1134           x=x,
   1135           y=y,

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in get_data_handler(*args, **kwargs)
   1362   if getattr(kwargs[""model""], ""_cluster_coordinator"", None):
   1363     return _ClusterCoordinatorDataHandler(*args, **kwargs)
-> 1364   return DataHandler(*args, **kwargs)
   1365 
   1366 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)
   1152     adapter_cls = select_data_adapter(x, y)
   1153     self._verify_data_adapter_compatibility(adapter_cls)
-> 1154     self._adapter = adapter_cls(
   1155         x,
   1156         y,

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in __init__(self, x, y, sample_weights, sample_weight_modes, batch_size, steps, shuffle, **kwargs)
    584                **kwargs):
    585     super(CompositeTensorDataAdapter, self).__init__(x, y, **kwargs)
--> 586     x, y, sample_weights = _process_tensorlike((x, y, sample_weights))
    587     sample_weight_modes = broadcast_sample_weight_modes(
    588         sample_weights, sample_weight_modes)

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _process_tensorlike(inputs)
   1044     return x
   1045 
-> 1046   inputs = nest.map_structure(_convert_numpy_and_scipy, inputs)
   1047   return nest.list_to_tuple(inputs)
   1048 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/util/nest.py in map_structure(func, *structure, **kwargs)
    865 
    866   return pack_sequence_as(
--> 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/util/nest.py in <listcomp>(.0)
    865 
    866   return pack_sequence_as(
--> 867       structure[0], [func(*x) for x in entries],
    868       expand_composites=expand_composites)
    869 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py in _convert_numpy_and_scipy(x)
   1039       if issubclass(x.dtype.type, np.floating):
   1040         dtype = backend.floatx()
-> 1041       return ops.convert_to_tensor_v2_with_dispatch(x, dtype=dtype)
   1042     elif _is_scipy_sparse(x):
   1043       return _scipy_sparse_to_sparse_tensor(x)

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs)
    204     """"""Call target, and fall back on dispatchers if there is a TypeError.""""""
    205     try:
--> 206       return target(*args, **kwargs)
    207     except (TypeError, ValueError):
    208       # Note: convert_to_eager_tensor currently raises a ValueError, not a

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2_with_dispatch(value, dtype, dtype_hint, name)
   1428     ValueError: If the `value` is a tensor not of given `dtype` in graph mode.
   1429   """"""
-> 1430   return convert_to_tensor_v2(
   1431       value, dtype=dtype, dtype_hint=dtype_hint, name=name)
   1432 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor_v2(value, dtype, dtype_hint, name)
   1434 def convert_to_tensor_v2(value, dtype=None, dtype_hint=None, name=None):
   1435   """"""Converts the given `value` to a `Tensor`.""""""
-> 1436   return convert_to_tensor(
   1437       value=value,
   1438       dtype=dtype,

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py in wrapped(*args, **kwargs)
    161         with Trace(trace_name, **trace_kwargs):
    162           return func(*args, **kwargs)
--> 163       return func(*args, **kwargs)
    164 
    165     return wrapped

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)
   1564 
   1565     if ret is None:
-> 1566       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
   1567 
   1568     if ret is NotImplemented:

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/tensor_conversion_registry.py in _default_conversion_function(***failed resolving arguments***)
     50 def _default_conversion_function(value, dtype, name, as_ref):
     51   del as_ref  # Unused.
---> 52   return constant_op.constant(value, dtype, name=name)
     53 
     54 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name)
    262     ValueError: if called on a symbolic tensor.
    263   """"""
--> 264   return _constant_impl(value, dtype, shape, name, verify_shape=False,
    265                         allow_broadcast=True)
    266 

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_impl(value, dtype, shape, name, verify_shape, allow_broadcast)
    274       with trace.Trace(""tf.constant""):
    275         return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
--> 276     return _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    277 
    278   g = ops.get_default_graph()

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in _constant_eager_impl(ctx, value, dtype, shape, verify_shape)
    299 def _constant_eager_impl(ctx, value, dtype, shape, verify_shape):
    300   """"""Implementation of eager constant.""""""
--> 301   t = convert_to_eager_tensor(value, ctx, dtype)
    302   if shape is None:
    303     return t

~/.conda/envs/esm_tth/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     97   ctx.ensure_initialized()
---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)
     99 
    100 

ValueError: Failed to convert a NumPy array to a Tensor (Unsupported object type list).
```
"
50400,NaNs/Infs in gradient of tf.image.ssim_multiscale,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): Installed from pip
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.6
- CUDA/cuDNN version: 11.2/8.1.1
- GPU model and memory: GTX 1060 6GB

**Describe the current behavior**
Calculating gradient of `tf.image.ssim_multiscale` fails (resulting in NaNs and +Infs) in case of MS-SSIM being equal to 0 (e.g. with negated images as inputs). 

**Describe the expected behavior**
Calculated gradient does not contain NaNs and Infs.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):  no

**Standalone code to reproduce the issue**
```python
import tensorflow as tf

tf.debugging.enable_check_numerics()

x = tf.Variable(tf.random.uniform(shape=(10, 64, 64, 1), minval=0, maxval=1))

with tf.GradientTape() as tape:
    y = tf.image.ssim_multiscale(x, 1 - x, max_val=1, filter_size=11, filter_sigma=1.5, power_factors=(0.07105472, 0.45297383, 0.47597145), k1=0.01, k2=0.045)

tape.gradient(y, x)
```

**Other info / logs**
Output of code above:
```
2021-06-22 13:49:31.098596: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-22 13:49:33.278314: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
2021-06-22 13:49:33.305643: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2021-06-22 13:49:33.305856: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-22 13:49:33.313716: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-22 13:49:33.313841: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-06-22 13:49:33.317789: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
2021-06-22 13:49:33.319230: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
2021-06-22 13:49:33.324269: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
2021-06-22 13:49:33.327990: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
2021-06-22 13:49:33.328966: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-22 13:49:33.329199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-22 13:49:33.329609: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-22 13:49:33.330158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties:
pciBusID: 0000:01:00.0 name: GeForce GTX 1060 6GB computeCapability: 6.1
coreClock: 1.7715GHz coreCount: 10 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 178.99GiB/s
2021-06-22 13:49:33.330405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-22 13:49:33.925802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-22 13:49:33.925979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0
2021-06-22 13:49:33.926031: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N
2021-06-22 13:49:33.926529: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4624 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)
2021-06-22 13:49:34.160168: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-22 13:49:34.532402: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8101
2021-06-22 13:49:34.874100: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-22 13:49:35.251241: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
Traceback (most recent call last):
  File ""ssim_multiscale.py"", line 12, in <module>
    tape.gradient(y, x)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\eager\backprop.py"", line 1080, in gradient
    unconnected_gradients=unconnected_gradients)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\eager\imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\eager\backprop.py"", line 159, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\ops\math_grad.py"", line 1519, in _PowGrad
    gx = grad * y * math_ops.pow(x, y - 1)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\util\dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\ops\math_ops.py"", line 688, in pow
    return gen_math_ops._pow(x, y, name=name)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\ops\gen_math_ops.py"", line 6722, in _pow
    _ctx, ""Pow"", name, x, y)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\debug\lib\check_numerics_callback.py"", line 301, in callback
    path_length_limit=self._path_length_limit))
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\ops\gen_array_ops.py"", line 1003, in check_numerics_v2
    _ops.raise_from_not_ok_status(e, name)
  File ""C:\python36_ver\cuda11_2_gpu_5_1_trunk\lib\site-packages\tensorflow\python\framework\ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError:

!!! Detected Infinity or NaN in output 0 of eagerly-executing op ""Pow"" (# of outputs: 1) !!!
  dtype: <dtype: 'float32'>
  shape: (10, 1, 3)
  # of +Inf elements: 20

  Input tensors (2):
         0: tf.Tensor(
[[[0.         0.         0.6353643 ]]

 [[0.         0.         0.53395146]]

 [[0.         0.         0.535903  ]]

 [[0.         0.         0.67927223]]

 [[0.         0.         0.5929147 ]]

 [[0.         0.         0.5948703 ]]

 [[0.         0.         0.6313471 ]]

 [[0.         0.         0.62584317]]

 [[0.         0.         0.71081793]]

 [[0.         0.         0.56649375]]], shape=(10, 1, 3), dtype=float32)
         1: tf.Tensor([-0.9289453  -0.54702616 -0.52402854], shape=(3,), dtype=float32)

 : Tensor had +Inf values [Op:CheckNumericsV2]
```

During diagnosing this problem, I have discovered that adding epsilon to `mcs_and_ssim` (e.g. in `image_ops_impl.py:4531`) seems to solve a problem. Probably, this is not a valid solution, but maybe it provides some useful information.
"
50399,Pip installation error ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 
- TensorFlow installed from (source or binary): Binary
- TensorFlow version: 1.15 (but affects any version i try)
- Python version: 3.6.9
- Installed using virtualenv? pip? conda?: pip, in a venv


**Describe the problem**
Trying to Install TensorFlow using pip yields the following error, I'm specifically trying to install 1.15 but I've tried 2.x also and the same happens. I have checked similar issues and am definitely using 64bit python 3.6; I know not using the correct version of python can cause this error, but the error still persists. 

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
(venv) ubuntu@dde1f763d711:~$ pip install --user --upgrade tensorflow==1.15
ERROR: Could not find a version that satisfies the requirement tensorflow==1.15 (from versions: none)
ERROR: No matching distribution found for tensorflow==1.15
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50398,"TypeError: An op outside of the function building code is being passed a ""Graph"" tensor.","Hi I got following error when including a customized DropConnectDense layer. The Error seems to be related to the update of  kernel and bias.  Any helps will be appreciated.  Thanks.

Traceback (most recent call last):

    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2
The graph tensor has name: MLRM_TFNN_DROPCONNECT/den2_1/dropout/Mul_1:0

The DropConnectDense class I have:
```
from tensorflow.python.ops import gen_math_ops
from tensorflow.python.ops import nn_ops

class DropConnectDense(Dense):
    def __init__(self, *args, **kwargs):
        self.rate = kwargs.pop('rate', 0.5)
        if 0. < self.rate < 1.:
            self.uses_learning_phase = True

        super(DropConnectDense, self).__init__(*args, **kwargs)

    def call(self, inputs):

        if 0. < self.rate < 1.:
            kernel = K.in_train_phase(nn_ops.dropout(self.kernel, rate = self.rate), self.kernel)
            bias = K.in_train_phase(nn_ops.dropout(self.bias, rate = self.rate), self.bias)
        else:
            kernel = self.kernel
            bias = self.bias
            self.kernel = kernel
            self.bias = bias

        outputs = gen_math_ops.MatMul(a = inputs, b = kernel)
        if self.use_bias:
          outputs = nn_ops.bias_add(outputs, bias)

        if self.activation is not None:
          outputs = self.activation(outputs)
        return outputs


    def get_config(self):
        config = super(DropConnectDense, self).get_config()
        config.update({
            'rate': self.rate,
            'uses_learning_phase': self.uses_learning_phase,
        })

        return config
```

"
50397,Crash in Backward-Pass of grouped Conv1D,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


Backward-Pass of grouped Convolution (Conv1D) crashes:
```
InvalidArgumentError:  filter_size does not have enough elements, requested 896, got 224
	 [[node gradient_tape/sequential/conv1d/conv1d/Conv2DBackpropFilter (defined at <ipython-input-6-c99a994f0883>:1) ]] [Op:__inference_train_function_898]
```

Forward-Pass (simple call) works.

Code is available at Colab:

https://colab.research.google.com/drive/1yyxtQCeM81cEUAVA6_z1PdtEY2P5kZ2C?usp=sharing"
50396,TFlite shape,"hello，
Computer environment:window 10 ;tensorflow 1.13.0; python3.6 ; tflite-runtime  2.1.0.post1; 
save tflite code
    **_def save(self):
        converter = tf.lite.TFLiteConverter.from_session(self.session, [self.ts.x, self.ts.x_peak], [self.ts.logits])
        tflite_model = converter.convert()
        with open(""model/"" + cfg.name + '/model.tflite', ""wb"") as f:
            f.write(tflite_model)
        print('model is in {}'.format(self.save_path))_**
when I set the  the x ( input model) shape is [10, 1801, 2], it product tflite is right ,and   in the tflite input_details is [10, 1801, 2],however, I need the shape is [1, 1801, 2];So, when I set the x shape is [None, 1801, 2] ; but when I train the model, it is error that as follows
**_Traceback (most recent call last):
  File ""C:/Users/Administrator/Desktop/mix/train.py"", line 400, in <module>
    app.train(train_data, val_data)
  File ""C:/Users/Administrator/Desktop/mix/train.py"", line 265, in train
    ts.train_summary, ts.loss_summary], feed)
  File ""D:\Users\Administrator\anaconda3\envs\python36\lib\site-packages\tensorflow\python\client\session.py"", line 929, in run
    run_metadata_ptr)
  File ""D:\Users\Administrator\anaconda3\envs\python36\lib\site-packages\tensorflow\python\client\session.py"", line 1128, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (10, 1801, 2) for Tensor 'self.x:0', which has shape '(1, 1801, 2)'_**

I know the reason is that producting tflite cause x that change it's shape ；but I don't know how to solve it ;I sincerely hope you can understand my poor English！Thanks！！！
"
50395,Model.fit halts/freezes when using GPU in tensorflow-macos on M1,"I'm not sure if this is the right place to report this but the Apple repo is marked archive and I'm not sure who is maintaining tensorflow-macos

**System information**
- I have a somewhat straightforward autoencoder which uses build in layers and models of tensorflow
- macOS 12.0 Beta
- TensorFlow installed from binary through MiniForge and conda:
- TensorFlow-macos version 2.5.0 tensorflow-metal 0.1.1:
- Python version: 3.9
- GPU model and memory: M1 iMac 8 core

**Describe the current behavior**

When running model.fit while using the GPU it will run fine for a random amount of iterations than suddenly stop and when looking at the activity monitor i can see that GPU% goes down to zero, also sometimes a sub-model of my autoencoder returns a tensor twice it's normal size (my assumption is that the cpu and gpu are conflicting and returning the same values so it results in a double tensor, although i have no idea if that is correct). The memory usage is very high and of course i only have 8gb of memory (which in general is way to little), but when running model.fit without the gpu the computer does just fine, although much slower, but it does not freeze up.

**Describe the expected behavior**
The expected behavior is for the gpu to not freeze up and stop running.

**Standalone code to reproduce the issue**
I can provide all of my code if requested i think i have not tried a bare minimum but I'm assuming this is related to memory usage and gpu exhaustion, so i haven't bothered with testing it on a smaller case.

**Other info / logs**
I have run it with verbose=1 and it doesn't output differrently, the execution seems to just stop.

I have tried limiting the memory usage of the GPU using:
tensorflow.experimental.per_process_gpu_memory_fraction = 0.75
tensorflow.experimental.per_process_memory_growth = True

and have tried this with various memory values:
gpus = tf.config.experimental.list_physical_devices('GPU')

tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])

but it has not worked. The only thing that works is disabling the GPU and that kind of defeats the purpose."
50394,Model can be benchmarked with GPU but can't be run with GPU inside android app,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android 7.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: RK3399
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the current behavior**
I use the `android_aarch64_benchmark_model_plus_flex` tool and it can run perfectly with gpu and I see performance gain. 
But when I follow this link (https://www.tensorflow.org/lite/performance/gpu_advanced) to load the same model inside my android app, the `isDelegateSupportedOnThisDevice` function returned false. 
**Describe the expected behavior**
`isDelegateSupportedOnThisDevice` should return true



Additionally, I figured with the benchmark script that I currently only have valid opencl installed. (if i set --gpu_backend=gl, the benchmark script doesn't work). However, in android, it seems it's not detecting the opencl runtime correctly. Not sure if I need to load any runtime libs for opencl manually or not but it wasn't in any documentation, so assume I don't need to do that. Just wanna check if this is indeed a bug or it's by design. 
"
50393,A name conflict bug caused by LoweringFunctionalOps pass,"
**System information**
- TensorFlow version: 2.5.0
- Python version: 3.6


**Describe the current behavior**

When running testSkipEagerCaseLoweringPreservesNameForFetch test from tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/functional_ops_test.py on a device that isn’t CPU/GPU the test fails due to a naming conflict in TF:
“Invalid argument: Node 'Case/branch_index/_3' is not unique”

During LoweringFunctionalOps pass, the Case node is replaced with a graph that contains a _SwitchN nodes with the name 'Case/branch_index/_3' with an input that is named 'Case/branch_index/’.
Later, during graph partitioning, a _Send node is added between the Const input and the _SwitchN node which happens to get the name 'Case/branch_index/_3' thus causing a conflict.

Here is a schematic of the situation:
![case2](https://user-images.githubusercontent.com/17768507/122736642-eb4cdb80-d288-11eb-8c0c-75cfb949df51.png)

- Do you want to contribute a PR? (yes/no): no

**Other info / logs** 
The _SwitchN name given during the lowering phase happens at:
tensorflow/core/common_runtime/lower_case_op.cc:122
The _Send name given during partitioning happens at:
tensorflow/core/graph/graph_partition.cc:241

"
50392,groups parameter for convolution layers is broken,"### System information

- Custom code
- Linux Debian buster
- tensorflow from binary 2.6.0-dev20210621
- python 3-7
- CPU only

### Code to reproduce the problem

```
  import tensorflow as tf
  import numpy as np
  
 
  def try_call_with_gradient(model, input_shape):
      try:
          for _ in range(2):
              with tf.GradientTape() as tape:
                  x = np.random.normal(size=input_shape).astype(np.float32)
                  y = np.random.randint(2, size=(input_shape[0])).astype(np.int32)
  
                  out = model(x)
                  out = tf.reduce_mean(out, list(range(1, len(input_shape))))
  
                  loss = tf.keras.losses.binary_crossentropy(y, out, from_logits=True)
  
              grads = tape.gradient(loss, model.trainable_variables)
      except Exception as e:
          print(e)

  input_shape_1d = (3,12,4)
  model_1d = tf.keras.layers.Conv1D(filters=6, kernel_size=3, groups=2)
  try_call_with_gradient(model_1d, input_shape_1d)
  

  input_shape_2d = (3,12,12,4)
  model_2d = tf.keras.layers.Conv2D(filters=6, kernel_size=3, groups=2)
  try_call_with_gradient(model_2d, input_shape_2d)
  

  input_shape_3d = (3,12,12,12,4)
  model_3d = tf.keras.layers.Conv3D(filters=6, kernel_size=3, groups=2)
  try_call_with_gradient(model_3d, input_shape_3d)

```
Output:
```
Computed input depth 4 doesn't match filter input depth 2 [Op:Conv2DBackpropInput]
Computed input depth 4 doesn't match filter input depth 2 [Op:Conv2DBackpropInput]
Number of channels in filter (2) must match last dimension of input (4) [Op:Conv3D]
```

### Describe the problem
Convolution layers with non-trivial groups parameter are broken. 

Additionally Conv1D and Conv2D have different behavior than Conv3D, regarding when an error is thrown. The former break at the gradient computation, the latter at the forward call.

From my understanding and from the documentation, the only requirement for the **groups** parameter is that **filters** and **input channels** must be a multiple of **groups**. So the parameter choices look just fine to me. 

### Full Tracebacks

Once for 1D
```
Traceback (most recent call last):
  File "".../scratches/scratch.py"", line 23, in <module>
    try_call_with_gradient(model_1d, input_shape_1d)
  File "".../scratches/scratch.py"", line 17, in try_call_with_gradient
    grads = tape.gradient(loss, model.trainable_variables)
  File "".../python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 1090, in gradient
    unconnected_gradients=unconnected_gradients)
  File "".../python3.7/site-packages/tensorflow/python/eager/imperative_grad.py"", line 77, in imperative_grad
    compat.as_str(unconnected_gradients.value))
  File "".../python3.7/site-packages/tensorflow/python/eager/backprop.py"", line 159, in _gradient_function
    return grad_fn(mock_op, *out_grads)
  File "".../python3.7/site-packages/tensorflow/python/ops/nn_grad.py"", line 590, in _Conv2DGrad
    data_format=data_format),
  File ""...python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1247, in conv2d_backprop_input
    _ops.raise_from_not_ok_status(e, name)
  File "".../python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6941, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Computed input depth 4 doesn't match filter input depth 2 [Op:Conv2DBackpropInput]

Process finished with exit code 1
```
and once for 3D
```
Traceback (most recent call last):
  File "".../scratches/scratch.py"", line 33, in <module>
    try_call_with_gradient(model_3d, input_shape_3d)
  File "".../scratches/scratch.py"", line 12, in try_call_with_gradient
    out = model(x)
  File "".../python3.7/site-packages/keras/engine/base_layer.py"", line 1037, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File "".../python3.7/site-packages/keras/layers/convolutional.py"", line 249, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File "".../python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File "".../python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1138, in convolution_v2
    name=name)
  File "".../python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 1268, in convolution_internal
    name=name)
  File "".../python3.7/site-packages/tensorflow/python/ops/nn_ops.py"", line 3136, in _conv3d_expanded_batch
    name=name)
  File ""...python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 1400, in conv3d
    _ops.raise_from_not_ok_status(e, name)
  File "".../python3.7/site-packages/tensorflow/python/framework/ops.py"", line 6941, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Number of channels in filter (2) must match last dimension of input (4) [Op:Conv3D]

Process finished with exit code 1
```
"
50391,GPU resource can not be found in tensorlfow-gpu2.3.0,"When I upgrade tensorflow from 2.1.0 to 2.3.0, the GPU can not be found.
My old environment: python3.6+CUDA10.2+Tensorflow-gpu2.1.0, I can successfully run my code using this environemnt, both in graph mode and non-Graph mode.

Then, I created a new environment: python3.7+Tensorflow-gpu2.3.0, and run my code using this environment. I found, the GPU resource could not be found (the output of the commound ""tf.config.experimental.list_physical_devices('GPU'"") is [ ]).
I think it's caused by the version of CUDA at the first glance. Then I upgrade CUDA from 10.2 to 11.0, but the issue remains unsolved.

Can anyone help solving this issue?"
50387,"Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void * )' to 'PyUFuncGenericFunction'","ERROR:.../pycharmprojects/pythonproject/tensorflow/tensorflow/python/BUILD:437:1: C++ compilation of rule '//tensorflow/python:bfloat16_lib' failed (
Exit 2)
tensorflow/python/lib/core/bfloat16.cc(635): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato
r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *
)' to 'PyUFuncGenericFunction'
tensorflow/python/lib/core/bfloat16.cc(635): note: None of the functions with this name in scope match the target type
tensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::
operator ()'
tensorflow/python/lib/core/bfloat16.cc(639): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato
r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *
)' to 'PyUFuncGenericFunction'
tensorflow/python/lib/core/bfloat16.cc(639): note: None of the functions with this name in scope match the target type
tensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::
operator ()'
tensorflow/python/lib/core/bfloat16.cc(643): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato
r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *
)' to 'PyUFuncGenericFunction'
tensorflow/python/lib/core/bfloat16.cc(643): note: None of the functions with this name in scope match the target type
tensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::
operator ()'
tensorflow/python/lib/core/bfloat16.cc(646): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato
r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *
)' to 'PyUFuncGenericFunction'
tensorflow/python/lib/core/bfloat16.cc(646): note: None of the functions with this name in scope match the target type
tensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::
operator ()'
tensorflow/python/lib/core/bfloat16.cc(650): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato
r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *
)' to 'PyUFuncGenericFunction'
tensorflow/python/lib/core/bfloat16.cc(650): note: None of the functions with this name in scope match the target type
tensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::
operator ()'
tensorflow/python/lib/core/bfloat16.cc(654): error C2664: 'bool tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::operato
r ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const': cannot convert argument 2 from 'void (__cdecl *)(char **,npy_intp *,npy_intp *,void *
)' to 'PyUFuncGenericFunction'
tensorflow/python/lib/core/bfloat16.cc(654): note: None of the functions with this name in scope match the target type
tensorflow/python/lib/core/bfloat16.cc(629): note: see declaration of 'tensorflow::`anonymous-namespace'::Initialize::<lambda_a7b8b31d992e5b5e4dc9d84f5f519c90>::
operator ()'
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 12840.328s, Critical Path: 449.09s
INFO: 7087 processes: 7087 local.
FAILED: Build did NOT complete successfully
///////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////
Tested build configuration 
cpu  tensorflow-2.2.0	python3.7	MSVC 2019	Bazel 2.0.0
"
50386,ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  windows 10
- TensorFlow installed from (source or binary):
- Tensorflow version (commit SHA if source):   Tensorflow 2.5.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):  ARM

**Describe the problem**

**Please provide the exact sequence of commands/steps when you ran into the problem**

After a long search in other forums for my problem, any solution fund won’t work for me. I hope that you can help me to overcome this problem.
the problem is while doing post-training integer quantization of a GRU model, it gives me the following error :
**ValueError: Failed to parse the model: pybind11::init(): factory function returned nullptr.**
I tried TensorFlow 2.4.1, tf-nightly, but no one works.
I think the problem is with my **representative_dataset** , but it works with CNN 1D.
**My code:**

`converter = tf.lite.TFLiteConverter.from_saved_model(LSTMMODEL_TF)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

def representative_dataset_gen():
	for sample in XX_data:
	    sample = np.expand_dims(sample.astype(np.float32), axis=0)
	    yield [sample]

converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

converter.representative_dataset = repr_data_gen
model_tflite = converter.convert()

open(LSTMODEL_TFLITE, ""wb"").write(model_tflite)`



"
50384,Ability to prevent a metric from accumulating during training,"I want a metric that computes the mean value over other metrics. E.g. something like the following:

```
def construct_mean_accuracy(metrics):
    ''' Pass in a list of all other metrics to compute the mean value over all of them. '''

    def mean_accuracy(y_true, y_pred, sample_weight=None):
        return tf.math.reduce_mean([m.result() for m in metrics])

    return mean_accuracy
```

The metrics that I'm computing the mean over here are themselves accumulated (i.e. 'streamed') accuracy values that build up over an epoch. The `mean_accuracy` value printed by TensorFlow during `Model.fit()` is wrong, because TF is automatically accumulating the batch-wise metric values of `mean_accuracy` through a rolling mean (I think). This behaviour has caused confusion for people in the past (see #9498, #15115, #42994), and I couldn't find any documentation on it. Is there a way to prevent the accumulation behaviour and just return the batch-wise value? The desired final `on_epoch_end` value for `mean_accuracy` should just be the mean over the metrics in the final batch (because the other metrics are themselves accumulated metrics).

Let me know if I can make this any clearer.
"
50383,Didn't find op for builtin opcode 'SUM' version '1',"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS 11.4
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): 2.5 (a4dfb8d1a71)
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM Cortex M4

**Describe the problem**
The SUM operator is not supported in TF Lite Microcontroller, even though it is supported in TF Lite. I assume it's a pretty easy operation to get ported to MCU? I'm getting 
`Didn't find op for builtin opcode 'SUM' version '1'`

**Please provide the exact sequence of commands/steps when you ran into the problem**
Run any TF Model with SUM operator, `AllocateTensors()` will print out `Didn't find op for builtin opcode 'SUM' version '1'`
"
50381,"support CSR (data, indices, indptr), [shape=(M, N)] format in tf.sparse ","**System information**
- TensorFlow version (you are using): 2.5
- Are you willing to contribute it (Yes/No): YES


**Describe the feature and the current behavior/state.**

The current `tf.sparse` constructor only support COO format `((data, (row_ind, col_ind)), [shape=(M, N)])`. However, the standard sparse representation is CSR `(((data, indices, indptr), [shape=(M, N)]))`. It would nice to see both options in `tf.sparse`. [Scipy](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html) and [pytorch](https://pytorch.org/docs/stable/generated/torch._sparse_csr_tensor.html) already support both these cases. 
I investigate a bit more and find out there are some functionalities that already implemented at [repo](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/sparse/) but apparently, they are no longer maintain or accessible:

e.g., ```tf.compat.v1.raw_ops.DenseToCSRSparseMatrix``` is not working:

```
import tensorflow.compat.v1 as tf

tf.disable_v2_behavior()

tmp = tf.constant([[1, 2, 3, 4, 5], [0, 0, 0, 2, 1]], dtype=tf.float32)
indices = tf.where(tf.not_equal(tmp, 0))
b = tf.raw_ops.DenseToCSRSparseMatrix(dense_input=ss, indices=indices)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    print(sess.run(b)) 
```

**Will this change the current api? How?** No, it can be handle with if-else condition based on args 

**Who will benefit with this feature?** tf.sparse

"
50380,where should I ask my question regarding understanding tensorflow and keras better?,"Pytorch has a great support community, instead of sending people to stack overflow the have their discussion forum.
Does tensorflow has any place that new beginners can go and ask their questions? or you just send people to internet and stackoverflow and wish them luck?
I notice that github is more for bugs and problems not issues regarding understanding, please let me know if I am missing something?"
50379,Memory leak in model.fit,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>


The following issue still remains to be prevailing regardless of TF versions for many people, and I ought to re-open this issue on memory leakage. 

https://github.com/tensorflow/tensorflow/issues/37505

Currently, the only solution is a crack to wrap the sequence into multiprocessing. I hope this bug could be fixed as soon as possible. "
50378,"Input to reshape is a tensor with 256 values, but the requested shape has 16 	 [[node sequential/dense_features/G/Reshape (defined at <ipython-input-39-b5709d2bb787>:2) ]] [Op:__inference_predict_function_9466]","def predict_input_fn(fileNames,patch,bands):

  # You have to know the following from your export.
  PATCH_WIDTH, PATCH_HEIGHT = patch
  PATCH_DIMENSIONS_FLAT = [PATCH_WIDTH * PATCH_HEIGHT, 1]

  # Note that the tensors are in the shape of a patch, one patch for each band.
  imageColumns = [
    tf.io.FixedLenSequenceFeature(shape=PATCH_DIMENSIONS_FLAT, dtype=tf.float32, allow_missing=True) 
      for k in bands
  ]

  featuresDict = dict(zip(bands, imageColumns))
  dataset = tf.data.TFRecordDataset(fileNames, compression_type='GZIP')
  
  # Make a parsing function
  def parse_image(example_proto):
    parsed_features = tf.io.parse_single_example(example_proto, featuresDict)
    return parsed_features

  dataset = dataset.map(parse_image, num_parallel_calls=4)
  
  # Break our long tensors into many littler ones

  dataset = dataset.flat_map(lambda features: tf.data.Dataset.from_tensor_slices(features))

  # Read in batches corresponding to patch size.
  dataset = dataset.batch(PATCH_WIDTH * PATCH_HEIGHT)

  return dataset


predict_db = predict_input_fn(fileNames=imageFilesList,patch=[4,4],bands=['R', 'G', 'B', 'NDVI'])
predictions = model.predict(predict_db)"
50377,Bug in custom layer input tensor,"System: Python 3.8.3 64-bit | Qt 5.12.9 | PyQt5 5.12.3 | Linux 5.8.0-55-generic 
              keras (newest version)
```#!/usr/bin/env python3
from keras.layers import Layer
import numpy as np
from keras.layers import Input
from keras.models import Model  

InputTensor = [0.1,0.2,0.3]      # Attention! Different tensors cause errors. 
                                 # See  below the code!

InputTensor = np.array(InputTensor) 

class MyLayer(Layer):
    
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, x):
        return x

inputlayer = Input(shape = InputTensor.shape)
layer = MyLayer()(inputlayer)

model = Model(inputlayer, layer ) 
model.summary()

Output = model.predict(InputTensor)
output = np.array(Output)
print(""dim(Output) = ""+str(output.ndim))
print(""Output = ""+str(Output))
```

1. Input tensor [0.1,0.2,0.3] cause no problems. Output is [[0.1][0.2][0.2]]. Access to tensor elements are possible. model.summary is (None,2), ndim is 2.
2. Input tensor [[0.1, 0.2,0.3],[0.2,0.3,0.4],[0.3,0.4,0.5]] has an output as [[0.1 0.2 0.3][0.2 0.3 0.4][0.3 0.4 0.5]] and model.summary indicate a shape like (None,3,3) so that a tensor of tree dimensions is expected. ndim should be 3 instead of 2! Access to tensor elements is possible if you consider 2 dimensions instead of 3.
3. Input tensor [[0.1,0.2,0.3],[0.2,0.3,0.4]] generates an error message. It expected a shape of (None,2,3) and found only (None,3) although model.summary shows (None,2,3). 

It shouldn't have any influence how many dimensions a tensor has, nor how big its elements are!
My task is, to get access to single elements of a tensor to generate an output tensor something like
```
class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, x):
        OutputTensor = [[0.0, 0.0],
                        [0.0, 0.0]]
        OutputTensor[0][0] = x[0][0]
        OutputTensor[1][1] = x[1][0]
        OutputTensor[1][0] = x[2][0]
        return OutputTensor
```
"
50376,question about the roadmap of xla send/recv ops supporting.,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TensorFlow master branch with commit id: 3ef6fbfd02716f774024afae711383fdb0d8a30c
- Are you willing to contribute it (Yes/No): Yes
 

**Describe the feature and the current behavior/state.**
I'm starting my project on top of tensorflow with cross-machine communication support. Currently, we want to use the send/recv ops for multicast/p2p communication in HLO graph. However, we find that the latest tensorflow has not fully supported such ops in xla. I would like to query the roadmap for the support.
The send/recv hlo file for test is as follows:

HloModule TwoSendRecvBothWayRecvFist_module

ENTRY %TwoSendRecvBothWayRecvFist.v3 () -> (f32[], token[]) {
  %token0 = token[] after-all()
  %recv = (f32[], u32[], token[]) recv(token[] %token0), channel_id=15, sharding={maximal device=1}
  ROOT %recv-done = (f32[], token[]) recv-done((f32[], u32[], token[]) %recv), channel_id=15, sharding={maximal device=1}
  %constant = f32[] constant(2.1), sharding={maximal device=0}
  %send = (f32[], u32[], token[]) send(f32[] %constant, token[] %token0), channel_id=16, sharding={maximal device=0}, control-predecessors={%recv}
  %send-done = token[] send-done((f32[], u32[], token[]) %send), channel_id=16, sharding={maximal device=0}
}

The current tensorflow  would end up with an error when compiling the hlo graph as follows:
Attempting to fetch value instead of handling error Internal: LHLO opcode recv is not supported.


We notice that the ""xla all_to_all"" communication primitive was added last month (https://github.com/tensorflow/tensorflow/blob/636833c8ff5f716dfa2eddd60a9eae905d8f715a/tensorflow/compiler/xla/service/gpu/nccl_all_to_all_thunk.cc#L65). So are the support of xla send/recv ops in scheduling?

Thanks a lot.


**Will this change the current api? How?**
Not sure.
**Who will benefit with this feature?**
This feature would enable us to develop more flexible parallelisms for large model training. such as pipeline parallelism support in xla level.

**Any Other info.**
None







"
50375,Issues in deciphering TensorFlowLite Interpreter output,"I have been doing an image classification problem where in the objective is to train a predefined neural network model with set of tfrecords and do inference. This all is happening with reasonable accuracy In Colab.

Subsequent to this I converted the saved_model.pb into model.tflite file. I have checked it against the netron app it is seemingly taking correct inputs (which is an image tensor).

After this I called interpreter.invoke().

Following this when I try to decipher the output tensor I should be able to at least render the output the image, but i am having difficulty in doing this.

This is the link of colab notebook https://colab.research.google.com/drive/1kisxBtImUS7sT2aQlsEKfcXaEnsKumGw?usp=sharing where I have maintained the code.

I have other colab notebooks where similar code was done with training for upto 7500 iterations, but i am stuck in all the case at the interpreter level, since i have to port this app on to Android platform"
50373,Django ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version: 3.9
- Installed using virtualenv? pip? conda?:Pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
 I get this error when installing Django and Django rest framework :
ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'c:\\python39\\Scripts\\django-admin.py'


**Provide the exact sequence of commands / steps that you executed before running into the problem**
pip install django djangorestframework

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

WARNING: Ignoring invalid distribution -ip (c:\python39\lib\site-packages)
WARNING: Ignoring invalid distribution -ip (c:\python39\lib\site-packages)
Collecting django
  Using cached Django-3.2.4-py3-none-any.whl (7.9 MB)
Collecting djangorestframework
  Using cached djangorestframework-3.12.4-py3-none-any.whl (957 kB)
Requirement already satisfied: asgiref<4,>=3.3.2 in c:\python39\lib\site-packages (from django) (3.3.4)
Requirement already satisfied: sqlparse>=0.2.2 in c:\python39\lib\site-packages (from django) (0.4.1)
Requirement already satisfied: pytz in c:\python39\lib\site-packages (from django) (2021.1)
WARNING: Ignoring invalid distribution -ip (c:\python39\lib\site-packages)
Installing collected packages: django, djangorestframework
ERROR: Could not install packages due to an OSError: [Errno 13] Permission denied: 'c:\\python39\\Scripts\\django-admin.py'
Consider using the `--user` option or check the permissions.

WARNING: Ignoring invalid distribution -ip (c:\python39\lib\site-packages)
WARNING: Ignoring invalid distribution -ip (c:\python39\lib\site-packages)
WARNING: Ignoring invalid distribution -ip (c:\python39\lib\site-packages)
WARNING: You are using pip version 21.1.1; however, version 21.1.2 is available.
You should consider upgrading via the 'c:\python39\python.exe -m pip install --upgrade pip' command."
50372,cannot import name '__version__' from 'keras',"If I open the codelab and follow the instructions, the code blocks eventually fail when 
any part of tensorflow imports keras.

This notebook uses nightly packages

 print(tf.version.GIT_VERSION, tf.version.VERSION)
v1.12.1-58938-g67a33c36c28 2.6.0-dev20210620

**Describe the expected behavior**
I expect to be able to `import keras`

https://www.tensorflow.org/guide/advanced_autodiff

**Describe the current behavior**

```
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
<ipython-input-4-4f42995a43b2> in <module>()
      3 import matplotlib as mpl
      4 import matplotlib.pyplot as plt
----> 5 import keras
      6 
      7 mpl.rcParams['figure.figsize'] = (8, 6)

9 frames
/usr/local/lib/python3.7/dist-packages/keras/api/_v2/keras/__init__.py in <module>()
      8 import sys as _sys
      9 
---> 10 from keras import __version__
     11 from keras.api._v2.keras import activations
     12 from keras.api._v2.keras import applications

ImportError: cannot import name '__version__' from 'keras' (/usr/local/lib/python3.7/dist-packages/keras/__init__.py)

---------------------------------------------------------------------------
NOTE: If your import is failing due to a missing package, you can
manually install dependencies using either !pip or !apt.

To view examples of installing some common dependencies, click the
""Open Examples"" button below.
-------------------------------
```
"
50370,Brand new Pipenv environment can't install tensorflow?,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
  ```bash
  ProductName:	macOS
  ProductVersion:	11.4
  BuildVersion:	20F71
  ```
- TensorFlow installed from (source or binary): source?
- TensorFlow version: 2.5.0 and below
- Python version: 3.8.10
- Installed using virtualenv? pip? conda?: `pipenv`
- GCC/Compiler version (if compiling from source):
  ```
  ❯ gcc --version
  Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/c++/4.2.1
  Apple clang version 12.0.5 (clang-1205.0.22.9)
  Target: x86_64-apple-darwin20.5.0
  Thread model: posix
  InstalledDir: /Library/Developer/CommandLineTools/usr/bin
  ```



**Describe the problem**
Tensorflow can't install in a brand new virtual environment.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```bash
~/test/Python
❯ pipenv install tensorflow
Installing tensorflow...
Adding tensorflow to Pipfile's [packages]...
✔ Installation Succeeded
Pipfile.lock (29ca33) out of date, updating to (cc17c6)...
Locking [dev-packages] dependencies...
Locking [packages] dependencies...
Building requirements...
Resolving dependencies...
✘ Locking Failed!
[ResolutionFailure]:   File ""/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv/resolver.py"", line 741, in _main
[ResolutionFailure]:       resolve_packages(pre, clear, verbose, system, write, requirements_dir, packages, dev)
[ResolutionFailure]:   File ""/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv/resolver.py"", line 702, in resolve_packages
[ResolutionFailure]:       results, resolver = resolve(
[ResolutionFailure]:   File ""/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv/resolver.py"", line 684, in resolve
[ResolutionFailure]:       return resolve_deps(
[ResolutionFailure]:   File ""/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv/utils.py"", line 1397, in resolve_deps
[ResolutionFailure]:       results, hashes, markers_lookup, resolver, skipped = actually_resolve_deps(
[ResolutionFailure]:   File ""/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv/utils.py"", line 1110, in actually_resolve_deps
[ResolutionFailure]:       resolver.resolve()
[ResolutionFailure]:   File ""/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv/utils.py"", line 835, in resolve
[ResolutionFailure]:       raise ResolutionFailure(message=str(e))
[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies.
  First try clearing your dependency cache with $ pipenv lock --clear, then try the original command again.
 Alternatively, you can use $ pipenv install --skip-lock to bypass this mechanism, then run $ pipenv graph to inspect the situation.
  Hint: try $ pipenv lock --pre if it is a pre-release dependency.
ERROR: Could not find a version that matches tensorflow (from -r /var/folders/28/2sy26r512hq5jdpx18q4ycjw0000gn/T/pipenvf89b7toyrequirements/pipenv-gherejrb-constraints.txt (line 6))
No versions found
Was https://pypi.org/simple reachable?
```

**Any other info / logs**
Pipenv support output
<details><summary>$ pipenv --support</summary>

Pipenv version: `'2021.5.29'`

Pipenv location: `'/Users/anthony/.local/pipx/venvs/pipenv/lib/python3.8/site-packages/pipenv'`

Python location: `'/Users/anthony/.local/pipx/venvs/pipenv/bin/python'`

Python installations found:

  - `3.9.5`: `/Users/anthony/.pyenv/versions/3.9.5/bin/python3`
  - `3.9.5`: `/usr/local/bin/python3`
  - `3.9.5`: `/usr/local/bin/python3.9`
  - `3.8.10`: `/Users/anthony/.pyenv/versions/3.8.10/bin/python3`
  - `3.8.2`: `/usr/bin/python3`
  - `3.7.10`: `/Users/anthony/.pyenv/versions/3.7.10/bin/python3`
  - `2.7.16`: `/usr/bin/python2`
  - `2.7.16`: `/usr/bin/python2.7`

PEP 508 Information:

```
{'implementation_name': 'cpython',
 'implementation_version': '3.8.10',
 'os_name': 'posix',
 'platform_machine': 'x86_64',
 'platform_python_implementation': 'CPython',
 'platform_release': '20.5.0',
 'platform_system': 'Darwin',
 'platform_version': 'Darwin Kernel Version 20.5.0: Sat May  8 05:10:33 PDT '
                     '2021; root:xnu-7195.121.3~9/RELEASE_X86_64',
 'python_full_version': '3.8.10',
 'python_version': '3.8',
 'sys_platform': 'darwin'}
```

System environment variables:

  - `TERM_SESSION_ID`
  - `SSH_AUTH_SOCK`
  - `LC_TERMINAL_VERSION`
  - `COLORFGBG`
  - `ITERM_PROFILE`
  - `XPC_FLAGS`
  - `LANG`
  - `PWD`
  - `SHELL`
  - `__CFBundleIdentifier`
  - `TERM_PROGRAM_VERSION`
  - `TERM_PROGRAM`
  - `PATH`
  - `LC_TERMINAL`
  - `COLORTERM`
  - `COMMAND_MODE`
  - `TERM`
  - `HOME`
  - `TMPDIR`
  - `USER`
  - `XPC_SERVICE_NAME`
  - `LOGNAME`
  - `ITERM_SESSION_ID`
  - `__CF_USER_TEXT_ENCODING`
  - `SHLVL`
  - `OLDPWD`
  - `P9K_TTY`
  - `_P9K_TTY`
  - `P9K_SSH`
  - `ZSH`
  - `MANPAGER`
  - `SDKROOT`
  - `PAGER`
  - `LESS`
  - `LSCOLORS`
  - `RBENV_SHELL`
  - `YSU_VERSION`
  - `_`
  - `PIP_DISABLE_PIP_VERSION_CHECK`
  - `PYTHONDONTWRITEBYTECODE`
  - `PIP_SHIMS_BASE_MODULE`
  - `PIP_PYTHON_PATH`
  - `PYTHONFINDER_IGNORE_UNSUPPORTED`

Pipenv–specific environment variables:


Debug–specific environment variables:

  - `PATH`: `/Users/anthony/.rbenv/shims:/Users/anthony/.pyenv/shims:/Users/anthony/.local/share/gem/ruby/3.0.0/bin:/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:/Users/anthony/.local/bin:/Users/anthony/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Library/TeX/texbin:/Library/Apple/usr/bin:/usr/local/opt/fzf/bin`
  - `SHELL`: `/bin/zsh`
  - `LANG`: `en_US.UTF-8`
  - `PWD`: `/Users/anthony/test/Python`


---------------------------

Contents of `Pipfile` ('/Users/anthony/test/Python/Pipfile'):

```toml
[[source]]
url = ""https://pypi.org/simple""
verify_ssl = true
name = ""pypi""

[packages]
pandas = ""*""
ipykernel = ""*""
tabulate = ""*""
markdown = ""*""

[dev-packages]

[requires]
python_version = ""3.8""

```


Contents of `Pipfile.lock` ('/Users/anthony/test/Python/Pipfile.lock'):

```json
{
    ""_meta"": {
        ""hash"": {
            ""sha256"": ""858b7acf7ae36f845ff8aa68ad3d39b0ffe174ba51524472f7adf2266429ca33""
        },
        ""pipfile-spec"": 6,
        ""requires"": {
            ""python_version"": ""3.8""
        },
        ""sources"": [
            {
                ""name"": ""pypi"",
                ""url"": ""https://pypi.org/simple"",
                ""verify_ssl"": true
            }
        ]
    },
    ""default"": {
        ""appnope"": {
            ""hashes"": [
                ""sha256:93aa393e9d6c54c5cd570ccadd8edad61ea0c4b9ea7a01409020c9aa019eb442"",
                ""sha256:dd83cd4b5b460958838f6eb3000c660b1f9caf2a5b1de4264e941512f603258a""
            ],
            ""markers"": ""sys_platform == 'darwin' and platform_system == 'Darwin'"",
            ""version"": ""==0.1.2""
        },
        ""backcall"": {
            ""hashes"": [
                ""sha256:5cbdbf27be5e7cfadb448baf0aa95508f91f2bbc6c6437cd9cd06e2a4c215e1e"",
                ""sha256:fbbce6a29f263178a1f7915c1940bde0ec2b2a967566fe1c65c1dfb7422bd255""
            ],
            ""version"": ""==0.2.0""
        },
        ""decorator"": {
            ""hashes"": [
                ""sha256:6e5c199c16f7a9f0e3a61a4a54b3d27e7dad0dbdde92b944426cb20914376323"",
                ""sha256:72ecfba4320a893c53f9706bebb2d55c270c1e51a28789361aa93e4a21319ed5""
            ],
            ""markers"": ""python_version >= '3.5'"",
            ""version"": ""==5.0.9""
        },
        ""ipykernel"": {
            ""hashes"": [
                ""sha256:29eee66548ee7c2edb7941de60c0ccf0a7a8dd957341db0a49c5e8e6a0fcb712"",
                ""sha256:e976751336b51082a89fc2099fb7f96ef20f535837c398df6eab1283c2070884""
            ],
            ""index"": ""pypi"",
            ""version"": ""==5.5.5""
        },
        ""ipython"": {
            ""hashes"": [
                ""sha256:9bc24a99f5d19721fb8a2d1408908e9c0520a17fff2233ffe82620847f17f1b6"",
                ""sha256:d513e93327cf8657d6467c81f1f894adc125334ffe0e4ddd1abbb1c78d828703""
            ],
            ""markers"": ""python_version >= '3.7'"",
            ""version"": ""==7.24.1""
        },
        ""ipython-genutils"": {
            ""hashes"": [
                ""sha256:72dd37233799e619666c9f639a9da83c34013a73e8bbc79a7a6348d93c61fab8"",
                ""sha256:eb2e116e75ecef9d4d228fdc66af54269afa26ab4463042e33785b887c628ba8""
            ],
            ""version"": ""==0.2.0""
        },
        ""jedi"": {
            ""hashes"": [
                ""sha256:18456d83f65f400ab0c2d3319e48520420ef43b23a086fdc05dff34132f0fb93"",
                ""sha256:92550a404bad8afed881a137ec9a461fed49eca661414be45059329614ed0707""
            ],
            ""markers"": ""python_version >= '3.6'"",
            ""version"": ""==0.18.0""
        },
        ""jupyter-client"": {
            ""hashes"": [
                ""sha256:c4bca1d0846186ca8be97f4d2fa6d2bae889cce4892a167ffa1ba6bd1f73e782"",
                ""sha256:e053a2c44b6fa597feebe2b3ecb5eea3e03d1d91cc94351a52931ee1426aecfc""
            ],
            ""markers"": ""python_version >= '3.5'"",
            ""version"": ""==6.1.12""
        },
        ""jupyter-core"": {
            ""hashes"": [
                ""sha256:79025cb3225efcd36847d0840f3fc672c0abd7afd0de83ba8a1d3837619122b4"",
                ""sha256:8c6c0cac5c1b563622ad49321d5ec47017bd18b94facb381c6973a0486395f8e""
            ],
            ""markers"": ""python_version >= '3.6'"",
            ""version"": ""==4.7.1""
        },
        ""markdown"": {
            ""hashes"": [
                ""sha256:31b5b491868dcc87d6c24b7e3d19a0d730d59d3e46f4eea6430a321bed387a49"",
                ""sha256:96c3ba1261de2f7547b46a00ea8463832c921d3f9d6aba3f255a6f71386db20c""
            ],
            ""index"": ""pypi"",
            ""version"": ""==3.3.4""
        },
        ""matplotlib-inline"": {
            ""hashes"": [
                ""sha256:5cf1176f554abb4fa98cb362aa2b55c500147e4bdbb07e3fda359143e1da0811"",
                ""sha256:f41d5ff73c9f5385775d5c0bc13b424535c8402fe70ea8210f93e11f3683993e""
            ],
            ""markers"": ""python_version >= '3.5'"",
            ""version"": ""==0.1.2""
        },
        ""numpy"": {
            ""hashes"": [
                ""sha256:1676b0a292dd3c99e49305a16d7a9f42a4ab60ec522eac0d3dd20cdf362ac010"",
                ""sha256:16f221035e8bd19b9dc9a57159e38d2dd060b48e93e1d843c49cb370b0f415fd"",
                ""sha256:43909c8bb289c382170e0282158a38cf306a8ad2ff6dfadc447e90f9961bef43"",
                ""sha256:4e465afc3b96dbc80cf4a5273e5e2b1e3451286361b4af70ce1adb2984d392f9"",
                ""sha256:55b745fca0a5ab738647d0e4db099bd0a23279c32b31a783ad2ccea729e632df"",
                ""sha256:5d050e1e4bc9ddb8656d7b4f414557720ddcca23a5b88dd7cff65e847864c400"",
                ""sha256:637d827248f447e63585ca3f4a7d2dfaa882e094df6cfa177cc9cf9cd6cdf6d2"",
                ""sha256:6690080810f77485667bfbff4f69d717c3be25e5b11bb2073e76bb3f578d99b4"",
                ""sha256:66fbc6fed94a13b9801fb70b96ff30605ab0a123e775a5e7a26938b717c5d71a"",
                ""sha256:67d44acb72c31a97a3d5d33d103ab06d8ac20770e1c5ad81bdb3f0c086a56cf6"",
                ""sha256:6ca2b85a5997dabc38301a22ee43c82adcb53ff660b89ee88dded6b33687e1d8"",
                ""sha256:6e51534e78d14b4a009a062641f465cfaba4fdcb046c3ac0b1f61dd97c861b1b"",
                ""sha256:70eb5808127284c4e5c9e836208e09d685a7978b6a216db85960b1a112eeace8"",
                ""sha256:830b044f4e64a76ba71448fce6e604c0fc47a0e54d8f6467be23749ac2cbd2fb"",
                ""sha256:8b7bb4b9280da3b2856cb1fc425932f46fba609819ee1c62256f61799e6a51d2"",
                ""sha256:a9c65473ebc342715cb2d7926ff1e202c26376c0dcaaee85a1fd4b8d8c1d3b2f"",
                ""sha256:c1c09247ccea742525bdb5f4b5ceeacb34f95731647fe55774aa36557dbb5fa4"",
                ""sha256:c5bf0e132acf7557fc9bb8ded8b53bbbbea8892f3c9a1738205878ca9434206a"",
                ""sha256:db250fd3e90117e0312b611574cd1b3f78bec046783195075cbd7ba9c3d73f16"",
                ""sha256:e515c9a93aebe27166ec9593411c58494fa98e5fcc219e47260d9ab8a1cc7f9f"",
                ""sha256:e55185e51b18d788e49fe8305fd73ef4470596b33fc2c1ceb304566b99c71a69"",
                ""sha256:ea9cff01e75a956dbee133fa8e5b68f2f92175233de2f88de3a682dd94deda65"",
                ""sha256:f1452578d0516283c87608a5a5548b0cdde15b99650efdfd85182102ef7a7c17"",
                ""sha256:f39a995e47cb8649673cfa0579fbdd1cdd33ea497d1728a6cb194d6252268e48""
            ],
            ""markers"": ""python_version >= '3.7'"",
            ""version"": ""==1.20.3""
        },
        ""pandas"": {
            ""hashes"": [
                ""sha256:167693a80abc8eb28051fbd184c1b7afd13ce2c727a5af47b048f1ea3afefff4"",
                ""sha256:2111c25e69fa9365ba80bbf4f959400054b2771ac5d041ed19415a8b488dc70a"",
                ""sha256:298f0553fd3ba8e002c4070a723a59cdb28eda579f3e243bc2ee397773f5398b"",
                ""sha256:2b063d41803b6a19703b845609c0b700913593de067b552a8b24dd8eeb8c9895"",
                ""sha256:2cb7e8f4f152f27dc93f30b5c7a98f6c748601ea65da359af734dd0cf3fa733f"",
                ""sha256:52d2472acbb8a56819a87aafdb8b5b6d2b3386e15c95bde56b281882529a7ded"",
                ""sha256:612add929bf3ba9d27b436cc8853f5acc337242d6b584203f207e364bb46cb12"",
                ""sha256:649ecab692fade3cbfcf967ff936496b0cfba0af00a55dfaacd82bdda5cb2279"",
                ""sha256:68d7baa80c74aaacbed597265ca2308f017859123231542ff8a5266d489e1858"",
                ""sha256:8d4c74177c26aadcfb4fd1de6c1c43c2bf822b3e0fc7a9b409eeaf84b3e92aaa"",
                ""sha256:971e2a414fce20cc5331fe791153513d076814d30a60cd7348466943e6e909e4"",
                ""sha256:9db70ffa8b280bb4de83f9739d514cd0735825e79eef3a61d312420b9f16b758"",
                ""sha256:b730add5267f873b3383c18cac4df2527ac4f0f0eed1c6cf37fcb437e25cf558"",
                ""sha256:bd659c11a4578af740782288cac141a322057a2e36920016e0fc7b25c5a4b686"",
                ""sha256:c601c6fdebc729df4438ec1f62275d6136a0dd14d332fc0e8ce3f7d2aadb4dd6"",
                ""sha256:d0877407359811f7b853b548a614aacd7dea83b0c0c84620a9a643f180060950""
            ],
            ""index"": ""pypi"",
            ""version"": ""==1.2.4""
        },
        ""parso"": {
            ""hashes"": [
                ""sha256:12b83492c6239ce32ff5eed6d3639d6a536170723c6f3f1506869f1ace413398"",
                ""sha256:a8c4922db71e4fdb90e0d0bc6e50f9b273d3397925e5e60a717e719201778d22""
            ],
            ""markers"": ""python_version >= '3.6'"",
            ""version"": ""==0.8.2""
        },
        ""pexpect"": {
            ""hashes"": [
                ""sha256:0b48a55dcb3c05f3329815901ea4fc1537514d6ba867a152b581d69ae3710937"",
                ""sha256:fc65a43959d153d0114afe13997d439c22823a27cefceb5ff35c2178c6784c0c""
            ],
            ""markers"": ""sys_platform != 'win32'"",
            ""version"": ""==4.8.0""
        },
        ""pickleshare"": {
            ""hashes"": [
                ""sha256:87683d47965c1da65cdacaf31c8441d12b8044cdec9aca500cd78fc2c683afca"",
                ""sha256:9649af414d74d4df115d5d718f82acb59c9d418196b7b4290ed47a12ce62df56""
            ],
            ""version"": ""==0.7.5""
        },
        ""prompt-toolkit"": {
            ""hashes"": [
                ""sha256:08360ee3a3148bdb5163621709ee322ec34fc4375099afa4bbf751e9b7b7fa4f"",
                ""sha256:7089d8d2938043508aa9420ec18ce0922885304cddae87fb96eebca942299f88""
            ],
            ""markers"": ""python_full_version >= '3.6.1'"",
            ""version"": ""==3.0.19""
        },
        ""ptyprocess"": {
            ""hashes"": [
                ""sha256:4b41f3967fce3af57cc7e94b888626c18bf37a083e3651ca8feeb66d492fef35"",
                ""sha256:5c5d0a3b48ceee0b48485e0c26037c0acd7d29765ca3fbb5cb3831d347423220""
            ],
            ""version"": ""==0.7.0""
        },
        ""pygments"": {
            ""hashes"": [
                ""sha256:a18f47b506a429f6f4b9df81bb02beab9ca21d0a5fee38ed15aef65f0545519f"",
                ""sha256:d66e804411278594d764fc69ec36ec13d9ae9147193a1740cd34d272ca383b8e""
            ],
            ""markers"": ""python_version >= '3.5'"",
            ""version"": ""==2.9.0""
        },
        ""python-dateutil"": {
            ""hashes"": [
                ""sha256:73ebfe9dbf22e832286dafa60473e4cd239f8592f699aa5adaf10050e6e1823c"",
                ""sha256:75bb3f31ea686f1197762692a9ee6a7550b59fc6ca3a1f4b5d7e32fb98e2da2a""
            ],
            ""markers"": ""python_version >= '2.7' and python_version not in '3.0, 3.1, 3.2, 3.3'"",
            ""version"": ""==2.8.1""
        },
        ""pytz"": {
            ""hashes"": [
                ""sha256:83a4a90894bf38e243cf052c8b58f381bfe9a7a483f6a9cab140bc7f702ac4da"",
                ""sha256:eb10ce3e7736052ed3623d49975ce333bcd712c7bb19a58b9e2089d4057d0798""
            ],
            ""version"": ""==2021.1""
        },
        ""pyzmq"": {
            ""hashes"": [
                ""sha256:089b974ec04d663b8685ac90e86bfe0e4da9d911ff3cf52cb765ff22408b102d"",
                ""sha256:0ea7f4237991b0f745a4432c63e888450840bf8cb6c48b93fb7d62864f455529"",
                ""sha256:0f0f27eaab9ba7b92d73d71c51d1a04464a1da6097a252d007922103253d2313"",
                ""sha256:12ffcf33db6ba7c0e5aaf901e65517f5e2b719367b80bcbfad692f546a297c7a"",
                ""sha256:1389b615917d4196962a9b469e947ba862a8ec6f5094a47da5e7a8d404bc07a4"",
                ""sha256:18dd2ca4540c476558099891c129e6f94109971d110b549db2a9775c817cedbd"",
                ""sha256:24fb5bb641f0b2aa25fc3832f4b6fc62430f14a7d328229fe994b2bcdc07c93a"",
                ""sha256:285514956c08c7830da9d94e01f5414661a987831bd9f95e4d89cc8aaae8da10"",
                ""sha256:41049cff5265e9cd75606aa2c90a76b9c80b98d8fe70ee08cf4af3cedb113358"",
                ""sha256:461ed80d741692d9457ab820b1cc057ba9c37c394e67b647b639f623c8b321f6"",
                ""sha256:4b8fb1b3174b56fd020e4b10232b1764e52cf7f3babcfb460c5253bdc48adad0"",
                ""sha256:4c4fe69c7dc0d13d4ae180ad650bb900854367f3349d3c16f0569f6c6447f698"",
                ""sha256:4e9b9a2f6944acdaf57316436c1acdcb30b8df76726bcf570ad9342bc5001654"",
                ""sha256:6355f81947e1fe6e7bb9e123aeb3067264391d3ebe8402709f824ef8673fa6f3"",
                ""sha256:68be16107f41563b9f67d93dff1c9f5587e0f76aa8fd91dc04c83d813bcdab1f"",
                ""sha256:68e2c4505992ab5b89f976f89a9135742b18d60068f761bef994a6805f1cae0c"",
                ""sha256:7040d6dd85ea65703904d023d7f57fab793d7ffee9ba9e14f3b897f34ff2415d"",
                ""sha256:734ea6565c71fc2d03d5b8c7d0d7519c96bb5567e0396da1b563c24a4ac66f0c"",
                ""sha256:9ee48413a2d3cd867fd836737b4c89c24cea1150a37f4856d82d20293fa7519f"",
                ""sha256:a1c77796f395804d6002ff56a6a8168c1f98579896897ad7e35665a9b4a9eec5"",
                ""sha256:b2f707b52e09098a7770503e39294ca6e22ae5138ffa1dd36248b6436d23d78e"",
                ""sha256:bf80b2cec42d96117248b99d3c86e263a00469c840a778e6cb52d916f4fdf82c"",
                ""sha256:c4674004ed64685a38bee222cd75afa769424ec603f9329f0dd4777138337f48"",
                ""sha256:c6a81c9e6754465d09a87e3acd74d9bb1f0039b2d785c6899622f0afdb41d760"",
                ""sha256:c6d0c32532a0519997e1ded767e184ebb8543bdb351f8eff8570bd461e874efc"",
                ""sha256:c8fff75af4c7af92dce9f81fa2a83ed009c3e1f33ee8b5222db2ef80b94e242e"",
                ""sha256:cb9f9fe1305ef69b65794655fd89b2209b11bff3e837de981820a8aa051ef914"",
                ""sha256:d3ecfee2ee8d91ab2e08d2d8e89302c729b244e302bbc39c5b5dde42306ff003"",
                ""sha256:d5e5be93e1714a59a535bbbc086b9e4fd2448c7547c5288548f6fd86353cad9e"",
                ""sha256:de5806be66c9108e4dcdaced084e8ceae14100aa559e2d57b4f0cceb98c462de"",
                ""sha256:f49755684a963731479ff3035d45a8185545b4c9f662d368bd349c419839886d"",
                ""sha256:fc712a90401bcbf3fa25747f189d6dcfccbecc32712701cad25c6355589dac57""
            ],
            ""markers"": ""python_version >= '3.6'"",
            ""version"": ""==22.1.0""
        },
        ""six"": {
            ""hashes"": [
                ""sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926"",
                ""sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254""
            ],
            ""markers"": ""python_version >= '2.7' and python_version not in '3.0, 3.1, 3.2, 3.3'"",
            ""version"": ""==1.16.0""
        },
        ""tabulate"": {
            ""hashes"": [
                ""sha256:d7c013fe7abbc5e491394e10fa845f8f32fe54f8dc60c6622c6cf482d25d47e4"",
                ""sha256:eb1d13f25760052e8931f2ef80aaf6045a6cceb47514db8beab24cded16f13a7""
            ],
            ""index"": ""pypi"",
            ""version"": ""==0.8.9""
        },
        ""tornado"": {
            ""hashes"": [
                ""sha256:0a00ff4561e2929a2c37ce706cb8233b7907e0cdc22eab98888aca5dd3775feb"",
                ""sha256:0d321a39c36e5f2c4ff12b4ed58d41390460f798422c4504e09eb5678e09998c"",
                ""sha256:1e8225a1070cd8eec59a996c43229fe8f95689cb16e552d130b9793cb570a288"",
                ""sha256:20241b3cb4f425e971cb0a8e4ffc9b0a861530ae3c52f2b0434e6c1b57e9fd95"",
                ""sha256:25ad220258349a12ae87ede08a7b04aca51237721f63b1808d39bdb4b2164558"",
                ""sha256:33892118b165401f291070100d6d09359ca74addda679b60390b09f8ef325ffe"",
                ""sha256:33c6e81d7bd55b468d2e793517c909b139960b6c790a60b7991b9b6b76fb9791"",
                ""sha256:3447475585bae2e77ecb832fc0300c3695516a47d46cefa0528181a34c5b9d3d"",
                ""sha256:34ca2dac9e4d7afb0bed4677512e36a52f09caa6fded70b4e3e1c89dbd92c326"",
                ""sha256:3e63498f680547ed24d2c71e6497f24bca791aca2fe116dbc2bd0ac7f191691b"",
                ""sha256:548430be2740e327b3fe0201abe471f314741efcb0067ec4f2d7dcfb4825f3e4"",
                ""sha256:6196a5c39286cc37c024cd78834fb9345e464525d8991c21e908cc046d1cc02c"",
                ""sha256:61b32d06ae8a036a6607805e6720ef00a3c98207038444ba7fd3d169cd998910"",
                ""sha256:6286efab1ed6e74b7028327365cf7346b1d777d63ab30e21a0f4d5b275fc17d5"",
                ""sha256:65d98939f1a2e74b58839f8c4dab3b6b3c1ce84972ae712be02845e65391ac7c"",
                ""sha256:66324e4e1beede9ac79e60f88de548da58b1f8ab4b2f1354d8375774f997e6c0"",
                ""sha256:6c77c9937962577a6a76917845d06af6ab9197702a42e1346d8ae2e76b5e3675"",
                ""sha256:70dec29e8ac485dbf57481baee40781c63e381bebea080991893cd297742b8fd"",
                ""sha256:7250a3fa399f08ec9cb3f7b1b987955d17e044f1ade821b32e5f435130250d7f"",
                ""sha256:748290bf9112b581c525e6e6d3820621ff020ed95af6f17fedef416b27ed564c"",
                ""sha256:7da13da6f985aab7f6f28debab00c67ff9cbacd588e8477034c0652ac141feea"",
                ""sha256:8f959b26f2634a091bb42241c3ed8d3cedb506e7c27b8dd5c7b9f745318ddbb6"",
                ""sha256:9de9e5188a782be6b1ce866e8a51bc76a0fbaa0e16613823fc38e4fc2556ad05"",
                ""sha256:a48900ecea1cbb71b8c71c620dee15b62f85f7c14189bdeee54966fbd9a0c5bd"",
                ""sha256:b87936fd2c317b6ee08a5741ea06b9d11a6074ef4cc42e031bc6403f82a32575"",
                ""sha256:c77da1263aa361938476f04c4b6c8916001b90b2c2fdd92d8d535e1af48fba5a"",
                ""sha256:cb5ec8eead331e3bb4ce8066cf06d2dfef1bfb1b2a73082dfe8a161301b76e37"",
                ""sha256:cc0ee35043162abbf717b7df924597ade8e5395e7b66d18270116f8745ceb795"",
                ""sha256:d14d30e7f46a0476efb0deb5b61343b1526f73ebb5ed84f23dc794bdb88f9d9f"",
                ""sha256:d371e811d6b156d82aa5f9a4e08b58debf97c302a35714f6f45e35139c332e32"",
                ""sha256:d3d20ea5782ba63ed13bc2b8c291a053c8d807a8fa927d941bd718468f7b950c"",
                ""sha256:d3f7594930c423fd9f5d1a76bee85a2c36fd8b4b16921cae7e965f22575e9c01"",
                ""sha256:dcef026f608f678c118779cd6591c8af6e9b4155c44e0d1bc0c87c036fb8c8c4"",
                ""sha256:e0791ac58d91ac58f694d8d2957884df8e4e2f6687cdf367ef7eb7497f79eaa2"",
                ""sha256:e385b637ac3acaae8022e7e47dfa7b83d3620e432e3ecb9a3f7f58f150e50921"",
                ""sha256:e519d64089b0876c7b467274468709dadf11e41d65f63bba207e04217f47c085"",
                ""sha256:e7229e60ac41a1202444497ddde70a48d33909e484f96eb0da9baf8dc68541df"",
                ""sha256:ed3ad863b1b40cd1d4bd21e7498329ccaece75db5a5bf58cd3c9f130843e7102"",
                ""sha256:f0ba29bafd8e7e22920567ce0d232c26d4d47c8b5cf4ed7b562b5db39fa199c5"",
                ""sha256:fa2ba70284fa42c2a5ecb35e322e68823288a4251f9ba9cc77be04ae15eada68"",
                ""sha256:fba85b6cd9c39be262fcd23865652920832b61583de2a2ca907dbd8e8a8c81e5""
            ],
            ""markers"": ""python_version >= '3.5'"",
            ""version"": ""==6.1""
        },
        ""traitlets"": {
            ""hashes"": [
                ""sha256:178f4ce988f69189f7e523337a3e11d91c786ded9360174a3d9ca83e79bc5396"",
                ""sha256:69ff3f9d5351f31a7ad80443c2674b7099df13cc41fc5fa6e2f6d3b0330b0426""
            ],
            ""markers"": ""python_version >= '3.7'"",
            ""version"": ""==5.0.5""
        },
        ""wcwidth"": {
            ""hashes"": [
                ""sha256:beb4802a9cebb9144e99086eff703a642a13d6a0052920003a230f3294bbe784"",
                ""sha256:c4d647b99872929fdb7bdcaa4fbe7f01413ed3d98077df798530e5b04f116c83""
            ],
            ""version"": ""==0.2.5""
        }
    },
    ""develop"": {}
}

```
</details>

"
50369, error C2664: “bool tensorflow::`anonymous-namespace',"tensorflow/python/lib/core/bfloat16.cc(635): error C2664: “bool tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf
61205e9403a18f850441a>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const”: 无法将参数 2 从“void (
__cdecl *)(char **,npy_intp *,npy_intp *,void *)”转换为“PyUFuncGenericFunction”
tensorflow/python/lib/core/bfloat16.cc(635): note: 在匹配目标类型的范围内没有具有该名称的函数
tensorflow/python/lib/core/bfloat16.cc(629): note: 参见“tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf61205e94
03a18f850441a>::operator ()”的声明
tensorflow/python/lib/core/bfloat16.cc(639): error C2664: “bool tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf
61205e9403a18f850441a>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const”: 无法将参数 2 从“void (
__cdecl *)(char **,npy_intp *,npy_intp *,void *)”转换为“PyUFuncGenericFunction”
tensorflow/python/lib/core/bfloat16.cc(639): note: 在匹配目标类型的范围内没有具有该名称的函数
tensorflow/python/lib/core/bfloat16.cc(629): note: 参见“tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf61205e94
03a18f850441a>::operator ()”的声明
tensorflow/python/lib/core/bfloat16.cc(643): error C2664: “bool tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf
61205e9403a18f850441a>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const”: 无法将参数 2 从“void (
__cdecl *)(char **,npy_intp *,npy_intp *,void *)”转换为“PyUFuncGenericFunction”
tensorflow/python/lib/core/bfloat16.cc(643): note: 在匹配目标类型的范围内没有具有该名称的函数
tensorflow/python/lib/core/bfloat16.cc(629): note: 参见“tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf61205e94
03a18f850441a>::operator ()”的声明
tensorflow/python/lib/core/bfloat16.cc(646): error C2664: “bool tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf
61205e9403a18f850441a>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const”: 无法将参数 2 从“void (
__cdecl *)(char **,npy_intp *,npy_intp *,void *)”转换为“PyUFuncGenericFunction”
tensorflow/python/lib/core/bfloat16.cc(646): note: 在匹配目标类型的范围内没有具有该名称的函数
tensorflow/python/lib/core/bfloat16.cc(629): note: 参见“tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf61205e94
03a18f850441a>::operator ()”的声明
tensorflow/python/lib/core/bfloat16.cc(650): error C2664: “bool tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf
61205e9403a18f850441a>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const”: 无法将参数 2 从“void (
__cdecl *)(char **,npy_intp *,npy_intp *,void *)”转换为“PyUFuncGenericFunction”
tensorflow/python/lib/core/bfloat16.cc(650): note: 在匹配目标类型的范围内没有具有该名称的函数
tensorflow/python/lib/core/bfloat16.cc(629): note: 参见“tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf61205e94
03a18f850441a>::operator ()”的声明
tensorflow/python/lib/core/bfloat16.cc(654): error C2664: “bool tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf
61205e9403a18f850441a>::operator ()(const char *,PyUFuncGenericFunction,const std::array<int,3> &) const”: 无法将参数 2 从“void (
__cdecl *)(char **,npy_intp *,npy_intp *,void *)”转换为“PyUFuncGenericFunction”
tensorflow/python/lib/core/bfloat16.cc(654): note: 在匹配目标类型的范围内没有具有该名称的函数
tensorflow/python/lib/core/bfloat16.cc(629): note: 参见“tensorflow::`anonymous-namespace'::Initialize::<lambda_8175a7cc6bf61205e94
03a18f850441a>::operator ()”的声明
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 1600.752s, Critical Path: 348.93s
INFO: 566 processes: 566 local.
FAILED: Build did NOT complete successfully
"
50368,Dictionary learning using tensorflow,"Hii, experts. I am looking for the implementation of dictionary learning using Tensorflow. Are there any API for the same.Please suggest."
50363,Does not furhter progress after `Successfully opened dynamic library libcublasLt` using multi NVIDIA V100 GPUs,"TensorFlow program runs successfully with one GPU. It also works with multiple NVIDIA GTX Titian GPUs within 2 minutes. However, when more than one NVIDIA V100 GPU should be used, TensorFlow does not further progress after `Successfully opened dynamic library libcublasLt.so.11` and the GPU utilization raises to 100% according to `nvidia-smi`. 

**System information**
- OS Platform and Distribution:
  - CentOS 7
- TensorFlow installed from pip
- TensorFlow version 2.5.0
- Python version 3.6.8
- CUDA version 11.2
- cuDNN version 8.2.1.32
- GPU NVIDIA V100 32GB


**Standalone code to reproduce the issue**
```
#!/usr/bin/env python3
import tensorflow as tf
import os

print(""Tensorflow version: {}"".format(tf.__version__))
import tensorflow_datasets as tfds

# load dataset
tfds.download.DownloadManager(download_dir=os.path.dirname(os.path.realpath(__file__)))
datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True,
                           data_dir=os.path.dirname(os.path.realpath(__file__)))
mnist_train, mnist_test = datasets['train'], datasets['test']

# Set Strategy
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
num_train_examples = info.splits['train'].num_examples
num_test_examples = info.splits['test'].num_examples
BUFFER_SIZE = 10000
BATCH_SIZE_PER_REPLICA = 64
BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

def scale(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)
eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)
# Create model
with strategy.scope():
  model = tf.keras.Sequential([
      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),
      tf.keras.layers.MaxPooling2D(),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(64, activation='relu'),
      tf.keras.layers.Dense(10)
  ])

  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=['accuracy'])
## Define the checkpoint directory to store the checkpoints
checkpoint_dir = './training_checkpoints'
## Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, ""ckpt_{epoch}"")

# Function for decaying the learning rate.
def decay(epoch):
  if epoch < 3:
    return 1e-3
  elif epoch >= 3 and epoch < 7:
    return 1e-4
  else:
    return 1e-5

# Callback for printing the LR at the end of each epoch.
class PrintLR(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
                                                      model.optimizer.lr.numpy()))
callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='./logs'),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    tf.keras.callbacks.LearningRateScheduler(decay),
    PrintLR()
]

# Train and evaluate
model.fit(train_dataset, epochs=12, callbacks=callbacks)
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
eval_loss, eval_acc = model.evaluate(eval_dataset)
print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))


# Export to SavedModel
path = 'saved_model/'
model.save(path, save_format='tf')
unreplicated_model = tf.keras.models.load_model(path)
unreplicated_model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer=tf.keras.optimizers.Adam(),
    metrics=['accuracy'])

eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)
print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))


with strategy.scope():
  replicated_model = tf.keras.models.load_model(path)
  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
                           optimizer=tf.keras.optimizers.Adam(),
                           metrics=['accuracy'])
  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)
  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))
```

**LOG with multiple V100 GPUs**
```
2021-06-19 11:38:35.846512: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-06-19 11:38:37.567052: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Failed precondition: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata"".
Extraction completed...: 100%|##########| 4/4 [00:00<00:00,  5.59 file/s]
Dl Size...: 100%|##########| 10/10 [00:00<00:00, 13.96 MiB/s]
Dl Completed...: 100%|##########| 4/4 [00:00<00:00,  5.58 url/s]9 file/s]
Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/2021-06-19 11:38:38.388762: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-06-19 11:38:38.495458: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2021-06-19 11:38:38.499057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:25:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2021-06-19 11:38:38.499141: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-06-19 11:38:38.505167: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-06-19 11:38:38.505300: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
2021-06-19 11:38:38.507215: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10
2021-06-19 11:38:38.507603: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10
2021-06-19 11:38:38.509115: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11
2021-06-19 11:38:38.510381: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11
2021-06-19 11:38:38.510627: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-06-19 11:38:38.521523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2021-06-19 11:38:38.522242: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-19 11:38:39.059808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2021-06-19 11:38:39.065165: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: 
pciBusID: 0000:25:00.0 name: NVIDIA Tesla V100S-PCIE-32GB computeCapability: 7.0
coreClock: 1.597GHz coreCount: 80 deviceMemorySize: 31.75GiB deviceMemoryBandwidth: 1.03TiB/s
2021-06-19 11:38:39.079312: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1
2021-06-19 11:38:39.079492: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-06-19 11:38:39.860307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-19 11:38:39.860413: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 
2021-06-19 11:38:39.860431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N Y 
2021-06-19 11:38:39.860442: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   Y N 
2021-06-19 11:38:39.876738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30995 MB memory) -> physical GPU (device: 0, name: NVIDIA Tesla V100S-PCIE-32GB, pci bus id: 0000:01:00.0, compute capability: 7.0)
2021-06-19 11:38:39.880520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30995 MB memory) -> physical GPU (device: 1, name: NVIDIA Tesla V100S-PCIE-32GB, pci bus id: 0000:25:00.0, compute capability: 7.0)
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
2021-06-19 11:38:58.854683: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.
2021-06-19 11:38:58.854764: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.
2021-06-19 11:38:58.854849: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1611] Profiler found 2 GPUs
2021-06-19 11:38:58.856387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcupti.so.11.2
2021-06-19 11:38:59.211473: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session tear down.
2021-06-19 11:38:59.216559: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1743] CUPTI activity buffer flushed
2021-06-19 11:38:59.323866: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:461] The `assert_cardinality` transformation is currently not handled by the auto-shard rewrite and will be removed.
2021-06-19 11:38:59.406847: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-19 11:38:59.407366: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 1999970000 Hz
2021-06-19 11:39:04.122930: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8
2021-06-19 11:39:04.528403: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2021-06-19 11:39:05.177074: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2021-06-19 11:39:05.622628: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11
2021-06-19 11:39:06.126034: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11
```
"
50361,my model can be loaded but when I send request to do the predict I got some errors,"The error I got:
File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 2447, in wsgi_app
    response = self.full_dispatch_request()
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1952, in full_dispatch_request
    rv = self.handle_user_exception(e)
  File ""/usr/local/lib/python3.8/site-packages/flask_cors/extension.py"", line 165, in wrapped_function
    return cors_after_request(app.make_response(f(*args, **kwargs)))
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1821, in handle_user_exception
    reraise(exc_type, exc_value, tb)
  File ""/usr/local/lib/python3.8/site-packages/flask/_compat.py"", line 39, in reraise
    raise value
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1950, in full_dispatch_request
    rv = self.dispatch_request()
  File ""/usr/local/lib/python3.8/site-packages/flask/app.py"", line 1936, in dispatch_request
    return self.view_functions[rule.endpoint](**req.view_args)
  File ""/usr/local/lib/python3.8/site-packages/simple_tensorflow_serving/server.py"", line 380, in decorated
    return f(*decorator_args, **decorator_kwargs)
  File ""/usr/local/lib/python3.8/site-packages/simple_tensorflow_serving/server.py"", line 180, in inference
    response = jsonify(json_result)
  File ""/usr/local/lib/python3.8/site-packages/flask/json/__init__.py"", line 370, in jsonify
    dumps(data, indent=indent, separators=separators) + ""\n"",
  File ""/usr/local/lib/python3.8/site-packages/flask/json/__init__.py"", line 211, in dumps
    rv = _json.dumps(obj, **kwargs)
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/__init__.py"", line 234, in dumps
    return cls(
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
  File ""/usr/local/lib/python3.8/site-packages/flask/json/__init__.py"", line 100, in default
    return _json.JSONEncoder.default(self, o)
  File ""/usr/local/Cellar/python@3.8/3.8.5/Frameworks/Python.framework/Versions/3.8/lib/python3.8/json/encoder.py"", line 179, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
TypeError: Object of type bytes is not JSON serializable


The request:
{
  ""model_name"": ""default"",
  ""model_version"": 1,
  ""data"": {
      ""inputs"":["""","""",""""]
    }
}

I feel have difficulties to find the correct data structure, may I get some help from you?"
50359,AttributeError: 'BinaryAccuracy' object has no attribute 'reset_state',"<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro Linux 21.0.5 (Ornara)
- TensorFlow installed from (source or binary): binary (using conda)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.9.5
- GCC/Compiler version (if compiling from source): python compiler version: GCC 7.5.0, (compiler section shows c++ (GCC) 10.2.0)
- CUDA/cuDNN version: 11.2
- GPU model and memory: Quadro K2200

Tested in another system:
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora release 34
- TensorFlow installed from (source or binary): binary (using conda)
- TensorFlow version (use command below): 2.4.1
- Python version: 3.9.5
- GCC/Compiler version (if compiling from source): python compiler version: GCC 7.5.0, (compiler section shows c++ (GCC) 11.1.1)



**Describe the current behavior**

**`reset_state()` for `tf.keras.metrics.BinaryAccuracy` does not work.** 

My current code:

```python
import numpy as np
import tensorflow as tf

m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()

m.reset_state()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]], sample_weight=[1, 0, 0, 1])
m.result().numpy()
```

And the corresponding result:

```
2021-06-18 22:33:53.388589: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-06-18 22:33:53.388807: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-18 22:33:53.390306: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
Traceback (most recent call last):
  File ""/home/subrata/Researches/MRI/cGAN_2D_test_old/reset_test_problem.py"", line 8, in <module>
    m.reset_state()
AttributeError: 'BinaryAccuracy' object has no attribute 'reset_state'
```

**Describe the expected behavior**
The official [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/BinaryAccuracy) shows that this code should basically give:
```
0.75
0.5 
```

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**
```python
import numpy as np
import tensorflow as tf

m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()

m.reset_state()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]], sample_weight=[1, 0, 0, 1])
m.result().numpy()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The existing versions managed by conda 
(generated using `conda list --export`,
and virtual en can be generated using the channel `conda-forge`
`conda create --name workingenv --file <following as a file>  -c conda-forge`)
```
# This file may be used to create an environment using:
# $ conda create --name <env> --file <this file>
# platform: linux-64
_libgcc_mutex=0.1=main
_openmp_mutex=4.5=1_gnu
_tflow_select=2.3.0=mkl
absl-py=0.12.0=py39h06a4308_0
aiohttp=3.7.4=py39h27cfd23_1
astor=0.8.1=py39h06a4308_0
astunparse=1.6.3=py_0
async-timeout=3.0.1=py39h06a4308_0
attrs=21.2.0=pyhd3eb1b0_0
blas=1.0=mkl
blinker=1.4=py39h06a4308_0
brotlipy=0.7.0=py39h27cfd23_1003
c-ares=1.17.1=h27cfd23_0
ca-certificates=2021.5.25=h06a4308_1
cachetools=4.2.2=pyhd3eb1b0_0
certifi=2021.5.30=py39h06a4308_0
cffi=1.14.5=py39h261ae71_0
chardet=3.0.4=py39h06a4308_1003
click=8.0.1=pyhd3eb1b0_0
coverage=5.5=py39h27cfd23_2
cryptography=3.4.7=py39hd23ed53_0
cycler=0.10.0=py39h06a4308_0
cython=0.29.23=py39h2531618_0
dbus=1.13.18=hb2f20db_0
expat=2.4.1=h2531618_2
fontconfig=2.13.1=h6c09931_0
freetype=2.10.4=h5ab3b9f_0
gast=0.4.0=py_0
glib=2.68.2=h36276a3_0
google-auth=1.31.0=pyhd3eb1b0_0
google-auth-oauthlib=0.4.1=py_2
google-pasta=0.2.0=py_0
grpcio=1.36.1=py39h2157cd5_1
gst-plugins-base=1.14.0=h8213a91_2
gstreamer=1.14.0=h28cd5cc_2
h5py=2.10.0=py39hec9cf62_0
hdf5=1.10.6=hb1b8bf9_0
icu=58.2=he6710b0_3
idna=2.10=pyhd3eb1b0_0
importlib-metadata=3.10.0=py39h06a4308_0
intel-openmp=2021.2.0=h06a4308_610
jpeg=9b=h024ee3a_2
keras=2.4.3=0
keras-base=2.4.3=py_0
keras-preprocessing=1.1.2=pyhd3eb1b0_0
kiwisolver=1.3.1=py39h2531618_0
lcms2=2.12=h3be6417_0
ld_impl_linux-64=2.35.1=h7274673_9
libffi=3.3=he6710b0_2
libgcc-ng=9.3.0=h5101ec6_17
libgfortran-ng=7.5.0=ha8ba4b0_17
libgfortran4=7.5.0=ha8ba4b0_17
libgomp=9.3.0=h5101ec6_17
libpng=1.6.37=hbc83047_0
libprotobuf=3.14.0=h8c45485_0
libstdcxx-ng=9.3.0=hd4cf53a_17
libtiff=4.2.0=h85742a9_0
libuuid=1.0.3=h1bed415_2
libwebp-base=1.2.0=h27cfd23_0
libxcb=1.14=h7b6447c_0
libxml2=2.9.10=hb55368b_3
lz4-c=1.9.3=h2531618_0
markdown=3.3.4=py39h06a4308_0
matplotlib=3.3.4=py39h06a4308_0
matplotlib-base=3.3.4=py39h62a2d02_0
mkl=2021.2.0=h06a4308_296
mkl-service=2.3.0=py39h27cfd23_1
mkl_fft=1.3.0=py39h42c9631_2
mkl_random=1.2.1=py39ha9443f7_2
multidict=5.1.0=py39h27cfd23_2
ncurses=6.2=he6710b0_1
nibabel=3.2.1=pyhd8ed1ab_0
numpy=1.20.2=py39h2d18471_0
numpy-base=1.20.2=py39hfae3a4d_0
oauthlib=3.1.1=pyhd3eb1b0_0
olefile=0.46=py_0
openssl=1.1.1k=h27cfd23_0
opt_einsum=3.3.0=pyhd3eb1b0_1
packaging=20.9=pyhd3eb1b0_0
pcre=8.44=he6710b0_0
pillow=8.2.0=py39he98fc37_0
pip=21.1.2=py39h06a4308_0
protobuf=3.14.0=py39h2531618_1
pyasn1=0.4.8=py_0
pyasn1-modules=0.2.8=py_0
pycparser=2.20=py_2
pydicom=2.1.2=pyhd3deb0d_0
pydot=1.2.4=py_0
pyjwt=2.1.0=py39h06a4308_0
pyopenssl=20.0.1=pyhd3eb1b0_1
pyparsing=2.4.7=pyhd3eb1b0_0
pyqt=5.9.2=py39h2531618_6
pysocks=1.7.1=py39h06a4308_0
python=3.9.5=h12debd9_4
python-dateutil=2.8.1=pyhd3eb1b0_0
python-flatbuffers=1.12=pyhd3eb1b0_0
pyyaml=5.4.1=py39h27cfd23_1
qt=5.9.7=h5867ecd_1
readline=8.1=h27cfd23_0
requests=2.25.1=pyhd3eb1b0_0
requests-oauthlib=1.3.0=py_0
rsa=4.7.2=pyhd3eb1b0_1
scipy=1.6.2=py39had2a1c9_1
setuptools=52.0.0=py39h06a4308_0
sip=4.19.13=py39h2531618_0
six=1.16.0=pyhd3eb1b0_0
sqlite=3.35.4=hdfb4753_0
tensorboard=2.5.0=py_0
tensorboard-plugin-wit=1.6.0=py_0
tensorflow=2.4.1=mkl_py39h4683426_0
tensorflow-base=2.4.1=mkl_py39h43e0292_0
tensorflow-estimator=2.5.0=pyh7b7c402_0
termcolor=1.1.0=py39h06a4308_1
tk=8.6.10=hbc83047_0
tornado=6.1=py39h27cfd23_0
typing-extensions=3.7.4.3=hd3eb1b0_0
typing_extensions=3.7.4.3=pyh06a4308_0
tzdata=2020f=h52ac0ba_0
urllib3=1.26.4=pyhd3eb1b0_0
werkzeug=1.0.1=pyhd3eb1b0_0
wheel=0.36.2=pyhd3eb1b0_0
wrapt=1.12.1=py39he8ac12f_1
xz=5.2.5=h7b6447c_0
yaml=0.2.5=h7b6447c_0
yarl=1.6.3=py39h27cfd23_0
zipp=3.4.1=pyhd3eb1b0_0
zlib=1.2.11=h7b6447c_3
zstd=1.4.9=haebb681_0
```


"
50358,How string tensor to numpy array float32?,"tf.Tensor(
[b'[ 0.02097283 -0.00030136  0.02027434  0.01413146 -0.05076262]'
 b'[ 0.0262014   0.06108821  0.02988876  0.02189768 -0.10167235]'
 b'[ 0.03268104 -0.006452   -0.02572624 -0.02183151 -0.0120374 ]'
 b'[ 0.05750999  0.03042973 -0.00485849  0.03983038  0.01241926]'
 b'[-0.05423493  0.03896134  0.08271325 -0.00260455 -0.0601045 ]'
 b'[-0.05363343  0.0085776   0.06006461 -0.04733948  0.03426042]'
 b'[-0.00538311 -0.03473232  0.05594373  0.02615744 -0.00020107]'
 b'[-0.09023542 -0.01520202  0.02594413  0.0152848   0.02993004]'
 b'[ 0.02597977  0.01155447 -0.03341909  0.01283126 -0.00788336]'
 b'[-0.0311645  -0.02193766  0.01195193 -0.00156491  0.02515437]'
 b'[-0.02063796 -0.02474995 -0.00826916 -0.07110358 -0.01128928]'
 b'[-0.02542101  0.01105374  0.06318881 -0.06937706 -0.01915297]'
 b'[-0.09416252  0.00973927  0.03317858  0.06058807 -0.04148066]'
 b'[-0.01365746  0.03451785  0.06412476  0.02822566 -0.00999724]'
 b'[-0.0171374   0.0715599   0.05882246  0.01296736 -0.00091868]'
 b'[ 0.00693085 -0.04560864  0.02032861 -0.04077329 -0.00946539]'
 b'[-0.06874597  0.04256656 -0.02906736  0.04013787  0.00206867]'
 b'[-0.01034078 -0.05094369  0.08312039 -0.00587852  0.01176884]'
 b'[-0.06719685  0.05213002  0.03297217 -0.01096502 -0.0350911 ]'
 b'[ 0.05109432  0.02179589  0.05981123  0.00137072 -0.03289427]'
 b'[ 0.00146687  0.06880862  0.05646181  0.02786043 -0.03881097]'
 b'[-0.02244576  0.0231379   0.06871274 -0.00127656 -0.0272399 ]'
 b'[-0.04374747 -0.06226141  0.13758671 -0.00678062 -0.03487266]'
 b'[-0.00418507  0.03274211  0.0689674  -0.05106321 -0.03625463]'
 b'[-0.03213548  0.00827215  0.00770648 -0.03131473 -0.01863099]'
 b'[ 0.06207936  0.00689174  0.10711489 -0.03350456  0.01081521]'
 b'[ 0.02757066 -0.0253685   0.12062814 -0.00970585  0.02902891]'
 b'[-0.02885368 -0.0160449   0.14081968  0.02503997 -0.00294884]'
 b'[ 0.07023901  0.05541587  0.02016265 -0.03657229  0.04771656]'
 b'[-0.0209682  -0.0051748   0.09555192  0.02832836 -0.08762222]'
 b'[-0.05938871  0.07186659  0.12110886 -0.02638434  0.00429018]'
 b'[-0.11481152 -0.04604804  0.11981387 -0.00891783 -0.02516906]'
 b'[-0.03681243  0.03241169  0.07067283  0.03979958  0.03641473]'
 b'[ 0.08654588 -0.01150938 -0.0563741  -0.02497471  0.04594599]'
 b'[-0.03297099  0.01769503  0.11319669  0.01238059  0.0082001 ]'
 b'[-0.00865924 -0.01036084  0.01999265 -0.02626343 -0.01680002]'
 b'[-0.05190443  0.03164011  0.02040888  0.00856969 -0.0272375 ]'
 b'[-0.05195722  0.05240251  0.1554347   0.03092176  0.03021973]'
 b'[ 0.02322851 -0.04756063  0.02221033 -0.01971874  0.02247192]'
 b'[-0.04352621  0.04046606  0.0664994  -0.04015313 -0.0013613 ]'
 b'[-0.03193593 -0.04119597  0.08940079 -0.08665874 -0.01585988]'
 b'[-0.03285274 -0.09046161  0.02063147 -0.01080728  0.01723639]'
 b'[-0.04092724 -0.00503651  0.04953369  0.02530581 -0.02194984]'
 b'[-0.03308328  0.0108201   0.08706747  0.01400463  0.00957526]'
 b'[ 0.01701859  0.00838621  0.08922331 -0.00337605  0.02646657]'
 b'[-0.06959544  0.04555301  0.02370284  0.01276516  0.03738073]'
 b'[-0.01415364  0.02345778  0.13541281  0.00027695  0.03363715]'
 b'[-0.06227937  0.01147403  0.04096422  0.06501009 -0.05931246]'
 b'[ 0.04575171  0.02199615  0.09396678 -0.03892348 -0.00266673]'
 b'[-0.00880066  0.07396847  0.05845954  0.0611381  -0.02724584]'
 b'[-0.09053511 -0.07695854  0.08236924 -0.0004557   0.03071151]'
 b'[-0.06687004 -0.00884794  0.06785945 -0.01278874  0.00910276]'
 b'[-0.06192718  0.04024284  0.04660054  0.03686318 -0.05565461]'
 b'[ 0.00812656 -0.03686302  0.09414893  0.02846115 -0.0320286 ]'
 b'[ 0.00260613  0.02772786  0.02776031 -0.05296541 -0.06279145]'
 b'[ 0.06081159  0.04025939  0.01868677 -0.01511183 -0.06156038]'
 b'[ 0.06705007  0.02358671  0.05917825 -0.04668655 -0.01996517]'
 b'[-0.02651405 -0.03667055  0.1276829   0.02881959 -0.05748772]'
 b'[-0.04663552  0.02453288  0.16361675 -0.04503557 -0.04188378]'
 b'[-0.0375299   0.0024564   0.08636113 -0.00160064 -0.00404025]'
 b'[ 0.10195461 -0.09637161  0.08657964  0.01170785 -0.00702063]'
 b'[-0.00990426  0.01018562  0.051896   -0.02190032  0.01513071]'
 b'[-0.01914701  0.08492666  0.0164949   0.04085653  0.01324011]'
 b'[ 0.05544649  0.00318151  0.05673654 -0.00993952 -0.02152128]'
 b'[-0.0254409   0.01935307  0.01199189 -0.00708791  0.00387142]'
 b'[-0.0282771   0.04030829  0.05126983 -0.04400632 -0.0082527 ]'
 b'[0.01513346 0.01526676 0.03014831 0.02528955 0.05057558]'
 b'[ 0.00170263 -0.05140645  0.01383791  0.1049732  -0.0187094 ]'
 b'[-0.05105732 -0.00549103 -0.00211255  0.00382627  0.03240367]'
 b'[ 0.08392965  0.05782474  0.04813674 -0.012268   -0.01868831]'
 b'[ 0.02100096  0.02253112 -0.01667141 -0.01646966  0.02927297]'
 b'[ 0.02955185 -0.0215682   0.06128283 -0.07923301 -0.03955455]'
 b'[-0.00260352  0.05469373  0.03105753  0.01570494  0.04612614]'
 b'[-0.04520263 -0.01102473  0.04200301  0.00765652  0.02229562]'
 b'[-0.03557624  0.02408457  0.04376483  0.01995172  0.03095791]'
 b'[-0.02472683  0.05018009  0.05735757 -0.03225219 -0.02325426]'
 b'[ 0.01198511  0.07219561 -0.04000699  0.00166795  0.02750094]'
 b'[ 0.03821035 -0.05754133  0.05844725 -0.01565812 -0.02900359]'
 b'[ 0.00653413  0.05587009  0.01621004 -0.01408313  0.0698965 ]'
 b'[ 0.00032702  0.04052418  0.05641509 -0.03344009 -0.03914972]'
 b'[-0.04089393 -0.00665233  0.12799472 -0.02873646 -0.00894405]'
 b'[-0.01444164 -0.03190748  0.00951358 -0.00564088 -0.03646583]'
 b'[ 0.00547115  0.05014327 -0.01592914 -0.00122629  0.04477547]'
 b'[-0.01207544  0.00356778  0.00447336 -0.0287638  -0.03753311]'
 b'[-0.05884602  0.01072115  0.02425918  0.02255262 -0.01806048]'
 b'[-0.01731539 -0.02231116  0.09437364  0.00170555 -0.05407009]'
 b'[ 0.06500567  0.01848128 -0.0014751  -0.0306924  -0.02501157]'
 b'[-0.05805401 -0.03961165  0.02174086 -0.04895382  0.08139951]'
 b'[ 0.00627512 -0.10157526  0.02130324 -0.0502402   0.01576446]'
 b'[-0.00976898 -0.05913119  0.04541657  0.05631701  0.05014214]'
 b'[ 0.012176    0.01870439 -0.09938233 -0.02261297  0.00181638]'
 b'[ 0.00652808 -0.05605447 -0.07694134 -0.0147412  -0.00597148]'
 b'[ 0.11674304  0.01894933  0.04533885 -0.06286294 -0.00410459]'
 b'[ 0.01085297 -0.01358635  0.0152023   0.02346188 -0.02940939]'
 b'[-0.07113044 -0.0611495   0.08770485 -0.06983516 -0.05291767]'
 b'[ 0.05899842 -0.01446779  0.07837778 -0.03993104  0.00525863]'
 b'[-0.10755081  0.0300941   0.07934867 -0.02394172 -0.02204445]'
 b'[-0.0311101   0.04656944  0.03284094 -0.03410514 -0.05289885]'
 b'[ 0.08936132  0.01549069 -0.08579474 -0.0308104   0.03784556]'
 b'[ 0.00364001 -0.01364339  0.01154843 -0.00796111 -0.04738059]'
 b'[-0.03339433 -0.00842785  0.04478682 -0.01196976 -0.04958511]'
 b'[-0.03358311  0.04321984  0.05168601 -0.01089774 -0.02395501]'
 b'[-0.06558623  0.04269782  0.0612884  -0.05247656 -0.00742848]'
 b'[-0.02506015  0.03288777 -0.02826415 -0.03411807  0.02337966]'
 b'[-0.0063935  -0.06393326 -0.00221541 -0.00846948 -0.00674942]'
 b'[-0.01611812 -0.04033646  0.07721135 -0.03097609 -0.0332734 ]'
 b'[ 0.06345142 -0.02645344  0.0762206   0.0113527   0.02931798]'
 b'[0.02701272 0.03514359 0.12304931 0.00486567 0.05368165]'
 b'[-0.01498978  0.00275506  0.09848753 -0.05815676 -0.02021742]'
 b'[-0.07171111 -0.09847055  0.10210885 -0.01390746 -0.01117994]'
 b'[ 0.05652616  0.020263    0.08677369  0.04779573 -0.02989006]'
 b'[-0.00321592  0.08285369  0.07023705  0.01291879  0.02472301]'
 b'[ 0.07298638 -0.01532689  0.02540569 -0.02986414  0.01640994]'
 b'[-0.03712279  0.01163841  0.13998468 -0.00304863  0.04347893]'
 b'[-0.03525798  0.01185983  0.13191286 -0.01959012  0.03257367]'
 b'[-0.04548401 -0.02758195  0.00168344 -0.03186819  0.00271737]'
 b'[-0.03803202 -0.00824933  0.06340004 -0.01111551  0.01700836]'
 b'[-0.06189108 -0.03465553  0.07799784  0.01622717 -0.03012843]'
 b'[ 0.01731179 -0.00914969  0.00600643 -0.04678116  0.02879524]'
 b'[-0.01725049  0.02091795  0.11079015  0.04469016  0.03707935]'
 b'[-0.04465106  0.02234716  0.06541137  0.01215733 -0.00971021]'
 b'[ 0.01089822 -0.01503999 -0.01136935  0.05091227 -0.01141826]'
 b'[-0.01596685 -0.01873063  0.05162324  0.04050293 -0.0415377 ]'
 b'[ 0.03071246 -0.01387434  0.17381501  0.02932012  0.0157283 ]'
 b'[-0.07127521  0.03927163  0.08532428 -0.04077393 -0.05403168]'
 b'[-0.06733387  0.01606675 -0.04003966  0.05473982 -0.05073515]'
 b'[-0.02546638  0.02652799  0.08205058 -0.01278413 -0.00037502]'
 b'[-0.00655765 -0.0049662   0.02634083  0.04414084 -0.00299913]'], shape=(128,), dtype=string)
 
a embedding columns of Pandas dataframe,  i will do
 ds = tf.data.Dataset.from_tensor_slices((dict(df), label))
 
 [(train_features, label_batch)] = ds.take(1)
            print('Every feature:', list(train_features.keys()))
            print('A batch of feed_embedding:', train_features['feed_embedding'])
            
how string tensor to numpy array float32?
            "
50357,HOW string Tensor  to Numpy array,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tens
[aaa.txt](https://github.com/tensorflow/tensorflow/files/6679901/aaa.txt)
orflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using):
- Are you willing to contribute it (Yes/No):



**Describe the feature and the current behavior/state.**

**Will this change the current api? How?**

**Who will benefit with this feature?**

**Any Other info.**
"
50356,FAILED: Build did NOT complete successfully,"
I'm using a tested environment in Win10
<!--StartFragment-->
tensorflow-2.2.0 | 3.5-3.8 | MSVC 2019 | Bazel 2.0.0
-- | -- | -- | --
error occurred
...\_bazel_dswxxxxx\evzptwex\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1028): error C2061: 语
法错误: 标识符“Kind”
.../_bazel_dswxxxxx\evzptwex\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1134): note: 请参阅对
要编译的类 模板 的实例化“Eigen::internal::StridedLinearBufferCopy<Scalar,IndexType>”的引用
...\_bazel_dswxxxxx\evzptwex\execroot\org_tensorflow\external\eigen_archive\unsupported\eigen\cxx11\src/Tensor/TensorBlock.h(1037): error C2061: 语
法错误: 标识符“Kind”

why?


<!--EndFragment-->

"
50354,Build from source (tensorflow),"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: No mobile 
- TensorFlow installed from (source or binary): Source
- TensorFlow version: master 
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: no environment 
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:CUDA 11.0, cuDNN 8
- GPU model and memory: RTX 2070 - (FYI, firstly I am trying to install on cpu).



**Describe the problem**
**Provide the exact sequence of commands/steps that you executed before running into the problem**

I am trying to install tensorflow from the source.  

https://www.tensorflow.org/install/source_windows

1. Install Bazel and add it to the path 
2. Install MSYS2 and add it to the path and did 
```
pacman -S git patch unzip
```

![Screenshot 2021-06-19 031324](https://user-images.githubusercontent.com/17668390/122616532-5d1bfe00-d0ac-11eb-9145-de01f950163a.png)


## Install Visual C++ Build Tools 2019
Next, as the document says, we need to install 

- Microsoft Visual C++ 2019 Redistributable
- Microsoft Build Tools 2019

From [here](https://visualstudio.microsoft.com/downloads/), I downloaded them and installed and restart the pc anyway. 


![Screenshot 2021-06-19 031550](https://user-images.githubusercontent.com/17668390/122616761-e29fae00-d0ac-11eb-8f71-6090a24ad2ca.png)
![Screenshot 2021-06-19 031633](https://user-images.githubusercontent.com/17668390/122616768-e4697180-d0ac-11eb-8baa-368f7964d4de.png)

## TensorFlow

```
git clone https://github.com/tensorflow/tensorflow.git

# download as zip (as a name tensorflow-master.zip
# unizip it and rename only tensorflow and then from CMD

cd tensorflow
```

## Configure 

Run the following command 

```
python ./configure.py
```

and get 

```
D:\tf_source\tensorflow>python ./configure.py
You have bazel 3.7.2 installed.
Please specify the location of python. [Default is C:\Users\innat\anaconda3\python.exe]:


Found possible Python library paths:
  C:\Users\innat\anaconda3\lib\site-packages
Please input the desired Python library path to use.  Default is [C:\Users\innat\anaconda3\lib\site-packages]

Do you wish to build TensorFlow with ROCm support? [y/N]:
No ROCm support will be enabled for TensorFlow.

Do you wish to build TensorFlow with CUDA support? [y/N]:
No CUDA support will be enabled for TensorFlow.

Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is /arch:AVX]:


Would you like to override eigen strong inline for some C++ compilation to reduce the compilation time? [Y/n]:
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]:
Not configuring the WORKSPACE for Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.
        --config=mkl            # Build with MKL support.
        --config=mkl_aarch64    # Build with oneDNN and Compute Library for the Arm Architecture (ACL).
        --config=monolithic     # Config for mostly static monolithic build.
        --config=numa           # Build with NUMA support.
        --config=dynamic_kernels        # (Experimental) Build kernels into separate shared objects.
        --config=v2             # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
        --config=nogcp          # Disable GCP support.
        --config=nonccl         # Disable NVIDIA NCCL support.
````

Next, run the following command 

```
bazel build --config=opt //tensorflow/tools/pip_package:build_pip_package
```
It gave error as follows 

```
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=136
INFO: Reading rc options for 'build' from d:\tf_source\tensorflow\.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Options provided by the client:
  'build' options: --python_path=C:/Users/innat/anaconda3/python.exe
INFO: Reading rc options for 'build' from d:\tf_source\tensorflow\.bazelrc:
  'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true
INFO: Reading rc options for 'build' from d:\tf_source\tensorflow\.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=C:/Users/innat/anaconda3/python.exe --action_env PYTHON_LIB_PATH=C:/Users/innat/anaconda3/lib/site-packages --python_path=C:/Users/innat/anaconda3/python.exe --copt=/d2ReducedOptimizeHugeFunctions --host_copt=/d2ReducedOptimizeHugeFunctions --define=override_eigen_strong_inline=true
INFO: Found applicable config definition build:short_logs in file d:\tf_source\tensorflow\.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file d:\tf_source\tensorflow\.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:opt in file d:\tf_source\tensorflow\.tf_configure.bazelrc: --copt=/arch:AVX --host_copt=/arch:AVX
INFO: Found applicable config definition build:windows in file d:\tf_source\tensorflow\.bazelrc: --copt=/W0 --copt=/D_USE_MATH_DEFINES --host_copt=/D_USE_MATH_DEFINES --cxxopt=/std:c++14 --host_cxxopt=/std:c++14 --config=monolithic --copt=-DWIN32_LEAN_AND_MEAN --host_copt=-DWIN32_LEAN_AND_MEAN --copt=-DNOGDI --host_copt=-DNOGDI --copt=/experimental:preprocessor --host_copt=/experimental:preprocessor --linkopt=/DEBUG --host_linkopt=/DEBUG --linkopt=/OPT:REF --host_linkopt=/OPT:REF --linkopt=/OPT:ICF --host_linkopt=/OPT:ICF --verbose_failures --distinct_host_configuration=false
INFO: Found applicable config definition build:monolithic in file d:\tf_source\tensorflow\.bazelrc: --define framework_shared_object=false
DEBUG: C:/users/innat/_bazel_innat/xafk73x4/external/tf_runtime/third_party/cuda/dependencies.bzl:51:10: The following command will download NVIDIA proprietary software. By using the software you agree to comply with the terms of the license agreement that accompanies the software. If you do not agree to the terms of the license agreement, do not use the software.
INFO: Build options --action_env, --copt, --define, and 1 more have changed, discarding analysis cache.
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (0 packages loaded, 28806 targets configured).
INFO: Found 1 target...
ERROR: D:/tf_source/tensorflow/tensorflow/compiler/mlir/lite/BUILD:170:18: TdGenerate tensorflow/compiler/mlir/lite/transforms/generated_optimize.inc failed (Exit -1073741515): mlir-tblgen.exe failed: error executing command
  cd C:/users/innat/_bazel_innat/xafk73x4/execroot/org_tensorflow
bazel-out/x64_windows-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen.exe -gen-rewriters tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td -I ./ -I bazel-out/x64_windows-opt/bin/./ -I external/llvm-project/mlir/include -I bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include -I tensorflow/compiler/mlir/lite/transforms -I bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/lite/transforms -o bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/lite/transforms/generated_optimize.inc
Execution platform: @local_execution_config_platform//:platform
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 877.798s, Critical Path: 174.93s
INFO: 1343 processes: 10 internal, 1333 local.
FAILED: Build did NOT complete successfully
````

Precisely the following issue 

```
INFO: Found 1 target...
ERROR: D:/tf_source/tensorflow/tensorflow/compiler/mlir/lite/BUILD:170:18: TdGenerate tensorflow/compiler/mlir/lite/transforms/generated_optimize.inc failed (Exit -1073741515): mlir-tblgen.exe failed: error executing command
  cd C:/users/innat/_bazel_innat/xafk73x4/execroot/org_tensorflow
bazel-out/x64_windows-opt-exec-50AE0418/bin/external/llvm-project/mlir/mlir-tblgen.exe -gen-rewriters tensorflow/compiler/mlir/lite/transforms/optimize_patterns.td -I ./ -I bazel-out/x64_windows-opt/bin/./ -I external/llvm-project/mlir/include -I bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include -I tensorflow/compiler/mlir/lite/transforms -I bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/lite/transforms -o bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/lite/transforms/generated_optimize.inc
Execution platform: @local_execution_config_platform//:platform
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 877.798s, Critical Path: 174.93s
INFO: 1343 processes: 10 internal, 1333 local.
FAILED: Build did NOT complete successfully
````

**Any other info/logs**
- I also try the `tensorflow 2.4` branch, it shows a different error. (CPU, GPU both)
- If the master branch is `tf 2.5`, then my CUDA/cuDNN is probably not compatible in case I want to compile with GPU. I've CUDA 8.0 and cuDNN 11.0 for tensorflow 2.4, and tensorflow 2.5 requires CUDA 8.1 and cuDNN 11.2.
- But I want to compile it for CPU from the master branch as I already have tensorflow 2.4 as a separate environment. 


The fact is, I need to compile tensorflow (from the master branch) on CPU only. And I've Cuda/cudnn version for tensorflow 2.4. Hope that would not conflict with the master branch of tensorflow. 







"
50353,Cannot convert a symbolic Tensor ({}) to a numpy array,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Big Sur - 11.4
- binary 
- tensorflow_macos
- https://github.com/apple/tensorflow_macos
- TensorFlow version (use command below):
- 2.4.0-rc0
- Python version:
- 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:
AMD Radeon Pro 5700 XT
You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)

python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
tf_macos-v0.1-alpha2-AS-67-gf3595294ab 2.4.0-rc0

**Describe the current behavior**
 raise NotImplementedError(
        ""Cannot convert a symbolic Tensor ({}) to a numpy array.""
        "" This error may indicate that you're trying to pass a Tensor to""
        "" a NumPy call, which is not supported"".format(self.name))

The code is in the GitHub repository Top2vec:

https://github.com/ddangelov/Top2Vec

<img width=""1402"" alt=""Screen Shot 2021-06-18 at 11 50 52 AM"" src=""https://user-images.githubusercontent.com/3105499/122614172-fca0a880-d03a-11eb-859f-e63156d01a7a.png"">

@staticmethod
    def _l2_normalize(vectors):

        if vectors.ndim == 2:
            return normalize(vectors)
        else:
            return normalize(vectors.reshape(1, -1))[0]

 self.word_vectors = self._l2_normalize(np.array(self.embed(self.vocab)))


**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50351,Tensorflow master build fails on s390x arch due to boringssl issues,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50349,Conv2D fails with sparse input,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Centos7
- TensorFlow installed from (source or binary):  Conda install
- TensorFlow version (use command below):  2.5.0-rc3
- Python version: 3.7.5

I'm not certain if this is a bug or missing feature?  Trying to run sparse data through convolutional layer, but error immediately results:

```
x = keras.Input(shape=(100,100,2), sparse=True)                        
C = keras.layers.Conv2D(16,(3,3))(x)  
```
Error as follows:

`TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""Placeholder_1:0"", shape=(None, 4), dtype=int64), values=Tensor(""Placeholder:0"", shape=(None,), dtype=float32), dense_shape=Tensor(""PlaceholderWithDefault:0"", shape=(4,), dtype=int64)). Consider casting elements to a supported type.`




"
50348,Bug in model.fit,"When calling model.fit in tensorflow version 2.5.0 the validation_data parameter doesn't work if the model takes 2 or more inputs. Same code in tensorflow 2.3.0 works perfectly fine. 


`history = model.fit(
	[X_train_pairs[:, 0], X_train_pairs[:, 1]], np.array(y_train),
  validation_data = ([X_val_pairs[:, 0], X_val_pairs[:, 1]], np.array(y_val)),
	batch_size=256,
	epochs=25)`

This is my code. "
50347,Build TFLite Micro for riscv32_mcu,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux  Ubuntu 20.04
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: Tensorflow 2.4.2
- Python version: 3.8.5
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): gcc 9.3.0, riscv64-unknown-elf-gcc (GCC) 10.2.0
- CUDA/cuDNN version: N/A 
- GPU model and memory: N/A
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): RISC-V


**Describe the problem**
Cannot build TFLite Micro for RISC-V target (TARGET=riscv32_mcu). The error message is shown at the bottom.
The command below doesn't work since the file _riscv32_mcu_makefile.inc_ doesn't exist.
`make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world`

Does anyone successfully build the hello_world example for RISCV (riscv32_mcu)? 
What are the exact commands and steps?



**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. `git clone --depth 1 https://github.com/tensorflow/tensorflow.git`
2. `cd tensorflow`
3. `vi tensorflow/lite/micro/tools/make/targets/mcu_riscv_makefile.inc`
4. change `ifeq ($(TARGET), riscv32_mcu)` to `ifeq ($(TARGET), mcu_riscv)`  
5. Move two FLAGS (`-fno-threadsafe-statics` and `-fno-use-cxa-atexit`) from PLATFORM_FLAGS to CXXFLAGS
6. `make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world`




**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.



$ make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv TARGET_ARCH=riscv32_mcu hello_world
tensorflow/lite/micro/tools/make/downloads/flatbuffers already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/pigweed already exists, skipping the download.
tensorflow/lite/micro/tools/make/downloads/person_model_int8 already exists, skipping the download.
riscv64-unknown-elf-g++ -std=c++11 -fno-rtti -fno-exceptions -fno-threadsafe-statics -Werror -fno-unwind-tables -ffunction-sections -fdata-sections -fmessage-length=0 -DTF_LITE_STATIC_MEMORY -DTF_LITE_DISABLE_X86_NEON -O3 -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wunused-function -Wswitch -Wvla -Wall -Wextra -Wstrict-aliasing -Wno-unused-parameter -DMCU_RISCV -march=rv32imac -mabi=ilp32 -mcmodel=medany -mexplicit-relocs -fno-builtin-printf -fno-exceptions -DTF_LITE_MCU_DEBUG_LOG -DTF_LITE_USE_GLOBAL_CMATH_FUNCTIONS -fno-unwind-tables -ffunction-sections -fdata-sections -funsigned-char -Wvla -Wall -Wextra -Wsign-compare -Wdouble-promotion -Wshadow -Wunused-variable -Wmissing-field-initializers -Wno-unused-parameter -Wno-write-strings -Wunused-function -fno-delete-null-pointer-checks -fomit-frame-pointer -Os -fpermissive -fno-rtti -fno-threadsafe-statics -fno-use-cxa-atexit --std=gnu++11 -I. -Itensorflow/lite/micro/tools/make/downloads/gemmlowp -Itensorflow/lite/micro/tools/make/downloads/flatbuffers/include -Itensorflow/lite/micro/tools/make/downloads/ruy -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/include -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/drivers/ -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env -Itensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1 -Itensorflow/lite/micro/tools/make/downloads/kissfft -o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/bin/hello_world tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/main.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/main_functions.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/model.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/output_handler.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/obj/tensorflow/lite/micro/examples/hello_world/constants.o tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/lib/libtensorflow-microlite.a -Wl,--fatal-warnings -Wl,--gc-sections -Ttensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/flash.lds -nostartfiles -Ltensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env --specs=nano.specs -lm
/opt/riscv/lib/gcc/riscv64-unknown-elf/10.2.0/../../../../riscv64-unknown-elf/bin/ld: warning: cannot find entry symbol _start; not setting start address
collect2: error: ld returned 1 exit status
make: *** [tensorflow/lite/micro/examples/hello_world/Makefile.inc:44: tensorflow/lite/micro/tools/make/gen/mcu_riscv_riscv32_mcu_micro/bin/hello_world] Error 1


"
50346,What direction is tensorflow moving to ,"I decided to formulate this as an issue as did not find any more appropriate place to write it.

Currently, there is a huge work done in between  tensorflow1 to tensorflow2. Tf 1 was heavily dependent on graphs, while tf2 has moved to functions. Meanwhile, there is a  `tf.compat` which tries to establish compatibility.  All this stuff rises some complexity. e.g,  someone may accounter to :  I want to eliminate variables so that my model run faster ( not with tf serving) ... ah... ok, I need to use tf.compat.v1 then transform it to graph... then you encounter with different methods of loading a model, with some functions in tf , some functions in tf.compat.v1 etc. All this stuff become not intuitive . I understand that all this stuff is transitional and will settle down at some point.  What would be nice to know in advance is where is it moving, so what is the destination. Is it a process of getting rid of graphs and becoming something similar to pytorch API ?Or,  will it settle down and become some nice combination of graphs+ functions so that the api is clear and comprehensible? What a tensorflow user should expect , what should they try to understand and what will be gone ? What will be the model of tensorflow in the future ?

It would be nice if you could explain your vision related to tensorflow  either in this issue or some different material. "
50345,ROCM: Segmentation fault late in build process,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Manjaro, completely updated
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): compiling from source
- TensorFlow version: 2.5.0
- Python version: 3.9.5
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source): 4.0.0
- GCC/Compiler version (if compiling from source):10.2.0-3
- CUDA/cuDNN version: cuda: 11.3.0-2,m, cudnn: 8.2.0.53-1
- GPU model and memory: AMD rx570 4GB



**Describe the problem**

Problem is the build fails

**Provide the exact sequence of commands / steps that you executed before running into the problem**

Building this aur package https://github.com/rocm-arch/tensorflow-rocm

After many hours of compilation I get a segmentation fault.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

```
    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel_generator_kernel.o [for host]; 2s local
    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f64_i1_kernel_generator_kernel.o [fERROR: /tmp/trizen-mario/tensorflow-rocm/src/tensorflow-2.5.0-rocm/tensorflow/core/kernels/mlir_generated/BUILD:957:23: compile tensorflow/core/kernels/mlir_generated/is_finite_gpu_f16_i1_kernel_generator_kernel.o [for host] failed: (Segmentation fault): tf_to_kernel failed: error executing command bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes=256' '--arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908' ... (remaining 4 argument(s) skipped)
[20,437 / 21,317] 11 actions running
    compile tensorflow/core/kernels/mlir_generated/is_finite_gpu_f64_i1_kernel_generator_kernel.o [for host]; 3s local
    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel_generator_kernel.o [for host]; 2s local
    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f64_i1_kernel_generator_kernel.o [for host]; 2s local
    compile tensorflow/core/kernels/mlir_generated/is_nan_gpu_f16_i1_kernel_generator_kernel.o [f2021-06-18 15:06:05.299828: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:210] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
0x556475d2e878: i1 = FP_CLASS 0x5564779f2e08, Constant:i32<504>TensorFlow crashed, please file a bug on https://github.com/tensorflow/tensorflow/issues with the trace below.
Stack dump:
0.	Program arguments: bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel --unroll_factors=4 --tile_sizes=256 --arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908 --input=bazel-out/host/bin/tensorflow/core/kernels/mlir_generated/is_finite_gpu_f16_i1.mlir --output=bazel-out/host/bin/tensorflow/core/kernels/mlir_generated/is_finite_gpu_f16_i1_kernel_generator_kernel.o --enable_ftz=False --cpu_codegen=False
1.	2.	Running pass 'CallGraph Pass Manager' on module 'acme'.
3.	Running pass 'AMDGPU DAG->DAG Pattern Instruction Selection' on function '@IsFinite_GPU_DT_HALF_DT_BOOL_kernel'
Stack dump without symbol names (ensure you have llvm-symbolizer in your PATH or set the environment var `LLVM_SYMBOLIZER_PATH` to point to it):
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x408d943)[0x556473344943]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x408bb0d)[0x556473342b0d]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x408bc94)[0x556473342c94]
/usr/lib/libpthread.so.0(+0x13870)[0x7f9763e9a870]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b070e8)[0x556471dbe0e8]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x18acc23)[0x556470b63c23]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a9adf2)[0x556471d51df2]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b6f3b6)[0x556471e263b6]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2bb88c6)[0x556471e6f8c6]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b6fa1e)[0x556471e26a1e]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2b6fb98)[0x556471e26b98]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a80a73)[0x556471d37a73]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a837e0)[0x556471d3a7e0]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2a856a6)[0x556471d3c6a6]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x2e0f83f)[0x5564720c683f]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3f03645)[0x5564731ba645]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3bc50e7)[0x556472e7c0e7]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3f030a1)[0x5564731ba0a1]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x169de1d)[0x556470954e1d]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x16a2bef)[0x556470959bef]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0xbe3199)[0x55646fe9a199]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361381d)[0x5564728ca81d]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361394a)[0x5564728ca94a]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361427b)[0x5564728cb27b]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3612b6f)[0x5564728c9b6f]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x36133ac)[0x5564728ca3ac]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x361394a)[0x5564728ca94a]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x3615c06)[0x5564728ccc06]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x7f78dd)[0x55646faae8dd]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x6b5eb8)[0x55646f96ceb8]
/usr/lib/libc.so.6(__libc_start_main+0xd5)[0x7f9763348b25]
bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel(+0x7f060e)[0x55646faa760e]
[20,437 / 21,317] 11 actions running
    compile tensorflow/core/kernels/mlir_generated/is_finite_gpu_f64_i1_kernel_generator_kernel.o [for host]; 3s local
    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f16_i1_kernel_generator_kernel.o [for host]; 2s local
    compile tensorflow/core/kernels/mlir_generated/is_inf_gpu_f64_i1_kernel_generator_kernel.o [for host]; 2s local
    compile tensorflow/core/kernels/mlir_generated/is_nan_gpu_f16_i1_kernel_generator_kernel.o [fERROR: /tmp/trizen-mario/tensorflow-rocm/src/tensorflow-2.5.0-rocm/tensorflow/tools/pip_package/BUILD:284:10 Middleman _middlemen/tensorflow_Stools_Spip_Upackage_Sbuild_Upip_Upackage-runfiles failed: (Segmentation fault): tf_to_kernel failed: error executing command bazel-out/host/bin/tensorflow/compiler/mlir/tools/kernel_gen/tf_to_kernel '--unroll_factors=4' '--tile_sizes=256' '--arch=gfx701,gfx702,gfx803,gfx900,gfx904,gfx906,gfx908' ... (remaining 4 argument(s) skipped)
INFO: Elapsed time: 11598.345s, Critical Path: 268.21s
INFO: 20448 processes: 1439 internal, 19009 local.
FAILED: Build did NOT complete successfully
==> ERROR: A failure occurred in build().
```"
50344,Request for support of ExtractImagePatches in Tensorflow Lite,"**System information**
-  Ubuntu 20.04:
- TensorFlow version 2.5:


**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime and are not recognized by TensorFlow. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DEPTHWISE_CONV_2D, ELU, L2_NORMALIZATION, LOGISTIC, MAX_POOL_2D, MUL, RESHAPE, RESIZE_BILINEAR, SOFTMAX, SUB, TRANSPOSE. Here is a list of operators for which you will need custom implementations: ExtractImagePatches.
```
"
50343,NotImplementedError: Cannot convert a symbolic Tensor to a numpy array,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): stock
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source and binary
- TensorFlow version (use command below): 2.5 and nightly
- Python version: 3.9
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc11 
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior**
`tf.reduce_mean` fails on a ragged tensor with `NotImplemeted` exception (see full error below). It happens in graph (non-eager) mode only. Furthermore, it *probably* does not happen with order versions of Numpy (1.19.*) but it definitely happens on Numpy 1.20.3

**Describe the expected behavior**
It must calculate means.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution(if contributing):
 * The current code does not catch this type of exception which is probably new in Numpy 1.20

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Following is the full error message.

```
NotImplementedError: in user code:

    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:869 train_function  *
        return step_function(self, iterator)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:859 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1286 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica
        return fn(*args, **kwargs)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:852 run_step  **
        outputs = model.train_step(data)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:813 train_step
        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:539 minimize
        grads_and_vars = self._compute_gradients(
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:591 _compute_gradients
        grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:473 _get_gradients
        grads = tape.gradient(loss, var_list, grad_loss)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:1084 gradient
        flat_grad = imperative_grad.imperative_grad(
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/eager/imperative_grad.py:71 imperative_grad
        return pywrap_tfe.TFE_Py_TapeGradient(
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/eager/backprop.py:159 _gradient_function
        return grad_fn(mock_op, *out_grads)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:522 _UnsortedSegmentSumGrad
        return _GatherDropNegatives(grad, op.inputs[1])[0], None, None
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/math_grad.py:488 _GatherDropNegatives
        array_ops.ones([array_ops.rank(gathered)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:3216 ones
        output = _constant_if_small(one, shape, dtype, name)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/ops/array_ops.py:2900 _constant_if_small
        if np.prod(shape) < 1000:
    <__array_function__ internals>:5 prod
        
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3030 prod
        return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/numpy/core/fromnumeric.py:87 _wrapreduction
        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
    /home/eli/virtenvs/tf_mkl/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:867 __array__
        raise NotImplementedError(

    NotImplementedError: Cannot convert a symbolic Tensor (gradient_tape/tree_model/inner_node/RaggedReduceMean/RaggedReduceSum/sub:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported
```"
50341,fatal error C1007,"fatal error C1007: 无法识别的标志“-ReducedOptimizeHugeFunctions”(在“p2”中)
Target //tensorflow/tools/pip_package:build_pip_package failed to build
INFO: Elapsed time: 16.981s, Critical Path: 1.50s
INFO: 8 processes: 8 internal.
FAILED: Build did NOT complete successfully
"
50340,Failed to build TensorFlow package builder,"Can anyone help me fix this error:
```
$ bazel build --config=opt --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" //tensorflow/tools/pip_package:build_pip_package
...
ERROR: /mnt/Archive/Downloads/tensorflow/tensorflow/tools/pip_package/BUILD:179:10: error loading package '@com_github_grpc_grpc//': at /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/com_github_grpc_grpc/bazel/grpc_build_system.bzl:28:6: at /home/notooth/.cache/bazel/_bazel_notooth/d5b3b610a5827abc5897d0ec1ef6cd79/external/build_bazel_rules_apple/apple/ios.bzl:37:5: initialization of module 'apple/internal/ios_rules.bzl' failed and referenced by '//tensorflow/tools/pip_package:licenses'
...
```"
50338,No registered 'swish_f32' OpKernel for GPU devices compatible with node {{node swish_3/swish_f32}},"When the model trained from tensorflow 1.4 infer images with tensorflow 2.3, the following message reported :

""2021-06-18 16:00:41.022400: W tensorflow/core/grappler/utils/graph_view.cc:836] No registered 'swish_f32' OpKernel for GPU devices compatible with node {{node swish_3/swish_f32}}.  Registered:  <no registered kernels>"".   

My codes  implemented with tf 1.x  has been upgraded to tf2.3.  So i need to check  the model trained previously  can run normally with tf2.3.  Session  is replaced as tf.compat.v1.Session(). And the model trained with tf2.3 without this message during inference. There is also no error report when training.  Whether the message can be dropped?

**System information**

- OS Platform: Windows 7
- TensorFlow version : 2.3
- Python version: 3.7.1
- cudatoolkit  version: 10.1
- GPU : RTX 2060


"
50336,tf.where function is less friendly in TensorFlow2.x than it in TenosrFlow1.x,"In TensorFlow1.x（including TensorFlow 1.15）, the function of tf.where function can broadcast automaticly just like this:

`x=tf.constant([[0.1, 0.1, 0.1], [0.1, 0.1, 0.1]], dtype=tf.float32)
y=tf.constant([[0.2, 0.2, 0.2], [0.2, 0.2, 0.2]], dtype=tf.float32)
m=tf.constant([0.4, 0.6],dtype=tf.float32)
z=tf.where(m>0.5, x, y)
`
It can get the right result z=[[0.2, 0.2, 0.2], [0.1, 0.1, 0.1]]

However, when it run in TensorFlow2.x， it will give the error like this:
**InvalidArgumentError: condition [2], then [2,3], and else [2,3] must be broadcastable [Op:SelectV2]**

So, why not continue the automatic broadcast feature in TensorFlow1.x？ "
50335,Error when loading tflite model created from saved_model,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 19041.1052
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

- I converted a saved model using `tf.lite.TFLiteConverter.from_saved_model()`

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model cannot be loaded by `tflite_runtime.interpreter.Interpreter`
- I get the following error: _ValueError: Op builtin_code out of range: 131. Are you using old TFLite binary with newer model?Registration failed._
- I have confirmed that both `tensorflow` and `tflite_runtime` and on version 2.5.0

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50334,Error build hello_world targeting RISCV ,"**System information**
- OS Platform and Distribution: Ubuntu 18.04
- TensorFlow installed from (source or binary): N/A
- TensorFlow version: v2.4.2
- Python version: N/A
- Installed using virtualenv? pip? conda?: N/A
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): GCC 7.5.0, riscv64-unknown-elf-g++ (GCC) 9.2.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the problem**
We are failed to build a runnable binary for RISCV as our targeting architecture following the steps described in this page.
https://www.tensorflow.org/lite/microcontrollers/library


**Provide the exact sequence of commands / steps that you executed before running into the problem**
1. clone the repository
2. make -f tensorflow/lite/micro/tools/make/Makefile hello_world_bin (this is okay)
3. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=mcu_riscv hello_world_bin (this builds, but it uses g++ instead of the riscv64-unknown-elf-g++)

4. So we copy mcu_riscv_makefile.inc to riscv32_mcu_makefile.inc
5. make -f tensorflow/lite/micro/tools/make/Makefile TARGET=riscv32_mcu hello_world_bin (then there are errors, asm related, the log is attached below)


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

cc1: warning: command line option '-fno-threadsafe-statics' is valid for C++/ObjC++ but not for C                                                               
cc1: warning: command line option '-fno-use-cxa-atexit' is valid for C++/ObjC++ but not for C                                                                           
In file included from tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:6:                                                  
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c: In function 'measure_cpu_freq':
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:3: error: 'asm' undeclared (first use in this function)                                
  170 |   asm volatile (""csrr %0, "" #reg : ""=r""(__tmp)); \                                                                                                      
      |   ^~~                                                                                                                                                          
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:152:32: note: in expansion of macro 'read_csr'               
  152 |   unsigned long start_mcycle = read_csr(mcycle);                                                                                                        
      |                                ^~~~~~~~                                                                                                                         
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:3: note: each undeclared identifier is reported only once for each function it appears in
  170 |   asm volatile (""csrr %0, "" #reg : ""=r""(__tmp)); \                                                                                    
      |   ^~~                                                                                                                                                                          
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:152:32: note: in expansion of macro 'read_csr'                                      
  152 |   unsigned long start_mcycle = read_csr(mcycle);                                                                                                                                
      |                                ^~~~~~~~                                                                                                                                     
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:7: error: expected ';' before 'volatile'                                                              
  170 |   asm volatile (""csrr %0, "" #reg : ""=r""(__tmp)); \                                                                                                                               
      |       ^~~~~~~~                                                                                                                                                                  
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:152:32: note: in expansion of macro 'read_csr'                                           
  152 |   unsigned long start_mcycle = read_csr(mcycle);                                                                                                                              
      |                                ^~~~~~~~                                                                                                                                          
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:7: error: expected ';' before 'volatile'                                                            
  170 |   asm volatile (""csrr %0, "" #reg : ""=r""(__tmp)); \                                                                                    
      |       ^~~~~~~~                                                         
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:158:32: note: in expansion of macro 'read_csr'
  158 |   unsigned long delta_mcycle = read_csr(mcycle) - start_mcycle;                                                    
      |                                ^~~~~~~~                            
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c: In function '_init':
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:225:25: warning: format '%d' expects argument of type 'int', but argument 2 has type 'long unsigned int' [-W
format=]                                                                       
  225 |   printf(""core freq at %d Hz\n"", get_cpu_freq());
      |                        ~^        ~~~~~~~~~~~~~~                                                                                                                                                     
      |                         |        |    
      |                         int      long unsigned int                                                                                                                          
      |                        %ld 
In file included from tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:6:
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:5: error: 'asm' undeclared (first use in this function)
  175 |     asm volatile (""csrw "" #reg "", %0"" :: ""i""(val)); \        
      |     ^~~                                                          
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:227:3: note: in expansion of macro 'write_csr'
  227 |   write_csr(mtvec, &trap_entry);                                                                                                                                                                    
      |   ^~~~~~~~~                              
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:9: error: expected ';' before 'volatile'
  175 |     asm volatile (""csrw "" #reg "", %0"" :: ""i""(val)); \
      |         ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:227:3: note: in expansion of macro 'write_csr'
  227 |   write_csr(mtvec, &trap_entry);
      |   ^~~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'
  177 |     asm volatile (""csrw "" #reg "", %0"" :: ""r""(val)); })
      |         ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:227:3: note: in expansion of macro 'write_csr'
  227 |   write_csr(mtvec, &trap_entry);
      |   ^~~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:170:7: error: expected ';' before 'volatile'
  170 |   asm volatile (""csrr %0, "" #reg : ""=r""(__tmp)); \
      |       ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:228:7: note: in expansion of macro 'read_csr'
  228 |   if (read_csr(misa) & (1 << ('F' - 'A'))) { // if F extension is present
      |       ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:9: error: expected ';' before 'volatile'
  175 |     asm volatile (""csrw "" #reg "", %0"" :: ""i""(val)); \
      |         ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:229:5: note: in expansion of macro 'write_csr'
  229 |     write_csr(mstatus, MSTATUS_FS); // allow FPU instructions without trapping
      |     ^~~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'
  177 |     asm volatile (""csrw "" #reg "", %0"" :: ""r""(val)); })
      |         ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:229:5: note: in expansion of macro 'write_csr'
  229 |     write_csr(mstatus, MSTATUS_FS); // allow FPU instructions without trapping                                                                                                 
      |     ^~~~~~~~~                                                                                                                                                               
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'                                                             
  177 |     asm volatile (""csrw "" #reg "", %0"" :: ""r""(val)); })                                                                                                                      
      |         ^~~~~~~~                                                                                                                                                                 
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:229:5: note: in expansion of macro 'write_csr'                                           
  229 |     write_csr(mstatus, MSTATUS_FS); // allow FPU instructions without trapping                                                                                                  
      |     ^~~~~~~~~                                                                                                                                                                    
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:175:9: error: expected ';' before 'volatile'                                                           
  175 |     asm volatile (""csrw "" #reg "", %0"" :: ""i""(val)); \
      |         ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:230:5: note: in expansion of macro 'write_csr'
  230 |     write_csr(fcsr, 0); // initialize rounding mode, undefined at reset
      |     ^~~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/encoding.h:177:9: error: expected ';' before 'volatile'
  177 |     asm volatile (""csrw "" #reg "", %0"" :: ""r""(val)); })
      |         ^~~~~~~~
tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.c:230:5: note: in expansion of macro 'write_csr'
  230 |     write_csr(fcsr, 0); // initialize rounding mode, undefined at reset
      |     ^~~~~~~~~
tensorflow/lite/micro/tools/make/Makefile:318: recipe for target 'tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freed
om-e300-hifive1/init.o' failed
make: *** [tensorflow/lite/micro/tools/make/gen/riscv32_mcu_riscv32_mcu/obj/tensorflow/lite/micro/tools/make/downloads/sifive_fe310_lib/bsp/env/freedom-e300-hifive1/init.o] Error 1
"
50332,Cannot import module for version mismatch?,"Always we are facing importing module problems. As for example, I am using TF 1.14 and keras 2.2.5. but it can not import this:
ImportError: cannot import name 'imagenet_utils' from 'tensorflow.keras.applications' (unknown location)
I don't know where it is, due to unknown location. Also I don't know the imagenet_utils is available in TF 1.14 or not!

Is there any documentation about it, especially for all version. Are there any differences or added method differences between TF 1.14 and TF 2.5? 
Would you provide any documentation about these issues? "
50330,TF Container does not contain TensorRT. Impossible to use TF-TRT,"CC; @bixia1 @sanjoy @pkanwar23 @whitefangbuck

We just noticed today that the tensorflow container seems to be incorrectly built:

```bash
docker run -it --rm \
>    --gpus=""all"" \
>    --shm-size=2g --ulimit memlock=-1 --ulimit stack=67108864 \
>    tensorflow/tensorflow:latest-gpu

root@9bc628c19dc5:/workspace# python
>>> from tensorflow.python.compiler.tensorrt import trt_convert as trt
>>> conversion_params = trt.TrtConversionParams()
>>> converter = trt.TrtGraphConverterV2(
...     conversion_params=conversion_params)

2021-06-17 20:18:13.799235: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvrtc.so.11.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64
2021-06-17 20:18:13.799292: F tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:49] getInferLibVersion symbol not found.
Aborted (core dumped)
```

As you can see TensorRT can not load and seems missing (maybe linked to TRT7 upgrade ?)

This is an issue because TF-TRT uses extentisely this container in our talks, tutorials and blog posts (NV and Google side)

Thanks for your help to address it"
50326,Getting nan-loss or CUDA_ERROR_ILLEGAL_ADDRESS after upgrading to 2.5.0 ,"**System information**
- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow version: v2.5.0-rc3-213-ga4dfb8d1a71 
- Python version: 3.8
- CUDA/cuDNN version: 11.1
- GPU model and memory: 4x GeFoce 1080 GTX
- NVIDIA driver version: 465.19.01

**Describe the current behavior**

After upgrading to 2.5.0 none of my experiments work anymore. 

To be precise, ~any training where I am using LSTM-layers~ (turns out the LSTM-layer itself cannot be the problem) one of my models always ends in either a `nan`-loss or a `CUDA_ERROR_ILLEGAL_ADDRESS`. This issue does not come right at the beginning. I am usually able to get a few hundred batches through before the problem occurs.

I don't know if these are two related or separate issues though. All I can say is that this problem did not exist before.

**Describe the expected behavior**

Work as previously in 2.4.1

**Other info / logs**

```
2021-06-18 11:33:42.734501: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2021-06-18 11:33:42.734547: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
Fatal Python error: Aborted

Thread 0x00007f14d7afd700 (most recent call first):
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py"", line 576 in _handle_results
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 892 in run
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 954 in _bootstrap_inner
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 912 in _bootstrap

Thread 0x00007f1780ff9700 (most recent call first):
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py"", line 528 in _handle_tasks
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 892 in run
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 954 in _bootstrap_inner
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 912 in _bootstrap

Thread 0x00007f17817fa700 (most recent call first):
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/selectors.py"", line 416 in select
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/connection.py"", line 936 in wait
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py"", line 499 in _wait_for_updates
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/multiprocessing/pool.py"", line 519 in _handle_workers
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 892 in run
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 954 in _bootstrap_inner
  File ""/home/sfalk/miniconda3/envs/asr2/lib/python3.9/threading.py"", line 912 in _bootstrap

Thread 0x00007f1781ffb700 (most recent call first):
  File ""/home/sfa2021-06-18 11:33:42.736286: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
lk/miniconda2021-06-18 11:33:42.736311: E tensorflow/stream_executor/cuda/cuda_driver.cc:1085] could not wait stream on event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
3/envs/asr2/lib/p2021-06-18 11:33:42.736331: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
ython3.9/2021-06-18 11:33:42.736358: F tensorflow/core/common_runtime/device/device_event_mgr.cc:221] Unexpected Event status: 1
multiprocessing/pool.py"", line 114 in workerAborted (core dumped)
```"
50325,Grappler ImplementationSelector fails to use CUDNN (for GRU & LSTM) when using tf.data.experimental.scan,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary (latest-gpu docker)
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0 (but this also reproduces in earlier versions).
- Python version: 3.6.9
- CUDA/cuDNN version: 11.2 / 8.1.0
- GPU model and memory: NVIDIA Quadro P1000

**Describe the current behavior**
As you can see in the code [here](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/layers/recurrent_v2.py#L832), the way CUDNN LSTM & GRU are implemented, is by adding two functions with the same name to the graph. Grappler's ImplementationSelector optimizer runs later and selects the correct implementation. However, when using `tf.data.experimental.scan`, from what I've previously seen (I don't quite remember well), the function gets renamed - which causes the implementation selector to fail.
What actually happens is the non-CUDNN version is the default implementation - so it falls back to that one.

**Describe the expected behavior**
The layers should run using CUDNN, when possible.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no): This might be a bit over my head, but if the solution is simple, I could implement it.
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
Unfortunately I couldn't think of a way to display this issue without Tensorboard, but here it is:
```
import tensorflow as tf

model = tf.keras.models.Sequential([
    tf.keras.layers.GRU(4),
])

@tf.function
def foo(dataset):
    a = 1.
    # Autograph converts this to tf.data.experimental.scan
    for i in dataset:
        a = model(i)
    return a # return the value to avoid pruning

inp_tensor = tf.random.normal([32, 10, 8])
dataset = tf.data.Dataset.from_tensor_slices([inp_tensor])

tf.profiler.experimental.start(""/tmp/tensorboard"")

# This successfully uses CUDNN
model.predict(inp_tensor)

# This doesn't use CUDNN
foo(dataset)

tf.profiler.experimental.stop()
```
Here you can see `model.predict` uses CUDNN:
![image](https://user-images.githubusercontent.com/51128928/122393399-8427e200-cf7d-11eb-98ef-28ec9cd75cf6.png)

And here you can see `foo(dataset)` does not use CUDNN:
![image](https://user-images.githubusercontent.com/51128928/122393625-bcc7bb80-cf7d-11eb-839d-2b9fc5437872.png)"
50324,`tf.keras.applications.EfficientNetB0` raises `CustomMaskWarning` when trained or saved,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8
- CUDA version: 
```
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_22:08:44_Pacific_Standard_Time_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
```
- cuDNN version: `cudnn-11.2-windows-x64-v8.1.1.33`
- GPU model and memory: RTX 2070 Super



**Describe the current behavior**
When saving an `EfficientNetB0` model to file ("".pb""-file) it shows a warning:
```
E:\projects\DaKanjiRecognizerML\.venv\lib\site-packages\tensorflow\python\keras\utils\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.
  warnings.warn('Custom mask layers require a config and must override '
```
This also happens when converting to tf lite.

**Describe the expected behavior**
Saving should work without a problem.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

eff_net = tf.keras.applications.EfficientNetB0(
    include_top=True,
    weights=None,
    input_shape=(64, 64, 1),
    classes=6543,
    pooling=""max"",
    classifier_activation=""softmax""
)

eff_net.save(PATH_TO_SAVE_TO)
```
"
50322,Keras models with shared layers are loaded incorrectly from h5 file,"**System information**
Default colab instance.

**Describe the current behavior**
Results of the different usages of a shared layer are permuted after the save/load procedure (h5 saved model format). This changes the network topology in an unpredictable manner.

**Describe the expected behavior**
Results are not permuted and the topology is intact.

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):  yes
- Briefly describe your candidate solution(if contributing):
The problem seems to lie in this part of model loading code (it has moved to a different file in tf 2.5, but is essentially the same)
https://github.com/tensorflow/tensorflow/blob/0931ea3d985bb9c8fdd054a5e29c4129623c849b/tensorflow/python/keras/engine/network.py#L1834-L1836
It handles the nodes we cannot compute right now. Such nodes are put back to the queue.
The problem arises when we process a shared layer (so there are several nodes associated with it) and some of the first nodes are not ready for computation, while some later nodes are. This leads to the order of nodes' computation being broken. This broken order is then used to find inputs for the following nodes, which leads to results permutation.

I propose two possible solutions:
1. Stop node processing process for entire layer upon encountering a node which is not ready for computation. This guaranties the correct order of computation, but would slow down the loading process to a certain degree.
2. Note the original node order while iterating over them and fill the node_index_map keys accordingly. This way the original order will be broken, but we would have information about it while searching for the following nodes' inputs.

**Standalone code to reproduce the issue**
https://colab.research.google.com/drive/1F8Rii8SmlURIlBK7GmMIkJwt5YkT6E7F?usp=sharing
"
50321,Convertion LSTM with Tensorflow Lite,"I trained a simple model with Keras :

```
model = tf.keras.models.Sequential([
                             tf.keras.layers.LSTM(20, time_major=False, unroll=False, input_shape=(28,28)), 
                             tf.keras.layers.Dense(10, activation=tf.nn.softmax, name='output')
])
```
Then, I converted my model with TfLite:

```
converter = tf.lite.TFLiteConverter.from_saved_model(""mnist_lstm_model"")
converter.experimental_new_converter = True
tflite_model = converter.convert()
```

I obtain a UNIDIRECTIONNAL_SEQUENCE_LSTM layer instead of LSTM. But I really need a LSTM layer for inference.

Thank you !
"
50320,Can i implement a model that learns weights that fit the quantization step size?,"I want to make model what can train quantization weights. Is there such a way?

I already know how to take out the weights of the trained model,
quantize them in an external tool, and then load them again.

Thanks for reading this question and have a nice day"
50319,Interpreter fails when trying to load CenterNet(Java API),"I have created an android example app and I am trying to run CenterNet(320x320, 512x512, 640x640).

When I run this model(trained): 
[centernet_320x320.zip](https://github.com/tensorflow/tensorflow/files/6668027/centernet_320x320.zip) in CPU works fine, but when I try to run with GPU Delegate I get the following error:
`java.lang.RuntimeException: java.lang.IllegalArgumentException: Internal error: Failed to apply delegate: Attempting to use a delegate that only supports static-sized tensors with a graph that has dynamic-sized tensors.`

When I run this two models(pre trained):
[centernet_model_test_640x640.zip](https://github.com/tensorflow/tensorflow/files/6668051/centernet_model_test_640x640.zip)
[centernet_model_test_512x512.zip](https://github.com/tensorflow/tensorflow/files/6668049/centernet_model_test_512x512.zip)
They work in CPU and GPU.

Now, when I try to run the same models in a Qualcomm device for better performance, for the first model I get the same behavior and for the two pre trained models when I run them in CPU they work(the output is good), but when I run the same model in QC **GPU the output of the models is not correct**. (its just weird when the same model in CPU gives good predictions and in GPU it gives really bad predictions)


```
    implementation 'org.tensorflow:tensorflow-lite:2.3.0'
    implementation 'org.tensorflow:tensorflow-lite-gpu:2.3.0'
```
I have tried with 2.3.0 , 2.4.0 and 2.5.0 and I get the same behavior.
"
50315,Unable to change task parameter in tensorflow decision forest (from default classification),"**System information**
- Have I written custom code: No
- OS: Linux Ubuntu 18.04
- TensorFlow installed from: pip
- TensorFlow version: 2.5.0
- Python version: 3.8.10
- Tensorflow decision forests version: 0.1.6

**Current Behavior**
Attempting to use the tensorflow decision forests module for regression.
Following this example:
https://www.tensorflow.org/decision_forests/migration#training_configuration

The problematic behavior is that the only way to change the task from the default (classification) to anything else is by specifying `task=tf.keras.Task.REGRESSION` (or `RANKING`) . But `module 'tensorflow.keras' has no attribute 'Task'` and so the model can only be used for classification as this is the default setting.

This behavior occurs both locally as well as on Google Colab notebooks.

**Expected Behavior**
The module `tf.keras` would contain the attribute `Task` and the `task` parameter would be adjustable.

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import tensorflow_decision_forests as tfdf
model = tfdf.keras.GradientBoostedTreesModel(task=tf.keras.Task.REGRESSION)
```
"
50308,deserialize pb stream to feature is too slow.,"We run tensorflow on A100 machine, and our model currently consumes TFRecord very fast, we find deserialize varint64 is hot spot, so I want to replace from varint64 to fixed64.
I find :
    1. TF don't support fixed64 in feature.proto.
    2. I have a test that fixed64 is faster(10x) than varint64.
Is there any way we can implement fixed64 without modifying the TF code?"
50307,TFLite Model Maker: Support custom input shape,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No): Not sure if I am up to the task

**Describe the feature and the current behavior/state.**
I want to use the TensorFlow Lite Model Maker to train a custom object detection model. I need a custom input shape, lets say 501x300. I tried changing the hparam `""image_size""` to `""420x420""`, but I get the following error with the given code:

<details>
  <summary>Error</summary>

  ```
  ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-5-4ed9b6a821a2> in <module>
----> 1 model = object_detector.create( train_data,
      2                                 validation_data=validation_data,
      3                                 model_spec=spec,
      4                                 epochs=1,
      5                                 batch_size=4,

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\object_detector.py in create(cls, train_data, model_spec, validation_data, epochs, batch_size, train_whole_model, do_train)
    285     if do_train:
    286       tf.compat.v1.logging.info('Retraining the models...')
--> 287       object_detector.train(train_data, validation_data, epochs, batch_size)
    288     else:
    289       object_detector.create_model()

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\object_detector.py in train(self, train_data, validation_data, epochs, batch_size)
    154       validation_ds, validation_steps, val_json_file = self._get_dataset_and_steps(
    155           validation_data, batch_size, is_training=False)
--> 156       return self.model_spec.train(self.model, train_ds, steps_per_epoch,
    157                                    validation_ds, validation_steps, epochs,
    158                                    batch_size, val_json_file)

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow_examples\lite\model_maker\core\task\model_spec\object_detector_spec.py in train(self, model, train_dataset, steps_per_epoch, val_dataset, validation_steps, epochs, batch_size, val_json_file)
    262             val_json_file=val_json_file,
    263             batch_size=batch_size))
--> 264     train.setup_model(model, config)
    265     train.init_experimental(config)
    266     model.fit(

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow_examples\lite\model_maker\third_party\efficientdet\keras\train.py in setup_model(model, config)
    111 def setup_model(model, config):
    112   """"""Build and compile model.""""""
--> 113   model.build((None, *config.image_size, 3))
    114   model.compile(
    115       steps_per_execution=config.steps_per_execution,

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\keras\engine\training.py in build(self, input_shape)
    417                            'method accepts an `inputs` argument.')
    418         try:
--> 419           self.call(x, **kwargs)
    420         except (errors.InvalidArgumentError, TypeError):
    421           raise ValueError('You cannot build your model by calling `build` '

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow_examples\lite\model_maker\third_party\efficientdet\keras\train_lib.py in call(self, inputs, training)
    883 
    884   def call(self, inputs, training):
--> 885     cls_outputs, box_outputs = self.base_model(inputs, training=training)
    886     for i in range(self.config.max_level - self.config.min_level + 1):
    887       cls_outputs[i] = self.classes(cls_outputs[i])

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\keras\engine\base_layer.py in __call__(self, *args, **kwargs)
   1010         with autocast_variable.enable_auto_cast_variables(
   1011             self._compute_dtype_object):
-> 1012           outputs = call_fn(inputs, *args, **kwargs)
   1013 
   1014         if self._activity_regularizer:

c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\autograph\impl\api.py in wrapper(*args, **kwargs)
    668       except Exception as e:  # pylint:disable=broad-except
    669         if hasattr(e, 'ag_error_metadata'):
--> 670           raise e.ag_error_metadata.to_exception(e)
    671         else:
    672           raise

ValueError: in user code:

    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow_hub\keras_layer.py:243 call  *
        result = smart_cond.smart_cond(training,
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\saved_model\load.py:668 _call_attribute  **
        return instance.__call__(*args, **kwargs)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py:828 __call__
        result = self._call(*args, **kwds)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py:871 _call
        self._initialize(args, kwds, add_initializers_to=initializers)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py:725 _initialize
        self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py:2969 _get_concrete_function_internal_garbage_collected
        graph_function, _ = self._maybe_define_function(args, kwargs)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py:3361 _maybe_define_function
        graph_function = self._create_graph_function(args, kwargs)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\function.py:3196 _create_graph_function
        func_graph_module.func_graph_from_py_func(
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\framework\func_graph.py:990 func_graph_from_py_func
        func_outputs = python_func(*func_args, **func_kwargs)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\eager\def_function.py:634 wrapped_fn
        out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    c:\users\felix\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\saved_model\function_deserialization.py:267 restored_function_body
        raise ValueError(

    ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (2 total):
        * Tensor(""inputs:0"", shape=(None, 420, 420, 3), dtype=float32)
        * False
      Keyword arguments: {}
    
    Expected these arguments to match one of the following 4 option(s):
    
    Option 1:
      Positional arguments (2 total):
        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='inputs')
        * True
      Keyword arguments: {}
    
    Option 2:
      Positional arguments (2 total):
        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='input_1')
        * False
      Keyword arguments: {}
    
    Option 3:
      Positional arguments (2 total):
        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='input_1')
        * True
      Keyword arguments: {}
    
    Option 4:
      Positional arguments (2 total):
        * TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='inputs')
        * False
      Keyword arguments: {}
  ```

</details>

```
spec = EfficientDetModelSpec(
    model_name=""efficientdet-lite0"",
    uri=""https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1"",
    hparams={
        ""image_size"": ""420x420""
    }
)
train_data, validation_data, test_data = object_detector.DataLoader.from_csv(""..."")
model = object_detector.create( train_data,
                                validation_data=validation_data,
                                model_spec=spec,
                                epochs=1,
                                batch_size=4,
                                train_whole_model=True)
```

According to the clear error message it seems that this is not supported yet or am I missing something?

**Will this change the current api? How?**
No.

**Who will benefit with this feature?**
Everyone who wants custom input shape for their object detection model. Which should be pretty common.
"
50306,layer order in functional API graph models,"Do the layers in a graph model that is defined with the functional API have a specific order in the model. 
For example, when I have an Add layer that adds the outputs of two other layers, how is it decided which of the two layers comes first in the model?
I am wondering since in most cases the order seems to be the same as the order the layers are given, but not always.
This can be confusing, for example, when I tried to get the weights of the model, the order of them was not as expected in rare cases.
The following code illustrates the issue. I expected to find the weights of the Conv2D before the DepthwiseConv2D when I perform  model.get_weights().
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
in0Con6326 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Dep66009 = tf.keras.layers.Input(shape=([2, 2, 2]))
in0Con7723 = tf.keras.layers.Input(shape=([1, 1, 1]))
Con6326 = keras.layers.Conv2D(3, (2, 2),strides=(1, 1), padding='valid' )(in0Con6326)
Dep66009 = keras.layers.DepthwiseConv2D((2, 1),strides=(2, 2), padding='valid',)(in0Dep66009)
Con7723 = keras.layers.Concatenate(axis=3, )([Dep66009,in0Con7723])
Add80107 = keras.layers.Add()([Con6326,Con7723])
model = tf.keras.models.Model(inputs=[in0Con6326,in0Dep66009,in0Con7723], outputs=Add80107)
print(model.get_weights())
print(model.summary())
```
"
50305,How to create Tensorflow data ingestion pipeline for multiple related CSVs?,"Let us say we have some Relational data. 
Making a simple example for a retail store chain:
* Dataset 1 --> Store_id, Daily_sales
* Dataset 2 --> Customer_id, store_id, Time in, Time out

Let us say the task is to predict ```Daily_sales```.


I know how how to create data batches for one single CSV. I can use ```tf.data.experimental.make_csv_dataset``` and iterate over the dataset iterable that it returns to read the batches lazily. 

However, I want to read in the batches from ```Dataset 1``` and ```Dataset 2``` described above where the common id is ```store_id``` such that the batch reads the rows with same ```store_id```s from both the datasets. I want to do this because I will run two networks (RNN on ```Dataset 2``` and a single Fully connected layer on ```Dataset 1```) on both datasets and then merge them in the final fully connected layer.

Can you please guide me on how to approach this problem in scenarios where:
 * The datasets can fit into the memory
 * The datasets can not fit into the memory

Here is a concrete example of the behaviour I am looking  for:
```
import pandas as pd
Dataset_1 = pd.DataFrame({'id':['a','b','c','d'],'col1':[1,2,3,4]})
print(Dataset_1)
  id  col1
0  a     1
1  b     2
2  c     3
3  d     4
Dataset_2 = pd.DataFrame({'id':['a','a','b','c','c','c','d'],'col1':[10,11,12,13,14,15,16]})
print(Dataset_2)
	id	col1
0	a	10
1	a	11
2	b	12
3	c	13
4	c	14
5	c	15
6	d	16
#Let us say i want to create 2 batches. The following dataframes are how i want my batches to look like
batch_1 = (pd.DataFrame({'id':['a','b'],'col1':[1,2]}),pd.DataFrame({'id':['a','a','b'],'col1':[10,11,12]}))
print(batch_1[0])
	id	col1
0	a	1
1	b	2
print(batch_1[1])
  id  col1
0  a    10
1  a    11
2  b    12
batch_2 = (pd.DataFrame({'id':['c','d'],'col1':[3,4]}),pd.DataFrame({'id':['c','c','c','d'],'col1':[13,14,15,16]}))
print(batch_2[0])
id  col1
0  c     3
1  d     4

print(batch_2[1])
 id  col1
0  c    13
1  c    14
2  c    15
3  d    16
```"
50304,how to auto format python code~,"I change some code in tensorflow internal .
I use clang-format to be perfectly compatible with c++ code, but I can’t find a tool that is perfectly compatible with python code anyway.  @_@~ 
please help me, I would like to get an auto-format tools. 
I'm try to auto format by rcfile in .//tensorflow/tools/ci_build/, and I try yapf, autopep8, pylint.

I'm using tf-1.15, I find a same problem on tf 2.3"
50303,Build fails with GCC-11,"Installation of Py-TensorFlow fails for below targets with GCC-11.

1. `third_party/abseil-cpp/absl/synchronization/internal/graphcycles.cc
`
2. `external/com_google_absl/absl/synchronization/internal/graphcycles.cc`

3. `external/ruy/ruy/block_map.cc`

**Fix-**
In respective header file, add `#include <limits>` and `#include <stdexcept>`
"
50302,tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
  no,the code from a example in [here](https://tensorflow.google.cn/tutorials/quickstart/advanced)
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   windows 10
- TensorFlow installed from (source or binary):
   Installed using PIP
- TensorFlow version (use command below):2.4.0
- Python version:3.8
- CUDA/cuDNN version:
   cuda:11.3
   cudnn:8.2.0
- GPU model and memory:NVIDIA GeForce RTX3060 Laptop GPU, 6g

**Describe the current behavior**
when i runing the code from example like this:
```
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Conv2D, Flatten, Dense
import os

os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'

# 加载数据
mnist = tf.keras.datasets.mnist
# dataset = mnist.load_data()
(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

# Add a channels dimension(60000, 28, 28)->(60000, 28, 28, 1)
x_train = x_train[..., tf.newaxis]
x_test = x_test[..., tf.newaxis]

# 用tf.data将数据打乱并分成batch,Batch_size = 32
train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)
test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)


class MyModel(Model):
    def get_config(self):
        pass

    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = Conv2D(32, 3, activation='relu')
        self.flatten = Flatten()
        self.d1 = Dense(128, activation='relu')
        self.d2 = Dense(10, activation='softmax')

    def call(self, x, training=None, mask=None):
        x = self.conv1(x)
        x = self.flatten(x)
        x = self.d1(x)
        return self.d2(x)


model = MyModel()

# 设置损失函数以及优化器
loss_object = tf.keras.losses.SparseCategoricalCrossentropy()
optimizer = tf.keras.optimizers.Adam()
# 选择衡量指标来评估模型的损失以及准确率
train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')


# 使用tf.GradientTape训练模型
@tf.function
def train_step(images, labels):
    with tf.GradientTape() as tape:
        predictions = model(images)
        # 计算损失
        loss = loss_object(labels, predictions)
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))

    train_loss(loss)
    train_accuracy(labels, predictions)


# 测试模型
@tf.function
def test_step(images, labels):
    predictions = model(images)
    t_loss = loss_object(labels, predictions)
    test_loss(t_loss)
    test_accuracy(labels, predictions)


EPOCHS = 5
for epoch in range(EPOCHS):
    # 在下一个epoch开始时，重置评估指标
    train_loss.reset_states()
    train_accuracy.reset_states()
    test_loss.reset_states()
    test_accuracy.reset_states()

    for images, labels in train_ds:
        train_step(images, labels)
    for test_images, test_labels in test_ds:
        test_step(test_images, test_labels)

    template = 'Epoch:{} loss:{} accuracy:{}, test loss:{}, test accuracy:{}'
    print(template.format(epoch + 1,
                          train_loss.result(),
                          train_accuracy.result() * 100,
                          test_loss.result(),
                          test_accuracy.result() * 100))
```
i have a error like this:
```
tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!
	 [[node my_model/conv2d/Conv2D (defined at E:/python/learnTensorFlow/UserGuide/快速入门.py:35) ]] [Op:__inference_train_step_531]
Errors may have originated from an input operation.
Input Source operations connected to node my_model/conv2d/Conv2D:
 Cast (defined at E:/python/learnTensorFlow/UserGuide/快速入门.py:58)
Function call stack:
train_step
```
when i runing the code, the full output info is:
```
2021-06-16 17:38:03.802929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-06-16 17:38:13.895377: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-06-16 17:38:13.921252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6
coreClock: 1.425GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2021-06-16 17:38:13.921449: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-06-16 17:38:13.933513: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-06-16 17:38:13.933669: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-06-16 17:38:13.937037: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-06-16 17:38:13.938456: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-06-16 17:38:13.940974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-06-16 17:38:13.943627: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-06-16 17:38:13.944370: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-06-16 17:38:13.944532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-06-16 17:38:13.944860: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-16 17:38:13.951923: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201621b5d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-16 17:38:13.952116: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-16 17:38:13.952322: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 3060 Laptop GPU computeCapability: 8.6
coreClock: 1.425GHz coreCount: 30 deviceMemorySize: 6.00GiB deviceMemoryBandwidth: 312.97GiB/s
2021-06-16 17:38:13.952504: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-06-16 17:38:13.952593: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-06-16 17:38:13.952686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-06-16 17:38:13.952770: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-06-16 17:38:13.952857: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-06-16 17:38:13.952940: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-06-16 17:38:13.953019: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-06-16 17:38:13.953121: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-06-16 17:38:13.953223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-06-16 17:38:14.323054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-16 17:38:14.323167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-06-16 17:38:14.323238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-06-16 17:38:14.323417: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 4733 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6)
2021-06-16 17:38:14.326010: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x201295d7b30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-16 17:38:14.326134: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6
2021-06-16 17:38:14.799589: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
2021-06-16 17:38:14.825461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-06-16 17:38:15.341945: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-06-16 17:38:15.343490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-06-16 17:38:16.059391: W tensorflow/core/framework/op_kernel.cc:1763] OP_REQUIRES failed at conv_ops.cc:1106 : Not found: No algorithm worked!
Traceback (most recent call last):
  File ""<input>"", line 1, in <module>
  File ""D:\PyCharm Community Edition 2021.1.1\plugins\python-ce\helpers\pydev\_pydev_bundle\pydev_umd.py"", line 197, in runfile
    pydev_imports.execfile(filename, global_vars, local_vars)  # execute the script
  File ""D:\PyCharm Community Edition 2021.1.1\plugins\python-ce\helpers\pydev\_pydev_imps\_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""E:/python/learnTensorFlow/UserGuide/快速入门.py"", line 86, in <module>
    train_step(images, labels)
  File ""D:\anaconda3\envs\LearnKeras_GPU\lib\site-packages\tensorflow\python\eager\def_function.py"", line 828, in __call__
    result = self._call(*args, **kwds)
  File ""D:\anaconda3\envs\LearnKeras_GPU\lib\site-packages\tensorflow\python\eager\def_function.py"", line 888, in _call
    return self._stateless_fn(*args, **kwds)
  File ""D:\anaconda3\envs\LearnKeras_GPU\lib\site-packages\tensorflow\python\eager\function.py"", line 2943, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File ""D:\anaconda3\envs\LearnKeras_GPU\lib\site-packages\tensorflow\python\eager\function.py"", line 1919, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""D:\anaconda3\envs\LearnKeras_GPU\lib\site-packages\tensorflow\python\eager\function.py"", line 560, in call
    ctx=ctx)
  File ""D:\anaconda3\envs\LearnKeras_GPU\lib\site-packages\tensorflow\python\eager\execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.NotFoundError:  No algorithm worked!
	 [[node my_model/conv2d/Conv2D (defined at E:/python/learnTensorFlow/UserGuide/快速入门.py:35) ]] [Op:__inference_train_step_531]
Errors may have originated from an input operation.
Input Source operations connected to node my_model/conv2d/Conv2D:
 Cast (defined at E:/python/learnTensorFlow/UserGuide/快速入门.py:58)
Function call stack:
train_step
```"
50301,Unable to convert .pb file into .tflite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation (pip package or built from source):
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

Provide code to help us reproduce your issues using one of the following options:

#### Option A: Reference colab notebooks

1)  Reference [TensorFlow Model Colab](https://colab.research.google.com/gist/ymodak/e96a4270b953201d5362c61c1e8b78aa/tensorflow-datasets.ipynb?authuser=1): Demonstrate how to build your TF model.
2)  Reference [TensorFlow Lite Model Colab](https://colab.research.google.com/gist/ymodak/0dfeb28255e189c5c48d9093f296e9a8/tensorflow-lite-debugger-colab.ipynb): Demonstrate how to convert your TF model to a TF Lite model (with quantization, if used) and run TFLite Inference (if possible).

```
(You can paste links or attach files by dragging & dropping them below)
- Provide links to your updated versions of the above two colab notebooks.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

#### Option B: Paste your code here or provide a link to a custom end-to-end colab

```
(You can paste links or attach files by dragging & dropping them below)
- Include code to invoke the TFLite Converter Python API and the errors.
- Provide links to your TensorFlow model and (optionally) TensorFlow Lite Model.
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

- Model produces wrong results and/or has lesser accuracy.
- Model produces correct results, but it is slower than expected.

### 4. (optional) RNN conversion support
If converting TF RNN to TFLite fused RNN ops, please prefix [RNN] in the title.

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50300,why the environment variable CUDA_VISIBLE_DEVICES no use,"hi, when i run tensorflow  project, i set those two  environment variables below to make the tensorflow use the GPUs as i expected, but the settings did not effect as i expected.

export CUDA_DEVICE_ORDER=""PCI_BUS_ID""
export CUDA_VISIBLE_DEVICES=""5,6""

as  i set the variable CUDA_VISIBLE_DEVICES as above, when i run tensorflow project , it only use the GPU:0 and GPU:1, it seems that it ignore what i set , just start with GPU:0.

can anyone give some advices?
thanks a lot.

PS my tensorflow version is 2.4.1"
50299,The latest docker image cannot work on my computer,"<em>I pulled the latest tensorflow-gpu image but I cannot run this image </em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04
- TensorFlow version: docker image: 'tensorflow/tensorflow:latest-gpu-jupyter'
- CUDA/cuDNN version:CUDA version 10.2
- GPU model and memory: 2080ti * 4, each with memory 11019MB
- Docker version 19.03.8, build afacb8b7f0



**Describe the problem**
Since tensorflow-io installed by pip cannot work together with tensorflow installed by conda, I decided to use the docker image of tensorflow. I installed nvidia-docker according to this page: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker. And the result is this:
 ![捕获](https://tvax4.sinaimg.cn/large/006dgyGJgy1grk3opnyi6j30sd0h3goc.jpg)
Then, I pulled the tensorflow image by this:
`docker pull tensorflow/tensorflow:latest-gpu-jupyter`
and run this image by this:
`docker run --gpus=all -it tensorflow/tensorflow:latest-gpu bash`
then, I meet this error:
_docker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused ""process_linux.go:449: container init caused \""process_linux.go:432: running prestart hook 0 caused \\\""error running hook: exit status 1, stdout: , stderr: nvidia-container-cli: requirement error: unsatisfied condition: cuda>=11.2, please update your driver to a newer version, or use an earlier cuda container\\\\n\\\""\"""": unknown.
ERRO[0000] error waiting for container: context canceled_
Why would it give this error: unsatisfied condition: cuda>=11.2


**Any other info / logs**
That's all images in this computer
![捕获](https://tvax4.sinaimg.cn/large/006dgyGJgy1grk3yb08ygj30tb06tq4o.jpg)"
50298,tf2 could NOT start more than 2 processes/sessions (one process one tf session) on intel i9 processor while tf1.15.4 is ok,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
   ```
   No
   ```
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   ```
   Linux 289d7745a47e 5.10.25-linuxkit #1 SMP Tue Mar 23 09:27:39 UTC 2021 x86_64 GNU/Linux
   ```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
   ```
   N.A.
   ```
- TensorFlow installed from (source or binary):
   ```
   binary, direct pip install, so far tested with version 2.3.1 and 2.5.0
   ```
- TensorFlow version (use command below):
   ```
   2.3.1, 2.5.0
   ```
- Python version:
   ```
   Python 3.6.12
   ```
- Bazel version (if compiling from source):
   ```
   N.A.
   ```
- GCC/Compiler version (if compiling from source):
   ```
   gcc (Debian 6.3.0-18+deb9u1) 6.3.0 20170516
   Copyright (C) 2016 Free Software Foundation, Inc.
   This is free software; see the source for copying conditions.  There is NO
   warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
   ```
- CUDA/cuDNN version:
   ```
   N.A.
   ```
- GPU model and memory:
   ```
   N.A.
   ```

1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
   ```
   N.A.
   ```
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
   ```
   v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
   ```

**Describe the current behavior**

tensorflow2 could **NOT** start more than 2 processes on **intel i9 processor**? (within docker container), i am not quite sure i9 processors is the issue here, but this is working great on my old laptop.

And i tested the exact behaviour with tensorflow 1.15.4 and it is ok.

```
Docker version 20.10.6, build 370c289
```
```
processor	: 0
vendor_id	: GenuineIntel
cpu family	: 6
model		: 158
model name	: Intel(R) Core(TM) i9-9880H CPU @ 2.30GHz
```
So far i tested on tensorflow `2.3.1` and `2.5.0`,  both cannot start more than 2 tf sessions, then behaviour is that for the first 2 tf sessions it is pretty quick, like a few seconds, but when i tried to start the 3rd tf session, it took quite long, like 5 or 6 mins+, eventually it still cant start, but one of the previous started tf session will get killed silently.

**Standalone code to reproduce the issue**

`tf2.py` to load model:
```
import tensorflow as tf

model = tf.saved_model.load(""/models/tf2_models/od_ssd_mobilenet/1"")

print('233333-------------')

while True:
  pass
```

the `od_ssd_mobilenet` model i download from here: [http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz)

- Step 1: do `python tf2.py &` to load the model and put the process to the background, and wait the `233333-------------` message get printed;
- Step 2: then use `ps -ef | grep tf2.py` to check the process is running;
- Repeat step 1 and 2 to start 2 ok processes;
- Repeat step 1 and 2 to try to start the 3rd process, you will find out it will take super long, and when eventually you see the `233333-------------`  message get printed, one of the previous 2 ok process was killed silently.

And i tested the exact behaviour with tensorflow 1.15.4 and it is ok.

the logs i had when the model was loading:
```
2021-06-16 04:33:57.384790: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2021-06-16 04:33:57.384844: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
2021-06-16 04:34:00.171257: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2021-06-16 04:34:00.171318: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2021-06-16 04:34:00.171409: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (289d7745a47e): /proc/driver/nvidia/version does not exist
2021-06-16 04:34:00.171848: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
233333-------------
```
"
50296,[_Derived_]RecvAsync is cancelled while running tensorflow RNN tutorial code on GPU,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:  CUDA: 11.2.0
- GPU model and memory: NVIDIA Titan RTX 24 GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am running this tutorial https://www.tensorflow.org/text/tutorials/text_classification_rnn and during  training I am getting below error,

Epoch 1/10
 67/391 [====>.........................] - ETA: 18s - loss: 0.6928 - accuracy: 0.4977
---------------------------------------------------------------------------
CancelledError                            Traceback (most recent call last)
<ipython-input-25-d9321db1417e> in <module>
----> 1 model.fit(train_dataset, epochs=10,
      2                     validation_data=test_dataset,
      3                     validation_steps=30)

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\keras\engine\training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)
   1181                 _r=1):
   1182               callbacks.on_train_batch_begin(step)
-> 1183               tmp_logs = self.train_function(iterator)
   1184               if data_handler.should_sync:
   1185                 context.async_wait()

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\def_function.py in _call(self, *args, **kwds)
    915       # In this case we have created variables on the first call, so we run the
    916       # defunned version which is guaranteed to never create variables.
--> 917       return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
    918     elif self._stateful_fn is not None:
    919       # Release the lock early so that multiple threads can perform the call

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in __call__(self, *args, **kwargs)
   3021       (graph_function,
   3022        filtered_flat_args) = self._maybe_define_function(args, kwargs)
-> 3023     return graph_function._call_flat(
   3024         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
   3025 

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in _call_flat(self, args, captured_inputs, cancellation_manager)
   1958         and executing_eagerly):
   1959       # No tape is watching; skip to running the function.
-> 1960       return self._build_call_outputs(self._inference_function.call(
   1961           ctx, args, cancellation_manager=cancellation_manager))
   1962     forward_backward = self._select_forward_and_backward_functions(

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\function.py in call(self, ctx, args, cancellation_manager)
    589       with _InterpolateFunctionError(self):
    590         if cancellation_manager is None:
--> 591           outputs = execute.execute(
    592               str(self.signature.name),
    593               num_outputs=self._num_outputs,

~\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\eager\execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     57   try:
     58     ctx.ensure_initialized()
---> 59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
     60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:

CancelledError:  [_Derived_]RecvAsync is cancelled.
	 [[{{node Adam/Adam/update/AssignSubVariableOp/_57}}]]
	 [[gradient_tape/sequential/embedding/embedding_lookup/Reshape/_54]] [Op:__inference_train_function_22715]

Function call stack:
train_function


**Describe the expected behavior**
It should work ok without issues. I have not changed any code, it is a tutorial from tensorflow official documentation. If you train on CPU there is no issue so something wrong with training on GPU

**[Contributing](https://www.tensorflow.org/community/contribute)**

- Do you want to contribute a PR? (yes/no):
- Briefly describe your candidate solution(if contributing):

**Standalone code to reproduce the issue**
It is very easy to reproduce. Just run https://www.tensorflow.org/text/tutorials/text_classification_rnn in a jupyter notebook on a machine that has GPU

This seems to be similar to https://github.com/tensorflow/tensorflow/issues/33721, opening this because that previous issue was closed without a resolution.
"
50288,Keras generic_utils Mangles ConvLSTM2D Default Layer Name,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.9.4


**Describe the current behavior**
```python
>>> import tensorflow as tf
>>> tf.keras.layers.ConvLSTM2D(1, 1).name
'conv_lst_m2d_0'
```

**Describe the expected behavior**
```python
>>> import tensorflow as tf
>>> tf.keras.layers.ConvLSTM2D(1, 1).name
'conv_lstm_2d_0'
```

**Other info**
Problem caused here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/base_layer.py#L2435
If a name is not provided to a `Layer` then it used the `generic_utils` function `to_snake_case`, which shows unexpected behavior here:
```python
>>> from tensorflow.python.keras.utils import generic_utils
>>> generic_utils.to_snake_case('ConvLSTM2D')
'conv_lst_m2d'
```

An ad hoc fix here seems inelegant in `generic_utils`. Moreover, changing the regex in the linked function will likely have repercussions for many other names where this may not be an issue.

Thoughts on setting a default name within ConvLSTM2D __init__ instead? https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/convolutional_recurrent.py#L154
Something passed to super like `name=kwargs.get('name', backend.unique_object_name('conv_lstm_2d', zero_based=True))`? I don't have an elegant solution here."
50286,Keras Reshape layers are not folded when converted to TFLite,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 11, Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): `2.5.0`, `tf-nightly`

### 2. Code

Keras `Reshape` layers are not correctly constant folded when converted to TFLite.

Consider the following two models which either use `tf.keras.layers.Reshape((50, 2))(x)`  or `tf.reshape(x, (-1, 50, 2))` to reshape tensors. The Keras layer here is equivalent to  calling `tf.reshape(x, (tf.shape(x)[0], 50, 2))` which unfortunately doesn't get fused due to a dynamic batch dimension.
This can be worked around by manually setting the Keras model batch size to 1, but I think this should be handled by the converter instead so that the default Keras reshape layer also gets converted to an efficient reshape op.

```python
import tensorflow as tf

def unfused_model():
    img = tf.keras.layers.Input(shape=(3, 3, 10))
    x = tf.keras.layers.Conv2D(100, (3, 3))(img)
    x = tf.keras.layers.Reshape((50, 2))(x)
    return tf.keras.Model(img, x)

def fused_model():
    img = tf.keras.layers.Input(shape=(3, 3, 10))
    x = tf.keras.layers.Conv2D(100, (3, 3))(img)
    x = tf.reshape(x, (-1, 50, 2))
    return tf.keras.Model(img, x)

def convert_model(model, filename):
    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    with open(filename, ""wb"") as f:
        f.write(converter.convert())

convert_model(unfused_model(), ""/tmp/unfused_model.tflite"")
convert_model(fused_model(), ""/tmp/fused_model.tflite"")
```

`unfused_model.tflite` | `fused_model.tflite`
---|---
<img width=""219"" alt=""Screenshot 2021-06-15 at 18 36 12"" src=""https://user-images.githubusercontent.com/13285808/122098756-207ca800-ce09-11eb-9880-2b91b147cb5d.png""> | <img width=""145"" alt=""Screenshot 2021-06-15 at 18 36 05"" src=""https://user-images.githubusercontent.com/13285808/122098753-1fe41180-ce09-11eb-9530-fc319eac8097.png"">

### 3. Possible Solution

I think it would be possible to implement a `tfl.reshape` constant folder even for the dynamic batch size case that checks if only one dimension of the reshape is dynamic and replaces this with `-1`. However, this would opt out of possible shape error checking in cases the reshape would be invalid. On the other hand from looking at the netron graph it seems like the batch dimension is fixed to 1 anyway, so I am not sure why the reshape didn't get folded correctly in the first place, but let me know if I am missing something."
50285,Device interconnect StreamExecutor with strength 1 edge matrix,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 
- TensorFlow installed from (source or binary): source 
- TensorFlow version (use command below): 2.4
- Python version: 3.6.5

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am running the BoostedTree, and for the only binary problem, I faced the following error.
I am using the CPU version of the TensorFlow, and it is working fine for multi-class classification
```python
Skipping registering GPU devices...
2021-06-15 17:58:51.972609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-15 17:58:51.972635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 
2021-06-15 17:58:51.972653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N 
2021-06-15 17:58:51.973854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N 
```



**Other info / logs** Include any logs or source code that would be helpful to
```python
 installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.

```
"
50282,mix precison bfloat16 lstm/conv1d support,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 **2.5.0**

- Python version: 3.8.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 
- CUDA/cuDNN version: 11.2
- GPU model and memory: 32G, tesla V100

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
```
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras import mixed_precision
 
policy = mixed_precision.Policy('bfloat16')  #'mixed_float16' 'mixed_bfloat16'等多种选择，后续的都会是该设置的类型
mixed_precision.set_global_policy(policy)
print('Compute dtype: %s' % policy.compute_dtype)
print('Variable dtype: %s' % policy.variable_dtype)
 

inputs = keras.Input(shape=(784,), name='digits')
if tf.config.list_physical_devices('GPU'):
  print('The model will run with 4096 units on a GPU')
  num_units = 4096
else:
  print('The model will run with 64 units on a CPU')
  num_units = 64
 
#dense test #测试cpu/GPU上可以跑
dense1 = layers.Dense(num_units, activation='relu', name='dense_1')
x = dense1(inputs)
dense2 = layers.Dense(num_units, activation='relu', name='dense_2')
x = dense2(x)
 
#lstm gpu test
inputs = tf.random.normal([32, 10, 8])
lstm = tf.keras.layers.LSTM(4)
output = lstm(inputs)
print(output.shape)
""""""
>>> output = lstm(inputs)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 668, in __call__
    return super(RNN, self).__call__(inputs, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1023, in __call__
    self._maybe_build(inputs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 2625, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 586, in build
    self.cell.build(step_input_shape)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/utils/tf_utils.py"", line 270, in wrapper
    output_shape = fn(instance, input_shape)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/recurrent.py"", line 2361, in build
    self.recurrent_kernel = self.add_weight(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 639, in add_weight
    variable = self._add_variable_with_custom_getter(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py"", line 810, in _add_variable_wi
th_custom_getter
    new_variable = getter(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py"", line 127, in make_vari
able
    return tf_variables.VariableV1(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 260, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 206, in _variable_v1_call
    return previous_getter(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 199, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py"", line 2612, in default_variable_cr
eator
    return resource_variable_ops.ResourceVariable(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/variables.py"", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1584, in __init__
    self._init_from_args(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py"", line 1722, in _init_from_a
rgs
    initial_value = initial_value()
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/initializers/initializers_v2.py"", line 606, in __ca
ll__
    q, r = gen_linalg_ops.qr(a, full_matrices=False)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/gen_linalg_ops.py"", line 2072, in qr
    _ops.raise_from_not_ok_status(e, name)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Value for attr 'T' of bfloat16 is not in the list of allowed values: double, float, half, comple
x64, complex128
        ; NodeDef: {{node Qr}}; Op<name=Qr; signature=input:T -> q:T, r:T; attr=full_matrices:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT
_HALF, DT_COMPLEX64, DT_COMPLEX128]> [Op:Qr]
```
""""""
 
#conv1d gpu test
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(32, 3, activation='relu',input_shape=input_shape[1:])(x)
'''
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1030, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 249, in call
    outputs = self._convolution_op(inputs, self.kernel)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1012, in convolution_v2
    return convolution_internal(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1142, in convolution_internal
    return op(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 602, in new_func
    return func(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 602, in new_func
    return func(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 1907, in conv1d
    return array_ops.squeeze(result, [spatial_start_dim])
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py"", line 535, in new_func
    return func(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py"", line 4471, in squeeze
    return gen_array_ops.squeeze(input, axis, name)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/gen_array_ops.py"", line 10176, in squeeze
    _ops.raise_from_not_ok_status(e, name)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: Tried to squeeze dim index -3 for tensor with 1 dimensions. [Op:Squeeze
'''
#conv2d gpu test
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(2, 3, activation='relu', input_shape=input_shape[1:])(x)
'''
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py"", line 1030, in __call__
    outputs = call_fn(inputs, *args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py"", line 267, in call
    outputs = nn.bias_add(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py"", line 206, in wrapper
    return target(*args, **kwargs)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py"", line 3377, in bias_add
    return gen_nn_ops.bias_add(
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py"", line 678, in bias_add
    _ops.raise_from_not_ok_status(e, name)
  File ""/proj/xcdhdstaff1/bokangz/anaconda3/envs/tfasr/lib/python3.8/site-packages/tensorflow/python/framework/ops.py"", line 6897, in raise_from_not_ok_status
    six.raise_from(core._status_to_exception(e.code, message), None)
  File ""<string>"", line 3, in raise_from
tensorflow.python.framework.errors_impl.InvalidArgumentError: cannot compute BiasAdd as input #1(zero-based) was expected to be a float tensor but is a bfloat16 tensor [Op:BiasAdd]
'''
**Describe the expected behavior**
lstm/conv1d run successfully as dense layers

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50281,ValueError: image not in JPEG format for Custom Object Detection using TF Lite Model Maker,"## URL(s) with the issue:
Codelabs Colab Notebook link for custom object detection using TF Lite Model Maker:
https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_object_detection.ipynb#scrollTo=p79NHCx0xFqb

## Description of issue (what needs changing):
I have been following the above mentioned Codelabs for training a model for custom object detection using TF Lite Model Maker. However, when I try importing the image dataset as a CSV from local path, I face an error that says, `ValueError: image not in JPEG format`; although all the training, validation and test images are of the said format i.e. JPEG.  

### Clear description
Below is the screenshot of the same.
![image](https://user-images.githubusercontent.com/37960279/122074390-ce428380-ce16-11eb-915e-703b84aed02e.png)

### Correct links

Is the link to the source code correct?
Yes

### Parameters defined
CSV file.

Are all parameters defined and formatted correctly?
Yes

### Returns defined

Are return values defined?
Yes

### Raises listed and defined

Are the errors defined? For example,
https://github.com/sozercan/tensorflow-object-detection/blob/master/tensorflow-object-detection/generic_create_pascal_tf_record.py

### Usage example

Is there a usage example?
Yes, the example is taken from the following Codelabs: https://codelabs.developers.google.com/tflite-object-detection-android#8
"
50278,Second gradient return None using tf.while_loop in TF2.x,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):Example provided below
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- TensorFlow installed from binary
- TensorFlow version (use command below): 2.5 and 1.15.2 both in Colab
- Python version: 3.7

**Describe the current behavior**
In tf2.5, when using `tf.while_loop`, we are able to call `.gradients()` in the `body()` of `tf.while_loop`. 
However, if we call `tf.GradientTape()`to get the first gradient outside the `tf.while_loop`, and then call `.gradients()` to get the second gradient in the `body()` of the `tf.while_loop`, the second gradient will be `None`.

**Standalone code to reproduce the issue**
[colab](https://colab.research.google.com/drive/1xV2vN_fZkkZz47AmoH-y-Fl1B6lmtpGB?usp=sharing)
```python3 
# step function
@tf.function
def step(dummy):
  with tf.GradientTape(persistent=True) as tape:
    loss = tf.reduce_sum(tf.pow(var, 2))  # loss = w^2
  grad = tape.gradient(loss, var)  # 1st gradient, grad = 2w
  train = loop(grad, var, dummy)

# Set loop funtion
def loop(grad, var, dummy):
  def cond(dummy):
    return True

  def body(dummy):
    s = tf.constant(1.)
    elemwise = math_ops.multiply(grad, array_ops.stop_gradient(s))
    grad_value = tf.gradients(elemwise, var) # 2nd gradient 
    # return None to grad_value, but expect grad_value is not None
    print('grad_value=', grad_value)
    return dummy

  return tf.while_loop(cond, body, [dummy])

# Set variables
var = tf.Variable(2., name= 'w1')
dummy = tf.constant(1.)
step(dummy)
```

**Describe the expected behavior**
Expect the grad_value is not `None`



**Other info / logs** 
 - tf2.5(**I focus on**) with `@tf.function` fails
 - tf1.15 In default graph mode works, but in eager mode with `@tf.function` fails
 - similar issue [this](https://github.com/tensorflow/tensorflow/issues/24866#issue-398513482) and [this](https://stackoverflow.com/questions/49555016/compute-gradients-for-each-time-step-of-tf-while-loop) 
"
50277,Bug: Bazel 3.7.2 fails to build with GCC 11.1.0 on ARMv8,"Hi, 
`Bazel 3.7.2.` is failing to build on  `ARMv8`, hopefully, in the latest Bazel, they have fixed it.
Any plan to use upgraded Bazel?

"
50276,TypeError: An op outside of the function building code is being passed ,"Hi, i am trying to replicate this https://www.kaggle.com/ratthachat/image-captioning-by-effnet-attention-in-tf2-1/notebook but when i try to run the below code 

```
# captions on the train set
rid = np.random.randint(0, len(img_name_train))
image = img_name_train[rid]
real_caption = ' '.join([tokenizer.index_word[i] for i in cap_train[rid] if i not in [0]])
result, attention_plot = evaluate(image)

print ('Real Caption:', real_caption)
print ('Prediction Caption:', ' '.join(result))
plot_attention(image, result, attention_plot)
```
i am getting this below error

TypeError: An op outside of the function building code is being passed
a ""Graph"" tensor. It is possible to have Graph tensors
leak out of the function building context by including a
tf.init_scope in your function building code.
For example, the following function will fail:
  @tf.function
  def has_init_scope():
    my_constant = tf.constant(1.)
    with tf.init_scope():
      added = my_constant * 2

tensorflow version i am using is 2.4.1.

"
50275,Conversion of tensor shape values to None in a keras model,"## Suppose I have a simple keras model as follows, 

    Model: ""sequential_1""
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 16, 16, 8)         224       
    _________________________________________________________________
    conv2d_1                     (None, 7, 7, 8)           584       
    _________________________________________________________________
    conv2d_2                     (None, 3, 3, 2)           584       
    =================================================================
    Total params: 1,392
    Trainable params: 1,392
    Non-trainable params: 0

## Now I want the final output layer shape as follows,


    Model: ""sequential_1""
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 16, 16, 8)         224       
    _________________________________________________________________
    conv2d_1                     (None, 7, 7, 8)           584       
    _________________________________________________________________
    conv2d_2                     (None, None, None, 2)     584       
    =================================================================
    Total params: 1,392
    Trainable params: 1,392
    Non-trainable params: 0

## Code

    m = Sequential()
    
    m.add(layers.InputLayer(input_shape=(32, 32, 1)))
    m.add(Conv2D(8, (3,3), padding='same', strides=2))
    m.add(Conv2D(8, (3,3), strides=2))
    m.add(Conv2D(2, (3,3), strides=2))

## Details
> I hope you got an understanding of the issue.
> Please try to make your own versions of the code. This is just a demo code.

I am working on image colorization problem where we input an image with one color channel and get output with 2 color channels.
The original code is huge and different from any codes on the internet. So please don't be concerned about suggesting me any image colorization codes. 

I really appreciate it if you can help me with this issue."
50274,ResNet giving different outputs in TF and TFLite in C++,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux CentOS 8
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.3.2
- Python version: 3.6.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): 8.3.1


**Describe the current behavior**
ResNet36 (and ResNet50) models give different outputs when running inference on TensorFlow and TensorFlow-Lite in C++.

**Describe the expected behavior**
Outputs should be same across all platforms and across different APIs.


**Standalone code to reproduce the issue**
Due to certain restrictions, I won't be able to provide fully reproducible code. Following are some snippets : 
Model : 
```
base_model = tf.keras.applications.ResNet50(
    include_top=False, weights='imagenet',
    input_shape=[96,112,3])

x = base_model.output
x = tf.keras.layers.Flatten()(x)
x = tf.keras.layers.Dense(128, use_bias=True, name='Bottleneck')(x)
   
model = tf.keras.models.Model( base_model.input, x )
```

TFLite inference : 
```
std::unique_ptr<tflite::FlatBufferModel> flat_buffer_model = tflite::FlatBufferModel::BuildFromBuffer((const char*)model_buffer->data(), model_buffer->size(), &err_reporter);
tflite::ops::builtin::BuiltinOpResolver builtins;
std::unique_ptr<tflite::Interpreter> interpreter;
tflite::InterpreterBuilder(*flat_buffer_model, builtins)(&interpreter);

const std::vector<int>& outputs = thread_interpreter.interpreter_->outputs();
TfLiteStatus tf_stat = thread_interpreter.interpreter_->Invoke();

```

**Other info / logs** 
I have a ResNet36 model trained using TFv1 and saved into a protobuf file format. This is used to generate inference on sample image using TensorFlow C API on windows. 
I used ```tf.compat.v1.lite.converter.from_frozen_graph()``` to convert this protobuf to a .tflite file.
I ran inference from TFLite using C++ API (on x86_64 architecture).
Additionally, I also ran inference from the TFLite model using Python.

```***All the 3 outputs are different.***```

Interestingly, I also have an InceptionResNet model (.pb file from TF1 converted into .tflite file using TF2) and it gives the exact same output when I run inference in TF (C) and TFLite (C++). So, this makes me question that something is wrong with tflite converter/API specifically when dealing with ResNet architecture.

Note : I also tested ResNet50 architecture which was trained in TF2 and got similar anomalous results. (This is just to rule out any functionalities that may not have been supported by backwards compatible converter).


I am interested in knowing pointers such as gcc compiler optimizations or anything else that may cause different outputs across TensroFlow and TensorFlow-Lite in C++.

Any help is much appreciated."
50273,CUPTI could not be loaded & cupti.dll not found,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- Windows 10
- TensorFlow installed from: Source
- TensorFlow version:  2.5.0
- Python version: 3.8.10
- CUDA/cuDNN version:  11.2 / 8.1.0 I believe
- GPU model and memory: RTX 2080
This is the error I've get;
I had [this issue](https://github.com/tensorflow/tensorflow/issues/43030#issuecomment-799969225). Once I did that copy trick, I started to get this issue...
![Ekran görüntüsü 2021-06-15 065118](https://user-images.githubusercontent.com/67585935/121990242-1d040500-cda6-11eb-9a79-b32762356f17.png)
"
50260,make_csv_dataset fails inside Dataset.interleave(),"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): debian
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: NA
- GPU model and memory: NA


**Describe the current behavior**
`make_csv_dataset`  fails  when used as a map function inside `data.Dataset.interleave()`


**Describe the expected behavior**
It must create datasets


**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
files = tf.data.Dataset.list_files(""*.csv"")
ds3 = files.interleave(lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))
```

[link to colab](https://colab.research.google.com/gist/eli-osherovich/ac1e62d1c2ba919fd450dfec4c178ab9/test.ipynb)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Running this code:
```python
files = tf.data.Dataset.list_files(""*.csv"")
ds3 = files.interleave(lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))
```
I get the following error:
```
OperatorNotAllowedInGraphError: in user code:

    <ipython-input-5-fcdae26c49e4>:2 None  *
        lambda x: tf.data.experimental.make_csv_dataset(x, batch_size=1))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/readers.py:480 make_csv_dataset_v2  **
        filenames = _get_file_names(file_pattern, False)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/experimental/ops/readers.py:1135 _get_file_names
        file_names = list(gfile.Glob(file_pattern))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py:383 get_matching_files
        return get_matching_files_v2(filename)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/lib/io/file_io.py:449 get_matching_files_v2
        for single_filename in pattern
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:520 __iter__
        self._disallow_iteration()
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:516 _disallow_iteration
        self._disallow_in_graph_mode(""iterating over `tf.Tensor`"")
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:496 _disallow_in_graph_mode
        "" this function with @tf.function."".format(task))

    OperatorNotAllowedInGraphError: iterating over `tf.Tensor` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.
```"
50259,Monolithic build for Tf Lite with CMake,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5
- Python version: 
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source): Cmake 3.16
- GCC/Compiler version (if compiling from source): 7.4
- CUDA/cuDNN version:
- GPU model and memory:

**Describe the problem**
I want to create a monolithin build of the libtensorflow-lite.so library file. 

With bazel, you used to just add `--config=monolithic` to the `bazel build` command in order to stop having to ensure libraries like libflatbuffers.a, libruy.a, etc. had to be passed on along with libtensorflow-lite.so.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
Steps provided from here:
https://www.tensorflow.org/lite/guide/build_cmake
"
50258,URL of tf.raw_ops.SparseReduceSum comprises Irrelevant/Garbage Values,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseReduceSum?hl=ko.Souten%20Ki%20Beti%20HD%20Jeetendra%20Rekha%20Jaya%20Prada%20Hindi%20Full%20MovieShyam

## Description of issue (what needs changing):

The **`Hyperlink`** of the Op, [**`tf.raw_ops.SparseReduceSum`**](https://www.tensorflow.org/api_docs/python/tf/raw_ops/SparseReduceSum?hl=ko.Souten%20Ki%20Beti%20HD%20Jeetendra%20Rekha%20Jaya%20Prada%20Hindi%20Full%20MovieShyam) has some garbage/irrelevant values like **Souten**, **Beti**, **Jeetendra**, **Rekha**, etc.."
50257,Fix for CVE-2021-29515,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3.2
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**

The implementation of `MatrixDiag*` operations(https://github.com/tensorflow/tensorflow/blob/4c4f420e68f1cfaf8f4b6e8e3eb857e9e4c3ff33/tensorflow/core/kernels/linalg/matrix_diag_op.cc#L195-L197) does not validate that the tensor arguments are non-empty. The fix will be included in TensorFlow 2.5.0. We will also cherrypick this commit on TensorFlow 2.4.2, TensorFlow 2.3.3, TensorFlow 2.2.3 and TensorFlow 2.1.4, as these are also affected and still in supported range.

**Will this change the current api? How?** No

**Who will benefit with this feature?** Vulnerability fix.

**Any Other info.**
When can we expect a fix on Tensorflow-2.3.3.
"
50255,Reading shape of tf.Tensor fails after sliced using an input inside tf.function,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Linux Ubuntu 20.04
- TensorFlow installed from: pip installation
- TensorFlow version: 2.5
- Python version: 3.8.10
- CUDA/cuDNN version: 7.6.5
- GPU model and memory: RTX 2060, 6GB

Running the following block:
```python
import tensorflow as tf

@tf.function
def fun(sample, pos):
    x = sample[:10-pos]
    l = tf.constant(x.shape[0])
    return l

sample = tf.random.uniform([10])
pos = tf.constant(2)
out = fun(sample, pos)
```
This results in the following error message:
```
ValueError                                Traceback (most recent call last)
<ipython-input-15-e85c7a447ebb> in <module>
      7 sample = tf.random.uniform([10])
      8 pos = tf.constant(2)
----> 9 out = fun(sample, pos)

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--> 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    761     self._graph_deleter = FunctionDeleter(self._lifted_initializer_graph)
    762     self._concrete_stateful_fn = (
--> 763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
    764             *args, **kwds))
    765 

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-> 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-> 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3277     arg_names = base_arg_names + missing_arg_names
   3278     graph_function = ConcreteFunction(
-> 3279         func_graph_module.func_graph_from_py_func(
   3280             self._name,
   3281             self._python_function,

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--> 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

~/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, ""ag_error_metadata""):
--> 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

ValueError: in user code:

    <ipython-input-15-e85c7a447ebb>:4 fun  *
        l = tf.constant(x_l.shape[0])
    /home/dinesh/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264 constant  **
        return _constant_impl(value, dtype, shape, name, verify_shape=False,
    /home/dinesh/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl
        tensor_util.make_tensor_proto(
    /home/dinesh/miniconda3/envs/tf-gpu/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:445 make_tensor_proto
        raise ValueError(""None values not supported."")

    ValueError: None values not supported.
```
As I understand, this means the shape of x after being sliced from a tensor using a value input to the autographed function is somehow set to None. Note that this code throws the same error message (sample is defined inside rather than as an argument):
```python
@tf.function
def fun(pos):
    sample = tf.random.uniform([10])
    x = sample[:10-pos]
    l = tf.constant(x.shape[0])
    return l

#sample = tf.random.uniform([10])
pos = tf.constant(2)
out = fun(pos)
```
However, this runs without any errors (pos is defined inside the function):
```python
@tf.function
def fun(sample):
    pos = tf.constant(2)
    x = sample[:10-pos]
    l = tf.constant(x.shape[0])
    return l

sample = tf.random.uniform([10])
#pos = tf.constant(2)
out = fun(sample)
```"
50254,Bug in keras custom layer,"System: Python 3.8.3 64-bit | Qt 5.12.9 | PyQt5 5.12.3 | Linux 5.8.0-55-generic 
              keras (newest version)
```#!/usr/bin/env python3
from keras.layers import Layer
import numpy as np
from keras.layers import Input
from keras.models import Model  

InputTensor = [0.1,0.2,0.3]      # Attention! Different tensors cause errors. 
                                 # See  below the code!

InputTensor = np.array(InputTensor) 

class MyLayer(Layer):
    
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, x):
        return x

inputlayer = Input(shape = InputTensor.shape)
layer = MyLayer()(inputlayer)

model = Model(inputlayer, layer ) 
model.summary()

Output = model.predict(InputTensor)
output = np.array(Output)
print(""dim(Output) = ""+str(output.ndim))
print(""Output = ""+str(Output))
```

1. Input tensor [0.1,0.2,0.3] cause no problems. Output is [[0.1][0.2][0.2]]. Access to tensor elements are possible. model.summary is (None,2), ndim is 2.
2. Input tensor [[0.1, 0.2,0.3],[0.2,0.3,0.4],[0.3,0.4,0.5]] has an output as [[0.1 0.2 0.3][0.2 0.3 0.4][0.3 0.4 0.5]] and model.summary indicate a shape like (None,3,3) so that a tensor of tree dimensions is expected. ndim should be 3 instead of 2! Access to tensor elements is possible if you consider 2 dimensions instead of 3.
3. Input tensor [[0.1,0.2,0.3],[0.2,0.3,0.4]] generates an error message. It expected a shape of (None,2,3) and found only (None,3) although model.summary shows (None,2,3). 

It shouldn't have any influence how many dimensions a tensor has, nor how big its elements are!
My task is, to get access to single elements of a tensor to generate an output tensor like
```
class MyLayer(Layer):
    def __init__(self, **kwargs):
        super(MyLayer, self).__init__(**kwargs)
    def build(self, input_shape):
        self.built = True
        super(MyLayer,self).build(input_shape)

    def call(self, x):
        OutputTensor = [[0.0, 0.0],
                        [0.0, 0.0]]
        OutputTensor[0][0] = x[0][0]
        OutputTensor[1][1] = x[1][0]
        OutputTensor[1][0] = x[2][0]
        return OutputTensor
```

"
50253,update all values of dict like in session.run tensorflow,"This function takes the pre-trained imagenet-vgg-verydeep-19.mat model and using its weights and input returns the graph(model)

```
def load_vgg_model(path):
    ...
    ...
    ...
    # Constructs the graph model.
    graph = {}
    graph['input']   = tf.Variable(np.zeros((1, 400, 300, 3)), dtype = 'float32')
    graph['conv1_1']  = _conv2d_relu(graph['input'], 0, 'conv1_1')      # returns tf.nn.relu
    graph['conv1_2']  = _conv2d_relu(graph['conv1_1'], 2, 'conv1_2')
    graph['avgpool1'] = _avgpool(graph['conv1_2'])                    # returns tf.nn.avg_pool
    graph['conv2_1']  = _conv2d_relu(graph['avgpool1'], 5, 'conv2_1')
    ....
    ....
    graph['conv5_4']  = _conv2d_relu(graph['conv5_3'], 34, 'conv5_4')
    graph['avgpool5'] = _avgpool(graph['conv5_4'])
    
    return graph
```



Here is the implementation of the same in TensorFlow 1

```
model = load_vgg_model(""imagenet-vgg-verydeep-19.mat"")
sess = tf.compat.v1.InteractiveSession()
content_image = imread(""images/louvre_small.jpg"")

sess.run(model['input'].assign(content_image))
out = model['conv4_2']
a_C = sess.run(out)
a_G = out
```

But I wanted to know the implementation in TensorFlow 2.x

I have read these documents https://www.tensorflow.org/guide/effective_tf2

https://www.tensorflow.org/guide/migrate
"
50252,tf.data.experimental.service does not support from_generator,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Tensorflow docker image (tensorflow/tensorflow:2.5.0-gpu)
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Get error message and trace;
```
2021-06-14 09:56:03.953422: W tensorflow/core/framework/op_kernel.cc:1755] Invalid argument: ValueError: callback pyfunc_0 is not found

Traceback (most recent call last):

  File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/script_ops.py"", line 238, in __call__
    raise ValueError(""callback %s is not found"" % token)

ValueError: callback pyfunc_0 is not found
```
**Describe the expected behavior**
I expect the dataset to handle data from generators. 

In this instance, I would expect to get the same output as if `tf.data.experimental.service.distribute` was not applied to the dataset, or if `tf.data.Dataset.from_generator` was replaced with `tf.data.Dataset.range(5)`.

While the documentation states explicitly that `processing_mode=""distributed_epoch""` has limitations, this is not the case for  `processing_mode=""parallel_epochs""` where it is stated that you can [""Create the dataset however you were before using the tf.data service.""](https://www.tensorflow.org/api_docs/python/tf/data/experimental/service#using_the_tfdata_service_from_your_training_job)

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): no

**Standalone code to reproduce the issue**

minimal dataset:
```
    import tensorflow as tf

    ds = (
        tf.data.Dataset.from_generator(
            range,
            output_signature=(tf.TensorSpec(shape=(), dtype=tf.int32)),
            args=[5])
        .apply(tf.data.experimental.service.distribute(
            processing_mode=""parallel_epochs"",
            service='grpc://0.0.0.0:32121')
        )
    )

    for data in ds:
        print(data)
```
minimal worker code (run on same device):
```
import tensorflow as tf
dispatch_config = tf.data.experimental.service.DispatcherConfig(port=32121)
dispatcher = tf.data.experimental.service.DispatchServer(dispatch_config)
worker_config = tf.data.experimental.service.WorkerConfig(
        port=32122,
        dispatcher_address=""0.0.0.0:32121"",
        worker_address=""0.0.0.0:32122"",
)
worker = tf.data.experimental.service.WorkerServer(worker_config)
dispatcher.join()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50251,Use custom model_spec for object detection,"**System information**
- MacOS
- TensorFlow from COLAB


I would like to use TF Lite Model Maker for transfer learning using a different model from EfficentDet[0-4].

so, is possible to use a different model spec?
if yes, how can I get model_spec for different model like ssd mobile net?

here is the problem:
 spec = model_spec.get('efficientdet_lite0')
"
50250,The performance of the saved model and the performance of the trained model seem to be different ,"I use tensorflow2.4 and generate images by using the GAN method.

During the training stage, the generated model at one certain step is good. And then I save it by .tf. However, when I load the model again and plan to generate new images, these images are pretty terrible. (I have tried many times). This should not be a common issue for the GAN method. So could you please tell me the reason why there are huge differences after saving the model?"
50249,mixed keras import levels lead to untracked variables,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5
- Python version: Ubuntu 20.04.2 
- CUDA/cuDNN version: 11.2/8.1.0
- GPU model and memory: RTX 2080 TI

**Describe the current behavior**
Mixing layers (e.g. Input, Dense) which are imported via
`from tensorflow import keras`
`Input = keras.layers.Input()`
and 
`from keras.layers import Dense`
`Linear = Dense()`
leads to variables not being tracked
**Describe the expected behavior**
variables should be tracked

**Standalone code to reproduce the issue**
```
from tensorflow import keras
from keras.layers import Input, Dense


input1 = keras.layers.Input(shape=(2, 2))
input2 = Input(shape=(2, 2))
linear_layer = Dense(4)

y = linear_layer(input1)
model1 = keras.Model(input1, y)

y = linear_layer(input2)
model2 = keras.Model(input2, y)
print(model1.trainable_variables)
#outputs:
#[]
print(model2.trainable_variables)
#outputs:
#[<tf.Variable 'dense/kernel:0' shape=(2, 4) dtype=float32, numpy=
#array([[ 0.26048756,  0.08715534,  0.3931222 , -0.02270007],
#       [-0.17754197, -0.7752099 , -0.3470106 ,  0.85839653]],
#      dtype=float32)>, <tf.Variable 'dense/bias:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]
```
**Any Other info**
Tensorflow issues a warning related to Lambda layers: 
```WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.tensordot), but
are not present in its tracked objects:
  <tf.Variable 'dense/kernel:0' shape=(2, 4) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.'```
"
50248,Android studio:Error occurred when detecting the image," I had trained my custom model with different objects for the detection purpose. I had used ssd_mobilenet_v1_coco for my training. I could pull out the object detection on desktop with the customised trained model by using the webcam or passing an image. After converted my tf model to tflite model, adding metadata and quantized my model I obtained the following error when I run my application in Android Studio:

E/AndroidRuntime: FATAL EXCEPTION: inference
    Process: org.tensorflow.lite.examples.detection, PID: 28456
    java.lang.AssertionError: Error occurred when detecting the image: Label map does not contain enough elements: model returned class index 79 but label map only contains 2 elements.
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.detectNative(Native Method)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(ObjectDetector.java:421)
        at org.tensorflow.lite.task.vision.detector.ObjectDetector.detect(ObjectDetector.java:401)
        at org.tensorflow.lite.examples.detection.tflite.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:94)
        at org.tensorflow.lite.examples.detection.DetectorActivity$2.run(DetectorActivity.java:181)
        at android.os.Handler.handleCallback(Handler.java:938)
        at android.os.Handler.dispatchMessage(Handler.java:99)
        at android.os.Looper.loop(Looper.java:246)
        at android.os.HandlerThread.run(HandlerThread.java:67)
I/Process: Sending signal. PID: 28456 SIG: 9


My application detect the objects, but after approximately 1 minute, the application stop every time I run the program .
Has anyone encountered this error before ?
"
50247,Cannot save custom model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on a mobile device: -
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.5
- Bazel version (if compiling from source): -
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: -
- GPU model and memory: -


**Describe the current behaviour**
```bash
Traceback (most recent call last):
  File ""/usr/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/usr/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/root/rl-toolkit/rl_toolkit/__main__.py"", line 155, in <module>
    agent.save()
  File ""/root/rl-toolkit/rl_toolkit/policy/learner.py"", line 215, in save
    self.model.save(os.path.join(self._save_path, ""model""))
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py"", line 2111, in save
    save.save_model(self, filepath, overwrite, include_optimizer, save_format,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/save.py"", line 150, in save_model
    saved_model_save.save(model, filepath, overwrite, include_optimizer,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/save.py"", line 89, in save
    saved_nodes, node_paths = save_lib.save_and_return_nodes(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"", line 1103, in save_and_return_nodes
    _build_meta_graph(obj, signatures, options, meta_graph_def,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"", line 1290, in _build_meta_graph
    return _build_meta_graph_impl(obj, signatures, options, meta_graph_def,
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"", line 1207, in _build_meta_graph_impl
    signatures = signature_serialization.find_function_to_export(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/signature_serialization.py"", line 99, in find_function_to_export
    functions = saveable_view.list_functions(saveable_view.root)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/save.py"", line 154, in list_functions
    obj_functions = obj._list_functions_for_serialization(  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/training.py"", line 2713, in _list_functions_for_serialization
    functions = super(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer.py"", line 3016, in _list_functions_for_serialization
    return (self._trackable_saved_model_saver
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/base_serialization.py"", line 92, in list_functions_for_serialization
    fns = self.functions_to_serialize(serialization_cache)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 73, in functions_to_serialize
    return (self._get_serialized_attributes(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/layer_serialization.py"", line 89, in _get_serialized_attributes
    object_dict, function_dict = self._get_serialized_attributes_internal(
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/model_serialization.py"", line 48, in _get_serialized_attributes_internal
    default_signature = save_impl.default_save_signature(self.obj)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saved_model/save_impl.py"", line 215, in default_save_signature
    fn = saving_utils.trace_model_call(layer)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saving_utils.py"", line 119, in trace_model_call
    raise_model_input_error(model)
  File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/saving/saving_utils.py"", line 90, in raise_model_input_error
    raise ValueError(
ValueError: Model <rl_toolkit.networks.actor_critic.ActorCritic object at 0x7f6d059b2ee0> cannot be saved because the input shapes have not been set. Usually, input shapes are automatically determined from calling `.fit()` or `.predict()`. To manually set the shapes, call `model.build(input_shape)`.
```

But before I'm saving the model, I ran:
```python
# Actor network (for learner)
self.model = ActorCritic(
    num_of_outputs=tf.reduce_prod(self._env.action_space.shape),
    gamma=gamma,
    tau=tau,
)
self.model.build((None,) + self._env.observation_space.shape)
self.model.compile(
    actor_optimizer=Adam(learning_rate=actor_learning_rate),
    critic_optimizer=Adam(learning_rate=critic_learning_rate),
    alpha_optimizer=Adam(learning_rate=alpha_learning_rate),
)

self.model.summary()
```

The result of `summary()`:
```bash
Model: ""actor_critic""
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
actor (Actor)                multiple                  139108
_________________________________________________________________
multi_critic (MultiCritic)   multiple                  270802
_________________________________________________________________
multi_critic_1 (MultiCritic) multiple                  270802
=================================================================
Total params: 680,714
Trainable params: 678,313
Non-trainable params: 2,401
_________________________________________________________________
```

Definition of custom model:
```python
class ActorCritic(Model):
    def __init__(self, num_of_outputs: int, gamma: float, tau: float, **kwargs):
        super(ActorCritic, self).__init__(**kwargs)

        self.gamma = tf.constant(gamma)
        self.tau = tf.constant(tau)

        # init param 'alpha' - Lagrangian constraint
        self.log_alpha = tf.Variable(0.0, trainable=True, name=""log_alpha"")
        self.alpha = tf.Variable(0.0, trainable=False, name=""alpha"")
        self.target_entropy = tf.cast(-num_of_outputs, dtype=tf.float32)

        # Actor
        self.actor = Actor(num_of_outputs)

        # Critic
        self.critic = MultiCritic(2)
        self.critic_target = MultiCritic(2)
        self._train_target(self.critic, self.critic_target, tau=tf.constant(1.0))

    def train_step(self, data):
        with tf.GradientTape(persistent=True) as tape:

        # ....

        return {
            ""actor_loss"": actor_loss,
            ""critic_loss"": Q_loss,
            ""alpha_loss"": alpha_loss,
        }

    def _train_target(self, source, target, tau):
        for source_weight, target_weight in zip(
            source.trainable_variables, target.trainable_variables
        ):
            target_weight.assign(tau * source_weight + (1.0 - tau) * target_weight)

    def call(self, inputs):
        action, log_pi = self.actor(inputs, with_log_prob=True)
        Q_value = tf.reduce_min(self.critic([inputs, action]), axis=1)
        Q_value_target = tf.reduce_min(self.critic_target([inputs, action]), axis=1)
        return [Q_value, Q_value_target, action, log_pi]

    def compile(self, actor_optimizer, critic_optimizer, alpha_optimizer):
        super(ActorCritic, self).compile()
        self.actor_optimizer = actor_optimizer
        self.critic_optimizer = critic_optimizer
        self.alpha_optimizer = alpha_optimizer
```

How can I use `self.model.save()` correctly on a custom model?
Thanks."
50246,Inconsistent gather_nd behavior on empty tensors,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): N/A
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.9.4
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
Using tf.gather_nd with empty tensors raises an invalid argument error, but works if tensors are not empty. tf.gather does not have this problem.

For example, this raises ""Invalid argument: Requested more than 0 entries, but params is empty.""
```python
tf.gather_nd(tf.zeros([2, 0, 5]), [[0]])
```

This alternative using tf.gather should be equivalent, but does not raise any error.
It returns an empty tensor of the right shape (1, 0, 5).
```python
tf.gather(tf.zeros([2, 0, 5]), [0])
```

The tf.gather_nd example works fine if the tensor is not empty. This returns a tensor of shape (1, 1, 5).
```python
tf.gather_nd(tf.zeros([2, 1, 5]), [[0]])
```

**Describe the expected behavior**
tf.gather_nd should be returning empty tensors of the appropriate shape just as tf.gather does. Note that while in this example tf.gather is an alternative, this is not always the case. For example, gathering based on multi-dimensional indices would force using tf.gather_nd, and any code that might face empty tensors now needs to handle these as corner cases.

**Standalone code to reproduce the issue**
The single line examples above are enough to reproduce the issue."
50244,Weird loss-inconsistencies and worse accuracy compared to older versions,"
**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Windows 10
- Mobile device: -
- TensorFlow installed from: pip install tensorflow-gpu
- TensorFlow version: v2.4.0-49-g85c8b2a817f 2.4.1
- Python version: 3.7.9
- Bazel version: -
- GCC/Compiler version: -
- CUDA/cuDNN version: 11.3
- GPU model and memory: Nvidia GTX 1060


I am working with ResNet50 from keras and am currently unable to achieve the same results I do with TF 1.13.1 in the newer TF 2.4.1.
Firstly the weights in ResNet50 have a very different range of values compared to TF 1.13.1.
In the old version for example one layer (i.e. model.layers[-6]) with 512*2048 weights has values between -0.15 and +0.15 while TF 2.4.1 has only values between -0.05 and +0.05. I also let the he_normal initializer generate 100 random numbers in TF 1.13.1 and TF 2.4.1 each and the old version also produces a range of values that is 3 times higher than the new version's one.
**Why did that change?**

I now tried to save the weights in TF 1.13.1 and import them in TF 2.4.1 to see if this is the breaking change that makes my accuracy and training progress worse.
However when I evaluate the untrained, randomly initialized model I get weird CategoricalCrossentropy loss values from model.evaluate.



On TF 1.13.1 I executed the following code to generate the saved weights file:
```
model = keras.applications.resnet50.ResNet50(weights=None, include_top=True, classes=5)
model.compile(loss=""categorical_crossentropy"", metrics=['accuracy','categorical_crossentropy'])
model.save(""TF-1-13-1before_training"")
```

On TF 2.4.1 I loaded a custom dataset and the model:
```
model = keras.models.load_model(""TF-1-13-1before_training"")
print(x_train.shape)  # (10, 224, 224, 3) 10 color-images, 2 for each of the 5 classes 224x224
print(y_train.shape)  # (10, 5) 10 times a one-hot encoded vector for 5 classes

predictions = model.predict(x_train)
cce = keras.losses.CategoricalCrossentropy()

cce(y_train, predictions).eval(session=tf.compat.v1.Session())  # Yields 12.894476
model.evaluate(x=x_train, y=y_train)  # Yields [463.46978759765625, 0.2, 463.4698]
model.metrics_names  # gives ['loss', 'acc', 'categorical_crossentropy']
```
```
print(y_train)
[[1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 1. 0. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 1. 0. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 1. 0.]
 [0. 0. 0. 0. 1.]
 [0. 0. 0. 0. 1.]]
```
```
print(predictions)
[[1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0.]]
```

**Why does model.evaluate return different categorical_crossentropy loss values than the standalone CCE function for the same data? How can the high value of ~463 be explained in contrast to the expected value of 12.89?**
The standalone CategoricalCrossentropy function applied on predictions and target values seems correct, the return value of model.evaluate on the other hand does not.

Thank you in advance for your help"
50243,`tf.TensorArray`  can't work in `tf.while_loop`,"**Describe the current behavior**
I tried to implement a model like an auto-regressive model which can recurrently generate samples, one of the ways is using `tf.TensorArray` and `tf.while_loop`, but I found it would go wrong. [Here](https://colab.research.google.com/gist/gdhy9064/9b59b7f9097b506446d68f1b656316db/test.ipynb) is a gist for reference. I have simplified the source, this gist is just for illustration.


**Describe the expected behavior**
I can use another way with `tf.keras.backend.rnn` to reach my goal, but I just wonder what the wrong is in this way. 

"
50239,How to convert .pkl file to tflite model,
50238,"Tensorflow object detection api Windows fatal exception: access violation, step stayed at 0","Trying to train custom object detection model following this tutorial:
https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/index.html
I have also tested a different dataset with different model from here:
https://github.com/TannerGilbert/Tensorflow-Object-Detection-API-Train-Model
but both return the same problem. 
All my file structures follows the first tutorial. 


OS: Windows 10
CUDA: 11.3
Tensorflow: 2.5.0
Python: 3.9

Log:
```
2021-06-12 20:17:32.813643: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-12 20:17:35.974727: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library nvcuda.dll
2021-06-12 20:17:36.003520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1650 with Max-Q Design computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2021-06-12 20:17:36.003577: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudart64_110.dll
2021-06-12 20:17:36.034868: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-12 20:17:36.034898: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-06-12 20:17:36.051579: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cufft64_10.dll
2021-06-12 20:17:36.055829: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library curand64_10.dll
2021-06-12 20:17:36.073808: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusolver64_11.dll
2021-06-12 20:17:36.077387: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cusparse64_11.dll
2021-06-12 20:17:36.078442: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-12 20:17:36.078540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-12 20:17:36.079006: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-12 20:17:36.106947: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce GTX 1650 with Max-Q Design computeCapability: 7.5
coreClock: 1.245GHz coreCount: 16 deviceMemorySize: 4.00GiB deviceMemoryBandwidth: 104.34GiB/s
2021-06-12 20:17:36.107008: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0
2021-06-12 20:17:36.619265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-12 20:17:36.619301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 
2021-06-12 20:17:36.619309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N 
2021-06-12 20:17:36.619452: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2145 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce GTX 1650 with Max-Q Design, pci bus id: 0000:02:00.0, compute capability: 7.5)
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
W0612 20:17:36.634464 11076 mirrored_strategy.py:379] Collective ops is not configured at program startup. Some performance features may not be enabled.
INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
I0612 20:17:36.728192 11076 mirrored_strategy.py:369] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)
INFO:tensorflow:Maybe overwriting train_steps: None
I0612 20:17:36.728192 11076 config_util.py:552] Maybe overwriting train_steps: None
INFO:tensorflow:Maybe overwriting use_bfloat16: False
I0612 20:17:36.728192 11076 config_util.py:552] Maybe overwriting use_bfloat16: False
WARNING:tensorflow:From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
W0612 20:17:36.744960 11076 deprecation.py:330] From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py:557: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.
Instructions for updating:
rename to distribute_datasets_from_function
INFO:tensorflow:Reading unweighted datasets: ['annotations/train.record']
I0612 20:17:36.760623 11076 dataset_builder.py:163] Reading unweighted datasets: ['annotations/train.record']
INFO:tensorflow:Reading record datasets for input file: ['annotations/train.record']
I0612 20:17:36.760623 11076 dataset_builder.py:80] Reading record datasets for input file: ['annotations/train.record']
INFO:tensorflow:Number of filenames to read: 1
I0612 20:17:36.760623 11076 dataset_builder.py:81] Number of filenames to read: 1
WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.
W0612 20:17:36.760623 11076 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.
WARNING:tensorflow:From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
W0612 20:17:36.760623 11076 deprecation.py:330] From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:101: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministic`.
WARNING:tensorflow:From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
W0612 20:17:36.776248 11076 deprecation.py:330] From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\builders\dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.map()
WARNING:tensorflow:From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
W0612 20:17:42.585761 11076 deprecation.py:330] From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:206: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.
WARNING:tensorflow:From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
W0612 20:17:45.209203 11076 deprecation.py:330] From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\util\dispatch.py:206: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.
Instructions for updating:
`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.
WARNING:tensorflow:From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\autograph\impl\api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
W0612 20:17:46.765527 11076 deprecation.py:330] From C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\autograph\impl\api.py:464: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.cast` instead.
2021-06-12 20:17:49.246970: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\keras\backend.py:435: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.
  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '
2021-06-12 20:18:20.053992: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cudnn64_8.dll
2021-06-12 20:18:20.578938: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8201
2021-06-12 20:18:21.387817: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 934.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:21.387940: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 934.75MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:21.605687: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublas64_11.dll
2021-06-12 20:18:22.316327: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library cublasLt64_11.dll
2021-06-12 20:18:22.544715: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:22.544804: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:22.680473: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 916.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:22.680578: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 916.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:22.680686: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:22.680753: W tensorflow/core/common_runtime/bfc_allocator.cc:271] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.54GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-06-12 20:18:33.129542: W tensorflow/core/common_runtime/bfc_allocator.cc:456] Allocator (GPU_0_bfc) ran out of memory trying to allocate 400.00MiB (rounded to 419430400)requested by op ResNet50V1_FPN/model/conv2_block2_3_bn/FusedBatchNormV3
If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. 
Current allocation summary follows.
Current allocation summary follows.
2021-06-12 20:18:33.129852: I tensorflow/core/common_runtime/bfc_allocator.cc:991] BFCAllocator dump for GPU_0_bfc
2021-06-12 20:18:33.130006: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (256): 	Total Chunks: 77, Chunks in use: 77. 19.2KiB allocated for chunks. 19.2KiB in use in bin. 8.7KiB client-requested in use in bin.
2021-06-12 20:18:33.130118: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (512): 	Total Chunks: 94, Chunks in use: 81. 47.0KiB allocated for chunks. 40.5KiB in use in bin. 35.3KiB client-requested in use in bin.
2021-06-12 20:18:33.130229: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (1024): 	Total Chunks: 263, Chunks in use: 261. 276.2KiB allocated for chunks. 274.0KiB in use in bin. 271.0KiB client-requested in use in bin.
2021-06-12 20:18:33.130329: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (2048): 	Total Chunks: 65, Chunks in use: 61. 134.5KiB allocated for chunks. 123.2KiB in use in bin. 121.5KiB client-requested in use in bin.
2021-06-12 20:18:33.130427: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (4096): 	Total Chunks: 32, Chunks in use: 31. 135.5KiB allocated for chunks. 130.8KiB in use in bin. 130.8KiB client-requested in use in bin.
2021-06-12 20:18:33.130529: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (8192): 	Total Chunks: 16, Chunks in use: 16. 134.0KiB allocated for chunks. 134.0KiB in use in bin. 128.0KiB client-requested in use in bin.
2021-06-12 20:18:33.130631: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (16384): 	Total Chunks: 5, Chunks in use: 4. 131.0KiB allocated for chunks. 103.0KiB in use in bin. 91.0KiB client-requested in use in bin.
2021-06-12 20:18:33.130732: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (32768): 	Total Chunks: 1, Chunks in use: 1. 36.8KiB allocated for chunks. 36.8KiB in use in bin. 36.8KiB client-requested in use in bin.
2021-06-12 20:18:33.130829: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (65536): 	Total Chunks: 8, Chunks in use: 6. 561.0KiB allocated for chunks. 416.0KiB in use in bin. 384.0KiB client-requested in use in bin.
2021-06-12 20:18:33.130929: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (131072): 	Total Chunks: 7, Chunks in use: 7. 1.13MiB allocated for chunks. 1.13MiB in use in bin. 1.13MiB client-requested in use in bin.
2021-06-12 20:18:33.131026: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (262144): 	Total Chunks: 11, Chunks in use: 9. 3.10MiB allocated for chunks. 2.44MiB in use in bin. 2.16MiB client-requested in use in bin.
2021-06-12 20:18:33.131123: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (524288): 	Total Chunks: 11, Chunks in use: 10. 6.41MiB allocated for chunks. 5.91MiB in use in bin. 5.57MiB client-requested in use in bin.
2021-06-12 20:18:33.131222: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (1048576): 	Total Chunks: 13, Chunks in use: 12. 14.22MiB allocated for chunks. 13.00MiB in use in bin. 12.00MiB client-requested in use in bin.
2021-06-12 20:18:33.131321: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (2097152): 	Total Chunks: 24, Chunks in use: 21. 55.75MiB allocated for chunks. 48.75MiB in use in bin. 46.50MiB client-requested in use in bin.
2021-06-12 20:18:33.131698: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (4194304): 	Total Chunks: 21, Chunks in use: 21. 98.05MiB allocated for chunks. 98.05MiB in use in bin. 98.05MiB client-requested in use in bin.
2021-06-12 20:18:33.131835: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (8388608): 	Total Chunks: 5, Chunks in use: 5. 50.61MiB allocated for chunks. 50.61MiB in use in bin. 50.61MiB client-requested in use in bin.
2021-06-12 20:18:33.131939: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (16777216): 	Total Chunks: 18, Chunks in use: 18. 348.91MiB allocated for chunks. 348.91MiB in use in bin. 344.19MiB client-requested in use in bin.
2021-06-12 20:18:33.132030: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (33554432): 	Total Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-06-12 20:18:33.132129: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (67108864): 	Total Chunks: 3, Chunks in use: 1. 270.09MiB allocated for chunks. 75.00MiB in use in bin. 75.00MiB client-requested in use in bin.
2021-06-12 20:18:33.132223: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (134217728): 	Total Chunks: 1, Chunks in use: 0. 136.58MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.
2021-06-12 20:18:33.132325: I tensorflow/core/common_runtime/bfc_allocator.cc:998] Bin (268435456): 	Total Chunks: 3, Chunks in use: 2. 1.13GiB allocated for chunks. 821.95MiB in use in bin. 800.00MiB client-requested in use in bin.
2021-06-12 20:18:33.132408: I tensorflow/core/common_runtime/bfc_allocator.cc:1014] Bin for 400.00MiB was 256.00MiB, Chunk State: 
2021-06-12 20:18:33.132526: I tensorflow/core/common_runtime/bfc_allocator.cc:1020]   Size: 337.16MiB | Requested Size: 19.51MiB | in_use: 0 | bin_num: 20, prev:   Size: 4.88MiB | Requested Size: 4.88MiB | in_use: 1 | bin_num: -1, for: Loss/Compare_10/IOU/Equal, stepid: 16762585343613708217, last_action: 126448115942167, for: UNUSED, stepid: 16762585343613708217, last_action: 126448115942071
2021-06-12 20:18:33.132604: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] Next region of size 2249614848
2021-06-12 20:18:33.132724: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000000 of size 1280 by op ScratchBuffer action_count 126448115939866 step 0 next 1
2021-06-12 20:18:33.132802: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000500 of size 256 by op AssignVariableOp action_count 126448115939867 step 0 next 2
2021-06-12 20:18:33.132875: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000600 of size 256 by op AssignVariableOp action_count 126448115939868 step 0 next 3
2021-06-12 20:18:33.132945: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000700 of size 256 by op Mul action_count 126448115939941 step 0 next 4
2021-06-12 20:18:33.133013: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000800 of size 256 by op SameWorkerRecvDone action_count 126448115939890 step 0 next 5
2021-06-12 20:18:33.133081: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0b000900 of size 78643200 by op SameWorkerRecvDone action_count 126448115939871 step 0 next 6
2021-06-12 20:18:33.133147: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb00900 of size 256 by op SameWorkerRecvDone action_count 126448115939892 step 0 next 7
2021-06-12 20:18:33.133212: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb00a00 of size 256 by op SameWorkerRecvDone action_count 126448115939893 step 0 next 8
2021-06-12 20:18:33.133278: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb00b00 of size 6400 by op SameWorkerRecvDone action_count 126448115939874 step 0 next 9
2021-06-12 20:18:33.133346: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb02400 of size 25600 by op SameWorkerRecvDone action_count 126448115939875 step 0 next 10
2021-06-12 20:18:33.133427: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb08800 of size 25600 by op SameWorkerRecvDone action_count 126448115939876 step 0 next 11
2021-06-12 20:18:33.133505: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb0ec00 of size 25600 by op SameWorkerRecvDone action_count 126448115939877 step 0 next 12
2021-06-12 20:18:33.133577: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb15000 of size 6400 by op SameWorkerRecvDone action_count 126448115939889 step 0 next 13
2021-06-12 20:18:33.133647: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb16900 of size 1792 by op SameWorkerRecvDone action_count 126448115939879 step 0 next 14
2021-06-12 20:18:33.133744: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17000 of size 1792 by op SameWorkerRecvDone action_count 126448115939880 step 0 next 15
2021-06-12 20:18:33.133813: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17700 of size 256 by op SameWorkerRecvDone action_count 126448115939881 step 0 next 16
2021-06-12 20:18:33.133879: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17800 of size 256 by op SameWorkerRecvDone action_count 126448115939882 step 0 next 17
2021-06-12 20:18:33.133947: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb17900 of size 6400 by op SameWorkerRecvDone action_count 126448115939883 step 0 next 18
2021-06-12 20:18:33.134016: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19200 of size 256 by op SameWorkerRecvDone action_count 126448115939894 step 0 next 19
2021-06-12 20:18:33.134086: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19300 of size 256 by op Fill action_count 126448115939947 step 0 next 33
2021-06-12 20:18:33.134155: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19400 of size 256 by op Fill action_count 126448115939948 step 0 next 30
2021-06-12 20:18:33.134221: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19500 of size 256 by op Fill action_count 126448115939949 step 0 next 29
2021-06-12 20:18:33.134286: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19600 of size 256 by op Fill action_count 126448115939950 step 0 next 28
2021-06-12 20:18:33.134350: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19700 of size 256 by op Fill action_count 126448115939951 step 0 next 27
2021-06-12 20:18:33.134414: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19800 of size 256 by op Fill action_count 126448115939952 step 0 next 26
2021-06-12 20:18:33.134485: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19900 of size 1024 by op Fill action_count 126448115940185 step 0 next 167
2021-06-12 20:18:33.134553: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb19d00 of size 1024 by op Fill action_count 126448115940186 step 0 next 168
2021-06-12 20:18:33.134607: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1a100 of size 4096 by op Fill action_count 126448115940192 step 0 next 169
2021-06-12 20:18:33.134616: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1b100 of size 4096 by op Fill action_count 126448115940193 step 0 next 171
2021-06-12 20:18:33.134624: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1c100 of size 4096 by op Fill action_count 126448115940194 step 0 next 172
2021-06-12 20:18:33.134632: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1d100 of size 4096 by op Fill action_count 126448115940195 step 0 next 173
2021-06-12 20:18:33.134639: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1e100 of size 1024 by op Fill action_count 126448115940201 step 0 next 174
2021-06-12 20:18:33.134648: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1e500 of size 1024 by op Fill action_count 126448115940202 step 0 next 176
2021-06-12 20:18:33.134657: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1e900 of size 1024 by op Fill action_count 126448115940203 step 0 next 177
2021-06-12 20:18:33.134666: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1ed00 of size 1024 by op Fill action_count 126448115940204 step 0 next 178
2021-06-12 20:18:33.134675: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1f100 of size 1024 by op Fill action_count 126448115940210 step 0 next 179
2021-06-12 20:18:33.134683: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1f500 of size 1024 by op Fill action_count 126448115940211 step 0 next 182
2021-06-12 20:18:33.134691: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1f900 of size 1024 by op Fill action_count 126448115940212 step 0 next 183
2021-06-12 20:18:33.134699: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb1fd00 of size 1024 by op Fill action_count 126448115940213 step 0 next 184
2021-06-12 20:18:33.134707: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb20100 of size 4096 by op Fill action_count 126448115940219 step 0 next 186
2021-06-12 20:18:33.134715: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb21100 of size 4096 by op Fill action_count 126448115940220 step 0 next 187
2021-06-12 20:18:33.134723: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb22100 of size 4096 by op Fill action_count 126448115940221 step 0 next 188
2021-06-12 20:18:33.134731: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb23100 of size 4096 by op Fill action_count 126448115940222 step 0 next 189
2021-06-12 20:18:33.134740: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24100 of size 1024 by op Fill action_count 126448115940228 step 0 next 191
2021-06-12 20:18:33.134748: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24500 of size 1024 by op Fill action_count 126448115940229 step 0 next 192
2021-06-12 20:18:33.134756: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24900 of size 1024 by op Fill action_count 126448115940230 step 0 next 193
2021-06-12 20:18:33.134763: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb24d00 of size 1024 by op Fill action_count 126448115940231 step 0 next 194
2021-06-12 20:18:33.134771: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25100 of size 1024 by op Fill action_count 126448115940237 step 0 next 197
2021-06-12 20:18:33.134779: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25500 of size 1024 by op Fill action_count 126448115940238 step 0 next 198
2021-06-12 20:18:33.134788: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25900 of size 1024 by op Fill action_count 126448115940239 step 0 next 199
2021-06-12 20:18:33.134796: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb25d00 of size 1024 by op Fill action_count 126448115940240 step 0 next 200
2021-06-12 20:18:33.134804: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb26100 of size 4096 by op Fill action_count 126448115940246 step 0 next 202
2021-06-12 20:18:33.134812: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb27100 of size 4096 by op Fill action_count 126448115940247 step 0 next 203
2021-06-12 20:18:33.134820: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb28100 of size 4096 by op Fill action_count 126448115940248 step 0 next 204
2021-06-12 20:18:33.134828: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb29100 of size 4096 by op Fill action_count 126448115940249 step 0 next 205
2021-06-12 20:18:33.134836: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2a100 of size 1024 by op Fill action_count 126448115940255 step 0 next 206
2021-06-12 20:18:33.134844: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2a500 of size 1024 by op Fill action_count 126448115940256 step 0 next 207
2021-06-12 20:18:33.134853: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2a900 of size 1024 by op Fill action_count 126448115940257 step 0 next 208
2021-06-12 20:18:33.134862: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2ad00 of size 1024 by op Fill action_count 126448115940258 step 0 next 209
2021-06-12 20:18:33.134871: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2b100 of size 1024 by op Fill action_count 126448115940264 step 0 next 212
2021-06-12 20:18:33.134879: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2b500 of size 1024 by op Fill action_count 126448115940265 step 0 next 34
2021-06-12 20:18:33.134887: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2b900 of size 256 by op Add action_count 126448115939943 step 0 next 32
2021-06-12 20:18:33.134895: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb2ba00 of size 37632 by op Add action_count 126448115939944 step 0 next 31
2021-06-12 20:18:33.134903: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb34d00 of size 1024 by op Fill action_count 126448115939958 step 0 next 23
2021-06-12 20:18:33.134911: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35100 of size 1024 by op Fill action_count 126448115939959 step 0 next 22
2021-06-12 20:18:33.134919: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35500 of size 1024 by op Fill action_count 126448115939960 step 0 next 21
2021-06-12 20:18:33.134927: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35900 of size 1024 by op Fill action_count 126448115939961 step 0 next 20
2021-06-12 20:18:33.134935: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35d00 of size 256 by op Fill action_count 126448115939967 step 0 next 35
2021-06-12 20:18:33.134943: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35e00 of size 256 by op Fill action_count 126448115939968 step 0 next 37
2021-06-12 20:18:33.134951: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb35f00 of size 256 by op Fill action_count 126448115939969 step 0 next 38
2021-06-12 20:18:33.134959: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36000 of size 256 by op Fill action_count 126448115939970 step 0 next 39
2021-06-12 20:18:33.134967: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36100 of size 256 by op Fill action_count 126448115939976 step 0 next 40
2021-06-12 20:18:33.134975: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36200 of size 256 by op Fill action_count 126448115939977 step 0 next 43
2021-06-12 20:18:33.134983: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36300 of size 256 by op Fill action_count 126448115939978 step 0 next 44
2021-06-12 20:18:33.134991: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36400 of size 256 by op Fill action_count 126448115939979 step 0 next 45
2021-06-12 20:18:33.135000: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36500 of size 1024 by op Fill action_count 126448115939985 step 0 next 48
2021-06-12 20:18:33.135008: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36900 of size 1024 by op Fill action_count 126448115939986 step 0 next 49
2021-06-12 20:18:33.135016: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb36d00 of size 1024 by op Fill action_count 126448115939987 step 0 next 50
2021-06-12 20:18:33.135024: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37100 of size 1024 by op Fill action_count 126448115939988 step 0 next 51
2021-06-12 20:18:33.135032: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37500 of size 256 by op Fill action_count 126448115939994 step 0 next 53
2021-06-12 20:18:33.135040: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37600 of size 256 by op Fill action_count 126448115939995 step 0 next 54
2021-06-12 20:18:33.135076: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37700 of size 256 by op Fill action_count 126448115939996 step 0 next 55
2021-06-12 20:18:33.135088: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37800 of size 256 by op Fill action_count 126448115939997 step 0 next 56
2021-06-12 20:18:33.135096: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37900 of size 256 by op Fill action_count 126448115940003 step 0 next 57
2021-06-12 20:18:33.135105: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37a00 of size 256 by op Fill action_count 126448115940004 step 0 next 60
2021-06-12 20:18:33.135113: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37b00 of size 256 by op Fill action_count 126448115940005 step 0 next 61
2021-06-12 20:18:33.135121: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37c00 of size 256 by op Fill action_count 126448115940006 step 0 next 62
2021-06-12 20:18:33.135129: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb37d00 of size 1024 by op Fill action_count 126448115940012 step 0 next 63
2021-06-12 20:18:33.135137: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38100 of size 1024 by op Fill action_count 126448115940013 step 0 next 64
2021-06-12 20:18:33.135146: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38500 of size 1024 by op Fill action_count 126448115940014 step 0 next 65
2021-06-12 20:18:33.135154: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38900 of size 1024 by op Fill action_count 126448115940015 step 0 next 66
2021-06-12 20:18:33.135162: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38d00 of size 256 by op Fill action_count 126448115940021 step 0 next 68
2021-06-12 20:18:33.135170: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38e00 of size 256 by op Fill action_count 126448115940022 step 0 next 69
2021-06-12 20:18:33.135178: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb38f00 of size 256 by op Fill action_count 126448115940023 step 0 next 70
2021-06-12 20:18:33.135186: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39000 of size 256 by op Fill action_count 126448115940024 step 0 next 71
2021-06-12 20:18:33.135194: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39100 of size 256 by op Fill action_count 126448115940030 step 0 next 74
2021-06-12 20:18:33.135202: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39200 of size 256 by op Fill action_count 126448115940031 step 0 next 75
2021-06-12 20:18:33.135210: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39300 of size 256 by op Fill action_count 126448115940032 step 0 next 76
2021-06-12 20:18:33.135218: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39400 of size 256 by op Fill action_count 126448115940033 step 0 next 77
2021-06-12 20:18:33.135227: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39500 of size 1024 by op Fill action_count 126448115940039 step 0 next 79
2021-06-12 20:18:33.135235: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39900 of size 1024 by op Fill action_count 126448115940040 step 0 next 80
2021-06-12 20:18:33.135243: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb39d00 of size 1024 by op Fill action_count 126448115940041 step 0 next 81
2021-06-12 20:18:33.135251: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3a100 of size 1024 by op Fill action_count 126448115940042 step 0 next 82
2021-06-12 20:18:33.135259: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3a500 of size 2048 by op Fill action_count 126448115940048 step 0 next 83
2021-06-12 20:18:33.135268: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3ad00 of size 2048 by op Fill action_count 126448115940049 step 0 next 86
2021-06-12 20:18:33.135306: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3b500 of size 2048 by op Fill action_count 126448115940050 step 0 next 87
2021-06-12 20:18:33.135317: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3bd00 of size 2048 by op Fill action_count 126448115940051 step 0 next 88
2021-06-12 20:18:33.135326: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3c500 of size 512 by op Fill action_count 126448115940057 step 0 next 91
2021-06-12 20:18:33.135334: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3c700 of size 512 by op Fill action_count 126448115940058 step 0 next 92
2021-06-12 20:18:33.135342: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3c900 of size 512 by op Fill action_count 126448115940059 step 0 next 93
2021-06-12 20:18:33.135350: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3cb00 of size 512 by op Fill action_count 126448115940060 step 0 next 94
2021-06-12 20:18:33.135358: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3cd00 of size 512 by op Fill action_count 126448115940066 step 0 next 97
2021-06-12 20:18:33.135367: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3cf00 of size 512 by op Fill action_count 126448115940067 step 0 next 98
2021-06-12 20:18:33.135375: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3d100 of size 512 by op Fill action_count 126448115940068 step 0 next 99
2021-06-12 20:18:33.135383: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3d300 of size 512 by op Fill action_count 126448115940069 step 0 next 100
2021-06-12 20:18:33.135391: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3d500 of size 2048 by op Fill action_count 126448115940075 step 0 next 36
2021-06-12 20:18:33.135400: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb3dd00 of size 28672 by op Add action_count 126448115939964 step 0 next 25
2021-06-12 20:18:33.135408: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb44d00 of size 65536 by op Add action_count 126448115939955 step 0 next 24
2021-06-12 20:18:33.135416: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb54d00 of size 2048 by op Fill action_count 126448115940076 step 0 next 101
2021-06-12 20:18:33.135424: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb55500 of size 2048 by op Fill action_count 126448115940077 step 0 next 103
2021-06-12 20:18:33.135432: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb55d00 of size 2048 by op Fill action_count 126448115940078 step 0 next 104
2021-06-12 20:18:33.135441: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56500 of size 512 by op Fill action_count 126448115940084 step 0 next 105
2021-06-12 20:18:33.135449: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56700 of size 512 by op Fill action_count 126448115940085 step 0 next 107
2021-06-12 20:18:33.135457: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56900 of size 512 by op Fill action_count 126448115940086 step 0 next 108
2021-06-12 20:18:33.135465: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56b00 of size 512 by op Fill action_count 126448115940087 step 0 next 109
2021-06-12 20:18:33.135473: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56d00 of size 512 by op Fill action_count 126448115940093 step 0 next 110
2021-06-12 20:18:33.135481: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb56f00 of size 512 by op Fill action_count 126448115940094 step 0 next 113
2021-06-12 20:18:33.135489: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57100 of size 512 by op Fill action_count 126448115940095 step 0 next 114
2021-06-12 20:18:33.135497: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57300 of size 512 by op Fill action_count 126448115940096 step 0 next 115
2021-06-12 20:18:33.135507: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57500 of size 2048 by op Fill action_count 126448115940102 step 0 next 117
2021-06-12 20:18:33.135516: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb57d00 of size 2048 by op Fill action_count 126448115940103 step 0 next 118
2021-06-12 20:18:33.135525: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb58500 of size 2048 by op Fill action_count 126448115940104 step 0 next 119
2021-06-12 20:18:33.135533: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb58d00 of size 2048 by op Fill action_count 126448115940105 step 0 next 120
2021-06-12 20:18:33.135541: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59500 of size 512 by op Fill action_count 126448115940111 step 0 next 122
2021-06-12 20:18:33.135549: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59700 of size 512 by op Fill action_count 126448115940112 step 0 next 123
2021-06-12 20:18:33.135557: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59900 of size 512 by op Fill action_count 126448115940113 step 0 next 124
2021-06-12 20:18:33.135565: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59b00 of size 512 by op Fill action_count 126448115940114 step 0 next 125
2021-06-12 20:18:33.135573: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59d00 of size 512 by op Fill action_count 126448115940120 step 0 next 128
2021-06-12 20:18:33.135582: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb59f00 of size 512 by op Fill action_count 126448115940121 step 0 next 129
2021-06-12 20:18:33.135590: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5a100 of size 512 by op Fill action_count 126448115940122 step 0 next 130
2021-06-12 20:18:33.135598: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5a300 of size 512 by op Fill action_count 126448115940123 step 0 next 131
2021-06-12 20:18:33.135606: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5a500 of size 2048 by op Fill action_count 126448115940129 step 0 next 133
2021-06-12 20:18:33.135614: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5ad00 of size 2048 by op Fill action_count 126448115940130 step 0 next 134
2021-06-12 20:18:33.135622: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5b500 of size 2048 by op Fill action_count 126448115940131 step 0 next 135
2021-06-12 20:18:33.135630: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5bd00 of size 2048 by op Fill action_count 126448115940132 step 0 next 136
2021-06-12 20:18:33.135639: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5c500 of size 512 by op Fill action_count 126448115940138 step 0 next 137
2021-06-12 20:18:33.135647: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5c700 of size 512 by op Fill action_count 126448115940139 step 0 next 138
2021-06-12 20:18:33.135655: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5c900 of size 512 by op Fill action_count 126448115940140 step 0 next 139
2021-06-12 20:18:33.135662: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5cb00 of size 512 by op Fill action_count 126448115940141 step 0 next 140
2021-06-12 20:18:33.135670: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5cd00 of size 512 by op Fill action_count 126448115940147 step 0 next 143
2021-06-12 20:18:33.135678: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5cf00 of size 512 by op Fill action_count 126448115940148 step 0 next 144
2021-06-12 20:18:33.135686: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5d100 of size 512 by op Fill action_count 126448115940149 step 0 next 145
2021-06-12 20:18:33.135695: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5d300 of size 512 by op Fill action_count 126448115940150 step 0 next 146
2021-06-12 20:18:33.135704: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5d500 of size 2048 by op Fill action_count 126448115940156 step 0 next 148
2021-06-12 20:18:33.135713: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5dd00 of size 2048 by op Fill action_count 126448115940157 step 0 next 149
2021-06-12 20:18:33.135722: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5e500 of size 2048 by op Fill action_count 126448115940158 step 0 next 150
2021-06-12 20:18:33.135730: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5ed00 of size 2048 by op Fill action_count 126448115940159 step 0 next 151
2021-06-12 20:18:33.135738: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb5f500 of size 4096 by op Fill action_count 126448115940165 step 0 next 152
2021-06-12 20:18:33.135745: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb60500 of size 4096 by op Fill action_count 126448115940166 step 0 next 155
2021-06-12 20:18:33.135753: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb61500 of size 4096 by op Fill action_count 126448115940167 step 0 next 156
2021-06-12 20:18:33.135762: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb62500 of size 4096 by op Fill action_count 126448115940168 step 0 next 157
2021-06-12 20:18:33.135770: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb63500 of size 1024 by op Fill action_count 126448115940174 step 0 next 160
2021-06-12 20:18:33.135778: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb63900 of size 1024 by op Fill action_count 126448115940175 step 0 next 161
2021-06-12 20:18:33.135786: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb63d00 of size 1024 by op Fill action_count 126448115940176 step 0 next 162
2021-06-12 20:18:33.135794: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64100 of size 1024 by op Fill action_count 126448115940177 step 0 next 163
2021-06-12 20:18:33.135802: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64500 of size 1024 by op Fill action_count 126448115940183 step 0 next 166
2021-06-12 20:18:33.135810: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64900 of size 1024 by op Fill action_count 126448115940184 step 0 next 46
2021-06-12 20:18:33.135818: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb64d00 of size 65536 by op Add action_count 126448115939982 step 0 next 47
2021-06-12 20:18:33.135826: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb74d00 of size 65536 by op Add action_count 126448115939991 step 0 next 52
2021-06-12 20:18:33.135835: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb84d00 of size 98304 by op Add action_count 126448115940009 step 0 next 41
2021-06-12 20:18:33.135843: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fb9cd00 of size 147456 by op Add action_count 126448115939973 step 0 next 42
2021-06-12 20:18:33.135852: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbc0d00 of size 65536 by op Add action_count 126448115940018 step 0 next 67
2021-06-12 20:18:33.135860: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd0d00 of size 2048 by op Fill action_count 126448115940408 step 0 next 294
2021-06-12 20:18:33.135868: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd1500 of size 2048 by op Fill action_count 126448115940409 step 0 next 295
2021-06-12 20:18:33.135876: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd1d00 of size 2048 by op Fill action_count 126448115940410 step 0 next 296
2021-06-12 20:18:33.135884: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd2500 of size 2048 by op Fill action_count 126448115940411 step 0 next 297
2021-06-12 20:18:33.135893: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd2d00 of size 8192 by op Fill action_count 126448115940417 step 0 next 299
2021-06-12 20:18:33.135903: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd4d00 of size 8192 by op Fill action_count 126448115940418 step 0 next 300
2021-06-12 20:18:33.135911: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd6d00 of size 8192 by op Fill action_count 126448115940419 step 0 next 301
2021-06-12 20:18:33.135919: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbd8d00 of size 8192 by op Fill action_count 126448115940420 step 0 next 302
2021-06-12 20:18:33.135927: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdad00 of size 1024 by op Fill action_count 126448115940426 step 0 next 304
2021-06-12 20:18:33.135935: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdb100 of size 1024 by op Fill action_count 126448115940432 step 0 next 305
2021-06-12 20:18:33.135943: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdb500 of size 1024 by op Fill action_count 126448115940438 step 0 next 307
2021-06-12 20:18:33.135951: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdb900 of size 1024 by op Fill action_count 126448115940439 step 0 next 308
2021-06-12 20:18:33.135959: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdbd00 of size 1024 by op Fill action_count 126448115940440 step 0 next 309
2021-06-12 20:18:33.135967: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdc100 of size 1024 by op Fill action_count 126448115940441 step 0 next 310
2021-06-12 20:18:33.135976: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdc500 of size 1024 by op Fill action_count 126448115940447 step 0 next 312
2021-06-12 20:18:33.135983: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdc900 of size 1024 by op Fill action_count 126448115940453 step 0 next 314
2021-06-12 20:18:33.135991: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdcd00 of size 1024 by op Fill action_count 126448115940454 step 0 next 315
2021-06-12 20:18:33.136000: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdd100 of size 1024 by op Fill action_count 126448115940455 step 0 next 316
2021-06-12 20:18:33.136007: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdd500 of size 1024 by op Fill action_count 126448115940456 step 0 next 317
2021-06-12 20:18:33.136015: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdd900 of size 1024 by op Fill action_count 126448115940462 step 0 next 318
2021-06-12 20:18:33.136024: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbddd00 of size 1024 by op Fill action_count 126448115940463 step 0 next 319
2021-06-12 20:18:33.136032: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbde100 of size 1024 by op Fill action_count 126448115940464 step 0 next 320
2021-06-12 20:18:33.136040: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbde500 of size 1024 by op Fill action_count 126448115940465 step 0 next 321
2021-06-12 20:18:33.136048: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbde900 of size 1024 by op Fill action_count 126448115940471 step 0 next 323
2021-06-12 20:18:33.136056: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbded00 of size 1024 by op Fill action_count 126448115940472 step 0 next 324
2021-06-12 20:18:33.136064: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdf100 of size 1024 by op Fill action_count 126448115940473 step 0 next 325
2021-06-12 20:18:33.136072: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdf500 of size 1024 by op Fill action_count 126448115940474 step 0 next 326
2021-06-12 20:18:33.136080: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdf900 of size 256 by op Mul action_count 126448115940491 step 0 next 327
2021-06-12 20:18:33.136090: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfa00 of size 256 by op Cast action_count 126448115940476 step 0 next 328
2021-06-12 20:18:33.136099: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfb00 of size 256 by op Cast action_count 126448115940479 step 0 next 329
2021-06-12 20:18:33.136107: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfc00 of size 256 by op Cast action_count 126448115940482 step 0 next 330
2021-06-12 20:18:33.136115: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfd00 of size 256 by op Cast action_count 126448115940485 step 0 next 331
2021-06-12 20:18:33.136123: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdfe00 of size 256 by op Cast action_count 126448115940488 step 0 next 332
2021-06-12 20:18:33.136131: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbdff00 of size 1024 by op Fill action_count 126448115940496 step 0 next 334
2021-06-12 20:18:33.136140: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0300 of size 1024 by op Fill action_count 126448115940497 step 0 next 335
2021-06-12 20:18:33.136147: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0700 of size 1024 by op Fill action_count 126448115940498 step 0 next 336
2021-06-12 20:18:33.136156: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0b00 of size 1024 by op Fill action_count 126448115940499 step 0 next 337
2021-06-12 20:18:33.136164: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe0f00 of size 1024 by op Fill action_count 126448115940505 step 0 next 339
2021-06-12 20:18:33.136172: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1300 of size 1024 by op Fill action_count 126448115940506 step 0 next 340
2021-06-12 20:18:33.136180: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1700 of size 1024 by op Fill action_count 126448115940507 step 0 next 341
2021-06-12 20:18:33.136188: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1b00 of size 1024 by op Fill action_count 126448115940508 step 0 next 342
2021-06-12 20:18:33.136196: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe1f00 of size 1024 by op Fill action_count 126448115940514 step 0 next 344
2021-06-12 20:18:33.136204: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2300 of size 1024 by op Fill action_count 126448115940515 step 0 next 345
2021-06-12 20:18:33.136212: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2700 of size 1024 by op Fill action_count 126448115940516 step 0 next 346
2021-06-12 20:18:33.136220: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2b00 of size 1024 by op Fill action_count 126448115940517 step 0 next 347
2021-06-12 20:18:33.136228: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe2f00 of size 1024 by op Fill action_count 126448115940523 step 0 next 349
2021-06-12 20:18:33.136237: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3300 of size 1024 by op Fill action_count 126448115940524 step 0 next 350
2021-06-12 20:18:33.136245: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3700 of size 1024 by op Fill action_count 126448115940525 step 0 next 351
2021-06-12 20:18:33.136253: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3b00 of size 1024 by op Fill action_count 126448115940526 step 0 next 352
2021-06-12 20:18:33.136261: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe3f00 of size 256 by op Fill action_count 126448115940532 step 0 next 353
2021-06-12 20:18:33.136268: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4000 of size 1024 by op Fill action_count 126448115940538 step 0 next 354
2021-06-12 20:18:33.136277: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4400 of size 1024 by op Fill action_count 126448115940539 step 0 next 355
2021-06-12 20:18:33.136286: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4800 of size 1024 by op Fill action_count 126448115940540 step 0 next 356
2021-06-12 20:18:33.136295: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe4c00 of size 1024 by op Fill action_count 126448115940541 step 0 next 357
2021-06-12 20:18:33.136304: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5000 of size 1024 by op Fill action_count 126448115940547 step 0 next 359
2021-06-12 20:18:33.136312: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5400 of size 1024 by op Fill action_count 126448115940548 step 0 next 360
2021-06-12 20:18:33.136320: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5800 of size 1024 by op Fill action_count 126448115940549 step 0 next 361
2021-06-12 20:18:33.136328: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe5c00 of size 1024 by op Fill action_count 126448115940550 step 0 next 362
2021-06-12 20:18:33.136336: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6000 of size 1024 by op Fill action_count 126448115940556 step 0 next 364
2021-06-12 20:18:33.136344: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6400 of size 1024 by op Fill action_count 126448115940557 step 0 next 365
2021-06-12 20:18:33.136352: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6800 of size 1024 by op Fill action_count 126448115940558 step 0 next 366
2021-06-12 20:18:33.136360: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe6c00 of size 1024 by op Fill action_count 126448115940559 step 0 next 367
2021-06-12 20:18:33.136368: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7000 of size 1024 by op Fill action_count 126448115940565 step 0 next 369
2021-06-12 20:18:33.136377: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7400 of size 1024 by op Fill action_count 126448115940566 step 0 next 370
2021-06-12 20:18:33.136385: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7800 of size 1024 by op Fill action_count 126448115940567 step 0 next 371
2021-06-12 20:18:33.136393: I tensorflow/core/common_runtime/bfc_allocator.cc:1046] InUse at b0fbe7c00 of size 1024 by op Fill action_count 126448115940568 step 0 next 372
Windows fatal exception: access violation

Thread 0x000056c4 (most recent call first):
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 576 in _handle_results
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 892 in run
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 954 in _bootstrap_inner
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 912 in _bootstrap

Thread 0x000038f4 (most recent call first):
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 528 in _handle_tasks
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 892 in run
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 954 in _bootstrap_inner
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 912 in _bootstrap

Thread 0x0000275c (most recent call first):
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\connection.py"", line 816 in _exhaustive_wait
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\connection.py"", line 884 in wait
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 499 in _wait_for_updates
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 519 in _handle_workers
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 892 in run
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 954 in _bootstrap_inner
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 912 in _bootstrap

Thread 0x00004788 (most recent call first):
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\multiprocessing\pool.py"", line 114 in worker
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 892 in run
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 954 in _bootstrap_inner
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\threading.py"", line 912 in _bootstrap

Thread 0x00002b44 (most recent call first):
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\execute.py"", line 59 in quick_execute
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\function.py"", line 591 in call
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\function.py"", line 1960 in _call_flat
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\function.py"", line 3023 in __call__
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\def_function.py"", line 950 in _call
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\eager\def_function.py"", line 889 in __call__
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\mirrored_run.py"", line 86 in call_for_each_replica
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\mirrored_strategy.py"", line 678 in _call_for_each_replica
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 2833 in call_for_each_replica
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\distribute\distribute_lib.py"", line 1285 in run
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 173 in _ensure_model_is_built
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 394 in load_fine_tune_checkpoint
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\object_detection\model_lib_v2.py"", line 599 in train_loop
  File ""C:\Users\Edward\Projects\Tensorflow\workspace\training_demo\model_main_tf2.py"", line 106 in main
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 251 in _run_main
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\absl\app.py"", line 303 in run
  File ""C:\Users\Edward\anaconda3\envs\tensorflow\lib\site-packages\tensorflow\python\platform\app.py"", line 40 in run
  File ""C:\Users\Edward\Projects\Tensorflow\workspace\training_demo\model_main_tf2.py"", line 115 in <module>
```
"
50237,tf.lite.OpsSet.SELECT_TF_OPS does not work properly in tf.math.erf,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04 and Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip install and build from source
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly==2.6.0-dev20210612 and 
 tensorflow==2.5.0 and tensorflow==2.4.1

### 2. Code
- Minimal reproducible Colabo URLs.
https://colab.research.google.com/drive/1raok6fu6uUR_VC64cg_5nRfcRFyNLzPJ?usp=sharing

### 3. Any other info / logs
The problem is simple: only **`tf.math.erf`** is not properly recognized as a target of **`tf.lite.OpsSet.SELECT_TF_OPS`**. The results are the same for **`tf-nightly`**, **`v2.5.0`**, and **`v2.4.1`**.
- from_keras_model
```python
x = Input(shape=(128,128,3), batch_size=1, dtype=tf.float32, name='input')
y = tf.math.erf(x=x)

model = Model(inputs=x, outputs=y)
model.summary()
model.save('saved_model')

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""test.tflite"", ""wb"").write(tflite_model)
```
```
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(1, 128, 128, 3)]        0         
_________________________________________________________________
tf.math.erf (TFOpLambda)     (1, 128, 128, 3)          0         
=================================================================
Total params: 0
Trainable params: 0
Non-trainable params: 0
_________________________________________________________________
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
INFO:tensorflow:Assets written to: saved_model/assets
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
INFO:tensorflow:Assets written to: /tmp/tmpk9q0o_st/assets
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    293                                                  debug_info_str,
--> 294                                                  enable_mlir_converter)
    295       return model_str

5 frames
Exception: /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:421:0: error: 'tf.Erf' op is neither a custom op nor a flex op
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/tflite_keras_util.py:184:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:672:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3289:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3444:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3050:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:764:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1273:0: note: called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Erf
Details:
	tf.Erf {device = """"}



During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    295       return model_str
    296     except Exception as e:
--> 297       raise ConverterError(str(e))
    298 
    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:421:0: error: 'tf.Erf' op is neither a custom op nor a flex op
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1030:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/tflite_keras_util.py:184:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:672:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:999:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3289:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3444:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py:3050:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:764:0: note: called from
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py:1273:0: note: called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Erf
Details:
	tf.Erf {device = """"}
```
- from_saved_model
```python
converter = tf.lite.TFLiteConverter.from_saved_model('saved_model')
converter.target_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]
tflite_model = converter.convert()
open(""test.tflite"", ""wb"").write(tflite_model)
```
```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    293                                                  debug_info_str,
--> 294                                                  enable_mlir_converter)
    295       return model_str

4 frames
Exception: <unknown>:0: error: loc(callsite(callsite(""model/tf.math.erf/Erf@__inference__wrapped_model_20"" at ""PartitionedCall@__inference_signature_wrapper_69"") at ""PartitionedCall"")): 'tf.Erf' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""PartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Erf
Details:
	tf.Erf {device = """"}



During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    295       return model_str
    296     except Exception as e:
--> 297       raise ConverterError(str(e))
    298 
    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(callsite(callsite(""model/tf.math.erf/Erf@__inference__wrapped_model_20"" at ""PartitionedCall@__inference_signature_wrapper_69"") at ""PartitionedCall"")): 'tf.Erf' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""PartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Erf
Details:
	tf.Erf {device = """"}
```"
50233,ValueError: numpy.ndarray size changed with numpy 1.19.5,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- Windows 10 
- TensorFlow installed from package: python-tensorflow
- TensorFlow version: 2.5.0
- Python version: 3.7
- Numpy: 1.19.5
- Installed using virtualenv? pip? conda?: Conda

When I try to train my model I get the error 
`ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject`
The solutions I found online all tell me to upgrade numpy to 1.20; however, as far as I am aware that has compatibility issues with Tensorflow. 
I run the command 
`python model_main_tf2.py --pipeline_config_path=training/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8.config --model_dir=training --alsologtostderr`


**Logs*

> 2021-06-11 19:10:57.108835: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found
2021-06-11 19:10:57.110211: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
Traceback (most recent call last):
  File ""model_main_tf2.py"", line 32, in <module>
    from object_detection import model_lib_v2
  File ""C:\Users\Mohidul Efaz\anaconda3\envs\trash\lib\site-packages\object_detection\model_lib_v2.py"", line 29, in <module>
    from object_detection import eval_util
  File ""C:\Users\Mohidul Efaz\anaconda3\envs\trash\lib\site-packages\object_detection\eval_util.py"", line 35, in <module>
    from object_detection.metrics import coco_evaluation
  File ""C:\Users\Mohidul Efaz\anaconda3\envs\trash\lib\site-packages\object_detection\metrics\coco_evaluation.py"", line 25, in <module>
    from object_detection.metrics import coco_tools
  File ""C:\Users\Mohidul Efaz\anaconda3\envs\trash\lib\site-packages\object_detection\metrics\coco_tools.py"", line 51, in <module>
    from pycocotools import coco
  File ""C:\Users\Mohidul Efaz\anaconda3\envs\trash\lib\site-packages\pycocotools\coco.py"", line 55, in <module>
    from . import mask as maskUtils
  File ""C:\Users\Mohidul Efaz\anaconda3\envs\trash\lib\site-packages\pycocotools\mask.py"", line 3, in <module>
    import pycocotools._mask as _mask
  File ""pycocotools\_mask.pyx"", line 1, in init pycocotools._mask
ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
50232,Why is cudaFree(nullptr) needed?,"Per code here: https://github.com/tensorflow/tensorflow/commit/4535bd5df4d077072a8f207146bf4cd051971237#diff-b0331d3a83e4697c835bec57bc002586de8a3f93b520cad43edf07218dc0ea0eR748, it is calling `cudaFree(nullptr)` as part of cuda initialization. However based on CUDA doc: https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ga042655cbbf3408f01061652a075e094, cudaFree(nullptr) performs no operation. Wonder why this is needed here? CCed @yzhwang "
50231,Basic image classification tutorial missing minor step,"URL(s) with the issue:
https://www.tensorflow.org/tutorials/keras/classification

The step where the last plot is being drawn (following code) is missing `plt.show()` to display the plot, despite the plot being shown as displayed in the doc:

```
plot_value_array(1, predictions_single[0], test_labels)
_ = plt.xticks(range(10), class_names, rotation=45)
# missing plt.show() here
```"
50230,IRFFT much slower than RFFT on GPU,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.4.0
- Python version: 3.8
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.0
- GPU model and memory: NVIDIA Geforce 970 M (but I've observed similar on other NVIDIA GPUs)

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

IRFFT takes much longer than RFFT (orders of magnitude) when running on the GPU. On the CPU, they take about the same amount of time.

**Describe the expected behavior**

IRFFT may be somewhat longer than RFFT, but hopefully at most a few times longer.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import numpy as np
import tensorflow as tf

dims = 1024
seq_len = 1024

use_gpu = True

tf.profiler.experimental.start(
    logdir=""test_fft_convolution_speed_log"" + ("""" if use_gpu else ""_cpu"")
)

with tf.device(""/GPU:0"" if use_gpu else ""/CPU:0""):
    u = tf.random.uniform(shape=(dims, seq_len), minval=-1, maxval=1, seed=0)
    v = tf.random.uniform(shape=(dims, seq_len), minval=-1, maxval=1, seed=0)

    fu = tf.signal.rfft(u, fft_length=[2 * seq_len], name=""fu"")
    fv = tf.signal.rfft(v, fft_length=[2 * seq_len], name=""fv"")

    fw = fu * fv
    w = tf.signal.irfft(fw, fft_length=[2 * seq_len], name=""iw"")[..., :seq_len]

    print(w.numpy()[0, 0])

tf.profiler.experimental.stop()
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50229,Custom tflite writer for pre-quantized model,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Hello,

I am looking for some help in the area of tflite write for conversion of a .pb file to a tflite(flatbuffer file).
1. I currently have a custom pre quantized optimized model, and I want to write this pretrained file to tflite format.
2. Being a custom quantized model, stock converter provided by tensorflow is not an ideal match for now.
3. As I have already optimized the pre-trained model, I want my conv layers to stored as int8 instead of float32, as the weights are in integer format.
4. Can we tweak the converter and make the datatype of conv layers as int8.

Thanks
"
50228, Error while Installing tensorflow nightly ,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
-windows 10:

Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:

TensorFlow installed from (source or binary): pip install tf-nightly

TensorFlow version:2.5.0

Python version:3.9

Installed using virtualenv? pip? conda?: pip

Bazel version (if compiling from source):

GCC/Compiler version (if compiling from source):

CUDA/cuDNN version:

GPU model and memory: 6 gb ram and gt71

**Describe the problem**
Throwing an exception error

**Provide the exact sequence of commands / steps that you executed before running into the problem**
File ""c:\users\ts\appdata\local\programs\python\python39\lib\site-packages\pip_vendor\resolvelib\resolvers.py"", line 171, in _merge_into_criterion
crit = self.state.criteria[name]
KeyError: 'tf-nightly-gpu'

During handling of the above exception, another exception occurred:below

**Any other info / logs**
`raise ReadTimeoutError(self._pool, None, ""Read timed out."")
pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out`
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50227,Using tf.map_fn but getting error: 'Iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.',"I am trying to implement a custom layer by wrapping around a function that reshapes my data. I got the following error `Iterating over `tf.Tensor` is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.'`

This happened upon trying to use my custom class, i.e: 
```
input = keras.Input(shape=3)
x = SparseConv2D(10)(input)
x = DenseFromSparse()(x)
x = layers.Flatten()(x)
x = layers.Dense(128, activation='relu')(x)
x = layers.Dense(NUM_CLASSES, activation='softmax')(x)

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.fit(sparse_x, y_train, epochs=5)
```
Here is my code for the custom class that is being problematic:

```
@tf.function
def dense_from_sparse(sparse_data_batch):
  """"""Transforms a sparse batch of data into the conventional data batch format
  used in CV applications.
    Args:
      padded_coordinates: A `Tensor` of type `int32`.
        [batch_size, max_num_coords_per_batch, 2], the padded 2D
        coordinates. max_num_coords_per_batch is the max number of coordinates in
        each batch item.
      num_valid_coordinates: A `Tensor` of type `int32`.
        [batch_size], the number of valid coordinates per batch
        item. Only the top num_valid_coordinates[i] entries in coordinates[i],
        padded_features[i] are valid. The rest of the entries
        are paddings.
      padded_features: A `Tensor` of type `float32`.
        [batch_size, max_num_coords_per_batch, in_channels] where
        in_channels is the channel size of the input feature.
    Returns: 
      data_batch: A 'Tensor' of type float32,
      [batch_size, image_width, image_height]""""""
  
  indices, num_valid_coordinates, padded_features = sparse_data_batch
  batch_size = padded_features.shape[0]
  sparse_tensors = tf.map_fn(sparse_tensor_fn, tf.range(batch_size))
  dense_batch = tf.map_fn(tf.sparse.to_dense, sparse_tensors)
  return dense_batch


class DenseFromSparse(tf.keras.layers.Layer):
  
  def call(self, inputs):
    return dense_from_sparse(inputs)


@tf.function
def sparse_tensor_fn(i):
  sparse_tensor = tf.sparse.SparseTensor(indices=indices[i][:num_valid_coordinates[i]], 
                    values=tf.squeeze(padded_features[i][:num_valid_coordinates[i]]),dense_shape=dense_shape)
  return sparse_tensor
```

Also any best practices for implementing and debugging custom classes would be greatly appreciated."
50226,[RNN] Rolled SimpleRNN and GRU TFLite conversion,"### 1. System information
local server
- OS Platform and Distribution: Ubuntu 16.04.4 LTS
- TensorFlow installation: pip package
- TensorFlow library version: 2.6.0-dev20210603

colab
- OS Platform and Distribution: Ubuntu 18.04.5 LTS
- TensorFlow installation: pip package
- TensorFlow library version: 2.6.0-dev20210611

### 2. Code

To reproduce the issue, an end-to-end example is in: https://colab.research.google.com/drive/1_e_pcvIjeuUA_OdqpAYeJhedAAjP2UbL?usp=sharing

### 3. Question
In the colab above, we can see using the fused or unfused way of converting rolled GRU/SimpleRNN layer will result in the same error in TFLite converter: `""ValueError: Failed to parse the model: Only models with a single subgraph are supported, model had 3 subgraphs.""`
If the conversion code in the example is correct, we are wondering whether there is any plan to support the rolled GRU/SimpleRNN conversion?
@daverim @jianlijianli @wwwind @akarmi for visibility"
50225,.predict() method when using saved & loaded subclassing model,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab (Windows/Linux as well)
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0  (Google Colab default one)
- Python version:  3.7.10 (Google Colab default one)

**Describe the current behavior**
When loading a specific custom subclassing model (overriding the train_step and multiple inputs), the `.predict()` method doesn't work. Where as it's working when the model is defined (and not loaded) and it's also working by using the `.call()` method on the loaded model (see [gist](https://colab.research.google.com/gist/quetil/2b48f95c3a18c0dd6e5fc39e83109a12/save-and-predict-with-custom-class.ipynb?authuser=1#scrollTo=ngyMn-qeuhov)).

The following error message:
```
ValueError: Could not find matching function to call loaded from the SavedModel. Got:
      Positional arguments (2 total):
        * (<tf.Tensor 'inputs:0' shape=(32, 2) dtype=float32>, <tf.Tensor 'inputs_1:0' shape=(32, 2) dtype=float32>)
        * False
      Keyword arguments: {}
    
    Expected these arguments to match one of the following 4 option(s):
    
    Option 1:
      Positional arguments (2 total):
        * [TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs/0'), TensorSpec(shape=(None, 2), dtype=tf.float32, name='inputs/1')]
        * False
      Keyword arguments: {}
```
The option 1 is really matching with the inputs received though...

**Describe the expected behavior**
 The `.predict()` method should work on loaded model.

**Standalone code to reproduce the issue**
I tried to do a simple example to show you the issue.
[Gist](https://colab.research.google.com/gist/quetil/2b48f95c3a18c0dd6e5fc39e83109a12/save-and-predict-with-custom-class.ipynb?authuser=1#scrollTo=ngyMn-qeuhov)

Thank you in advance for you help, I imagine something is missing in the definition of the class to be able to use the `predict()` method on saved & loaded model.
"
50224,[Feature request] Ability to specify TensorSpec.name in tf.Dataset,"**System information**
- TensorFlow version (you are using): 2.5.0 (Colab)
- Are you willing to contribute it (Yes/No): ❌

**Describe the feature and the current behavior/state.**

Please see an example Colab notebook [here](https://colab.research.google.com/drive/1CAPt7TqQrwaHA2WoHqx0VB-ybUCxqtTk?usp=sharing).

Consider a `tf.Dataset` of the following form:

```python
import numpy as np
import tensorflow as tf

def gen():
    # some data generator
    for x in range(10):
        yield {
            'x': np.arange(x + 1),
            'y': np.arange(15).reshape(3, 5).astype(np.float32),
        }

tf_dataset = tf.data.Dataset.from_generator(
    gen,
    output_shapes={
        'x': (None,),
        'y': (None, 5)
    },
    output_types={
        'x': tf.int32,
        'y': tf.float32,
    },
)

# tf_dataset.element_spec contains the following:
# {'x': TensorSpec(shape=(None,), dtype=tf.int32, name=None),
#  'y': TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)}
# There seems to be no API to specify the name fields.

# An attempt to set the name directly fails with AttributeError: can't set attribute
tf_dataset.element_spec['x'].name = 'x'
```

It would be desirable to have an API to set `TensorSpec.name`.

**Will this change the current api? How?**

There could be an extra option for `from_generator`, such as `output_names`, that would specify tensor names for the generator output.

**Who will benefit with this feature?**

Anyone who needs to debug `keras.Model.fit()`. When there is a mismatch between what the model expects and what the dataset provides, one can see in the debugger exactly which tensors are yielded from the dataset. Currently, since Keras models can only receive tuples as inputs, there is no way to annotate these tensors other than via `Tensor.name` (if Keras models could receive dicts, there would have been no such problem)."
50223,tensorflow lite on armv7,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):  Debian package repo
- TensorFlow version: 
- Python version:3.6.9
- Installed using virtualenv? pip? conda?:apt-get
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Installed from Debian repo as my OS is ubuntu 16.04 on ARMv7l.
On apt-get update i see following message:
N: Skipping acquire of configured file 'main/binary-armhf/Packages' as repository 'https://apt.corretto.aws stable InRelease' doesn't support architecture 'armhf'
N: Skipping acquire of configured file 'main/binary-armhf/Packages' as repository 'https://apt.kitware.com/ubuntu bionic InRelease' doesn't support architecture 'armhf'
Subsequently installed : sudo apt-get install python3-tflite-runtime
But tflite_runtime import failed.
Does it mean the repo does not support current target hardware
**Provide the exact sequence of commands / steps that you executed before running into the problem**
Following are the commands:
echo ""deb https://packages.cloud.google.com/apt coral-edgetpu-stable main"" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list
curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-get update
sudo apt-get install python3-tflite-runtime

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

"
50222,model question,"my problem:
TypeError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:845 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:838 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:800 train_step
        self.compiled_metrics.update_state(y, y_pred, sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/compile_utils.py:460 update_state
        metric_obj.update_state(y_t, y_p, sample_weight=mask)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/metrics_utils.py:86 decorated
        update_op = update_state_fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:177 update_state_fn
        return ag_update_state(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:666 update_state  **
        matches, sample_weight=sample_weight)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py:424 update_state
        with ops.control_dependencies([value_sum]):
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:5390 control_dependencies
        return get_default_graph().control_dependencies(control_inputs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py:363 control_dependencies
        self.control_captures.add(graph_element)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/keras_tensor.py:242 __hash__
        'Instead, use tensor.ref() as the key.' % self)

    TypeError: Tensors are unhashable. (KerasTensor(type_spec=TensorSpec(shape=(), dtype=tf.float32, name=None), name='tf.math.reduce_sum_1/Sum:0', description=""created by layer 'tf.math.reduce_sum_1'""))Instead, use tensor.ref() as the key.
and my model:vae
batch_size=6
x = Input(shape=(120,120,3))
conv1 = Conv2d_BN(x, 8, (3, 3))
conv1 = Conv2d_BN(conv1, 8, (3, 3))
pool1 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv1)
conv2 = Conv2d_BN(pool1, 16, (3, 3))
conv2 = Conv2d_BN(conv2, 16, (3, 3))
pool2 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv2)
conv3 = Conv2d_BN(pool2, 32, (3, 3))
conv3 = Conv2d_BN(conv3, 32, (3, 3))
pool3 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv3)
conv4 = Conv2d_BN(pool3, 64, (3, 3))
conv4 = Conv2d_BN(conv4, 64, (3, 3))
pool4 = MaxPooling2D(pool_size=(2,2),strides=(2,2),padding='same')(conv4)
conv5 = Conv2d_BN(pool4, 128, (3, 3))
conv5 = Dropout(0.5)(conv5)
conv5 = Conv2d_BN(conv5, 128, (3, 3))
conv5 = Dropout(0.5)(conv5)
F=Flatten()(conv5)
h = Dense(256, activation='relu')(F)
z_mean = Dense(2)(h)
z_log_var = Dense(2)(h)
def sampling(args):
    z_mean, z_log_var = args
    #print(type(args))
    #print(type(z_log_var))
    #print(type(z_mean))
    epsilon = K.random_normal(shape=(2,), mean=0.,
                              stddev=1.0)
    #print(type(epsilon))
    return z_mean + K.exp(z_log_var / 2) * epsilon
z = Lambda(sampling,name = 'sampling', output_shape=(2))([z_mean, z_log_var])
decoder_h = Dense(256, activation='relu')
decoder_mean = Dense(256, activation='sigmoid')
h_decoded = decoder_h(z)
x_decoded_mean = decoder_mean(h_decoded)
decoder=Dense(8*8*128,activation='relu')(x_decoded_mean)
decoder=Reshape((8,8,128))(decoder)
decoder = Conv2DTranspose(128, kernel_size=(3,3), strides=2)(decoder)
decoder = Conv2D(filters=128, kernel_size=(3,3), strides=(1,1), activation='sigmoid')(decoder)
convt1 = Conv2dT_BN(decoder, 64, (3, 3))
convt1 = Dropout(0.5)(convt1)
conv6 = Conv2d_BN(convt1, 64, (3, 3))
conv6 = Conv2d_BN(conv6, 64, (3, 3))
convt2 = Conv2dT_BN(conv6, 32, (3, 3))
convt2  = Dropout(0.5)(convt2)
conv7 = Conv2d_BN(convt2 , 32, (3, 3))
conv7 = Conv2d_BN(conv7, 32, (3, 3))
convt3 = Conv2dT_BN(conv7, 16, (3, 3))
convt3 = Dropout(0.5)(convt3)
conv8 = Conv2d_BN(convt3, 16, (3, 3))
conv8 = Conv2d_BN(conv8, 16, (3, 3))
outpt = Conv2D(filters=3, kernel_size=(1,1), strides=(1,1), padding='same', activation='sigmoid')(conv8)
vae = Model(inputs=x, outputs=outpt)
vae.compile(optimizer='adam',loss='mean_squared_error',metrics = [KL_loss, recon_loss])
vae.summary()
how can i solve this question"
50220,Make tf.io.gfile / tf.data separate (standalone) packages?,"**System information**
- TensorFlow version (you are using): 2.4.x
- Are you willing to contribute it (Yes/No): No (time constraints) 

**Describe the feature and the current behavior/state.**

At the moment we have to install `tensorflow` as a whole even though our data-processing pipeline only requires access to `tf.gfile` and/or `tf.data`.

Imo it could be beneficial if it was possible to install these two packages separately (standalone) without having to install the rest of Tensorflow. Especially `tf.gfile` could be useful in projects where not a single line related to machine-learning is written so this would be a candidate to become its own `pip`-installable package maybe with the option to install the `tf.data` functionalities. 

```
pip install tensorflow_gfile        # gfile only
pip install tensorflow_gfile[data]  # gfile + data
pip install tensorflow              # All the things
```

**Will this change the current api? How?**

The API should not change, only the fact that packages can be selectively installed.

**Who will benefit with this feature?**

Everyone who works on larger projects.
"
50219,Function Signature of `_resource_apply_gradients` and sparse equivalents in abstract `Optimizer`,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/optimizer_v2/optimizer_v2.py#L1237-L1298

## Description of issue (what needs changing):

It appears that the function signature is out of date: The second argument is called `handle` and the description indicates that the function expects a `resource` variable pointing to the variable that needs updating, but if we look at the [SGD Optimizer](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/optimizer_v2/gradient_descent.py#L129-L149) for example it appears that in reality the variable itself is passed to this function. This problem is something I could fix myself in a pull request.

The second part I am not clear on: What is the return value supposed to be? [cf. StackOverflow Question](https://stackoverflow.com/questions/67902325/what-type-should-the-method-resource-apply-gradient-from-tensorflow-keras-op). In particular I do not know how to construct an operation without using the inbuilt functions used in the provided implementations. Which is an issue if I want to create a custom optimizer.

I am thinking that 

```python
return tf.raw_ops.ResourceApplyGradientDescent(
        var=var.handle,
        alpha=coefficients[""learning_rate""],
        delta=grad,
        use_locking=self._use_locking,
)
```

might be equivalent to

```python
return var.assign_sub(
    delta=coefficients[""learning_rate""]*grad,
    use_locking=self._use_locking,
    read_value=False
)
```
but I am not sure and I can not find any documentation on that"
50218,The method layers.experimental.preprocessing.RandomRotation is not available in M1 chip and macOS,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur 11.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary from miniconda3
- TensorFlow version (use command below): 2.5
- Python version: 3.9.2
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: M1 chip

I have installed Tensorflow 2.5 according to [Tensorflow Installation Instructions for Apple M1](https://developer.apple.com/metal/tensorflow-plugin/)
And I wanna run the code from Keras example which is the [Image classification from scratch](https://keras.io/examples/vision/image_classification_from_scratch/)

When I run this code:
```Python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import os

num_skipped = 0
dir_path = ""/Users/MyMac/PetImages""
for folder_name in (""Cat"",""Dog""):
    folder_path = os.path.join(dir_path,folder_name)
    for fname in os.listdir(folder_path):
        fpath = os.path.join(folder_path, fname)
        try:
            fobj = open(fpath, ""rb"")
            is_jfif = tf.compat.as_bytes(""JFIF"") in fobj.peek(10)
        finally:
            fobj.close()

        if not is_jfif:
            num_skipped += 1
            # Delete corrupted image
            os.remove(fpath)

print(""Deleted %d images"" % num_skipped)

image_size = (180, 180)
batch_size = 32

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dir_path,
    validation_split=0.2,
    subset=""training"",
    seed=1337,
    image_size=image_size,
    batch_size=batch_size,
)
val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    dir_path,
    validation_split=0.2,
    subset=""validation"",
    seed=1337,
    image_size=image_size,
    batch_size=batch_size,
)

data_augmentation = keras.Sequential(
    [
        layers.experimental.preprocessing.RandomFlip(""horizontal""),
        layers.experimental.preprocessing.RandomRotation(0.1),
    ]
)

plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
    for i in range(9):
        augmented_images = data_augmentation(images)
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(augmented_images[0].numpy().astype(""uint8""))
        plt.axis(""off"")
```
The output is:
```Python
---------------------------------------------------------------------------
NotFoundError                             Traceback (most recent call last)
<ipython-input-6-2d5292b3c876> in <module>
      2 for images, _ in train_ds.take(1):
      3     for i in range(9):
----> 4         augmented_images = data_augmentation(images)
      5         ax = plt.subplot(3, 3, i + 1)
      6         plt.imshow(augmented_images[0].numpy().astype(""uint8""))

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1028         with autocast_variable.enable_auto_cast_variables(
   1029             self._compute_dtype_object):
-> 1030           outputs = call_fn(inputs, *args, **kwargs)
   1031 
   1032         if self._activity_regularizer:

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/sequential.py in call(self, inputs, training, mask)
    378       if not self.built:
    379         self._init_graph_network(self.inputs, self.outputs)
--> 380       return super(Sequential, self).call(inputs, training=training, mask=mask)
    381 
    382     outputs = inputs  # handle the corner case where self.layers is empty

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py in call(self, inputs, training, mask)
    418         a list of tensors if there are more than one outputs.
    419     """"""
--> 420     return self._run_internal_graph(
    421         inputs, training=training, mask=mask)
    422 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/functional.py in _run_internal_graph(self, inputs, training, mask)
    554 
    555         args, kwargs = node.map_arguments(tensor_dict)
--> 556         outputs = node.layer(*args, **kwargs)
    557 
    558         # Update tensor_dict.

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
   1028         with autocast_variable.enable_auto_cast_variables(
   1029             self._compute_dtype_object):
-> 1030           outputs = call_fn(inputs, *args, **kwargs)
   1031 
   1032         if self._activity_regularizer:

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in call(self, inputs, training)
    825           interpolation=self.interpolation)
    826 
--> 827     output = control_flow_util.smart_cond(training, random_rotated_inputs,
    828                                           lambda: inputs)
    829     output.set_shape(inputs.shape)

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/utils/control_flow_util.py in smart_cond(pred, true_fn, false_fn, name)
    107     return control_flow_ops.cond(
    108         pred, true_fn=true_fn, false_fn=false_fn, name=name)
--> 109   return smart_module.smart_cond(
    110       pred, true_fn=true_fn, false_fn=false_fn, name=name)
    111 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/smart_cond.py in smart_cond(pred, true_fn, false_fn, name)
     52   if pred_value is not None:
     53     if pred_value:
---> 54       return true_fn()
     55     else:
     56       return false_fn()

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/keras/layers/preprocessing/image_preprocessing.py in random_rotated_inputs()
    816       min_angle = self.lower * 2. * np.pi
    817       max_angle = self.upper * 2. * np.pi
--> 818       angles = self._rng.uniform(
    819           shape=[batch_size], minval=min_angle, maxval=max_angle)
    820       return transform(

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in uniform(self, shape, minval, maxval, dtype, name)
    811             minval=minval, maxval=maxval, name=name)
    812       else:
--> 813         rnd = self._uniform(shape=shape, dtype=dtype)
    814         return math_ops.add(rnd * (maxval - minval), minval, name=name)
    815 

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _uniform(self, shape, dtype)
    714   def _uniform(self, shape, dtype):
    715     if compat.forward_compatible(2020, 10, 25):
--> 716       key, counter = self._prepare_key_counter(shape)
    717       return gen_stateless_random_ops_v2.stateless_random_uniform_v2(
    718           shape=shape,

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _prepare_key_counter(self, shape)
    632   def _prepare_key_counter(self, shape):
    633     delta = math_ops.reduce_prod(shape)
--> 634     counter_key = self.skip(delta)
    635     counter_size = _get_counter_size(self.algorithm)
    636     counter = array_ops.bitcast(counter_key[:counter_size], dtypes.uint64)

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in skip(self, delta)
    584     """"""
    585     if compat.forward_compatible(2020, 10, 25):
--> 586       return self._skip(delta)
    587     gen_stateful_random_ops.rng_skip(
    588         self.state.handle, math_ops.cast(self.algorithm, dtypes.int64),

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _skip(self, delta)
    614           return ds_context.get_strategy().extended.update(
    615               self.state, update_fn)
--> 616     return update_fn(self.state)
    617 
    618   def _preprocess_key(self, key):

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in update_fn(v)
    598   def _skip(self, delta):
    599     def update_fn(v):
--> 600       return self._skip_single_var(v, delta)
    601     # TODO(b/170515001): Always call strategy.extended.update after calling it
    602     #   from both replica context and cross-replica context is supported.

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/stateful_random_ops.py in _skip_single_var(self, var, delta)
    592   def _skip_single_var(self, var, delta):
    593     # TODO(wangpeng): Cache the cast algorithm instead of casting everytime.
--> 594     return gen_stateful_random_ops.rng_read_and_skip(
    595         var.handle, alg=math_ops.cast(self.algorithm, dtypes.int32),
    596         delta=math_ops.cast(delta, dtypes.uint64))

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/ops/gen_stateful_random_ops.py in rng_read_and_skip(resource, alg, delta, name)
    113       return _result
    114     except _core._NotOkStatusException as e:
--> 115       _ops.raise_from_not_ok_status(e, name)
    116     except _core._FallbackException:
    117       pass

~/miniforge3/lib/python3.9/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6895   message = e.message + ("" name: "" + name if name is not None else """")
   6896   # pylint: disable=protected-access
-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)
   6898   # pylint: enable=protected-access
   6899 

~/miniforge3/lib/python3.9/site-packages/six.py in raise_from(value, from_value)

NotFoundError: No registered 'RngReadAndSkip' OpKernel for 'GPU' devices compatible with node {{node RngReadAndSkip}}
	.  Registered:  device='XLA_CPU_JIT'
  device='CPU'
 [Op:RngReadAndSkip]
```

But I modify the code like this:
```Python
data_augmentation = keras.Sequential(
    [
        layers.experimental.preprocessing.RandomFlip(""horizontal""),
        # layers.experimental.preprocessing.RandomRotation(0.1),
    ]
)
```
It's running successfully!
I check the Tensorflow document, the procedure to call RandomRotation method is correct.
Please advise me how to use RandomRotation.
Thanks!"
50217,Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0.,"OS: Windows 10
Python: 3.9.1
Tensorflow:  2.5.0
CUDA: 11.3
cudnn: 8.2.0

Output of `nvcc --version`
```
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:24:09_Pacific_Daylight_Time_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0
```
```python
build['cuda_version']  = 64_112
build['cudnn_version'] = 64_8
```


I encountered problem stated in title when testing a model which I trained on the same machine.
During training, a log message stated that `cudnn version 8200` was loaded
```
2021-06-11 15:49:39.798483: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8200
```

But then during testing, the message below appeared.
```
2021-06-11 15:49:10.188342: E tensorflow/stream_executor/cuda/cuda_dnn.cc:352] Loaded runtime CuDNN library: 8.0.5 but source was compiled with: 8.1.0. 
CuDNN library needs to have matching major version and equal or higher minor version. 
If using a binary install, upgrade your CuDNN library. 
If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
```

How should I solve the problem?
Thank you."
50214,Backpropagation operators get blocked by NCCL operators when TF Profiler is enabled,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian GNU/Linux 10 (buster).
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.4.1
- Python version: 3.7.3
- Nvidia Driver: 418.116.00
- CUDA version: 11.0
- cuDNN: 8.0.5
- GPU model and memory: Tesla V100-SXM2-32GB
- NCCL version: 2.8.3
- Horovod version: 0.21.0
- Connection: 2 physical machines, each with 2 V100 GPUs
- Network: 100 Gbps RDMA

**Describe the current behavior**
When using TensorFlow's profiler `tf.profiler.experimental` API to see the timeline of a distributed training job with Horovod+NCCL, it seems that backpropagation (**BP** in short) operators are blocked by NCCL operators. The following figure is the timeline of rank 0 GPU, the row of `Stream #30` shows NCCL traces and the row of `TensorFlow Ops` shows the computation operators . Here we observe two cases of blocking:
1. When the `NCCL 1` operator starts to run, one **BP** operator is still running. The execution time of this **BP** operator is always larger than that of `NCCL 1`, saying that this **BP** operator is blocked and can only finishes after `NCCL 1` is finished.
2. Another case is that during the execution of `NCCL 2`, no `BP` operator is scheduled to run.
![image](https://user-images.githubusercontent.com/17765864/121631175-26912280-cab1-11eb-8c09-ec36c36e95f1.png)


**Describe the expected behavior**
However, these two kinds of blocking will not occur when TensorFlow Profiler is disabled. The following figure shows the corresponding timeline (rank 0 GPU) profiled with `nvprof`. We can see that there is no obvious gap between **BP** operators or extremely long **BP** operators.
![image](https://user-images.githubusercontent.com/17765864/121632493-a6b88780-cab3-11eb-84d8-b26d3ba05ada.png)

Actually, we also didn't observe these two kinds of blocking in Tensorflow 1.x, **it seems that it is a problem of the profiler of  TensorFlow 2.x**. Is that expected or a bug of TF 2' profiler ?

**Standalone code to reproduce the issue**
The python Script to reproduce:
https://github.com/joapolarbear/horovod/blob/b_v0.21.0_tfissue/examples/tensorflow2/tensorflow2_synthetic_benchmark.py
### Profile with TensorFlow Profiler
```
export TRACE_DIR= /path/to/dir
mpirun -np ${TOTAL_GPU_NUM} -H ${HOST_LIST} \
    -bind-to none -map-by slot -mca plm_rsh_args '-p 12345' \
    -mca pml ob1 -mca btl ^openib --allow-run-as-root \
    python3 tensorflow2_synthetic_benchmark.py --profile_range 10,20 --trace_dir ${TRACE_DIR}
```
### Profile with nvprof
```
export TRACE_DIR=/path/to/dir
export NVPROF_CMD=""nvprof -o $TRACE_DIR/simple.%q{OMPI_COMM_WORLD_RANK}.nvprof ""
mpirun -np ${TOTAL_GPU_NUM} -H ${HOST_LIST} \
    -bind-to none -map-by slot -mca plm_rsh_args '-p 12345' \
    -mca pml ob1 -mca btl ^openib --allow-run-as-root \ 
    ${NVPROF_CMD} python3 tensorflow2_synthetic_benchmark.py
```
**Other info / logs** Include any logs or source code that would be helpful to
The trace files we used in above two examples:
[tf_profiler](https://github.com/tensorflow/tensorflow/files/6635952/trace.json.gz)
[nvprof](https://github.com/tensorflow/tensorflow/files/6635957/rank0.json.zip)

"
50213,tensorflow-gpu 2.2 works with CUDA 10.2 but requires cuDNN 7.6.4 which doesn't have a download file in NVIDIA archive for CUDA 10.2,"also documented here https://stackoverflow.com/questions/67931031/tensorflow-gpu-2-2-works-with-cuda-10-2-but-requires-cudnn-7-6-4-which-doesnt-h
<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 
```
$ lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.9.2009 (Core)
Release:	7.9.2009
Codename:	Core
```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): `pip`
- TensorFlow version (use command below): tensorflow-gpu 2.2
- Python version: `Python 3.8.5 (default, Mar 31 2021, 02:37:07)` 
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): `[GCC 7.3.1 20180303 (Red Hat 7.3.1-5)] on linux`
- CUDA/cuDNN version:
```
$ stat /usr/local/cuda
  File: ‘/usr/local/cuda’ -> ‘/usr/local/cuda-10.2’
  Size: 20        	Blocks: 0          IO Block: 4096   symbolic link
Device: fd00h/64768d	Inode: 67157410    Links: 1
Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Context: unconfined_u:object_r:usr_t:s0
Access: 2021-06-10 22:12:20.673080083 -0400
Modify: 2020-09-21 09:39:18.559883390 -0400
Change: 2020-09-21 09:39:18.559883390 -0400
 Birth: -
```
**I cannot directly figure what the cuDNN version is but the error states that cuDNN 7.4.2 is loaded.**
- GPU model and memory:
GeForce 1080 Ti (2x) each 12GB memory

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
`v2.2.0-rc4-8-g2b96f3662b 2.2.0`


**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


-----------------------------------------------------------------------------------------------------------------------------------------


**Describe the current behavior**

These are what I see from nvidia archive:
```
https://developer.nvidia.com/rdp/cudnn-archive

Download cuDNN v7.6.4 (September 27, 2019), for CUDA 10.1
Download cuDNN v7.6.4 (September 27, 2019), for CUDA 10.0
Download cuDNN v7.6.4 (September 27, 2019), for CUDA 9.2
Download cuDNN v7.6.4 (September 27, 2019), for CUDA 9.0
```
As you see there is no cuDNN for CUDA 10.2 however, I need to use CUDA 10.2 for the rest of my framework. tensorflow-gpu 2.2 works with CUDA 10.2 but I get this error which implies I need to use cuDNN 7.6.4 instead of 7.4.2

```
2021-06-10 22:03:04.201770: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2021-06-10 22:03:04.420481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2021-06-10 22:03:05.034154: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.4.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
2021-06-10 22:03:05.038684: E tensorflow/stream_executor/cuda/cuda_dnn.cc:319] Loaded runtime CuDNN library: 7.4.2 but source was compiled with: 7.6.4.  CuDNN library major and minor version needs to match or have higher minor version in case of CuDNN 7.0 or later version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.
```

Full log can be found here: https://pastebin.com/raw/0WQw8ktB"
50212, Rebuild TensorFlow with the appropriate compiler flags getting error,"2021-06-11 08:55:40.955563: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
 * Serving Flask app 'app' (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: on
 * Restarting with stat
2021-06-11 08:55:44.741287: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags."
50208,Tensorflow-2.5.0 does not build with tensorrt,"Tried to build tensorflow-2.5.0 with tensorrt but failed with
```
/home/bernard/opt/python38/tensorflow-2.5.0/tensorflow/compiler/tf2tensorrt/BUILD:39:11: C++ compilation of rule '//tensorflow/compiler/tf2tensorrt:tensorrt_stub' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/compiler/tf2tensorrt/_objs/tensorrt_stub/nvinfer_stub.pic.d ... (remaining 149 argument(s) skipped)
In file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:
./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:5:7: error: declaration of ‘void* createInferBuilder_INTERNAL(void*, int)’ has a different exception specifier
    5 | void* createInferBuilder_INTERNAL(void* logger, int version) {
      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:
bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:8083:30: note: from previous declaration ‘void* createInferBuilder_INTERNAL(void*, int32_t) noexcept’
 8083 | extern ""C"" TENSORRTAPI void* createInferBuilder_INTERNAL(void* logger, int32_t version) noexcept;
      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:
./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:12:7: error: declaration of ‘void* createInferRuntime_INTERNAL(void*, int)’ has a different exception specifier
   12 | void* createInferRuntime_INTERNAL(void* logger, int version) {
      |       ^~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,
                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:
bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2253:30: note: from previous declaration ‘void* createInferRuntime_INTERNAL(void*, int32_t) noexcept’
 2253 | extern ""C"" TENSORRTAPI void* createInferRuntime_INTERNAL(void* logger, int32_t version) noexcept;
      |                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~
In file included from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:56:
./tensorflow/compiler/tf2tensorrt/stub/NvInfer_5_0.inc:33:28: error: declaration of ‘nvinfer1::IPluginRegistry* getPluginRegistry()’ has a different exception specifier
   33 | nvinfer1::IPluginRegistry* getPluginRegistry() {
      |                            ^~~~~~~~~~~~~~~~~
In file included from bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInfer.h:54,
                 from tensorflow/compiler/tf2tensorrt/stub/nvinfer_stub.cc:17:
bazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers/third_party/tensorrt/NvInferRuntime.h:2264:51: note: from previous declaration ‘nvinfer1::IPluginRegistry* getPluginRegistry() noexcept’
 2264 | extern ""C"" TENSORRTAPI nvinfer1::IPluginRegistry* getPluginRegistry() noexcept;
      |                                                   ^~~~~~~~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 7109.058s, Critical Path: 184.50s
INFO: 23489 processes: 8232 internal, 15257 local.
FAILED: Build did NOT complete successfully

```
It builds successfully if tensorrt is not chosen when configure.

System Ubuntu 20.04

python-3.8.10

gcc 9.3.0

bazel 3.7.2

Nvidia GTX1070

cuda 11.0

tesorrt 8 

cudnn 8"
50205,Tensorflow Training Crashing,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.2
- Python version: 3.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.3
- GPU model and memory: Tesla K80 and 12GB

I have created a GCP VM with Tesla K80 GPU attached to it. I have installed Nvidia 465 drivers for Ubuntu 20.04 along with Cuda 11.

I am trying to use tensorflow on the GCP machine and each time when the training starts the machine crashes after few epochs. Here is the log

```
216/216 [==============================] - ETA: 0s - loss: 2.5774 - accuracy: 0.2203   
216/216 [==============================] - 173s 800ms/step - loss: 2.5774 - accuracy: 0.2203 - val_loss: 47.4114 - val_accuracy: 0.1372 - lr: 0.0100
Epoch 2/50
216/216 [==============================] - ETA: 0s - loss: 1.9055 - accuracy: 0.3265  
216/216 [==============================] - 137s 633ms/step - loss: 1.9055 - accuracy: 0.3265 - val_loss: 46.8945 - val_accuracy: 0.2023 - lr: 0.0100
Epoch 3/50
216/216 [==============================] - ETA: 0s - loss: 1.7601 - accuracy: 0.3899  
216/216 [==============================] - 137s 633ms/step - loss: 1.7601 - accuracy: 0.3899 - val_loss: 1.9010 - val_accuracy: 0.3895 - lr: 0.0100
Epoch 4/50
216/216 [==============================] - ETA: 0s - loss: 1.5993 - accuracy: 0.4417  
216/216 [==============================] - 137s 632ms/step - loss: 1.5993 - accuracy: 0.4417 - val_loss: 1.7880 - val_accuracy: 0.3919 - lr: 0.0100
Epoch 5/50
216/216 [==============================] - ETA: 0s - loss: 1.2965 - accuracy: 0.5580  
216/216 [==============================] - 134s 618ms/step - loss: 1.2965 - accuracy: 0.5580 - val_loss: 1.9468 - val_accuracy: 0.3919 - lr: 0.0100
Epoch 6/50
 60/216 [=======>......................] - ETA: 1:20 - loss: 1.0874 - accuracy: 0.63542021-06-10 19:12:36.997237: E tensorflow/stream_executor/cuda/cuda_event.cc:29] Error polling for event status: failed to query event: CUDA_ERROR_ILLEGAL_ADDRESS: an illegal memory access was encountered
2021-06-10 19:12:36.997296: F tensorflow/core/common_runtime/gpu/gpu_event_mgr.cc:273] Unexpected Event status: 1
Aborted (core dumped)

```

Please advise if you have run into a similar sort of error before.

"
50204,tensorflow removed numpy 1.20.3,"So I built tensorflow 2.5.0 successfully, but when I installed the wheel it removed my numpy 1.20.3 and downgraded it to 1.19.5 instead. 

Is this version pin necessary?"
50202,"How to ""tf.data.experimental.AutoShardPolicy.OFF"" if tf.data is not used? But gets this message while using tf.keras.model.fit.","**System information**
- TensorFlow version (you are using):2.5


**Describe the feature and the current behavior/state.**
How to ""tf.data.experimental.AutoShardPolicy.OFF"" if tf.data is not used? But gets this message while using tf.keras.model.fit.

"
50200,Mixed precision failed with OOM on A100 GPUs and Tensorflow 2.5,"We have a keras based code that we would like to run in mixed precision.

We enable the mixed precision computation with

```
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)
```

The code works when mixed precision policy is disabled.
The code works when mixed precision is enabled with Tensorflow 2.4 on a nvidia titan RTX cards
The code fails  with OOM error when mixed precision is enabled with Tensorflow 2.5 on A100 GPU even with a reduced batch size compared to float32 version

```
node06:632909:635501 [6] bazel-out/k8-opt/bin/external/nccl_archive/_virtual_includes/include_hdrs/alloc.h:41 NCCL WARN Cuda failure 'out of memory'
```"
50199,Subtract minimum area from AUC metric,"**System information**
- TensorFlow version (you are using): nightly
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**
Current behavior is to report ROC and PR AUC as a raw number.  Since the theoretical AUC for ROC of a chance classifier is 0.5 and for PR is the ratio of true positives to all data points, make this an option for reporting.

**Will this change the current api? How?**
This will add a new optional Boolean argument, subtract_minimum_area, to the dunder init.

**Who will benefit with this feature?**
Everyone who wants to know whether their classifier is better than flipping a balanced coin.

**Any Other info.**
Code is on my fork at https://github.com/brethvoice/tensorflow
"
50198,Massive memory allocations and reduced performance in GradientTape for loop and Optimizer when using jit_compile,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Fedora 34 Workstation Edition
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.5.0-rc2-14-gfcdf6593470 2.5.0-rc3
- Python version: 3.9.5
- GPU model and memory: Running on CPU
(If it is important the CPU is a 3900X and has 32GB of RAM)

I recently ran into issues with a custom training loop and `jit_compile` so i ran some small tests and found massive performance issues when enabling `jit_compile`. 

The code is a simple for loop inside a `tf.GradientTape` context (see below) where i accumulate a loss value. (Yes in this case i could batch the calls but that sadly isn't possible in the original issue i was talking about earlier)

In my test i ran the exact same training code once with `jit_compile` enabled and once with just using `tf.function`. I found that when enabling `jit_compile` the code runs approximately 1.7 times slower while consuming 12GB of memory as opposed to 1.4GB without `jit_compile`. I do not know if the slowdown is a direct result of the memory usage or due to other issues.
If i remove the `tf.GradientTape` the memory issue vanishes however the speed issue still persists. I also tried specifically setting the watched variables with no change.
If i leave the gradient tape in and just remove the call to the optimizer the memory usage drops to 8.8GB however performance is now over 25 times worse (16.5s instead of 0.6s). Which seems to suggest that both the optimizer and the GradientTape have problems.
Lastly i also tried setting a constant value directly in the range inside the for loop which also did not change the memory or performance issues.

I would expect the jit compiled code to at least have the same memory footprint as the not compiled version.

**The code:**
```
import tensorflow as tf
import timeit

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer((2, )),
    tf.keras.layers.Dense(2048, activation='elu'),
    tf.keras.layers.Dense(2048, activation='elu'),
    tf.keras.layers.Dense(1)
])

opt = tf.keras.optimizers.Adam()


@tf.function
def normal_func(x, y, k):
    print(""Tracing normal"")
    with tf.GradientTape() as tape:
        loss = tf.constant((0, ), dtype=tf.float32)
        for i in tf.range(k):
            geny = model(tf.expand_dims(x[i], axis=0), training=True)[0]
            loss = loss + tf.square(y[i] - geny)

    grads = tape.gradient(loss, model.trainable_variables)
    opt.apply_gradients(zip(grads, model.trainable_variables))
    return loss


@tf.function(jit_compile=True)
def jit_func(x, y, k):
    print(""Tracing jit"")
    with tf.GradientTape() as tape:
        loss = tf.constant((0, ), dtype=tf.float32)
        for i in tf.range(k):
            geny = model(tf.expand_dims(x[i], axis=0), training=True)[0]
            loss = loss + tf.square(y[i] - geny)

    grads = tape.gradient(loss, model.trainable_variables)
    opt.apply_gradients(zip(grads, model.trainable_variables))
    return loss


x_ = tf.reshape(tf.range(0, 1024, dtype=tf.float32), shape=(512, 2))
y_ = tf.reshape(tf.range(0, 512, dtype=tf.float32), shape=(512, ))

k_ = tf.constant(8)
k2_ = tf.constant(256)
k3_ = tf.constant(512)

# Sanity tests for retracing
normal_func(x_, y_, k_)
normal_func(x_, y_, k2_)
normal_func(x_, y_, k3_)

jit_func(x_, y_, k_)
jit_func(x_, y_, k2_)
jit_func(x_, y_, k3_)

# Actual performance tests
print('Runtime without jit_compile:')
print(timeit.timeit(lambda: normal_func(x_, y_, k_), number=10))
print(timeit.timeit(lambda: normal_func(x_, y_, k2_), number=10))
print(timeit.timeit(lambda: normal_func(x_, y_, k3_), number=10))

print('\nRuntime with jit_compile:')
print(timeit.timeit(lambda: jit_func(x_, y_, k_), number=10))
print(timeit.timeit(lambda: jit_func(x_, y_, k2_), number=10))
print(timeit.timeit(lambda: jit_func(x_, y_, k3_), number=10))
```

The output when running on my machine:
[output.log](https://github.com/tensorflow/tensorflow/files/6631656/output.log)"
50196,Overwhelming issues on Mac M1 with newest TF 2.5,"Several issues after installing the TensorFlow metal plugin for Mac M1 according to the provided [documentation](https://developer.apple.com/metal/tensorflow-plugin/):

- Model crashes when using Adam optimizer (logs will follow)
- Even on SGD optimizer, though it does not crash, the loss is as high as possible and does not change
- More issues, but since these were so fundamentally flawed, I think I'll just leave it at that

Here's a basic script, taken from a short [demo notebook](https://www.tensorflow.org/tutorials/quickstart/beginner):
```python
import tensorflow as tf

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

model = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(128, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10)
])

predictions = model(x_train[:1]).numpy()
tf.nn.softmax(predictions).numpy()

loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

loss_fn(y_train[:1], predictions).numpy()

model.compile(optimizer = 'sgd', loss = loss_fn)
model.fit(x_train, y_train, epochs=100)
```

Here's what one would expect the logs to look like (run on a Colab runtime):
```
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 [==============================] - 0s 0us/step
Epoch 1/5
1875/1875 [==============================] - 2s 1ms/step - loss: 0.7249
Epoch 2/5
1875/1875 [==============================] - 2s 1ms/step - loss: 0.3882
Epoch 3/5
1875/1875 [==============================] - 2s 1ms/step - loss: 0.3217
Epoch 4/5
1875/1875 [==============================] - 2s 1ms/step - loss: 0.2853
Epoch 5/5
1875/1875 [==============================] - 2s 1ms/step - loss: 0.2566
```

And here's what they look like when run on the Mac M1:
```
Init Plugin
Init Graph Optimizer
Init Kernel
Metal device set to: Apple M1
2021-06-10 08:05:36.921503: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-06-10 08:05:36.921605: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2021-06-10 08:05:37.062193: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-10 08:05:37.062396: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Epoch 1/5
2021-06-10 08:05:37.140640: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000
Epoch 2/5
1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000
Epoch 3/5
1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000
Epoch 4/5
1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000
Epoch 5/5
1875/1875 [==============================] - 5s 3ms/step - loss: 60000.0000
```

Here's what happens when you try to supplement that with the `'adam'` optimizer:
```
Init Plugin
Init Graph Optimizer
Init Kernel
Metal device set to: Apple M1
2021-06-10 08:07:02.300603: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2021-06-10 08:07:02.300696: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2021-06-10 08:07:02.375056: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-10 08:07:02.375226: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz
Epoch 1/5
2021-06-10 08:07:02.458202: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.
2021-06-10 08:07:02.487 python3[42618:4085236] -[MPSNDArrayIdentity encodeToCommandEncoder:commandBuffer:sourceArrays:resultState:destinationArray:kernelDAGObject:]: unrecognized selector sent to instance 0x1695be290
2021-06-10 08:07:02.490 python3[42618:4085236] *** Terminating app due to uncaught exception 'NSInvalidArgumentException', reason: '-[MPSNDArrayIdentity encodeToCommandEncoder:commandBuffer:sourceArrays:resultState:destinationArray:kernelDAGObject:]: unrecognized selector sent to instance 0x1695be290'
*** First throw call stack:
(
	0   CoreFoundation                      0x0000000199a4a320 __exceptionPreprocess + 240
	1   libobjc.A.dylib                     0x0000000199778c04 objc_exception_throw + 60
	2   CoreFoundation                      0x0000000199ad9020 -[NSObject(NSObject) __retain_OA] + 0
	3   CoreFoundation                      0x00000001999ac184 ___forwarding___ + 1444
	4   CoreFoundation                      0x00000001999abb30 _CF_forwarding_prep_0 + 96
	5   libmetal_plugin.dylib               0x0000000134a55db4 ___ZN12metal_plugin14MPSApplyAdamOpIfE7ComputeEPNS_15OpKernelContextE_block_invoke.115 + 92
	6   libdispatch.dylib                   0x0000000199725420 _dispatch_client_callout + 20
	7   libdispatch.dylib                   0x0000000199733a98 _dispatch_lane_barrier_sync_invoke_and_complete + 60
	8   libmetal_plugin.dylib               0x0000000134a533c0 _ZN12metal_plugin14MPSApplyAdamOpIfE7ComputeEPNS_15OpKernelContextE + 4884
	9   libmetal_plugin.dylib               0x0000000134a51e78 _ZN12metal_pluginL15ComputeOpKernelINS_14MPSApplyAdamOpIfEEEEvPvP18TF_OpKernelContext + 44
	10  _pywrap_tensorflow_internal.so      0x0000000148076460 _ZN10tensorflow15PluggableDevice7ComputeEPNS_8OpKernelEPNS_15OpKernelContextE + 148
	11  libtensorflow_framework.2.dylib     0x0000000115d78664 _ZN10tensorflow12_GLOBAL__N_113ExecutorStateINS_15PropagatorStateEE7ProcessENS2_10TaggedNodeEx + 3080
	12  libtensorflow_framework.2.dylib     0x0000000115d79dbc _ZNSt3__110__function6__funcIZN10tensorflow12_GLOBAL__N_113ExecutorStateINS2_15PropagatorStateEE7RunTaskIZNS6_13ScheduleReadyEPN4absl14lts_2020_09_2313InlinedVectorINS5_10TaggedNodeELm8ENS_9allocatorISB_EEEEPNS5_20TaggedNodeReadyQueueEEUlvE0_EEvOT_EUlvE_NSC_ISL_EEFvvEEclEv + 56
	13  _pywrap_tensorflow_internal.so      0x00000001487aa230 _ZN5Eigen15ThreadPoolTemplIN10tensorflow6thread16EigenEnvironmentEE10WorkerLoopEi + 1508
	14  _pywrap_tensorflow_internal.so      0x00000001487a9b28 _ZZN10tensorflow6thread16EigenEnvironment12CreateThreadENSt3__18functionIFvvEEEENKUlvE_clEv + 80
	15  libtensorflow_framework.2.dylib     0x0000000116314b08 _ZN10tensorflow12_GLOBAL__N_17PThread8ThreadFnEPv + 120
	16  libsystem_pthread.dylib             0x00000001998d206c _pthread_start + 320
	17  libsystem_pthread.dylib             0x00000001998ccda0 thread_start + 8
)
libc++abi.dylib: terminating with uncaught exception of type NSException
[1]    42618 abort      python3 tf-demo.py
```

Run on both Python 3.8 and Python 3.9, the same errors occur."
50194,Error while converting .pb file into tflite ,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed using pip3



**Provide the text output from tflite_convert**

```
Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: FULLY_CONNECTED, SOFTMAX. Here is a list of operators for which you will need custom implementations: AudioSpectrogram, DecodeWav, Mfcc. 
```

**Standalone code to reproduce the issue** 
tflite_convert --output_file=./model_converted.tflite --graph_def_file=./Pretrained_models/DNN/DNN_L.pb --input_shapes=0 --input_arrays=wav_data --output_arrays=labels_softmax --enable_select_tf_ops.

Link of model: https://github.com/ARM-software/ML-KWS-for-MCU/tree/master/Pretrained_models


"
50193,Why tensorflow convert the strings to bytes when using from_tensor_slices,"tensorflow version: 2.5.0
ds.df is a pandas dataframe whose content are sentences.


```
dstf = tf.data.Dataset.from_tensor_slices((ds.df['content'].values, ds.df['label'].values))
dstf = dstf.shuffle(buffer_size=10000).batch(32)
for mm in dstf.map(lambda x, y: (x, y) ).take(5):
    print(mm)
```
however, it converts the `content` to bytes.
How to avoid this? 

![image](https://user-images.githubusercontent.com/26405281/121509758-8b993980-ca19-11eb-87e7-0f7db8eaddda.png)

"
50192,Unable to install tensorflow 2.5.0 on jetson nano,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04, Jetpack 4.5.1
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): automated pip install (see log below)
- TensorFlow version: 2.5.0
- Python version: 3.7.5
- Installed using virtualenv? pip? conda?: python3-venv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): gcc (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: 10.2
- GPU model and memory: Nvidia

**Describe the problem**
Unable to install tensorflow 2.5.0 on nvidia jetson nano. I'm running the installation inside a virtual environment.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I followed https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md
I was at the step: python -m pip install --use-feature=2020-resolver ., which is ""python -m pip install  ."" with the new version of pip.

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Processing /home/leap/workspace/models/research
Requirement already satisfied: Cython in /home/leap/workspace/tf/lib/python3.7/site-packages (from object-detection==0.1)
Collecting apache-beam (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/dd/46/145223f4f89c6b2db0256d3770e28d02ab88fb8b700a84afa89a343cd094/apache-beam-2.30.0.zip
Collecting avro-python3 (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/cc/97/7a6970380ca8db9139a3cc0b0e3e0dd3e4bc584fb3644e1d06e71e1a55f0/avro-python3-1.10.2.tar.gz
Collecting contextlib2 (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl
Collecting lvis (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/72/b6/1992240ab48310b5360bfdd1d53163f43bb97d90dc5dc723c67d41c38e78/lvis-0.5.3-py3-none-any.whl
Collecting lxml (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/e5/21/a2e4517e3d216f0051687eea3d3317557bde68736f038a3b105ac3809247/lxml-4.6.3.tar.gz
Collecting matplotlib (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/60/d3/286925802edaeb0b8834425ad97c9564ff679eb4208a184533969aa5fc29/matplotlib-3.4.2.tar.gz
Collecting pandas (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/e8/81/f7be049fe887865200a0450b137f2c574647b9154503865502cfd720ab5d/pandas-1.2.4.tar.gz
Collecting pillow (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/21/23/af6bac2a601be6670064a817273d4190b79df6f74d8012926a39bc7aa77f/Pillow-8.2.0.tar.gz
Collecting pycocotools (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/de/df/056875d697c45182ed6d2ae21f62015896fdb841906fe48e7268e791c467/pycocotools-2.0.2.tar.gz
Collecting scipy (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/fe/fd/8704c7b7b34cdac850485e638346025ca57c5a859934b9aa1be5399b33b7/scipy-1.6.3.tar.gz
Collecting six (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/d9/5a/e7c31adbe875f2abbb91bd84cf2dc52d792b5a01506781dbcf25c91daf11/six-1.16.0-py2.py3-none-any.whl
Collecting tf-models-official (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/96/08/81bbc275e8e9c6d1e03dd26daec3a67f45e6322804cbce3d51f93eae1961/tf_models_official-2.5.0-py2.py3-none-any.whl
Collecting tf-slim (from object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/02/97/b0f4a64df018ca018cc035d44f2ef08f91e2e8aa67271f6f19633a015ff7/tf_slim-1.1.0-py2.py3-none-any.whl
Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz
Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz
Collecting fastavro<2,>=0.21.4 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/6b/74/ea2e40fb661dcfcca3ab7744f8719869954514c69c1be69b409393860668/fastavro-1.4.1.tar.gz
Collecting future<1.0.0,>=0.18.2 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz
Collecting grpcio<2,>=1.29.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/8c/34/7dafc9052bd9b2b41c5a8912aeeca01e179d16de17e9c275633d4b807330/grpcio-1.38.0.tar.gz
Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/08/f7/4c3fad73123a24d7394b6f40d1ec9c1cbf2e921cfea1797216ffd0a51fb1/hdfs-2.6.0-py3-none-any.whl
Collecting httplib2<0.20.0,>=0.8 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/15/dc/d14bce03f4bfd0214b90a3f556d7c96f75bb94ad597c816a641b962f22e9/httplib2-0.19.1-py3-none-any.whl
Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /home/leap/workspace/tf/lib/python3.7/site-packages (from apache-beam->object-detection==0.1)
Collecting oauth2client<5,>=2.0.1 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/95/a9/4f25a14d23f0786b64875b91784607c2277eff25d48f915e39ff0cff505a/oauth2client-4.1.3-py2.py3-none-any.whl
Collecting protobuf<4,>=3.12.2 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/d5/e0/20ba06eb42155cdb4c741e5caf9946e4569e26d71165abaecada18c58603/protobuf-3.17.3-py2.py3-none-any.whl
Collecting pyarrow<4.0.0,>=0.15.1 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/62/d3/a482d8a4039bf931ed6388308f0cc0541d0cab46f0bbff7c897a74f1c576/pyarrow-3.0.0.tar.gz
Collecting pydot<2,>=1.2.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/ea/76/75b1bb82e9bad3e3d656556eaa353d8cd17c4254393b08ec9786ac8ed273/pydot-1.4.2-py2.py3-none-any.whl
Collecting pymongo<4.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/c4/2f/79e933655adcf6dbd00738b556cecae5f8ec709301ac10df6f488d83bb53/pymongo-3.11.4.tar.gz
Collecting python-dateutil<3,>=2.8.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/d4/70/d60450c3dd48ef87586924207ae8907090de0b306af2bce5d134d78615cb/python_dateutil-2.8.1-py2.py3-none-any.whl
Collecting pytz>=2018.3 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/70/94/784178ca5dd892a98f113cdd923372024dc04b8d40abe77ca76b5fb90ca6/pytz-2021.1-py2.py3-none-any.whl
Collecting requests<3.0.0,>=2.24.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl
Collecting typing-extensions<3.8.0,>=3.7.0 (from apache-beam->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl
Collecting kiwisolver>=1.1.0 (from lvis->object-detection==0.1)
  Downloading https://files.pythonhosted.org/packages/90/55/399ab9f2e171047d28933ae4b686d9382d17e6c09a01bead4a6f6b5038f4/kiwisolver-1.3.1.tar.gz (53kB)
    100% |████████████████████████████████| 61kB 2.1MB/s 
Collecting opencv-python>=4.1.0.25 (from lvis->object-detection==0.1)
  Using cached https://files.pythonhosted.org/packages/bb/08/9dbc183a3ac6baa95fabf749ddb531bd26256edfff5b6c2195eca26258e9/opencv-python-4.5.1.48.tar.gz
Requirement already satisfied: pyparsing>=2.4.0 in /home/leap/workspace/tf/lib/python3.7/site-packages (from lvis->object-detection==0.1)
Collecting cycler>=0.10.0 (from lvis->object-detection==0.1)
  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl
Requirement already satisfied: setuptools>=18.0 in /home/leap/workspace/tf/lib/python3.7/site-packages (from pycocotools->object-detection==0.1)
Collecting tensorflow>=2.5.0 (from tf-models-official->object-detection==0.1)
  Could not find a version that satisfies the requirement tensorflow>=2.5.0 (from tf-models-official->object-detection==0.1) (from versions: )
No matching distribution found for tensorflow>=2.5.0 (from tf-models-official->object-detection==0.1)


Thanks for the help.

Kashyap"
50190,Code in the Documentation is resulting in Warning Message,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch#using_the_gradienttape_a_first_end-to-end_example

## Description of issue (what needs changing): 
Need to modify the code such that Warning goes away.

Please find the [Github Gist](https://colab.research.google.com/gist/worldpeaceaspirer/73e9f295a64cb513d6b7cd3aefdc2c38/warning_in_the_tutorial.ipynb) that demonstrates the Warning."
50189,Does tf.math.equal supports tf.sparse.SparseTensor?,"## URL with the issue:
https://www.tensorflow.org/api_docs/python/tf/math/equal

## Description of issue (what needs changing):
The doc says `tf.math.equal` supports sparse tensor as inputs, but it is actually not supported at the moment. See the example below. I haven't checked all math operations that claims to support sparse tensors, maybe there are other similar doc errors like this.

I'm using tf `2.6.0-dev20210601`.

### Usage example
```python
import tensorflow as tf

a = tf.sparse.SparseTensor(
    indices=[[0, 0], [0, 1], [1, 2]],
    values=[1, 1, 1],
    dense_shape=[2, 3]
)

b = tf.sparse.SparseTensor(
    indices=[[0, 0], [0, 1], [1, 2]],
    values=[1, 1, 1],
    dense_shape=[2, 3]
)

tf.math.equal(a, b)  # raises ValueError
```

I think making `equal` operation supports sparse tensor is needed.

"
50188,How to use TF.data.Dataset for relational database like nuScenes,"**System information**
- TensorFlow version (you are using): tensorflow==2.5.0 (pip installed, Ubuntu 20.04, CUDA 11.)

**Describe the feature and the current behavior/state.**
tf.data.Dataset is excellent while implementing the data-pipeline, with map, shuffle, batch and prefetch (inducing AUTOTUNE features) . But currently the relational database like nuScenes (https://www.nuscenes.org/nuscenes#data-format). has difficulty to implement with multiple sensor data like camera images and pointCloud's. Please guide or inform the steps to be taken, with examples.

**Will this change the current api? How?**

**Who will benefit with this feature?**
All relational database like nuScenes can use the tf.data.Dataset and also perform better while using the distributed training. 
"
50187,"When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'","When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'. But when we try to send one tuple that includes one more parameters to the net instead of parameters directly,the net worked smoothly.Could someone tells me the reason?"
50186,"When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'. ","When we save the model with the api of model.save, received the error of 'TpyeError call() missing 1 required positional argument'. But when we try to send one tuple that includes one more parameters to the net instead of parameters directly,the net worked smoothly.Could someone tells me the reason?   "
50182,Unable to read TFLite after conversion,"### Problem Statement
I tried converting a frozen garph - https://github.com/blaueck/tf-mtcnn/blob/master/mtcnn.pb to tflite using the TF v2.5.0. The  conversion was successful, but I am unable to load the generated TFLite for inference.

### Code for conversion to tflite
```
import tensorflow as tf

input_arrays = ['input', 'min_size', 'thresholds', 'factor']

output_node_names = ['prob', 'landmarks', 'box']  # Output nodes

graph_def_file = 'mtcnn.pb'


converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(graph_def_file, input_arrays, output_node_names)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.
    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.
]

converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

if __name__ == ""__main__"":
    with tf.io.gfile.GFile('mtcnn.tflite', 'wb') as f:
        f.write(tflite_model)
```

### Code for loading TFLite 
```
    model_path = r'mtcnn.tflite'
    interpreter = tf.lite.Interpreter(model_path=model_path)
```

### Error while loading TFLite
```
File ""C:\Users\a84191678\Anaconda3\lib\site-packages\tensorflow\lite\python\interpreter.py"", line 348, in __init__
    _interpreter_wrapper.CreateWrapperFromFile(
ValueError: Did not get operators or tensors in subgraph 1.
```

### System information
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **TensorFlow installed from (source or binary)**: pip
-   **TensorFlow version (use command below)**:2.5.0
-   **Python version**:3.8

No CUDA/GPU.
"
50180,TFLite: `Segmentation fault (core dumped)` when converting `CropAndResize` operation,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from: pip
- TensorFlow version: 2.4.1
- Python version: 3.6.9

**Describe the current behavior**
I am having trouble converting the `tf.image.crop_and_resize` operation into TFLite when I use the `representative_dataset`. It can be converted without it but when I add it, a `Segmentation fault (core dumped)` error is raised during the conversion. Thank you for your upcoming answer.

**Describe the expected behavior**
The layer and the model created around the layer is working well in Tensorflow, the error seems to occur only during the conversion.

**Standalone code to reproduce the issue**
```
import tensorflow as tf

IMG_SIZE = 128
NUM_BOXES = 100
CROP_SIZE = 28
NB_DATA_SAMPLES = 32
BATCH_SIZE = 1


class CropLayer(tf.keras.layers.Layer):
    def __init__(self):
        super().__init__()

    def call(self, inputs, **kwargs):
        images_, boxes_ = inputs
        box_indices = tf.reshape(
            tf.repeat(
                tf.expand_dims(tf.range(tf.shape(images_)[0], dtype=tf.int32), axis=-1),
                NUM_BOXES,
                axis=-1
            ),
            shape=(-1,)
        )
        cropped_images = tf.image.crop_and_resize(
            image=images_,
            boxes=tf.reshape(boxes_, (-1, 4)),
            box_indices=box_indices,
            crop_size=(CROP_SIZE, CROP_SIZE))
        return cropped_images


images = tf.random.normal(
    shape=(NB_DATA_SAMPLES, IMG_SIZE, IMG_SIZE, 3))
boxes = tf.random.uniform((NB_DATA_SAMPLES, NUM_BOXES, 4), maxval=1)

layer = CropLayer()
# Ensure that it is working, should be (NB_DATA_SAMPLES * NUM_BOXES, CROP_SIZE, CROP_SIZE, 3)
print('Should be (NB_DATA_SAMPLES * NUM_BOXES, CROP_SIZE, CROP_SIZE, 3), i.e. (3200, 28,28, 3):')
print(layer([images, boxes]).shape)

inputs_model = [
    tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3), batch_size=BATCH_SIZE),
    tf.keras.Input(shape=(NUM_BOXES, 4), batch_size=BATCH_SIZE)
]

model = tf.keras.models.Model(inputs=inputs_model, outputs=layer(inputs_model))


def representative_dataset_generator():
    for image, bboxes in zip(images.numpy(), boxes.numpy()):
        image_ = tf.expand_dims(image, 0)
        bboxes_ = tf.expand_dims(bboxes, 0)
        yield [image_, bboxes_]

# Run the model on the representative dataset generator
for sample in representative_dataset_generator():
    model(sample).shape

# Converter
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]

converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS
]

print('Converting without representative_dataset')
quant_model = converter.convert()
with open('model.tflite', ""wb"") as f:
    f.write(quant_model)
print('Converted and saved')

print('Converting with representative_dataset')
converter.representative_dataset = representative_dataset_generator
quant_model = converter.convert()
```

**Other information**
TF 2.5.0 does not fix the issue, neither does TF-nightly 2.6.0-dev20210607"
50178,What could explain that MultiWorkerMirroredStrategy is very slow ?,"We are trying to use a multi-worker strategy with Keras and tensorflow 2.5. We have 2 hosts with 8 A100 GPU on each host.

Our code looks like the following

```
strategy = tf.distribute.MultiWorkerMirroredStrategy()

with strategy.scope():
    model = build_model()
    model.compile(...)

train_data = build_train_data()
train_data = train_data.take(1).cache().repeat()

model.fit(train_data)
```

As you notice in this pseudo code, we cache one batch of our dataset to reduce the I/O bound and to measure precisely the training time.

The code where run on two nodes is very slow. All GPUs are almost idle every time.

When we run the same code on a single node (8 GPUs) with MirroredStrategy we get a very high speed.

So, what can be the bottleneck here ?"
50176,"Whose bug is this: gcc, MKL, or TF?","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: master
- Python version: 3.9
- Installed using virtualenv? pip? conda?:conda
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): gcc (Debian 11.1.0-2) 11.1.0
- CUDA/cuDNN version: NA
- GPU model and memory: NA



**Describe the problem**
TF build fails with a strange error:
```shell
external/mkl_dnn_v1/src/common/primitive_cache.cpp: In member function 'virtual void dnnl::impl::lru_primitive_cache_t::update_entry(const key_t&, const dnnl::impl::primitive_desc_t*)':
external/mkl_dnn_v1/src/common/primitive_cache.cpp:155:60: error: no match for 'operator!=' (operand types are 'const std::thread::id' and 'const std::thread::id')
  155 |     if (it == cache_mapper_.end() || it->first.thread_id() != key.thread_id())
      |                                      ~~~~~~~~~~~~~~~~~~~~~ ^~ ~~~~~~~~~~~~~~~
      |                                                         |                  |
      |                                                         |                  const std::thread::id
      |                                                         const std::thread::id
In file included from /usr/include/c++/11/utility:70,

```

It looks that the `!=`  operator is not defined for `std::thread::id`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
I am building with MKL:
```
bazel build --verbose_failures --config=mkl --config=v2 --config=nogcp --config=nonccl  -c opt --copt=-march=native --copt=""-O3"" -s //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50173,Error while using tf.io.encode_jpeg,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Colab
- TensorFlow version (use command below): 2.5
- Python version: 3.x
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior** : Using **`tf.io.encode_jpeg`** is resulting in error,

```python
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-3-7f8b07d2c90d> in <module>()
      1 import tensorflow as tf
      2 
----> 3 tf.io.encode_jpeg('Developing_1.jpeg')

8 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)
     96       dtype = dtypes.as_dtype(dtype).as_datatype_enum
     97   ctx.ensure_initialized()
---> 98   return ops.EagerTensor(value, ctx.device_name, dtype)
     99 
    100 

TypeError: Cannot convert 'Developing_1.jpeg' to EagerTensor of dtype uint8
```

**Describe the expected behavior** : There should be no error

**Standalone code to reproduce the issue**  : 

```python
import tensorflow as tf

tf.io.encode_jpeg('Any_Image.jpeg')
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50172,batch normalization folding issue in Conv2d with dilation>1,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04
- TensorFlow version (use command below):
- Python version: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0

**Describe the current behavior**
I'm converting a simple Conv2d->BatchNorm->ReLU network to TFLite. In previous versions, the BatchNorm would be folded into the Conv2D weights to avoid the extra quantization. In TF 2.5.0, Conv2D with dilation>1 is applied with space2batch - batch2space nodes, which seems to confuse the converter.

**Describe the expected behavior**
The expected behaviour should be to fold the BatchNorm weights into the Conv2D weights (The same way as in TF 2.4.1)

**Standalone code to reproduce the issue**
```
import tensorflow as tf
import numpy as np


def representative_dataset():
    for _ in range(10):
        data = np.random.rand(1, 244, 244, 3)
        yield [data.astype(np.float32)]


dil = 2
for ptq in [False, True]:
    _in = tf.keras.Input((244, 244, 3))
    if ptq:
        x = _in
    else:
        x = tf.quantization.fake_quant_with_min_max_args(_in)

    x = tf.keras.layers.Conv2D(16, 3, padding='same', dilation_rate=(dil, dil), use_bias=False)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    if not ptq:
        x = tf.quantization.fake_quant_with_min_max_args(x)
    model = tf.keras.Model(_in, x)

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    if ptq:
        converter.representative_dataset = representative_dataset
    tflite_quant_model = converter.convert()

    s = '_ptq' if ptq else ''
    tflite_file = f'/tmp/dilated_conv2d/dil{dil}{s}{c}.tflite'
    with open(tflite_file, 'wb') as f:
        f.write(tflite_quant_model)

    print(f'Generated {tflite_file} with TF {tf.__version__}')

```
TF 2.5.0: Quantizting with fake_quant nodes
![dil2](https://user-images.githubusercontent.com/78862769/121343752-b1e0ab80-c92b-11eb-88a7-5a53e5583b89.png)

TF 2.5.0: Quantizting without fake_quant nodes
![dil2_ptq](https://user-images.githubusercontent.com/78862769/121343757-b311d880-c92b-11eb-94e6-408be513cc00.png)

TF 2.4.1: Quantizting without fake_quant nodes
![dil2_ptq_TF241](https://user-images.githubusercontent.com/78862769/121343755-b2794200-c92b-11eb-83c9-b86e0e506d8b.png)

(Images generated with Netron: https://netron.app/)
"
50171,EfficientNet fails to load with float16 policy on TF 2.5.0.,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary.
- TensorFlow version (use command below): v2.5.0-0-ga4dfb8d1a71 2.5.0
- Python version: 3.7.10
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: 11.0 / 8.0.4
- GPU model and memory: Nvidia Tesla T4, 15109MiB 

**Describe the current behavior**
The model cannot be created when using the mixed precision policy.

```python
import tensorflow as tf

policy = tf.keras.mixed_precision.Policy(""mixed_float16"")
tf.keras.mixed_precision.set_global_policy(policy)

model = tf.keras.applications.efficientnet.EfficientNetB0()
```

The above will raise `ValueError` : `Tensor conversion requested dtype float16 for Tensor with dtype float32: <tf.Tensor 'normalization_1/Cast:0' shape=(None, 224, 224, 3) dtype=float32>`

Full error below.


**Describe the expected behavior**
The model should work in mixed policy - I remember it used to work in TF 2.4.1, perhaps some changes were made?
```python
!pip uninstall tensorlfow
!pip install tensorflow==2.4.1
policy = tf.keras.mixed_precision.Policy(""mixed_float16"")
tf.keras.mixed_precision.set_global_policy(policy)

model = tf.keras.applications.efficientnet.EfficientNetB0()  # OK
```

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - yes, BUT: I do not exactly know which part causes the issue (I assume the problem is in the Normalization layer). I could try to fix the issue with some help provided.

**Standalone code to reproduce the issue**
```python
import tensorflow as tf

policy = tf.keras.mixed_precision.Policy(""mixed_float16"")
tf.keras.mixed_precision.set_global_policy(policy)

model = tf.keras.applications.efficientnet.EfficientNetB0()
```

**Other info / logs** 
Full error message:
```python
```python
ValueError                                Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y)
   1244             r_op = getattr(y, ""__r%s__"" % op_name)
-> 1245             out = r_op(x)
   1246             if out is NotImplemented:

17 frames
ValueError: Tensor conversion requested dtype float16 for Tensor with dtype float32: <tf.Tensor 'normalization_1/Cast:0' shape=(None, 224, 224, 3) dtype=float32>

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(op_type_name, name, **keywords)
    556                 ""%s type %s of argument '%s'."" %
    557                 (prefix, dtypes.as_dtype(attrs[input_arg.type_attr]).name,
--> 558                  inferred_from[input_arg.type_attr]))
    559 
    560         types = [values.dtype]

TypeError: Input 'y' of 'Sub' Op has type float16 that does not match type float32 of argument 'x'.
```

Best regards,
Sebastian
"
50170,"tf.Variable cannot be used inside the gradient function of a tf.custom_gradient(), when tf.cond() and graph mode are used","**System information**
- Have I written custom code: yes
- OS Platform and Distribution: Red Hat Enterprise Linux release 8.2 (Ootpa)
- TensorFlow installed from: binary
- TensorFlow version: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.6.8

**Describe the current behavior**
`tf.Variable` cannot be used inside the gradient function of a `tf.custom_gradient()`, when `tf.cond()` and graph mode are used. Trying to do so results in a crash.

**Describe the expected behavior**
It should be possible to use such objects in this specific case, and there should be not crash.

**Standalone code to reproduce the issue**

```python3
import tensorflow as tf

w = tf.Variable(2.0, trainable=False)

@tf.function
def test(x):
    @tf.custom_gradient
    def multiply_by_w(x):
        y = w * x
        def grad_fn(grad_y):
            return grad_y * w
        return (y, grad_fn)

    with tf.GradientTape() as tape:
        tape.watch(x)
        y = tf.cond(tf.constant(True), lambda: multiply_by_w(x), lambda: x)

    return tape.gradient(y, x)

test(tf.constant(5.0))
```
**Other info / logs**

Running the code above gives the following traceback:
```
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-1-a323f51f0fc4> in <module>
     18     return tape.gradient(y, x)
     19 
---> 20 test(tf.constant(5.0))

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in __call__(self, *args, **kwds)
    887 
    888       with OptionalXlaContext(self._jit_compile):
--> 889         result = self._call(*args, **kwds)
    890 
    891       new_tracing_count = self.experimental_get_tracing_count()

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in _call(self, *args, **kwds)
    931       # This is the first call of __call__, so we have to initialize.
    932       initializers = []
--> 933       self._initialize(args, kwds, add_initializers_to=initializers)
    934     finally:
    935       # At this point we know that the initialization is complete (or less

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in _initialize(self, args, kwds, add_initializers_to)
    762     self._concrete_stateful_fn = (
    763         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
--> 764             *args, **kwds))
    765 
    766     def invalid_creator_scope(*unused_args, **unused_kwds):

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/function.py in _get_concrete_function_internal_garbage_collected(self, *args, **kwargs)
   3048       args, kwargs = None, None
   3049     with self._lock:
-> 3050       graph_function, _ = self._maybe_define_function(args, kwargs)
   3051     return graph_function
   3052 

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/function.py in _maybe_define_function(self, args, kwargs)
   3442 
   3443           self._function_cache.missed.add(call_context_key)
-> 3444           graph_function = self._create_graph_function(args, kwargs)
   3445           self._function_cache.primary[cache_key] = graph_function
   3446 

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/function.py in _create_graph_function(self, args, kwargs, override_flat_arg_shapes)
   3287             arg_names=arg_names,
   3288             override_flat_arg_shapes=override_flat_arg_shapes,
-> 3289             capture_by_value=self._capture_by_value),
   3290         self._function_attributes,
   3291         function_spec=self.function_spec,

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/framework/func_graph.py in func_graph_from_py_func(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)
    997         _, original_func = tf_decorator.unwrap(python_func)
    998 
--> 999       func_outputs = python_func(*func_args, **func_kwargs)
   1000 
   1001       # invariant: `func_outputs` contains only Tensors, CompositeTensors,

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/eager/def_function.py in wrapped_fn(*args, **kwds)
    670         # the function a weak reference to itself to avoid a reference cycle.
    671         with OptionalXlaContext(compile_with_xla):
--> 672           out = weak_wrapped_fn().__wrapped__(*args, **kwds)
    673         return out
    674 

~/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/framework/func_graph.py in wrapper(*args, **kwargs)
    984           except Exception as e:  # pylint:disable=broad-except
    985             if hasattr(e, ""ag_error_metadata""):
--> 986               raise e.ag_error_metadata.to_exception(e)
    987             else:
    988               raise

TypeError: in user code:

    <ipython-input-1-a323f51f0fc4>:11 grad_fn  *
        return grad_y * w
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1250 binary_op_wrapper
        raise e
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1234 binary_op_wrapper
        return func(x, y, name=name)
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:1575 _mul_dispatch
        return multiply(x, y, name=name)
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/math_ops.py:530 multiply
        return gen_math_ops.mul(x, y, name)
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py:6250 mul
        ""Mul"", x=x, y=y, name=name)
    /home/loic/venv/tf_2.5.0/lib64/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:531 _apply_op_helper
        repr(values), type(values).__name__, err))

    TypeError: Expected float32 passed to parameter 'y' of op 'Mul', got <tf.Variable 'Variable:0' shape=() dtype=float32> of type 'ResourceVariable' instead. Error: Expected resource passed to parameter 'resource' of op 'ReadVariableOp', got <tf.Tensor: shape=(), dtype=resource, numpy=<unprintable>> of type 'EagerTensor' instead. Error: _capture_helper() takes 3 positional arguments but 4 were given
```
Some observations:
- The code crashes inside `grad_fn()` in operation `grad_y * w` because `w` is of unexpected type.
- No crash when not using `tf.custom_gradient`, i.e. by using `multiply_by_w = lambda x: w * x` instead.
- No crash when removing `tf.cond()`, i.e., by replacing `y = tf.cond(tf.constant(True), lambda: multiply_by_w(x), lambda: x)` by `y = multiply_by_w(x)`.
- No crash in eager mode (i.e. by commenting out the `tf.function` decorator).
"
50169,Add weight to ```tf.keras.layers.experimental.preprocessing.TextVectorization``` ,"**System information**
- TensorFlow version (you are using): tensorflow2.5.0
- Are you willing to contribute it (Yes/No): No



**Describe the feature and the current behavior/state.**
```tf.keras.layers.experimental.preprocessing.TextVectorization``` is a very useful layer for nlp project , but it can't deal with a string which contains weight like this : 
```
This:0.1 is:0.2 a:0.3 tensorflow:0.25 issue:0.5 . 
```
So my feature request is that add a parameter like ```is_contain_weight``` to ```tf.keras.layers.experimental.preprocessing.TextVectorization``` . When the parameter is ```True``` , ```tf.keras.layers.experimental.preprocessing.TextVectorization``` will return both word index and word weight
**Will this change the current api? How?**
No , since we can set the default value of ```is_contain_weight``` to ```False```

**Who will benefit with this feature?**
Anyone who want input weight with text to ```tf.keras``` model

"
50168,Conv2DTranspose output shapes vary with output_padding=0 and output_padding=None,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5
- Python version: 3.8.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
In the [Conv2DTranspose documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose), the following formula is used to calculate ```new_rows``` and ```new_cols``` if ```output_padding``` is specified:
```
new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
```
For the following settings: 
```input  shape=(128, 128, 3)```
```filters=32, kernel_size=(4,4), strides=(2,2), padding='same', output_padding=(0,0)``` ,

the above mentioned formula gives ```new_rows=256``` and ```new_cols=256``` (which is actually the **expected shape**).

I defined a function to calculate the shape of ```new_rows``` and ```new_cols``` using the above formula to verify the output shape:
```
def compute_shape(rows=128, cols=128, strides=(2, 2), kernel_size=(4, 4),
                      padding=(0, 0), output_padding=(0, 0)):
    new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
                    output_padding[0])
    new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
                    output_padding[1])
    return new_rows, new_cols

new_shape = compute_shape(padding=(1, 1))
print(new_shape)
```

Now if I specify ```output_padding=0``` for Conv2DTranspose layer for the same settings mentioned above , I get the output shape as ```(254, 254, 32)```.

Output:
```
Model: ""model""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 128, 128, 3)]     0         
_________________________________________________________________
conv2d_transpose (Conv2DTran (None, 254, 254, 32)      1568      
=================================================================
Total params: 1,568
Trainable params: 1,568
Non-trainable params: 0
_________________________________________________________________
```

**Describe the expected behavior**
If I don't specify ```output_padding``` or use ```output_padding=None```, I get the expected output shape ```(256, 256, 32)```.


Why does the **difference arise in the output shape**  when ```output_padding=None``` and ```output_padding=0```? Doesn't ```output_padding=None``` and ```output_padding=0``` mean the same thing? Shouldn't both cases ideally give the **same output**?

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
```
from tensorflow.keras.layers import Input, Conv2DTranspose
from tensorflow.keras import Model

 x = Input((128, 128, 3))
 y = Conv2DTranspose(filters=32, kernel_size=4, strides=2, padding='same', output_padding=0)(x)
 model = Model(x, y)
 model.summary()

 def compute_shape(rows=128, cols=128, strides=(2, 2), kernel_size=(4, 4),
                      padding=(0, 0), output_padding=(0, 0)):
     new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
                    output_padding[0])
     new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
                    output_padding[1])
     return new_rows, new_cols

 new_shape = compute_shape(padding=(1, 1))
 print(new_shape)
```
**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50167,tensorflow load saved_model throw error: google.protobuf.message.DecodeError: Error parsing message,"Background: i set up a new MBP with MacOS Big Sur 11.4 then my TensorFlow model loading script is not working anymore (which works previously and still works on my another laptop), i tried to set up exactly same python virtual environment, and even use exactly same Dockerfile, still one is working and the other is not working...

So apparently this is OS setup issue, i just cannot figure out what is wrong, and already hit the wall for days....

**Pls help to point out what potential stuffs i can check to fix this...**

(and if possible pls don't close, as i already asked this question in StackOverflow yesterday: https://stackoverflow.com/q/67886470/7658313 and till now no answer yet)

And the other confusion is: how come within the docker container also one laptop is working and another is not working, so it must be some physical host OS stuff can even affect the docker container...

The error trace stack:
```
Traceback (most recent call last):
  File ""/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py"", line 68, in parse_saved_model
    saved_model.ParseFromString(file_content)
google.protobuf.message.DecodeError: Error parsing message

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""naive_tf1_model_loader.py"", line 55, in <module>
    ""/tf1_models/object_detection/1""
  File ""naive_tf1_model_loader.py"", line 14, in __init__
    self._sess, [tf.saved_model.SERVING], model_path
  File ""/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/util/deprecation.py"", line 324, in new_func
    return func(*args, **kwargs)
  File ""/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py"", line 268, in load
    loader = SavedModelLoader(export_dir)
  File ""/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py"", line 284, in __init__
    self._saved_model = parse_saved_model(export_dir)
  File ""/venvs/tf1_15/lib/python3.6/site-packages/tensorflow_core/python/saved_model/loader_impl.py"", line 71, in parse_saved_model
    raise IOError(""Cannot parse file %s: %s."" % (path_to_pb, str(e)))
OSError: Cannot parse file b'/path/saved_model.pb': Error parsing message.
```

The model is downloaded from [tensorflow model zoo: this one][1].

-----

FYI: the model loading script:
```
import time
import tensorflow as tf

_INPUT_KEYS_TO_TENSOR       = ""input_keys_to_tensor""
_OUTPUT_KEYS_KEYS_TO_TENSOR = ""output_keys_to_tensor""

class TF1Model:

  def __init__(self, model_path):
    self._sess = tf.compat.v1.Session()

    if hasattr(tf.compat.v1.saved_model, ""load""):
      graph_meta_def = tf.compat.v1.saved_model.load(
        self._sess, [tf.saved_model.SERVING], model_path
      )
    else:
      graph_meta_def = tf.compat.v1.saved_model.loader.load(
        self._sess, [tf.saved_model.SERVING], model_path
      )

    signature = graph_meta_def.signature_def

    self._signature_tensor_mapping = {}
    for signature_name in signature.keys():
      indiv_sig_data = self._signature_tensor_mapping[signature_name] = {
        _INPUT_KEYS_TO_TENSOR: {},
        _OUTPUT_KEYS_KEYS_TO_TENSOR: {}
      }

      inputs = signature[signature_name].inputs
      for k in inputs.keys():
        tensor = self._sess.graph.get_tensor_by_name(inputs[k].name)
        indiv_sig_data[_INPUT_KEYS_TO_TENSOR][k] = tensor

      outputs = signature[signature_name].outputs
      for k in outputs.keys():
        tensor = self._sess.graph.get_tensor_by_name(outputs[k].name)
        indiv_sig_data[_OUTPUT_KEYS_KEYS_TO_TENSOR][k] = tensor

  def predict(self, payload):
    start = time.time()
    payload_sig = payload[""signature_name""]
    res = self._sess.run(
      self._signature_tensor_mapping[payload_sig][_OUTPUT_KEYS_KEYS_TO_TENSOR],
      {
        self._signature_tensor_mapping[payload_sig][_INPUT_KEYS_TO_TENSOR][""inputs""]: payload[""inputs""][""inputs""]
      }
    )
    print(""prediction took: {}s"".format(time.time() - start))
    return res

if __name__ == ""__main__"":

  model = TF1Model(""/tf1_models/object_detection/1"")

  payload = {
    ""signature_name"":""serving_default"",
    ""inputs"":{
      ""inputs"":[
        [[[0, 0, 0], [0, 0, 0]]]
      ]
    }
  }

  for _ in range(3):
    model.predict(payload)
```

  [1]: http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz"
50166,how to set the buffer_size in tf.data,"hi,dear
my code runs more slowly then not distribute training ,
with big dataset,I use tf.data.Dataset.from_generator,
is the buffer_size helpful for speeding up the training ?
similar to the [issue](https://github.com/tensorflow/tensorflow/issues/33898),but a little different

could you pls help me ?
thx
"
50165,Problems to register a custom op in TF Lite,"@tensorflow/micro

  **System information**

 - OS Platform and Distribution: Linux Ubuntu 20.10
 - TensorFlow version: 2.4.1
 - Python version : 3.8 (installed via pip)
 - STM32CubeIDE-Lnx: 1.3.0
 - STM32 Nucleo-64 development board with STM32F401RE MCU

**Describe the problem**

I'd like to forecast RMSE values for seven days from a household power consumption dataset, using seven input test values corresponding to certain power consumption values, with a model which uses a custom op (Conv1D) and two builtin ops (Reshape and Fully Connected). I'm using a STM32 Nucleo-64 development board. The code is the following:

          #include ""main.h""
          #include <string.h>
         
          #include ""tensorflow/lite/micro/kernels/micro_ops.h""
          #include ""tensorflow/lite/micro/micro_error_reporter.h""
          #include ""tensorflow/lite/micro/micro_interpreter.h""
          #include ""tensorflow/lite/micro/micro_mutable_op_resolver.h""
          #include ""tensorflow/lite/version.h""
          #include ""tensorflow/lite/schema/schema_generated.h""
          #include ""power_consumption.h""
          #include ""tensorflow/lite/micro/micro_allocator.h""
          #include ""tensorflow/lite/micro/micro_op_resolver.h""
          #include ""tensorflow/lite/core/api/flatbuffer_conversions.h""
          #include ""flatbuffers/flatbuffers.h""  // from @flatbuffers
          #include ""tensorflow/lite/c/builtin_op_data.h""
          #include ""tensorflow/lite/c/common.h""
          #include ""tensorflow/lite/core/api/error_reporter.h""
          #include ""tensorflow/lite/kernels/internal/compatibility.h""
          #include ""tensorflow/lite/kernels/internal/reference/conv_1d.h""
          #include ""tensorflow/lite/schema/schema_generated.h""
          
          CRC_HandleTypeDef hcrc;
          TIM_HandleTypeDef htim11;
          UART_HandleTypeDef huart2;
          
          // TFLite globals
          namespace {
          tflite::ErrorReporter* error_reporter = nullptr;
          const tflite::Model* model = nullptr;
          tflite::MicroInterpreter* interpreter = nullptr;
          TfLiteTensor* model_input = nullptr;
          TfLiteTensor* model_output = nullptr;
          constexpr int kTensorArenaSize = 2 * 1024;
          __attribute__((aligned(16)))uint8_t tensor_arena[kTensorArenaSize];
          } // namespace
    
    
          void SystemClock_Config(void);
          static void MX_GPIO_Init(void);
          static void MX_USART2_UART_Init(void);
          static void MX_CRC_Init(void);
          static void MX_TIM11_Init(void);

          int main(void)
          {

            char buf[100];
            int buf_len = 0;
            TfLiteStatus tflite_status;
            uint32_t num_elements;
            uint32_t timestamp;
            float y_val[7];
            int i;

            HAL_Init();
            SystemClock_Config();
            MX_GPIO_Init();
            MX_USART2_UART_Init();
            MX_CRC_Init();
            MX_TIM11_Init();
            HAL_TIM_Base_Start(&htim11);

            static tflite::MicroErrorReporter micro_error_reporter;
            error_reporter = &micro_error_reporter;

            // Say something to test error reporter
            error_reporter->Report(""STM32 TensorFlow Lite test"");
        
           // Map the model into a usable data structure
           model = tflite::GetModel(power_consumption);
           if (model->version() != TFLITE_SCHEMA_VERSION)
           {
            error_reporter->Report(""Model version does not match Schema"");
            while(1);
           }

           // Pull in only needed operations (should match NN layers). 
           static tflite::MicroMutableOpResolver<3> micro_op_resolver;

           // Add custom neural network layer operation
           tflite_status = micro_op_resolver.AddCustom(
           ""cd1"", tflite::ops::custom::Register_CONV_1D());

           if (tflite_status != kTfLiteOk) {
      
            error_reporter->Report(""Could not add Conv op"");
            while(1);
           }

           tflite_status = micro_op_resolver.AddReshape();
 
           if (tflite_status != kTfLiteOk) {
             error_reporter->Report(""Could not add RESHAPE op"");
             while(1);
           }

          tflite_status = micro_op_resolver.AddFullyConnected();

          if (tflite_status != kTfLiteOk) {
          error_reporter->Report(""Could not add FULLY CONNECTED op"");
          while(1);
          }

          // Build an interpreter to run the model with.
          static tflite::MicroInterpreter static_interpreter(
          model, micro_op_resolver, tensor_arena, kTensorArenaSize, error_reporter);
          interpreter = &static_interpreter;

          // Allocate memory from the tensor_arena for the model's tensors.
          tflite_status = interpreter->AllocateTensors();
          if (tflite_status != kTfLiteOk) {
          error_reporter->Report(""AllocateTensors() failed"");
          while(1);
          }

         // Assign model input and output buffers (tensors) to pointers
         model_input = interpreter->input(0);
         model_output = interpreter->output(0);

         // Get number of elements in input tensor
         num_elements = model_input->bytes / sizeof(float);
         buf_len = sprintf(buf, ""Number of input elements: %lu\r\n"", num_elements);
         HAL_UART_Transmit(&huart2, (uint8_t *)buf, buf_len, 100);


          /* Infinite loop */
           while (1)
           {
            // Fill input buffer (use test value)
            for (uint32_t i = 0; i < num_elements; i++)
            {
              model_input->data.f[i] = 150.0f;
            }

            // Get current timestamp
            timestamp = htim11.Instance->CNT;

            // Run inference
            tflite_status = interpreter->Invoke();
            if (tflite_status != kTfLiteOk)
            {
              error_reporter->Report(""Invoke failed"");
            }

           // Read output RMSE (predicted y) of neural network
           for(i=0; i<7; i++) {
           y_val[i] = model_output->data.f[i];
           }

           // Print output of neural network along with inference time (microseconds)
           for(i=0; i<7; i++) {
           buf_len = sprintf(buf,
                        ""Output: %f | Duration: %lu\r\n"",
                        y_val[i],
                        htim11.Instance->CNT - timestamp);
           HAL_UART_Transmit(&huart2, (uint8_t *)buf, buf_len, 100);
           }
          // Wait before doing it again
          HAL_Delay(500);

       }
      }


I can compile it, but when I open PuTTY, I can read anything.
For the implementation of the custom op Conv_1D, I added a .cpp file and a .h file in tensorflow/lite/kernels.

The header file for the custom op is like this:

            #ifndef TENSORFLOW_LITE_KERNELS_CONV1D_H_
            #define TENSORFLOW_LITE_KERNELS_CONV1D_H_

            #include ""tensorflow/lite/kernels/internal/types.h""
            #include ""tensorflow/lite/kernels/kernel_util.h""

            namespace tflite {
            namespace ops {
            namespace custom {

            TfLiteRegistration* Register_CONV_1D();

             }  // namespace custom
             }  // namespace ops
             }  // namespace tflite

            #endif  // TENSORFLOW_LITE_KERNELS_CONV1D_H_

The .cpp file is like this (it's very simple and probably there are errors, for now I use certain weights because I don't know how to read the real ones from the trained model):

                #include ""tensorflow/lite/kernels/conv_1d.h""

                #include <math.h>
                #include <stddef.h>
                #include <stdint.h>
                
                #include <vector>
                
                #include ""tensorflow/lite/c/common.h""
                
                #include ""tensorflow/lite/kernels/internal/common.h""
                #include ""tensorflow/lite/kernels/internal/tensor.h""
                #include ""tensorflow/lite/kernels/internal/tensor_ctypes.h""
                #include ""tensorflow/lite/kernels/kernel_util.h""

                namespace tflite {
                namespace ops {
                namespace custom {
                namespace conv_1d {

                const int dim = 5;
                int dim_in;  
                int dim_out;  
                int dim_k = 3;    //kernel dimension
                float copy[dim];

                constexpr float kernel[3] = {1.2,2.0,4.2};
                constexpr int dilation = 2;   //dilation

                TfLiteStatus Conv1dPrepare(TfLiteContext* context, TfLiteNode* node) {
  
                TF_LITE_ENSURE_EQ(context, NumInputs(node), 1);
                TF_LITE_ENSURE_EQ(context, NumOutputs(node), 1);

                const TfLiteTensor* input = GetInput(context, node, 0);
                TfLiteTensor* output = GetOutput(context, node, 0);

                 int num_dims = NumDimensions(input);

                 TfLiteIntArray* output_size = TfLiteIntArrayCreate(num_dims);
                  for (int i=0; i<num_dims; ++i) {
                  output_size->data[i] = input->dims->data[i];
                  }

                  return context->ResizeTensor(context, output, output_size);
                 }

                 TfLiteStatus Conv1dEval(TfLiteContext* context, TfLiteNode* node) {
  
                  const TfLiteTensor* input = GetInput(context, node,0);
                  TfLiteTensor* output = GetOutput(context, node,0);

                  float* input_data = input->data.f;
                  float* output_data = output->data.f;
  
                  if (output->dims->data[0] > 1) 
                  dim_out = output->dims->data[0];
    
                  else dim_out = output->dims->data[1];
    
                  if (input->dims->data[0] > 1) 
                  dim_in = input->dims->data[0];
    
                  else dim_in = input->dims->data[1];
  
                  float copy0[4+dim_in];
  
                  for (int i=0; i<4; i++) {
                  copy0[i] = 0;
                  }
  
                  for (int i=0; i<dim_in; i++) {
                  copy0[i+4] = input_data[i];
                  }

                   for (int i=0; i<dim_out; i++) {
                   for (int m=0; m<dim; m++) {
                   copy[m] = copy0[m+i];
                   } 
                   for (int j=0; j<dim_k; j++) {
                   output_data[i] = output_data[i] + copy[j*dilation]*kernel[j];
                   }
    
                   }
                   return kTfLiteOk;
                   }


               }  // namespace conv_1d

              TfLiteRegistration* Register_CONV_1D() {
              static TfLiteRegistration r = {nullptr, nullptr, conv_1d::Conv1dPrepare, conv_1d::Conv1dEval};
              return &r;
              }

              }  // namespace custom
              }  // namespace ops
              }  // namespace tflite

I think the problem is also not having register.cpp and register_ref.cpp files (I don't find them in tensorflow/lite/kernels), so I don't know if I can create these files myself or not (in this case I should be careful with the various header files).

In the BUILD file (I don't even have that) I'd write like this:

            cc_library(
            name = ""builtin_op_kernels"",
            srcs = BUILTIN_KERNEL_SRCS + [
               ""conv_1d.cc"",
             ], 
             hdrs = [
                ""dequantize.h"",
                ""conv_1d.h"",
             ],
             copts = tflite_copts() + tf_opts_nortti_if_android() + EXTRA_EIGEN_COPTS,
             visibility = [""//visibility:private""],
             deps = BUILTIN_KERNEL_DEPS + [
             ""@ruy//ruy/profiler:instrumentation"",
             ""//tensorflow/lite/kernels/internal:cppmath"",
             ""//tensorflow/lite:string"",
             ""@farmhash_archive//:farmhash"",
             ],
             )


I'm not sure if I'm missing some other steps to register the custom op.
I used this tutorial as a reference: https://www.digikey.com/en/maker/projects/tinyml-getting-started-with-tensorflow-lite-for-microcontrollers/c0cdd850f5004b098d263400aa294023."
50158, NameError: name 'interpreter_wrapper' is not defined ,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.

Hi,
I am executing tflite through mlperf on riscv linux, getting following error, how to fix it?

>     ./run_local.sh tflite --profile ssd-mobilenet-tf
> 
>     INFO:main:Namespace(accuracy=False, backend='tflite', cache=0, count=None, data_format=None, dataset='coco-300', dataset_list=None, dataset_path='/home/root/Ankita/data/coco', debug=False, find_peak_performance=False, inputs=['image_tensor:0'], max_batchsize=32, max_latency=None, mlperf_conf='../../mlperf.conf', model='/home/root/Ankita/model/resnet50_v1.tflite', model_name='ssd-mobilenet', output='/home/root/Ankita/inference/vision/classification_and_detection/output/tflite-cpu/resnet50', outputs=['num_detections:0', 'detection_boxes:0', 'detection_scores:0', 'detection_classes:0'], profile='ssd-mobilenet-tf', qps=None, samples_per_query=None, scenario='SingleStream', threads=4, time=None, user_conf='user.conf')
>     INFO:coco:reduced image list, 4997 images not found
>     INFO:coco:loaded 3 images, cache=0, took=9.0sec
>     /home/root/Ankita/inference/vision/classification_and_detection/python/coco.py:115: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray
>     self.label_list = np.array(self.label_list)
>     Traceback (most recent call last):
>     File ""python/main.py"", line 558, in
>     main()
>     File ""python/main.py"", line 436, in main
>     model = backend.load(args.model, inputs=args.inputs, outputs=args.outputs)
>     File ""/home/root/Ankita/inference/vision/classification_and_detection/python/backend_tflite.py"", line 32, in load
>     self.sess = interpreter_wrapper.Interpreter(model_path=model_path)
>     NameError: name 'interpreter_wrapper' is not defined

Thanks"
50155,"TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.","I am implementing a 1d conv net on Google Colab but I am getting the following error when I run the attached part of code wich indicates my model.


TypeError: Cannot convert a symbolic Keras input/output to a numpy array. This error may indicate that you're trying to pass a symbolic value to a NumPy call, which is not supported. Or, you may be trying to pass Keras symbolic inputs/outputs to a TF API that does not register dispatching, preventing Keras from automatically converting the API call to a lambda layer in the Functional Model.

My model is as follows:

```
input = tf.keras.Input(shape=(100,1), name='input')

x_0 = Conv1D(20,1,strides=1,activation='relu', name='C0')(input)

x_1 = Conv1D(50,2,strides=1, activation='relu', name='C1')(x_0)
x_1 = Dropout(0.3, name='DR1')(x_1)
x_1 = GlobalMaxPooling1D(name='MP1')(x_1)

x_2 = Conv1D(35,3,strides=1,activation='relu', name='C2')(x_0)
x_2 = Dropout(0.3, name='DR2')(x_2)
x_2 = GlobalMaxPooling1D(name='MP2')(x_2)

x_3 = Conv1D(25,4,strides=1,activation='relu', name='C3')(x_0)
x_3 = Dropout(0.3, name='DR3')(x_3)
x_3 = GlobalMaxPooling1D(name='MP3')(x_3)

x_4 = Conv1D(20,5,strides=1,activation='relu', name='C4')(x_0)
x_4 = Dropout(0.3, name='DR4')(x_4)
x_4 = GlobalMaxPooling1D(name='MP4')(x_4)

concat = Concatenate(axis=1, name='concat')([x_1, x_2, x_3, x_4])
concat = BatchNormalization(name='BN')(concat)

output = Dense(units=1, activation='linear',name='output')(concat)

model = tf.keras.Model(inputs=input, outputs=output, name='VRP')
```"
50154,MetricsContainer.update_state passes mask as sample_weight to metric_obj.update_state,"**System information**
- Have I written custom code YES
- OS Platform and Distribution OS X 10.15.6
- TensorFlow installed from binary
- TensorFlow version 2.5.0
- Python version 3.8


**Describe the current behavior**
After upgrading from tensorflow 2.2.1 to 2.5.0, I get an error while training an LSTM model with F1Score for the metrics (issue does not exist if using CategoricalAccuracy for the metrics). After comparing the source code for both versions I found a bug (I think).
In version 2.2.1, in tensorflow/python/keras/engine/compile_utils.py, line 411 (inside MetricsContainer.update_state), the metric_obj is updated by passing y_t and y_p, the optional argument sample_weight is not passed. In version 2.5.0, same method, line 460, something is passed for the optional argument sample_weight, but it is the mask obtained from y_p, hence of dtype tf.bool instead of tf.float32, which gives a TypeError when updating the state of F1Score, because it tries to multiply the boolean mask with tf.float32 tensors. I think tis is incorrect.

Full error:
```
in user code:

    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow_addons/metrics/f_scores.py:157 _weighted_sum  *
        val = tf.math.multiply(val, tf.expand_dims(sample_weight, 1))
    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper  **
        return target(*args, **kwargs)
    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:530 multiply
        return gen_math_ops.mul(x, y, name)
    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py:6249 mul
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    /Users/aleksandra/Projects/Metamaze/metamaze-ml/nlp_ner_lstm/venv_lstm/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:555 _apply_op_helper
        raise TypeError(

    TypeError: Input 'y' of 'Mul' Op has type bool that does not match type float32 of argument 'x'.

```

**Describe the expected behavior**
No TypeError when updating metrics during training.

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? no

**Standalone code to reproduce the issue**
```
import tensorflow as tf
from tensorflow.python.keras.layers import Masking, Bidirectional, LSTM, Dropout, TimeDistributed, Dense, Lambda
from tensorflow_addons.metrics import F1Score
import numpy as np

window_length = 50
embedding_dimension = 100
num_classes = 5

tf.random.set_seed(42)
embeddings = tf.keras.Input(shape=(window_length, embedding_dimension), dtype=tf.float32, name=""embedding_sequence"")
nwords = tf.keras.Input(shape=(), dtype=tf.int32, name=""nwords"")

masked_embedding = Masking()(embeddings)

bilstm = Bidirectional(
    LSTM(
        units=16,
        return_sequences=True,
        dropout=0.0,
        recurrent_dropout=0.0,
    )
)(masked_embedding)

bilstm = Dropout(rate=0.5, seed=42)(bilstm)

logits = TimeDistributed(Dense(num_classes, activation=""softmax""), name=""logits"")(bilstm)

pred_ids = tf.argmax(logits, axis=2, output_type=tf.int32)

naming_layer = Lambda(lambda x: x, name=""pred_ids"")
pred_ids = naming_layer(pred_ids)

loss = {""logits"": ""categorical_crossentropy""}

model = tf.keras.Model(inputs=[embeddings, nwords], outputs=[logits, pred_ids], name=""ner_bilstm"")

model.compile(
    optimizer=tf.keras.optimizers.Adam(
        learning_rate= 0.001),
    loss=loss,
    metrics={""logits"": [F1Score(num_classes=num_classes, average=""micro"")]},
)

dummy_pred_ids = np.array([""dummy""] * 7)



data_x = {""embedding_sequence"": np.random.rand(7,window_length,embedding_dimension), ""nwords"": np.array([window_length]*7)}
data_y = {
    ""logits"": np.zeros((7, window_length, num_classes), dtype=int),
    ""pred_ids"": dummy_pred_ids,
}
model.fit(x=data_x, y=data_y, validation_data=(data_x, data_y))
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50150,tensorflow.python.keras.applications.vgg16 model got low accuracy on the ILSVRC2012 validation dataset,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: No
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**: NA
-   **TensorFlow installed from (source or binary)**: pip install
-   **TensorFlow version (use command below)**: 2.4
-   **Python version**: 3.7
-   **Bazel version (if compiling from source)**: NA
-   **GCC/Compiler version (if compiling from source)**: NA
-   **CUDA/cuDNN version**: CUDA 11.0/cuDNN 8.0
-   **GPU model and memory**: GTX1060 6G
-   **Exact command to reproduce**: NA

You can collect some of this information using our environment capture script:

https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh

You can obtain the TensorFlow version with:

```bash
python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""
```

### Describe the problem
I imported the VGG-16 model from tensorflow.python.keras.applications.vgg16 and evaluated it on the ILSVRC2012 validation dataset. According to ""https://keras.io/api/applications/"", the top-1 and top-5 accuracy of VGG-16 model should be 71.3% and 90.1%, but I got only 65.7% and 86.8% on the ILSVRC2012 validation dataset.

I downloaded the validation dataset and loaded it using ""image_dataset_from_directory()"" imported from tensorflow.keras.preprocessing. I preprocessed the images using ""preprocess_input()"" imported from tensorflow.python.keras.applications.vgg16. I have also dealed with the inconsistency between the original imagenet ILSVRC2012_ID and the class index which the pre-trained model uses.

I wonder why I got low accuracy. I suspect that the image resize interpolation method affects data destribution. I suggest that the image preprocessing method should be explicitly listed since it might vary from model to model.

I also found a bug. The function image_dataset_from_directory( ..., interpolation='nearest') will return a dataset of which the datat type is 'uint8' instead of 'float32', while other interpolation methods always return 'float32' dataset.

### Source code / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.

source code:
```
from __future__ import print_function as _print_function

import sys as _sys

from tensorflow.python.keras.applications.vgg16 import VGG16
from tensorflow.python.keras.applications.vgg16 import decode_predictions
from tensorflow.python.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image_dataset_from_directory

import numpy as np

def preprocess(img, label):
    img = preprocess_input(img)
    return img, label

if __name__ == '__main__':
    model = VGG16(include_top=True, weights='imagenet')
    #model.compile(optimizer=""adam"",loss='sparse_categorical_crossentropy',metrics=['accuracy']) # 65.68%
    model.compile(optimizer=""adam"",loss='sparse_categorical_crossentropy',metrics=['sparse_top_k_categorical_accuracy']) # 86.75%

    ds = image_dataset_from_directory('C:\ILSVRC2012\ILSVRC2012_img_val',
        labels=list(np.loadtxt('new_val_truth.txt', dtype='int')),
        label_mode='int', color_mode='rgb', batch_size=32,
        image_size=(224, 224), shuffle=False, interpolation='bilinear')
    ds = ds.map(preprocess)

    model.evaluate(ds, verbose=1)
```

logs:
```
1563/1563 [==============================] - 684s 432ms/step - loss: 1.4227 - accuracy: 0.6568
1563/1563 [==============================] - 682s 430ms/step - loss: 1.4227 - sparse_top_k_categorical_accuracy: 0.8675
```"
50149,"Could compile C API library but I get ""undefined reference"" errors (Linking problem) using CMake","<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Ubuntu 18.04**
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): **from source**
- TensorFlow version: **master branch commit 700533808e6016dc458bb2eeecfca4babfc482ec**
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): **CMake version 3.20.3**
- CUDA/cuDNN version:
- GPU model and memory:



**I could successfully compile the C API library, however, I get ""undefined reference"" errors when trying to link the library to the following simple CMake project. I would appreciate your help on this**

```
#include <stdio.h>
#include ""tensorflow/lite/c/c_api.h""

int main() {
  printf(""Hello from TensorFlow C library version %s\n"", TfLiteVersion());
  return 0;
}
```

**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
cmake ..
make
```


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
`
[ 50%] Linking CXX executable bin/example
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(activations.cc.o): In function `tflite::ops::builtin::activations::SoftmaxFloat(TfLiteContext*, TfLiteTensor const*, TfLiteTensor*, TfLiteSoftmaxParams*, tflite::ops::builtin::activations::KernelType)':
activations.cc:(.text+0x6338): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, signed char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
batch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e3): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'
batch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3a1): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, signed char>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, signed char> const&, int, int, int, int, ruy::Mat<signed char>*) const [clone .isra.664]':
batch_matmul.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiaE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiaEEiiiiPNS_3MatIaEE.isra.664[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e1): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'
batch_matmul.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiaE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiaEEiiiiPNS_3MatIaEE.isra.664[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c1): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)1, signed char, signed char, int, signed char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
batch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3cb): undefined reference to `ruy::detail::MultiplyByQuantizedMultiplier(int, int, int)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, signed char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
batch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e3): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'
batch_matmul.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiaEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3a1): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `tflite::optimized_ops::BatchMatMul(tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x5ce): undefined reference to `ruy::get_ctx(ruy::Context*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x5dd): undefined reference to `ruy::Ctx::clear_performance_advisories()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x794): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0x7f9): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xbab): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xc11): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xc19): undefined reference to `ruy::Ctx::GetMainAllocator()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xc62): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xd48): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xd50): undefined reference to `ruy::Ctx::GetMainAllocator()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKfS3_S5_S3_PfPNS_17CpuBackendContextE]+0xd99): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `tflite::optimized_ops::BatchMatMul(tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, float const*, int const*, int*, tflite::RuntimeShape const&, int*, float*, bool*, tflite::CpuBackendContext*)':
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x7e1): undefined reference to `ruy::get_ctx(ruy::Context*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x7ec): undefined reference to `ruy::Ctx::clear_performance_advisories()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x948): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x9c7): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x1120): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x1128): undefined reference to `ruy::Ctx::GetMainAllocator()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_12RuntimeShapeEPKaS3_S5_PKfPKiPiS3_SA_PfPbPNS_17CpuBackendContextE]+0x117b): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)113, signed char, signed char, int, signed char>(ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::MulParams<int, signed char> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':
batch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x110): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
batch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
batch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1e1): undefined reference to `ruy::Ctx::GetMainAllocator()'
batch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x232): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x2ae): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EaaiaEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x4c3): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(batch_matmul.cc.o): In function `tflite::optimized_ops::BatchMatMul(tflite::FullyConnectedParams const&, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*)':
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x6b0): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x760): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x8ca): undefined reference to `ruy::get_ctx(ruy::Context*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x8d9): undefined reference to `ruy::Ctx::clear_performance_advisories()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xb20): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xb28): undefined reference to `ruy::Ctx::GetMainAllocator()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xb7f): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0xc20): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x100f): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x1185): undefined reference to `ruy::get_ctx(ruy::Context*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x1194): undefined reference to `ruy::Ctx::clear_performance_advisories()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x13b8): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x1452): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x14a3): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x14ab): undefined reference to `ruy::Ctx::GetMainAllocator()'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x14f7): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x158c): undefined reference to `ruy::Allocator::AllocateBytes(long)'
batch_matmul.cc:(.text._ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops11BatchMatMulERKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKaS6_S8_S6_PaPNS_17CpuBackendContextE]+0x197f): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, unsigned char>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, unsigned char> const&, int, int, int, int, ruy::Mat<unsigned char>*) const [clone .isra.1141]':
conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaihE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIihEEiiiiPNS_3MatIhEE.isra.1141[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e1): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'
conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaihE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIihEEiiiiPNS_3MatIhEE.isra.1141[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c1): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)1, unsigned char, unsigned char, int, unsigned char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3cb): undefined reference to `ruy::detail::MultiplyByQuantizedMultiplier(int, int, int)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `gemmlowp::WorkersPool::LegacyExecuteAndDestroyTasks(std::vector<gemmlowp::Task*, std::allocator<gemmlowp::Task*> > const&)':
conv.cc:(.text._ZN8gemmlowp11WorkersPool28LegacyExecuteAndDestroyTasksERKSt6vectorIPNS_4TaskESaIS3_EE[_ZN8gemmlowp11WorkersPool28LegacyExecuteAndDestroyTasksERKSt6vectorIPNS_4TaskESaIS3_EE]+0x2a0): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `void ruy::Mul<(ruy::Path)113, signed char, signed char, int, int>(ruy::Matrix<signed char> const&, ruy::Matrix<signed char> const&, ruy::MulParams<int, int> const&, ruy::Context*, ruy::Matrix<int>*)':
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0xbe): undefined reference to `ruy::get_ctx(ruy::Context*)'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0xc9): undefined reference to `ruy::Ctx::clear_performance_advisories()'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x27e): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x30b): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x349): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x351): undefined reference to `ruy::Ctx::GetMainAllocator()'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x39e): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE[_ZN3ruy3MulILNS_4PathE113EaaiiEEvRKNS_6MatrixIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_7ContextEPNS2_ISD_EE]+0x4ee): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<signed char, signed char, int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)2>::Run(tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char*, tflite::cpu_backend_gemm::GemmParams<int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)2> const&, tflite::CpuBackendContext*)':
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x1cc): undefined reference to `ruy::get_ctx(ruy::Context*)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x1d7): undefined reference to `ruy::Ctx::clear_performance_advisories()'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x426): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x4d8): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x5d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x5d9): undefined reference to `ruy::Ctx::GetMainAllocator()'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x628): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x6b9): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE2EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_2EEEPNS_17CpuBackendContextE]+0x747): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `tflite::optimized_ops::Conv(tflite::ConvParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':
conv.cc:(.text._ZN6tflite13optimized_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_PNS_17CpuBackendContextE[_ZN6tflite13optimized_ops4ConvERKNS_10ConvParamsERKNS_12RuntimeShapeEPKfS6_S8_S6_S8_S6_PfS6_S9_PNS_17CpuBackendContextE]+0x2db): undefined reference to `ruy::get_ctx(ruy::Context*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)113, unsigned char, unsigned char, int, unsigned char>(ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::MulParams<int, unsigned char> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':
conv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x107): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
conv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
conv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d9): undefined reference to `ruy::Ctx::GetMainAllocator()'
conv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x22a): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x2a6): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhihEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x53b): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<unsigned char, unsigned char, int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char*, tflite::cpu_backend_gemm::GemmParams<int, unsigned char, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*)':
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x1c2): undefined reference to `ruy::get_ctx(ruy::Context*)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x1cd): undefined reference to `ruy::Ctx::clear_performance_advisories()'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x40f): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x4c1): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x5b9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x5c1): undefined reference to `ruy::Ctx::GetMainAllocator()'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x610): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x6a1): undefined reference to `ruy::Allocator::AllocateBytes(long)'
conv.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhihLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_S8_PhRKNS0_10GemmParamsIihLS3_1EEEPNS_17CpuBackendContextE]+0x72f): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, unsigned char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfa): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'
conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, unsigned char> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfa): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'
conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaihEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::ops::builtin::fully_connected::EvalHybrid(TfLiteContext*, TfLiteNode*, TfLiteFullyConnectedParams*, tflite::ops::builtin::fully_connected::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
fully_connected.cc:(.text+0x31a1): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, short>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, short> const&, int, int, int, int, ruy::Mat<short>*) const [clone .isra.958]':
fully_connected.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaisE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIisEEiiiiPNS_3MatIsEE.isra.958[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x2e6): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'
fully_connected.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaisE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIisEEiiiiPNS_3MatIsEE.isra.958[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c1): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)1, unsigned char, unsigned char, int, short> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
fully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE1EhhisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x3c3): undefined reference to `ruy::detail::MultiplyByQuantizedMultiplier(int, int, int)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `gemmlowp::WorkersPool::CreateWorkers(unsigned long)':
fully_connected.cc:(.text._ZN8gemmlowp11WorkersPool13CreateWorkersEm[_ZN8gemmlowp11WorkersPool13CreateWorkersEm]+0x12d): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::optimized_ops::FullyConnectedSparseWeight1x4(TfLiteSparsity const&, tflite::FullyConnectedParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':
fully_connected.cc:(.text._ZN6tflite13optimized_ops29FullyConnectedSparseWeight1x4ERK14TfLiteSparsityRKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKfS9_SB_S9_SB_S9_PfPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops29FullyConnectedSparseWeight1x4ERK14TfLiteSparsityRKNS_20FullyConnectedParamsERKNS_12RuntimeShapeEPKfS9_SB_S9_SB_S9_PfPNS_17CpuBackendContextE]+0x8d7): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<signed char, signed char, int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char const*, tflite::cpu_backend_gemm::MatrixParams<signed char> const&, signed char*, tflite::cpu_backend_gemm::GemmParams<int, signed char, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*)':
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x1cd): undefined reference to `ruy::get_ctx(ruy::Context*)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x1d8): undefined reference to `ruy::Ctx::clear_performance_advisories()'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x427): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x4d9): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x5d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x5d9): undefined reference to `ruy::Ctx::GetMainAllocator()'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x628): undefined reference to `ruy::Allocator::AllocateBytes(long)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x6b9): undefined reference to `ruy::Allocator::AllocateBytes(long)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIaaiaLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIaEEPKaS8_SA_S8_PaRKNS0_10GemmParamsIiaLS3_1EEEPNS_17CpuBackendContextE]+0x747): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, short> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
fully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfb): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'
fully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, short> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
fully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0xfb): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'
fully_connected.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaisEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x121): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `void ruy::detail::CreateTrMulParamsAssumingColMajorDst<(ruy::Path)113, unsigned char, unsigned char, int, short>(ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::Mat<short> const&, ruy::MulParams<int, short> const&, ruy::ChannelDimension, ruy::Ctx*, ruy::TrMulParams*)':
fully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x10a): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
fully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
fully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x1d9): undefined reference to `ruy::Ctx::GetMainAllocator()'
fully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x22a): undefined reference to `ruy::Allocator::AllocateBytes(long)'
fully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x2a6): undefined reference to `ruy::Allocator::AllocateBytes(long)'
fully_connected.cc:(.text._ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE[_ZN3ruy6detail36CreateTrMulParamsAssumingColMajorDstILNS_4PathE113EhhisEEvRKNS_3MatIT0_EERKNS3_IT1_EERKNS3_IT3_EERKNS_9MulParamsIT2_SC_EENS_16ChannelDimensionEPNS_3CtxEPNS_11TrMulParamsE]+0x4b3): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(fully_connected.cc.o): In function `tflite::cpu_backend_gemm::detail::GemmImplUsingRuy<unsigned char, unsigned char, int, short, (tflite::cpu_backend_gemm::QuantizationFlavor)1>::Run(tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<unsigned char> const&, unsigned char const*, tflite::cpu_backend_gemm::MatrixParams<short> const&, short*, tflite::cpu_backend_gemm::GemmParams<int, short, (tflite::cpu_backend_gemm::QuantizationFlavor)1> const&, tflite::CpuBackendContext*)':
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x1d0): undefined reference to `ruy::get_ctx(ruy::Context*)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x1db): undefined reference to `ruy::Ctx::clear_performance_advisories()'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x41a): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x4cc): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x5c1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x5c9): undefined reference to `ruy::Ctx::GetMainAllocator()'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x618): undefined reference to `ruy::Allocator::AllocateBytes(long)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x6a9): undefined reference to `ruy::Allocator::AllocateBytes(long)'
fully_connected.cc:(.text._ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm6detail16GemmImplUsingRuyIhhisLNS0_18QuantizationFlavorE1EE3RunERKNS0_12MatrixParamsIhEEPKhS8_SA_RKNS5_IsEEPsRKNS0_10GemmParamsIisLS3_1EEEPNS_17CpuBackendContextE]+0x737): undefined reference to `ruy::Allocator::AllocateBytes(long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(interpreter.cc.o): In function `tflite::Interpreter::Invoke()':
interpreter.cc:(.text+0x3c3): undefined reference to `ruy::ScopedSuppressDenormals::ScopedSuppressDenormals()'
interpreter.cc:(.text+0x48c): undefined reference to `ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals()'
interpreter.cc:(.text+0x71f): undefined reference to `ruy::ScopedSuppressDenormals::~ScopedSuppressDenormals()'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(interpreter_builder.cc.o): In function `tflite::AcquireFlexDelegate()':
interpreter_builder.cc:(.text+0x292): undefined reference to `dlsym'
interpreter_builder.cc:(.text+0x2cd): undefined reference to `dlopen'
interpreter_builder.cc:(.text+0x2e1): undefined reference to `dlsym'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(rfft2d.cc.o): In function `tflite::ops::builtin::rfft2d::Rfft2dImpl(int, int, double**, int*, double*)':
rfft2d.cc:(.text+0x7f0): undefined reference to `rdft2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)64, ruy::FixedKernelLayout<(ruy::Order)1, 1, 16>, float, float>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi16EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi16EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x260): undefined reference to `ruy::PackFloatColMajorForAvx512(float const*, float const*, int, int, int, float*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)32, ruy::FixedKernelLayout<(ruy::Order)1, 1, 8>, float, float>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x228): undefined reference to `ruy::PackFloatColMajorForAvx2(float const*, float const*, int, int, int, float*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::Kernel<(ruy::Path)64, float, float, float, float>::Run(ruy::PMat<float> const&, ruy::PMat<float> const&, ruy::MulParams<float, float> const&, int, int, int, int, ruy::Mat<float>*) const [clone .isra.87]':
transpose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EffffE3RunERKNS_4PMatIfEES6_RKNS_9MulParamsIffEEiiiiPNS_3MatIfEE.isra.87[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x14d): undefined reference to `ruy::KernelFloatAvx512(ruy::KernelParamsFloat<16, 16> const&)'
transpose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EffffE3RunERKNS_4PMatIfEES6_RKNS_9MulParamsIffEEiiiiPNS_3MatIfEE.isra.87[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x189): undefined reference to `ruy::KernelFloatAvx512SingleCol(ruy::KernelParamsFloat<16, 16> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, float, float, float, float> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x136): undefined reference to `ruy::KernelFloatAvx2(ruy::KernelParamsFloat<8, 8> const&)'
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x171): undefined reference to `ruy::KernelFloatAvx2SingleCol(ruy::KernelParamsFloat<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)64, ruy::FixedKernelLayout<(ruy::Order)0, 4, 16>, unsigned char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x113): undefined reference to `ruy::Pack8bitRowMajorForAvx512(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x225): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2c4): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)64, ruy::FixedKernelLayout<(ruy::Order)0, 4, 16>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x11f): undefined reference to `ruy::Pack8bitRowMajorForAvx512(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x232): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE64ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi16EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2d0): undefined reference to `ruy::Pack8bitColMajorForAvx512(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)32, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, unsigned char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x113): undefined reference to `ruy::Pack8bitRowMajorForAvx2(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x225): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2c4): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)32, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x11f): undefined reference to `ruy::Pack8bitRowMajorForAvx2(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x232): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE32ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2d0): undefined reference to `ruy::Pack8bitColMajorForAvx2(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, unsigned char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x113): undefined reference to `ruy::Pack8bitRowMajorForAvx(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x225): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEhaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2c4): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)0, 4, 8>, signed char, signed char>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x11f): undefined reference to `ruy::Pack8bitRowMajorForAvx(unsigned char const*, int, int, signed char*, int, int, int, int, int, int, int, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x232): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE0ELi4ELi8EEEaaEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x2d0): undefined reference to `ruy::Pack8bitColMajorForAvx(signed char const*, signed char, signed char const*, int, int, int, signed char*, int*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)32, signed char, signed char, int, int> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x22d): undefined reference to `ruy::Kernel8bitAvx2(ruy::KernelParams8bit<8, 8> const&)'
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE32EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x281): undefined reference to `ruy::Kernel8bitAvx2SingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, float, float, float, float> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x136): undefined reference to `ruy::KernelFloatAvx(ruy::KernelParamsFloat<8, 8> const&)'
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EffffEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x171): undefined reference to `ruy::KernelFloatAvxSingleCol(ruy::KernelParamsFloat<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::Kernel<(ruy::Path)64, signed char, signed char, int, int>::Run(ruy::PMat<signed char> const&, ruy::PMat<signed char> const&, ruy::MulParams<int, int> const&, int, int, int, int, ruy::Mat<int>*) const [clone .isra.651]':
transpose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiiE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiiEEiiiiPNS_3MatIiEE.isra.651[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x22c): undefined reference to `ruy::Kernel8bitAvx512(ruy::KernelParams8bit<16, 16> const&)'
transpose_conv.cc:(.text._ZNK3ruy6KernelILNS_4PathE64EaaiiE3RunERKNS_4PMatIaEES6_RKNS_9MulParamsIiiEEiiiiPNS_3MatIiEE.isra.651[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE64EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x271): undefined reference to `ruy::Kernel8bitAvx512SingleCol(ruy::KernelParams8bit<16, 16> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::RunPack<(ruy::Path)16, ruy::FixedKernelLayout<(ruy::Order)1, 1, 8>, float, float>(ruy::Tuning, ruy::EMat const&, ruy::PEMat*, int, int)':
transpose_conv.cc:(.text._ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii[_ZN3ruy7RunPackILNS_4PathE16ENS_17FixedKernelLayoutILNS_5OrderE1ELi1ELi8EEEffEEvNS_6TuningERKNS_4EMatEPNS_5PEMatEii]+0x270): undefined reference to `ruy::PackFloatColMajorForAvx(float const*, float const*, int, int, int, float*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `ruy::RunKernel<ruy::Kernel<(ruy::Path)16, signed char, signed char, int, int> >::Run(ruy::Tuning, ruy::SidePair<ruy::PEMat> const&, void const*, ruy::SidePair<int> const&, ruy::SidePair<int> const&, ruy::EMat*)':
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x22d): undefined reference to `ruy::Kernel8bitAvx(ruy::KernelParams8bit<8, 8> const&)'
transpose_conv.cc:(.text._ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE[_ZN3ruy9RunKernelINS_6KernelILNS_4PathE16EaaiiEEE3RunENS_6TuningERKNS_8SidePairINS_5PEMatEEEPKvRKNS6_IiEESF_PNS_4EMatE]+0x281): undefined reference to `ruy::Kernel8bitAvxSingleCol(ruy::KernelParams8bit<8, 8> const&)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::MulFrontEnd<(ruy::Path)113, float, float, float, float>(ruy::Mat<float> const&, ruy::Mat<float> const&, ruy::MulParams<float, float> const&, ruy::Ctx*, ruy::Mat<float>*)':
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x37): undefined reference to `ruy::Ctx::clear_performance_advisories()'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x223): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2b0): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2e9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2f1): undefined reference to `ruy::Ctx::GetMainAllocator()'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x33b): undefined reference to `ruy::Allocator::AllocateBytes(long)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EffffEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x459): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void tflite::cpu_backend_gemm::Gemm<float, float, float, float, (tflite::cpu_backend_gemm::QuantizationFlavor)0>(tflite::cpu_backend_gemm::MatrixParams<float> const&, float const*, tflite::cpu_backend_gemm::MatrixParams<float> const&, float const*, tflite::cpu_backend_gemm::MatrixParams<float> const&, float*, tflite::cpu_backend_gemm::GemmParams<float, float, (tflite::cpu_backend_gemm::QuantizationFlavor)0> const&, tflite::CpuBackendContext*)':
transpose_conv.cc:(.text._ZN6tflite16cpu_backend_gemm4GemmIffffLNS0_18QuantizationFlavorE0EEEvRKNS0_12MatrixParamsIT_EEPKS4_RKNS3_IT0_EEPKSA_RKNS3_IT2_EEPSG_RKNS0_10GemmParamsIT1_SG_XT3_EEEPNS_17CpuBackendContextE[_ZN6tflite16cpu_backend_gemm4GemmIffffLNS0_18QuantizationFlavorE0EEEvRKNS0_12MatrixParamsIT_EEPKS4_RKNS3_IT0_EEPKSA_RKNS3_IT2_EEPSG_RKNS0_10GemmParamsIT1_SG_XT3_EEEPNS_17CpuBackendContextE]+0x257): undefined reference to `ruy::get_ctx(ruy::Context*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::MulFrontEnd<(ruy::Path)113, signed char, signed char, int, int>(ruy::Mat<signed char> const&, ruy::Mat<signed char> const&, ruy::MulParams<int, int> const&, ruy::Ctx*, ruy::Mat<int>*)':
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x37): undefined reference to `ruy::Ctx::clear_performance_advisories()'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x220): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2a5): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2e1): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2e9): undefined reference to `ruy::Ctx::GetMainAllocator()'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x333): undefined reference to `ruy::Allocator::AllocateBytes(long)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EaaiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x453): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void tflite::ops::builtin::transpose_conv::EvalQuantizedPerChannel<(tflite::ops::builtin::transpose_conv::KernelType)1>(TfLiteContext*, TfLiteTransposeConvParams const*, tflite::ops::builtin::transpose_conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
transpose_conv.cc:(.text._ZN6tflite3ops7builtin14transpose_conv23EvalQuantizedPerChannelILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_[_ZN6tflite3ops7builtin14transpose_conv23EvalQuantizedPerChannelILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_]+0x615): undefined reference to `ruy::get_ctx(ruy::Context*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void ruy::MulFrontEnd<(ruy::Path)113, unsigned char, unsigned char, int, int>(ruy::Mat<unsigned char> const&, ruy::Mat<unsigned char> const&, ruy::MulParams<int, int> const&, ruy::Ctx*, ruy::Mat<int>*)':
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x37): undefined reference to `ruy::Ctx::clear_performance_advisories()'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x20a): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x28f): undefined reference to `ruy::MulFrontEndFromTrMulParams(ruy::Ctx*, ruy::TrMulParams*)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2c9): undefined reference to `ruy::Ctx::set_performance_advisory(ruy::PerformanceAdvisory)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x2d1): undefined reference to `ruy::Ctx::GetMainAllocator()'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x31b): undefined reference to `ruy::Allocator::AllocateBytes(long)'
transpose_conv.cc:(.text._ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE[_ZN3ruy11MulFrontEndILNS_4PathE113EhhiiEEvRKNS_3MatIT0_EERKNS2_IT1_EERKNS_9MulParamsIT2_T3_EEPNS_3CtxEPNS2_ISD_EE]+0x41f): undefined reference to `ruy::Ctx::SelectPath(ruy::Path)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(transpose_conv.cc.o): In function `void tflite::ops::builtin::transpose_conv::EvalQuantized<(tflite::ops::builtin::transpose_conv::KernelType)1>(TfLiteContext*, TfLiteTransposeConvParams const*, tflite::ops::builtin::transpose_conv::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
transpose_conv.cc:(.text._ZN6tflite3ops7builtin14transpose_conv13EvalQuantizedILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_[_ZN6tflite3ops7builtin14transpose_conv13EvalQuantizedILNS2_10KernelTypeE1EEEvP13TfLiteContextPK25TfLiteTransposeConvParamsPNS2_6OpDataEPK12TfLiteTensorSE_SE_SE_PSC_SF_SF_]+0x60b): undefined reference to `ruy::get_ctx(ruy::Context*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::SubgraphInvoke(TfLiteContext*, TfLiteNode*)':
xnnpack_delegate.cc:(.text+0x762): undefined reference to `xnn_setup_runtime'
xnnpack_delegate.cc:(.text+0x7c5): undefined reference to `xnn_invoke_runtime'
xnnpack_delegate.cc:(.text+0x85c): undefined reference to `xnn_setup_runtime'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReshapeNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteReshapeParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&) [clone .isra.102]':
xnnpack_delegate.cc:(.text+0xe75): undefined reference to `xnn_define_static_reshape'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitReluNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, float, float, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x1151): undefined reference to `xnn_define_clamp'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMaxPool2DNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x142b): undefined reference to `xnn_define_max_pooling_2d'
xnnpack_delegate.cc:(.text+0x157c): undefined reference to `xnn_define_clamp'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMeanNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteReducerParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x175e): undefined reference to `xnn_define_global_average_pooling_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitResizeBilinearNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteResizeBilinearParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x1cb6): undefined reference to `xnn_define_static_resize_bilinear_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitPadNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x21c8): undefined reference to `xnn_define_static_constant_pad'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeMaxPoolingNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x24c8): undefined reference to `xnn_define_argmax_pooling_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitDepthwiseConv2DNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteDepthwiseConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x2b75): undefined reference to `xnn_define_depthwise_convolution_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitFullyConnectedNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteFullyConnectedParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x3442): undefined reference to `xnn_define_fully_connected'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeDeconvolutionNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteTransposeConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x391b): undefined reference to `xnn_define_deconvolution_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitPreluNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x3b39): undefined reference to `xnn_define_prelu'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitMediaPipeUnpoolingNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLitePoolParams const*, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x3fbe): undefined reference to `xnn_define_unpooling_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitConv2DNode(xnn_subgraph*, TfLiteContext*, int, TfLiteNode*, TfLiteTensor const*, TfLiteConvParams const*, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x4848): undefined reference to `xnn_define_convolution_2d'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::VisitNode(xnn_subgraph*, TfLiteContext*, TfLiteRegistration*, TfLiteNode*, int, std::unordered_set<int, std::hash<int>, std::equal_to<int>, std::allocator<int> > const&, std::vector<unsigned int, std::allocator<unsigned int> > const&)':
xnnpack_delegate.cc:(.text+0x4a20): undefined reference to `xnn_define_bankers_rounding'
xnnpack_delegate.cc:(.text+0x4b40): undefined reference to `xnn_define_elu'
xnnpack_delegate.cc:(.text+0x4c58): undefined reference to `xnn_define_ceiling'
xnnpack_delegate.cc:(.text+0x4e15): undefined reference to `xnn_define_minimum2'
xnnpack_delegate.cc:(.text+0x4f8d): undefined reference to `xnn_define_maximum2'
xnnpack_delegate.cc:(.text+0x50a0): undefined reference to `xnn_define_sigmoid'
xnnpack_delegate.cc:(.text+0x5203): undefined reference to `xnn_define_leaky_relu'
xnnpack_delegate.cc:(.text+0x5406): undefined reference to `xnn_define_subtract'
xnnpack_delegate.cc:(.text+0x5689): undefined reference to `xnn_define_average_pooling_2d'
xnnpack_delegate.cc:(.text+0x586a): undefined reference to `xnn_define_add2'
xnnpack_delegate.cc:(.text+0x5980): undefined reference to `xnn_define_floor'
xnnpack_delegate.cc:(.text+0x5ab5): undefined reference to `xnn_define_depth_to_space'
xnnpack_delegate.cc:(.text+0x5c18): undefined reference to `xnn_define_abs'
xnnpack_delegate.cc:(.text+0x5d8d): undefined reference to `xnn_define_squared_difference'
xnnpack_delegate.cc:(.text+0x5f5e): undefined reference to `xnn_define_multiply2'
xnnpack_delegate.cc:(.text+0x6186): undefined reference to `xnn_define_divide'
xnnpack_delegate.cc:(.text+0x6318): undefined reference to `xnn_define_square_root'
xnnpack_delegate.cc:(.text+0x6428): undefined reference to `xnn_define_negate'
xnnpack_delegate.cc:(.text+0x65a4): undefined reference to `xnn_define_square'
xnnpack_delegate.cc:(.text+0x66cf): undefined reference to `xnn_define_hardswish'
xnnpack_delegate.cc:(.text+0x67e8): undefined reference to `xnn_define_softmax'
xnnpack_delegate.cc:(.text+0x6ddc): undefined reference to `xnn_define_clamp'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `TfLiteXNNPackDelegateCreate':
xnnpack_delegate.cc:(.text+0x7000): undefined reference to `xnn_initialize'
xnnpack_delegate.cc:(.text+0x709a): undefined reference to `pthreadpool_destroy'
xnnpack_delegate.cc:(.text+0x7189): undefined reference to `pthreadpool_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(xnnpack_delegate.cc.o): In function `tflite::xnnpack::(anonymous namespace)::Subgraph::Create(TfLiteContext*, TfLiteDelegateParams const*, tflite::xnnpack::(anonymous namespace)::Delegate const*)':
xnnpack_delegate.cc:(.text+0x8c23): undefined reference to `xnn_create_subgraph'
xnnpack_delegate.cc:(.text+0x9493): undefined reference to `xnn_delete_subgraph'
xnnpack_delegate.cc:(.text+0x978d): undefined reference to `xnn_define_quantized_tensor_value'
xnnpack_delegate.cc:(.text+0x9974): undefined reference to `xnn_define_tensor_value'
xnnpack_delegate.cc:(.text+0x9dad): undefined reference to `xnn_delete_subgraph'
xnnpack_delegate.cc:(.text+0x9edf): undefined reference to `xnn_create_runtime_v2'
xnnpack_delegate.cc:(.text+0x9f24): undefined reference to `xnn_delete_runtime'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(audio_spectrogram.cc.o): In function `tflite::ops::custom::audio_spectrogram::Init(TfLiteContext*, char const*, unsigned long)':
audio_spectrogram.cc:(.text+0x1011): undefined reference to `flatbuffers::ClassicLocale::instance_'
audio_spectrogram.cc:(.text+0x13ab): undefined reference to `flatbuffers::ClassicLocale::instance_'
audio_spectrogram.cc:(.text+0x15dc): undefined reference to `flatbuffers::ClassicLocale::instance_'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(conv3d.cc.o): In function `tflite::ops::builtin::conv3d::EvalFloat(tflite::ops::builtin::conv3d::KernelType, TfLiteContext*, TfLiteNode*, TfLiteConv3DParams*, tflite::ops::builtin::conv3d::OpData*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor const*, TfLiteTensor*, TfLiteTensor*, TfLiteTensor*)':
conv3d.cc:(.text+0x1405): undefined reference to `ruy::get_ctx(ruy::Context*)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::SetMaxNumThreads(int)':
cpu_backend_context.cc:(.text+0x1f): undefined reference to `ruy::Context::set_max_num_threads(int)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::~CpuBackendContext()':
cpu_backend_context.cc:(.text+0xe2): undefined reference to `pthread_join'
cpu_backend_context.cc:(.text+0x158): undefined reference to `ruy::Context::~Context()'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::CpuBackendContext()':
cpu_backend_context.cc:(.text+0x250): undefined reference to `ruy::Context::Context()'
cpu_backend_context.cc:(.text+0x3cb): undefined reference to `pthread_join'
cpu_backend_context.cc:(.text+0x446): undefined reference to `ruy::Context::~Context()'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(cpu_backend_context.cc.o): In function `tflite::CpuBackendContext::ClearCaches()':
cpu_backend_context.cc:(.text._ZN6tflite17CpuBackendContext11ClearCachesEv[_ZN6tflite17CpuBackendContext11ClearCachesEv]+0x5): undefined reference to `ruy::Context::ClearPrepackedCache()'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `tflite::optimized_integer_ops::DepthwiseConvHybridPerChannel(tflite::DepthwiseParams const&, float const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, float const*, int*, tflite::CpuBackendContext*)':
depthwise_conv.cc:(.text._ZN6tflite21optimized_integer_ops29DepthwiseConvHybridPerChannelERKNS_15DepthwiseParamsEPKfRKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PfS5_PiPNS_17CpuBackendContextE[_ZN6tflite21optimized_integer_ops29DepthwiseConvHybridPerChannelERKNS_15DepthwiseParamsEPKfRKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PfS5_PiPNS_17CpuBackendContextE]+0x7f2): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `tflite::optimized_integer_ops::DepthwiseConvPerChannel(tflite::DepthwiseParams const&, int const*, int const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, signed char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, signed char*, tflite::CpuBackendContext*)':
depthwise_conv.cc:(.text._ZN6tflite21optimized_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKiS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PaPNS_17CpuBackendContextE[_ZN6tflite21optimized_integer_ops23DepthwiseConvPerChannelERKNS_15DepthwiseParamsEPKiS5_RKNS_12RuntimeShapeEPKaS8_SA_S8_S5_S8_PaPNS_17CpuBackendContextE]+0x7e2): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `void tflite::optimized_ops::DepthwiseConv<float, float>(tflite::DepthwiseParams const&, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float const*, tflite::RuntimeShape const&, float*, tflite::CpuBackendContext*)':
depthwise_conv.cc:(.text._ZN6tflite13optimized_ops13DepthwiseConvIffEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE[_ZN6tflite13optimized_ops13DepthwiseConvIffEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE]+0x984): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(depthwise_conv.cc.o): In function `void tflite::optimized_ops::DepthwiseConv<unsigned char, int>(tflite::DepthwiseParams const&, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, unsigned char const*, tflite::RuntimeShape const&, int const*, tflite::RuntimeShape const&, unsigned char*, tflite::CpuBackendContext*)':
depthwise_conv.cc:(.text._ZN6tflite13optimized_ops13DepthwiseConvIhiEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE[_ZN6tflite13optimized_ops13DepthwiseConvIhiEEvRKNS_15DepthwiseParamsERKNS_12RuntimeShapeEPKT_S7_SA_S7_PKT0_S7_PS8_PNS_17CpuBackendContextE]+0x964): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(detection_postprocess.cc.o): In function `tflite::ops::custom::detection_postprocess::Init(TfLiteContext*, char const*, unsigned long)':
detection_postprocess.cc:(.text+0x313c): undefined reference to `flatbuffers::ClassicLocale::instance_'
detection_postprocess.cc:(.text+0x3191): undefined reference to `flatbuffers::ClassicLocale::instance_'
detection_postprocess.cc:(.text+0x31f3): undefined reference to `flatbuffers::ClassicLocale::instance_'
detection_postprocess.cc:(.text+0x3264): undefined reference to `flatbuffers::ClassicLocale::instance_'
detection_postprocess.cc:(.text+0x3863): undefined reference to `flatbuffers::ClassicLocale::instance_'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(detection_postprocess.cc.o):detection_postprocess.cc:(.text._ZNK11flexbuffers9Reference7AsInt64Ev[_ZNK11flexbuffers9Reference7AsInt64Ev]+0xce): more undefined references to `flatbuffers::ClassicLocale::instance_' follow
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(eigen_support.cc.o): In function `tflite::eigen_support::GetThreadPoolDevice(TfLiteContext*)':
eigen_support.cc:(.text+0x1e84): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(lsh_projection.cc.o): In function `tflite::ops::builtin::lsh_projection::RunningSignBit(TfLiteTensor const*, TfLiteTensor const*, float)':
lsh_projection.cc:(.text+0x2e1): undefined reference to `util::Fingerprint64(char const*, unsigned long)'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(mfcc.cc.o): In function `tflite::ops::custom::mfcc::Init(TfLiteContext*, char const*, unsigned long)':
mfcc.cc:(.text+0x13a0): undefined reference to `flatbuffers::ClassicLocale::instance_'
mfcc.cc:(.text+0x1414): undefined reference to `flatbuffers::ClassicLocale::instance_'
mfcc.cc:(.text+0x148a): undefined reference to `flatbuffers::ClassicLocale::instance_'
mfcc.cc:(.text+0x195b): undefined reference to `flatbuffers::ClassicLocale::instance_'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(mirror_pad.cc.o): In function `tflite::ops::builtin::mirror_pad::Eval(TfLiteContext*, TfLiteNode*)':
mirror_pad.cc:(.text+0x1e24): undefined reference to `pthread_create'
mirror_pad.cc:(.text+0x1fc2): undefined reference to `pthread_create'
mirror_pad.cc:(.text+0x24b4): undefined reference to `pthread_create'
mirror_pad.cc:(.text+0x2654): undefined reference to `pthread_create'
mirror_pad.cc:(.text+0x29ac): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(numeric_verify.cc.o): In function `tflite::ops::custom::numeric_verify::Init(TfLiteContext*, char const*, unsigned long)':
numeric_verify.cc:(.text+0xc91): undefined reference to `flatbuffers::ClassicLocale::instance_'
numeric_verify.cc:(.text+0x10a7): undefined reference to `flatbuffers::ClassicLocale::instance_'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(reduce.cc.o): In function `tflite::optimized_ops::Mean(tflite::MeanParams const&, tflite::RuntimeShape const&, unsigned char const*, int, float, tflite::RuntimeShape const&, unsigned char*, int, float, tflite::CpuBackendContext*)':
reduce.cc:(.text._ZN6tflite13optimized_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKhifS6_PhifPNS_17CpuBackendContextE[_ZN6tflite13optimized_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKhifS6_PhifPNS_17CpuBackendContextE]+0xef6): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(reduce.cc.o): In function `tflite::optimized_integer_ops::Mean(tflite::MeanParams const&, tflite::RuntimeShape const&, signed char const*, int, float, tflite::RuntimeShape const&, signed char*, int, float, tflite::CpuBackendContext*)':
reduce.cc:(.text._ZN6tflite21optimized_integer_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKaifS6_PaifPNS_17CpuBackendContextE[_ZN6tflite21optimized_integer_ops4MeanERKNS_10MeanParamsERKNS_12RuntimeShapeEPKaifS6_PaifPNS_17CpuBackendContextE]+0xf26): undefined reference to `pthread_create'
/home/yeverino/.conan/data/tensorFlowLite/0.1/demo/testing/package/d351525cc53ebe68279edf1978846402420066e7/lib/libtensorflow-lite.a(spectrogram.cc.o): In function `tflite::internal::Spectrogram::ProcessCoreFFT()':
spectrogram.cc:(.text+0xed): undefined reference to `rdft'
collect2: error: ld returned 1 exit status
CMakeFiles/example.dir/build.make:96: recipe for target 'bin/example' failed
make[2]: *** [bin/example] Error 1
CMakeFiles/Makefile2:82: recipe for target 'CMakeFiles/example.dir/all' failed
make[1]: *** [CMakeFiles/example.dir/all] Error 2
Makefile:90: recipe for target 'all' failed
make: *** [all] Error 2
`
"
50148,How to qat model with BN," 1. System information

- Linux Ubuntu 16.04):
- TensorFlow-gpu2.2.0


Error:
 Layer batch_normalization:<class 'tensorflow.python.keras.layers.normalization_v2.BatchNormalization'> is not supported. You can quantize this layer by passing a `tfmot.quantization.keras.QuantizeConfig` instance to the `quantize_annotate_layer` API.
"
50147,systemlib com_github_grpc_grpc fails,"**System information**
- OS Platform and Distribution (e.g., openSUSE Tumbleweed):

- TensorFlow installed from source
- TensorFlow version: 2.5.0
- Python version: 3.8.10
- Bazel version: 3.7
- GCC/Compiler version: 11.1.1

**Describe the problem**
When using the systemlib `com_github_grpc_grpc` the compilation fails with following error
```
  Repository rule _tf_http_archive defined at:
  /home/abuild/rpmbuild/BUILD/tensorflow2-2.5.0/third_party/repo.bzl:65:35: in <toplevel>
 ERROR: /home/abuild/rpmbuild/BUILD/tensorflow2-2.5.0/tensorflow/core/data/service/BUILD:536:16: no such package '@com_github_grpc_grpc//src/compiler': BUILD file not found in directory 'src/compiler' of external repository @com_github_grpc_grpc. Add a BUILD file to a directory to mark it as a package. and referenced by '//tensorflow/core/data/service:_worker_cc_grpc_proto_grpc_codegen'
 ERROR: Analysis of target '//tensorflow/tools/pip_package:build_pip_package' failed; build aborted: Analysis failed
```
I have checked the file `tensorflow/workspace2.bzl` where following definitions can be found:
```
    tf_http_archive(
        name = ""com_github_grpc_grpc"",
        sha256 = ""b956598d8cbe168b5ee717b5dafa56563eb5201a947856a6688bbeac9cac4e1f"",
        strip_prefix = ""grpc-b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd"",
        system_build_file = ""//third_party/systemlibs:grpc.BUILD"",
        patch_file = ""//third_party/grpc:generate_cc_env_fix.patch"",
        system_link_files = {
            ""//third_party/systemlibs:BUILD"": ""bazel/BUILD"",
            ""//third_party/systemlibs:grpc.BUILD"": ""src/compiler/BUILD"",
            ""//third_party/systemlibs:grpc.bazel.grpc_deps.bzl"": ""bazel/grpc_deps.bzl"",
            ""//third_party/systemlibs:grpc.bazel.grpc_extra_deps.bzl"": ""bazel/grpc_extra_deps.bzl"",
            ""//third_party/systemlibs:grpc.bazel.cc_grpc_library.bzl"": ""bazel/cc_grpc_library.bzl"",
            ""//third_party/systemlibs:grpc.bazel.generate_cc.bzl"": ""bazel/generate_cc.bzl"",
            ""//third_party/systemlibs:grpc.bazel.protobuf.bzl"": ""bazel/protobuf.bzl"",
        },
        urls = [
            ""https://storage.googleapis.com/mirror.tensorflow.org/github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz"",
            ""https://github.com/grpc/grpc/archive/b54a5b338637f92bfcf4b0bc05e0f57a5fd8fadd.tar.gz"",
        ],
    )
```
which means that instead of `src/compiler/BUILD`  the file `//third_party/systemlibs:grpc.BUILD` is used. This file is available, so I do not understand why bazel is complaing. 

Is this a problem with the systemlbs in 3.5.0 or a problem with `com_github_grpc_grpc`?"
50145,Building current master fails due to missing files,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian 11
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version: git/master
- Python version: 3.9
- Installed using virtualenv? pip? conda?: conda
- Bazel version (if compiling from source): 4.1.0
- GCC/Compiler version (if compiling from source): 11
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A



**Describe the problem**

Building TF from the current source on GH fails due to missing files (all from `llvm-openmp`):
`tools.pm`
`kmp.h`
`kmp_platform.h`
`kmp_os.h`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
My build command was as follows:
```bash
bazel build  --config=mkl -config=nogcp --config=nonccl  -c opt --copt=-march=native --copt=-O3 -s //tensorflow/tools/pip_package:build_pip_package
```

**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50144,Re-initializing backends on demand,"**Describe the current behavior**

While using Jax, I have a use case where I wish to control the CPU devices being used based on `num_devices` input. But this couldn't be changed once the backend has been initialized.

```python
>>> import os
>>> import jax
>>> from jax.lib import xla_bridge
>>> from jaxlib import xla_client
>>> print(jax.devices(""cpu""))
[CpuDevice(id=0)]
>>> os.environ[""XLA_FLAGS""] = ""--xla_force_host_platform_device_count=8""
>>> print(xla_client.get_local_backend(""cpu"").devices())
[CpuDevice(id=0)]
```

**Describe the expected behavior**

I wish to re-initialize the backend and the new backend pick up the env variables, and return 8 CPU devices in the above example.
"
50142," Input 0 of layer max_pooling2d is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, None, None, None, 16)","my generator codde is:
IMG_HEIGHT=256
IMG_WIDTH=256
MASK_HEIGHT=256
MASK_WIDTH=256
IMG_CHANNELS=3
# generator fxn
 
def Generator(X_list, y_list, batch_size = 4):
    c = 0
 
    while(True):
        X = np.empty((batch_size, IMG_HEIGHT, IMG_WIDTH,IMG_CHANNELS), dtype = 'float32')
        y = np.empty((batch_size, MASK_HEIGHT, MASK_WIDTH,1), dtype = 'float32')
        
        for i in range(c,c+batch_size):
            image = X_list[i]
          
            mask =  y_list[i]
          
           
        X = X[:,:,:,np.newaxis] / 255   # normalization 
        y = y[:,:,:,np.newaxis] / 255   # normalization
        
        c += batch_size
        if(c+batch_size >= len(X_list)):
            c = 0
        yield X, y

and model code is:
#Build the model
inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))
s = tf.keras.layers.Lambda(lambda x: x / 255)(inputs)

#Contraction path
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)
c1 = tf.keras.layers.Dropout(0.1)(c1)
c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)
p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)

c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)
c2 = tf.keras.layers.Dropout(0.1)(c2)
c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)
p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)
 
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)
c3 = tf.keras.layers.Dropout(0.2)(c3)
c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)
p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)
 
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)
c4 = tf.keras.layers.Dropout(0.2)(c4)
c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)
p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)
 
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)
c5 = tf.keras.layers.Dropout(0.3)(c5)
c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)

#Expansive path 
u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)
u6 = tf.keras.layers.concatenate([u6, c4])
c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)
c6 = tf.keras.layers.Dropout(0.2)(c6)
c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)
 
u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)
u7 = tf.keras.layers.concatenate([u7, c3])
c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)
c7 = tf.keras.layers.Dropout(0.2)(c7)
c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)
 
u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)
u8 = tf.keras.layers.concatenate([u8, c2])
c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)
c8 = tf.keras.layers.Dropout(0.1)(c8)
c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)
 
u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)
u9 = tf.keras.layers.concatenate([u9, c1], axis=3)
c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)
c9 = tf.keras.layers.Dropout(0.1)(c9)
c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)
 
outputs = tf.keras.layers.Conv2D(1, (1, 1), activation='softmax')(c9)

when i am trying to train this model using:
results1 = model.fit(train_gen, steps_per_epoch=steps_per_epoch, epochs = epochs,
                             validation_data = val_gen, validation_steps = validation_steps,callbacks=[checkpoint1], verbose=2)
i am getting this error:
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **
        outputs = model.train_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step
        y_pred = self(x, training=True)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:425 call
        inputs, training=training, mask=mask)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph
        outputs = node.layer(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility
        str(tuple(shape)))

    ValueError: Input 0 of layer max_pooling2d is incompatible with the layer: expected ndim=4, found ndim=5. Full shape received: (None, None, None, None, 16)


how to resolve this issue?"
50139,What is the reason for very low GPU utilization?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
```
$ lsb_release -a
LSB Version:	:core-4.1-amd64:core-4.1-noarch
Distributor ID:	CentOS
Description:	CentOS Linux release 7.9.2009 (Core)
Release:	7.9.2009
Codename:	Core
```

- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below):
```
$ pip freeze | grep tensorflow
tensorflow-estimator==2.2.0
tensorflow-gpu==2.2.0
 
```
- Python version:
```
$ python
Python 3.8.5 (default, Mar 31 2021, 02:37:07) 
[GCC 7.3.1 20180303 (Red Hat 7.3.1-5)] on linux
```

- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
```
$ gcc --version
gcc (GCC) 7.3.0
Copyright (C) 2017 Free Software Foundation, Inc.
```
- CUDA/cuDNN version:
```
stat /usr/local/cuda
  File: ‘/usr/local/cuda’ -> ‘/usr/local/cuda-10.2’
  Size: 20          Blocks: 0          IO Block: 4096   symbolic link
Device: fd00h/64768d    Inode: 67157410    Links: 1
Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
Context: unconfined_u:object_r:usr_t:s0
Access: 2021-05-20 10:43:06.864530636 -0400
Modify: 2020-09-21 09:39:18.559883390 -0400
Change: 2020-09-21 09:39:18.559883390 -0400
 Birth: -
```

- GPU model and memory:
```
$ sudo lshw -C display
[sudo] password for jalal: 
  *-display                 
       description: VGA compatible controller
       product: GP102 [GeForce GTX 1080 Ti]
       vendor: NVIDIA Corporation
       physical id: 0
       bus info: pci@0000:06:00.0
       version: a1
       width: 64 bits
       clock: 33MHz
       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom
       configuration: driver=nvidia latency=0
       resources: irq:89 memory:f8000000-f8ffffff memory:a0000000-afffffff memory:b0000000-b1ffffff ioport:d000(size=128) memory:f9000000-f907ffff
  *-display
       description: VGA compatible controller
       product: GP102 [GeForce GTX 1080 Ti]
       vendor: NVIDIA Corporation
       physical id: 0
       bus info: pci@0000:05:00.0
       version: a1
       width: 64 bits
       clock: 33MHz
       capabilities: pm msi pciexpress vga_controller bus_master cap_list rom
       configuration: driver=nvidia latency=0
       resources: irq:88 memory:fa000000-faffffff memory:c0000000-cfffffff memory:d0000000-d1ffffff ioport:e000(size=128) memory:fb000000-fb07ffff

```

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`
```
>>> import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)
v2.2.0-rc4-8-g2b96f3662b 2.2.0

```
**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.


I am not sure why this code is barely using GPU memory and why the GPU utilization is very low? It shows it is using both of my GPUs indeed. How can I fix this? Also, for 1000 images, how long does this step take approximately?

`$ python Feature_extraction.py --input_list ../vertex_path_GT.txt --model pointnet_hico --model_path ../Feature_extraction/model_10000.ckpt`

I run the code from this directory:
`/scratch3/research/code/DJ-RN-dawnlight/pointnet`


![Screenshot from 2021-06-08 02-13-21](https://user-images.githubusercontent.com/76495162/121132783-8fea0a80-c7ff-11eb-83c6-980227676076.png)
![Screenshot from 2021-06-08 02-13-26](https://user-images.githubusercontent.com/76495162/121132785-8fea0a80-c7ff-11eb-81ff-672f69a94151.png)


Here's a copy of your code:

```
import argparse
import math
import h5py
import numpy as np
#import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
import socket
import importlib
import os
import sys
#import cPickle as pickle
import pickle
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append(BASE_DIR)
sys.path.append(os.path.join(BASE_DIR, 'models'))
sys.path.append(os.path.join(BASE_DIR, 'utils'))
import tf_util

parser = argparse.ArgumentParser()
parser.add_argument('--gpu', type=int, default=0, help='GPU to use [default: GPU 0]')
parser.add_argument('--model', default='pointnet_hico', help='Model name: pointnet_cls or pointnet_cls_basic [default: pointnet_cls]')
parser.add_argument('--num_point', type=int, default=1228, help='Point Number [256/512/1024/2048] [default: 1024]')
parser.add_argument('--model_path', default='log/model.ckpt', help='model checkpoint file path [default: log/model.ckpt]')
parser.add_argument('--input_list', default='./', help='Path list of your point cloud files [default: ./pc_list.txt]')
FLAGS = parser.parse_args()


NUM_POINT = FLAGS.num_point
#GPU_INDEX = FLAGS.gpu
GPU_INDEX = 1
print(""GPU_INDEX: "", GPU_INDEX)
MODEL_PATH = FLAGS.model_path
BATCH_SIZE = 1
MODEL = importlib.import_module(FLAGS.model) # import network module
MODEL_FILE = os.path.join(BASE_DIR, 'models', FLAGS.model+'.py')

MAX_NUM_POINT = 1228
NUM_CLASSES = 600

HOSTNAME = socket.gethostname()
print('HOSTNAME: ', HOSTNAME)

def evaluate():
    #with tf.device('/gpu:'+str(GPU_INDEX)):
    with tf.device('/device:gpu:1'):
        pointclouds_pl = MODEL.placeholder_inputs(BATCH_SIZE, NUM_POINT)
        is_training_pl = tf.placeholder(tf.bool, shape=())


        # simple model
        feat = MODEL.get_model(pointclouds_pl, is_training_pl)
        
        # Add ops to save and restore all the variables.
        saver = tf.train.Saver()
        
    # Create a session
    config = tf.ConfigProto()
    config.gpu_options.allow_growth = True
    config.allow_soft_placement = True
    config.log_device_placement = True
    sess = tf.Session(config=config)

    # Restore variables from disk.
    saver.restore(sess, MODEL_PATH)

    ops = {'pointclouds_pl': pointclouds_pl,
           'is_training_pl': is_training_pl,
           'feat': feat}

    eval_one_epoch(sess, ops)

   
def eval_one_epoch(sess, ops):
    is_training = False
    input_list = None
    with open(FLAGS.input_list, 'r') as f:
        input_list = f.readlines()
    
    for fn in range(len(input_list)):
        current_data = pickle.load(open(fn, 'rb'))
        current_data = current_data[None, :NUM_POINT, :]
        
            
        feed_dict = {ops['pointclouds_pl']: current_data,
                     ops['is_training_pl']: is_training}
        feat = sess.run([ops['feat']], feed_dict=feed_dict)
        print('filename: ', fn)
        pickle.dump(feat, open(fn[:-4] + '_feature.pkl', 'wb'))

with tf.Graph().as_default():
    evaluate()
```

here's `nvtop` output:
![Screenshot from 2021-06-08 02-24-06](https://user-images.githubusercontent.com/76495162/121133688-a6449600-c800-11eb-9893-b2763ebbbaca.png)

https://github.com/DirtyHarryLYL/DJ-RN/issues/64
"
50138,Need to fetch version from TFLite Model,"https://github.com/tensorflow/tensorflow/issues/47176 

I have downloaded / created tfLite Model.

How do i know the version of the tfLite Model from just the flatbuffer model(*.tflite ) file..
Can i fetch the version information from this model ?

During automation, I was trying to fetch the TFLite Models and running inference over them. Currently , I am using TFLite 2.4.1 library. The models created above this versions which has unsupported operations, need to error out.

What is the best way of handling ? How to get TFLite version from the model."
50137,custom op: how to delete variables in private,"For `OpKernel`, the example shows OpKernelConstruction and Compute. Which function can we override to delete and free pointer variables defined in `private`?  
For example, pytorch custom op can do 
```
private:
   SpecialVariable<T> *vector_variables;

~CustomOp() override {
  delete [] vector_variables
}
```

tensorflow example: https://www.tensorflow.org/guide/create_op"
50136,How to get any tensorflow version to work with CUDA 10.2 in CentOS 7?,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.



How should I fix this in CentOS 7?

    [jalal@goku ~]$ pip freeze | grep tensorflow
    tensorflow-estimator==2.2.0
    tensorflow-gpu==2.2.0
    [jalal@goku ~]$ python
    Python 3.8.5 (default, Mar 31 2021, 02:37:07) 
    [GCC 7.3.1 20180303 (Red Hat 7.3.1-5)] on linux
    Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
    >>> import tensorflow as tf
    >>> print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))
    2021-06-07 23:50:07.811271: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
    2021-06-07 23:50:07.867796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
    pciBusID: 0000:05:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
    coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
    2021-06-07 23:50:07.869403: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
    pciBusID: 0000:06:00.0 name: GeForce GTX 1080 Ti computeCapability: 6.1
    coreClock: 1.6705GHz coreCount: 28 deviceMemorySize: 10.92GiB deviceMemoryBandwidth: 451.17GiB/s
    2021-06-07 23:50:07.870136: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:
    2021-06-07 23:50:07.874249: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
    2021-06-07 23:50:07.877819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
    2021-06-07 23:50:07.878745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
    2021-06-07 23:50:07.882687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
    2021-06-07 23:50:07.884788: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
    2021-06-07 23:50:07.890952: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
    2021-06-07 23:50:07.891011: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
    Skipping registering GPU devices...
    Num GPUs Available:  0

This is despite having two GPUs:
[![enter image description here][1]][1]

    [jalal@goku ~]$ lsb_release -a
    LSB Version:	:core-4.1-amd64:core-4.1-noarch
    Distributor ID:	CentOS
    Description:	CentOS Linux release 7.9.2009 (Core)
    Release:	7.9.2009
    Codename:	Core


also, 

    $ nvcc --version
    nvcc: NVIDIA (R) Cuda compiler driver
    Copyright (c) 2005-2018 NVIDIA Corporation
    Built on Sat_Aug_25_21:08:01_CDT_2018
    Cuda compilation tools, release 10.0, V10.0.130

I tried the following as suggested by https://github.com/tensorflow/tensorflow/issues/38194#issuecomment-629801937 and didn't work:

    [jalal@goku djrn]$ ls /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2
    lrwxrwxrwx. 1 root root 20 Sep 21  2020 /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 -> libcudart.so.10.2.89
    [jalal@goku djrn]$ sudo ln -s /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudart.so.10.2 /usr/lib/x86_64-linux-gnu/libcudart.so.10.1
    [sudo] password for jalal: 
    ln: failed to create symbolic link ‘/usr/lib/x86_64-linux-gnu/libcudart.so.10.1’: No such file or directory



To be specific, I need tensforflow to work with CUDA 10.2, I am fine with any version of tensorflow (preference is tensorflow 2+), however couldn't find a version that works with CUDA 10.2. https://www.tensorflow.org/install/source#tested_build_configurations 

Also, based on this, my `CUDA` version is `10.2` which is different from both `nvidia-smi` and `nvcc --version` versions: 

    $ stat /usr/local/cuda
      File: ‘/usr/local/cuda’ -> ‘/usr/local/cuda-10.2’
      Size: 20        	Blocks: 0          IO Block: 4096   symbolic link
    Device: fd00h/64768d	Inode: 67157410    Links: 1
    Access: (0777/lrwxrwxrwx)  Uid: (    0/    root)   Gid: (    0/    root)
    Context: unconfined_u:object_r:usr_t:s0
    Access: 2021-05-20 10:43:06.864530636 -0400
    Modify: 2020-09-21 09:39:18.559883390 -0400
    Change: 2020-09-21 09:39:18.559883390 -0400
     Birth: -


P.S.: I have made my virtual environment using `python venv` command and don't want to use `conda` or `pyenv`.
  [1]: https://i.stack.imgur.com/g2CFZ.png

"
50132,CMake Error Building Tensorflow Lite,"**System information**
- OS Platform: Windows 10 Professional
- TensorFlow installed from source
- TensorFlow version: Latest
- Installed using Git/CMake Version 3.20.0
- Visual Studio Community 2019 16.9.31112.23


**Describe the problem**
**Provide the exact sequence of commands / steps that you executed before running into the problem**

I am attempting to build TFLM in VS2019 with the Eigen library, as reported in Issue #48255: https://github.com/tensorflow/tensorflow/issues/48255 

As part of troubleshooting that issue, I am trying to follow the basic instructions for building Tensorflow Lite using CMake, and the first basic build attempt fails. Following the instructions as per this page:

https://www.tensorflow.org/lite/guide/build_cmake#build_tensorflow_lite_c_library

My entire command prompt history for following the above instructions are below.
C:\Users\jtork>mkdir cmake_fflite
C:\Users\jtork>cd cmake_tflite
C:\Users\jtork\cmake_tflite>git clone https://github.com/tensorflow/tensorflow.git tensorflow_src
C:\Users\jtork\cmake_tflite>mkdir tflite_build
C:\Users\jtork\cmake_tflite>cd tflite_build
C:\Users\jtork\cmake_tflite\tflite_build>cmake ../tensorflow_src/tensorflow/lite

The above lines complete successfully. The output logs from the first cmake are included below in ""Output Log 1"".

Then the command below is entered to build the project:
C:\Users\jtork\cmake_tflite\tflite_build>cmake --build . -j

The above line fails. The output logs from this command are included below in ""Output Log 2"". The failure occurs in building the project tflite_build\flatbuffers-flatc.vcxproj, which throws an error [C2220](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-errors-1/compiler-error-c2220?f1url=%3FappId%3DDev16IDEF1%26l%3DEN-US%26k%3Dk(C2220)%26rd%3Dtrue&view=msvc-160) for warning treated as an error, for warning [C5430](https://docs.microsoft.com/en-us/cpp/error-messages/compiler-warnings/compiler-warning-level-1-c4530?f1url=%3FappId%3DDev16IDEF1%26l%3DEN-US%26k%3Dk(C4530)%26rd%3Dtrue&view=msvc-160). 

This warning is generated from the tflite_build\flatbuffers-flatc\src\flatbuffers\flatc-build\flatc.vcxproj project, which references a ""vector"" (no extension) file located in C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include. The error is line 847 of this file, which uses a _TRY_BEGIN macro that apparently doesn't use unwind semantics for a try handler (the project is not configured to use unwind semantics). I was not able to identify the compiler option for it to use unwind semantics, which is the /EHsc compiler option.

I believe this will be a multi-layered onion in getting to a point where I can successfully build a TFLM example project that uses and pulls in the Eigen library for LSTM models. But this is the first step, and as far as I can tell the basic CMake Tensorflow Lite build is broken.

*****************************************************************
************************* Output Log 1 ************************
*****************************************************************
C:\Users\jtork\cmake_tflite\tflite_build>cmake ../tensorflow_src/tensorflow/lite
-- Building for: Visual Studio 16 2019
-- Setting build type to Release, for debug builds use'-DCMAKE_BUILD_TYPE=Debug'.
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- The C compiler identification is MSVC 19.28.29913.0
-- The CXX compiler identification is MSVC 19.28.29913.0
-- Detecting C compiler ABI info
-- Detecting C compiler ABI info - done
-- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped
-- Detecting C compile features
-- Detecting C compile features - done
-- Detecting CXX compiler ABI info
-- Detecting CXX compiler ABI info - done
-- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped
-- Detecting CXX compile features
-- Detecting CXX compile features - done
-- Looking for pthread.h
-- Looking for pthread.h - not found
-- Found Threads: TRUE
-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11
-- Performing Test EIGEN_COMPILER_SUPPORT_CPP11 - Failed
-- Performing Test COMPILER_SUPPORT_std=cpp03
-- Performing Test COMPILER_SUPPORT_std=cpp03 - Failed
-- Performing Test standard_math_library_linked_to_automatically
-- Performing Test standard_math_library_linked_to_automatically - Success
-- Standard libraries to link to explicitly: none
-- Performing Test COMPILER_SUPPORT_OPENMP
-- Performing Test COMPILER_SUPPORT_OPENMP - Success
-- Looking for a Fortran compiler
-- Looking for a Fortran compiler - NOTFOUND
--
-- Configured Eigen 3.4.99
--
-- Available targets (use: cmake --build . --target TARGET):
-- ---------+--------------------------------------------------------------
-- Target   |   Description
-- ---------+--------------------------------------------------------------
-- install  | Install Eigen. Headers will be installed to:
--          |     <CMAKE_INSTALL_PREFIX>/<INCLUDE_INSTALL_DIR>
--          |   Using the following values:
--          |     CMAKE_INSTALL_PREFIX: C:/Program Files (x86)/tensorflow-lite
--          |     INCLUDE_INSTALL_DIR:  include/eigen3
--          |   Change the install location of Eigen headers using:
--          |     cmake . -DCMAKE_INSTALL_PREFIX=yourprefix
--          |   Or:
--          |     cmake . -DINCLUDE_INSTALL_DIR=yourdir
-- doc      | Generate the API documentation, requires Doxygen & LaTeX
-- blas     | Build BLAS library (not the same thing as Eigen)
-- uninstall| Remove files installed by the install target
-- ---------+--------------------------------------------------------------
--
-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT
-- Performing Test FARMHASH_HAS_BUILTIN_EXPECT - Failed
-- Looking for _strtof_l
-- Looking for _strtof_l - found
-- Looking for _strtoui64_l
-- Looking for _strtoui64_l - found
-- The ASM compiler identification is MSVC
-- Found assembler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe
-- Downloading clog to C:/Users/jtork/cmake_tflite/tflite_build/clog-source (define CLOG_SOURCE_DIR to avoid it)
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/clog-download
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'clog'
  Performing download step (download, verify and extract) for 'clog'
  -- Downloading...
     dst='C:/Users/jtork/cmake_tflite/tflite_build/clog-download/clog-prefix/src/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'
     timeout='none'
     inactivity timeout='none'
  -- Using src='https://github.com/pytorch/cpuinfo/archive/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'
  -- [download 100% complete]
  -- verifying file...
         file='C:/Users/jtork/cmake_tflite/tflite_build/clog-download/clog-prefix/src/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'
  -- Downloading... done
  -- extracting...
       src='C:/Users/jtork/cmake_tflite/tflite_build/clog-download/clog-prefix/src/d5e37adf1406cf899d7d9ec1d317c47506ccb970.tar.gz'
       dst='C:/Users/jtork/cmake_tflite/tflite_build/clog-source'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  -- extracting... [clean up]
  -- extracting... done
  Generating clog-prefix/src/clog-stamp/Debug/clog-update
  Skipping patch step (no custom command) for 'clog'
  No configure step for 'clog'
  No build step for 'clog'
  No install step for 'clog'
  No test step for 'clog'
  Completed 'clog'
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/clog-download/CMakeLists.txt
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/clog-download/CMakeLists.txt
-- Downloading cpuinfo to C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-source (define CPUINFO_SOURCE_DIR to avoid it)
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'cpuinfo'
  Performing download step (download, verify and extract) for 'cpuinfo'
  -- Downloading...
     dst='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/cpuinfo-prefix/src/5916273f79a21551890fd3d56fc5375a78d1598d.zip'
     timeout='none'
     inactivity timeout='none'
  -- Using src='https://github.com/pytorch/cpuinfo/archive/5916273f79a21551890fd3d56fc5375a78d1598d.zip'
  -- [download 100% complete]
  -- verifying file...
         file='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/cpuinfo-prefix/src/5916273f79a21551890fd3d56fc5375a78d1598d.zip'
  -- Downloading... done
  -- extracting...
       src='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/cpuinfo-prefix/src/5916273f79a21551890fd3d56fc5375a78d1598d.zip'
       dst='C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-source'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  -- extracting... [clean up]
  -- extracting... done
  Generating cpuinfo-prefix/src/cpuinfo-stamp/Debug/cpuinfo-update
  Performing patch step (custom command) for 'cpuinfo'
  No configure step for 'cpuinfo'
  No build step for 'cpuinfo'
  No install step for 'cpuinfo'
  No test step for 'cpuinfo'
  Completed 'cpuinfo'
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/CMakeLists.txt
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/cpuinfo-download/CMakeLists.txt
-- Downloading FP16 to C:/Users/jtork/cmake_tflite/tflite_build/FP16-source (define FP16_SOURCE_DIR to avoid it)
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/FP16-download
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'fp16'
  Performing download step (download, verify and extract) for 'fp16'
  -- Downloading...
     dst='C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/fp16-prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'
     timeout='none'
     inactivity timeout='none'
  -- Using src='https://github.com/Maratyszcza/FP16/archive/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'
  -- [download 100% complete]
  -- verifying file...
         file='C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/fp16-prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'
  -- Downloading... done
  -- extracting...
       src='C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/fp16-prefix/src/0a92994d729ff76a58f692d3028ca1b64b145d91.zip'
       dst='C:/Users/jtork/cmake_tflite/tflite_build/FP16-source'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  -- extracting... [clean up]
  -- extracting... done
  Generating fp16-prefix/src/fp16-stamp/Debug/fp16-update
  Skipping patch step (no custom command) for 'fp16'
  No configure step for 'fp16'
  No build step for 'fp16'
  No install step for 'fp16'
  No test step for 'fp16'
  Completed 'fp16'
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/CMakeLists.txt
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FP16-download/CMakeLists.txt
-- Downloading FXdiv to C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-source (define FXDIV_SOURCE_DIR to avoid it)
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'fxdiv'
  Performing download step (download, verify and extract) for 'fxdiv'
  -- Downloading...
     dst='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/fxdiv-prefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'
     timeout='none'
     inactivity timeout='none'
  -- Using src='https://github.com/Maratyszcza/FXdiv/archive/b408327ac2a15ec3e43352421954f5b1967701d1.zip'
  -- [download 100% complete]
  -- verifying file...
         file='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/fxdiv-prefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'
  -- Downloading... done
  -- extracting...
       src='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/fxdiv-prefix/src/b408327ac2a15ec3e43352421954f5b1967701d1.zip'
       dst='C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-source'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  -- extracting... [clean up]
  -- extracting... done
  Generating fxdiv-prefix/src/fxdiv-stamp/Debug/fxdiv-update
  Skipping patch step (no custom command) for 'fxdiv'
  No configure step for 'fxdiv'
  No build step for 'fxdiv'
  No install step for 'fxdiv'
  No test step for 'fxdiv'
  Completed 'fxdiv'
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/CMakeLists.txt
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/FXdiv-download/CMakeLists.txt
-- Downloading pthreadpool to C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-source (define PTHREADPOOL_SOURCE_DIR to avoid it)
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'pthreadpool'
  Performing download step (download, verify and extract) for 'pthreadpool'
  -- Downloading...
     dst='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/pthreadpool-prefix/src/545ebe9f225aec6dca49109516fac02e973a3de2.zip'
     timeout='none'
     inactivity timeout='none'
  -- Using src='https://github.com/Maratyszcza/pthreadpool/archive/545ebe9f225aec6dca49109516fac02e973a3de2.zip'
  -- [download 100% complete]
  -- verifying file...
         file='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/pthreadpool-prefix/src/545ebe9f225aec6dca49109516fac02e973a3de2.zip'
  -- Downloading... done
  -- extracting...
       src='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/pthreadpool-prefix/src/545ebe9f225aec6dca49109516fac02e973a3de2.zip'
       dst='C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-source'
  -- extracting... [tar xfz]
  -- extracting... [analysis]
  -- extracting... [rename]
  -- extracting... [clean up]
  -- extracting... done
  Generating pthreadpool-prefix/src/pthreadpool-stamp/Debug/pthreadpool-update
  Skipping patch step (no custom command) for 'pthreadpool'
  No configure step for 'pthreadpool'
  No build step for 'pthreadpool'
  No install step for 'pthreadpool'
  No test step for 'pthreadpool'
  Completed 'pthreadpool'
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/CMakeLists.txt
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/pthreadpool-download/CMakeLists.txt
-- Downloading PSimd to C:/Users/jtork/cmake_tflite/tflite_build/psimd-source (define PSIMD_SOURCE_DIR to avoid it)
-- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/psimd-download
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'psimd'
  Performing download step (git clone) for 'psimd'
  Cloning into 'psimd-source'...
  Already on 'master'
  Your branch is up to date with 'origin/master'.
  Performing update step (git update) for 'psimd'
  Skipping patch step (no custom command) for 'psimd'
  No configure step for 'psimd'
  No build step for 'psimd'
  No install step for 'psimd'
  No test step for 'psimd'
  Completed 'psimd'
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/psimd-download/CMakeLists.txt
  Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/psimd-download/CMakeLists.txt
-- Configuring done
-- Generating done
-- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build

C:\Users\jtork\cmake_tflite\tflite_build>
*****************************************************************
************************* Output Log 2 ************************
*****************************************************************

C:\Users\jtork\cmake_tflite\tflite_build>cmake --build . -j
Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
Copyright (C) Microsoft Corporation. All rights reserved.

  Checking Build System
  Creating directories for 'flatbuffers-flatc'
  Skipping download step (SOURCE_DIR given) for 'flatbuffers-flatc'
  Generating flatbuffers-flatc/src/flatbuffers-flatc-stamp/Debug/flatbuffers-flatc-update
  Skipping patch step (no custom command) for 'flatbuffers-flatc'
  Performing configure step for 'flatbuffers-flatc'
  -- Selecting Windows SDK version 10.0.18362.0 to target Windows 10.0.19041.
  -- The C compiler identification is MSVC 19.28.29913.0
  -- The CXX compiler identification is MSVC 19.28.29913.0
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Check for working C compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Check for working CXX compiler: C:/Program Files (x86)/Microsoft Visual Studio/2019/Community/VC/Tools/MSVC/14.28.29910/bin/Hostx64/x64/cl.exe - skipped
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  -- Looking for _strtof_l
  -- Looking for _strtof_l - found
  -- Looking for _strtoui64_l
  -- Looking for _strtoui64_l - found
  -- Configuring done
  -- Generating done
  -- Build files have been written to: C:/Users/jtork/cmake_tflite/tflite_build/flatbuffers-flatc/src/flatbuffers-flatc-build
  Performing build step for 'flatbuffers-flatc'
  Microsoft (R) Build Engine version 16.9.0+57a23d249 for .NET Framework
  Copyright (C) Microsoft Corporation. All rights reserved.

    Checking Build System
    Building Custom Rule C:/Users/jtork/cmake_tflite/tflite_build/flatbuffers/CMakeLists.txt
    idl_parser.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_text.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    reflection.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\src\reflection.cpp(196): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::_Simple_types<_T
  y>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\src\reflection.cpp(306): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    util.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\istream(519,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\f
latbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\istream(513): message : while compiling class template member function 'std::basic_istream<char,std::char_traits<char>> &std::basic_istream<char,std::char_traits<char>>::read(_Elem *,std::streamsize)' [C:\Users\jtork\c
  make_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Elem=char
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\src\util.cpp(64): message : see reference to function template instantiation 'std::basic_istream<char,std::char_traits<char>> &std::basic_istream<char,std::char_traits<char>>::read(_Elem *,std::streamsize)' being compiled [C:\Users\jtork\cmake_tflite\tflite_bui
  ld\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Elem=char
            ]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\istream(699): message : see reference to class template instantiation 'std::basic_istream<char,std::char_traits<char>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-b
  uild\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\istream(519,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Us
ers\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_cpp.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_csharp.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_dart.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_kotlin.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_go.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_java.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_js_ts.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_php.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_python.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_lobster.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_lua.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_rust.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_fbs.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_grpc.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_json_schema.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    idl_gen_swift.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    Generating Code...
    Compiling...
    flatc.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    flatc_main.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    code_generators.cpp
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\fl
atbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(815): message : while compiling class template member function 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vec
  tor_val<std::_Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(1335): message : see reference to function template instantiation 'std::_Vector_iterator<std::_Vector_val<std::_Simple_types<_Ty>>> std::vector<_Ty,std::allocator<_Ty>>::insert(std::_Vector_const_iterator<std::_Vector_val<std::
  _Simple_types<_Ty>>>,const unsigned __int64,const _Ty &)' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
            with
            [
                _Ty=uint8_t
            ]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/flexbuffers.h(875): message : see reference to class template instantiation 'std::vector<uint8_t,std::allocator<uint8_t>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\vector(847,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Use
rs\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    cpp_generator.cc
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\f
latbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\Users\jtork\cmake_tflite\t
  flite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\Users\jtork\cmake_tflite\tflite_b
  uild\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-b
  uild\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Us
ers\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    go_generator.cc
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\f
latbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\Users\jtork\cmake_tflite\t
  flite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\grpc\src\compiler\go_generator.cc(43): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\Users\jtork\cmake_tflite\tf
  lite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-b
  uild\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Us
ers\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    java_generator.cc
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\f
latbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\Users\jtork\cmake_tflite\t
  flite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\Users\jtork\cmake_tflite\tflite_b
  uild\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-b
  uild\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Us
ers\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    python_generator.cc
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\f
latbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\Users\jtork\cmake_tflite\t
  flite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\Users\jtork\cmake_tflite\tflite_b
  uild\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-b
  uild\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Us
ers\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    swift_generator.cc
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): error C2220: the following warning is treated as an error [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Users\jtork\cmake_tflite\tflite_build\f
latbuffers-flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(269): message : while compiling class template member function 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' [C:\Users\jtork\cmake_tflite\t
  flite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers\include\flatbuffers/util.h(196): message : see reference to function template instantiation 'std::basic_ostream<char,std::char_traits<char>> &std::basic_ostream<char,std::char_traits<char>>::operator <<(int)' being compiled [C:\Users\jtork\cmake_tflite\tflite_b
  uild\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj]
  C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(641): message : see reference to class template instantiation 'std::basic_ostream<char,std::char_traits<char>>' being compiled [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-b
  uild\flatc.vcxproj]
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include\ostream(284,1): warning C4530: C++ exception handler used, but unwind semantics are not enabled. Specify /EHsc [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers-flatc\src\flatbuffers-flatc-build\flatc.vcxproj] [C:\Us
ers\jtork\cmake_tflite\tflite_build\flatbuffers-flatc.vcxproj]
    Generating Code...
C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Microsoft\VC\v160\Microsoft.CppCommon.targets(240,5): error MSB8066: Custom build for 'C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-flatc-mkdir.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMak
eFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-flatc-download.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-flatc-update.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-flatc-patch.rule;C:\Users\jtork\
cmake_tflite\tflite_build\CMakeFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-flatc-configure.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-flatc-build.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\677b4dc9deae3119b927753ee87e3f27\flatbuffers-fl
atc-install.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\079e40c0cfd707bfff9f1d804fd56055\flatbuffers-flatc-complete.rule;C:\Users\jtork\cmake_tflite\tflite_build\CMakeFiles\f737f6c2b0c30e020289204dac9114d1\flatbuffers-flatc.rule' exited with code 1. [C:\Users\jtork\cmake_tflite\tflite_build\flatbuffers
-flatc.vcxproj]

C:\Users\jtork\cmake_tflite\tflite_build>




"
50130,TensorFlow renames inputs when restoring/resaving SavedModel,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Any
- TensorFlow installed from (source or binary): Binary
- TensorFlow version (use command below): tf-nightly
- Python version: 3.7
- CUDA/cuDNN version: CPU
- GPU model and memory: CPU

**Describe the current behavior**
When a user restores a SavedModel and resaves it, TensorFlow renames the model inputs in the signature, prefixing them with `""inputs/""`. I'm not sure when this behavior was introduced (it happened non-deterministically in `tensorflow==2.4.1`, but is deterministic in `tensorflow==2.5.0` and `tf-nightly`) or expected. I have a use case where I am constrained in my input names in the signature, although I can work around it if renaming is expected (and deterministic) so this issue is mainly to raise awareness if the renaming is unexpected 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
import subprocess
import tensorflow as tf

inputs = {
    ""first_feature"": tf.constant([1, 2, 3]),
    ""second_feature"": tf.constant([4, 5, 6]),
}

class MyModel(tf.keras.Model):
    def call(self, inputs, training=None):
        return tf.random.uniform([3, 1])

model = MyModel()
model.compile(optimizer=""sgd"", loss=""mse"")
model.fit(inputs, tf.constant([7, 8, 9]))
model.save(""model"")
dict_model_def = subprocess.check_output(""saved_model_cli show --dir model --tag_set serve --signature_def serving_default"".split())
print(""MODEL"")
print(dict_model_def.decode(""utf-8""))

print(""RESTORED MODEL"")
restored_model = tf.keras.models.load_model(""model"")
restored_model.save(""restored_model"")
restored_model_def = subprocess.check_output(""saved_model_cli show --dir restored_model --tag_set serve --signature_def serving_default"".split())
print(restored_model_def.decode(""utf-8""))
```

Output:
```
1/1 [==============================] - 0s 117ms/step - loss: 56.5648
MODEL
The given SavedModel SignatureDef contains the following input(s):
  inputs['first_feature'] tensor_info:
      dtype: DT_INT32
      shape: (-1, 1)
      name: serving_default_first_feature:0
  inputs['second_feature'] tensor_info:
      dtype: DT_INT32
      shape: (-1, 1)
      name: serving_default_second_feature:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['output_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (3, 1)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict

RESTORED MODEL
The given SavedModel SignatureDef contains the following input(s):
  inputs['inputs/first_feature'] tensor_info:
      dtype: DT_INT32
      shape: (-1, 1)
      name: serving_default_inputs/first_feature:0
  inputs['inputs/second_feature'] tensor_info:
      dtype: DT_INT32
      shape: (-1, 1)
      name: serving_default_inputs/second_feature:0
The given SavedModel SignatureDef contains the following output(s):
  outputs['output_1'] tensor_info:
      dtype: DT_FLOAT
      shape: (3, 1)
      name: StatefulPartitionedCall:0
Method name is: tensorflow/serving/predict
```"
50127,GPU and NNAPI not working in Tensorflow Classify,"I am using this android aplication : https://github.com/tensorflow/examples/tree/master/lite/examples/image_classification

 In this when I am selecting GPU or NNAPI along with Float_EfficientNet or Float_MobileNet, I am getting the following error: ""Manipulating the hardware accelerators is not allowed in the task library. Only CPU is allowed"". I am using K20 Pro mobile which is equipped with Adreno 640 GPU. I have added all the dependencies which were asked."
50126, ' ValueError: `to_quantize` can only either be a tf.keras Sequential or Functional model.,"I've tried QAT implementation in my model training script. I'm using functional API for the model creation. 
Steps that I followed to implement QAT API:

1. Build the model acrhitecture
2. Inserted the appropriate quantize_model function
3. Train the model
Let me provide you the code snippet for more clearance
         
        words_input = Input(shape=(None,),dtype='int32',name='words_input')
        words = Embedding(input_dim=wordEmbeddings.shape[0], output_dim=wordEmbeddings.shape[1],  weights=[wordEmbeddings], trainable=False)(words_input)    
        ............
        convd_output= SeparableConv1D(kernel_size=4, filters=128, padding='same', activation='relu', strides=1, depth_multiplier=3, bias_regularizer=regularizers.l2(0.0001), kernel_constraint =min_max_norm(0.4,0.9))(output)
        #convd_output=tfmot.quantization.keras.quantize_annotate_layer(AveragePooling1D(pool_size=2, strides=1, padding='same')(convd_output)
        convd_output=AveragePooling1D(pool_size=2, strides=1, padding='same')(convd_output)
        convd_output=Dropout(0.1)(convd_output)	
        ...................
        flatten_output=TimeDistributed(Flatten())(pool_output)          
        
        output = TimeDistributed(Dense(len(label2Idx), activation='softmax'))(flatten_output)

        inputs_list=[words_input, casing_input,newline_input]
        model = Model(*inputs_list, output)
        q_aware_model = tfmot.quantization.keras.quantize_model(model)
        opt = Nadam(lr=0.0005)       
        q_aware_model.compile(loss='sparse_categorical_crossentropy', optimizer=opt,metrics = ['sparse_categorical_accuracy'])
        print(q_aware_model.summary())

**Version details**
tensorflow==1.15.3
tensorflow-model-optimization==0.5.0


**Issue :** I'm using functional API which i supported in QAT API, but still I'm getting a value error

 quantize_model     '`to_quantize` can only either be a tf.keras Sequential or
**ValueError:** `to_quantize` can only either be a tf.keras Sequential or Functional model.

Couldn't able to figure out the issue. It will be helpful if someone help to get over this. 
Thanks in advance"
50125,tf.case evaluates all operations,"Hi:

https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/case

From this official link, The tensors returned by the first pair whose predicate evaluated to True, or those returned by default if none does. If exclusive==False, execution stops at the first predicate which evaluates to True. But it seems that the tf.case still tries to run all operations although it should stop at the first one.

My code:
~~~~~~~~~~~~~~~~~~~~~
import tensorflow as tf
sess = tf.InteractiveSession()
x = 1.0
f1 = lambda: tf.constant(10.)
f2 = lambda: tf.constant(x/(x-1.0))
r = tf.case([(tf.less(x, 100.), f1)], default=f2, exclusive=False)
~~~~~~~~~~~~~~~~~~~~~~~
It produces error. r is supposed to return the value of f1, but it still tries to run f2 and produces error as follows: ZeroDivisionError: float division by zero.

Any suggestion what function I should use so the function can avoid run function f2 so it does not produce error? I have a more complex situation with this same issue. Thanks! I also tried tf.cond, it also produces error.  Thanks! 

**System information**
- OS Platform and Distribution: linux
- TensorFlow version: 1.15.0
- Python version: 3.7.6"
50124,Keras `image_dataset_from_directory` shuffles labels,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8
- CUDA version: 
```
nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_22:08:44_Pacific_Standard_Time_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
```
- cuDNN version: `cudnn-11.2-windows-x64-v8.1.1.33`
- GPU model and memory: RTX 2070 Super

**Describe the current behavior**
I have a folder structure with ~6500 different classes.
The structure is the following:
```
etlcdb
|->  0000
|       -> 0.jpg
|       -> 1.jpg
|       ...
|-> 0001
|       -> 0.jpg
|       -> 1.jpg
|       ...
|   .
|   .
|   .
|-> 6542
|       -> 0.jpg
|       -> 1.jpg
|       ...
```
I am using `tf.keras.preprocessing.image_dataset_from_directory` to create a `tf.data.dataset` from this folder structure.
Like this:
```
#batch size
bs=512
# class names
classes = [""%04d"" % i for i in range(len(labels))]

train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    directory=r'F:\data_sets\etlcdb',
    labels=""inferred"",
    label_mode=""categorical"",
    class_names=classes,
    color_mode=""grayscale"",
    batch_size=bs,
    image_size=(64, 64),
    validation_split=0.15,
    subset=""training"",
    seed=123
)
```
Output:
```
Found 6731099 files belonging to 6543 classes.
Using 5721435 files for training.
```
Afterwards I define some processing:
```
train = train_dataset.map(
    lambda x, y : (tf.cast(x, tf.float16), (tf.cast(y, tf.float16))),
    num_parallel_calls=tf.data.AUTOTUNE
)
train = train.cache(r""F:\data_sets\etlcdb_cache\cache_train"")
#train = train.shuffle(buffer_size=bs*3)
train = train.prefetch(buffer_size=tf.data.AUTOTUNE)
```
Finally I train the network with `fit()`.<br/><br/>
When than predictions are made the labels do not match anymore.
With a variable `labels` which contains the class labels matching the folder structure (attached as a .txt file) I execute following code:
```
sample = tf.keras.preprocessing.image.load_img(
    path= r""F:\data_sets\etlcdb\0002\3.jpg"",
    color_mode=""grayscale""
)
sample = tf.keras.preprocessing.image.img_to_array(sample)
sample = sample.reshape((1, 64, 64, 1))

prediction = f16_model.predict(sample)
```
Result (the text on top of the image is the prediction and its ""accuracy""):
![image](https://user-images.githubusercontent.com/51273483/121015823-b70ef080-c79b-11eb-984f-4d61e49e5e7f.png)
Because the CNN does detect all 2's as 惹 I am certain that the labels somehow get mixed up.

**Describe the expected behavior**
The labels inferred by the folder structure should line-up with the output tensor 
from the last layer of the CNN.

**Standalone code to reproduce the issue**
The notebook where this occurred can be found [here](https://github.com/CaptainDario/DaKanji-ML/blob/feature-recognize_more_characters/single_kanji_cnn/single_kanji_cnn_training.ipynb).
But the appropriate folder structure needs to be created.


[labels.txt](https://github.com/tensorflow/tensorflow/files/6608772/labels.txt)"
50121,No module named 'compat',"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- TensorFlow installed from (source or binary):binary
- TensorFlow version (use command below):2.x

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
i install conda env(include tensorflow, slim, object-detection etc)
No module named 'compat'

![企业微信截图_16230466543453](https://user-images.githubusercontent.com/50159788/120968328-4ff13c00-c79b-11eb-83b6-73bed46b9ac9.png)

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
50120,Adding Object-Detection / Segmentation applications to tf.keras.applications,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): TF 2.x
- Are you willing to contribute it (Yes/No): Yes 


## Describe the feature and the current behavior/state.

Currently, we have a convenient feature in `tf.keras.applications` to use sota **classification models**. I'm wondering it would be great to have sota **object detection (OD)** (e.g efficient-det) and **segmentation model (Seg)** also (e.g. unet like an arch. with classification model backbone). 

There are many open-source implementations of **OD** or **Seg** using `keras` or `tf.keras` but reliability and efficiency would be ensured if we have this application into `tf.keras.applications` modules. A similar framework like `pytorch` already has such [features](https://pytorch.org/vision/stable/models.html). It also provides a 3D model for video classification (ResNet 3D). 

**Will this change the current api? How?** would be enhanced. 

**Who will benefit from this feature?** ml practitioners. 

**Any Other info.**
Just don't close this issue without having discussion or feedback, (for example like [this one](https://github.com/keras-team/keras/issues/10596).)

P.S: I am aware of this https://github.com/tensorflow/models/tree/master/official). 


---

As `keras` moved to own repo, 
https://github.com/keras-team/keras/issues/15263"
50118,Provide option for FTRL Optimizer to not reset variable value as zero when its first gradient is zero,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Under current implementation of FTRL optimizer, the value of the variable becomes zero regardless of its original value when its gradient is zero on initial `apply_gradients`.

Reproducible snippet:

```python
opt = tf.keras.optimizers.Ftrl(learning_rate=0.1, initial_accumulator_value=0.1)
x = tf.Variable([-2, -1, 0, 1, 2], dtype=tf.float32)

with tf.GradientTape() as tape:
    loss = tf.math.reduce_sum(tf.nn.relu(x))

grads = tape.gradient(loss, [x])


print('Before applying gradients', x.numpy())
print('Gradients:', grads)
opt.apply_gradients(zip(grads, [x]))
print('After applying gradients', x.numpy())
```

Result:
```
Before applying gradients [-2. -1.  0.  1.  2.]
Gradients: [<tf.Tensor: shape=(5,), dtype=float32, numpy=array([0., 0., 0., 1., 1.], dtype=float32)>]
After applying gradients [0.        0.        0.        0.6031424 1.301631 ]
```
The first three values became zero even their gradients were zero.

This behavior is not a bug as we initialize all model parameters as zero in the original paper. This was not a problem for linear models, but is problematic for deep models due to the increased importance of initial weight distribution.

We can support its usage for deep models as well by providing an option to prevent this phenomenon.

**Will this change the current api? How?**
Yes, we may add a new boolean parameter in the constructor of `tf.keras.optimizers.Ftrl`.

**Who will benefit with this feature?**
Whoever uses FTRL optimizer for their deep models.

**Any Other info.**
I guess we can fix it by initializing the value of the `linear` slot based on the `variable` value when the slot is first created.
If it's desired change, I can prepare PR to fix it."
50117,TensorFlow build issue,"**System information**
Ubuntu 20.04.2 LTS

- TensorFlow installed from (source or binary): Built from source
- 
- TensorFlow version: 2.6.0
- Python version: 3.8.6
- Installed using virtualenv? pip? conda?: Installed in virtualenv after building from source
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source):  9.3.0
- CUDA/cuDNN version: Not used
- GPU model and memory: Not used



I have built tensorflow by following the steps in the website :
https://www.tensorflow.org/install/source#build_the_package

It was built using : 
bazel build—jobs=1 --config=opt//tensorflow/tools/pip_package:build_pip_package

and I hadn't included any additions flags for the build.

I managed to build it successfully and was able to use run the python codes for tensorflow.
But the following statements have been coming as warnings in the terminal when I run all the tensorflow python codes.How can this be rectified?


2021-06-06 12:36:27.944619: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count
2021-06-06 12:36:27.944670: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count
2021-06-06 12:36:27.944685: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api
2021-06-06 12:36:27.944718: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api
2021-06-06 12:36:28.236837: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/count
2021-06-06 12:36:28.236872: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/count
2021-06-06 12:36:28.236887: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/write/api
2021-06-06 12:36:28.236915: E tensorflow/core/lib/monitoring/collection_registry.cc:77] Cannot register 2 metrics with the same name: /tensorflow/core/saved_model/read/api




"
50116,how does tf name a variable in a partition?,"I found var names are not as what I defined them:
```
base-network/fuly_connected/weights/ghts/part_9/Adagrad
base-network/fuly_connected/weights/ights/part_10/Adagrad
```

How do these substrings come up? And I found diff num of partitions leads to diff var names. Could someone tell where the corresponding code are? Thanks!"
50115,ImportError: DLL load failed with error code 3221225501 while importing _pywrap_tensorflow_internal,"Traceback (most recent call last):
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""F:\python\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""F:\python\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501 while importing _pywrap_tensorflow_internal

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""demo5.py"", line 2, in <module>
    import tensorflow as tf
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\__init__.py"", line 50, in <module>
    from tensorflow.python import pywrap_tensorflow
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 69, in <module>
    raise ImportError(msg)
ImportError: Traceback (most recent call last):
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File ""C:\Users\风中叶子\AppData\Roaming\Python\Python38\site-packages\tensorflow\python\pywrap_tensorflow_internal.py"", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
  File ""F:\python\lib\imp.py"", line 242, in load_module
    return load_dynamic(name, filename, file)
  File ""F:\python\lib\imp.py"", line 342, in load_dynamic
    return _load(spec)
ImportError: DLL load failed with error code 3221225501 while importing _pywrap_tensorflow_internal


Failed to load the native TensorFlow runtime.

See https://www.tensorflow.org/install/errors

for some common reasons and solutions.  Include the entire stack trace
above this error message when asking for help.
"
50112,tf.gather prints a deprecation warning when inside a @tf.function,"**System information**
- Have I written custom code: Yes
- OS Platform and Distribution: Ubuntu 20.04
- TensorFlow installed from: binary (from pip)
- TensorFlow version: 2.5.0
- Python version: 3.8.5

**Describe the current behavior**

tf.gather prints a deprecation warning about validate_indices when used inside a @tf.function, even when the call to the tf.gather does not provide a validate_indices.

**Describe the expected behavior**

No deprecation warning.

**Standalone code to reproduce the issue**

```python
import tensorflow as tf

@tf.function
def foo(x):
  return tf.gather(x, 0)

a = tf.constant([1, 2, 3])
print('\n\nThe next line causes a deprecation warning.\n\n')
print(foo(a))
```
"
50111,ValueError: No gradients provided for any variable: ['Variable:0'],"I am using a custom training loop with GradientTape. This error `ValueError: No gradients provided for any variable: ['Variable:0']` is being thrown. 

I have written a complicated loss function so I imagine some operation I am doing is causing GradientTape to fail. What are some best practices to follow or common gotchas when doing calculations within a gradientape context? 

For example, should I avoid bit-wise operations, or working with numpy arrays instead of tensors? "
50110,"ptxas exited with non-zero error code 256, output ","version tensorflow 2.3.0-gpu

```shell
2021-06-06 03:21:05.290270: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-06-06 03:21:07.202513: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1
2021-06-06 03:21:07.231310: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.232673: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:00:08.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2021-06-06 03:21:07.232823: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-06-06 03:21:07.234815: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-06-06 03:21:07.236731: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-06-06 03:21:07.237019: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-06-06 03:21:07.239001: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-06-06 03:21:07.240099: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-06-06 03:21:07.244241: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-06-06 03:21:07.244420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.245793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.247102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-06-06 03:21:07.247458: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-06 03:21:07.254386: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2399995000 Hz
2021-06-06 03:21:07.254792: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7f42b809f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2021-06-06 03:21:07.254818: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2021-06-06 03:21:07.353171: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.354886: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f7f42a982f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2021-06-06 03:21:07.354922: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P40, Compute Capability 6.1
2021-06-06 03:21:07.355126: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.356205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: 
pciBusID: 0000:00:08.0 name: Tesla P40 computeCapability: 6.1
coreClock: 1.531GHz coreCount: 30 deviceMemorySize: 22.38GiB deviceMemoryBandwidth: 323.21GiB/s
2021-06-06 03:21:07.356253: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-06-06 03:21:07.356289: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-06-06 03:21:07.356307: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10
2021-06-06 03:21:07.356322: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10
2021-06-06 03:21:07.356337: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10
2021-06-06 03:21:07.356352: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10
2021-06-06 03:21:07.356367: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-06-06 03:21:07.356421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.357489: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:07.358524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0
2021-06-06 03:21:07.358576: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1
2021-06-06 03:21:08.072108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1257] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-06 03:21:08.072157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1263]      0 
2021-06-06 03:21:08.072167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1276] 0:   N 
2021-06-06 03:21:08.072387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:08.073502: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2021-06-06 03:21:08.074578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1402] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 21292 MB memory) -> physical GPU (device: 0, name: Tesla P40, pci bus id: 0000:00:08.0, compute capability: 6.1)
Model: ""movinet_classifier""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
image (InputLayer)           [(None, None, None, None, 0         
_________________________________________________________________
movinet (Movinet)            ({'stem': (None, None, No 911583    
_________________________________________________________________
classifier_head (ClassifierH (None, 600)               2214488   
=================================================================
Total params: 3,126,071
Trainable params: 3,111,799
Non-trainable params: 14,272
_________________________________________________________________
2021-06-06 03:21:17.983060: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10
2021-06-06 03:21:18.261529: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7
2021-06-06 03:21:19.360108: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running ptxas --version returned 256
2021-06-06 03:21:19.405225: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code 256, output: 
Relying on driver to perform ptx compilation. 
Modify $PATH to customize ptxas location.
This message will be only logged once.
2021-06-06 03:21:19.592207: W tensorflow/compiler/xla/service/gpu/buffer_comparator.cc:592] Internal: ptxas exited with non-zero error code 256, output: 
Relying on driver to perform ptx compilation. 
Setting XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda  or modifying $PATH can be used to set the location of ptxas
This message will only be logged once.
2021-06-06 03:21:19.671090: F tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:419] ptxas returned an error during compilation of ptx to sass: 'Internal: ptxas exited with non-zero error code 256, output: '  If the error message indicates that a file could not be written, please verify that sufficient filesystem space is provided.
Aborted (core dumped)
```"
50109,How to set/modify quantization_parameters (specifically quantized_dimension) for model conversion?,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installation (pip package or built from source): pip
- TensorFlow library (version, if pip package or github SHA, if built from source): tf-nightly 2.6.0.dev20210523  

### 2. Code

I have a model with an input shape (1, 128, 64, 16) with the ordering (NHWC), that is images with 16 channels. Conversion to tflite seems to be fine. Conversion to a quantized model also works fine using the following code.

```
converter = tf.lite.TFLiteConverter.from_saved_model(model_output_path)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8, tf.lite.OpsSet.SELECT_TF_OPS]
converter.inference_input_type =  tf.int8
converter.inference_output_type =  tf.int8
converter.representative_dataset = representative_dataset_gen
tflite_model = converter.convert()
with open(<TFLITE_FILE>, 'wb') as w:
    w.write(tflite_model)
```

However, the resulting quality of the quantized model is quite bad. Seems to be a problem with the quantization. Checking the input_details of the quantized model using

```
import tflite_runtime
from tflite_runtime.interpreter import Interpreter
interpreter = tflite_runtime.interpreter.Interpreter(<TFLITE_FILE>)
interpreter.allocate_tensors()
input_details = interpreter.get_input_details()
print(""Input details: {}"".format(input_details))
```

returns

```
Input details: [{'name': 'serving_default_model:0', 'index': 0, 'shape': array([  1, 128,  64,  16], dtype=int32), 'shape_signature': array([  1, 128,  64,  16], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.006251777522265911, -128), 'quantization_parameters': {'scales': array([0.00625178], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]
```

_**Obviously, the ""quantized_dimension"" is zero (that is, why we have only one scales and zero_points value. I would like to have ""quantized_dimension"" = 3 with 16 scales and 16 zero_points.**_

_**How can i tell the converter to use quantized_dimension = 3?**_


### 3. Failure after conversion

_**Conversion seems to be fine, however for 16 channel input images (input tensor shape = (1, 128, 64, 16)) the quantized_dimension = 0 (and i think it should be 3, to get channel-wise quantization).**_

Thanks for your help!
"
50108,ERROR: tensorflow-2.6.0-cp38-cp38-macosx_11_0_x86_64.whl is not a supported wheel on this platform.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Big Sur 11.4
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
-Source
- TensorFlow version:
- Tensorflow 2.6
- Python version:
- Python 3.8
- Installed using virtualenv? pip? conda?:
- Conda
- Bazel version (if compiling from source):
- basilisk
- GCC/Compiler version (if compiling from source):
- Apple clang version 12.0.5 (clang-1205.0.22.9)
Target: x86_64-apple-darwin20.5.0

- CUDA/cuDNN version:
- GPU model and memory:
- AMD Radeon



**Describe the problem**
ERROR: tensorflow-2.6.0-cp38-cp38-macosx_11_0_x86_64.whl is not a supported wheel on this platform.

**Provide the exact sequence of commands / steps that you executed before running into the problem**
 pip install /tmp/tensorflow_pkg/tensorflow-2.6.0-cp38-cp38-macosx_11_0_x86_64.whl


**Any other info / logs**
Here are the currently supported wheels.  Big Sur is OS X 11, prior wheels are OS X 10...

https://pypi.org/project/tensorflow/2.5.0/#files

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50123,Train cnn model,"tensorflow==2.5.0
run code on google colab

when I train my model I get this error and my code and message error bellow

My Code

import os

from silence_tensorflow import silence_tensorflow

silence_tensorflow()

import tensorflow as tf
from tensorflow.keras.layers import Dense, Activation, Dropout, Input, Conv2D, \
    MaxPooling2D, Flatten, BatchNormalization
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
tf.autograph.set_verbosity(0)
tf.get_logger().setLevel('ERROR')


class CharCNN(object):
    def __init__(self):
        self.Model = Sequential()
        self.build()

    def build(self):
        self.Model.add(Input(name='the_input', shape=(224, 224, 1), batch_size=16, dtype='float32'))
        self.Model.add(Conv2D(32, (3, 3), activation='sigmoid', name='conv1'))
        self.Model.add(MaxPooling2D(pool_size=(2, 2)))
        self.Model.add(Conv2D(32, (3, 3), activation='sigmoid', name='conv2'))
        self.Model.add(MaxPooling2D(pool_size=(2, 2)))
        self.Model.add(Conv2D(64, (3, 3), activation='relu', name='conv3'))
        self.Model.add(MaxPooling2D(pool_size=(2, 2)))
        self.Model.add(Flatten())

        self.Model.add(Dense(512))
        self.Model.add(Dropout(0.5))
        self.Model.add(BatchNormalization(scale=False))
        self.Model.add(Activation('relu'))
        self.Model.add(Dropout(0.5))
        self.Model.add(Dense(26, activation='softmax'))

    def summary(self):
        self.Model.summary()


if __name__ == ""__main__"":
    common_path = '/content/drive/MyDrive/alphdataset/'
    C = CharCNN()
    C.Model.compile(optimizer=""Adam"", loss='categorical_crossentropy', metrics=['accuracy'])
    C.Model.summary()

    with tf.device('/device:GPU:0'):
        batch_size = 16
        epochs = 50
        train_dir = common_path + 'train/'
        test_dir = common_path + 'test/'
        checkpoint_path =   '/content/drive/MyDrive/SavedModels/Alphabet/'
        train_image_generator = ImageDataGenerator(rescale=1. / 255)  # Generator for our training data
        train_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,
                                                                   directory=train_dir,
                                                                   shuffle=True,
                                                                   target_size=(224, 224),
                                                                   class_mode='categorical',
                                                                   color_mode='grayscale')

        test_image_generator = ImageDataGenerator(rescale=1. / 255)  # Generator for our test data
        test_data_gen = test_image_generator.flow_from_directory(batch_size=batch_size,
                                                                 directory=test_dir,
                                                                 shuffle=False,
                                                                 target_size=(224, 224),
                                                                 class_mode='categorical',
                                                                 color_mode='grayscale')

        # C.Model = tf.keras.models.load_model(checkpoint_path)
        
        callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',
                                                    patience=10,
                                                    restore_best_weights=True,
                                                    baseline=0.45)
        
        history = C.Model.fit(train_data_gen,
                              steps_per_epoch=337,  # Number of images // Batch size
                              epochs=epochs,
                              verbose=1,
                              validation_data=test_data_gen,
                              validation_steps=37,
                              callbacks=[callback])

        C.Model.save(checkpoint_path, save_format='tf')
        # Evaluate Model:
        # Accuracy: 48.13%
        C.Model.evaluate(test_data_gen)


--------------------------------------------------------------------------------------------------------------------

Message Error 

  File ""/content/drive/MyDrive/Lipify-LipReading/NN_Models/CharacterCNN.py"", line 85, in <module>
    callbacks=[callback])
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py"", line 1183, in fit
    tmp_logs = self.train_function(iterator)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py"", line 917, in _call
    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 3024, in __call__
    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 1961, in _call_flat
    ctx, args, cancellation_manager=cancellation_manager))
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py"", line 596, in call
    ctx=ctx)
  File ""/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py"", line 60, in quick_execute
    inputs, attrs, num_outputs)
tensorflow.python.framework.errors_impl.InvalidArgumentError:  required broadcastable shapes at loc(unknown)
	 [[node sequential/dropout/dropout/Mul_1 (defined at content/drive/MyDrive/Lipify-LipReading/NN_Models/CharacterCNN.py:85) ]] [Op:__inference_train_function_1105]

Function call stack:
train_function

^C
"
50107," Input 0 is incompatible with layer model: expected shape=(None, 256, 256, 1), found shape=(None, 128, 128, 3)","i am trying to evaluate my model but when i run model.evaluate(test_image), it showed this error:
ValueError: in user code:

    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1323 test_function  *
        return step_function(self, iterator)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1314 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
        return fn(*args, **kwargs)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1307 run_step  **
        outputs = model.test_step(data)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1266 test_step
        y_pred = self(x, training=False)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__
        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)
    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py:270 assert_input_compatibility
        ', found shape=' + display_shape(x.shape))

    ValueError: Input 0 is incompatible with layer model: expected shape=(None, 256, 256, 1), found shape=(None, 128, 128, 3)



How do i resolve it?"
50105, error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus',"**System information**
- OS Platform and Distribution: Ubuntu 18.04.1 [64-bit]
- TensorFlow installed from: source
- TensorFlow version: 2.4
- Python version: 3.7.5 [64-bit]
- Installed using: virtualenv
- Bazel version: 3.4.0
- GCC/Compiler version (if compiling from source):  7.5.0 (Ubuntu/Linaro 7.5.0-3ubuntu1~18.04) 
- Hardware: Raspberry Pi 4 Model B Rev 1.4
- Architecture: aarch64

**Describe the problem**
I could build TF 2.4 on a RPi 4 with Ubuntu 18.04 (64-bit) using bazel. However, when I tried doing this on a fresh one, it failed to build. What should I look into to make a comparison and see where the problem is?
Both have the same GCC version (7.5.0) and same bazel version (3.4.0).  Also in both caes, I'm using virtual enviornment (Python 3.7). I get this error:
`tensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'
                      dense_shape.AddDimWithStatus(input_shape_flat(i)));`

**Provide the exact sequence of commands / steps that you executed before running into the problem**
<pre>ubuntu@ubuntu:~$ sudo apt-get install build-essential pkg-confi gi2c-tools avahi-utils joystick libopenjp2–7-dev libtiff5-dev gfortran libatlas-base-dev libopenblas-dev libhdf5-serial-dev git ntp

ubuntu@ubuntu:~$ sudo apt-get install python3.7 python3.7-dev python3.7-venv python3.7-distutils python3.7-lib2to3 python3.7-gdbm python3.7-tk python3-pip

ubuntu@ubuntu:~$ python3.7 -m venv py37env

ubuntu@ubuntu:~$ source py37env/bin/activate

(py37env) ubuntu@ubuntu:~$ python3.7 -m pip install — upgrade pip

(py37env) ubuntu@ubuntu:~$ pip install cython  wheel numpy h5py pybind11

(py37env) ubuntu@ubuntu:~$ pip install -U keras_preprocessing — no-deps

(py37env) ubuntu@ubuntu:~$ git clone https://github.com/tensorflow/tensorflow.git

(py37env) ubuntu@ubuntu:~$ cd tensorflow/

(py37env) ubuntu@ubuntu:~/tensorflow$ git checkout r2.4

ubuntu@ubuntu:~/tensorflow$ python3.7 configure.py 
You have bazel 3.4.0 installed.
Please specify the location of python. [Default is /home/ubuntu/py37env/bin/python3.7]:
Found possible Python library paths:
 /home/ubuntu/py37env/lib/python3.7/site-packages
Please input the desired Python library path to use. Default is [/home/ubuntu/py37env/lib/python3.7/site-packages]
Do you wish to build TensorFlow with ROCm support? [y/N]: n
No ROCm support will be enabled for TensorFlow.
Do you wish to build TensorFlow with CUDA support? [y/N]: n
No CUDA support will be enabled for TensorFlow.
Do you wish to download a fresh release of clang? (Experimental) [y/N]: n
Clang will not be downloaded.
Please specify optimization flags to use during compilation when bazel option “ — config=opt” is specified [Default is -Wno-sign-compare]:
Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: n
Not configuring the WORKSPACE for Android builds.
Preconfigured Bazel build configs. You can use any of the below by adding “ — config=<>” to your build command. See .bazelrc for more details.
 — config=mkl # Build with MKL support.
 — config=mkl_aarch64 # Build with oneDNN support for Aarch64.
 — config=monolithic # Config for mostly static monolithic build.
 — config=ngraph # Build with Intel nGraph support.
 — config=numa # Build with NUMA support.
 — config=dynamic_kernels # (Experimental) Build kernels into separate shared objects.
 — config=v2 # Build TensorFlow 2.x instead of 1.x.
Preconfigured Bazel build configs to DISABLE default on features:
 — config=noaws # Disable AWS S3 filesystem support.
 — config=nogcp # Disable GCP support.
 — config=nohdfs # Disable HDFS support.
 — config=nonccl # Disable NVIDIA NCCL support.

(py37env) ubuntu@ubuntu:~/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package

(py37env) ubuntu@ubuntu:~/tensorflow$ python
Python 3.7.5 
[GCC 8.4.0] on linux
Type “help”, “copyright”, “credits” or “license” for more information.
>>> </pre>


**Any other info / logs**
<pre>(py37env) ubuntu@ubuntu:~/tensorflow$ bazel build //tensorflow/tools/pip_package:build_pip_package
INFO: Options provided by the client:
  Inherited 'common' options: --isatty=1 --terminal_columns=142
INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow/.bazelrc:
  Inherited 'common' options: --experimental_repo_remote_exec
INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow/.bazelrc:
  'build' options: --apple_platform_type=macos --define framework_shared_object=true --define open_source_build=true --java_toolchain=//third_party/toolchains/java:tf_java_toolchain --host_java_toolchain=//third_party/toolchains/java:tf_java_toolchain --define=tensorflow_enable_mlir_generated_gpu_kernels=0 --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --config=short_logs --config=v2
INFO: Reading rc options for 'build' from /home/ubuntu/tensorflow/.tf_configure.bazelrc:
  'build' options: --action_env PYTHON_BIN_PATH=/home/ubuntu/py37env/bin/python3.7 --action_env PYTHON_LIB_PATH=/home/ubuntu/py37env/lib/python3.7/site-packages --python_path=/home/ubuntu/py37env/bin/python3.7 --config=xla --action_env TF_CONFIGURE_IOS=0
INFO: Found applicable config definition build:short_logs in file /home/ubuntu/tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING
INFO: Found applicable config definition build:v2 in file /home/ubuntu/tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1
INFO: Found applicable config definition build:xla in file /home/ubuntu/tensorflow/.bazelrc: --define=with_xla_support=true
INFO: Found applicable config definition build:linux in file /home/ubuntu/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels
INFO: Found applicable config definition build:dynamic_kernels in file /home/ubuntu/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS
DEBUG: Rule 'io_bazel_rules_go' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1557349968 -0400""
DEBUG: Repository io_bazel_rules_go instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>
DEBUG: Rule 'io_bazel_rules_docker' indicated that a canonical reproducible form can be obtained by modifying arguments shallow_since = ""1556410077 -0400""
DEBUG: Repository io_bazel_rules_docker instantiated at:
  no stack (--record_rule_instantiation_callstack not enabled)
Repository rule git_repository defined at:
  /home/ubuntu/.cache/bazel/_bazel_ubuntu/ad1e09741bb4109fbc70ef8216b59ee2/external/bazel_tools/tools/build_defs/repo/git.bzl:195:33: in <toplevel>
WARNING: Download from https://mirror.bazel.build/github.com/aws/aws-sdk-cpp/archive/1.7.336.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found
INFO: Analyzed target //tensorflow/tools/pip_package:build_pip_package (171 packages loaded, 26394 targets configured).
INFO: Found 1 target...
ERROR: /home/ubuntu/tensorflow/tensorflow/core/kernels/BUILD:4689:18: C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1)
In file included from ./tensorflow/core/framework/op_kernel.h:35:0,
                 from tensorflow/core/kernels/sparse_split_op.cc:19:
tensorflow/core/kernels/sparse_split_op.cc: In member function 'void tensorflow::SparseSplitOp<T>::Compute(tensorflow::OpKernelContext*)':
tensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'
                      dense_shape.AddDimWithStatus(input_shape_flat(i)));
                                  ^
./tensorflow/core/framework/op_requires.h:52:29: note: in definition of macro 'OP_REQUIRES_OK'
     ::tensorflow::Status _s(__VA_ARGS__);                    \
                             ^~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
INFO: Elapsed time: 46060.242s, Critical Path: 750.27s
INFO: 9369 processes: 9369 local.
FAILED: Build did NOT complete successfully</pre>
"
50104,Incorrect figure on word2vec tutorial,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/text/word2vec#summary

## Description of issue (what needs changing):

### Clear description

The summary figure on https://www.tensorflow.org/tutorials/text/word2vec#summary seems incorrect.

The figure shows the index of shimmered is 7 while the code above says the index is 5. 

The red part of the figure has word 'temperature' and 'code', which didn't appear in the input sentence at all. 

The figure also has indexes like 784 and 589. I don't know what's going on here.

As a result, the figure becomes confusing for learners.

Please fix the figure.

### Submit a pull request?

No
"
50103,Ambiguous examples on word2vec tutorial?,"## URL(s) with the issue:

https://www.tensorflow.org/tutorials/text/word2vec

## Description of issue (what needs changing):

### Clear description

The code of the tutorial outputs the following:

```
target_index    : 3
target_word     : road
context_indices : [1 2 1 4 3]
context_words   : ['the', 'wide', 'the', 'shimmered', 'road']
label           : [1 0 0 0 0]
```

The context word 'the' has label both 1 and 0. I feel this example would be ambiguous.

When generating negative samples, should the one in postive samples be excluded?

### Submit a pull request?

No. I'm learning word2vec, not capable of writing a correct algorithm."
50099,Tensoflow tf.data.Dataset.from_tensor_slices couldn't generate a simple dataset to fit a linear model,"Just want to generate a dataset by myself, but it get errors. It puzzled me a lot. Any help will be greatly appreciated!

My system is Windows 10, Tensorflow version is tensorflow-cpu 2.5.

Here is code:

      import tensorflow as tf
      import numpy as np
    
      def simulation_linear_function(x, a, b):  # y = ax+b
          y = tf.matmul(a, x)+b
          return y
      
      # Generate data set
      num_input = 3
      num_dense_unit = 2
      a = np.array([0.1,0.2,0.3,0.4]).reshape((2,2))
      b = np.array([0.5,0.6]).reshape((2,1))
      
      num_data = 300
      inputs = np.random.random((num_data,2))
      labels = np.zeros((num_data,2))
      for n in range(num_data):
          x = inputs[n]
          y = simulation_linear_function(np.reshape(x, (2,1)), a, b)
          labels[n] = np.reshape(y, (2,))
      dataset = tf.data.Dataset.from_tensor_slices((inputs, labels))
      
      linear_layer = tf.keras.layers.Dense(units=2, input_shape=(2,))
      linear_model = tf.keras.Sequential([linear_layer])
      
      linear_model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(1))
      history = linear_model.fit(dataset, epochs=100, verbose=True)
      # history = linear_model.fit(inputs, labels, epochs=100, verbose=True)
  
The error is: Traceback (most recent call last): ..... ValueError: Input 0 of layer sequential is incompatible with the layer: expected axis -1 of input shape to have value 2 but received input with shape (2, 1)

But if you switch to the last commented line, the model can be trained correctly.

Thanks, Tom"
50096,How to address ptxas warning?,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution : CentOS Linux release 7.9.2009 (Core)
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version: 2.5.0
- Python version: 3.7.9
- Installed using: virtualenv
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA Version: 11.0/ Cuda compilation tools, release 9.1, V9.1.85
- GPU model and memory: Nvidia Tesla P100-16GB



**Describe the problem**
W tensorflow/stream_executor/gpu/asm_compiler.cc:99] *** WARNING *** You are using ptxas 9.1.121, which is older than 9.2.88. ptxas 9.x before 9.2.88 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.

You do not need to update to CUDA 9.2.88; cherry-picking the ptxas binary is sufficient.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50086,argmax for tf.sparse,"**Describe the feature and the current behavior/state.**
Similar to tf.sparse.reduce_max, define an argmax operation. I think, computing an argmax over a dimension is not possible with sparse tensors otherwise. If it is, a hint would be appreciated

**Will this change the current api? How?**
Add function tf.sparse.argmax

**Who will benefit with this feature?**
People using argmax / sparse tensors

**Any Other info.**

Thanks!"
50083,Training stuck with custom loss,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.0
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: RTX 2070 Ti

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
The training stops running after printing Epoch 1/20 when I use model.fit to run my model. It was fine until I added this loss function:

```
class snc_loss(tf.keras.losses.Loss):
    def call(self, y_true, y_pred):
        l_sigma_I = tf.constant(100.0, dtype=tf.float32)
        l_sigma_X = tf.constant(16.0, dtype=tf.float32)
        l_R = tf.constant(5.0, dtype=tf.float32)
        shape = tf.shape(y_pred)
        loss = tf.ones((shape[0],)) * tf.cast(shape[3], dtype=tf.float32)
        for channel in tf.range(shape[-1]):
            sub = tf.zeros((shape[0],))
            denom = tf.ones((shape[0],)) * 1e-6
            for i in tf.range(shape[1] * shape[2]):
                y_i = i%shape[2]
                x_i = i//shape[2]
                for j in tf.range(i+1, shape[1]*shape[2]):
                    y_j = j%shape[2]
                    x_j = j//shape[2]
                    eu_dis = tf.cast(tf.math.square(x_i - x_j)+tf.math.square(y_i - y_j), dtype=tf.float32)
                    pixel_dis = tf.cast(tf.math.square(y_true[:, x_i, y_i] - y_true[:, x_j, y_j]), dtype=tf.float32)
                    if tf.math.less(tf.math.sqrt(eu_dis), l_R):
                        weight = tf.math.exp(-1 * pixel_dis / l_sigma_I - eu_dis / l_sigma_X)
                        sub += weight * y_pred[:, x_j, y_j, channel] * y_pred[:, x_i, y_i, channel]
                        denom += weight * y_pred[:, x_i, y_i, channel]

            loss -= sub/denom

        return loss
```

**Describe the expected behavior**
It should be training

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing): no

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

https://colab.research.google.com/drive/1CqZTxlyUNlUDkNipoVRPhA3HHzjlvuUj?usp=sharing

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The error was still there when I was using Tensorflow 2.4.1, but it used to tell me SubProcess ended with return code : 0
Also, when I run a similar code as a job on a server, it throws a bus error, so might be related to memory."
50082,"tf not imported in core.py, yet used.","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  **CentOS 7.n**
- TensorFlow installed from (source or binary): **Binary**
- TensorFlow version (use command below):  **v2.4.0-49-g85c8b2a817f 2.4.1**
- Python version: **3.7.6**
- CUDA/cuDNN version: **N/A**
- GPU model and memory: **N/A**

**Describe the current behavior**
`...full.path.../lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py` code uses `tf`, yet it isn't imported.  I'm not sure why it failed in this particular case.

**Describe the expected behavior**
I am reporting this because I think its incorrect that a file references a name (import) which isn't imported ""above"".  But I don't understand the code base, so it may be perfectly acceptable.

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - ** no **

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs**

_Log slightly reduced and cleaned _

```
In [2]: input_model = keras.models.load_model(config['model']['path'])
2021-06-04 10:26:23.467261: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-06-04 10:26:23.470714: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: 
2021-06-04 10:26:23.470834: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)
2021-06-04 10:26:23.470916: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (us01odcvde12327): /proc/driver/nvidia/version does not exist
2021-06-04 10:26:23.471718: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-04 10:26:23.471955: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-06-04 10:26:24.001545: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.
2021-06-04 10:26:24.060087: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.
2021-06-04 10:26:24.091049: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.
2021-06-04 10:26:24.161196: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.
2021-06-04 10:26:24.211141: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 37748736 exceeds 10% of free system memory.
${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:1059: UserWarning: backend is not loaded, but a Lambda layer uses it. It may cause errors.
  , UserWarning)
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
${PROJECT_PATH}/capsule/export_model.py in <module>
----> 1 input_model = keras.models.load_model(config['model']['path'])

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py in load_model(filepath, custom_objects, compile, options)
    210       if isinstance(filepath, six.string_types):
    211         loader_impl.parse_saved_model(filepath)
--> 212         return saved_model_load.load(filepath, compile, options)
    213 
    214   raise IOError(

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in load(path, compile, options)
    145 
    146   # Finalize the loaded layers and remove the extra tracked dependencies.
--> 147   keras_loader.finalize_objects()
    148   keras_loader.del_tracking()
    149 

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in finalize_objects(self)
    610 
    611     # Initialize graph networks, now that layer dependencies have been resolved.
--> 612     self._reconstruct_all_models()
    613 
    614   def _unblock_model_reconstruction(self, layer_id, layer):

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_all_models(self)
    629       all_initialized_models.add(model_id)
    630       model, layers = self.model_layer_dependencies[model_id]
--> 631       self._reconstruct_model(model_id, model, layers)
    632       _finalize_config_layers([model])
    633 

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/load.py in _reconstruct_model(self, model_id, model, layers)
    676       (inputs, outputs,
    677        created_layers) = functional_lib.reconstruct_from_config(
--> 678            config, created_layers={layer.name: layer for layer in layers})
    679       model.__init__(inputs, outputs, name=config['name'])
    680       functional_lib.connect_ancillary_layers(model, created_layers)

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in reconstruct_from_config(config, custom_objects, created_layers)
   1283       if layer in unprocessed_nodes:
   1284         for node_data in unprocessed_nodes.pop(layer):
-> 1285           process_node(layer, node_data)
   1286 
   1287   input_tensors = []

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/functional.py in process_node(layer, node_data)
   1231         input_tensors = (
   1232             base_layer_utils.unnest_if_single_tensor(input_tensors))
-> 1233       output_tensors = layer(input_tensors, **kwargs)
   1234 
   1235       # Update node index map.

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in __call__(self, *args, **kwargs)
    950     if _in_functional_construction_mode(self, inputs, args, kwargs, input_list):
    951       return self._functional_construction_call(inputs, args, kwargs,
--> 952                                                 input_list)
    953 
    954     # Maintains info about the `Layer.call` stack.

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _functional_construction_call(self, inputs, args, kwargs, input_list)
   1089         # Check input assumptions set after layer building, e.g. input shape.
   1090         outputs = self._keras_tensor_symbolic_call(
-> 1091             inputs, input_masks, args, kwargs)
   1092 
   1093         if outputs is None:

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _keras_tensor_symbolic_call(self, inputs, input_masks, args, kwargs)
    820       return nest.map_structure(keras_tensor.KerasTensor, output_signature)
    821     else:
--> 822       return self._infer_output_signature(inputs, args, kwargs, input_masks)
    823 
    824   def _infer_output_signature(self, inputs, args, kwargs, input_masks):

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in _infer_output_signature(self, inputs, args, kwargs, input_masks)
    861           # TODO(kaftan): do we maybe_build here, or have we already done it?
    862           self._maybe_build(inputs)
--> 863           outputs = call_fn(inputs, *args, **kwargs)
    864 
    865         self._handle_activity_regularization(inputs, outputs)

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in call(self, inputs, mask, training)
    915     with backprop.GradientTape(watch_accessed_variables=True) as tape,\
    916         variable_scope.variable_creator_scope(_variable_creator):
--> 917       result = self.function(inputs, **kwargs)
    918     self._check_variables(created_variables, tape.watched_variables())
    919     return result

${CONDA_PATH}/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py in space_to_depth_x2(x)
     48         # the function to implement the orgnization layer (thanks to github.com/allanzelener/YAD2K)
     49         def space_to_depth_x2(x):
---> 50             return tf.nn.space_to_depth(x, block_size=2)
     51 
     52         # Layer 1

NameError: name 'tf' is not defined

```
"
50081,How to get feature value in custom loss function,"How can i get not only y_true and y_pred, but the last feature value from row that predicts y_pred in custom loss function to calculate loss with it. I use rnn in tf.keras."
50080,"XLA AOT compile failed for certain models on s390x, `aot_compiled_test` fail to build","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): N
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): v2.5.0-0-ga4dfb8d1a71 2.5.0
- Python version: 3.8.5
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
- CUDA/cuDNN version: N/A
- GPU model and memory: N/A

**Describe the current behavior**
When running test case `//tensorflow/python/tools:aot_compiled_test`, it will fail to build on s390x machine.

**Describe the expected behavior**
The test case should build and pass

**[Contributing](https://www.tensorflow.org/community/contribute)** 
The direct cause of the issue is `vector_register_num_elements` function in [target_machine_features.h](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/compiler/xla/service/cpu/target_machine_features.h#L87) returning 0 when aot compiler calls it. And this only happens for model [MatMulSmall](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/tools/aot_compiled_test.cc#L102). This issue is model-specific because if the output matrix (for matmul) has both dimensions greater than 32, then there will be no issue; the error only happens when either dimension of the output matrix is equal to or less than 32.

I am not entirely sure why `vector_register_num_elements` is returning 0 in certain cases, but I do have the following observation. In this test case, it calls the `saved_model_cli` command-line interface for compiling. The full command is as follow:
```
bazel-out/host/bin/tensorflow/python/tools/saved_model_cli aot_compile_cpu --dir ""$(dirname bazel-out/s390x-opt/bin/tensorflow/python/tools/x_matmul_y_small/saved_model.pb)"" --output_prefix bazel-out/s390x-opt/bin/tensorflow/python/tools/aot_compiled_x_matmul_y_small --cpp_class XMatmulYSmall --variables_to_feed '' --signature_def_key serving_default --multithreading False --target_triple systemz-none-linux-gnu --tag_set serve
```
I noticed that if we add `--target_cpu z14` flag here, the test case will pass. This is interesting because the `target_cpu` flag was added in this [commit](https://github.com/tensorflow/tensorflow/commit/e25d9862ca5c42997112c564f1253fd001bc4a15). I believe it should be left empty unless we are cross-compiling. In other words, if it is empty, LLVM compiler should use the host cpu by default. 

Due to the fact that once we explicitly specify the host cpu the test will pass, I think the issue here is that LLVM compiler could not detect the host cpu in default case. I have a temporary fix that could feed the `target_cpu` flag with host cpu to LLVM when it is absent:
```diff
diff --git a/tensorflow/python/BUILD b/tensorflow/python/BUILD
index 4cfc389eac6..c97b8c845aa 100644
--- a/tensorflow/python/BUILD
+++ b/tensorflow/python/BUILD
@@ -394,6 +394,7 @@ tf_python_pybind_extension(
     module_name = ""_pywrap_tfcompile"",
     deps = [
         "":tfcompile_headers_lib"",
+        ""@llvm-project//llvm:Support"",
         ""@pybind11"",
         ""//third_party/python_runtime:headers"",
         ""//tensorflow/python/lib/core:pybind11_lib"",
diff --git a/tensorflow/python/tfcompile_wrapper.cc b/tensorflow/python/tfcompile_wrapper.cc
index c8818309919..c24fbfbcb8d 100644
--- a/tensorflow/python/tfcompile_wrapper.cc
+++ b/tensorflow/python/tfcompile_wrapper.cc
@@ -15,6 +15,7 @@ limitations under the License.

 #include <string>

+#include ""llvm/Support/Host.h""
 #include ""pybind11/cast.h""
 #include ""pybind11/pybind11.h""
 #include ""pybind11/pytypes.h""
@@ -45,7 +46,8 @@ PYBIND11_MODULE(_pywrap_tfcompile, m) {
         flags.graph = std::move(graph);
         flags.config = std::move(config);
         flags.target_triple = std::move(target_triple);
-        flags.target_cpu = std::move(target_cpu);
+        flags.target_cpu = std::move(target_cpu.empty() ?
+                       llvm::sys::getHostCPUName().str() : target_cpu);
         flags.target_features = std::move(target_features);
         flags.entry_point = std::move(entry_point);
         flags.cpp_class = std::move(cpp_class);
```
I understand this might not be an ideal solution for the issue, so I wonder if there is another way we could feed such info to LLVM compiler? It will also be very helpful if anyone could help me identify the core issue here (why LLVM compiler cannot detect the host cpu in the default case?)

**Standalone code to reproduce the issue**
Run `bazel test --cache_test_results=no --build_tests_only --test_output=errors --verbose_failures -- //tensorflow/python/tools:aot_compiled_test` on s390x machine

**Other info / logs** 
Attaching the test log here: [aot_compiled_test.log](https://github.com/tensorflow/tensorflow/files/6599110/aot_compiled_test.log)
Please also note that this test case was passing for TensorFlow 2.4.0 and 2.4.1 release

"
50079, TypeError: tf__call() got an unexpected keyword argument 'y',"**System information**
I am using TensorFlow 2.5 with Python 3.6 I have pip-installed TF2.4 within an anaconda environment.


**Provide the text output from tflite_convert**

**Standalone code to reproduce the issue** 
import tensorflow as tf
assert float(tf.__version__[:3]) >= 2.3
config = tf.compat.v1.ConfigProto()
config.gpu_options.allow_growth = True
session = tf.compat.v1.InteractiveSession(config=config)
import tensorflow.keras as keras
import pathlib
import numpy as np
import tensorflow_model_optimization as tfmot
from tensorflow.keras.preprocessing.image import ImageDataGenerator

origin_model = tf.keras.applications.MobileNetV3Small(
    input_shape=(224, 224, 3), alpha=1.0, include_top=True, weights='imagenet',
    input_tensor=None, pooling=None, classes=1000,
    classifier_activation='softmax'
)
quant_aware_model = tfmot.quantization.keras.quantize_model(origin_model)
quant_aware_model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.00001),  # 0.045, momentum=0.9, decay=0.98),
                              loss='sparse_categorical_crossentropy',
                              metrics=['accuracy'])

**Any other info / logs**

![image](https://user-images.githubusercontent.com/32632952/120812356-7d619e00-c57f-11eb-84fb-d11cc3389b03.png)

When I use the API ""tfmot.quantization.keras.quantize_model"", there is a bug.I guess that some layers in mobilenetV3 do not support quantization operations, such as the lamba layer. I wonder if you have encountered it?

thanks in advance."
50078,Failed to convert SparseTensor to Tensor (TensorFlow 2.5),"Dear experts,
I am using TensorFlow 2.5 with Python 3.8.10. I have pip-installed TF2.5 within an anaconda environment.
When trying out this code [1] (where I create a dummy dataset consisting of 2 classes (label) of `SparseTensor`s and use it to train a simple CNN), I get the error [2]. I have seen that this issue has been raised for TF2.4 (https://github.com/tensorflow/tensorflow/issues/47931), but didn't see an issue raised for TF2.5. I know I can convert the sparse tensor to dense 'on-the-fly' in a generator Dataset and pass that is input. However, I wanted to run the training on sparse tensors themselves to speed it up. Is there any workaround known for TF2.5?

Thanks in advance.

[1]
```
import numpy
import tensorflow

from tensorflow.keras import datasets, layers, models
from tensorflow.keras import mixed_precision


# No. of images
nImg = 1000

nBinX = 50
nBinY = 50

# No. of layers/channels
nLayer = 1

nPixelTot = nBinX*nBinY

l_idx = []
l_val = []
l_label = []

# Create a sparse dataset with random entries
for iImg in range(0, nImg) :
    
    # Fill at most 60 pixels
    nFill = numpy.random.randint(low = 1, high = 61)
    
    for iFill in range(0, nFill) :
        
        # Index of the filled pixel
        # [image idx, row idx, col idx, layer]
        idx = [
            iImg,
            numpy.random.randint(low = 0, high = nBinY),
            numpy.random.randint(low = 0, high = nBinX),
            nLayer-1,
        ]
        
        if (idx in l_idx) :
            continue
        
        l_idx.append(idx)
        l_val.append(numpy.random.rand())
    
    l_label.append(numpy.random.randint(low = 0, high = 2))


img_shape = (nBinY, nBinX, nLayer)
dense_shape = (nImg, nBinY, nBinX, nLayer)

# Create the sparse rensor
input_img_sparseTensor = tensorflow.sparse.reorder(tensorflow.sparse.SparseTensor(
    indices = l_idx,
    values = l_val,
    dense_shape = dense_shape,
))


print(""=====> Creating dataset..."")

dataset_img = tensorflow.data.Dataset.from_tensor_slices(input_img_sparseTensor)
dataset_label = tensorflow.data.Dataset.from_tensor_slices(l_label)

batch_size = 100

dataset = tensorflow.data.Dataset.zip((dataset_img, dataset_label)).batch(batch_size)

print(""dataset.element_spec:"", dataset.element_spec)
print(""=====> Created dataset..."")


# Dummy CNN model
model = models.Sequential()

##model.add(layers.InputLayer(input_shape = img_shape, sparse = True, batch_size = batch_size))
model.add(layers.Conv2D(10, kernel_size = (10, 10), activation = ""relu"", input_shape = img_shape))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(5, kernel_size = (5, 5), activation = ""relu""))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(50, activation = ""relu""))
model.add(layers.Dense(2, activation = ""relu""))

model.summary()

print(""=====> Compiling model..."")

model.compile(
    optimizer = ""adam"",
    loss = tensorflow.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics = [""accuracy""],
)

print(""=====> Compiled model..."")

print(""=====> Starting fit..."")

# Use the same data for train and test, just to check if it runs
history = model.fit(
    x = dataset,
    epochs = 5,
    #batch_size = batch_size,
    validation_data = dataset,
    shuffle = False,
)
```

[2]
```
Traceback (most recent call last):
  File ""test_tensorflowSparseTensor.py"", line 100, in <module>
    history = model.fit(
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py"", line 1183, in fit
    tmp_logs = self.train_function(iterator)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 889, in __call__
    result = self._call(*args, **kwds)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 933, in _call
    self._initialize(args, kwds, add_initializers_to=initializers)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 763, in _initialize
    self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3050, in _get_concrete_function_internal_garbage_collected
    graph_function, _ = self._maybe_define_function(args, kwargs)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3444, in _maybe_define_function
    graph_function = self._create_graph_function(args, kwargs)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/function.py"", line 3279, in _create_graph_function
    func_graph_module.func_graph_from_py_func(
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 999, in func_graph_from_py_func
    func_outputs = python_func(*func_args, **func_kwargs)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py"", line 672, in wrapped_fn
    out = weak_wrapped_fn().__wrapped__(*args, **kwds)
  File ""/nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py"", line 986, in wrapper
    raise e.ag_error_metadata.to_exception(e)
TypeError: in user code:

    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:855 train_function  *
        return step_function(self, iterator)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:845 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica
        return fn(*args, **kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:838 run_step  **
        outputs = model.train_step(data)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:795 train_step
        y_pred = self(x, training=True)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:380 call
        return super(Sequential, self).call(inputs, training=training, mask=mask)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:420 call
        return self._run_internal_graph(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py:556 _run_internal_graph
        outputs = node.layer(*args, **kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1030 __call__
        outputs = call_fn(inputs, *args, **kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/keras/layers/convolutional.py:249 call
        outputs = self._convolution_op(inputs, self.kernel)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206 wrapper
        return target(*args, **kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:1012 convolution_v2
        return convolution_internal(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:1142 convolution_internal
        return op(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/nn_ops.py:2596 _conv2d_expanded_batch
        return gen_nn_ops.conv2d(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/ops/gen_nn_ops.py:969 conv2d
        _, _, _op, _outputs = _op_def_library._apply_op_helper(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:525 _apply_op_helper
        raise err
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py:517 _apply_op_helper
        values = ops.convert_to_tensor(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:163 wrapped
        return func(*args, **kwargs)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor
        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function
        return constant(v, dtype=dtype, name=name)
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:264 constant
        return _constant_impl(value, dtype, shape, name, verify_shape=False,
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:281 _constant_impl
        tensor_util.make_tensor_proto(
    /nfs/dust/cms/user/sobhatta/opt/anaconda3p8/envs/conda_env_python3p8/lib/python3.8/site-packages/tensorflow/python/framework/tensor_util.py:551 make_tensor_proto
        raise TypeError(""Failed to convert object of type %s to Tensor. ""

    TypeError: Failed to convert object of type <class 'tensorflow.python.framework.sparse_tensor.SparseTensor'> to Tensor. Contents: SparseTensor(indices=Tensor(""DeserializeSparse:0"", shape=(None, 4), dtype=int64), values=Tensor(""DeserializeSparse:1"", shape=(None,), dtype=float32), dense_shape=Tensor(""stack:0"", shape=(4,), dtype=int64)). Consider casting elements to a supported type.
```"
50076,how to get the Tensor's numpy in process ?,"hi,dear
if I just want to get the tensor's values in process?
chould you please help me ?
codes down
```
mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

strategy = tf.distribute.MirroredStrategy()
print ('Number of devices: {}'.format(strategy.num_replicas_in_sync))
BUFFER_SIZE = len(x_train)
BATCH_SIZE_PER_REPLICA = 64
GLOBAL_BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync

# Add a channels dimension
x_train = x_train[..., tf.newaxis].astype(""float32"")
x_test = x_test[..., tf.newaxis].astype(""float32"")
train_ds = tf.data.Dataset.from_tensor_slices(
    (x_train, y_train)).shuffle(10000).batch(GLOBAL_BATCH_SIZE)
train_dist_dataset = strategy.experimental_distribute_dataset(train_ds)

class MyModel(tf.keras.Model):
    def __init__(self):
        super(MyModel, self).__init__()
        self.conv1 = tf.keras.layers.Conv2D(32, 3, activation='relu')
        self.flatten = tf.keras.layers.Flatten()
        self.d1 = tf.keras.layers.Dense(128, activation='relu')
        self.d2 = tf.keras.layers.Dense(10)
    #
    def call(self, x):
        x = self.conv1(x)
        x0 = self.flatten(x)
        x = self.d1(x0)
        return self.d2(x)
```
how to get the x0's values ?
thx
"
50075,"Using GradientTape with gather, with axis parameter > 1, can cause InvalidArgumentError and wrong gradients.","**System information**
- Have I written custom code: yes (I might be misunderstanding this question)
- OS Platform and Distribution: Linux Ubuntu 20.04.2
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.3 ('default', 'Jul  2 2020 16:21:59')
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: N/A
- GPU model and memory: Not using GPU at the moment.

**Describe the current behavior**
Using `gather` in the code block below causes an `InvalidArgumentError`:
`InvalidArgumentError: segment_ids[1] = 20 is out of range [0, 12) [Op:UnsortedSegmentSum]`
Code example ([colab notebook](https://colab.research.google.com/drive/1mU1l_k9yx8cu06NMh3hTkw0Ehg4QwI0r?usp=sharing)):
```
import tensorflow as tf
A = tf.Variable(tf.random.normal([4, 20, 3]))
a = tf.random.uniform([4],0,3,dtype=tf.int32)
with tf.GradientTape() as tape:
    x = tf.gather(A,a,batch_dims=1,axis=2)
    gradients = tape.gradient(tf.reduce_mean(x), [A])
print(gradients,A,a,x)
```
This doesn't occur if `A` is smaller, so for example, if `A = tf.Variable(tf.random.normal([4, 2, 3]))` then the gather operation seems correct, although the gradients are wrong still.

Also I've just discovered that if I transpose A so `gather`'s axis parameter is `axis=1`, it all works. I'll use this as a work-around for now:
```
import tensorflow as tf
A = tf.Variable(tf.random.normal([4, 3, 200]))
a = tf.random.uniform([4],0,3,dtype=tf.int32)
with tf.GradientTape() as tape:
    x = tf.gather(A,a,batch_dims=1,axis=1)
    gradients = tape.gradient(tf.reduce_mean(x), [A])
print(gradients,A,a,x)
```

**Describe the expected behavior**

For this example `a = tf.Tensor([0 0 1 0], shape=(4,), dtype=int32)`.

So for the simple example with `A = tf.Variable(tf.random.normal([4, 2, 3]))` the `gather` output is correct, although the gradients I would *expect* to be:
```
[<tf.Tensor: shape=(4, 2, 3), dtype=float32, numpy=
array([[[0.125, 0.   , 0],
        [0.125, 0.   , 0]],

       [[0.125   , 0.   , 0],
        [0. 125  , 0.   , 0]],

       [[0.  , 0.125,  0.   ],
        [0.  , 0.125, 0.   ]],

       [[0.125   , 0.   , 0.   ],
        [0.125   , 0.   , 0.   ]]], dtype=float32)>] 
```
but they are:
```
[<tf.Tensor: shape=(4, 2, 3), dtype=float32, numpy=
array([[[0.125, 0.   , 0.125],
        [0.125, 0.   , 0.125]],

       [[0.   , 0.   , 0.125],
        [0.   , 0.   , 0.125]],

       [[0.125, 0.   , 0.   ],
        [0.125, 0.   , 0.   ]],

       [[0.   , 0.   , 0.   ],
        [0.   , 0.   , 0.   ]]], dtype=float32)>] 
```
In this example `A` is:
```
<tf.Variable 'Variable:0' shape=(4, 2, 3) dtype=float32, numpy=
array([[[ 1.34239629e-01, -9.54777598e-01, -9.89130437e-01],
        [-2.85939670e+00, -6.14062808e-02,  1.24629235e+00]],

       [[ 2.51984522e-02, -2.28253782e-01, -1.09111404e+00],
        [-5.17144203e-01, -8.65574062e-01,  8.04844439e-01]],

       [[-1.41558272e-03,  2.71541625e-01, -1.02697104e-01],
        [ 4.04205054e-01, -5.29084206e-01,  2.75117427e-01]],

       [[-5.37024558e-01,  7.28068471e-01,  1.82375908e+00],
        [-5.38604558e-01,  1.40457046e+00, -1.63010335e+00]]],
      dtype=float32)> 
```
So `x` (the output of gather) is correct:
```
tf.Tensor(
[[ 0.13423963 -2.8593967 ]
 [ 0.02519845 -0.5171442 ]
 [ 0.27154163 -0.5290842 ]
 [-0.53702456 -0.53860456]], shape=(4, 2), dtype=float32)
```

So in this case it has picked the 0 column from the top 2x3 sub-tensor, the 0 column from the next one, the 1 column from the next and the 0 column from the last.

I want this to work for bigger tensors, i.e. if `A = tf.Variable(tf.random.normal([4, 20, 3]))`, for example. And for the gradients to be correct.

It's quite possible I've just massively misunderstood what the 'axis' parameter means in `gather`. I'm very sorry if that's the case!

**[Contributing](https://www.tensorflow.org/community/contribute)**
No

**Standalone code to reproduce the issue**
[colab notebook](https://colab.research.google.com/drive/1mU1l_k9yx8cu06NMh3hTkw0Ehg4QwI0r?usp=sharing).

**Other info / logs**

Here's the trace:

```---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-49-2079e0db2264> in <module>
      4 with tf.GradientTape() as tape:
      5     x = tf.gather(A,a,batch_dims=1,axis=2)
----> 6     gradients = tape.gradient(tf.reduce_mean(x), [A])
      7 print(gradients,A,a,x)

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients)
   1072                           for x in nest.flatten(output_gradients)]
   1073 
-> 1074     flat_grad = imperative_grad.imperative_grad(
   1075         self._tape,
   1076         flat_targets,

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py in imperative_grad(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)
     69         ""Unknown value for unconnected_gradients: %r"" % unconnected_gradients)
     70 
---> 71   return pywrap_tfe.TFE_Py_TapeGradient(
     72       tape._tape,  # pylint: disable=protected-access
     73       target,

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py in _gradient_function(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)
    157       gradient_name_scope += forward_pass_name_scope + ""/""
    158     with ops.name_scope(gradient_name_scope):
--> 159       return grad_fn(mock_op, *out_grads)
    160   else:
    161     return grad_fn(mock_op, *out_grads)

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py in _GatherV2Grad(op, grad)
    694     values_transpose = array_ops.transpose(values, transpose_dims)
    695 
--> 696     params_grad = _BatchGatherGrad(params_shape, values_transpose, indices,
    697                                    batch_dims, params_shape[axis])
    698 

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_grad.py in _BatchGatherGrad(params_shape, values, indices, batch_dims, gather_dim_size)
    625 
    626   indices = array_ops.reshape(indices, indices_size)
--> 627   params_grad = math_ops.unsorted_segment_sum(values, indices, gather_dim_size)
    628 
    629   if batch_dims:

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py in unsorted_segment_sum(data, segment_ids, num_segments, name)
  11431       return _result
  11432     except _core._NotOkStatusException as e:
> 11433       _ops.raise_from_not_ok_status(e, name)
  11434     except _core._FallbackException:
  11435       pass

~/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name)
   6895   message = e.message + ("" name: "" + name if name is not None else """")
   6896   # pylint: disable=protected-access
-> 6897   six.raise_from(core._status_to_exception(e.code, message), None)
   6898   # pylint: enable=protected-access
   6899 

~/anaconda3/lib/python3.8/site-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: segment_ids[1] = 20 is out of range [0, 12) [Op:UnsortedSegmentSum]```
"
50069,"Error while quantizing a tensorflow bert model: ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT32 for input 0, name: input_ids ","### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installation pip package
- TensorFlow library 2.4

I created a Bert Model for text Classification and then trying to convert to fully integer quantization where I am developing the representative_data_set as following. but on converter.convert, it gives me the following error.

### 2. Code
num_calibration_steps = 100

def representative_dataset_gen():
    for i in range(num_calibration_steps):
        value = np.expand_dims(input_text_ids[i], axis=0).astype('float32')
        yield [value]


and converts the model as 

converter = tf.lite.TFLiteConverter.from_keras_model(themodel)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_dataset_gen
converter.allow_custom_ops = True
print('Converting')

tflite_model = converter.convert()

------------------------------------------------------------------------------------




### 2. Error : 
ValueError: Cannot set tensor: Got value of type FLOAT32 but expected type INT32 for input 0, name: input_ids 
"
50067,TF_StringEncode & TF_StringEncodedSize not found,"![Screenshot_2](https://user-images.githubusercontent.com/68463485/120733533-97649780-c511-11eb-95e9-759a51232b57.png)

i need a source of TF_StringEncode & TF_StringEncodedSize."
50054,Adding EfficientNetV2 to tf.keras.applications ,"I've noticed that the source code of [EfficientNetV2](https://github.com/google/automl/tree/master/efficientnetv2) has been released in `tf. keras` already. The subclassed API is used to build the model by customizing the `.fit` method. It seems interesting and unlike previous `keras` models until now where functional API is used.

So, I was just wondering will it be part of the `tf.keras.applications` any near soon? or it won't. "
50048,Incorrect batch size info with keras.utils.Sequence,"I had reported this issue before, but I now find the source is in the `train_generator_v1.py`. 

TensorFlow 2.4/2.5

Batch size info is miscalculated when data is wrapped in Sequence. The example below is the output for the first epoch training on 1,000 points with batch_size=32, which should result in 32 mini-batch updates. The batch number on the far left is correct (32/32), however, the size and batch before loss seem to be incorrect.

Output:
Epoch 1/100
Total samples: 1000 
Batch size: 32 
Total batches: 32 

Epoch 1/10
32/32 [==============================] - 1s 2ms/step - **batch: 15.5000 - size: 31.2500** - loss: 0.1369
Epoch 2/10
32/32 [==============================] - 0s 1ms/step - **batch: 15.5000 - size: 31.2500** - loss: 0.0307
Epoch 3/10
32/32 [==============================] - 0s 1ms/step - **batch: 15.5000 - size: 31.2500** - loss: 0.0148

```python
import tensorflow as tf
import numpy as np 
print(tf.__version__)
from tensorflow import keras as k
tf.compat.v1.disable_eager_execution()

class Data(k.utils.Sequence):
    """"""
    Converts fit() into fit_generator() interface.
    """"""

    def __init__(self, inputs, outputs, sample_weights, batch_size, shuffle):
        self._inputs = inputs
        self._outputs = outputs
        self._sample_weights = sample_weights
        self._size = inputs[0].shape[0]
        self._batch_size = batch_size
        self._num_batches = int((self._size-1)/batch_size) + 1
        self._shuffle = shuffle
        self._ids = np.arange(0, self._size)
        self._reshuffle()
        print(""\nTotal samples: {} "".format(self._size))
        print(""Batch size: {} "".format(min(self._batch_size, self._size)))
        print(""Total batches: {} \n"".format(self._num_batches))

    def __len__(self):
        return self._num_batches

    def __getitem__(self, index):
        start = index * self._batch_size
        end = min(start + self._batch_size, self._size)
        ids = self._ids[start: end]
        inputs = [v[ids, :] for v in self._inputs]
        outputs = [v[ids, :] for v in self._outputs]
        sample_weights = [v[ids] for v in self._sample_weights]
        return inputs, outputs, sample_weights
    
    def on_epoch_end(self):
        self._reshuffle()

    def get_data(self):
        return self._inputs, self._outputs, self._sample_weights

    def _reshuffle(self):
        if self._num_batches > 1 and self._shuffle:
            self._ids = np.random.choice(self._size, self._size, replace=False)


x = k.Input((1,))
l1 = k.layers.Dense(10, activation='tanh')(x)
y = k.layers.Dense(1)(l1)

model = k.Model(x, y)
model.compile(loss=k.losses.MSE)

inputs = [np.linspace(0, 1, 1000).reshape(-1,1)]
outputs = list(map(lambda x: np.sin(2*x), inputs))
weights = list(map(lambda x: np.ones(x.size), inputs))

dg = Data(inputs, outputs, weights, 32, True)

model.fit(dg, epochs=10)
````"
50045,Python: Tensorflow 2.5 requires grpcio 1.34,"Tensorflow 2.5.0 requires grpcio 1.34, which prevents installation with some other packages that require 1.37 and later.

Related: #48109, https://github.com/tensorflow/tensorflow/commit/f1a51f07937e28e3194b3f8964a06f8390e798c3#diff-f526feeafa1000c4773410bdc5417c4022cb2c7b686ae658b629beb541ae9112

It seems like the above commit should've landed for 2.5.0, but it appears to not be that way in pypi.

Log output from Poetry:

```
(snip)
And because tensorflow (2.5.0) depends on grpcio (>=1.34.0,<1.35.0),
(snip)
```"
50042,Tensorflow 2.5 wheels depend on an unreleased keras version and restrict numpy to 1.19,"The published wheels for tensorflow 2.5 depend on a nightly dev version of keras and restrict numpy to 1.19.x.

Depending on a nightly build of some package in a stable version is just, in lack of a better word, crazy. 

The dependency on a specific numpy major version is maybe necessary, but is it really impossible to have a wheel that supports numpy >1.19 (including the current 1.20 release)?


```
❯ unzip -p tensorflow-2.5.0-cp37-cp37m-macosx_10_11_x86_64.whl  tensorflow-2.5.0.dist-info/METADATA | grep Requires-Dist
Requires-Dist: numpy (~=1.19.2)
Requires-Dist: absl-py (~=0.10)
Requires-Dist: astunparse (~=1.6.3)
Requires-Dist: flatbuffers (~=1.12.0)
Requires-Dist: google-pasta (~=0.2)
Requires-Dist: h5py (~=3.1.0)
Requires-Dist: keras-preprocessing (~=1.1.2)
Requires-Dist: opt-einsum (~=3.3.0)
Requires-Dist: protobuf (>=3.9.2)
Requires-Dist: six (~=1.15.0)
Requires-Dist: termcolor (~=1.1.0)
Requires-Dist: typing-extensions (~=3.7.4)
Requires-Dist: wheel (~=0.35)
Requires-Dist: wrapt (~=1.12.1)
Requires-Dist: gast (==0.4.0)
Requires-Dist: tensorboard (~=2.5)
Requires-Dist: tensorflow-estimator (<2.6.0,>=2.5.0rc0)
Requires-Dist: keras-nightly (~=2.5.0.dev)
Requires-Dist: grpcio (~=1.34.0)
```

Most version requirements here seem rather restrictive."
50040,Different outputs for same input & model in Colab vs Laptop,"
<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab and Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): pip install tensorflow==2.5.0
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8
- Bazel version (if compiling from source): na
- GCC/Compiler version (if compiling from source): na
- CUDA/cuDNN version:na
- GPU model and memory:na

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I am getting different output for the same model.
The model is here: https://drive.google.com/file/d/1yUjCG5rMeVCKTSPbST2F9BrRRlkDPzEA/view
My test script is:

```
import tensorflow as tf
import PIL
import numpy as np

m = tf.keras.models.load_model('./model.h5')
ar = np.expand_dims(np.asarray(PIL.Image.open('./test.jpeg')), axis=0)
ar1 = (ar-127.5) / 127.5
e = m.predict(ar1)
```
**here e is different on my laptop/mobile(tflite) vs my server/Colab. i.e. output in my laptop matches tflite output on my mobile but is different from Colab/My Server.**

**Describe the expected behavior**
embeddings should be exactly same up to 4-5 decimal points!

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.



![sample](https://user-images.githubusercontent.com/29163297/120754851-d4895380-c52a-11eb-9e3b-816db34af347.jpeg)"
50029,Build tensorflow-lite for armv7-a (embedded linux) failed,"
**System information**
- Host platform - Ubuntu 20.04.1 LTS:
- target platform - embedded linux - OS Openwrt, CPU - IMX6ULL;
- TensorFlow installed from source:
- TensorFlow version: 2.6.0
- Cross compilation using C++ and CMake. CMake version - 3.16.0



**Describe the problem**

Trying to cross-compile Tensorflow-lite minimal c++ example using CMake.

```
-DCMAKE_SYSTEM_PROCESSOR=armv7-a
```

Checked my target hardware as explained here: 
https://www.tensorflow.org/lite/guide/build_cmake_arm#check_your_target_environment
Here my target device /proc/cpuinfo output:
```
root@OpenWrt:/# cat /proc/cpuinfo                                                                                                                                                        
processor       : 0                                                                                                                                                                      
model name      : ARMv7 Processor rev 5 (v7l)                                                                                                                                            
BogoMIPS        : 109.09                                                                                                                                                                 
Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpv4 idiva idivt vfpd32 lpae                                                                                              
CPU implementer : 0x41                                                                                                                                                                   
CPU architecture: 7                                                                                                                                                                      
CPU variant     : 0x0                                                                                                                                                                    
CPU part        : 0xc07                                                                                                                                                                  
CPU revision    : 5                                                                                                                                                                      
                                                                                                                                                                                         
Hardware        : Freescale i.MX6 Ultralite (Device Tree)                                                                                                                                
Revision        : 0000                                                                                                                                                                   
Serial          : 0000000000000000         
```                                                                                                                                              

Build fails with following error:

```
[ 85%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/math/roundz-neonv8.c.o
[ 85%] Building C object _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-neondot.c.o
arm-openwrt-linux-muslgnueabi-gcc: error: unrecognized argument in option '-march=armv8.2-a+dotprod'
arm-openwrt-linux-muslgnueabi-gcc: note: valid arguments to '-march=' are: armv2 armv2a armv3 armv3m armv4 armv4t armv5 armv5e armv5t armv5te armv5tej armv6 armv6-m armv6j armv6k armv6kz armv6s-m armv6t2 armv6z armv6zk armv7 armv7-a armv7-m armv7-r armv7e-m armv7ve armv8-a armv8-a+crc armv8-m.base armv8-m.main armv8-m.main+dsp armv8.1-a armv8.2-a armv8.2-a+fp16 iwmmxt iwmmxt2 native; did you mean 'armv8.2-a+fp16'?
make[5]: *** [_deps/xnnpack-build/CMakeFiles/XNNPACK.dir/build.make:14766: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/src/qs8-gemm/gen/1x8c4-minmax-neondot.c.o] Error 1
make[5]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/tensorflow-sdk-build'
make[4]: *** [CMakeFiles/Makefile2:4062: _deps/xnnpack-build/CMakeFiles/XNNPACK.dir/all] Error 2
make[4]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/tensorflow-sdk-build'
make[3]: *** [Makefile:152: all] Error 2
make[3]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/tensorflow-sdk-build'
make[2]: *** [Makefile:67: /home/al/imx6ull-openwrt2/imx6ull-openwrt/build_dir/target-arm_cortex-a7+neon-vfpv4_musl_eabi/tensorflow/.built] Error 2
make[2]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt/package/tensorflow'
time: package/tensorflow/compile#211.83#30.92#311.67
make[1]: *** [package/Makefile:113: package/tensorflow/compile] Error 2
make[1]: Leaving directory '/home/al/imx6ull-openwrt2/imx6ull-openwrt'
make: *** [/home/al/imx6ull-openwrt2/imx6ull-openwrt/include/toplevel.mk:227: package/tensorflow/compile] Error 2
```

seems that error produced by following lines in `xnnpack/CMakeLists.txt` file:

```
IF(CMAKE_SYSTEM_PROCESSOR MATCHES ""^armv[5-8]"" OR IOS_ARCH MATCHES ""^armv7"")
  SET_PROPERTY(SOURCE ${XNNPACK_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -marm "")
  SET_PROPERTY(SOURCE ${XNNPACK_NEON_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -march=armv7-a -mfpu=neon "")
  SET_PROPERTY(SOURCE ${XNNPACK_NEONFMA_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -march=armv7-a -mfpu=neon-vfpv4 "")
  IF(IOS)
    SET_PROPERTY(SOURCE ${XNNPACK_NEONV8_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -mcpu=cyclone -mtune=generic "")
    SET_PROPERTY(SOURCE ${XNNPACK_AARCH32_ASM_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -arch ${IOS_ARCH} "")
  ELSE()
    SET_PROPERTY(SOURCE ${XNNPACK_NEONV8_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -march=armv8-a -mfpu=neon-fp-armv8 "")
    SET_PROPERTY(SOURCE ${XNNPACK_NEONDOT_MICROKERNEL_SRCS} APPEND_STRING PROPERTY COMPILE_FLAGS "" -march=armv8.2-a+dotprod -mfpu=neon-fp-armv8 "")
  ENDIF()
```
Tried to edit `xnnpack/CMakeLists.txt` manually by replacing `armv8.2-a+dotprod` to `armv7-a` and `neon-fp-armv8` to `neon-vfpv4` without luck. New errors happens like `incompatible types when assigning to type 'int32x4_t' from type 'int'`.

Could you please point me to some solution of that issue?
As I understand target hardware CPU supports tensorflow lite."
50028,tf.contrib.framework.assign_from_variables in Tensorflow 2,"I'm running an inference model that used tf1, and I've changed the code to tf2 but I can't find a proper tf2 alternative to contrib.framework.assign_from_variables anywhere. I've tried tf.compat.v1 but still the same issue. 
"
50027,Tensorflow 1.14.0 C++ can't get GPU device,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux centos 7.7
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary): source
- TensorFlow version: 1.14
- Python version:  3.6
- Installed using virtualenv? pip? conda?: None
- Bazel version (if compiling from source): 0.25.2
- GCC/Compiler version (if compiling from source): 4.85
- CUDA/cuDNN version: CUDA 10.0, cudnn 7.4
- GPU model and memory: Tesla T4

**Describe the problem**
 - run service can't get GPU device by use libtensorflow_cc.so and libtensorflow_framework.so.
    bazel build tensorflow_cc
- python3.6:
    - print(device_lib.list_local_devices())
        - [name: ""/device:CPU:0""
device_type: ""CPU""
memory_limit: 268435456
locality {
}
incarnation: 7969440054434969841
, name: ""/device:XLA_GPU:0""
device_type: ""XLA_GPU""
memory_limit: 17179869184
locality {
}
incarnation: 13181855577668801818
physical_device_desc: ""device: XLA_GPU device""
, name: ""/device:XLA_CPU:0""
device_type: ""XLA_CPU""
memory_limit: 17179869184
locality {
}
incarnation: 13243497781981531688
physical_device_desc: ""device: XLA_CPU device""
, name: ""/device:GPU:0""
device_type: ""GPU""
memory_limit: 14892338381
locality {
  bus_id: 1
  links {
  }
}
incarnation: 8775195624210653350
physical_device_desc: ""device: 0, name: Tesla T4, pci bus id: 0000:00:07.0, compute capability: 7.5""
]

- C++:
     - std::vector<tensorflow::DeviceAttributes> resp;
     - TF_CHECK_OK(session_->ListDevices(&resp));
     - for (const auto& dev : resp)   {std::cout<< ""dev type = ""<<dev.device_type()<<std::endl;}
         - dev type = CPU
dev type = XLA_CPU

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
None
"
50026,Issue converting full scale BERT Model to TFLite Model.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): ***Ubuntu 18.04.5 LTS*** (Colab)
- TensorFlow installation (pip package or built from source): ***pip package*** (Colab)
- TensorFlow library (version, if pip package or github SHA, if built from source): ***Tensorflow 2.5.0*** (Colab)

### 2. Context
I was following tutorial on Classification of text with BERT from [Classify text with BERT](https://www.tensorflow.org/text/tutorials/classify_text_with_bert) notebook I am using the same code from that notebook without any changes.

To convert  the model to TFLite I am following the documentation from [TensorFlow Lite converter](https://www.tensorflow.org/lite/convert) on converting a model to TFLite model with the recommended method i.e. ```from_saved_model()```.

I have also tried with other method ```from_keras_model()``` but this also shows error.

P.S. I am aware that lite version of BERT is also available ([ALBERT](https://tfhub.dev/tensorflow/albert_en_base/3), [TF Lite Model Maker](https://www.tensorflow.org/lite/tutorials/model_maker_text_classification) ) but I want to know why TFLiteConverter is not able to convert the full scale TF model.
I have also tried with tf-nightly too and the issue still persists.
I want to know can this issue be solved or is there any workaround right now or I should wait for future support from Tensorflow regarding this.

### 3. Code

### Please find the Google Colab Notebook link: [Colab Notebook](https://colab.research.google.com/drive/1q7ShcS0sk7-ZMH2Nwc3n5khaOzCExysk)

## Code Block where error occurs ```from_saved_model``` [Colab Notebook block](https://colab.research.google.com/drive/1q7ShcS0sk7-ZMH2Nwc3n5khaOzCExysk#scrollTo=-UPELrKGRdoj):
```
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
tflite_model = converter.convert()
lite_model_name = ""BERTLite.tflite""
with open(f'{lite_model_name}', 'wb') as f:
	f.write(tflite_model)
```

## Code Block where error occurs ```from_keras_model``` [Colab Notebook block](https://colab.research.google.com/drive/1q7ShcS0sk7-ZMH2Nwc3n5khaOzCExysk#scrollTo=lXzszo1EiGkl):
```
converter = tf.lite.TFLiteConverter.from_keras_model(classifier_model)
tflite_model = converter.convert()
```

### 4. Error Traceback:

### When trying with ```from_saved_model()```.
```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    293                                                  debug_info_str,
--> 294                                                  enable_mlir_converter)
    295       return model_str

/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/wrap_toco.py in wrapped_toco_convert(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
     37       debug_info_str,
---> 38       enable_mlir_converter)
     39 

Exception: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_63122"") at ""StatefulPartitionedCall@__inference_restored_function_body_151700"") at ""model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562"") at ""StatefulPartitionedCall@__inference_signature_wrapper_158612"") at ""StatefulPartitionedCall_2"")): 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall_2""): called from
<unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_63122"") at ""StatefulPartitionedCall@__inference_restored_function_body_151700"") at ""model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562"") at ""StatefulPartitionedCall@__inference_signature_wrapper_158612"") at ""StatefulPartitionedCall_2"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall_2""): called from


During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
<ipython-input-25-8d1ce648fcd0> in <module>()
      1 converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)
----> 2 tflite_model = converter.convert()
      3 lite_model_name = ""BERTLite.tflite""
      4 with open(f'{lite_model_name}', 'wb') as f:
      5         f.write(tflite_model)

/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self)
    911     converter_kwargs.update(quant_mode.converter_flags())
    912 
--> 913     result = _convert_saved_model(**converter_kwargs)
    914     if self.experimental_new_quantizer:
    915       calibrate_and_quantize, flags = quant_mode.quantizer_flags(

/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in convert_saved_model(saved_model_dir, saved_model_version, saved_model_tags, saved_model_exported_names, **kwargs)
    725       None,  # input_data, unused
    726       None,  # debug_info_str, unused
--> 727       enable_mlir_converter=True)
    728   return data
    729 

/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    295       return model_str
    296     except Exception as e:
--> 297       raise ConverterError(str(e))
    298 
    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_63122"") at ""StatefulPartitionedCall@__inference_restored_function_body_151700"") at ""model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562"") at ""StatefulPartitionedCall@__inference_signature_wrapper_158612"") at ""StatefulPartitionedCall_2"")): 'tf.TensorListReserve' op requires element_dtype to be 1-bit/8-bit/16-bit/32-bit/64-bit integer or 16-bit/32-bit/64-bit float type during TF Lite transformation pass
<unknown>:0: note: loc(""StatefulPartitionedCall_2""): called from
<unknown>:0: error: loc(callsite(callsite(callsite(callsite(callsite(callsite(""map/TensorArrayV2_2@__inference_bert_pack_inputs_layer_call_and_return_conditional_losses_63086"" at ""bert_pack_inputs/PartitionedCall@__inference_model_layer_call_and_return_conditional_losses_63100"") at ""StatefulPartitionedCall@__inference_model_layer_call_fn_63122"") at ""StatefulPartitionedCall@__inference_restored_function_body_151700"") at ""model/preprocessing/StatefulPartitionedCall@__inference__wrapped_model_152562"") at ""StatefulPartitionedCall@__inference_signature_wrapper_158612"") at ""StatefulPartitionedCall_2"")): failed to legalize operation 'tf.TensorListReserve' that was explicitly marked illegal
<unknown>:0: note: loc(""StatefulPartitionedCall_2""): called from
```

### When trying with ```from_keras_model()```.

```
WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 910). These functions will not be directly callable after loading.
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-26-d96cda79a162> in <module>()
      1 converter = tf.lite.TFLiteConverter.from_keras_model(classifier_model)
----> 2 tflite_model = converter.convert()


/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/lite.py in convert(self)
   1034     frozen_func, graph_def = (
   1035         _convert_to_constants.convert_variables_to_constants_v2_as_graph(
-> 1036             self._funcs[0], lower_control_flow=False))
   1037 
   1038     input_tensors = [

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/convert_to_constants.py in convert_variables_to_constants_v2_as_graph(func, lower_control_flow, aggressive_inlining)
   1110       func=func,
   1111       lower_control_flow=lower_control_flow,
-> 1112       aggressive_inlining=aggressive_inlining)
   1113 
   1114   output_graph_def, converted_input_indices = _replace_variables_by_constants(

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/convert_to_constants.py in __init__(self, func, lower_control_flow, aggressive_inlining, variable_names_allowlist, variable_names_denylist)
    805         variable_names_allowlist=variable_names_allowlist,
    806         variable_names_denylist=variable_names_denylist)
--> 807     self._build_tensor_data()
    808 
    809   def _build_tensor_data(self):

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/convert_to_constants.py in _build_tensor_data(self)
    824         data = map_index_to_variable[idx].numpy()
    825       else:
--> 826         data = np.array(val_tensor.numpy())
    827       self._tensor_data[tensor_name] = _TensorData(
    828           numpy=data,

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in numpy(self)
   1092     """"""
   1093     # TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.
-> 1094     maybe_arr = self._numpy()  # pylint: disable=protected-access
   1095     return maybe_arr.copy() if isinstance(maybe_arr, np.ndarray) else maybe_arr
   1096 

/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py in _numpy(self)
   1060       return self._numpy_internal()
   1061     except core._NotOkStatusException as e:  # pylint: disable=protected-access
-> 1062       six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access
   1063 
   1064   @property

/usr/local/lib/python3.7/dist-packages/six.py in raise_from(value, from_value)

InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.
```


Any Suggestion? Is there something I am doing wrong?
Thanks."
50025,Comparison of conversion and int8 conversion for TFLite ,"Hi, I'm working on converting trained tensorflow model to uint8 and int8. But I found that the results between the two models are different, the followings are settings of conversion:

### 1. System information

- OS Platform and Distribution: Ubuntu 16.04
- TensorFlow installation (pip package or built from source): pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 1.15.0

### 2. Code
[int8 conversion]
```
    converter.optimizations = [tf.lite.Optimize.DEFAULT] 
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8 
    converter.inference_output_type = tf.int8 
    converter.representative_dataset = representative_data_gen
    tflite_int8_model = converter.convert()
```

[uint8 conversion]
```
    converter.target_ops = [tf.compat.v1.lite.OpsSet.TFLITE_BUILTINS, tf.compat.v1.lite.OpsSet.SELECT_TF_OPS]
    converter.inference_type = tf.uint8

    input_arrays = converter.get_input_arrays()
    converter.quantized_input_stats = {input_arrays[0]: (127, 127)}    
    converter.default_ranges_stats = (-128, 127) 
    tflite_uint8_model = converter.convert()
```


### 3. Failure after conversion
The results between the int8 and uint8 models are different
(int 8 model with representative dataset)
![int8_with_representative_dataset](https://user-images.githubusercontent.com/8951991/120624329-2bdbe500-c493-11eb-94a8-4f24ffccad42.png)

(uint8 model with input_stats and ranges_stats)
![uint8_with_range](https://user-images.githubusercontent.com/8951991/120624431-47df8680-c493-11eb-9cbe-cdef9949518e.png)

 ### 5. (optional) Any other info
I also tried to applying `converter.optimizations = [tf.lite.Optimize.DEFAULT], converter.representative_dataset ` and `converter.quantized_input_stats, converter.default_ranges_stats` simultaneously, but the generated model contains two quantize layers, which generates wrong results.
![wrong_conversion](https://user-images.githubusercontent.com/8951991/120625297-24690b80-c494-11eb-86b0-8b3bdc16fa97.png)

Is it possible to generate uint8 models with `converter.representative_dataset ` with all layers are quantized as uint8 instead of int8?

Best regards"
50024,Datosmoviles,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
50023,problem with tf.image.ssim_multiscale 's filter size,"when i use this function(with filter size=11) to compare two batches of images' (width:100,height:100)multiscale ssim score, i got error like 'Expected 'tf.Tensor(False, shape=(), dtype=bool)' to be true. Summarized data: 10, 7, 7, 3 11'. I think this is due to the wrong filter size choosing, since 100 > 11, i think there is something wrong when you implement this function 
"
50022,Convert tflite output location to screen location,"Hi everyone. I have an android app that use a detection model. the model is working fine. I just have problem with converting the output location to screen location that user can see. I used the tflite android example and extract some code out of it. but the conversion is not correct. here is my function to converting:

```
`private fun mapOutputCoordinates(
        location: RectF,
        rotationDegrees: Int,
        width: Int,
        height: Int
    ): RectF {
        val frameToCropTransform = getTransformationMatrix(
            width,
            height,
            ModelInputSize,
            ModelInputSize,
            rotationDegrees,
            MAINTAIN_ASPECT
        )

        val cropToFrameTransform = Matrix()
        frameToCropTransform.invert(cropToFrameTransform)

        cropToFrameTransform.mapRect(location)
        val rotated = rotationDegrees % 180 == 90

        val multiplier = min(
            camera.height / (if (rotated) width else height).toFloat(),
            camera.width / (if (rotated) height else width).toFloat()
        )
        val frameToCanvasMatrix = getTransformationMatrix(
            width,
            height,
            (multiplier * if (rotated) height else width).toInt(),
            (multiplier * if (rotated) width else height).toInt(),
            rotationDegrees,
            false
        )
        //frameToCanvasMatrix.mapRect(location)
        val rgbFrameToScreen = Matrix(frameToCanvasMatrix)

        val detectionFrameRect = RectF(location)

        val detectionScreenRect = RectF()
        rgbFrameToScreen.mapRect(detectionScreenRect, detectionFrameRect)
        return detectionScreenRect
    }
}`
```

I don't know if this place is the right place for asking this question but i'm straggling with this for over a weak. "
50021,U-Net is not converging in RTX3090,"<em>Please make sure that this is an issue related to performance of TensorFlow.
As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:performance_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS 7.9
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 1.14.0(both tensorflow and tensorflow-gpu)
- Python version: 2.7
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA-11.0, cudatoolkit-10.0, cudnn-7.6.5
- GPU model and memory: GTX3090(24G mem)

**Describe the current behavior**
packages(tensorflow, cudnn, cudatoolkit) installed using anaconda; 
Tyring to replicate result from a U-Net project(https://github.com/hongweilibran/wmh_ibbmTum) by running train_leave_one_out.py; 
The model cannot converge, loss not drop from the beginning. Same code worked as expected on Google colab and other remote runtime environment.

Other TF2.X and pytorch toy project works fine on this runtime.
Any input is appreaciated!
"
50019,How to merge results from distribute worker in tf.distribute.experimental.MultiWorkerMirroredStrategy,"This template is for miscellaneous issues not covered by the other issue categories.

For questions on how to work with TensorFlow, or support for problems that are not verified bugs in TensorFlow, please go to [StackOverflow](https://stackoverflow.com/questions/tagged/tensorflow).

If you are reporting a vulnerability, please use the [dedicated reporting process](https://github.com/tensorflow/tensorflow/blob/master/SECURITY.md).

For high-level discussions about TensorFlow, please post to discuss@tensorflow.org, for questions about the development or internal workings of TensorFlow, or if you would like to know how to contribute to TensorFlow, please post to developers@tensorflow.org.
"
50017,tf.linalg.eigh yields invalid eigensystem when used inside a decorated tf.function with jitcompile=True flag,"

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
Linux Ubuntu 20.04 host using docker image tensorflow/tensorflow:2.5.0-gpu
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
N/A
- TensorFlow installed from (source or binary):
docker image tag 2.5.0-gpu
- TensorFlow version (use command below):
v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0

- Python version:
3.6.9
- Bazel version (if compiling from source):
N/A
- GCC/Compiler version (if compiling from source):
N/A
- CUDA/cuDNN version:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudart.so.11.2.152
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudart_static.a

- GPU model and memory:
NVIDIA RTX 2070 8 GB

**Describe the current behavior**
Using the jitcompile=True flag for a decorated function that uses tf.linalg.eigh returns an incorrect solution. Removing the flag yields a correct solution. Note that this did work in TF 2.4.1 using the same code with experimentalcompile=True. It also fails for when CUDA_VISIBLE_DEVICES=-1 is to disable GPU evaluation.

**Describe the expected behavior**

The code snippet pasted below, which tests that the eigen system is a valid solution, should not raise an error. 

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

Possibly, if I can get some guidance on where the issue might be. As it stands I wouldn't know where to begin.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```
# fails both with and without this environment variable set to disable GPU evaluation
# import os
# os.environ[""CUDA_VISIBLE_DEVICES""] = ""-1""
import numpy as np
import tensorflow as tf


a_mat = np.array([
    [0., -1., -1.,  1.],
    [-1.,  0.,  1., -1.],
    [-1.,  1.,  0., -1.],
    [1., -1., -1.,  0.]], dtype=np.float32)


@tf.function(input_signature=[
    tf.TensorSpec(None, dtype=tf.float32)],
    jit_compile=False)
def eigh_uncompiled(arg):
    return tf.linalg.eigh(arg)


@tf.function(input_signature=[
    tf.TensorSpec(None, dtype=tf.float32)],
    jit_compile=True)
def eigh_compiled(arg):
    return tf.linalg.eigh(arg)


for eigh in [np.linalg.eigh, eigh_uncompiled, eigh_compiled]:
    val, vec = eigh(a_mat)
    # assert A.x = lambda x for eigen system
    print(tf.linalg.matmul(a_mat, vec) - val[tf.newaxis] * vec)
    if not np.allclose(tf.linalg.matmul(a_mat, vec), val[tf.newaxis] * vec, atol=1e-6):
        raise AssertionError(f'Test fails for function {eigh.__name__}')
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

N/A. Should be easy to reproduce with the above code snippet.
"
50016,TensorFlow Profiler stuck loading data,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colaboratory
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): N/A
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Tesla T4, 16 GB

**Describe the current behavior**
When trying to use the TensorFlow Profiler on a GPU-enabled Colab notebook, the TensorBoard page displays the message ""Loading data"" and appears to be stuck there.
<img width=""1008"" alt=""Screen Shot 2021-06-02 at 9 26 44 PM"" src=""https://user-images.githubusercontent.com/12498403/120582805-bc161d80-c3f2-11eb-9367-68ec7f9795c5.png"">

**Describe the expected behavior**
The TensorFlow Profiler should appear as a TensorBoard page.
<img width=""850"" alt=""Screen Shot 2021-06-02 at 10 38 13 PM"" src=""https://user-images.githubusercontent.com/12498403/120583129-4494be00-c3f3-11eb-9a70-a6720eb74de7.png"">

**Standalone code to reproduce the issue**
This issue can be reproduced by running the TensorFlow Profiler tutorial notebook.
https://colab.research.google.com/github/tensorflow/tensorboard/blob/master/docs/tensorboard_profiling_keras.ipynb
"
50014,ValueError: inputs must be an iterable of at least one Tensor/IndexedSlices with the same dtype and shape,"
```
>>> for epoch in range(3):
...         for inputs in train2.batch(1):
...             inputs=[tf.squeeze(k) for k in inputs]
...             with tf.GradientTape() as tape:
...                 loss=model.train_step(inputs)
```

codes similar as the [issue](https://github.com/tensorflow/tensorflow/issues/49907), but the bug is not

```
Traceback (most recent call last):
  File ""<stdin>"", line 5, in <module>
  File ""/data/logs/xulm1/debiasing_rush/tf2_dist_data2.py"", line 145, in train_step
    loss=self.loss_function(next_item,logits)
  File ""/data/logs/xulm1/debiasing_rush/tf2_dist_data2.py"", line 132, in loss_function
    self.loss_train = self.loss + tf.add_n([tf.nn.l2_loss(v) for v in tf.compat.v1.trainable_variables()]) * self.l2#最后一项影响较大
  File ""/data/logs/xulm1/myconda/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py"", line 201, in wrapper
    return target(*args, **kwargs)
  File ""/data/logs/xulm1/myconda/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py"", line 3497, in add_n
    raise ValueError(""inputs must be an iterable of at least one ""
ValueError: inputs must be an iterable of at least one Tensor/IndexedSlices with the same dtype and shape
```

could you please help me ?
thx
"
50005,support SyncBatchNormalization gradient in loaded saved_model,"**System information**
- TensorFlow version (you are using): 2.4.1
- Are you willing to contribute it (Yes/No):
Absolutely.  Just need some code pointers/guidance.

**Describe the feature and the current behavior/state.**
Loading the saved_model included with the [SimCLR](https://github.com/google-research/simclr/tree/master/tf2) repository using `tf.saved_model.load` yields the error message:
```
W0602 17:01:50.030123 3770209 function_deserialization.py:573] Importing a function (__inference_sync_batch_normalization_2_layer_call_
and_return_conditional_losses_29971) with ops with unsaved custom gradients. Will likely fail if a gradient is requested.
```

This is caused by tf.keras.layers.experimental.SyncBatchNormalization. 

**Will this change the current api? How?**
Won't

**Who will benefit with this feature?**
Anyone attempting to use tf.keras.layers.experimental.SyncBatchNormalization from a saved_model.

**Any Other info.**
I'm interested in contributing this as it would help me with some research I am performing.   I am attempting to implement some functions that measure the robustness of a given saved_model for [Neural Structured Learning](https://www.tensorflow.org/neural_structured_learning).  In order to do this, I'd like to perform Projected Gradient Descent on a given SavedModel, which requires the gradient to be included."
50004,ImportError: cannot import name 'LayerNormalization' ,"Description:
----------------
Cannot import tensorflow 2.5.0 or 2.4.0 due to this error (started only recently and abruptly)


Trace:
-----------
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py"", line 41, in <module>
    from tensorflow.python.tools import module_util as _module_util
  File ""/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/__init__.py"", line 48, in <module>
    from tensorflow.python import keras
  File ""/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/__init__.py"", line 27, in <module>
    from tensorflow.python.keras import models
  File ""/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/models.py"", line 27, in <module>
    from tensorflow.python.keras.engine import sequential
  File ""/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py"", line 28, in <module>
    from tensorflow.python.keras import layers as layer_module
  File ""/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/__init__.py"", line 177, in <module>
    from tensorflow.python.keras.layers.normalization import LayerNormalization
ImportError: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/Users/I538891/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization/__init__.py)


Environment:
--------------
Python Version: 3.7.8

attaching tf_env_collect.sh script results as well, the env results are for 2.4.0 but at first, it began in tf 2.5.0. (downgrading did not work too)

[old-tf_env.txt](https://github.com/tensorflow/tensorflow/files/6587992/old-tf_env.txt)
[tf_env.txt](https://github.com/tensorflow/tensorflow/files/6587994/tf_env.txt)


"
50002,Kernel dies on interpreter.invoke(),"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version 2.5


**So here is the attached code in which I am loading a tflite model and I want to perform inference on that, for 1 input it works fine but when I changed the input_tensor to 1000, it kills the kernel. Any solution for this? Thanks!**

```
from transformers import MobileBertModel, MobileBertConfig, MobileBertTokenizer, MobileBertForSequenceClassification, MobileBertTokenizerFast, TFMobileBertModel
import numpy as np
import tensorflow as tf
import pandas as pd
path = 'mobilequantmodel.tflite'

# Load the TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=path)
interpreter.allocate_tensors()

# model_name = 'bert-base-uncased'
model_name = ""google/mobilebert-uncased""
print(model_name)

# Max length of tokens
max_length = 100

config = MobileBertConfig.from_pretrained(model_name)
config.output_hidden_states = False

# Load BERT tokenizer
tokenizer = MobileBertTokenizerFast.from_pretrained(pretrained_model_name_or_path = model_name, config = config)
df = pd.read_csv('file_sentences.csv')

# Get input and output tensors.
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

input_shape = input_details[0]['shape']

ddf = df[(df['useful'] == 1) & (df['language']=='en')]

x_test = tokenizer(
  text= ddf['paragraph'].to_list(),
  add_special_tokens=True,
  max_length=max_length,
  truncation=True,
  return_tensors='tf',
  padding='max_length', 
  return_token_type_ids = False,
  return_attention_mask = False,
  verbose = True)

input_text = x_test['input_ids']

interpreter.resize_tensor_input(input_details[0]['index'], [1000, 100])
interpreter.allocate_tensors()

ipp = input_text[:1000]
interpreter.get_input_details()

interpreter.set_tensor(input_details[0]['index'], ipp)

interpreter.invoke()
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
"
49993,SparseTensor indices stored as tf.int32 instead of tf.int64,`tf.SparseTensor ` currently only accepts indices as `tf.int64`. Which mean memory of indices > values (`tf.float32`) in general. Would it make sense to have an option to store indices as `tf.int32`? This would be useful for storing large sparse matrices where the index values are < 2^32 as it would free up some RAM
49983,TF 2.5.0 with CUDA 10.2 (in container) Build error extract_volume_patches_op_gpu failed: crosstool_wrapper_driver_is_not_gcc,"**System information**
- OS Platform and Distribution: Linux Ubuntu 18.04 (base container, see below) 
- Docker base container: nvidia/cuda:10.2-cudnn8-devel-ubuntu18.04
- TensorFlow installed from:source
- TensorFlow version: 2.5.0
- Python version: 3.8
- Installed using: building for pip install
- Bazel version: 3.7.2
- GCC/Compiler version: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
- CUDA/cuDNN version: 10.2 / 8.2
- GPU model and memory: RTX 3090 / 24GB

**Describe the problem**

When trying to build TF 2.5.0 with Cuda 10.2, after applying the patch proposed in https://github.com/tensorflow/tensorflow/pull/48393 I get the following build error:

```
- Environment variables set:
TF_CUDA_CLANG=0
TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5
TF_CUDA_VERSION=10.2
TF_CUDNN_VERSION=8
TF_DOWNLOAD_CLANG=0
TF_DOWNLOAD_MKL=0
TF_ENABLE_XLA=0
TF_NCCL_VERSION=2
TF_NEED_AWS=0
TF_NEED_COMPUTECPP=0
TF_NEED_CUDA=1
TF_NEED_GCP=0
TF_NEED_GDR=0
TF_NEED_HDFS=0
TF_NEED_JEMALLOC=1
TF_NEED_KAFKA=0
TF_NEED_MKL=0
TF_NEED_MPI=0
TF_NEED_OPENCL=0
TF_NEED_OPENCL_SYCL=0
TF_NEED_ROCM=0
TF_NEED_S3=0
TF_NEED_TENSORRT=0
TF_NEED_VERBS=0
TF_SET_ANDROID_WORKSPACE=0
GCC_HOST_COMPILER_PATH=/usr/bin/gcc
CC_OPT_FLAGS=
PYTHON_BIN_PATH=/usr/local/bin/python
PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages
-- ./configure output:
You have bazel 3.7.2 installed.
Found CUDA 10.2 in:
    /usr/local/cuda-10.2/targets/x86_64-linux/lib
    /usr/local/cuda-10.2/targets/x86_64-linux/include
Found cuDNN 8 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
Found NCCL 2 in:
    /usr/lib/x86_64-linux-gnu
    /usr/include
```

Built using `bazel build --verbose_failures --config=opt --config=v2 --config=cuda //tensorflow/tools/pip_package:build_pip_package`

Error log
```
[25,186 / 36,992] Compiling tensorflow/lite/toco/tflite/operator.cc; 13s local ... (32 actions, 31 running)
ERROR: /usr/local/src/tensorflow/tensorflow/core/kernels/image/BUILD:241:18: C++ compilation of rule '//tensorflow/core/kernels/image:extract_volume_patches_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/local/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5 \
    TF_CUDA_VERSION=10.2 \
    TF_CUDNN_VERSION=8 \
    TF_NCCL_VERSION=2 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-opt/bin/external/mkl_dnn_v1 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_60' '--cuda-gpu-arch=sm_60' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' '--cuda-include-ptx=sm_70' '--cuda-gpu-arch=sm_70' '--cuda-include-ptx=sm_75' '--cuda-gpu-arch=sm_75' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/image/extract_volume_patches_op_gpu.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o)
Execution platform: @local_execution_config_platform//:platform
external/com_google_absl/absl/functional/function_ref.h:100:29: error: parameter packs not expanded with ‘...’:
   template <typename F, typename = EnableIfCompatible<const F&>>
                             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                
external/com_google_absl/absl/functional/function_ref.h:100:29: note:         ‘Args’
external/com_google_absl/absl/functional/function_ref.h:114:13: error: parameter packs not expanded with ‘...’:
       typename F, typename = EnableIfCompatible<F*>,
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                
external/com_google_absl/absl/functional/function_ref.h:114:13: note:         ‘Args’
Target //tensorflow/tools/pip_package:build_pip_package failed to build
ERROR: /usr/local/src/tensorflow/tensorflow/lite/toco/python/BUILD:89:10 C++ compilation of rule '//tensorflow/core/kernels/image:extract_volume_patches_op_gpu' failed (Exit 1): crosstool_wrapper_driver_is_not_gcc failed: error executing command 
  (cd /root/.cache/bazel/_bazel_root/bbcc73fcc5c2b01ab08b6bcf7c29e42e/execroot/org_tensorflow && \
  exec env - \
    CUDA_TOOLKIT_PATH=/usr/local/cuda-10.2 \
    GCC_HOST_COMPILER_PATH=/usr/bin/x86_64-linux-gnu-gcc-7 \
    LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \
    PATH=/root/.cache/bazelisk/downloads/bazelbuild/bazel-3.7.2-linux-x86_64/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \
    PWD=/proc/self/cwd \
    PYTHON_BIN_PATH=/usr/local/bin/python \
    PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \
    TF2_BEHAVIOR=1 \
    TF_CUDA_COMPUTE_CAPABILITIES=6.0,6.1,7.0,7.5 \
    TF_CUDA_VERSION=10.2 \
    TF_CUDNN_VERSION=8 \
    TF_NCCL_VERSION=2 \
  external/local_config_cuda/crosstool/clang/bin/crosstool_wrapper_driver_is_not_gcc -MD -MF bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.d '-frandom-seed=bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o' -DTENSORFLOW_USE_CUSTOM_CONTRACTION_KERNEL -DTENSORFLOW_USE_MKLDNN_CONTRACTION_KERNEL -DEIGEN_MPL2_ONLY '-DEIGEN_MAX_ALIGN_BYTES=64' -iquote . -iquote bazel-out/k8-opt/bin -iquote external/com_google_absl -iquote bazel-out/k8-opt/bin/external/com_google_absl -iquote external/nsync -iquote bazel-out/k8-opt/bin/external/nsync -iquote external/eigen_archive -iquote bazel-out/k8-opt/bin/external/eigen_archive -iquote external/gif -iquote bazel-out/k8-opt/bin/external/gif -iquote external/libjpeg_turbo -iquote bazel-out/k8-opt/bin/external/libjpeg_turbo -iquote external/com_google_protobuf -iquote bazel-out/k8-opt/bin/external/com_google_protobuf -iquote external/com_googlesource_code_re2 -iquote bazel-out/k8-opt/bin/external/com_googlesource_code_re2 -iquote external/farmhash_archive -iquote bazel-out/k8-opt/bin/external/farmhash_archive -iquote external/fft2d -iquote bazel-out/k8-opt/bin/external/fft2d -iquote external/highwayhash -iquote bazel-out/k8-opt/bin/external/highwayhash -iquote external/zlib -iquote bazel-out/k8-opt/bin/external/zlib -iquote external/local_config_cuda -iquote bazel-out/k8-opt/bin/external/local_config_cuda -iquote external/local_config_rocm -iquote bazel-out/k8-opt/bin/external/local_config_rocm -iquote external/local_config_tensorrt -iquote bazel-out/k8-opt/bin/external/local_config_tensorrt -iquote external/mkl_dnn_v1 -iquote bazel-out/k8-opt/bin/external/mkl_dnn_v1 -Ibazel-out/k8-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual -Ibazel-out/k8-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers -isystem external/nsync/public -isystem bazel-out/k8-opt/bin/external/nsync/public -isystem third_party/eigen3/mkl_include -isystem bazel-out/k8-opt/bin/third_party/eigen3/mkl_include -isystem external/eigen_archive -isystem bazel-out/k8-opt/bin/external/eigen_archive -isystem external/gif -isystem bazel-out/k8-opt/bin/external/gif -isystem external/com_google_protobuf/src -isystem bazel-out/k8-opt/bin/external/com_google_protobuf/src -isystem external/farmhash_archive/src -isystem bazel-out/k8-opt/bin/external/farmhash_archive/src -isystem external/zlib -isystem bazel-out/k8-opt/bin/external/zlib -isystem external/local_config_cuda/cuda -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda -isystem external/local_config_cuda/cuda/cuda/include -isystem bazel-out/k8-opt/bin/external/local_config_cuda/cuda/cuda/include -isystem external/local_config_rocm/rocm -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm -isystem external/local_config_rocm/rocm/rocm/include -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include -isystem external/local_config_rocm/rocm/rocm/include/rocrand -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand -isystem external/local_config_rocm/rocm/rocm/include/roctracer -isystem bazel-out/k8-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer -isystem external/mkl_dnn_v1/include -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/include -isystem external/mkl_dnn_v1/src -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src -isystem external/mkl_dnn_v1/src/common -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common -isystem external/mkl_dnn_v1/src/common/ittnotify -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/common/ittnotify -isystem external/mkl_dnn_v1/src/cpu -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu -isystem external/mkl_dnn_v1/src/cpu/gemm -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/gemm -isystem external/mkl_dnn_v1/src/cpu/x64/xbyak -isystem bazel-out/k8-opt/bin/external/mkl_dnn_v1/src/cpu/x64/xbyak -Wno-builtin-macro-redefined '-D__DATE__=""redacted""' '-D__TIMESTAMP__=""redacted""' '-D__TIME__=""redacted""' -fPIC -U_FORTIFY_SOURCE '-D_FORTIFY_SOURCE=1' -fstack-protector -Wall -fno-omit-frame-pointer -no-canonical-prefixes -fno-canonical-system-headers -DNDEBUG -g0 -O2 -ffunction-sections -fdata-sections -w -DAUTOLOAD_DYNAMIC_KERNELS -Wno-sign-compare '-std=c++14' -x cuda '-DGOOGLE_CUDA=1' '-Xcuda-fatbinary=--compress-all' '--no-cuda-include-ptx=all' '--cuda-include-ptx=sm_60' '--cuda-gpu-arch=sm_60' '--cuda-include-ptx=sm_61' '--cuda-gpu-arch=sm_61' '--cuda-include-ptx=sm_70' '--cuda-gpu-arch=sm_70' '--cuda-include-ptx=sm_75' '--cuda-gpu-arch=sm_75' -DEIGEN_AVOID_STL_ARRAY -Iexternal/gemmlowp -Wno-sign-compare '-ftemplate-depth=900' -fno-exceptions '-DGOOGLE_CUDA=1' '-DTENSORFLOW_USE_NVCC=1' -DINTEL_MKL -msse3 -pthread '-nvcc_options=relaxed-constexpr' '-nvcc_options=ftz=true' -c tensorflow/core/kernels/image/extract_volume_patches_op_gpu.cu.cc -o bazel-out/k8-opt/bin/tensorflow/core/kernels/image/_objs/extract_volume_patches_op_gpu/extract_volume_patches_op_gpu.cu.pic.o)
Execution platform: @local_execution_config_platform//:platform
INFO: Elapsed time: 1137.195s, Critical Path: 92.10s
INFO: 25995 processes: 11503 internal, 14492 local.
FAILED: Build did NOT complete successfully
FAILED: Build did NOT complete successfully
Command exited with non-zero status 1
```



**Any other info / logs**

Diff from known functional build for 10.2 with 2.4.1
https://github.com/datamachines/cuda_tensorflow_opencv/compare/20210601

Steps to reproduce: 
```
% git clone https://github.com/datamachines/cuda_tensorflow_opencv.git
% cd cuda_tensorflow_opencv
% git checkout 737f860
% make make cudnn_tensorflow_opencv-10.2_2.5.0_3.4.14
```

Attaching full build log.
[cudnn_tensorflow_opencv-10.2_2.5.0_3.4.14-20210601.log.txt](https://github.com/tensorflow/tensorflow/files/6586712/cudnn_tensorflow_opencv-10.2_2.5.0_3.4.14-20210601.log.txt)
"
49982,test build and install issue,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version:
- Python version:
- Installed using virtualenv? pip? conda?:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
49980,MirroredStrategy throws warning during startup,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0 and 2.6.0-dev20210602
- Python version: 3.7 and 3.8

**Describe the current behavior**
When instantiating a `MirroredStrategy` at the top of a program using:
```python
import tensorflow as tf

tf.distribute.MirroredStrategy()
```
It throws the following warning suggesting users that they might run into performance problems:
```
WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.
```
This warning is thrown in
https://github.com/tensorflow/tensorflow/blob/280d27a99d5057f3ef421dd684fbb327cd1ff20a/tensorflow/python/distribute/mirrored_strategy.py#L375-L380
and it will be thrown as well even if the code is running directly after importing TensorFlow.

**Describe the expected behavior**

No warning should be thrown by default or the warning should include actionable items for the user to resolve this issue.

**Standalone code to reproduce the issue**
Checkout [this notebook for full reproduction](https://colab.research.google.com/drive/1h5b4Ve71n0e-VDngHRDMUL4CHwGqxvPE?usp=sharing)."
49979,"RuntimeError: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (3 != 4)Node number 21 (PAD) failed to prepare.","**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- TensorFlow installed from (source or binary):
- TensorFlow version 2.4

I have converted the mobilebert model to tflite and now I want to perform inference on that, for a single value, it works fine but for a batch, I am reshaping the input size of the tensor. I am running the following lines of code but it gives me the dimension error.

Code I am running: 
```
input_data10 = np.expand_dims(input_text[1:1001], axis=1)
interpreter.resize_tensor_input(input_details[0]['index'], [1000, 1, 100])
interpreter.allocate_tensors()
interpreter.set_tensor(input_details[0]['index'], input_data10)
interpreter.allocate_tensors()
interpreter.invoke()
```

Error I am receiving


```
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-65-7d35ed1dfe14> in <module>
----> 1 interpreter.invoke()

/usr/local/lib/python3.6/dist-packages/tensorflow/lite/python/interpreter.py in invoke(self)
    538     """"""
    539     self._ensure_safe()
--> 540     self._interpreter.Invoke()
    541 
    542   def reset_all_variables(self):

RuntimeError: tensorflow/lite/kernels/pad.cc:79 SizeOfDimension(op_context->paddings, 0) != op_context->dims (3 != 4)Node number 21 (PAD) failed to prepare.

``"
49978,TFlite conversion incompatibility with tf.Data,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Big Sur (11.4)
- TensorFlow installation (pip package or built from source): Pip package
- TensorFlow library (version, if pip package or github SHA, if built from source): 2.5.0

### 2. Code

This code has been tested on a colab instance with TF 2.5.0 and reproduces the error.

```
import numpy as np
import tensorflow as tf
batch_size = 100
ds = tf.data.Dataset.from_tensor_slices((np.random.normal(size=(10000,10)), np.zeros(10000))) \
.batch(batch_size) \
.prefetch(tf.data.experimental.AUTOTUNE)

model = tf.keras.Sequential([
  tf.keras.layers.Dense(256),
  tf.keras.layers.Dense(1)
])


optimizer = tf.keras.optimizers.Adam()
model.compile(optimizer=optimizer,
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=False))
model.fit(ds,
          epochs=1)


model.save(""test_model"",save_format=""tf"")
converter = tf.lite.TFLiteConverter.from_saved_model('test_model',signature_keys=['serving_default'])
tflite_model = converter.convert()
```

### 3. Failure after conversion
If the conversion is successful, but the generated model is wrong, then state what is wrong:

The code fails to convert the above TF model, and yields the following exception:

```
---------------------------------------------------------------------------
Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    293                                                  debug_info_str,
--> 294                                                  enable_mlir_converter)
    295       return model_str

4 frames
Exception: <unknown>:0: error: loc(callsite(callsite(""sequential/dense/Cast@__inference__wrapped_model_711"" at ""StatefulPartitionedCall@__inference_signature_wrapper_888"") at ""StatefulPartitionedCall"")): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Cast
Details:
	tf.Cast {Truncate = false, device = """"}



During handling of the above exception, another exception occurred:

ConverterError                            Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py in toco_convert_protos(model_flags_str, toco_flags_str, input_data_str, debug_info_str, enable_mlir_converter)
    295       return model_str
    296     except Exception as e:
--> 297       raise ConverterError(str(e))
    298 
    299   if distutils.spawn.find_executable(_toco_from_proto_bin) is None:

ConverterError: <unknown>:0: error: loc(callsite(callsite(""sequential/dense/Cast@__inference__wrapped_model_711"" at ""StatefulPartitionedCall@__inference_signature_wrapper_888"") at ""StatefulPartitionedCall"")): 'tf.Cast' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Cast
Details:
	tf.Cast {Truncate = false, device = """"}
```

### 5. (optional) Any other info / logs
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.

Converting the above model works fine if the tf.Data input pipeline is replaced with numpy arrays. Is it possible to get this to work with the tf.Data API? It seems minimally complex, it's unclear where the issue is arising. Thank you."
49977,Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 (64bit)
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 1.15.0
- Python version: `Python 3.6.13 |Anaconda, Inc.| (default, Mar 16 2021, 11:37:27) [MSC v.1916 64 bit (AMD64)] on win32`
- Installed using virtualenv? pip? conda?: I installed tensorflow-gpu==1.15 inside a conda environment, and this automatically installed cuDNN too, but I had installed manually the Cuda Computing Toolkit and I also added the following environment variables `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin` and `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64`, and there's also this system variable `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0` and `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.3` (with different names: yes, I installed 2 versions of the Cuda Computing Toolkit because, after having installed the latest one, v11.3, I realised that TF 1.15 requires version 10.0)
- CUDA/cuDNN version: `cudatoolkit               10.0.130                      0` and `cudnn                     7.6.5                cuda10.0_0`  (shown with `conda list`)
- GPU model and memory: `NVIDIA Quadro RTX 4000 major: 7 minor: 5 memoryClockRate(GHz): 1.545`



**Describe the problem**

I am getting the following errors

```
name: NVIDIA Quadro RTX 4000 major: 7 minor: 5 memoryClockRate(GHz): 1.545
pciBusID: 0000:01:00.0
2021-06-01 19:41:45.931750: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudart64_100.dll
2021-06-01 19:41:45.931822: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cublas64_100.dll
2021-06-01 19:41:45.931893: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cufft64_100.dll
2021-06-01 19:41:45.931965: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library curand64_100.dll
2021-06-01 19:41:45.932036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusolver64_100.dll
2021-06-01 19:41:45.932109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cusparse64_100.dll
2021-06-01 19:41:45.932181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library cudnn64_7.dll
2021-06-01 19:41:45.932274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2021-06-01 19:41:45.932356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-06-01 19:41:45.932427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2021-06-01 19:41:45.932472: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2021-06-01 19:41:45.932569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6710 MB memory) -> physical GPU (device: 0, name: NVIDIA Quadro RTX 4000, pci bus id: 0000:01:00.0, compute capability: 7.5)
2021-06-01 19:41:51.889919: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.
2021-06-01 19:41:51.889989: W tensorflow/core/profiler/lib/profiler_session.cc:213] Encountered error while starting profiler: Unavailable: CUPTI error: CUPTI could not be loaded or symbol could not be found.
2021-06-01 19:41:51.893350: I tensorflow/core/platform/default/device_tracer.cc:588] Collecting 0 kernel records, 0 memcpy records.
2021-06-01 19:41:51.893617: E tensorflow/core/platform/default/device_tracer.cc:70] CUPTI error: CUPTI could not be loaded or symbol could not be found.
2021-06-01 19:41:52.546494: I tensorflow/core/profiler/lib/profiler_session.cc:205] Profiler session started.
```

although, as I said, cuDNN seems to be installed (by conda automatically when installing tensorflow-gpu 1.15), the Cuda Computing Toolkit is also installed, and I added the following paths to the system variables

1.  `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\bin` (only to my user variables),
2.  `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64` (user variables)
3.  `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0` (system variable)
4.  `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.3` (system variable)

**Provide the exact sequence of commands / steps that you executed before running into the problem**

I don't have the exact sequence because I cannot share my code right now. But, as you can see above, the DDLs like `cudart64_100.dll` are successfully loaded, so I don't understand why I get this error

> CUPTI error: CUPTI could not be loaded or symbol could not be found.

It's so annoying because I don't know whether I am actually using the GPU. Right now, my experiments take a long time and I don't know if this is due to the fact that I am or not using the GPU.

**Any other info / logs**

I need to use tensorflow v 1.15 because I am using another library that use this version. Moreover, I am aware of similar questions/issues on the web, but people seem to suggest to add `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0\extras\CUPTI\libx64` to the system variables (e.g. [here](https://stackoverflow.com/a/57592360)), which I already did. 
"
49976,model.layers is empty after upgrading to 2.5.0,"**System information**
- custom CNN model
- Linux Ubuntu 20.04
- TensorFlow installed from pip
- TensorFlow version: v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.8.5
- CUDA/cuDNN version: 11.1
- GPU model and memory: Quadro K5200

I upgraded Tensorflow  to 2.5.0 through pip so I could get support from Cuda 11.1.
After this upgrade, when I tried to access keras.optimizers.Adam, it wasn't there anymore, so I switched to tf.keras as it's recommended I see, but doing so I then realized that model.layers is now an empty list after building my model:

        self.model = keras.models.Model(inputs=[input_img], outputs=[outputs])
        self.model.compile(optimizer=keras.optimizers.Adam(),
                           loss=""binary_crossentropy"",
                           metrics=[""accuracy""])
        print(self.model.layers) -> returns empty list

Tried updating tensorflow to a different computer, same issue with model.layers but I can still access keras.optimizers.Adam (this one has a slightly different setup as I'm using miniconda)."
49975,terminate called after throwing an instance of 'std::bad_alloc'   what():  std::bad_alloc," I have  Cross-compile the  .a file and Cross-compile the label_image , but when I put  files  in the armv7, it is a error that _[./label_iamge:cannot execute binary file :Exec format error](url). I have check  the Cross-compile process ,it is seem that's ok.
Cross-compile tools :  g++-arm-linux-gnueabihf 、gcc-arm-linux-gnueabihf
arm: NXP i.MX6Q Cortex-A9 
root@EmbedSky-Board:/xzy/mix# lscpu
_Architecture:          armv7l
Byte Order:            Little Endian
CPU(s):                4
On-line CPU(s) list:   0-3
Thread(s) per core:    1
Core(s) per socket:    4
Socket(s):             1
Model name:            ARMv7 Processor rev 10 (v7l)
CPU max MHz:           996.0000
CPU min MHz:           792.0000
root@EmbedSky-Board:/xzy/mix# cat /proc/cpuinfo
processor       : 0 
model name      : ARMv7 Processor rev 10 (v7l)
BogoMIPS        : 6.00
Features        : half thumb fastmult vfp edsp neon vfpv3 tls vfpd32
CPU implementer : 0x41
CPU architecture: 7
CPU variant     : 0x2
CPU part        : 0xc09
CPU revision    : 10_
"
49973,How to do inference when using tf.lite model when input is string ,"tf ver: 2.5.0

I previously post a thread : https://github.com/tensorflow/tensorflow/issues/48449
 to discuss how to convert model with tf.hub layers to tf.lite model. In 2.5.0 version, it succeed.

I run the whole process, where I use tf.data flow as input for the large text data. The task is just text classification where the input sentences are string but not natural language.(It does not matter).

My code:
```
import sys,os, logging,argparse,math,string,json,gzip
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd 
from tensorflow.keras import layers
import glob,pickle,random
from sklearn.metrics import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.metrics import *
from tensorflow.python.keras.metrics import sparse_top_k_categorical_accuracy

print(tf.__version__)

from transblock import * 
def acc_top4(y_true, y_pred):
    return sparse_top_k_categorical_accuracy(y_true, y_pred, k=4)

def acc_top8(y_true, y_pred):
    return sparse_top_k_categorical_accuracy(y_true, y_pred, k=8)


with open('./app_map_5000.json','r') as f:
    white_5000 = json.load(f)

init = tf.lookup.KeyValueTensorInitializer(
    keys=tf.constant(list(white_5000.keys())),
    values=tf.constant(list([ int(i) for i in white_5000.values()]), dtype=tf.int64))

table = tf.lookup.StaticVocabularyTable(
   init, lookup_key_dtype=tf.string,
   num_oov_buckets=5)

def parser(x):
    x_ = tf.strings.regex_replace(x, '""','')
    tokens = tf.strings.split(x_, sep=',')
    #label = 
    #label = tf.strings.to_number(tokens[-1], tf.int32)
    #features = []
    #words = tf.strings.split(tokens[0], sep=' ')
    #f = table.lookup(words)
    #features.append(tf.concat([f1, tf.expand_dims(f2,-1)], axis=0))
    return tokens[0], table.lookup(tokens[-1])

def get_ds(files, val):
    ds = tf.data.TextLineDataset(files, buffer_size=12800, num_parallel_reads=48 ,compression_type='GZIP')
    ds = ds.map(lambda x: parser(x), tf.data.experimental.AUTOTUNE )
    if not val:
        ds = ds.shuffle(12800)#.repeat()
    ds = ds.batch(128).prefetch(tf.data.experimental.AUTOTUNE)
    return ds


ds_train = get_ds(files=glob.glob('./sents_0420_0428_nofil/part-*.csv.gz'),  val=False)
ds_test =  get_ds(files=glob.glob('./sents_0429_nofil/part-*.csv.gz'),  val=True)

for features in ds_test.take(10):
    print(features)

def get_model_transormer(num_classes):
    preprocessor_file = ""./albert_en_preprocess_3"" # https://tfhub.dev/tensorflow/albert_en_preprocess/3
    preprocessor_layer = hub.KerasLayer(preprocessor_file)
    preprocessor = hub.load(preprocessor_file)
    vocab_size = preprocessor.tokenize.get_special_tokens_dict()['vocab_size'].numpy()

    embed_dim = 32  # Embedding size for each token
    num_heads = 2  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feed forward network inside transformer
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) 

    encoder_inputs = preprocessor_layer(text_input)['input_word_ids']

    embedding_layer = TokenAndPositionEmbedding(encoder_inputs.shape[1], vocab_size, embed_dim)
    x = embedding_layer(encoder_inputs)
    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(512, activation=""relu"")(x)

    outputs = layers.Dense(num_classes, activation=""softmax"")(x)
    model = keras.Model(inputs=text_input, outputs=outputs)
    model.compile(""adam"", ""sparse_categorical_crossentropy"", metrics=[""acc"", acc_top4, acc_top8])
    return model

# def get_model_bert(num_classes, m='albert'):

#     text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) # shape=(None,) dtype=string
#     m_file = {'albert':""./albert_en_base_2"", 'electra':'./electra_base_2', 'dan':""./universal-sentence-encoder_4""}

#     encoder = hub.KerasLayer(m_file[m], trainable=True)

#     if m in ['albert', 'electra']:
#         encoder_inputs = preprocessor_layer(text_input)
#         outputs = encoder(encoder_inputs)
#         embed = outputs[""pooled_output""]  
#     elif m in ['dan']:
#         embed = encoder(text_input)
#     else:
#         raise KeyError(""model illegal!"")

#     out = layers.Dense(num_classes, activation=""softmax"")(embed)
#     model = tf.keras.Model(inputs=text_input, outputs=out)
#     model.compile(Adam(learning_rate=1e-5), ""sparse_categorical_crossentropy"", metrics=[""acc"", acc_top4, acc_top8])
#     return model

# x_train, y_train = df_train['content'].values.reshape(-1,1), df_train['label'].values
# x_test, y_test = df_test['content'].values.reshape(-1,1), df_test['label'].values

#model = get_model_bert(df_train['label'].max()+1)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='./checkpoint_epoch{epoch}',
    save_weights_only=False,
    monitor='val_acc_top8',
    mode='max',
    save_best_only=False)

model = get_model_transormer(5001)
model.save('checkpoint__')

history = model.fit(
    ds_train, validation_data=ds_test, epochs=1, \
     steps_per_epoch=1000, validation_steps=1000,
     verbose=1, callbacks=[model_checkpoint_callback]
)

# transform to another model which use the dense_2 layer as output
model_file = './checkpoint_epoch'
model = tf.keras.models.load_model(model_file, \
                        custom_objects={""acc_top4"":acc_top4, ""acc_top8"":acc_top8, \
                       ""softmax"":tf.keras.activations.softmax})
intermediate_layer_model = tf.keras.Model(inputs=model.input,
                                 outputs=model.get_layer('dense_2').output)
intermediate_layer_model.save(model_file+""_inter"")
y = intermediate_layer_model.predict(ds_test, verbose=1, steps=10)

# save the model
converter = tf.lite.TFLiteConverter.from_saved_model(model_file+""_inter"")

#### ver: 2.5.0
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

# write the tflite model
tflite_model = converter.convert()
open(""{}.tflite"".format(model_file+""_inter""), ""wb"").write(tflite_model)

########## reload 
#model = tf.keras.models.load_model('./model_transoformer_1216_epoch_1')
interpreter = tf.lite.Interpreter(model_path= ""{}.tflite"".format(model_file+""_inter"") )    
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.allocate_tensors()
```

The code can run without any error. In the final stage, I saved the model in tf.lite format. Then it can be reloaded again as 
`interpreter`. 
I want it to do inference when the input is `sentence`
`sentence = 'ysbang.cn dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 curdow3 curhour8'`

`sentence` is the string type, same as when I do training. 
 `sentence` is processed by the tf.hub `preprocessor_layer` in the model.

Here I want `interpreter` to do inference with `sentence` as input.

I tries this :
```
interpreter.set_tensor(input_details[0]['index'], tf.convert_to_tensor([sentence]) )
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
pred = output_data[0][0]
```
but it crashed with error: 

> segmentation fault (core dumped)

My question is how to set the input to get the output when using `interpreter` ?


"
49972,Running Hello World example on STM32F746G-DISCO results in HardFault,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- TensorFlow installed from (source or binary): Source
- Tensorflow version (commit SHA if source): Release 2.6.0
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): STM32F746G-DISCO
- MBed version: 1.10.5

**Describe the problem**
After compiling and copying the mbed.bin to the STM32F746G-DISCO, the program runs for a short bit and then experiences a Hard Fault.

**Please provide the exact sequence of commands/steps when you ran into the problem**
Instructions followed:
[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world](url)

cp ./BUILD/DISCO_F746NG/GCC_ARM/mbed.bin /media/jomodev/DIS_F746NG/

Error seen from the STM32F7 DISCO Term output:
```
Script started on 2021-06-01 20:31:25-0700
$ screen /dev/ttyACM0 9600
x_value: 1.4361557*2^-4, y_value: 1.7621826*2^-4
x_value: 1.4361557*2^-3, y_value: 1.8977352*2^-3
x_value: 1.0771168*2^-2, y_value: 1.389413*2^-2
x_value: 1.4361557*2^-2, y_value: 1.5588537*2^-2
x_value: 1.7951952*2^-2, y_value: 1.7960706*2^-2
x_value: 1.0771168*2^-1, y_value: 1.965511*2^-2
x_value: 1.2566366*2^-1, y_value: 1.1183078*2^-1
x_value: 1.4361557*2^-1, y_value: 1.3724688*2^-1
x_value: 1.6156756*2^-1, y_value: 1.4910772*2^-1
x_value: 1.7951952*2^-1, y_value: 1.6605182*2^-1
x_value: 1.9747147*2^-1, y_value: 1.7791265*2^-1
x_value: 1.0771168*2^0, y_value: 1.7791265*2^-1
x_value: 1.1668766*2^0, y_value: 1.8977352*2^-1
x_value: 1.2566366*2^0, y_value: 1.9316229*2^-1
x_value: 1.3463962*2^0, y_value: 1.965511*2^-1
x_value: 1.4361557*2^0, y_value: 1.0166438*2^0
[46C
++ MbedOS Fault Handler ++
[26C
[26CFaultType: HardFault
[46C
[46CContext:
[54CR0: 72
[60CR1: FFFE
[68CR2: FFDB4437
[80CR3: 72
[86CR4: FFFFFFEF
[98CR5: A
[103CR6: A
[108CR7: FFDB4437
[8CR8: 0
[13CR9: 72
[19CR10: 8
[25CR11: 68
[32CR12: 38F
[40CSP   : 2004FF98
[55CLR   : 800051B
[69CPC   : 8000488
[83CxPSR : 810B0000
[98CPSP  : 0
[106CMSP  : 2004FF30
[9CCPUID: 410FC271
[24CHFSR : 40000000
[39CMMFSR: 0
[47CBFSR : 4
[55CUFSR : 0
[63CDFSR : 9
[71CAFSR : 0
[79CMode : Thread
[92CPriv : Privileged
[109CStack: MSP
[7C-- MbedOS Fault Handler --
[33C
[33C
[33C
[33C++ MbedOS Error Info ++
[56CError Status: 0x80FF013D Code: 317 Module: 255
[102CError Message: Fault exception
[20CLocation: 0x8000488
[39CError Value: 0x20002DEC
[62CFor more info, visit: https://mbed.com/s/error?error=0x80FF013D&tgt=

```

"
49969,'tf.Mul' op is neither a custom op nor a flex op when I convert saved_model to tflite model,"Environment is NVIDIA-SMI 450.80.02    Driver Version: 450.80.02    CUDA Version: 11.0 tensorflow-2.5/2.6 python3.8.

My net contain the ops: matmul, multiply. When I convert the saved model to tflite model, it report that

<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: loc(callsite(callsite(callsite(callsite(""Mul_1@__inference_call_8437"" at ""conformer_encoder/conformer_block_3/mhsa_module/multi_head_attention_3/StatefulPartitionedCall@__inference_call_8638"") at ""transducer_encoder/StatefulPartitionedCall@__inference__wrapped_model_8931"") at ""StatefulPartitionedCall@__inference_signature_wrapper_18974"") at ""StatefulPartitionedCall"")): 'tf.Mul' op is neither a custom op nor a flex op
<unknown>:0: note: loc(""StatefulPartitionedCall""): called from
<unknown>:0: error: failed while converting: 'main': 
Some ops are not supported by the native TFLite runtime, you can enable TF kernels fallback using TF Select. See instructions: https://www.tensorflow.org/lite/guide/ops_select 
TF Select ops: Mul
Details:
        tf.Mul {device = """"}

How can l solve it?
"
49968,Can not convert to tf.lite when feeding model with tf.data,"tf ver: 2.5.0

I previously post a thread : https://github.com/tensorflow/tensorflow/issues/48449
 to discuss how to convert model with tf.hub layers to tf.lite model. In 2.5.0 version, it succeed.

I run the whole process, where I use tf.data flow as input for the large text data.
My code:
```
import sys,os, logging,argparse,math,string,json,gzip
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd 
from tensorflow.keras import layers
import glob,pickle,random
from sklearn.metrics import *
from tensorflow.keras.callbacks import *
from tensorflow.keras.metrics import *
from tensorflow.python.keras.metrics import sparse_top_k_categorical_accuracy

print(tf.__version__)

from transblock import * 
def acc_top4(y_true, y_pred):
    return sparse_top_k_categorical_accuracy(y_true, y_pred, k=4)

def acc_top8(y_true, y_pred):
    return sparse_top_k_categorical_accuracy(y_true, y_pred, k=8)


with open('./app_map_5000.json','r') as f:
    white_5000 = json.load(f)

init = tf.lookup.KeyValueTensorInitializer(
    keys=tf.constant(list(white_5000.keys())),
    values=tf.constant(list([ int(i) for i in white_5000.values()]), dtype=tf.int64))

table = tf.lookup.StaticVocabularyTable(
   init, lookup_key_dtype=tf.string,
   num_oov_buckets=5)

def parser(x):
    x_ = tf.strings.regex_replace(x, '""','')
    tokens = tf.strings.split(x_, sep=',')
    #label = 
    #label = tf.strings.to_number(tokens[-1], tf.int32)
    #features = []
    #words = tf.strings.split(tokens[0], sep=' ')
    #f = table.lookup(words)
    #features.append(tf.concat([f1, tf.expand_dims(f2,-1)], axis=0))
    return tokens[0], table.lookup(tokens[-1])

def get_ds(files, val):
    ds = tf.data.TextLineDataset(files, buffer_size=12800, num_parallel_reads=48 ,compression_type='GZIP')
    ds = ds.map(lambda x: parser(x), tf.data.experimental.AUTOTUNE )
    if not val:
        ds = ds.shuffle(12800)#.repeat()
    ds = ds.batch(128).prefetch(tf.data.experimental.AUTOTUNE)
    return ds


ds_train = get_ds(files=glob.glob('./sents_0420_0428_nofil/part-*.csv.gz'),  val=False)
ds_test =  get_ds(files=glob.glob('./sents_0429_nofil/part-*.csv.gz'),  val=True)

for features in ds_test.take(10):
    print(features)

def get_model_transormer(num_classes):
    preprocessor_file = ""./albert_en_preprocess_3"" # https://tfhub.dev/tensorflow/albert_en_preprocess/3
    preprocessor_layer = hub.KerasLayer(preprocessor_file)
    preprocessor = hub.load(preprocessor_file)
    vocab_size = preprocessor.tokenize.get_special_tokens_dict()['vocab_size'].numpy()

    embed_dim = 32  # Embedding size for each token
    num_heads = 2  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feed forward network inside transformer
    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) 

    encoder_inputs = preprocessor_layer(text_input)['input_word_ids']

    embedding_layer = TokenAndPositionEmbedding(encoder_inputs.shape[1], vocab_size, embed_dim)
    x = embedding_layer(encoder_inputs)
    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dense(512, activation=""relu"")(x)

    outputs = layers.Dense(num_classes, activation=""softmax"")(x)
    model = keras.Model(inputs=text_input, outputs=outputs)
    model.compile(""adam"", ""sparse_categorical_crossentropy"", metrics=[""acc"", acc_top4, acc_top8])
    return model

# def get_model_bert(num_classes, m='albert'):

#     text_input = tf.keras.layers.Input(shape=(), dtype=tf.string) # shape=(None,) dtype=string
#     m_file = {'albert':""./albert_en_base_2"", 'electra':'./electra_base_2', 'dan':""./universal-sentence-encoder_4""}

#     encoder = hub.KerasLayer(m_file[m], trainable=True)

#     if m in ['albert', 'electra']:
#         encoder_inputs = preprocessor_layer(text_input)
#         outputs = encoder(encoder_inputs)
#         embed = outputs[""pooled_output""]  
#     elif m in ['dan']:
#         embed = encoder(text_input)
#     else:
#         raise KeyError(""model illegal!"")

#     out = layers.Dense(num_classes, activation=""softmax"")(embed)
#     model = tf.keras.Model(inputs=text_input, outputs=out)
#     model.compile(Adam(learning_rate=1e-5), ""sparse_categorical_crossentropy"", metrics=[""acc"", acc_top4, acc_top8])
#     return model

# x_train, y_train = df_train['content'].values.reshape(-1,1), df_train['label'].values
# x_test, y_test = df_test['content'].values.reshape(-1,1), df_test['label'].values

#model = get_model_bert(df_train['label'].max()+1)

model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath='./checkpoint_epoch{epoch}',
    save_weights_only=False,
    monitor='val_acc_top8',
    mode='max',
    save_best_only=False)

model = get_model_transormer(5001)
model.save('checkpoint__')

history = model.fit(
    ds_train, validation_data=ds_test, epochs=1, \
     steps_per_epoch=1000, validation_steps=1000,
     verbose=1, callbacks=[model_checkpoint_callback]
)

# transform to another model which use the dense_2 layer as output
model_file = './checkpoint_epoch'
model = tf.keras.models.load_model(model_file, \
                        custom_objects={""acc_top4"":acc_top4, ""acc_top8"":acc_top8, \
                       ""softmax"":tf.keras.activations.softmax})
intermediate_layer_model = tf.keras.Model(inputs=model.input,
                                 outputs=model.get_layer('dense_2').output)
intermediate_layer_model.save(model_file+""_inter"")
y = intermediate_layer_model.predict(ds_test, verbose=1, steps=10)

# save the model
converter = tf.lite.TFLiteConverter.from_saved_model(model_file+""_inter"")

#### ver: 2.5.0
converter.target_spec.supported_ops = [
  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.
  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.
]

# write the tflite model
tflite_model = converter.convert()
open(""{}.tflite"".format(model_file+""_inter""), ""wb"").write(tflite_model)

########## reload 
#model = tf.keras.models.load_model('./model_transoformer_1216_epoch_1')
interpreter = tf.lite.Interpreter(model_path= ""{}.tflite"".format(model_file+""_inter"") )    
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()
interpreter.allocate_tensors()
```

The code can run without any error. In the final stage, I saved the model in tf.lite format. Then it can be reloaded again as 
`interpreter`. 
I want it to do inference when the input is `sentence`
`sentence = 'ysbang.cn dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 com.tencent.mm dow3 hour8 curdow3 curhour8'`

`sentence` is the string type, same as when I do training. I suppose `sentence` is processed by the tf.hub `preprocessor_layer`.

Here I want `interpreter` to do inference with `sentence` as input.

I tries this :
```
interpreter.set_tensor(input_details[0]['index'], tf.convert_to_tensor([sentence]) )
interpreter.invoke()
output_data = interpreter.get_tensor(output_details[0]['index'])
pred = output_data[0][0]
```
but it crashed with error: 

> segmentation fault (core dumped)

My question is how to set the input to get the output when using `interpreter` ?


"
49962,AutoGraph could not transform call method of keras.Model subclass,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
    - I am following along with a tutorial provided by TensorFlow:
        - https://www.tensorflow.org/tutorials/structured_data/time_series
        - The error messages arise when compiling/evaluating the ""Baseline"" model.
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
    - Ubuntu 20.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
    - N/A
- TensorFlow installed from (source or binary):
    - Anaconda
- TensorFlow version (use command below):
    - After running the command `conda list --export`:
        ```
        # This file may be used to create an environment using:
        # $ conda create --name <env> --file <this file>
        # platform: linux-64
        _libgcc_mutex=0.1=main
        _tflow_select=2.3.0=mkl
        absl-py=0.12.0=py39h06a4308_0
        aiohttp=3.7.4=py39h27cfd23_1
        anyio=2.2.0=py39h06a4308_1
        argon2-cffi=20.1.0=py39h27cfd23_1
        astor=0.8.1=py39h06a4308_0
        astunparse=1.6.3=py_0
        async-timeout=3.0.1=py39h06a4308_0
        async_generator=1.10=pyhd3eb1b0_0
        attrs=21.2.0=pyhd3eb1b0_0
        babel=2.9.1=pyhd3eb1b0_0
        backcall=0.2.0=pyhd3eb1b0_0
        blas=1.0=mkl
        bleach=3.3.0=pyhd3eb1b0_0
        blinker=1.4=py39h06a4308_0
        brotlipy=0.7.0=py39h27cfd23_1003
        c-ares=1.17.1=h27cfd23_0
        ca-certificates=2021.5.25=h06a4308_1
        cachetools=4.2.2=pyhd3eb1b0_0
        certifi=2021.5.30=py39h06a4308_0
        cffi=1.14.5=py39h261ae71_0
        chardet=3.0.4=py39h06a4308_1003
        click=8.0.1=pyhd3eb1b0_0
        coverage=5.5=py39h27cfd23_2
        cryptography=3.4.7=py39hd23ed53_0
        cycler=0.10.0=py39h06a4308_0
        cython=0.29.23=py39h2531618_0
        dbus=1.13.18=hb2f20db_0
        decorator=5.0.9=pyhd3eb1b0_0
        defusedxml=0.7.1=pyhd3eb1b0_0
        entrypoints=0.3=py39h06a4308_0
        expat=2.4.1=h2531618_2
        fontconfig=2.13.1=h6c09931_0
        freetype=2.10.4=h5ab3b9f_0
        gast=0.4.0=py_0
        glib=2.68.2=h36276a3_0
        google-auth=1.30.1=pyhd3eb1b0_0
        google-auth-oauthlib=0.4.4=pyhd3eb1b0_0
        google-pasta=0.2.0=py_0
        grpcio=1.36.1=py39h2157cd5_1
        gst-plugins-base=1.14.0=h8213a91_2
        gstreamer=1.14.0=h28cd5cc_2
        h5py=2.10.0=py39hec9cf62_0
        hdf5=1.10.6=hb1b8bf9_0
        icu=58.2=he6710b0_3
        idna=2.10=pyhd3eb1b0_0
        importlib-metadata=3.10.0=py39h06a4308_0
        importlib_metadata=3.10.0=hd3eb1b0_0
        intel-openmp=2021.2.0=h06a4308_610
        ipykernel=5.3.4=py39hb070fc8_0
        ipython=7.22.0=py39hb070fc8_0
        ipython_genutils=0.2.0=pyhd3eb1b0_1
        jedi=0.17.2=py39h06a4308_1
        jinja2=3.0.0=pyhd3eb1b0_0
        jpeg=9b=h024ee3a_2
        json5=0.9.5=py_0
        jsonschema=3.2.0=py_2
        jupyter-packaging=0.7.12=pyhd3eb1b0_0
        jupyter_client=6.1.12=pyhd3eb1b0_0
        jupyter_core=4.7.1=py39h06a4308_0
        jupyter_server=1.4.1=py39h06a4308_0
        jupyterlab=3.0.14=pyhd3eb1b0_1
        jupyterlab_pygments=0.1.2=py_0
        jupyterlab_server=2.4.0=pyhd3eb1b0_0
        keras-preprocessing=1.1.2=pyhd3eb1b0_0
        kiwisolver=1.3.1=py39h2531618_0
        lcms2=2.12=h3be6417_0
        ld_impl_linux-64=2.33.1=h53a641e_7
        libffi=3.3=he6710b0_2
        libgcc-ng=9.1.0=hdf63c60_0
        libgfortran-ng=7.3.0=hdf63c60_0
        libpng=1.6.37=hbc83047_0
        libprotobuf=3.14.0=h8c45485_0
        libsodium=1.0.18=h7b6447c_0
        libstdcxx-ng=9.1.0=hdf63c60_0
        libtiff=4.2.0=h85742a9_0
        libuuid=1.0.3=h1bed415_2
        libwebp-base=1.2.0=h27cfd23_0
        libxcb=1.14=h7b6447c_0
        libxml2=2.9.10=hb55368b_3
        lz4-c=1.9.3=h2531618_0
        markdown=3.3.4=py39h06a4308_0
        markupsafe=2.0.1=py39h27cfd23_0
        matplotlib=3.3.4=py39h06a4308_0
        matplotlib-base=3.3.4=py39h62a2d02_0
        mistune=0.8.4=py39h27cfd23_1000
        mkl=2021.2.0=h06a4308_296
        mkl-service=2.3.0=py39h27cfd23_1
        mkl_fft=1.3.0=py39h42c9631_2
        mkl_random=1.2.1=py39ha9443f7_2
        multidict=5.1.0=py39h27cfd23_2
        nbclassic=0.2.6=pyhd3eb1b0_0
        nbclient=0.5.3=pyhd3eb1b0_0
        nbconvert=6.0.7=py39h06a4308_0
        nbformat=5.1.3=pyhd3eb1b0_0
        ncurses=6.2=he6710b0_1
        nest-asyncio=1.5.1=pyhd3eb1b0_0
        notebook=6.4.0=py39h06a4308_0
        numpy=1.20.2=py39h2d18471_0
        numpy-base=1.20.2=py39hfae3a4d_0
        oauthlib=3.1.0=py_0
        olefile=0.46=py_0
        openssl=1.1.1k=h27cfd23_0
        opt_einsum=3.3.0=pyhd3eb1b0_1
        packaging=20.9=pyhd3eb1b0_0
        pandas=1.2.4=py39h2531618_0
        pandoc=2.12=h06a4308_0
        pandocfilters=1.4.3=py39h06a4308_1
        parso=0.7.0=py_0
        pcre=8.44=he6710b0_0
        pexpect=4.8.0=pyhd3eb1b0_3
        pickleshare=0.7.5=pyhd3eb1b0_1003
        pillow=8.2.0=py39he98fc37_0
        pip=21.1.1=py39h06a4308_0
        prometheus_client=0.10.1=pyhd3eb1b0_0
        prompt-toolkit=3.0.17=pyh06a4308_0
        protobuf=3.14.0=py39h2531618_1
        ptyprocess=0.7.0=pyhd3eb1b0_2
        pyasn1=0.4.8=py_0
        pyasn1-modules=0.2.8=py_0
        pycparser=2.20=py_2
        pygments=2.9.0=pyhd3eb1b0_0
        pyjwt=2.1.0=py39h06a4308_0
        pyopenssl=20.0.1=pyhd3eb1b0_1
        pyparsing=2.4.7=pyhd3eb1b0_0
        pyqt=5.9.2=py39h2531618_6
        pyrsistent=0.17.3=py39h27cfd23_0
        pysocks=1.7.1=py39h06a4308_0
        python=3.9.5=hdb3f193_3
        python-dateutil=2.8.1=pyhd3eb1b0_0
        python-flatbuffers=1.12=pyhd3eb1b0_0
        pytz=2021.1=pyhd3eb1b0_0
        pyzmq=20.0.0=py39h2531618_1
        qt=5.9.7=h5867ecd_1
        readline=8.1=h27cfd23_0
        requests=2.25.1=pyhd3eb1b0_0
        requests-oauthlib=1.3.0=py_0
        rsa=4.7.2=pyhd3eb1b0_1
        scipy=1.6.2=py39had2a1c9_1
        seaborn=0.11.1=pyhd3eb1b0_0
        send2trash=1.5.0=pyhd3eb1b0_1
        setuptools=52.0.0=py39h06a4308_0
        sip=4.19.13=py39h2531618_0
        six=1.15.0=py39h06a4308_0
        sniffio=1.2.0=py39h06a4308_1
        sqlite=3.35.4=hdfb4753_0
        tensorboard=2.4.0=pyhc547734_0
        tensorboard-plugin-wit=1.6.0=py_0
        tensorflow=2.4.1=mkl_py39h4683426_0
        tensorflow-base=2.4.1=mkl_py39h43e0292_0
        tensorflow-estimator=2.5.0=pyh7b7c402_0
        termcolor=1.1.0=py39h06a4308_1
        terminado=0.9.4=py39h06a4308_0
        testpath=0.4.4=pyhd3eb1b0_0
        tk=8.6.10=hbc83047_0
        tornado=6.1=py39h27cfd23_0
        traitlets=5.0.5=pyhd3eb1b0_0
        typing-extensions=3.7.4.3=hd3eb1b0_0
        typing_extensions=3.7.4.3=pyh06a4308_0
        tzdata=2020f=h52ac0ba_0
        urllib3=1.26.4=pyhd3eb1b0_0
        wcwidth=0.2.5=py_0
        webencodings=0.5.1=py39h06a4308_1
        werkzeug=1.0.1=pyhd3eb1b0_0
        wheel=0.36.2=pyhd3eb1b0_0
        wrapt=1.12.1=py39he8ac12f_1
        xz=5.2.5=h7b6447c_0
        yarl=1.6.3=py39h27cfd23_0
        zeromq=4.3.4=h2531618_0
        zipp=3.4.1=pyhd3eb1b0_0
        zlib=1.2.11=h7b6447c_3
        zstd=1.4.9=haebb681_0
        ```
- Python version:
    - 3.9.5
- Bazel version (if compiling from source):
    - N/A
- GCC/Compiler version (if compiling from source):
    - N/A
- CUDA/cuDNN version:
    - After running the command `nvcc  --version`:
        ```
        nvcc: NVIDIA (R) Cuda compiler driver
        Copyright (c) 2005-2020 NVIDIA Corporation
        Built on Mon_Oct_12_20:09:46_PDT_2020
        Cuda compilation tools, release 11.1, V11.1.105
        Build cuda_11.1.TC455_06.29190527_0
        ```
- GPU model and memory:
    - GeForce RTX 3090, 24 GB
    - NVIDIA-SMI: 460.73.01
    - Driver Version: 460.73.01
    - CUDA Version: 11.2

**Describe the current behavior**
When I  execute the following code:
```
tf.autograph.set_verbosity(10)

baseline = Baseline(label_index=column_indices['T (degC)'])

baseline.compile(loss=tf.losses.MeanSquaredError(),
                 metrics=[tf.metrics.MeanAbsoluteError()])

val_performance = {}
performance = {}
val_performance['Baseline'] = baseline.evaluate(single_step_window.val)
performance['Baseline'] = baseline.evaluate(single_step_window.test, verbose=0)
```
I receive the following error messages:
```
INFO:tensorflow:Converted call: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f3b4c2773a0>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>, <tf.Tensor 'args_1:0' shape=(14017,) dtype=int32>)
    kwargs: {}

INFO:tensorflow:Allowlisted: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f3b4c2773a0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function sequences_from_indices.<locals>.<lambda> at 0x7f3b4b9509d0>
    args: (<tf.Tensor 'args_0:0' shape=(14018, 19) dtype=float32>, <tf.Tensor 'args_1:0' shape=(None,) dtype=int32>)
    kwargs: {}

INFO:tensorflow:Allowlisted: <function sequences_from_indices.<locals>.<lambda> at 0x7f3b4b9509d0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method WindowGenerator.split_window of Total window size: 2
Input indices: [0]
Label indices: [1]
Label column name(s): ['T (degC)']>
    args: (<tf.Tensor 'args_0:0' shape=(None, None, 19) dtype=float32>,)
    kwargs: {}

INFO:tensorflow:Allowlisted <bound method WindowGenerator.split_window of Total window size: 2
Input indices: [0]
Label indices: [1]
Label column name(s): ['T (degC)']>: from cache
INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.test_function at 0x7f3b4b950280>
    args: (<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3b4b954df0>,)
    kwargs: {}

INFO:tensorflow:<function Model.make_test_function.<locals>.test_function at 0x7f3b4b950280> is not cached for subkey ConversionOptions[{}]
INFO:tensorflow:Source code of <function Model.make_test_function.<locals>.test_function at 0x7f3b4b950280>:

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
def test_function(iterator):
  """"""Runs an evaluation execution with one step.""""""
  return step_function(self, iterator)


INFO:tensorflow:Transformed <function Model.make_test_function.<locals>.test_function at 0x7f3b4b950280>:

# coding=utf-8
def tf__test_function(iterator):
    'Runs an evaluation execution with one step.'
    with ag__.FunctionScope('test_function', 'fscope', ag__.ConversionOptions(recursive=True, user_requested=True, optional_features=(), internal_convert_user_code=True)) as fscope:
        do_return = False
        retval_ = ag__.UndefinedReturnValue()
        try:
            do_return = True
            retval_ = ag__.converted_call(ag__.ld(step_function), (ag__.ld(self), ag__.ld(iterator)), None, fscope)
        except:
            do_return = False
            raise
        return fscope.ret(retval_, do_return)

INFO:tensorflow:Defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__test_function at 0x7f3b4b7f2310> : None
INFO:tensorflow:KW defaults of <function outer_factory.<locals>.inner_factory.<locals>.tf__test_function at 0x7f3b4b7f2310> : None
INFO:tensorflow:Calling <function outer_factory.<locals>.inner_factory.<locals>.tf__test_function at 0x7f3b4b7f2310> with
    iterator: <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3b4b954df0>

INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.step_function at 0x7f3b4b91bca0>
    args: (<__main__.Baseline object at 0x7f3b4b8e75e0>, <tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3b4b954df0>)
    kwargs: None

INFO:tensorflow:Allowlisted: <function Model.make_test_function.<locals>.step_function at 0x7f3b4b91bca0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function Model.make_test_function.<locals>.step_function.<locals>.run_step at 0x7f3b4b7f2ca0>
    args: ((<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 19) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 1, 1) dtype=float32>),)
    kwargs: {}

INFO:tensorflow:Allowlisted: <function Model.make_test_function.<locals>.step_function.<locals>.run_step at 0x7f3b4b7f2ca0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method Baseline.call of <__main__.Baseline object at 0x7f3b4b8e75e0>>
    args: (<tf.Tensor 'IteratorGetNext:0' shape=(None, 1, 19) dtype=float32>,)
    kwargs: {}

INFO:tensorflow:Not allowed: <method-wrapper '__call__' of method object at 0x7f3b4b932a80>: default rule
INFO:tensorflow:Not allowed: <class '__main__.Baseline'>: default rule
INFO:tensorflow:Not allowed: <bound method Baseline.call of <__main__.Baseline object at 0x7f3b4b8e75e0>>: default rule
INFO:tensorflow:<bound method Baseline.call of <__main__.Baseline object at 0x7f3b4b8e75e0>> is not cached for subkey ConversionOptions[{}]
INFO:tensorflow:Source code of <bound method Baseline.call of <__main__.Baseline object at 0x7f3b4b8e75e0>>:

def call(self, inputs):
    if self.label_index is None:
        return inputs
    result = inputs[:, :, self.label_index]
    return result[:, :, tf.newaxis]


INFO:tensorflow:Error transforming entity <bound method Baseline.call of <__main__.Baseline object at 0x7f3b4b8e75e0>>
Traceback (most recent call last):
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 447, in converted_call
    converted_f = _convert_actual(target_entity, program_ctx)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 284, in _convert_actual
    transformed, module, source_map = _TRANSPILER.transform(entity, program_ctx)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 286, in transform
    return self.transform_function(obj, user_context)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 470, in transform_function
    nodes, ctx = super(PyToPy, self).transform_function(fn, user_context)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/transpiler.py"", line 363, in transform_function
    result = self.transform_ast(node, context)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 252, in transform_ast
    node = self.initial_analysis(node, ctx)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py"", line 239, in initial_analysis
    node = qual_names.resolve(node)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 252, in resolve
    return QnResolver().visit(node)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/ast.py"", line 483, in generic_visit
    value = self.visit(value)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/ast.py"", line 492, in generic_visit
    new_node = self.visit(old_value)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/ast.py"", line 407, in visit
    return visitor(node)
  File ""/home/mark/anaconda3/envs/tf2/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/qual_names.py"", line 232, in visit_Subscript
    if not isinstance(s, gast.Index):
AttributeError: module 'gast' has no attribute 'Index'
WARNING:tensorflow:AutoGraph could not transform <bound method Baseline.call of <__main__.Baseline object at 0x7f3b4b8e75e0>> and will run it as-is.
Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.
Cause: module 'gast' has no attribute 'Index'
To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert
INFO:tensorflow:Converted call: <bound method LossFunctionWrapper.call of <tensorflow.python.keras.losses.MeanSquaredError object at 0x7f3b4b30ec10>>
    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 1, 1) dtype=float32>, <tf.Tensor 'baseline/strided_slice_1:0' shape=(None, 1, 1) dtype=float32>)
    kwargs: {}

INFO:tensorflow:Allowlisted: <bound method LossFunctionWrapper.call of <tensorflow.python.keras.losses.MeanSquaredError object at 0x7f3b4b30ec10>>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method Reduce.update_state of <tensorflow.python.keras.metrics.Mean object at 0x7f3b4b89cb80>>
    args: (<tf.Tensor 'mean_squared_error/weighted_loss/value:0' shape=() dtype=float32>,)
    kwargs: {'sample_weight': <tf.Tensor 'strided_slice:0' shape=() dtype=int32>}

INFO:tensorflow:Allowlisted: <bound method Reduce.update_state of <tensorflow.python.keras.metrics.Mean object at 0x7f3b4b89cb80>>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method MeanMetricWrapper.update_state of <tensorflow.python.keras.metrics.MeanAbsoluteError object at 0x7f3b4b89c460>>
    args: (<tf.Tensor 'IteratorGetNext:1' shape=(None, 1, 1) dtype=float32>, <tf.Tensor 'baseline/strided_slice_1:0' shape=(None, 1, 1) dtype=float32>)
    kwargs: {'sample_weight': None}

INFO:tensorflow:Allowlisted: <bound method MeanMetricWrapper.update_state of <tensorflow.python.keras.metrics.MeanAbsoluteError object at 0x7f3b4b89c460>>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method Reduce.result of <tensorflow.python.keras.metrics.Mean object at 0x7f3b4b89cb80>>
    args: ()
    kwargs: {}

INFO:tensorflow:Allowlisted: <bound method Reduce.result of <tensorflow.python.keras.metrics.Mean object at 0x7f3b4b89cb80>>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method Reduce.result of <tensorflow.python.keras.metrics.MeanAbsoluteError object at 0x7f3b4b89c460>>
    args: ()
    kwargs: {}

INFO:tensorflow:Allowlisted <bound method Reduce.result of <tensorflow.python.keras.metrics.MeanAbsoluteError object at 0x7f3b4b89c460>>: from cache
439/439 [==============================] - 0s 514us/step - loss: 0.0135 - mean_absolute_error: 0.0806
INFO:tensorflow:Converted call: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f3b4b9500d0>
    args: (<tf.Tensor 'args_0:0' shape=() dtype=int64>, <tf.Tensor 'args_1:0' shape=(7009,) dtype=int32>)
    kwargs: {}

INFO:tensorflow:Allowlisted: <function timeseries_dataset_from_array.<locals>.<lambda> at 0x7f3b4b9500d0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <function sequences_from_indices.<locals>.<lambda> at 0x7f3b4b3f88b0>
    args: (<tf.Tensor 'args_0:0' shape=(7010, 19) dtype=float32>, <tf.Tensor 'args_1:0' shape=(None,) dtype=int32>)
    kwargs: {}

INFO:tensorflow:Allowlisted: <function sequences_from_indices.<locals>.<lambda> at 0x7f3b4b3f88b0>: DoNotConvert rule for tensorflow
INFO:tensorflow:Converted call: <bound method WindowGenerator.split_window of Total window size: 2
Input indices: [0]
Label indices: [1]
Label column name(s): ['T (degC)']>
    args: (<tf.Tensor 'args_0:0' shape=(None, None, 19) dtype=float32>,)
    kwargs: {}

INFO:tensorflow:Allowlisted <bound method WindowGenerator.split_window of Total window size: 2
Input indices: [0]
Label indices: [1]
Label column name(s): ['T (degC)']>: from cache
```

**Describe the expected behavior**
I expect to receive zero error messages.

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you want to contribute a PR? (yes/no): - Briefly describe your candidate solution (if contributing):
No.

**Standalone code to reproduce the issue**
All of the code is available in the TensorFlow tutorial at: https://www.tensorflow.org/tutorials/structured_data/time_series
The Anaconda environment may also be created in the ""TensorFlow version"" section above.
"
49961,tflite esrgan super image resolution limited 50x50,"how to train super image resolution, that accept any input image"
49956,Wrong hostnames in SlurmClusterResolver from expand_range_expression,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): CentOS Linux 7 (Core)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): v2.5.0-rc3-213-ga4dfb8d1a71 2.5.0
- Python version: 3.8.10
- CUDA/cuDNN version: 11.2
- GPU model and memory: Tesla V100 and 32GB


**Describe the current behavior**
We have a Slurm cluster and I wanted to train with a multi-node setup. For the cluster_resolver I used tf.distribute.cluster_resolver.SlurmClusterResolver (without any configuration) and let this class fetch the information from the Slurm environment variables. Now ""expand_hostlist(hostlist)"" tries to get a list of hostnames, but depending on the naming convention of the hosts this leads to the following problem:
If the input names have a leading '0' like 'n[009-011]' in the string, it will be converted to ['n9', 'n10', 'n11'].

The ultimate conversion problem occurs in expand_range_expression(range_exp), where the range string is converted to an int():
```
for i in range(int(sub_range[0]), int(sub_range[1]) + 1):
        yield i
```
This will remove the leading 0’s.

**Describe the expected behavior**
The correct conversion would be [‘n009’, ‘n010’, ‘n011’]


**[Contributing](https://www.tensorflow.org/community/contribute)** 
- Do you want to contribute a PR? (yes/no): yes
- Briefly describe your candidate solution (if contributing):
```
string_length = len(sub_range[0])
for i in range(int(sub_range[0]), int(sub_range[1]) + 1):
        yield str(i).zfill(string_length)
```

**Standalone code to reproduce the issue**
I remove all the slurm stuff, because that is not necessary to show the problem. 
[colab example](https://colab.research.google.com/drive/1cH6UKVs-fS3QlRWONjItF432f_CO4uM-?usp=sharing)

"
49950,Tensorflow Decove Wav,"Please, help me with tf.audio.decode_wav:https://www.tensorflow.org/api_docs/python/tf/audio/decode_wav
(The -32768 to 32767 signed 16-bit values will be scaled to -1.0 to 1.0 in  float tensor.)

I want to read 16bit PCM wav file as 8 bit PCM (The -128 to 127 and scaled to -1.0 to 1.0 in  float tensor)

What should i do?"
49949,need convert opencv to TfLiteTensorCopyFromBuffer,"https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.cpp

why so many python or java references.
while C++ is very little.

I'm angry because why developers don't provide compiled source code, why should we do the compilation ourselves.

at this line 108 [https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.cpp#L108](url) 
how to convert cv::mat to TfLiteTensorCopyFromBuffer

and then at this line 137 [https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.cpp#L137](url)
how to convert TfLiteTensorCopyToBuffer to cv::mat

i hope there person, will help,
because tensorflow reference on this link [https://github.com/tensorflow/examples/blob/master/lite/examples/super_resolution/android/app/src/main/cc/SuperResolution.cpp](url)
is not structured. Thanks"
49948,AttributeError: module 'tensorflow._api.v2.nn' has no attribute 'seq2seq',
49947,"Try to use TensorflowLite (API C++) NNAPI acceleration, meet errors.","**BUILD .SO System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):   Ubuntu 16.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: MI 9 (android 9, Snapdragon855)
- TensorFlow installed from (source or binary):   source  (branch in * tf25   15d5b93 [origin/r2.5] Merge pull request #49270 from angerson/r2.5)
- TensorFlow version: 2.5
- Python version: 3.7.3
- Installed using virtualenv? pip? conda?: conda 
- Bazel version (if compiling from source):  3.7.2
- GCC/Compiler version (if compiling from source):  7.3.0
- CUDA/cuDNN version: NVIDIA-SMI 410.129      Driver Version: 410.129      CUDA Version: 10.0  CUDANN Version: 7.3.1
- GPU model and memory:  Tesla K40m 11441MiB
- cmake version: 3.20.0
- NDK version : 21b (NDK API level us 21)
- SDK API level  use version 28 
**BUILD .apk System information**
- Windows 10 10.0
- Android build tools use versions: 28.0.3
- Android Studio 4.2.1
- compile sdk version: 28
- NDK version： 21.1.6352462

 
**Describe the problem**
Briefly：
 ubuntu build libnnapi_delegate.so ->  build .apk in windows10 Android Studio ->meet linkerror：java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol referenced by so.
verbosely： 
I refer to this super_resolution [example ](https://github.com/tensorflow/examples/tree/master/lite/examples/super_resolution/android) try to run the AI model on the mobile DSP，I build nnapi delegate  .so like this ：
####
bazel build -c opt --repository_cache=/home/wjl/dockerworkspace/tensorflow_16ubuntu/2cache --distdir=/home/wjl/dockerworkspace/tensorflow_16ubuntu/2cache --config=android_arm64  --cxxopt=""-std=c++14""       //tensorflow/lite/delegates/nnapi:nnapi_delegate
####
then I get ：
bazel-out/arm64-v8a-opt/bin/tensorflow/lite/delegates/nnapi/libnnapi_delegate.so (about  1.6M).
I add so and header files to the super_resolution demo.
No errors or warnings were reported when compiling the apk. But at runtime, logcat reports an error: jProcess: org.tensorflow.lite.examples.superresolution, PID: 15939
    java.lang.UnsatisfiedLinkError: dlopen failed: cannot locate symbol ""_ZN6tflite16logging_internal13MinimalLogger3LogENS_11LogSeverityEPKcz"" referenced by ""/data/app/org.tensorflow.lite.examples.superresolution-50dhWLUyWoZrFw8sIcvg5w==/lib/arm64/libnnapi_delegate_strip.so""...
        at java.lang.Runtime.loadLibrary0(Runtime.java:1016)
        at java.lang.System.loadLibrary(System.java:1669)
        at org.tensorflow.lite.examples.superresolution.MainActivity.<clinit>(MainActivity.java:47)
        at java.lang.Class.newInstance(Native Method)  ...

1. How to solve the missing symbol of so？
2. I add the following code to the SR demo to call nnapi,  I have to use c++ interface and NDK .Is it correct to call nnapi like this? My apk hasn't run yet.
""""""""""""""add this """"""""""""
       #include ""tensorflow/lite/delegates/nnapi/nnapi_delegate.h"" 
       ...
      // reference link: ​https://github.com/lackhole/CuteModel/blob/master/CuteModel.cpp
     nnapiOptions = tflite::StatefulNnApiDelegate::Options();  // nnapi
      nnApiDelegate_ =  new tflite::StatefulNnApiDelegate(nnapiOptions);
      TfLiteInterpreterOptionsAddDelegate(options_, nnApiDelegate_);

"
49946,Wrong gradient from complex determinant,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS Big Sur, Version: 11.2.3 (20D91)
- TensorFlow installed from (source or binary): pip
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7

**Describe the current behavior**
I implemented a straightforward example which illustrates the issue:
I have a 2-dimensional tensor which is mapped to a complex tensor as:

x_i -> z_i = (x_i, lamb_i * x_i)

with some real values for lambda = [lambda_1, lambda_2].
Afterwards I calculate the Jacobian (with respect to x) and its determinant, which is
also easy to do as it is just a 2x2 matrix.
In the end, I take |det|^2 as final output L.

Analytically, you would now get for the gradient:
grad L = [ 2 * lambda_1 * (1+lambda_2^2), 2 * lambda_2 * (1+lambda_1^2)

So if you insert lambda_test = [ 1.0 , 2.0]
You should obtain: grad L(lambda_test) = [ 10, 8]

However, Tensorflow yields: TF-Grad = [-14, -8.8],
which obviously is completely off. 

Additionally, we also implemented a simple numerical derivative ourselves (just the basic definition of the gradient in terms of difference quotient). This calculation does yield the correct gradient (within numerical uncertainties)

**Standalone code to reproduce the issue**
The issue can be reproduced in this [gist](https://colab.research.google.com/drive/1C8PQKBWS-ykraftMqVq6MWmOjcmdQbhH?usp=sharing).

"
49945,"TFLM project generation fail, missing gemmlowp ","@tensorflow/micro

**System information**
Ubuntu 20.04
- TensorFlow installed from (source or binary): source
- Tensorflow version (commit SHA if source): f19bfff186754e621d59a65494b04ff9ab4e9ede
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.):cortex-m4

**Describe the problem**

Using project genration failed, the log shows that the gemmlowp is missing.  Maybe there's a missing gemlowp download script in the make folder?

**Please provide the exact sequence of commands/steps when you ran into the problem**
STEP1: 
python tensorflow/lite/micro/tools/project_generation/create_tflm_tree.py --makefile_options=""OPTIMIZED_KERNEL_DIR=cmsis_nn TARGET_ARCH=cortex-m4"" ""project""

OUTPUT:
Traceback (most recent call last):
  File ""tensorflow/lite/micro/tools/project_generation/create_tflm_tree.py"", line 193, in <module>
    _copy(src_files, dest_files)
  File ""tensorflow/lite/micro/tools/project_generation/create_tflm_tree.py"", line 105, in _copy
    shutil.copy(src, dst)
  File ""/home/yc/anaconda3/lib/python3.8/shutil.py"", line 418, in copy
    copyfile(src, dst, follow_symlinks=follow_symlinks)
  File ""/home/yc/anaconda3/lib/python3.8/shutil.py"", line 264, in copyfile
    with open(src, 'rb') as fsrc, open(dst, 'wb') as fdst:
FileNotFoundError: [Errno 2] No such file or directory: 'tensorflow/lite/micro/tools/make/downloads/gemmlowp/fixedpoint/fixedpoint.h'"
49944,QuantizedOpsTest.testAxis fails on cascade lake CPUs,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): no 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux RHEL 8.3
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.6
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): GCC 10.2.0
- CUDA/cuDNN version: None

**Describe the current behavior**

QuantizedOpsTest.testAxis fails on cascade lake systems when native optimizations are enabled

**Standalone code to reproduce the issue**

Run the TF test `//tensorflow/python:quantized_ops_test` through bazel

**Other info / logs**
```
FAIL: testAxis (__main__.QuantizedOpsTest)
QuantizedOpsTest.testAxis
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/ops/quantized_ops_test.py"", line 94, in testAxis
    self.assertAllEqual(quantized, expected_quantized)
  File ""/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 1253, in decorated
    return f(*args, **kwds)
  File ""/dev/shm/build-branfosj-admin/branfosj-admin-up/TensorFlow/2.5.0/foss-2020b/tmp8qildda2-bazel-tf/1b512851602cc5932dcb1cd30c5fbde4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/python/quantized_ops_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py"", line 2881, in assertAllEqual
    np.testing.assert_array_equal(a, b, err_msg=""\n"".join(msgs))
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/SciPy-bundle/2020.11-foss-2020b/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 930, in assert_array_equal
    assert_array_compare(operator.__eq__, x, y, err_msg=err_msg,
  File ""/rds/bear-apps/devel/eb-sjb-up/EL8/EL8-cas/software/SciPy-bundle/2020.11-foss-2020b/lib/python3.8/site-packages/numpy/testing/_private/utils.py"", line 840, in assert_array_compare
    raise AssertionError(msg)
AssertionError: 
Arrays are not equal

not equal where = (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1,
       1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 1, 1, 1,
       1, 1, 2, 2, 2, 2, 2, 3, 0, 0, 0, 0]), array([0, 1, 2, 3, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0,
       1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2,
       3, 4, 0, 1, 2, 3, 4, 0, 0, 1, 2, 3]))
not equal lhs = array([ -64,    0,   38,  102,   38,  102,   71,   64,   64, -128,  -64,
          0,  102,   71,   64, -128, -128,  -64,    0,   38,   64, -128,
        -64,    0,    0,   38,  102,   71,    0,   38,  102,   71,   71,
         64, -128,  -64,  102,   71,   64, -128, -128,  -64,    0,   38,
         71,   64, -128,  -64,  -64,    0,   38,  102,   38,  102,   71,
         64], dtype=int8)
not equal rhs = array([-128,  -64,    0,   38,  -64,    0,   38,  102,   71,   64, -128,
        -64,    0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,
         71,   64, -128,  -64,    0,   38,  102,   71,   64, -128,  -64,
          0,   38,  102,   71,   64, -128,  -64,    0,   38,  102,   71,
         64, -128,  -64,    0,   38,  102,   71,   64,  102,   71,   64,
       -128], dtype=int32)
Mismatched elements: 56 / 120 (46.7%)
Max absolute difference: 230
Max relative difference: 4.36842105
 x: array([[[[ -64,    0,   38,  102,  102],
         [  71,   64, -128,   38,  102],
         [  71,   64,   64, -128,  -64],...
 y: array([[[[-128,  -64,    0,   38,  102],
         [  71,   64, -128,  -64,    0],
         [  38,  102,   71,   64, -128],...
```

This seems to be related to https://github.com/tensorflow/tensorflow/issues/47179 which can also only be observed on cascade lake systems."
49943,No tensorflow.lite.tools.evaluation.proto files inside tf 2.5.0 after bazel build,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
- Python version:
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory:

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
49941,Modify tf.math.reduce_std so that it is compatible with ragged tensors,"Same issue as `reduce_variance` addressed in #37000 for `reduce_std`
"
49940,tf.keras.models.load_model does not load optimizer's weigths,"**System information**
Tensorflow: 2.5.0

**Describe current behaviour**
When using `tf.keras.models.load_model()` on a savedmodel containing a keras model saved using `tf.keras.Model.save(include_optimizer=True)`, the optimizer's weights are NOT loaded

**Describe expected behaviour**
The reconstructed model should contain the optimizer's weights to be able to resume training.

**Standalone code to reproduce issue**
https://colab.research.google.com/drive/1Y0y-pvE2QDw1Wj68XnbpLaUggXAXyzcx?usp=sharing
"
49939,Cannot convert a Tensor of dtype resource to a NumPy array,"Having issue in converting autokeras functional model to onnx 
Using tensorflow == 2.3.1 keras == 2.4.3
```
from autokeras import StructuredDataClassifier
model = StructuredDataClassifier(max_trials=100)
model.fit(x=X_train, y=y_train, validation_data=(X_valid, y_valid), epochs=1000, verbose=1)
autoKeras_model = model.export_model()
autoKeras_model.summary()

Model: ""functional_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 18)]              0         
_________________________________________________________________
multi_category_encoding (Mul (None, 18)                0         
_________________________________________________________________
dense (Dense)                (None, 32)                608       
_________________________________________________________________
re_lu (ReLU)                 (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                1056      
_________________________________________________________________
re_lu_1 (ReLU)               (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 33        
_________________________________________________________________
classification_head_2 (Activ (None, 1)                 0         
=================================================================
Total params: 1,697
Trainable params: 1,697
Non-trainable params: 0
_________________________________________________________________
```
converting to onnx
```
import onnxruntime
import keras2onnx
onnx_model = keras2onnx.convert_keras(autoKeras_model, ""autokeras"", debug_mode=1)
```
I get error
```
tf executing eager_mode: True
INFO:keras2onnx:tf executing eager_mode: True
tf.keras model eager_mode: False
INFO:keras2onnx:tf.keras model eager_mode: False
Model: ""functional_1""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 18)]              0         
_________________________________________________________________
multi_category_encoding (Mul (None, 18)                0         
_________________________________________________________________
dense (Dense)                (None, 32)                608       
_________________________________________________________________
re_lu (ReLU)                 (None, 32)                0         
_________________________________________________________________
dense_1 (Dense)              (None, 32)                1056      
_________________________________________________________________
re_lu_1 (ReLU)               (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 33        
_________________________________________________________________
classification_head_2 (Activ (None, 1)                 0         
=================================================================
Total params: 1,697
Trainable params: 1,697
Non-trainable params: 0
_________________________________________________________________
None
Traceback (most recent call last):

  File ""<ipython-input-76-0567e6de6858>"", line 1, in <module>
    onnx_model = keras2onnx.convert_keras(ExportedautoKeras_model, model_name, debug_mode=1)

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\keras2onnx\main.py"", line 62, in convert_keras
    tf_graph = build_layer_output_from_model(model, output_dict, input_names, output_names)

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\keras2onnx\_parser_tf.py"", line 302, in build_layer_output_from_model
    return extract_outputs_from_subclassing_model(model, output_dict, input_names, output_names)

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\keras2onnx\_parser_tf.py"", line 264, in extract_outputs_from_subclassing_model
    concrete_func, lower_control_flow=True)

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\keras2onnx\_graph_cvt.py"", line 437, in convert_variables_to_constants_v2
    tensor_data = _get_tensor_data(func)

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\keras2onnx\_graph_cvt.py"", line 209, in _get_tensor_data
    data = val_tensor.numpy()

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1063, in numpy
    maybe_arr = self._numpy()  # pylint: disable=protected-access

  File ""C:\Users\Pe\Anaconda3\lib\site-packages\tensorflow\python\framework\ops.py"", line 1031, in _numpy
    six.raise_from(core._status_to_exception(e.code, e.message), None)  # pylint: disable=protected-access

  File ""<string>"", line 3, in raise_from

InvalidArgumentError: Cannot convert a Tensor of dtype resource to a NumPy array.
```
tf life also gives same error
```
import tensorflow as tf
converter = tf.lite.TFLiteConverter.from_keras_model(autoKeras_model)
tflite_model = converter.convert()
```
Any help much appreciated, Thanks
"
49938,Want to save this training weight by epoch,"https://github.com/lancerane/Adversarial-domain-adaptation/blob/master/Domain%20adaptation,%20TF2.0.ipynb

Thanks Oscar Bennett provide this code, but i don't know how to save this weight then test another data.
And if can train supervise val loss  to learn avoid overfitting.
Because this model DANN have three part and i can't use keras save weight, Can u give me some direction?
---------------
EPOCHS = 100

alpha = 1
for epoch in range(EPOCHS):
  reset_metrics()
  
  for domain_data, label_data in zip(domain_train_ds, train_ds):
    
    try:
      train_step(label_data[0], label_data[1], domain_data[0], domain_data[1], alpha=alpha)
     
    #End of the smaller dataset
    except ValueError: 
      pass
    
  for test_data, m_test_data in zip(test_ds,mnist_m_test_ds):
    test_step(test_data[0], test_data[1], m_test_data[0], m_test_data[1])
  
  template = 'Epoch {}, Train Accuracy: {}, Domain Accuracy: {}, Source Test Accuracy: {}, Target Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_accuracy.result()*100,
                         conf_train_accuracy.result()*100,
                         test_accuracy.result()*100,
                         m_test_accuracy.result()*100,))
-----------------------------"
49937,outputs  from tensor flow lite doesn't meet python script to generate output from any layer if i train the model ,"https://i.stack.imgur.com/qZ2rl.png

image from ""Quantization and Training of Neural Networks for Efficient
Integer-Arithmetic-Only Inference""

Hello , i am trying to catch the error which is simply i take input ,weight ,zeros , scales then add bias to the result and apply this equation in first image 
using python comparing the result with output from tensor flow lite 
but a weird thing happen which is if i untrained model just initialize it with random values the two output from python and tensor flow lite meet but if i train model and do the same thing i didn't get the same output. 
in details : to get output from point wise layer (conv2d ) 
i extract weights,zeros,inputs,scale from tensor flow lite 
and apply the equation in image get same result if i initialize model with random values 
in moment that  i train model the result is different , note that i extract the new parameters out for sure 
i  implemented moblie net and using normal relu not relu 6 is this can make problem ?
i upload my neutron model :
https://drive.google.com/file/d/1ENVZCf0hSOiIIh8-0X_-7I23sv3Az2_2/view?usp=sharing"
49936,[RNN] Tflite's fused LSTM does not support masking,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7
- TensorFlow installation (pip package or built from source): tensorflow-nightly

### 2. Code

```
        model = LSTM_model
	run_model = tf.function(lambda x: model(x))
	## This is important, let's fix the input size.
	BATCH_SIZE = 1
	INPUT_LEN = 150
	INPUT_SIZE = 9
	concrete_func = run_model.get_concrete_function(
	    [tf.TensorSpec([BATCH_SIZE, INPUT_LEN, INPUT_SIZE], ""float32"")])
	MODEL_DIR = ""lstm""
	model.save(MODEL_DIR, save_format=""tf"", signatures=concrete_func)
	# Convert the model.
	converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_DIR)
        #converter = tf.lite.TFLiteConverter.from_keras_model(model)
	tflite_model = converter.convert()
```

### 3. Failure after conversion

   I find that the performance of LSTM with masking decreases heavily if I convert it into fused version by tf.function (The Tflite model work OK if I directly convert it with function ""from_keras_model"". And I find that the masking seems not to work in the fused version.
"
49935,Pass epoch number as arg to `tf.data.Dataset.from_generator`,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 2.3
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**: I am trying to implement a tensorflow dataset which uses a generator underneath yielding batches following a linearly increasing batch size scheme. So, the first epoch it could yield batches of size `x`, the next epoch it would yield batches of size `x+c`. `c` is dependent on the current epoch number. I am defining my tensorflow dataset as:

```
def gen(epoch_number):
    yield epoch_number # Just representative. In reality this will be some function of epoch_number and data.

dataset = tf.data.Dataset.from_generator(gen, output_types=(tf.int32), args=[x])

# A custom keras model derived from tf.keras.Model
model = SomeModel()

model.fit(dataset)
```

I am trying to figure out what to pass as `x` in the above code so that the generator has information about the current epoch number when the generator is called.
As far as I know, there is no instance attribute of `tf.keras.Model` which could be used as `x` so that it's evaluated at runtime and passed to the generator.

Is it possible to do so?
I'd like to avoid overriding the `fit` or `train_step` method of the model.

**Will this change the current api? How?**:  I don't think so.

**Who will benefit with this feature?**: Batch yielding from dataset can use training stages and depend on them, for example number of epochs passed in the above example.

"
49934,need a custom implementation: BroadcastTo.,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows 10, x86-64
- TensorFlow installed from (source or binary): source
- TensorFlow version (or github SHA if from source): 1.15


**Provide the text output from tflite_convert**

```

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: ADD, CONCATENATION, CONV_2D, DIV, LEAKY_RELU, MAX_POOL_2D, MUL, SUB, TANH, TRANSPOSE_CONV. Here is a list of operators for which you will need custom implementations: BroadcastTo.
```

**Standalone code to reproduce the issue** 
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Also, please include a link to a GraphDef or the model if possible.

**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem.
If including tracebacks, please include the full traceback. Large logs and files
should be attached.
Traceback (most recent call last):
  File ""c:\users\anaconda3\envs\pytorch1\lib\runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""c:\users\anaconda3\envs\pytorch1\lib\runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""C:\Users\Anaconda3\envs\pytorch1\Scripts\toco_from_protos.exe\__main__.py"", line 7, in <module>
  File ""c:\users\anaconda3\envs\pytorch1\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""c:\users\anaconda3\envs\pytorch1\lib\site-packages\tensorflow_core\python\platform\app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""C:\Users\AppData\Roaming\Python\Python37\site-packages\absl\app.py"", line 303, in run
    _run_main(main, args)
  File ""C:\Users\AppData\Roaming\Python\Python37\site-packages\absl\app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""c:\users\anaconda3\envs\pytorch1\lib\site-packages\tensorflow_core\lite\toco\python\toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)"
49933,DepthwiseConv2D documentation: a different filter per channel vs the same filter per channel,"The documentation for [tf.keras.layers.DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D) describes this operation as: split input into individual channels, convolve each with the layer's kernel and finally stack the results. This gives the impression the same kernel is applied to all channels. But I believe the actual implementation applies a different kernel per channel.

Relevant code sections
- Backend implementation: https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/ops/nn_impl.py#L760-L766
- Keras layer: https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/convolutional.py#L2260-L2269

Also see [MobileNet Paper](https://arxiv.org/pdf/1704.04861.pdf) page 3 formula 3"
49931,Model converted to TFlite performs slow during inference,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Source
- TensorFlow version (use command below): 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: No

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Tensorflow saved model converted to TFlite model gives poor performance during inference. The inference time is same as (or greater than in some cases) tensorflow model

**Describe the expected behavior**
Inference time should be low according to the documentation

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing): Yes

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
https://colab.research.google.com/drive/16IAyeNAIdfgRrS1CLhOqBrAc0bvFNt5A?usp=sharing"
49930,Incorrect variance in normalization layer of EfficientNet,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04.5 LTS (Google Colab)
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: n/a
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v2.5.0-0-ga4dfb8d1a71 2.5.0
- Python version: 3.7.10
- Bazel version (if compiling from source): n/a
- GCC/Compiler version (if compiling from source): n/a
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a

**Describe the current behavior**

EfficientNet includes [a normalization layer within its model definition](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/applications/efficientnet.py#L321), but it seems like the variance is incorrect. The variance is `[0.229, 0.224, 0.225]`, but [those values are the standard deviations of the ImageNet dataset](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/applications/imagenet_utils.py#L196). When the normalization layer is called on new inputs, the inputs are normalized using the mean (which looks correct) and the square root of the variance. So in the current EfficientNet implementation, inputs are normalized using the square root of the standard deviation of ImageNet.

**Describe the expected behavior**

I expect inputs to EfficientNet to be normalized according to the standard deviation of the ImageNet dataset.

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): no, because I don't know where I would make this change. The change would have to be within the saved models.

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

```python
import numpy as np
import tensorflow as tf

model = tf.keras.applications.EfficientNetB0(weights=""imagenet"")
norm_layer = model.layers[2]
assert ""normalization"" in norm_layer.name
print(norm_layer.mean.numpy())  # [0.485 0.456 0.406]
print(norm_layer.variance.numpy())  # [0.229 0.224 0.225]
# Generate sample inputs.
tf.random.set_seed(42)
x = tf.random.uniform((1, 224, 224, 3), 0, 255, dtype=""int32"", seed=42)


def get_reference(inputs):
    """"""Get the reference normalized outputs.""""""
    x = np.asarray(inputs).astype(""float32"")
    x /= 255.0
    x[..., 0] -= 0.485
    x[..., 1] -= 0.456
    x[..., 2] -= 0.406
    x[..., 0] /= 0.229
    x[..., 1] /= 0.224
    x[..., 2] /= 0.225
    return x


def get_current_tf_efficientnet_norm_output(inputs):
    """"""Get the normalized outputs from the current implementation.""""""
    x = np.asarray(inputs).astype(""float32"")
    x /= 255.0
    x[..., 0] -= 0.485
    x[..., 1] -= 0.456
    x[..., 2] -= 0.406
    x[..., 0] /= np.sqrt(0.229)
    x[..., 1] /= np.sqrt(0.224)
    x[..., 2] /= np.sqrt(0.225)
    return x


model_normalizer = tf.keras.Model(model.input, norm_layer.output)
# Below is True (they are the same)
np.allclose(get_current_tf_efficientnet_norm_output(x), model_normalizer(x), atol=1e-07)
# Below is False (they are different)
np.allclose(get_reference(x), model_normalizer(x), atol=1e-07)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

The [Normalization layer normalizes using the square root of the variance](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/layers/preprocessing/normalization.py#L242-L243) (which equal to the standard deviation) "
49929,Elu int8 quantization doesn't reduce the size needed by the model on Arduino,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution : Mac OS 10.14.8 
- TensorFlow installed from source
- Tensorflow version : 2.5.0
- Target platform : Arduino Nano 33

**Describe the problem**
Context :
I would like to run inferencing of a DL-model on an Arduino and, since I don't have much memory available, I need to int8-quantize my model.
But the quantization of my model doesn't seem to be working, and it seems to be linked to the Elu activations functions in the model.
Indeed, I get no error during both the conversion and the quantization of the model on Python and the inferencing on Arduino, but the necessary size for the model on the Arduino remains the same than without quantization.

What I tried :
- I retrained a model in which I changed the Elu for Relu activation functions. Then quantization works : thanks to the line : tflInterpreter->arena_used_bytes() on Arduino, I can see that quantization helped me to reduce the necessary size for the model by 3.
- I analysed the model (quantized, with Elu) on the Netron App and I realised that there are steps of de-quantization and re-quantization before and after each call of Elu function : model de-quantize and re-quantize. I don't understand why is this doing so, when it doesn't append with Relu functions: <img width=""582"" alt=""Screenshot 2021-05-31 at 15 42 59"" src=""https://user-images.githubusercontent.com/78730433/120202463-e5824e00-c226-11eb-8ac6-fe37c5d5911e.png"">
- Finally, I found this commit on Tensorflow Git, which made me believe that int8 quantization for Elu is implemented : [commit Elu quant](https://github.com/tensorflow/tensorflow/commit/918f876bf812fd744151fea29b2df4aa18acfa8f) Nevertheless, they mentioned a LUT approach, which I don't understand and might (?) be linked to the troubles I am facing.

Does anyone face the same king of troubles for quantization of model containing Elu ? Do you have any idea of how to solve this problem ?

Thank you really much !"
49928,"Tensorflow1.10 CPU Exceed 10% of system memory, Segmentation fault ","**I would like to ask why the program will have segmentation fault if it exceeds 10% of the system memory. Obviously, there is still a lot of memory to use.**
**100G of memory is used, and 400G  unused.**

Thank you for your answer！
code:
os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(FLAGS.gpu_id)
... 
    with tf.train.MonitoredTrainingSession(
            checkpoint_dir=model_dir,
            hooks=hooks,
            config=config
    ) as sess:
        while not sess.should_stop():
            sess.run(train_op)
...
Exceed 10% of system memory, Segmentation fault !
"
49927,[RNN] Post-training integer quantization got low performance when increasing the size of representative dataset.,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS 10.15.7 
- TensorFlow installation (pip package or built from source): tensorflow-nightly
- TensorFlow library (version, if pip package or github SHA, if built from source):

### 2. Code

My model is a 5-layer Bidirectional LSTM. The originally trained model accuracy is about 0.90. When I increase the size of the representative dataset from 400 to 20000，the quantitative model's acc decrease from 0.85 to 0.50. How is the representative dataset used? In my opinion, the more data I provide, the better quantization performance I will obtain. But it seems not, why?

```
def representative_data_gen():
  for input_value in tf.data.Dataset.from_tensor_slices(train_data).batch(1).take(400):
    yield [input_value]

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

tflite_model_quant = converter.convert()
```


"
49926,Improvement for tf.keras.utils.get_file,"**System information**
- TensorFlow version (you are using): 2.5.0
- Are you willing to contribute it (Yes/No): Yes


**Describe the feature and the current behavior/state.**
tf.keras.utils.get_file only works with `GET` request. I think it can work for `POST` request too if the `data` argument is utilized in [urlretrieve](https://github.com/tensorflow/tensorflow/blob/ecc0010f151cabb0a4730777790631fd6fff8f6f/tensorflow/python/keras/utils/data_utils.py#L259). 

**Will this change the current api? How?**
Yes, get_file would take in one other parameter (don't know which name and type would be best) to pass into the retrieve function.

**Who will benefit with this feature?**
Whoever wants to get file that's accessed through post method, such as files that require tokens or authorization.

**Side note on documentation.**
In [get_file documentation](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file), `untar` and `md5_hash` arguments have been depreciated in favor of `extract` and `file_hash`. I thought about some changes but I don't know if they would add value.
* Change [example](https://www.tensorflow.org/api_docs/python/tf/keras/utils/get_file#example) argument from `untar=True` to `extract=True`
* Rearrange arguments in documentation and function to show `extract` and `file_hash` before `untar` and `md5_hash`.
"
49923,build tensorflow 1.15 from source code based on cuda 11,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: None
- TensorFlow installed from (source or binary):  source
- TensorFlow version: 1.15
- Python version: 3.6
- Installed using virtualenv? pip? conda?: docker container, build using tensorflow official dockerfile
- Bazel version (if compiling from source): 0.26.1
- GCC/Compiler version (if compiling from source): gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)
- CUDA/cuDNN version: 11.2/8.1
- GPU model and memory: v100/16G



**Describe the problem**
I want to build tensorflow 1.15 from source code, because I have added some code in contrib.
first, I build a docker image which is dev-gpu-bazel0.26.1-py3-cuda11.2-cudnn8.1.0-ubuntu18.04
then create a container base on this image. 
After run ./configure, a error occurred,
""ValueError: dictionary update sequence element #9 has length 1; 2 is required""
This seems that function to find cuda10 information is not work for cuda 11. So I replace ""./third_party/gpus/cuda_configure.bzl""
and ""third_party/gpus/find_cuda_config.py"" with corresponding files in tensorflow2.
But, another error occurred, seems that the code are depended by other code and depend on other code.


I found a issue and get a reply,  https://github.com/tensorflow/tensorflow/issues/43629#issuecomment-702851560
said that, I can not use tf1.15 on cuda11.
But I found another page said, they can install tensorflow 1.15 on cuda 11, but by pip not by build from source code.
,  https://developer.nvidia.com/blog/accelerating-tensorflow-on-a100-gpus/

So, what I want is that, How can I build tensorflow 1.15 when cuda versio is 11.2.

**Provide the exact sequence of commands / steps that you executed before running into the problem**


**Any other info / logs**
Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
49920,Changes in Input size of the model do not save when calling saved_model.save,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 2004
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip install tensorflow
- TensorFlow version (use command below): 2.5.0
- Python version: 3.8.5
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: irrelevant?
- GPU model and memory:


**Describe the current behavior**
Changes in input shape of the model are not applied when saving the model, as can be seen that Input shape is changed before saving the model, and when opening the saved model the input shape reverts to [None,None,None,3] .



**Describe the expected behavior**
Input shape in the new saved model should change to the [1,640,640,3].

This is mainly required to convert the model into TFlite compatible one.

**Standalone code to reproduce the issue**

import tensorflow as tf

print(tf.__version__)


model = tf.saved_model.load(""exported-models/ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8/saved_model"") #taken from Object detection zoo
model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY].inputs[0].set_shape([1, 640, 640, 3])
tf.saved_model.save(model, ""saved_model_updated"", signatures=model.signatures[tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY])
print(model.signatures['serving_default'].inputs[0])



model2 = tf.saved_model.load(""saved_model_updated"")

print(model2.signatures['serving_default'].inputs[0])

####################################
2.5.0

WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 155). These functions will not be directly callable after loading.

INFO:tensorflow:Assets written to: saved_model_updated\assets

INFO:tensorflow:Assets written to: saved_model_updated\assets

Tensor(""input_tensor:0"", shape=(1, 640, 640, 3), dtype=uint8)
Tensor(""input_tensor:0"", shape=(None, None, None, 3), dtype=uint8)
"
49918,Check layer by layer execution time in tensorflow object detection API model,"How to calculate the execution time of each layers in tensorflow object detection API model?
Also I am able to see the node names using the below code.How to use tf.graph() to get the layer names?


import tensorflow as tf
# read pb into graph_def


with tf.io.gfile.GFile(""frozen_inference_graph.pb"", ""rb"") as f:
  graph_def = tf.compat.v1.GraphDef()
  graph_def.ParseFromString(f.read())

with tf.Graph().as_default() as graph:
  tf.import_graph_def(graph_def)


for op in graph.get_operations(): 
  print(op.name, [inp for inp in op.inputs])
"
49917,tf random gives the same sequence of numbers when it shouldn't,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): pip3 install
- TensorFlow version (use command below): 2.5.0
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: CUDA11.3 cudnn 8.2.0
- GPU model and memory: NVIDIA TITAN V 12 GB

**Describe the current behavior**
see below
**Describe the expected behavior**
generate different numbers 

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
colab: https://colab.research.google.com/drive/1ffXUYw6M7rDokL2EkPqKjna8AD5QEslA?usp=sharing
```python
import tensorflow as tf
import numpy as np

def _h(model,x):
    model(x)

class b(tf.keras.layers.Layer):
    def call(self, inputs):
        if tf.constant(True):
            tf.print(tf.random.uniform([5,]),summarize=-1)
        else:
            pass
        return inputs

tf.random.set_seed(123)
inputs = tf.keras.Input(shape=(784,))

x=b()(inputs)
x=b()(x)
x=b()(x)
outputs=b()(x)

model = tf.keras.Model(inputs=inputs, outputs=outputs)

h=tf.function(_h)

print('first run:')
h(model,tf.constant(np.random.rand(64,784)))
print('second run:')
h(model,tf.constant(np.random.rand(64,784)))
```
output:
first run:
[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]
[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]
[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]
[0.277247906 0.994074821 0.379808426 0.71479249 0.50061965]
second run:
[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]
[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]
[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]
[0.176383495 0.109812617 0.334476113 0.66576612 0.116794825]

The phenomenon generalize to other related api like `tf.random.categorical` , etc.
I read this issue, https://github.com/tensorflow/tensorflow/issues/33297, and realized that tf random could be buggy, but there is no any warning to new users in the api page.(e.g. https://www.tensorflow.org/api_docs/python/tf/random/uniform) The warning is shown in https://www.tensorflow.org/guide/random_numbers, which is too difficult to notice. The random number docs is inadequate as well, how do I use more complicated api like `tf.random.categorical` from `tf.random.Generator`?

"
49914,SafeH2DMemcpy removed in thunk,"I am looking into some profiling issue of XLA. I noticed that in tf2.5 and newer **SafeH2DMemcpy** part is remove from convolution_thunk.cc/custom_call_thunk/convolution_thunk.

By looking into old code of convlution_thunk's memcpy part
```python

  // Write the output tuple.
  const int kNumOutputs = 2;
  auto ptrs = absl::make_unique<void*[]>(kNumOutputs);
  ptrs[0] = result_buffer.opaque();
  ptrs[1] = scratch.opaque();
  se::DeviceMemory<void*> tuple_addr(
      buffer_allocations.GetDeviceAddress(tuple_result_buffer_));
  SafeH2DMemcpy(tuple_addr, std::move(ptrs), kNumOutputs, params.stream,
                params.deferred_host_callbacks);
```
I am wondering
- What are result_buffer, scratch_buffer and tuple_result_buffer here? Are they cpu buffer or gpu buffer?
- Why it does a H2D copy instead of D2H copy? Why result is not from gpu to cpu? 
- Why this memcpy is now removed? What is the original purpose of this?

Would really appreciate any ideas on this."
49913,TfLite CUMSUM operator missing reference test ,"While porting the CUMSUM kernel op to TFLM, it was discovered that the reference code for the operator in tensorflow/lite/kernels/internal/reference/cumsum.h did not have a dedicated test.  The existing TfLite test for the CUMSUM kernel only tested the optimized version of the operator.  The Eval code for the TfLite CUMSUM kernel only calls the optimized operator code.

A test for the reference version of the operator has been created.

"
49912,tensorflow,"Thank you for submitting a TensorFlow documentation issue. Per our GitHub
policy, we only address code/doc bugs, performance issues, feature requests, and
build/installation issues on GitHub.

The TensorFlow docs are open source! To get involved, read the documentation
contributor guide: https://www.tensorflow.org/community/contribute/docs

## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/MyMethod

## Description of issue (what needs changing):

### Clear description

For example, why should someone use this method? How is it useful?

### Correct links

Is the link to the source code correct?

### Parameters defined

Are all parameters defined and formatted correctly?

### Returns defined

Are return values defined?

### Raises listed and defined

Are the errors defined? For example,
https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column/categorical_column_with_vocabulary_file#raises

### Usage example

Is there a usage example?

See the API guide: https://www.tensorflow.org/community/contribute/docs_ref
on how to write testable usage examples.

### Request visuals, if applicable

Are there currently visuals? If not, will it clarify the content?

### Submit a pull request?

Are you planning to also submit a pull request to fix the issue? See the docs
contributor guide: https://www.tensorflow.org/community/contribute/docs,
docs API guide: https://www.tensorflow.org/community/contribute/docs_ref and the
docs style guide: https://www.tensorflow.org/community/contribute/docs_style
"
49911,Issue in building tensorflow wheel using bazel,"Hi

I built tensorflow wheel using bazel I got wheel name: tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl
this wheel is installed successfully on the machine where it is built which is MacOs Big Sur 11 but I am getting an error when installing it on other macOS big sur 11 (VMWARE7.1):

bs-mac43:~ buildmachine$ python3.9 -m pip install tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl 
ERROR: tensorflow-2.5.0-cp39-cp39-macosx_11_0_universal2.whl is not a supported wheel on this platform.

Installing the released wheel tensorflow-2.5.0-cp39-cp39-macosx_10_11_x86_64.whl works fine on both OS.

1. What is wrong with the wheel I built?
2. How to build wheel that support Mac OS 10 and 11?
3. How to build wheel that support x86 and 64?

Thanks"
49910,Syntax warning in lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py,"Hi

After moving to python3.9 I get syntax warning in: 
lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py lines 142, 144, 146

error message:
pythonVirtualEnv/lib/python3.9/site-packages/tensorflow/python/keras/benchmarks/benchmark_util.py:142: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?

Code causing error:
if 'x' is None:
    raise ValueError('Input data is required.')
  if 'optimizer' is None:
    raise ValueError('Optimizer is required.')
  if 'loss' is None:
    raise ValueError('Loss function is required.')

'x' 'optimizer' 'loss' isn't None since it is a string so python3.9 do not like it and raise a warning.
In order to fix it I removed those lines locally then no longer getting syntax warning.

Can you fix it? And when?

Thanks"
49908,debugging issue,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): source
- TensorFlow version (use command below): 2.4.1
- Python version: 3.6.9
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: Quadro RTX 6000


**Describe the current behavior**

`InvalidArgumentError: condition [24], then [23], and else [] must be broadcastable 	 [[{{node SelectV2_7}}]] [Op:IteratorGetNext]`

What does this mean? I tried debugging using tf.debugging all my cases were passed. still not able to figure how how to debug this. Is is possible to print the graph and nodes of the function. Any help will be appreciated. 


**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing): Yes"
49907,Failure when training with tf.GradientTape() for regression problems,"**System information**
- OS Platform and Distribution: Windows 10 (the same thing happens on Linux)
- TensorFlow version: 2.1.0 (the same thing happens on 2.5.0)
- Python version: 3.6.6

**Describe the current behavior**
For the exactly the same model, training via `tf.GradientTape()` either: (a) does not converge, or (b) converges with worse score than model training via `tf.keras.Model.fit` method. 

**Describe the expected behavior**
Similar training outcome when using `tf.GradinetTape()` and `tf.keras.Model.fit`.

**Standalone code to reproduce the issue**
```
import numpy as np
from sklearn.datasets import load_boston
import tensorflow as tf

tf.random.set_seed(0)


def make_model():
    input_layer = tf.keras.layers.Input(shape=(np.shape(x)[1]))
    inner_layer_1 = tf.keras.layers.Dense(
        units=10,
        activation='selu',
        kernel_initializer=tf.keras.initializers.lecun_normal()
    )(input_layer)
    inner_layer_2 = tf.keras.layers.Dense(
        units=10,
        activation='selu',
        kernel_initializer=tf.keras.initializers.lecun_normal()
    )(inner_layer_1)
    output_layer = tf.keras.layers.Dense(
        units=1,
        activation='linear'
    )(inner_layer_2)
    return tf.keras.Model(
        input_layer,
        output_layer
    )

# Get raw data.
raw_features, y = load_boston(return_X_y=True)

# Standardize features.
x = (raw_features-raw_features.mean(axis=0)) / raw_features.std(axis=0)
print(np.std(x, axis=0))

# Reshape targets.
y = np.array(np.reshape(y, newshape=(-1, 1)), dtype=np.float32)

# Training parameters.
optimizer = tf.keras.optimizers.Adam()
optimizer_mp = tf.keras.mixed_precision.experimental.LossScaleOptimizer(
    optimizer, ""dynamic""
)
objective = tf.keras.losses.MeanSquaredError()
batch_size = 4
epochs = 5

# Fit via fit method.
train_fit = make_model()
train_fit.summary()
train_fit.compile(optimizer, objective)
train_fit.fit(x=x, y=y, batch_size=batch_size, epochs=epochs)

# Fit via gradient tape.
gradient_tape_fit = make_model()
gradient_tape_fit.summary()
dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(batch_size)
for epoch in range(0, epochs):
    for step, (x_batch, y_batch) in enumerate(dataset):
        with tf.GradientTape() as tape:
            predictions = gradient_tape_fit(x_batch, training=True)
            loss = objective(y_batch, predictions)
            scaled_loss = optimizer_mp.get_scaled_loss(loss)
        scaled_grads = tape.gradient(
            scaled_loss, gradient_tape_fit.trainable_weights
        )
        gradients = optimizer_mp.get_unscaled_gradients(
            scaled_grads
        )
        optimizer.apply_gradients(zip(
            gradients, gradient_tape_fit.trainable_weights
        ))
    predictions_via_tape = gradient_tape_fit.predict(x)
    print(
        'Tape MSE: %s' % np.mean(np.power(y-predictions_via_tape, 2))
    )

predictions_via_train = train_fit.predict(x)
print(
    'Fit MSE: %s' % np.mean(np.power(y-predictions_via_train, 2))
)
```"
49906,Eager Execution Documentation includes Sections not Describing Relation or relevance to Eager Execution,"## URL(s) with the issue:

https://www.tensorflow.org/guide/eager

## Description of issue (what needs changing):

In the documentation for Eager Execution, the following sections are included but no detail is provided as to why these sections are related to Eager Execution. A description is required to indicate if these capabilities/operations are only available, operate differently or in what other way are related to Eager Execution. Right now, it is entirely unclear why these sections are located on the Eager Execution page.

""Object-based saving""
""Summaries and TensorBoard""
""Advanced automatic differentiation topics""
""Performance"""
49905,Build for macos_arm64 on m1 fails with missing toolchain,"

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macos big sur 11.2.3
- TensorFlow installed from (source or binary): source
- TensorFlow version: 2.5.0 tag
- Python version: 3.8.2
- Installed using virtualenv? pip? conda?: venv
- Bazel version (if compiling from source): 3.7.2
- GCC/Compiler version (if compiling from source): 
% clang --version
Apple clang version 12.0.5 (clang-1205.0.22.9)
Target: arm64-apple-darwin20.3.0
- CUDA/cuDNN version: n/a
- GPU model and memory: n/a


**Describe the problem**
Building libtensorflow fails with 
ERROR: /private/var/tmp/_bazel_mnelson/fcd9b76ea591ce077b01d60187d6f334/external/local_config_cc/BUILD:48:19: in cc_toolchain_suite rule @local_config_cc//:toolchain: cc_toolchain_suite '@local_config_cc//:toolchain' does not contain a toolchain for cpu 'darwin_arm64'


**Provide the exact sequence of commands / steps that you executed before running into the problem**
```
./bazel-3.7.2 build --jobs 4 --config=macos_arm64 --config=noaws --config=nogcp --config=nohdfs --config=nonccl //tensorflow:libtensorflow.dylib
```

"
49904,Fail to load weights of Keras MobilenetV2,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux? (Google Colab)
- TensorFlow version (use command below): 2.5.0
- Python version: 3.7.10

**Describe the current behavior**
tensorflow.keras.applications.MobileNetV2() fail to return a model with Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1_224_no_top.h5: 404 -- Not Found.

**Describe the expected behavior**
Expect to get a model with loaded weights.

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): No

**Standalone code to reproduce the issue**
import tensorflow
model = tensorflow.keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    alpha=1,
    include_top=False,
    weights='imagenet',
    input_tensor=tensorflow.keras.layers.Input((224, 224, 3)),
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)

Colab to reproduce: https://colab.research.google.com/drive/1d9aaTVCFDMWyyiKrEsIHBe_BFnEEiKKD?usp=sharing

**Other info / logs**
HTTPError                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/data_utils.py in get_file(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)
    257       try:
--> 258         urlretrieve(origin, fpath, dl_progress)
    259       except urllib.error.HTTPError as e:

9 frames
HTTPError: HTTP Error 404: Not Found

During handling of the above exception, another exception occurred:

Exception                                 Traceback (most recent call last)
/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/data_utils.py in get_file(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)
    258         urlretrieve(origin, fpath, dl_progress)
    259       except urllib.error.HTTPError as e:
--> 260         raise Exception(error_msg.format(origin, e.code, e.msg))
    261       except urllib.error.URLError as e:
    262         raise Exception(error_msg.format(origin, e.errno, e.reason))

Exception: URL fetch failure on https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1_224_no_top.h5: 404 -- Not Found
"
49902,How to build TfLite C API for arm-linux-gnueabihf,"## URL(s) with the issue:

<https://www.tensorflow.org/lite/guide/build_rpi>
<https://www.tensorflow.org/lite/guide/build_cmake_arm>

## Description of issue (what needs changing):

### Clear description

How to build TfLite C API for arm-linux-gnueabihf?
(ARMv7 NEON enabled, using CMake)
I need to build both static and dynamic libraries."
49901,Lambda and Subclass layer,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- Using Google colab
- TensorFlow version: `tensorflow._api.v2.version`
- GPU model and memory: Python Google Compute Engine Backend (2 cores)

This is the warning I get:
<pre>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5
94773248/94765736 [==============================] - 1s 0us/step
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.tensordot), but
are not present in its tracked objects:
  <tf.Variable 'dense_1/kernel:0' shape=(2048, 2048) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.nn.bias_add), but
are not present in its tracked objects:
  <tf.Variable 'dense_1/bias:0' shape=(2048,) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.tensordot_1), but
are not present in its tracked objects:
  <tf.Variable 'dense/kernel:0' shape=(2048, 5) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.
WARNING:tensorflow:
The following Variables were used a Lambda layer's call (tf.nn.bias_add_1), but
are not present in its tracked objects:
  <tf.Variable 'dense/bias:0' shape=(5,) dtype=float32>
It is possible that this is intended behavior, but it is more likely
an omission. This is a strong indication that this layer should be
formulated as a subclassed Layer rather than a Lambda layer.</pre>

**Standalone code to reproduce the issue**
<pre>input_tensor = tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, CANAL))

base = ResNet50(weights=""imagenet"", include_top=False,input_tensor=input_tensor)
#base.load_weights('resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')

top = Dense(N_CLASSES, activation='softmax')(
            Dropout(0.5)(
                Dense(2048, activation='relu')(
                    Dropout(0.5)(
                        MaxPooling2D()(base.output)
                    )
                )
            )
        )

model = Model(input_tensor, top)</pre>

Kindly help since this isn't allowing me to train all the pre-existing layers in the model.

N_CLASSES= 5
CANAL = 3
IMG_SIZE = 512"
49900,No consideration of Activation Functions in the implementation of Initializers,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes 
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Everything
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): NA
- TensorFlow version (use command below): 2.5
- Python version:
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior** : In the Tensorflow Source Code of neither the [He Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L849-L883), nor the [Glorot Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L711-L752), along with the `Code` of the **`Parent Class`**, [VarianceScaling Initializer](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/initializers/initializers_v2.py#L430-L531), there is no mention of the dependency on **`Activation Function's`** in the code of **`Initializers`**.

**Describe the expected behavior** : As per the screenshots shown below, (first one is from the Table 11-1 of the book, **`Hands on Machine Learning with Scikit-Learn and Tensorflow`**, **`Version 2`**, and the bottom one is from the **`First Version`** of the same book), the formula for [Glorot](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal) and [He](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal) Initializers should be dependent on the **`Activation Functions`**. 

![image](https://user-images.githubusercontent.com/48206667/120094668-66661a80-c13f-11eb-8398-8c381f56e182.png)


![image](https://user-images.githubusercontent.com/48206667/120095045-4a637880-c141-11eb-9550-c4524bb728d5.png)

**Standalone code to reproduce the issue** : Source Code of the [He Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L849-L883), [Glorot Initializer](https://github.com/tensorflow/tensorflow/blob/a4dfb8d1a71385bd6d122e4f27f86dcebb96712d/tensorflow/python/keras/initializers/initializers_v2.py#L711-L752), and the `Code` of the **`Parent Class`**, [VarianceScaling Initializer](https://github.com/tensorflow/tensorflow/blob/v2.5.0/tensorflow/python/keras/initializers/initializers_v2.py#L430-L531)."
49899,TfLite C API safety requirements,"## URL(s) with the issue:

https://github.com/tensorflow/tensorflow/blob/eaa6753876e6a5974258e7244f46bfe8b27f906d/tensorflow/lite/c/c_api.h#L142-L152

## Description of issue (what needs changing):

### Clear description

What would happen if the user does not follow the calling order?

Does the interpreter perform a runtime safety check?

Can I read the input tensors before I fill values into them?"
49898,Failed Build 2.5.0 on Windows with Cuda 11.2,"System information

OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
TensorFlow installed from (source or binary): source
TensorFlow version: 2.5.0
Python version: 3.8
Bazel version (if compiling from source): 3.7.2
CUDA/cuDNN version: 11.2/8.1.0
GPU model and memory: 2070

The following error occurs when I attempt to build 2.5.0. 

**ERROR: C:/sdks/tensorflow/tensorflow/compiler/mlir/hlo/BUILD:423:11: C++ compilation of rule //tensorflow/compiler/mlir/hlo:lhlo' failed (Exit 2): cl.exe failed: error executing command**

> cd C:/users/adam/_bazel_adam/e7merofc/execroot/org_tensorflow
>   SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
>     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
>     SET PWD=/proc/self/cwd
>     SET PYTHON_BIN_PATH=C:/Users/Adam/anaconda3/python.exe
>     SET PYTHON_LIB_PATH=C:/Users/Adam/anaconda3/lib/site-packages
>     SET RUNFILES_MANIFEST_ONLY=1
>     SET TEMP=C:\Users\Adam\AppData\Local\Temp
>     SET TF2_BEHAVIOR=1
>     SET TMP=C:\Users\Adam\AppData\Local\Temp
>   C:/Program Files (x86)/Microsoft Visual Studio/2019/Enterprise/VC/Tools/MSVC/14.28.29333/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgSparseOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen /Itensorflow/compiler/mlir/hlo/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX2 /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_objs/lhlo/lhlo_ops_structs.obj /c tensorflow/compiler/mlir/hlo/lib/Dialect/mhlo/IR/lhlo_ops_structs.cc
> Execution platform: @local_execution_config_platform//:platform
> cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
> cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(25): error C2665: 'mlir::DictionaryAttr::get': none of the 3 overloads could convert all the argument types
> bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen\mlir/IR/BuiltinAttributes.h.inc(261): note: could be 'mlir::DictionaryAttr mlir::DictionaryAttr::get(mlir::MLIRContext *,llvm::ArrayRef<mlir::NamedAttribute>)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(25): note: while trying to match the argument list '(llvm::SmallVector<mlir::NamedAttribute,2>, mlir::MLIRContext *)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(116): error C2665: 'mlir::DictionaryAttr::get': none of the 3 overloads could convert all the argument types
> bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen\mlir/IR/BuiltinAttributes.h.inc(261): note: could be 'mlir::DictionaryAttr mlir::DictionaryAttr::get(mlir::MLIRContext *,llvm::ArrayRef<mlir::NamedAttribute>)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(116): note: while trying to match the argument list '(llvm::SmallVector<mlir::NamedAttribute,9>, mlir::MLIRContext *)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(266): error C2665: 'mlir::DictionaryAttr::get': none of the 3 overloads could convert all the argument types
> bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen\mlir/IR/BuiltinAttributes.h.inc(261): note: could be 'mlir::DictionaryAttr mlir::DictionaryAttr::get(mlir::MLIRContext *,llvm::ArrayRef<mlir::NamedAttribute>)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(266): note: while trying to match the argument list '(llvm::SmallVector<mlir::NamedAttribute,4>, mlir::MLIRContext *)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(356): error C2665: 'mlir::DictionaryAttr::get': none of the 3 overloads could convert all the argument types
> bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen\mlir/IR/BuiltinAttributes.h.inc(261): note: could be 'mlir::DictionaryAttr mlir::DictionaryAttr::get(mlir::MLIRContext *,llvm::ArrayRef<mlir::NamedAttribute>)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(356): note: while trying to match the argument list '(llvm::SmallVector<mlir::NamedAttribute,4>, mlir::MLIRContext *)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(446): error C2665: 'mlir::DictionaryAttr::get': none of the 3 overloads could convert all the argument types
> bazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen\mlir/IR/BuiltinAttributes.h.inc(261): note: could be 'mlir::DictionaryAttr mlir::DictionaryAttr::get(mlir::MLIRContext *,llvm::ArrayRef<mlir::NamedAttribute>)'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.cc.inc(446): note: while trying to match the argument list '(llvm::SmallVector<mlir::NamedAttribute,4>, mlir::MLIRContext *)'
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 7072.771s, Critical Path: 304.21s
> INFO: 7627 processes: 619 internal, 7008 local.
> FAILED: Build did NOT complete successfully

Running again, different error message, but looks related.


**ERROR: C:/sdks/tensorflow/tensorflow/compiler/mlir/xla/BUILD:265:11: C++ compilation of rule '//tensorflow/compiler/mlir/xla:hlo_utils' failed (Exit 2): cl.exe failed: error executing command**

>   cd C:/users/adam/_bazel_adam/e7merofc/execroot/org_tensorflow
>   SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.18362.0\cppwinrt
>     SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Windows Kits\10\bin\10.0.18362.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\\MSBuild\Current\Bin;C:\Windows\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja
>     SET PWD=/proc/self/cwd
>     SET PYTHON_BIN_PATH=C:/Users/Adam/anaconda3/python.exe
>     SET PYTHON_LIB_PATH=C:/Users/Adam/anaconda3/lib/site-packages
>     SET RUNFILES_MANIFEST_ONLY=1
>     SET TEMP=C:\Users\Adam\AppData\Local\Temp
>     SET TF2_BEHAVIOR=1
>     SET TMP=C:\Users\Adam\AppData\Local\Temp
>   C:/Program Files (x86)/Microsoft Visual Studio/2019/Enterprise/VC/Tools/MSVC/14.28.29333/bin/HostX64/x64/cl.exe /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0601 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /bigobj /Zm500 /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/llvm-project /Ibazel-out/x64_windows-opt/bin/external/llvm-project /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/libjpeg_turbo /Ibazel-out/x64_windows-opt/bin/external/libjpeg_turbo /Iexternal/com_google_protobuf /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf /Iexternal/com_googlesource_code_re2 /Ibazel-out/x64_windows-opt/bin/external/com_googlesource_code_re2 /Iexternal/farmhash_archive /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive /Iexternal/fft2d /Ibazel-out/x64_windows-opt/bin/external/fft2d /Iexternal/highwayhash /Ibazel-out/x64_windows-opt/bin/external/highwayhash /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/snappy /Ibazel-out/x64_windows-opt/bin/external/snappy /Iexternal/local_config_cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda /Iexternal/local_config_rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm /Iexternal/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/canonicalize_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/chlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinDialectIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinLocationAttributesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/BuiltinTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CallOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CastOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/InferTypeOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/OpAsmInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/RegionKindInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SideEffectInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SymbolInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_base_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/hlo_ops_pattern_gen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineMemoryOpInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/AffineOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LoopLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MemRefOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ControlFlowInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/StandardOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorBaseIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TensorOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/VectorInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ViewLikeInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/CopyOpInterfaceIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgInterfacesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgStructuredOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/LinalgSparseOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ParserTokenKinds /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/SCFPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/MLIRShapeCanonicalizationIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ShapeOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLTypesIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/PDLInterpOpsIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/ConversionPassIncGen /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/_virtual_includes/TransformsPassIncGen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_structs_inc_gen /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/_virtual_includes/cuda_headers_virtual /Ibazel-out/x64_windows-opt/bin/external/local_config_tensorrt/_virtual_includes/tensorrt_headers /Itensorflow/compiler/mlir/xla/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/xla/include /Itensorflow/compiler/mlir/hlo/include /Ibazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include /Iexternal/llvm-project/mlir/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/mlir/include /Iexternal/llvm-project/llvm/include /Ibazel-out/x64_windows-opt/bin/external/llvm-project/llvm/include /Iexternal/zlib /Ibazel-out/x64_windows-opt/bin/external/zlib /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/gif /Ibazel-out/x64_windows-opt/bin/external/gif /Iexternal/gif/windows /Ibazel-out/x64_windows-opt/bin/external/gif/windows /Iexternal/com_google_protobuf/src /Ibazel-out/x64_windows-opt/bin/external/com_google_protobuf/src /Iexternal/farmhash_archive/src /Ibazel-out/x64_windows-opt/bin/external/farmhash_archive/src /Iexternal/double_conversion /Ibazel-out/x64_windows-opt/bin/external/double_conversion /Iexternal/local_config_cuda/cuda /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda /Iexternal/local_config_cuda/cuda/cuda/include /Ibazel-out/x64_windows-opt/bin/external/local_config_cuda/cuda/cuda/include /Iexternal/local_config_rocm/rocm /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm /Iexternal/local_config_rocm/rocm/rocm/include /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include /Iexternal/local_config_rocm/rocm/rocm/include/rocrand /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/rocrand /Iexternal/local_config_rocm/rocm/rocm/include/roctracer /Ibazel-out/x64_windows-opt/bin/external/local_config_rocm/rocm/rocm/include/roctracer /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_CRT_NONSTDC_NO_DEPRECATE /D_CRT_NONSTDC_NO_WARNINGS /D_SCL_SECURE_NO_DEPRECATE /D_SCL_SECURE_NO_WARNINGS /DUNICODE /D_UNICODE /DLLVM_ENABLE_STATS /D__STDC_LIMIT_MACROS /D__STDC_CONSTANT_MACROS /D__STDC_FORMAT_MACROS /DLLVM_BUILD_GLOBAL_ISEL /DTF_USE_SNAPPY /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /Oy- /DNDEBUG /wd4117 -D__DATE__=""redacted"" -D__TIMESTAMP__=""redacted"" -D__TIME__=""redacted"" /Gy /Gw /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX2 /std:c++14 /Fobazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/xla/_objs/hlo_utils/hlo_utils.obj /c tensorflow/compiler/mlir/xla/hlo_utils.cc
> Execution platform: @local_execution_config_platform//:platform
> cl : Command line warning D9035 : option 'experimental:preprocessor' has been deprecated and will be removed in a future release
> cl : Command line warning D9036 : use 'Zc:preprocessor' instead of 'experimental:preprocessor'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc(13): error C2011: 'mlir::mhlo::ChannelHandle': 'class' type redefinition
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(13): note: see declaration of 'mlir::mhlo::ChannelHandle'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc(32): error C2011: 'mlir::mhlo::ConvDimensionNumbers': 'class' type redefinition
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(32): note: see declaration of 'mlir::mhlo::ConvDimensionNumbers'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc(65): error C2011: 'mlir::mhlo::DotDimensionNumbers': 'class' type redefinition
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(65): note: see declaration of 'mlir::mhlo::DotDimensionNumbers'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc(88): error C2011: 'mlir::mhlo::GatherDimensionNumbers': 'class' type redefinition
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(88): note: see declaration of 'mlir::mhlo::GatherDimensionNumbers'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops_structs.h.inc(111): error C2011: 'mlir::mhlo::ScatterDimensionNumbers': 'class' type redefinition
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(111): note: see declaration of 'mlir::mhlo::ScatterDimensionNumbers'
> external/llvm-project/llvm/include\llvm/Support/type_traits.h(75): error C2079: 'llvm::detail::copy_construction_triviality_helper<T>::t' uses undefined class 'mlir::mhlo::ChannelHandle'
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\type_traits(630): note: see reference to class template instantiation 'llvm::detail::copy_construction_triviality_helper<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> external/llvm-project/llvm/include\llvm/Support/type_traits.h(100): note: see reference to class template instantiation 'std::is_copy_constructible<llvm::detail::copy_construction_triviality_helper<T>>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'llvm::is_trivially_copy_constructible<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(705): note: see reference to class template instantiation 'llvm::Optional<mlir::mhlo::ChannelHandle>' being compiled
> C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\type_traits(788): error C2139: 'mlir::mhlo::ChannelHandle': an undefined class is not allowed as an argument to compiler intrinsic type trait '__is_trivially_assignable'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(13): note: see declaration of 'mlir::mhlo::ChannelHandle'
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'std::is_trivially_copy_assignable<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\type_traits(769): error C2139: 'mlir::mhlo::ChannelHandle': an undefined class is not allowed as an argument to compiler intrinsic type trait '__is_trivially_constructible'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(13): note: see declaration of 'mlir::mhlo::ChannelHandle'
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'std::is_trivially_move_constructible<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\type_traits(661): error C2139: 'mlir::mhlo::ChannelHandle': an undefined class is not allowed as an argument to compiler intrinsic type trait '__is_constructible'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(13): note: see declaration of 'mlir::mhlo::ChannelHandle'
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'std::is_move_constructible<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\type_traits(798): error C2139: 'mlir::mhlo::ChannelHandle': an undefined class is not allowed as an argument to compiler intrinsic type trait '__is_trivially_assignable'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(13): note: see declaration of 'mlir::mhlo::ChannelHandle'
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'std::is_trivially_move_assignable<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\VC\Tools\MSVC\14.28.29333\include\type_traits(705): error C2139: 'mlir::mhlo::ChannelHandle': an undefined class is not allowed as an argument to compiler intrinsic type trait '__is_assignable'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(13): note: see declaration of 'mlir::mhlo::ChannelHandle'
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(54): note: see reference to class template instantiation 'std::is_move_assignable<T>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(240): error C2976: 'llvm::optional_detail::OptionalStorage': too few template arguments
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(60): note: see declaration of 'llvm::optional_detail::OptionalStorage'
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(168): error C2079: 'llvm::optional_detail::OptionalStorage<T,true>::value' uses undefined class 'mlir::mhlo::ChannelHandle'
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> external/llvm-project/llvm/include\llvm/ADT/Optional.h(240): note: see reference to class template instantiation 'llvm::optional_detail::OptionalStorage<T,true>' being compiled
>         with
>         [
>             T=mlir::mhlo::ChannelHandle
>         ]
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2194): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2194): error C3646: 'target_arg_mapping': unknown override specifier
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2194): error C2059: syntax error: '('
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2194): error C2238: unexpected token(s) preceding ';'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2224): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2224): error C3646: 'target_arg_mappingAttr': unknown override specifier
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2224): error C2059: syntax error: '('
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2224): error C2238: unexpected token(s) preceding ';'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2225): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2225): error C2065: 'CustomCallTargetArgMapping': undeclared identifier
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2225): error C2923: 'llvm::Optional': 'CustomCallTargetArgMapping' is not a valid template type argument for parameter 'T'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2229): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2229): error C2061: syntax error: identifier 'CustomCallTargetArgMapping'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2231): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2231): error C2061: syntax error: identifier 'CustomCallTargetArgMapping'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2232): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2232): error C2061: syntax error: identifier 'CustomCallTargetArgMapping'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2233): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2233): error C2061: syntax error: identifier 'CustomCallTargetArgMapping'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2234): error C2039: 'CustomCallTargetArgMapping': is not a member of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2175): note: see declaration of 'mlir::lmhlo'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/_virtual_includes/lhlo_ops_inc_gen\mlir-hlo/Dialect/mhlo/IR/lhlo_ops.h.inc(2234): error C2061: syntax error: identifier 'CustomCallTargetArgMapping'
> tensorflow/compiler/mlir/xla/hlo_utils.cc(262): error C2027: use of undefined type 'mlir::mhlo::GatherDimensionNumbers'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(88): note: see declaration of 'mlir::mhlo::GatherDimensionNumbers'
> tensorflow/compiler/mlir/xla/hlo_utils.cc(261): error C2027: use of undefined type 'mlir::mhlo::GatherDimensionNumbers'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(88): note: see declaration of 'mlir::mhlo::GatherDimensionNumbers'
> tensorflow/compiler/mlir/xla/hlo_utils.cc(279): error C2027: use of undefined type 'mlir::mhlo::GatherDimensionNumbers'
> bazel-out/x64_windows-opt/bin/tensorflow/compiler/mlir/hlo/include\mlir-hlo/Dialect/mhlo/IR/hlo_ops_base_structs.h.inc(88): note: see declaration of 'mlir::mhlo::GatherDimensionNumbers'
> tensorflow/compiler/mlir/xla/hlo_utils.cc(279): error C3861: 'get': identifier not found
> Target //tensorflow/tools/pip_package:build_pip_package failed to build
> INFO: Elapsed time: 468.026s, Critical Path: 56.72s
> INFO: 504 processes: 24 internal, 480 local.
> FAILED: Build did NOT complete successfully"
49897,Tensorflow Lite Fail On Release Build but success on Debug Build,"Debug Build = success
Release Build = Failed Severity	Code	Description	Project	File	Line	Suppression State
Error	C1001	An internal error has occurred in the compiler.	tensorflow-lite	d:\tensorflow\tflite_build\gemmlowp\internal\output.h	176	

Photo of debug build
![Screenshot_2](https://user-images.githubusercontent.com/68463485/120086983-39e5da80-c10e-11eb-8dc2-11521a4ba270.png)

Photo of release build
![Screenshot_3](https://user-images.githubusercontent.com/68463485/120086990-49652380-c10e-11eb-8e20-1101310aa6e3.png)

Error on this line 176 file output.h
![Screenshot_4](https://user-images.githubusercontent.com/68463485/120087057-f6d83700-c10e-11eb-96f0-55c110e39a2e.png)
"
49896,"Means of GlorotNormal Initializer and HeNormal Initializer are not zero, respectively","**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): NA
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA
- TensorFlow installed from (source or binary): Google Colab
- TensorFlow version (use command below): 2.5
- Python version:
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: NA
- GPU model and memory: NA

**Describe the current behavior** : **`Mean`** of the **`Initial Weights`** using **`Glorot Normal Initializer`** is not **`zero`**. Same is the case with **`HeNormal Initializer`**.

**Describe the expected behavior**: As per the documentation of [Glorot Normal](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotNormal), and [HeNormal](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal), **`mean`** of the **`Normal Distribution`** of the **`Initial Weights`** should be **`zero`**. 

> Draws samples from a truncated normal distribution centered on 0

**Standalone code to reproduce the issue** : [Colab Gist for Glorot Normal](https://colab.research.google.com/gist/rakeshmothukuru1/a44f7aaa1fd261247a1388c2e2f5a167/glorot_initializer_bug.ipynb)

[Colab Gist for HeNormal](https://colab.research.google.com/gist/rakeshmothukuru1/c2f6a4896f0a5414581bd438218f2eff/he_normal_initializer_bug.ipynb)."
49895,Plugin-able Optimizers,"**System information**
- Tensorflow version 2.5
- Are you willing to contribute it (Yes):



## Describe the feature and the current behavior/state.

Currently there are at least four different paths a user has to take to try out a different optimizer:

1. The Optimizer might be found under `tf.keras.optimizers`
2. If not, it might be included in `tfa.optimizers` (tensorflow/addons) or `tfp.optimizers` (tensorflow/probability)
3. If not, the user might need to (statically?) install a subclass `tf.keras.optimizers.Optimizer` defined in some github repo - if lucky there is a `setup.py`
4. There might not even be an implementation available subclassing `tf.keras.optimizers.Optimizer` so the user would have to reimplement that

Now wouldn't it be cool if any optimizer could be installed with `pip install my-optimizer` and then accessed using `tf.optimizers.MyOptimizer`? Now the python packaging guide includes a [section about plugins](https://packaging.python.org/guides/creating-and-discovering-plugins/#using-package-metadata) which would make just that possible:

Let's say I have implemented the class `MyOptimizer(tf.keras.optimizers.Optimizer)` in the file `my_optimizer.py`, then I could write the following `setup.py`:

```python
from setuptools import setup

setup(
    name='my-optimizer',
    version='1.0',
    py_modules=[ 'my_optimizer' ],
    install_requires= [...],
    entry_points='''
        [tf.optimizers]
        my-optimizer = my_optimizer:MyOptimizer
    '''
)
```

and upload the package. Now tensorflow could include this optimizer into `tf.optimizers` by doing the following:

```python
# tensorflow.optimizers.__init__.py

import importlib

PLUGINS = {
    plugin.attr : plugin.module for plugin in
    importlib.metadata.entry_points()[""tf.optimizers""]
}

def __getattr__(name: str) -> Any:
    return getattr(
        importlib.import_module(PLUGINS[name]), 
        name
)
```

(Alternatively one could use importlib in the dictionary comprehension already for eager loading. I am not sure which one is better. But people probably only import Optimizers only once and do not use all of them so I defaulted to the lazy version)

This [works for python 3.7+](https://stackoverflow.com/a/48916205/6662425), for python 3.6 one would have to either [replace the module with a class](https://stackoverflow.com/a/7668273/6662425)  or use something like `setattr()` on the module, `exec(f""{plugin.attr} = importlib.import_module(plugin.module)"")`, or some other magic

<details>
  <summary>Old (probably wrong) suggestion</summary>

I think I have misunderstood the purpose of the tf_export decorators here  

```python
import importlib
from tensorflow.python.util import tf_export
from importlib.metadata import entry_points

for plugin in entry_points()[""tf.optimizers""]:
    custom_optim_class = getattr(importlib.import_module(plugin.module), plugin.attr)
    # export the plugin optimizer class under the name `plugin.attr` which is its self-given name 
    tf_export.tf_export(""optimizers."" + plugin.attr, metaclass=abc.ABCMeta)(custom_optim_class)
```

</details>


Users could then install any optimizer published with such an `entry_point` with pip and find the optimizer inside of `tf.optimizers`.

## Changes to the API

`tf.optimizers` would be filled with plugin Optimizers. The optimizers could alternatively be included into `tf.keras.optimizers` or any other place, ideally it would unify the place where one could find optimizers in the long term.

## Who will benefit from this feature?

I am assuming that the reason `tensorflow/addons` and `tensorflow/probability` exists, is to reduce the base install size. If all optimizers are plugins which are extremely easy to install but all found in the same module by some import magic, then it will be easier to try out custom optimizers. And if the current ""batteries included"" optimizers are converted into such plugins as well, then you could not only reduce the base install of tensorflow (making them [optional dependencies](https://setuptools.readthedocs.io/en/latest/userguide/dependency_management.html#optional-dependencies) - e.g. `tensorflow[all]` would still include them), but these default plugins would also become standalone ""examples"" how to subclass the Optimizer class properly.

It is also fairly trivial to create a GitHub Template Repository with such a setup.py and stumps of such an Optimizer class with included github actions that deploy the package to pypa.

So people who would want to create new optimizers could simply click ""use template"" on those repositories, replace ""my-optimizer"" occurances with their optimizer name, implement the method stumps, and set the pypa access token in the github secrets. And their optimizer would immediately be available to download with pip and automatically included in the `tf.optimizers` module.

This means that the first three install methods would be merged. And since creating a new optimizer is made easier using this template, it is likely that more people would do so, moving more of the fourth category optimizers into this as well.

The general idea is to take the ""software engineering"" aspect out of writing Optimizers as much as possible such that researchers have an easier time.

Lastly things like

```python
import tensorflow as tf

def optimizers_as_dict():
    return { name : cls for name, cls in tf.optimizers.__dict__.items() if issubclass(cls, tf.keras.optimizers.Optimizer) }
```

could provide easy dynamic access to all installed optimizers making it a lot easier to do AutoML with them. This could also be included as a method in `tf.optimizers`.

## Extension

This plugin approach could also be applied to layers, loss functions, etc. to make tensorflow more dynamic in general.
"
49894,List of Initializers are specified twice in the Classes Section of documentation,"Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/initializers#classes

## Description of issue (what needs changing):
Every **`Initializer`** has been specified twice, (`Constant`, `GlorotUniform`,etc..) and (`constant`, `glorot_uniform`, etc..).

However, they, for example [GlorotUniform](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform) and [glorot_uniform](https://www.tensorflow.org/api_docs/python/tf/keras/initializers/GlorotUniform), point to the same link.

Is there any specific reason for this redundancy?"
49893,Probability is mismatching  for the image classification example,"used same code what given in the below url ,
https://www.tensorflow.org/tutorials/images/classification

created the .tflite for Android and imported the tflite but while executing the code Probability is not matching with the python code.

Percentage in python is ,
This image most likely belongs to sunflowers with a **95.65** percent confidence. 

The android code:
val model = Model.newInstance(this)
        val bitmap: Bitmap =
            BitmapFactory.decodeResource(resources, R.drawable.image)
// Creates inputs for reference.
        val image = TensorImage.fromBitmap(bitmap)

// Runs model inference and gets result.
        val outputs = model.process(image)
        val probability = outputs.probabilityAsCategoryList

// Releases model resources if no longer used.
        model.close()

        val outputsProbability = probability.apply {
            sortByDescending { it.score } // Sort with highest confidence first
        }.take(5)

        val items = mutableListOf<Recognition>()
        for (output in outputsProbability) {
            items.add(Recognition(output.label, output.score))
        }

        Log.d(""propability"", items.toString())
        
o/p:
D/propability: [dandelion / 554.4%, daisy / 61.7%, roses / -75.3%, tulips / -282.9%, sunflowers / **-426.3**%]"
49891,TensorflowJS Issue : Looking for previous version of cusolver64_10,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlowJS version: 3.6.1
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version: 11.3
- GPU model and memory: RTX 2060 MAXQ
**Describe the problem**
GPU build is looking for cusolver64_10 instead of latest cusolver64_11 version"
49890,Documentation for Model.trainable_weights is missing,"## URL(s) with the issue:

Please provide a link to the documentation entry, for example:
https://www.tensorflow.org/api_docs/python/tf/keras/Model

## Description of issue (what needs changing): 
**`model.trainable_weights`** returns the **`Weights`** and **`Biases`** of all the **`Layers`** of the **`Model`**. Documentation to that function is missing in [Tensorflow.org](https://www.tensorflow.org/api_docs/python/tf/keras/Model)."
49889,PiecewiseConstantDecay doesn't work with Wrapping Optimizer on GPUs,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 20.04
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5/2.6
- Python version: 3.8
- Bazel version (if compiling from source): NA
- GCC/Compiler version (if compiling from source): NA
- CUDA/cuDNN version: 11.2
- GPU model and memory: 8.2

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

When using the PiecewiseConstantDecay for an optimizer and wrapping optimizer, like:
```python
lr_fn = PiecewiseConstantDecay()
opt = SGD(lr_fn)
opt = WrapOpt(opt)
```
We will hit the error of
```
InvalidArgumentError: Cannot assign a device for operation sequential_1/dense_1/Tensordot/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential_1/dense_1/Tensordot/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices 
```
Note, this error seems to be hit only when PiecewiseConstantDecay schedule is used on GPUs.

**Describe the expected behavior**

We shouldn't see such error when using PiecewiseConstantDecay on GPUs.

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

Below is the colab link and please repro it with ***Runtime=GPU***.
https://colab.research.google.com/drive/1QPx4IqQNVpSR-ALfPYbjJjRUffyHo06G?usp=sharing

```python
import tensorflow as tf
from tensorflow.keras import layers, optimizers, models
print(tf.__version__)
class OptimizerWrapper(optimizers.Optimizer):
  def __init__(self, optimizer, name=None, **kwargs):
    super(OptimizerWrapper, self).__init__(name, **kwargs)
    self._optimizer = optimizer

  def _create_slots(self, var_list):
    self._optimizer._create_slots(var_list)

  def _resource_apply_dense(self, grad, var):
    return self._optimizer._resource_apply_dense(grad, var)

  def _resource_apply_sparse(self, grad, var):
    return self._optimizer._resource_apply_sparse(grad, var)

  def get_config(self):
    return self._optimizer.get_config()


model = tf.keras.Sequential()
model.add(layers.Dense(8))
x = tf.constant(12., shape=(5, 1, 2, 4))
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)
#learning_rate_fn = optimizers.schedules.ExponentialDecay(
#    0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
#learning_rate_fn = optimizers.schedules.PolynomialDecay(
#    0.1, 10000, 0.01, power=0.5)
opt = optimizers.SGD(learning_rate=learning_rate_fn, momentum=1.0)
opt = OptimizerWrapper(opt)

@tf.function
def train_step(x):
  with tf.GradientTape(persistent=True) as tape:
    y = model(x)
    loss = tf.reduce_mean(y)

  grads = tape.gradient(loss, model.variables)
  opt.apply_gradients(zip(grads, model.variables))
  return loss

for i in range(3):
  loss = train_step(x)
  print(""Loss:"", loss)
```

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
```
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-2-354cdd24a945> in <module>()
     45 
     46 for i in range(3):
---> 47   loss = train_step(x)
     48   print(""Loss:"", loss)
     49 

5 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError: Cannot assign a device for operation sequential_1/dense_1/Tensordot/ReadVariableOp: Could not satisfy explicit device specification '' because the node {{colocation_node sequential_1/dense_1/Tensordot/ReadVariableOp}} was colocated with a group of nodes that required incompatible device '/job:localhost/replica:0/task:0/device:GPU:0'. All available devices [/job:localhost/replica:0/task:0/device:CPU:0, /job:localhost/replica:0/task:0/device:GPU:0]. 
Colocation Debug Info:
Colocation group had the following types and supported devices: 
Root Member(assigned_device_name_index_=1 requested_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' assigned_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' resource_device_name_='/job:localhost/replica:0/task:0/device:GPU:0' supported_device_types_=[CPU] possible_devices_=[]
```

cc. @nluehr "
49870,TF lite issue,"Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, LOGISTIC, MAX_POOL_2D. Here is a list of operators for which you will need custom implementations: IdentityN.
Traceback (most recent call last):
  File ""/home/skycope/venv_py36_gen/bin/toco_from_protos"", line 8, in <module>
    sys.exit(main())
  File ""/home/skycope/venv_py36_gen/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 89, in main
    app.run(main=execute, argv=[sys.argv[0]] + unparsed)
  File ""/home/skycope/venv_py36_gen/lib/python3.6/site-packages/tensorflow_core/python/platform/app.py"", line 40, in run
    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)
  File ""/home/skycope/venv_py36_gen/lib/python3.6/site-packages/absl/app.py"", line 303, in run
    _run_main(main, args)
  File ""/home/skycope/venv_py36_gen/lib/python3.6/site-packages/absl/app.py"", line 251, in _run_main
    sys.exit(main(argv))
  File ""/home/skycope/venv_py36_gen/lib/python3.6/site-packages/tensorflow_core/lite/toco/python/toco_from_protos.py"", line 52, in execute
    enable_mlir_converter)
Exception: We are continually in the process of adding support to TensorFlow Lite for more ops. It would be helpful if you could inform us of how this conversion went by opening a github issue at https://github.com/tensorflow/tensorflow/issues/new?template=40-tflite-op-request.md
 and pasting the following:

Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If those are native TensorFlow operators, you might be able to use the extended runtime by passing --enable_select_tf_ops, or by setting target_ops=TFLITE_BUILTINS,SELECT_TF_OPS when calling tf.lite.TFLiteConverter(). Otherwise, if you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.lite.TFLiteConverter(). Here is a list of builtin operators you are using: AVERAGE_POOL_2D, CONV_2D, DEPTHWISE_CONV_2D, FULLY_CONNECTED, LOGISTIC, MAX_POOL_2D. Here is a list of operators for which you will need custom implementations: IdentityN.
"
49869,"failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error : Ubuntu 20.04.2, RTX 2070 SUPER GPU ","
**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 20.04.2 LTS
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): binary
- TensorFlow version: 2.5.0
- Python version: 3.7.10
- Installed using virtualenv? pip? conda?: pip
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source): 9.3.0
- CUDA/cuDNN version: 11.3
- GPU model and memory: NVIDIA Corporation TU104 [GeForce RTX 2070 SUPER] / 8GB

I have been receiving the ""E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error""  for running the simplest tensorflow command of creating a constant. The commanf execute successfuly, but I could see tensorflow using only CPU for this. I have also checked this by a running a simple script that comapres the time of execution of a compute problem between GPU and CPU in which case same error message and also the time of computation for both CPU and GPU came out to be same. I feel tensorflow is not able to detect the GPU in this system.
The tensorflow and CUDA installation were based on the instructions provided in this link ""https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local"" which are follows,
1. wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin
2. sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600
3. wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.debsudo dpkg -i cuda-repo-ubuntu2004-11-3-local_11.3.1-465.19.01-1_amd64.deb
4. sudo apt-key add /var/cuda-repo-ubuntu2004-11-3-local/7fa2af80.pub
5. sudo apt-get update
6. sudo apt-get -y install cuda

I have used the following instructions to install cudnn,
1.sudo dpkg -i libcudnn8_8.2.0.53-1+cuda11.3_amd64.deb
2. sudo dpkg -i libcudnn8-dev_8.2.0.53-1+cuda11.3_amd64.deb

Following is the snapshot of output I get from creating the tensorflow constant variable,

(tkeras) smart@smart-B460MDS3H:/usr/local$ python
Python 3.7.10 (default, Feb 26 2021, 18:47:35) 
[GCC 7.3.0] :: Anaconda, Inc. on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2021-05-29 10:47:51.427226: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
>>> a = tf.constant(20)
2021-05-29 10:47:55.936327: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-05-29 10:47:55.959954: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error
2021-05-29 10:47:55.959977: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: smart-B460MDS3H
2021-05-29 10:47:55.959982: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: smart-B460MDS3H
2021-05-29 10:47:55.960023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.73.1
2021-05-29 10:47:55.960041: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.73.1
2021-05-29 10:47:55.960046: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.73.1
2021-05-29 10:47:55.960242: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
>>> tf.__version__
'2.5.0'

I am also attaching the gpu versus cpu time comparison script with this issue.
[tempScript.txt](https://github.com/tensorflow/tensorflow/files/6562795/tempScript.txt)

"
49867,"Deploy micro_speech to ESP32 , is running ,but do not get any output,how to modify?where is wrong ","I (63) boot: Chip Revision: 1
I (64) boot_comm: chip revision: 1, min. bootloader chip revision: 0
I (39) boot: ESP-IDF v4.0.2-442-g41efdb0b3 2nd stage bootloader
I (39) boot: compile time 22:29:55
I (39) boot: Enabling RNG early entropy source...
I (45) qio_mode: Enabling default flash chip QIO
I (50) boot: SPI Speed      : 80MHz
I (54) boot: SPI Mode       : QIO
I (58) boot: SPI Flash Size : 2MB
I (62) boot: Partition Table:
I (66) boot: ## Label            Usage          Type ST Offset   Length
I (73) boot:  0 nvs              WiFi data        01 02 00009000 00006000
I (81) boot:  1 phy_init         RF data          01 01 0000f000 00001000
I (88) boot:  2 factory          factory app      00 00 00010000 00100000
I (96) boot: End of partition table
I (100) boot_comm: chip revision: 1, min. application chip revision: 0
I (107) esp_image: segment 0: paddr=0x00010020 vaddr=0x3f400020 size=0x0e314 ( 58132) map
I (132) esp_image: segment 1: paddr=0x0001e33c vaddr=0x3ffb0000 size=0x01cd4 (  7380) load
I (134) esp_image: segment 2: paddr=0x00020018 vaddr=0x400d0018 size=0x23d98 (146840) map
0x400d0018: _stext at ??:?

I (178) esp_image: segment 3: paddr=0x00043db8 vaddr=0x3ffb1cd4 size=0x002ac (   684) load
I (179) esp_image: segment 4: paddr=0x0004406c vaddr=0x40080000 size=0x00400 (  1024) load
0x40080000: _WindowOverflow4 at /Users/cafe/esp-idf/components/freertos/xtensa_vectors.S:1778

I (185) esp_image: segment 5: paddr=0x00044474 vaddr=0x40080400 size=0x09a8c ( 39564) load
I (213) boot: Loaded app from partition at offset 0x10000
I (213) boot: Disabling RNG early entropy source...
I (214) cpu_start: Pro cpu up.
I (218) cpu_start: Application information:
I (222) cpu_start: Project name:     micro_speech
I (228) cpu_start: App version:      v1.12.1-57713-gda15b34d114-dirt
I (235) cpu_start: Compile time:     May 28 2021 22:29:50
I (241) cpu_start: ELF file SHA256:  f9f5b5eb91cff7e9...
I (247) cpu_start: ESP-IDF:          v4.0.2-442-g41efdb0b3
I (253) cpu_start: Starting app cpu, entry point is 0x40081064
0x40081064: call_start_cpu1 at /Users/cafe/esp-idf/components/esp32/cpu_start.c:272

I (0) cpu_start: App cpu up.
I (263) heap_init: Initializing. RAM available for dynamic allocation:
I (270) heap_init: At 3FFAE6E0 len 00001920 (6 KiB): DRAM
I (276) heap_init: At 3FFB6858 len 000297A8 (165 KiB): DRAM
I (283) heap_init: At 3FFE0440 len 00003AE0 (14 KiB): D/IRAM
I (289) heap_init: At 3FFE4350 len 0001BCB0 (111 KiB): D/IRAM
I (295) heap_init: At 40089E8C len 00016174 (88 KiB): IRAM
I (302) cpu_start: Pro cpu start user code
I (319) spi_flash: detected chip: generic
I (319) spi_flash: flash io: qio
W (319) spi_flash: Detected size(4096k) larger than the size in the binary image header(2048k). Using the size in the binary image header.
I (330) cpu_start: Starting scheduler on PRO CPU.
I (0) cpu_start: Starting scheduler on APP CPU.
init success
I (424) I2S: DMA Malloc info, datalen=blocksize=2400, dma_buf_count=3
I (424) I2S: PLL_D2: Req RATE: 16000, real rate: 16025.000, BITS: 32, CLKM: 39, BCK: 4, MCLK: 4096000.000, SCLK: 1025600.000000, diva: 64, divb: 4
W (474) TF_LITE_AUDIO_PROVIDER: mic data:3200
I (474) TF_LITE_AUDIO_PROVIDER: Audio Recording started
"
49866,recognise silence 0 ,"esp-idf 4.0
divice：esp32 eye

build  code ok and flash 

but：
recognise silence 0 

why？"
49865,TF batch gradient calculation,"I am trying to figure out the math model (or data flow) to compute gradient for batch gradient descent algorithms. All the materials I have been reading seems to say that the final gradient of a batch is the average of all gradients evaluated for all training samples of a batch, but when I check the TF loss function of a batch, I found the output of a TF loss function is a scalar value rather than (BATCH_SIZE,) -- a scalar vector of length equals to BATCH_SIZE. So I am wondering how TF calculates the gradient of a batch, does it just use this scalar loss value to calculate the gradient only ONCE for the entire batch on the trainable variables, or this loss value is used by BATCH_SIZE times to calculate the gradients for each training pairs of a batch, and then it does an average. 

Any reference is welcome. Thanks."
49864,polymorphism dtype tensor for custom op Attr?,"Can I ask how we can add polymorphism dtype tensor for custom op Attr? For example, how can we allow the below `attr` dtype to be DT_FLOAT or DT_HALF?

example:
```
REGISTER_OP(""DoStuff"")
    .Attr(""attr: tensor = { dtype: DT_FLOAT }"")
    .Input(""in: float"")
    .Output(""out: float"");
```

```
REGISTER_OP(""DoStuff"")
    .Attr(""attr: tensor = { dtype: DT_HALF }"")
    .Input(""in: float"")
    .Output(""out: float"");
```"
49862,Wrong libcudart.so loaded with TF 2.5.0,"**System information**
- OS Platform and Distribution : ubuntu 20.04
- TensorFlow installed from : binary via pip3
- TensorFlow version: 2.5.0
- Python version: 3.8.5
- Installed using : pip
- CUDA/cuDNN version: NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2  
- GPU model and memory: GeForce GTX 1070


After installing nvidia drivers, cuda and cudnn with matching versions from:
- https://www.tensorflow.org/install/source#gpu
- https://docs.nvidia.com/deploy/cuda-compatibility/index.html
- https://docs.nvidia.com/deeplearning/cudnn/support-matrix/index.html

and installing tensorflow with pip3 I get this message:
```
Python 3.8.5 (default, Jan 27 2021, 15:41:15)
[GCC 9.3.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import tensorflow as tf
2021-05-28 16:34:56.376524**: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory**
2021-05-28 16:34:56.376553: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
>>> tf.__version__
'2.5.0'
```
Why is tensorflow 2.5.0 loading cuda 11.0 but the tensorflow compatibility page says that it must work with cuda 11.2 ?


nvcc output:
```
» /usr/local/cuda/bin/nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Feb_14_21:12:58_PST_2021
Cuda compilation tools, release 11.2, V11.2.152
Build cuda_11.2.r11.2/compiler.29618528_0
```
nvidia-smi output:
Fri May 28 16:38:20 2021       
```
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 1070    Off  | 00000000:01:00.0  On |                  N/A |
|  0%   32C    P8     9W / 200W |    138MiB /  8119MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
```

"
49858,tflite model produces different results if using the CPU on a Ubuntu PC or a NPU+NNAPI on an embedded system,"Please go to Stack Overflow for help and support:

https://stackoverflow.com/questions/tagged/tensorflow

If you open a GitHub issue, here is our policy:

1.  It must be a bug, a feature request, or a significant problem with the
    documentation (for small docs fixes please send a PR instead).
2.  The form below must be filled out.
3.  It shouldn't be a TensorBoard issue. Those go
    [here](https://github.com/tensorflow/tensorboard/issues).

**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.

------------------------

### System information

-   **Have I written custom code (as opposed to using a stock example script
    provided in TensorFlow)**: no
-   **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 18.04 / Linux Yocto
-   **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue
    happens on a mobile device**:
-   **TensorFlow installed from (source or binary)**: binary
-   **TensorFlow version (use command below)**: 2.4.1
-   **Python version**: 3.6
-   **Bazel version (if compiling from source)**:
-   **GCC/Compiler version (if compiling from source)**:
-   **CUDA/cuDNN version**:
-   **GPU model and memory**:
-   **Exact command to reproduce**:


### Describe the problem
I trained a classification network and I performed quantization-aware training using Tensorflow Model Optimization. Then I converted the trained and quantized model to tflite format.
The model is supposed to be used in an embedded system running Yocto. The system is provided with a NPU and the inference is performed with the [tflite_runtime](https://www.tensorflow.org/lite/guide/python) package and the NNAPI.
The problem is that the same tflite model produces different results if tested on my PC with TF2.4.1 and on the Yocto board. The result are often very similar but there are some cases where they are completely different, resulting in an accuracy drop on the board. I don't know why this happens. Is there a way to prevent it during training? 


"
49855,Update GPU support page to reflect correct libraries required for TF 2.5.0,"
## URL(s) with the issue:

https://www.tensorflow.org/install/gpu

## Description of issue (what needs changing):

The documentation has not been updated to state the correct versions of cuda, cudnn and nvinfer that the current stable Tensorflow version (2.5.0) requires.

For example in the Ubuntu 18.04 configuration, instead of:

```bash
# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-11-0 \
    libcudnn8=8.0.4.30-1+cuda11.0  \
    libcudnn8-dev=8.0.4.30-1+cuda11.0
```

It should be something like:

```bash
# Install development and runtime libraries (~4GB)
sudo apt-get install --no-install-recommends \
    cuda-11-2 \
    libcudnn8=8.1.0.77-1-1+cuda11.2  \
    libcudnn8-dev=8.1.0.77-1+cuda11.2
```

It's also particularly unclear which version of the nvinfer libraries are required to enable TF-TRT optimizations since it's also not stated in the [Build from source](https://www.tensorflow.org/install/source) page and it's also not obvious how to check that TRT is enabled.
 "
49854,tensorflowlite quantized model get incorrect output in tensorflowlite C API,"@tensorflow/micro

**System information**
- Host OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04
- TensorFlow installed from (source or binary): 
- Tensorflow version (commit SHA if source): r2.4
- Target platform (e.g. Arm Mbed OS, Arduino Nano 33 etc.): Qualcomm Snapdragon XR2 

**Describe the problem**
I have two tflite model, one is quantized to float16 the other is quantized to full integer. Both model can get correct result on python for PC. But only model quantized to float16 get correct result on tensorflowlite C API. Model quantized to int8 with different input always get the same output. Both model run for CPU on Qualcomm Snapdragon XR2.

Model quantized to float 16: [link](https://drive.google.com/file/d/119wMGk2Z-Kx4o-Zxji4h-KOmDVuwG3Rq/view?usp=sharing)
Model quantized to full integer: [link](https://drive.google.com/file/d/1oEHtKag7AM_FFNkgyIGg2PsAGt3zERKm/view?usp=sharing)

Thanks
"
49853,"React native  runModelOnImage have an error ""Cannot convert between a TensorFlowLite tensor with type UINT8 and a Java object of type [[[[F (which is compatible with the TensorFlowLite type FLOAT32)""","When I training data  and export to Tflite on centos7 using Anaconda3 with Tensorflow, it have warning
![image](https://user-images.githubusercontent.com/44739285/119945247-63dcb700-bfbf-11eb-8730-2d50d6bedf12.png)
The [model.tflite](https://drive.google.com/file/d/1L5KFlHSv1NajUM5Ltpp354O-i8AzKAkH/view?usp=sharing) and [labels.txt](https://drive.google.com/file/d/1eXDEO81Ach45qJTryBWX_O0pupS61GiW/view?usp=sharing) after export.
Below is my code and the datasets [here](https://drive.google.com/file/d/1SE8DJNU8ctaim2BptUMWZ3dkxPPVSpfT/view?usp=sharing).
```python
import os
import numpy as np
import tensorflow as tf
assert tf.__version__.startswith('2')

from tflite_model_maker import configs
from tflite_model_maker import ExportFormat
from tflite_model_maker import image_classifier
from tflite_model_maker import model_spec
from tflite_model_maker import ImageClassifierDataLoader
import matplotlib.pyplot as plt

data = ImageClassifierDataLoader.from_folder('images')
train_data, rest_data = data.split(0.8)
validation_data, test_data = rest_data.split(0.5)
plt.figure(figsize=(10,10))
for i, (image, label) in enumerate(data.gen_dataset().unbatch().take(25)):
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(image.numpy(), cmap=plt.cm.gray)
  plt.xlabel(data.index_to_label[label.numpy()])
plt.show()

model = image_classifier.create(train_data, validation_data=validation_data)
model.summary()
loss, accuracy = model.evaluate(test_data)
def get_label_color(val1, val2):
  if val1 == val2:
    return 'black'
  else:
    return 'red'
plt.figure(figsize=(20, 20))
predicts = model.predict_top_k(test_data)
for i, (image, label) in enumerate(test_data.gen_dataset().unbatch().take(100)):
  ax = plt.subplot(10, 10, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(image.numpy(), cmap=plt.cm.gray)

  predict_label = predicts[i][0][0]
  color = get_label_color(predict_label,
                          test_data.index_to_label[label.numpy()])
  ax.xaxis.label.set_color(color)
  plt.xlabel('Predicted: %s' % predict_label)
plt.show()
model.export(export_dir='.')
model.export(export_dir='./duy_tree', export_format=ExportFormat.LABEL)
```

Same code above but running on window, after export tflite model can be used.
Help me, please!"
49852,Why is profiler dir deleted for non-chief nodes in MWMS in keras?,"Hi,
Based on the keras tensorboard callback: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2472
The profiler dir of non-chief node is set to a temp dir when MWMS is being used, which will be deleted later on:
https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/callbacks.py#L2227
https://github.com/tensorflow/tensorflow/blob/da15b34d11432b789527e4a3c3ad4eac5193a5f0/tensorflow/python/keras/callbacks.py#L2256

I understand checkpoints for non-chief nodes should be cleaned up, but why profiling result? 
Happy to contribute a PR if this is a bug. 

CC @jbaiocchi  @omalleyt12

"
49850,Tensorflow build failed with error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus' on s390x,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 8.1 on s390x platform
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version: r2.3
- Python version: 3.7
- Installed using virtualenv? pip? conda?: 
- Bazel version (if compiling from source):3.5.0
- GCC/Compiler version (if compiling from source): gcc-toolset-9
- CUDA/cuDNN version:
- GPU model and memory:



**Describe the problem**
Compilation of source code failed with the following error message

```
ERROR: /tmp/source/tensorflow/tensorflow/core/kernels/BUILD:5530:18: C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1)
In file included from ./tensorflow/core/framework/allocator.h:26,
                 from ./tensorflow/core/framework/op_kernel.h:24,
                 from tensorflow/core/kernels/sparse_split_op.cc:19:
./tensorflow/core/framework/numeric_types.h: In function 'tensorflow::bfloat16 FloatToBFloat16(float)':
./tensorflow/core/framework/numeric_types.h:51:13: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
     return *reinterpret_cast<tensorflow::bfloat16*>(
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
         reinterpret_cast<uint16_t*>(&float_val));
         ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
./tensorflow/core/framework/numeric_types.h:51:13: warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing]
In file included from ./tensorflow/core/framework/op_kernel.h:35,
                 from tensorflow/core/kernels/sparse_split_op.cc:19:
tensorflow/core/kernels/sparse_split_op.cc: In member function 'void tensorflow::SparseSplitOp<T>::Compute(tensorflow::OpKernelContext*)':
tensorflow/core/kernels/sparse_split_op.cc:71:34: error: 'class tensorflow::TensorShape' has no member named 'AddDimWithStatus'
                      dense_shape.AddDimWithStatus(input_shape_flat(i)));
                                  ^~~~~~~~~~~~~~~~
./tensorflow/core/framework/op_requires.h:52:29: note: in definition of macro 'OP_REQUIRES_OK'
     ::tensorflow::Status _s(__VA_ARGS__);                    \
                             ^~~~~~~~~~~
Target //tensorflow/tools/pip_package:build_pip_package failed to build
Use --verbose_failures to see the command lines of failed build steps.
ERROR: /tmp/source/tensorflow/tensorflow/python/tools/BUILD:82:10 C++ compilation of rule '//tensorflow/core/kernels:sparse_split_op' failed (Exit 1)
INFO: Elapsed time: 5244.642s, Critical Path: 263.52s
INFO: 5985 processes: 5985 local.
FAILED: Build did NOT complete successfully
```
**Provide the exact sequence of commands / steps that you executed before running into the problem**

```
yum -y install atlas-devel gcc-toolset-9 python3-scipy python36-devel python3-scipy python3-pip pkgconf-pkg-config zip unzip libaec-deve libtool wget git hdf5-dev openssl-devel 

pip3 install cython wheel numpy scipy virtualenv future h5py portpicker wheel protobuf keras_preprocessing 
export GRPC_PYTHON_BUILD_SYSTEM_OPENSSL=True
pip3 install grpcio

```
Build bazel 3.5.0
```
mkdir -p $SOURCE_ROOT \
    && cd $SOURCE_ROOT \
    && mkdir bazel \
    && cd bazel \
    && wget https://github.com/bazelbuild/bazel/releases/download/3.5.0/bazel-3.5.0-dist.zip \
    && unzip bazel-3.5.0-dist.zip
RUN cd $SOURCE_ROOT/bazel \
    && chmod -R +w . \
# Add patch to resolve java oom issue
    && sed -i ""147s/-classpath/-J-Xms16g -J-Xmx16g -classpath/"" scripts/bootstrap/compile.sh

cd $SOURCE_ROOT/bazel \
    && env EXTRA_BAZEL_ARGS=""--host_javabase=@local_jdk//:jdk"" bash ./compile.sh
```

Build Tensorflow
```
cd $SOURCE_ROOT \
    && git clone https://github.com/tensorflow/tensorflow.git \
    && cd tensorflow \
    && git checkout r2.3 \
# Configure
    && yes """" | ./configure
# Build TensorFlow
 cd $SOURCE_ROOT/tensorflow \
    && bazel --host_jvm_args=""-Xms16g"" --host_jvm_args=""-Xmx16g"" build \
       --define=tensorflow_mkldnn_contraction_kernel=0 --copt=-Wno-maybe-uninitialized --copt=-mzvector --copt=-funroll-loops --copt=-march=z14 --color=yes --curses=yes \
       //tensorflow/tools/pip_package:build_pip_package
```
**Any other info / logs**

Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.
"
49847,tf.image.resize behavior,"not working code
`
@tf.function
def read_dicom(sample_dicom_path):

	image_file_bytes=tf.io.read_file(sample_dicom_path)
	image=tfio.image.decode_dicom_image(image_file_bytes, dtype=tf.float32)
	
	image = tf.squeeze(image)
	image = tf.stack([image, image, image], axis=-1)

	size=tf.cast(tf.constant([224,224]), tf.int32)
	image = tf.image.resize(image, size)

	return image`

image=read_dicom('sample.dcm')

Running this the code yields the following error message:ValueError: 
![Screen Shot 2021-05-27 at 5 50 45 PM](https://user-images.githubusercontent.com/58089750/119913957-10f9f580-bf14-11eb-994d-570386f63fc3.png)



"
49841,ImportError: DLL load failed: The specified module could not be found.,"<em>Please make sure that this is a build/installation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:build_template</em>

**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): windows version 20h2
- Python version:  3.7.7
- Installed using conda

Trying to run python in terminal, already done:

`(base) C:\Users\Yinqi\trRosetta2\trRosetta>conda create -n tf tensorflow`

But ran into this error:

`ImportError: DLL load failed: The specified module could not be found. Failed to load the native TensorFlow runtime.`

Full traceback:

```
File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
        from tensorflow.python._pywrap_tensorflow_internal import *
    ImportError: DLL load failed: The specified module could not be found.
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File ""predict.py"", line 6, in <module>
        import tensorflow as tf
      File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\__init__.py"", line 41, in <module>
        from tensorflow.python.tools import module_util as _module_util
      File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\python\__init__.py"", line 40, in <module>
        from tensorflow.python.eager import context
      File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in <module>
        from tensorflow.python import pywrap_tfe
      File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in <module>
        from tensorflow.python import pywrap_tensorflow
      File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in <module>
        raise ImportError(msg)
    ImportError: Traceback (most recent call last):
      File ""C:\Users\Yinqi\anaconda3\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in <module>
        from tensorflow.python._pywrap_tensorflow_internal import *
    ImportError: DLL load failed: The specified module could not be found.
```"
49836,Faild Build 2.5.0 Windows,"**System information**
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- TensorFlow installed from (source or binary): source 
- TensorFlow version: 2.5.0
- Python version: 3.8
- Installed using virtualenv? pip? conda?: Conda x64
- Bazel version (if compiling from source): 3.7.2
- CUDA/cuDNN version: 11.3/8.2.0
- GPU model and memory: 3060

Problems arise only when building 2.5.0. Build version 2.4.1 is not problems

Error
```
ERROR: F:/tensorflow/tensorflow/python/util/BUILD:609:27: C++ compilation of rule '//tensorflow/python/util:fast_module_type.so' failed (Exit 2): python.exe failed: error executing command
  cd C:/users/X/_bazel_X/2qttxlm7/execroot/org_tensorflow
  SET CUDA_TOOLKIT_PATH=C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.3
    SET INCLUDE=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\ATLMFC\include;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\include;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\include\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\ucrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\shared;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\um;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\winrt;C:\Program Files (x86)\Windows Kits\10\include\10.0.19041.0\cppwinrt
    SET LIB=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\ATLMFC\lib\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\lib\x64;C:\Program Files (x86)\Windows Kits\NETFXSDK\4.8\lib\um\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\ucrt\x64;C:\Program Files (x86)\Windows Kits\10\lib\10.0.19041.0\um\x64
    SET PATH=C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\\Extensions\Microsoft\IntelliCode\CLI;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Tools\MSVC\14.28.29910\bin\HostX64\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\VCPackages;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TestWindow;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\TeamFoundation\Team Explorer;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\MSBuild\Current\bin\Roslyn;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Team Tools\Performance Tools;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\\x64;C:\Program Files (x86)\Microsoft Visual Studio\Shared\Common\VSPerfCollectionTools\vs2019\;C:\Program Files (x86)\Microsoft SDKs\Windows\v10.0A\bin\NETFX 4.8 Tools\x64\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\FSharp\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\devinit;C:\Program Files (x86)\Windows Kits\10\bin\10.0.19041.0\x64;C:\Program Files (x86)\Windows Kits\10\bin\x64;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\\MSBuild\Current\Bin;C:\WINDOWS\Microsoft.NET\Framework64\v4.0.30319;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\Tools\;;C:\WINDOWS\system32;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\CMake\bin;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\CommonExtensions\Microsoft\CMake\Ninja;C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\Common7\IDE\VC\Linux\bin\ConnectionManagerExe
    SET PWD=/proc/self/cwd
    SET PYTHON_BIN_PATH=C:/Anaconda3/python.exe
    SET PYTHON_LIB_PATH=C:/Anaconda3/lib/site-packages
    SET RUNFILES_MANIFEST_ONLY=1
    SET TEMP=C:\Users\X\AppData\Local\Temp
    SET TF2_BEHAVIOR=1
    SET TF_CUDA_COMPUTE_CAPABILITIES=8.6
    SET TMP=C:\Users\X\AppData\Local\Temp
  C:/Anaconda3/python.exe -B external/local_config_cuda/crosstool/windows/msvc_wrapper_for_nvcc.py /nologo /DCOMPILER_MSVC /DNOMINMAX /D_WIN32_WINNT=0x0600 /D_CRT_SECURE_NO_DEPRECATE /D_CRT_SECURE_NO_WARNINGS /D_SILENCE_STDEXT_HASH_DEPRECATION_WARNINGS /bigobj /Zm500 /J /Gy /GF /EHsc /wd4351 /wd4291 /wd4250 /wd4996 /I. /Ibazel-out/x64_windows-opt/bin /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/com_google_absl /Ibazel-out/x64_windows-opt/bin/external/com_google_absl /Iexternal/nsync /Ibazel-out/x64_windows-opt/bin/external/nsync /Iexternal/local_config_python /Ibazel-out/x64_windows-opt/bin/external/local_config_python /Iexternal/pybind11 /Ibazel-out/x64_windows-opt/bin/external/pybind11 /Iexternal/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/bazel_tools /Ibazel-out/x64_windows-opt/bin/external/pybind11/_virtual_includes/pybind11 /Ithird_party/eigen3/mkl_include /Ibazel-out/x64_windows-opt/bin/third_party/eigen3/mkl_include /Iexternal/eigen_archive /Ibazel-out/x64_windows-opt/bin/external/eigen_archive /Iexternal/nsync/public /Ibazel-out/x64_windows-opt/bin/external/nsync/public /Iexternal/local_config_python/python_include /Ibazel-out/x64_windows-opt/bin/external/local_config_python/python_include /Iexternal/pybind11/include /Ibazel-out/x64_windows-opt/bin/external/pybind11/include /DEIGEN_MPL2_ONLY /DEIGEN_MAX_ALIGN_BYTES=64 /showIncludes /MD /O2 /DNDEBUG /W0 /D_USE_MATH_DEFINES -DWIN32_LEAN_AND_MEAN -DNOGDI /experimental:preprocessor /d2ReducedOptimizeHugeFunctions /arch:AVX /std:c++14 -fno-strict-aliasing -fexceptions /Fobazel-out/x64_windows-opt/bin/tensorflow/python/util/_objs/fast_module_type.so/fast_module_type.obj /c tensorflow/python/util/fast_module_type.cc
```"
49835,TFLite on GPU does not support quantised models by default,"## URL(s) with the issue:

https://www.tensorflow.org/lite/performance/gpu_advanced

## Description of issue (what needs changing):

The documentation reads: ""*Android APIs support quantized models by default. To disable, do the following:*""

However, when I try running my int8 quantised model on mobile GPU, it silently defaults to running on CPU. I'm importing the nightly version as below in build.gradle:
```
dependencies {
    implementation ""androidx.appcompat:appcompat:1.1.0""

    implementation ""org.tensorflow:tensorflow-lite:0.0.0-nightly""
    implementation ""org.tensorflow:tensorflow-lite-support:0.0.0-nightly""
    implementation ""org.tensorflow:tensorflow-lite-gpu:0.0.0-nightly""
}
```

And when I do the below in my Android activity:
```
gpuOptions.setQuantizedModelsAllowed(true);
```
it successfully runs on GPU (confirmed by looking at GPU usage, also it runs much faster on GPU). 

This suggests that the Android APIs actually DON'T support quantised models on GPU by default, and the flag needs to be set manually.
"
49834,Use image orientation tag in exif data,"<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>


**System information**
- TensorFlow version (you are using): 1.15, 2.4
- Are you willing to contribute it (Yes/No): Yes



**Describe the feature and the current behavior/state.**

Currently tensorflow will use images without rotating them per the orientation flag in the exif data. This can result in bounding boxes in annotation files being placed in the wrong locations when Tensorflow loads the image. This can be _very_ difficult to find and diagnose, even if using a utility like Tensorboard to view eval images and training statistics as you need to luck out and have it show an affected image where the groundtruth boxes are in the wrong place. Further compounding this issue is that most annotation and image viewing tools _will_ use the orientation flag to rotate images accordingly, making finding and correcting affected images an extremely painful and time consuming activity.

Currently my workaround is to strip all exif data from each image as a quick-and-dirty way of ensuring that I'm annotating the image exactly as tensorflow will load it. I lucked out that I found out about this while training on a fairly small custom dataset because correcting this _after_ annotating images involves finding any images with an orientation tag and resaving a rotated image without the exif data before rebuilding the tfrecords file with the new images. While building the tools necessary to do this isn't that difficult it could be entirely unneeded if tensorflow simply rotated the images correctly when loading, the way labelImg, labelme, and all other annotation tools do.
 
**Will this change the current api? How?**

Usage of the api will be unaffected, images will simply be loaded with the correct orientation without the need for extra input from the user

**Who will benefit with this feature?**

Anyone who uses images that contain an orientation flag other than 1 in the exif data

**Any Other info.**

-
"
49833,sched_getaffinity fails on systems with more than 1024 cores,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): 2.5.0
- Python version: 3.9

**Describe the current behavior**

On doing `import tensorflow` a warning is shown 2 times:

```
sched_getaffinity: Invalid argument
can't determine number of CPU cores: assuming 4
```

**Describe the expected behavior**

TF works correctly on large systems

**Other info / logs**

The reason is, that the default struct for the CPU set contains only space for 1024 cores, so it will fail with EINVAL on systems with more cores, even when they are inactive.
The solution is more or less simple, by using the macros of glibc to allocate a larger struct. See e.g. the CPython os Module implementation."
49832,MLIR-based TFLite converter - 16bit fake quantization,"### 1. System information

- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 18.04.5
- TensorFlow installation (pip package or built from source): 2.4.0
- TensorFlow library (version, if pip package or github SHA, if built from source): pip package

### 2. Code

Provide code to help us reproduce your issues using one of the following options:
https://colab.research.google.com/drive/1Adz4A66PjnsvPigUi3cRL21C2aEe_dMI?usp=sharing

Error during tflite conversion:
Failed to convert element type '!quant.uniform<u16:f32, 6.1036087586785687E-5>': 'isSigned' can only be set for 8-bits integer type


Hi,
The model conversion works fine when using TOCO converter (which is deprecated) but fails when using MLIR-based TFLite converter.
It seems that fake-quant can be converted only when it quantizes using 8 bits...
Any ideas?

Thanks!


"
49831,Error while trying to run train.py on Experiencor's yolov3 with keras project,"The git: 
https://github.com/experiencor/keras-yolo3

The execution:
[full_run.txt](https://github.com/tensorflow/tensorflow/files/6553076/full_run.txt)

The error: 
[error.txt](https://github.com/tensorflow/tensorflow/files/6553063/error.txt)

"
49829,image_dataset_from_directory returns dtype 'uint8' Dataset if interpolation='nearest',"
## URL(s) with the issue:
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory

### Clear description
tf.keras.preprocessing.image_dataset_from_directory returns dtype 'uint8' Dataset if interpolation='nearest'
however function docstring reports misleading:

### Returns defined

Returns:    A `tf.data.Dataset` object.      - If `label_mode` is None, it yields `float32` tensors of shape        `(batch_size, image_size[0], image_size[1], num_channels)`,        encoding images (see below for rules regarding `num_channels`).      - Otherwise, it yields a tuple `(images, labels)`, where `images`        has shape `(batch_size, image_size[0], image_size[1], num_channels)`,        and `labels` follows the format described below.

[...]
## Description of issue (what needs changing):

return dtype 'uint8' for interpolation='nearest should be specified, such as:


Returns:    A `tf.data.Dataset` object.      - If `label_mode` is None, it yields `float32` tensors of shape        `(batch_size, image_size[0], image_size[1], num_channels)`,        encoding images (see below for rules regarding `num_channels`).     
 - If `label_mode` is None and interpolation is 'bilinear' it yields `uint8`.
 - Otherwise, it yields a tuple `(images, labels)`, where `images`        has shape `(batch_size, image_size[0], image_size[1], num_channels)`,        and `labels` follows the format described below.

[...]


"
49828,What is the purpose of autoshardpolicy when it actually gets the same outcome when it is turned off?,"## URL(s) with the issue: https://www.tensorflow.org/tutorials/distribute/input

## Description of issue (what needs changing):
the docs said when you set `options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF` , 
each worker will process all the data.
However, in my experimentation, tf.data.experimental.AutoShardPolicy.OFF gets the same results(in terms of how much data a worker will process) as tf.data.experimental.AutoShardPolicy.DATA
So what is purpose of autoshardpolicy? or do I have any misunderstanding here?

codes:
```
import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()
print(f'using distribution strategy\nnumber of gpus:{strategy.num_replicas_in_sync}')
tf2_dataset=tf.data.Dataset.from_tensor_slices(range(10))
tf2_dataset=tf2_dataset.shuffle(10)
tf2_dataset=tf2_dataset.batch(3)
options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF
tf2_dataset=tf2_dataset.with_options(options)
dt=strategy.experimental_distribute_dataset(tf2_dataset)
for i,j in enumerate(dt):
    print(f'batch {i}:')
    print(j)
```
number of gpus:2
batch 0:
PerReplica:{
  0: tf.Tensor([0 7], shape=(2,), dtype=int32),
  1: tf.Tensor([1], shape=(1,), dtype=int32)
}
batch 1:
PerReplica:{
  0: tf.Tensor([3 9], shape=(2,), dtype=int32),
  1: tf.Tensor([6], shape=(1,), dtype=int32)
}
batch 2:
PerReplica:{
  0: tf.Tensor([5 2], shape=(2,), dtype=int32),
  1: tf.Tensor([8], shape=(1,), dtype=int32)
}
batch 3:
PerReplica:{
  0: tf.Tensor([4], shape=(1,), dtype=int32),
  1: tf.Tensor([], shape=(0,), dtype=int32)
}
```
import tensorflow as tf
strategy = tf.distribute.MirroredStrategy()
print(f'using distribution strategy\nnumber of gpus:{strategy.num_replicas_in_sync}')
tf2_dataset=tf.data.Dataset.from_tensor_slices(range(10))
tf2_dataset=tf2_dataset.shuffle(10)
tf2_dataset=tf2_dataset.batch(3)
options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
tf2_dataset=tf2_dataset.with_options(options)
dt=strategy.experimental_distribute_dataset(tf2_dataset)
for i,j in enumerate(dt):
    print(f'batch {i}:')
    print(j)
```
number of gpus:2
batch 0:
PerReplica:{
  0: tf.Tensor([6 1], shape=(2,), dtype=int32),
  1: tf.Tensor([2], shape=(1,), dtype=int32)
}
batch 1:
PerReplica:{
  0: tf.Tensor([9 4], shape=(2,), dtype=int32),
  1: tf.Tensor([8], shape=(1,), dtype=int32)
}
batch 2:
PerReplica:{
  0: tf.Tensor([3 0], shape=(2,), dtype=int32),
  1: tf.Tensor([7], shape=(1,), dtype=int32)
}
batch 3:
PerReplica:{
  0: tf.Tensor([5], shape=(1,), dtype=int32),
  1: tf.Tensor([], shape=(0,), dtype=int32)
}

"
49826,RTX 3090 training custom data stuck on first epoch,"**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A
- TensorFlow installed from (source or binary): binary
- TensorFlow version (use command below): v1.12.1-54779-g1236f723c6d 2.6.0-dev20210413
- Python version: 3.8.8
- Bazel version (if compiling from source): N/A
- GCC/Compiler version (if compiling from source): N/A
- CUDA/cuDNN version: CUDA 11.1/CUDNN 8.1.0
- GPU model and memory: Nvidia RTX 3090 64GB

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
I used to run the code on Colab and it run successfully without any errors. I want to copy this code on my new computer.
But when I run the code, it will stuck on the first epoch and never continue.
I want to know what cause this problem, I can't see any errors on the log.

**Describe the expected behavior**
Run the training code successfully with GPU RTX 3090 and tensorflow.

**Standalone code to reproduce the issue**

**Other info / logs** Include any logs or source code that would be helpful to
2021-05-27 14:03:20.253134: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
  1/221 [..............................] - ETA: 1:53:26 - loss: 1.8405 - accuracy: 0.12502021-05-27 14:03:27.764972: I tensorflow/core/profiler/lib/profiler_session.cc:126] Profiler session initializing.
2021-05-27 14:03:27.765052: I tensorflow/core/profiler/lib/profiler_session.cc:141] Profiler session started.


"
49824,/usr/local/cuda/bin/ptxas --version returned -1,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):
   ```
   Linux 5.4.0-1048-aws #50~18.04.1-Ubuntu SMP Tue May 4 17:40:02 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux
   ```
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary):
- TensorFlow version (use command below):
   ```
   tensorflow-gpu         2.2.0
   ```
- Python version:
   ```
   Python 3.6.9
   ```
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
   ```
   nvcc: NVIDIA (R) Cuda compiler driver
   Copyright (c) 2005-2019 NVIDIA Corporation
   Built on Sun_Jul_28_19:07:16_PDT_2019
   Cuda compilation tools, release 10.1, V10.1.243
   ```
- GPU model and memory:
   ```
   StreamExecutor device (0): Tesla V100-SXM2-16GB, Compute Capability 7.0
   ```

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**

In the TF logs i can see:
```
2021-05-27 02:41:38.569570: W tensorflow/stream_executor/gpu/asm_compiler.cc:81] Running /usr/local/cuda/bin/ptxas --version returned -1
2021-05-27 02:41:38.651452: W tensorflow/stream_executor/gpu/redzone_allocator.cc:314] Internal: ptxas exited with non-zero error code -1, output:
Relying on driver to perform ptx compilation.
Modify $PATH to customize ptxas location.
This message will be only logged once.
```

**which cause the first request take insanely long, like 1min+,**

but within the container i check the and get:
```
root@3584ebf46aa4:/centaur# which ptxas
/usr/local/cuda/bin/ptxas
root@3584ebf46aa4:/centaur# ptxas --version
ptxas: NVIDIA (R) Ptx optimizing assembler
Copyright (c) 2005-2019 NVIDIA Corporation
Built on Sun_Jul_28_19:06:54_PDT_2019
Cuda compilation tools, release 10.1, V10.1.243
```

So get lost here, why TF get `-1` for the command `/usr/local/cuda/bin/ptxas --version`, think it could be a bug.

**Describe the expected behavior**

**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.
"
49823,AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' when using keras,"<em>Please make sure that this is a bug. As per our
[GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md),
we only address code/doc bugs, performance issues, feature requests and
build/installation issues on GitHub. tag:bug_template</em>

**System information**
- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No
- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10
- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:
- TensorFlow installed from (source or binary): Google colab
- TensorFlow version (use command below): 1.15.0
- Python version: 3.7.10
- Bazel version (if compiling from source):
- GCC/Compiler version (if compiling from source):
- CUDA/cuDNN version:
- GPU model and memory: colab gpu

You can collect some of this information using our environment capture
[script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)
You can also obtain the TensorFlow version with:
1. TF 1.0: `python -c ""import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)""`
2. TF 2.0: `python -c ""import tensorflow as tf; print(tf.version.GIT_VERSION, tf.version.VERSION)""`

**Describe the current behavior**
Getting the error AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__' when importing keras. Has only been happening very recently, although no changes to code were made during the time between.
**Describe the expected behavior**
Python file to run normally
**[Contributing](https://www.tensorflow.org/community/contribute)** - Do you
want to contribute a PR? (yes/no): - Briefly describe your candidate solution
(if contributing):

**Standalone code to reproduce the issue**
Provide a reproducible test case that is the bare minimum necessary to generate
the problem. If possible, please share a link to Colab/Jupyter/any notebook.
[https://colab.research.google.com/drive/1e3d5U85ijexXILaKx2BXjAJ8Z010fQ2X?usp=sharing](https://colab.research.google.com/drive/1e3d5U85ijexXILaKx2BXjAJ8Z010fQ2X?usp=sharing)

**Other info / logs** Include any logs or source code that would be helpful to
diagnose the problem. If including tracebacks, please include the full
traceback. Large logs and files should be attached.

Error when running: 
`from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda`
```
 Using TensorFlow backend.
    Traceback (most recent call last):
      File ""train.py"", line 6, in <module>
        from yolo import create_yolov3_model, dummy_loss
      File ""/content/drive/MyDrive/yolo/yolo_plz_work/yolo.py"", line 1, in <module>
        from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda
      File ""/usr/local/lib/python3.7/dist-packages/keras/__init__.py"", line 3, in <module>
        from . import utils
      File ""/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py"", line 26, in <module>
        from .vis_utils import model_to_dot
      File ""/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py"", line 7, in <module>
        from ..models import Model
      File ""/usr/local/lib/python3.7/dist-packages/keras/models.py"", line 10, in <module>
        from .engine.input_layer import Input
      File ""/usr/local/lib/python3.7/dist-packages/keras/engine/__init__.py"", line 3, in <module>
        from .input_layer import Input
      File ""/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py"", line 7, in <module>
        from .base_layer import Layer
      File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 12, in <module>
        from .. import initializers
      File ""/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py"", line 124, in <module>
        populate_deserializable_objects()
      File ""/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py"", line 49, in populate_deserializable_objects
        LOCAL.GENERATED_WITH_V2 = tf.__internal__.tf2.enabled()
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/module_wrapper.py"", line 193, in __getattr__
        attr = getattr(self._tfmw_wrapped_module, name)
    AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__'
```
"
